[
  {
    "owner": "fareedkhan-dev",
    "repo": "all-rag-techniques",
    "content": "TITLE: Implementing Complete Self-RAG Pipeline in Python\nDESCRIPTION: This function implements the complete Self-RAG pipeline, including determining retrieval necessity, document retrieval, relevance evaluation, response generation, and assessment. It takes a query, vector store, and optional parameters as input, and returns a dictionary with the query, best response, and various metrics.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/13_self_rag.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef self_rag(query, vector_store, top_k=3):\n    \"\"\"\n    Implements the complete Self-RAG pipeline.\n    \n    Args:\n        query (str): User query\n        vector_store (SimpleVectorStore): Vector store containing document chunks\n        top_k (int): Number of documents to retrieve initially\n        \n    Returns:\n        dict: Results including query, response, and metrics from the Self-RAG process\n    \"\"\"\n    print(f\"\\n=== Starting Self-RAG for query: {query} ===\\n\")\n    \n    # Step 1: Determine if retrieval is necessary\n    print(\"Step 1: Determining if retrieval is necessary...\")\n    retrieval_needed = determine_if_retrieval_needed(query)\n    print(f\"Retrieval needed: {retrieval_needed}\")\n    \n    # Initialize metrics to track the Self-RAG process\n    metrics = {\n        \"retrieval_needed\": retrieval_needed,\n        \"documents_retrieved\": 0,\n        \"relevant_documents\": 0,\n        \"response_support_ratings\": [],\n        \"utility_ratings\": []\n    }\n    \n    best_response = None\n    best_score = -1\n    \n    if retrieval_needed:\n        # Step 2: Retrieve documents\n        print(\"\\nStep 2: Retrieving relevant documents...\")\n        query_embedding = create_embeddings(query)\n        results = vector_store.similarity_search(query_embedding, k=top_k)\n        metrics[\"documents_retrieved\"] = len(results)\n        print(f\"Retrieved {len(results)} documents\")\n        \n        # Step 3: Evaluate relevance of each document\n        print(\"\\nStep 3: Evaluating document relevance...\")\n        relevant_contexts = []\n        \n        for i, result in enumerate(results):\n            context = result[\"text\"]\n            relevance = evaluate_relevance(query, context)\n            print(f\"Document {i+1} relevance: {relevance}\")\n            \n            if relevance == \"relevant\":\n                relevant_contexts.append(context)\n        \n        metrics[\"relevant_documents\"] = len(relevant_contexts)\n        print(f\"Found {len(relevant_contexts)} relevant documents\")\n        \n        if relevant_contexts:\n            # Step 4: Process each relevant context\n            print(\"\\nStep 4: Processing relevant contexts...\")\n            for i, context in enumerate(relevant_contexts):\n                print(f\"\\nProcessing context {i+1}/{len(relevant_contexts)}...\")\n                \n                # Generate response based on the context\n                print(\"Generating response...\")\n                response = generate_response(query, context)\n                \n                # Assess how well the response is supported by the context\n                print(\"Assessing support...\")\n                support_rating = assess_support(response, context)\n                print(f\"Support rating: {support_rating}\")\n                metrics[\"response_support_ratings\"].append(support_rating)\n                \n                # Rate the utility of the response\n                print(\"Rating utility...\")\n                utility_rating = rate_utility(query, response)\n                print(f\"Utility rating: {utility_rating}/5\")\n                metrics[\"utility_ratings\"].append(utility_rating)\n                \n                # Calculate overall score (higher for better support and utility)\n                support_score = {\n                    \"fully supported\": 3, \n                    \"partially supported\": 1, \n                    \"no support\": 0\n                }.get(support_rating, 0)\n                \n                overall_score = support_score * 5 + utility_rating\n                print(f\"Overall score: {overall_score}\")\n                \n                # Keep track of the best response\n                if overall_score > best_score:\n                    best_response = response\n                    best_score = overall_score\n                    print(\"New best response found!\")\n        \n        # If no relevant contexts were found or all responses scored poorly\n        if not relevant_contexts or best_score <= 0:\n            print(\"\\nNo suitable context found or poor responses, generating without retrieval...\")\n            best_response = generate_response(query)\n    else:\n        # No retrieval needed, generate directly\n        print(\"\\nNo retrieval needed, generating response directly...\")\n        best_response = generate_response(query)\n    \n    # Final metrics\n    metrics[\"best_score\"] = best_score\n    metrics[\"used_retrieval\"] = retrieval_needed and best_score > 0\n    \n    print(\"\\n=== Self-RAG Completed ===\")\n    \n    return {\n        \"query\": query,\n        \"response\": best_response,\n        \"metrics\": metrics\n    }\n```\n\n----------------------------------------\n\nTITLE: Generating Responses for Multi-Modal RAG Queries in Python\nDESCRIPTION: This function generates a response based on a user query and retrieved multi-modal content. It formats the context from text and image results, uses a system message to guide the AI assistant, and generates a response using the OpenAI API with a specified model.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/15_multimodel_rag.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef generate_response(query, results):\n    \"\"\"\n    Generate a response based on the query and retrieved results.\n    \n    Args:\n        query (str): User query\n        results (List[Dict]): Retrieved content\n        \n    Returns:\n        str: Generated response\n    \"\"\"\n    # Format the context from the retrieved results\n    context = \"\"\n    \n    for i, result in enumerate(results):\n        # Determine the type of content (text or image caption)\n        content_type = \"Text\" if result[\"metadata\"].get(\"type\") == \"text\" else \"Image caption\"\n        # Get the page number from the metadata\n        page_num = result[\"metadata\"].get(\"page\", \"unknown\")\n        \n        # Append the content type and page number to the context\n        context += f\"[{content_type} from page {page_num}]\\n\"\n        # Append the actual content to the context\n        context += result[\"content\"]\n        context += \"\\n\\n\"\n    \n    # System message to guide the AI assistant\n    system_message = \"\"\"You are an AI assistant specializing in answering questions about documents \n    that contain both text and images. You have been given relevant text passages and image captions \n    from the document. Use this information to provide a comprehensive, accurate response to the query.\n    If information comes from an image or chart, mention this in your answer.\n    If the retrieved information doesn't fully answer the query, acknowledge the limitations.\"\"\"\n\n    # User message containing the query and the formatted context\n    user_message = f\"\"\"Query: {query}\n\n    Retrieved content:\n    {context}\n\n    Please answer the query based on the retrieved content.\n    \"\"\"\n    \n    # Generate the response using the OpenAI API\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": user_message}\n        ],\n        temperature=0.1\n    )\n    \n    # Return the generated response\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Complete RAG Pipeline with Relevant Segment Extraction\nDESCRIPTION: End-to-end function that implements the full RAG pipeline using the Relevant Segment Extraction algorithm. It processes a document, extracts chunks, finds the best segments based on the query, reconstructs the segments, and generates a response using a language model.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/9_rse.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef rag_with_rse(pdf_path, query, chunk_size=800, irrelevant_chunk_penalty=0.2):\n    \"\"\"\n    Complete RAG pipeline with Relevant Segment Extraction.\n    \n    Args:\n        pdf_path (str): Path to the document\n        query (str): User query\n        chunk_size (int): Size of chunks\n        irrelevant_chunk_penalty (float): Penalty for irrelevant chunks\n        \n    Returns:\n        Dict: Result with query, segments, and response\n    \"\"\"\n    print(\"\\n=== STARTING RAG WITH RELEVANT SEGMENT EXTRACTION ===\")\n    print(f\"Query: {query}\")\n    \n    # Process the document to extract text, chunk it, and create embeddings\n    chunks, vector_store, doc_info = process_document(pdf_path, chunk_size)\n    \n    # Calculate relevance scores and chunk values based on the query\n    print(\"\\nCalculating relevance scores and chunk values...\")\n    chunk_values = calculate_chunk_values(query, chunks, vector_store, irrelevant_chunk_penalty)\n    \n    # Find the best segments of text based on chunk values\n    best_segments, scores = find_best_segments(\n        chunk_values, \n        max_segment_length=20, \n        total_max_length=30, \n        min_segment_value=0.2\n    )\n    \n    # Reconstruct text segments from the best chunks\n    print(\"\\nReconstructing text segments from chunks...\")\n    segments = reconstruct_segments(chunks, best_segments)\n    \n    # Format the segments into a context string for the language model\n    context = format_segments_for_context(segments)\n    \n    # Generate a response from the language model using the context\n    response = generate_response(query, context)\n    \n    # Compile the result into a dictionary\n    result = {\n        \"query\": query,\n        \"segments\": segments,\n        \"response\": response\n    }\n    \n    print(\"\\n=== FINAL RESPONSE ===\")\n    print(response)\n    \n    return result\n```\n\n----------------------------------------\n\nTITLE: Querying Multi-Modal RAG System in Python\nDESCRIPTION: This function queries a multi-modal RAG system, retrieving relevant text and image content based on a user query. It uses a vector store for similarity search and separates results by content type.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/15_multimodel_rag.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef query_multimodal_rag(query, vector_store, k=5):\n    \"\"\"\n    Query the multi-modal RAG system.\n    \n    Args:\n        query (str): User query\n        vector_store (MultiModalVectorStore): Vector store with document content\n        k (int): Number of results to retrieve\n        \n    Returns:\n        Dict: Query results and generated response\n    \"\"\"\n    print(f\"\\n=== Processing query: {query} ===\\n\")\n    \n    # Generate embedding for the query\n    query_embedding = create_embeddings(query)\n    \n    # Retrieve relevant content from the vector store\n    results = vector_store.similarity_search(query_embedding, k=k)\n    \n    # Separate text and image results\n    text_results = [r for r in results if r[\"metadata\"].get(\"type\") == \"text\"]\n    image_results = [r for r in results if r[\"metadata\"].get(\"type\") == \"image\"]\n    \n    print(f\"Retrieved {len(results)} relevant items ({len(text_results)} text, {len(image_results)} image captions)\")\n    \n    # Generate a response using the retrieved content\n    response = generate_response(query, results)\n    \n    return {\n        \"query\": query,\n        \"results\": results,\n        \"response\": response,\n        \"text_results_count\": len(text_results),\n        \"image_results_count\": len(image_results)\n    }\n```\n\n----------------------------------------\n\nTITLE: Implementing Complete RAG Pipeline with Reranking Options in Python\nDESCRIPTION: A comprehensive Retrieval Augmented Generation (RAG) pipeline that incorporates different reranking methods. It performs initial vector search retrieval, applies either LLM-based or keyword-based reranking to improve result relevance, and generates a response based on the reranked context.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/8_reranker.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef rag_with_reranking(query, vector_store, reranking_method=\"llm\", top_n=3, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Complete RAG pipeline incorporating reranking.\n    \n    Args:\n        query (str): User query\n        vector_store (SimpleVectorStore): Vector store\n        reranking_method (str): Method for reranking ('llm' or 'keywords')\n        top_n (int): Number of results to return after reranking\n        model (str): Model for response generation\n        \n    Returns:\n        Dict: Results including query, context, and response\n    \"\"\"\n    # Create query embedding\n    query_embedding = create_embeddings(query)\n    \n    # Initial retrieval (get more than we need for reranking)\n    initial_results = vector_store.similarity_search(query_embedding, k=10)\n    \n    # Apply reranking\n    if reranking_method == \"llm\":\n        reranked_results = rerank_with_llm(query, initial_results, top_n=top_n)\n    elif reranking_method == \"keywords\":\n        reranked_results = rerank_with_keywords(query, initial_results, top_n=top_n)\n    else:\n        # No reranking, just use top results from initial retrieval\n        reranked_results = initial_results[:top_n]\n    \n    # Combine context from reranked results\n    context = \"\\n\\n===\\n\\n\".join([result[\"text\"] for result in reranked_results])\n    \n    # Generate response based on context\n    response = generate_response(query, context, model)\n    \n    return {\n        \"query\": query,\n        \"reranking_method\": reranking_method,\n        \"initial_results\": initial_results[:top_n],\n        \"reranked_results\": reranked_results,\n        \"context\": context,\n        \"response\": response\n    }\n```\n\n----------------------------------------\n\nTITLE: Document Processing Pipeline Implementation\nDESCRIPTION: Complete pipeline for processing documents in RAG system, including text extraction, chunking, embedding generation, and vector store population.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/8_reranker.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n    \"\"\"\n    Process a document for RAG.\n\n    Args:\n    pdf_path (str): Path to the PDF file.\n    chunk_size (int): Size of each chunk in characters.\n    chunk_overlap (int): Overlap between chunks in characters.\n\n    Returns:\n    SimpleVectorStore: A vector store containing document chunks and their embeddings.\n    \"\"\"\n    # Extract text from the PDF file\n    print(\"Extracting text from PDF...\")\n    extracted_text = extract_text_from_pdf(pdf_path)\n    \n    # Chunk the extracted text\n    print(\"Chunking text...\")\n    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n    print(f\"Created {len(chunks)} text chunks\")\n    \n    # Create embeddings for the text chunks\n    print(\"Creating embeddings for chunks...\")\n    chunk_embeddings = create_embeddings(chunks)\n    \n    # Initialize a simple vector store\n    store = SimpleVectorStore()\n    \n    # Add each chunk and its embedding to the vector store\n    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n        store.add_item(\n            text=chunk,\n            embedding=embedding,\n            metadata={\"index\": i, \"source\": pdf_path}\n        )\n    \n    print(f\"Added {len(chunks)} chunks to the vector store\")\n    return store\n```\n\n----------------------------------------\n\nTITLE: Comparing HyDE and Standard RAG Approaches\nDESCRIPTION: Function to compare HyDE and standard RAG approaches for a given query. Takes a query and vector store as input and returns detailed comparison results including responses from both approaches.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/19_HyDE_rag.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef compare_approaches(query, vector_store, reference_answer=None):\n    \"\"\"\n    Compare HyDE and standard RAG approaches for a query.\n    \n    Args:\n        query (str): User query\n        vector_store (SimpleVectorStore): Vector store with document chunks\n        reference_answer (str, optional): Reference answer for evaluation\n        \n    Returns:\n        Dict: Comparison results\n    \"\"\"\n    # Run HyDE RAG\n    hyde_result = hyde_rag(query, vector_store)\n    hyde_response = hyde_result[\"response\"]\n    \n    # Run standard RAG\n    standard_result = standard_rag(query, vector_store)\n    standard_response = standard_result[\"response\"]\n    \n    # Compare results\n    comparison = compare_responses(query, hyde_response, standard_response, reference_answer)\n    \n    return {\n        \"query\": query,\n        \"hyde_response\": hyde_response,\n        \"hyde_hypothetical_doc\": hyde_result[\"hypothetical_document\"],\n        \"standard_response\": standard_response,\n        \"reference_answer\": reference_answer,\n        \"comparison\": comparison\n    }\n```\n\n----------------------------------------\n\nTITLE: Implementing Response Generation with Query Type Handling in Python\nDESCRIPTION: Function that generates contextual responses using LLM based on different query types (Factual, Analytical, Opinion, Contextual). Uses system prompts tailored to each query type and combines retrieved document context with user queries.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/12_adaptive_rag.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef generate_response(query, results, query_type, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Generate a response based on query, retrieved documents, and query type.\n    \n    Args:\n        query (str): User query\n        results (List[Dict]): Retrieved documents\n        query_type (str): Type of query\n        model (str): LLM model\n        \n    Returns:\n        str: Generated response\n    \"\"\"\n    # Prepare context from retrieved documents by joining their texts with separators\n    context = \"\\n\\n---\\n\\n\".join([r[\"text\"] for r in results])\n    \n    # Create custom system prompt based on query type\n    if query_type == \"Factual\":\n        system_prompt = \"\"\"You are a helpful assistant providing factual information.\n    Answer the question based on the provided context. Focus on accuracy and precision.\n    If the context doesn't contain the information needed, acknowledge the limitations.\"\"\"\n        \n    elif query_type == \"Analytical\":\n        system_prompt = \"\"\"You are a helpful assistant providing analytical insights.\n    Based on the provided context, offer a comprehensive analysis of the topic.\n    Cover different aspects and perspectives in your explanation.\n    If the context has gaps, acknowledge them while providing the best analysis possible.\"\"\"\n        \n    elif query_type == \"Opinion\":\n        system_prompt = \"\"\"You are a helpful assistant discussing topics with multiple viewpoints.\n    Based on the provided context, present different perspectives on the topic.\n    Ensure fair representation of diverse opinions without showing bias.\n    Acknowledge where the context presents limited viewpoints.\"\"\"\n        \n    elif query_type == \"Contextual\":\n        system_prompt = \"\"\"You are a helpful assistant providing contextually relevant information.\n    Answer the question considering both the query and its context.\n    Make connections between the query context and the information in the provided documents.\n    If the context doesn't fully address the specific situation, acknowledge the limitations.\"\"\"\n        \n    else:\n        system_prompt = \"\"\"You are a helpful assistant. Answer the question based on the provided context. If you cannot answer from the context, acknowledge the limitations.\"\"\"\n    \n    # Create user prompt by combining the context and the query\n    user_prompt = f\"\"\"\n    Context:\n    {context}\n\n    Question: {query}\n\n    Please provide a helpful response based on the context.\n    \"\"\"\n    \n    # Generate response using the OpenAI client\n    response = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0.2\n    )\n    \n    # Return the generated response content\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Complete RAG Pipeline with Query Transformations in Python\nDESCRIPTION: This function implements the complete RAG pipeline with optional query transformations. It processes a document, performs retrieval with transformed queries, and generates a response based on the retrieved context.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/7_query_transform.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef rag_with_query_transformation(pdf_path, query, transformation_type=None):\n    \"\"\"\n    Run complete RAG pipeline with optional query transformation.\n    \n    Args:\n        pdf_path (str): Path to PDF document\n        query (str): User query\n        transformation_type (str): Type of transformation (None, 'rewrite', 'step_back', or 'decompose')\n        \n    Returns:\n        Dict: Results including query, transformed query, context, and response\n    \"\"\"\n    # Process the document to create a vector store\n    vector_store = process_document(pdf_path)\n    \n    # Apply query transformation and search\n    if transformation_type:\n        # Perform search with transformed query\n        results = transformed_search(query, vector_store, transformation_type)\n    else:\n        # Perform regular search without transformation\n        query_embedding = create_embeddings(query)\n        results = vector_store.similarity_search(query_embedding, k=3)\n    \n    # Combine context from search results\n    context = \"\\n\\n\".join([f\"PASSAGE {i+1}:\\n{result['text']}\" for i, result in enumerate(results)])\n    \n    # Generate response based on the query and combined context\n    response = generate_response(query, context)\n    \n    # Return the results including original query, transformation type, context, and response\n    return {\n        \"original_query\": query,\n        \"transformation_type\": transformation_type,\n        \"context\": context,\n        \"response\": response\n    }\n```\n\n----------------------------------------\n\nTITLE: Implementing Complete RAG Pipeline with Adaptive Retrieval in Python\nDESCRIPTION: Main pipeline function that orchestrates the entire RAG process including document processing, query classification, adaptive retrieval, and response generation. Handles PDF input and returns comprehensive results including query analysis and retrieved documents.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/12_adaptive_rag.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef rag_with_adaptive_retrieval(pdf_path, query, k=4, user_context=None):\n    \"\"\"\n    Complete RAG pipeline with adaptive retrieval.\n    \n    Args:\n        pdf_path (str): Path to PDF document\n        query (str): User query\n        k (int): Number of documents to retrieve\n        user_context (str): Optional user context\n        \n    Returns:\n        Dict: Results including query, retrieved documents, query type, and response\n    \"\"\"\n    print(\"\\n=== RAG WITH ADAPTIVE RETRIEVAL ===\")\n    print(f\"Query: {query}\")\n    \n    # Process the document to extract text, chunk it, and create embeddings\n    chunks, vector_store = process_document(pdf_path)\n    \n    # Classify the query to determine its type\n    query_type = classify_query(query)\n    print(f\"Query classified as: {query_type}\")\n    \n    # Retrieve documents using the adaptive retrieval strategy based on the query type\n    retrieved_docs = adaptive_retrieval(query, vector_store, k, user_context)\n    \n    # Generate a response based on the query, retrieved documents, and query type\n    response = generate_response(query, retrieved_docs, query_type)\n    \n    # Compile the results into a dictionary\n    result = {\n        \"query\": query,\n        \"query_type\": query_type,\n        \"retrieved_documents\": retrieved_docs,\n        \"response\": response\n    }\n    \n    print(\"\\n=== RESPONSE ===\")\n    print(response)\n    \n    return result\n```\n\n----------------------------------------\n\nTITLE: Implementing Complete HyDE RAG Pipeline in Python\nDESCRIPTION: This function implements the full Hypothetical Document Embedding (HyDE) RAG pipeline. It generates a hypothetical document based on the query, creates an embedding for that document, uses the embedding to retrieve similar document chunks, and optionally generates a final response based on the retrieved chunks.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/19_HyDE_rag.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef hyde_rag(query, vector_store, k=5, should_generate_response=True):\n    \"\"\"\n    Perform RAG using Hypothetical Document Embedding.\n    \n    Args:\n        query (str): User query\n        vector_store (SimpleVectorStore): Vector store with document chunks\n        k (int): Number of chunks to retrieve\n        generate_response (bool): Whether to generate a final response\n        \n    Returns:\n        Dict: Results including hypothetical document and retrieved chunks\n    \"\"\"\n    print(f\"\\n=== Processing query with HyDE: {query} ===\\n\")\n    \n    # Step 1: Generate a hypothetical document that answers the query\n    print(\"Generating hypothetical document...\")\n    hypothetical_doc = generate_hypothetical_document(query)\n    print(f\"Generated hypothetical document of {len(hypothetical_doc)} characters\")\n    \n    # Step 2: Create embedding for the hypothetical document\n    print(\"Creating embedding for hypothetical document...\")\n    hypothetical_embedding = create_embeddings([hypothetical_doc])[0]\n    \n    # Step 3: Retrieve similar chunks based on the hypothetical document\n    print(f\"Retrieving {k} most similar chunks...\")\n    retrieved_chunks = vector_store.similarity_search(hypothetical_embedding, k=k)\n    \n    # Prepare the results dictionary\n    results = {\n        \"query\": query,\n        \"hypothetical_document\": hypothetical_doc,\n        \"retrieved_chunks\": retrieved_chunks\n    }\n    \n    # Step 4: Generate a response if requested\n    if should_generate_response:\n        print(\"Generating final response...\")\n        response = generate_response(query, retrieved_chunks)\n        results[\"response\"] = response\n    \n    return results\n```\n\n----------------------------------------\n\nTITLE: Building Document Processing Pipeline for RAG\nDESCRIPTION: Function to process a document for RAG, including text extraction, chunking, embedding generation, and storing in a vector store. It takes a PDF path and chunk parameters as input.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/10_contextual_compression.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n    \"\"\"\n    Process a document for RAG.\n\n    Args:\n    pdf_path (str): Path to the PDF file.\n    chunk_size (int): Size of each chunk in characters.\n    chunk_overlap (int): Overlap between chunks in characters.\n\n    Returns:\n    SimpleVectorStore: A vector store containing document chunks and their embeddings.\n    \"\"\"\n    # Extract text from the PDF file\n    print(\"Extracting text from PDF...\")\n    extracted_text = extract_text_from_pdf(pdf_path)\n    \n    # Chunk the extracted text into smaller segments\n    print(\"Chunking text...\")\n    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n    print(f\"Created {len(chunks)} text chunks\")\n    \n    # Create embeddings for each text chunk\n    print(\"Creating embeddings for chunks...\")\n    chunk_embeddings = create_embeddings(chunks)\n    \n    # Initialize a simple vector store to store the chunks and their embeddings\n    store = SimpleVectorStore()\n    \n    # Add each chunk and its corresponding embedding to the vector store\n    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n        store.add_item(\n            text=chunk,\n            embedding=embedding,\n            metadata={\"index\": i, \"source\": pdf_path}\n        )\n    \n    print(f\"Added {len(chunks)} chunks to the vector store\")\n    return store\n```\n\n----------------------------------------\n\nTITLE: RSE Document Processing Pipeline\nDESCRIPTION: Main function implementing the RSE document processing pipeline, including text extraction, chunking, embedding generation, and vector store population.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/9_rse.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef process_document(pdf_path, chunk_size=800):\n    \"\"\"\n    Process a document for use with RSE.\n    \n    Args:\n        pdf_path (str): Path to the PDF document\n        chunk_size (int): Size of each chunk in characters\n        \n    Returns:\n        Tuple[List[str], SimpleVectorStore, Dict]: Chunks, vector store, and document info\n    \"\"\"\n    print(\"Extracting text from document...\")\n    # Extract text from the PDF file\n    text = extract_text_from_pdf(pdf_path)\n    \n    print(\"Chunking text into non-overlapping segments...\")\n    # Chunk the extracted text into non-overlapping segments\n    chunks = chunk_text(text, chunk_size=chunk_size, overlap=0)\n    print(f\"Created {len(chunks)} chunks\")\n    \n    print(\"Generating embeddings for chunks...\")\n    # Generate embeddings for the text chunks\n    chunk_embeddings = create_embeddings(chunks)\n    \n    # Create an instance of the SimpleVectorStore\n    vector_store = SimpleVectorStore()\n    \n    # Add documents with metadata (including chunk index for later reconstruction)\n    metadata = [{\"chunk_index\": i, \"source\": pdf_path} for i in range(len(chunks))]\n    vector_store.add_documents(chunks, chunk_embeddings, metadata)\n    \n    # Track original document structure for segment reconstruction\n    doc_info = {\n        \"chunks\": chunks,\n        \"source\": pdf_path,\n    }\n    \n    return chunks, vector_store, doc_info\n```\n\n----------------------------------------\n\nTITLE: Implementing Complete Graph RAG Pipeline in Python\nDESCRIPTION: End-to-end function that implements the complete Graph RAG pipeline from document processing to answer generation. It extracts text from PDFs, builds a knowledge graph, traverses it for relevant information, generates a response to a query, and visualizes the graph traversal process.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/17_graph_rag.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef graph_rag_pipeline(pdf_path, query, chunk_size=1000, chunk_overlap=200, top_k=3):\n    \"\"\"\n    Complete Graph RAG pipeline from document to answer.\n    \n    Args:\n        pdf_path (str): Path to the PDF document\n        query (str): The user's question\n        chunk_size (int): Size of text chunks\n        chunk_overlap (int): Overlap between chunks\n        top_k (int): Number of top nodes to consider for traversal\n        \n    Returns:\n        Dict: Results including answer and graph visualization data\n    \"\"\"\n    # Extract text from the PDF document\n    text = extract_text_from_pdf(pdf_path)\n    \n    # Split the extracted text into overlapping chunks\n    chunks = chunk_text(text, chunk_size, chunk_overlap)\n    \n    # Build a knowledge graph from the text chunks\n    graph, embeddings = build_knowledge_graph(chunks)\n    \n    # Traverse the knowledge graph to find relevant information for the query\n    relevant_chunks, traversal_path = traverse_graph(query, graph, embeddings, top_k)\n    \n    # Generate a response based on the query and the relevant chunks\n    response = generate_response(query, relevant_chunks)\n    \n    # Visualize the graph traversal path\n    visualize_graph_traversal(graph, traversal_path)\n    \n    # Return the query, response, relevant chunks, traversal path, and the graph\n    return {\n        \"query\": query,\n        \"response\": response,\n        \"relevant_chunks\": relevant_chunks,\n        \"traversal_path\": traversal_path,\n        \"graph\": graph\n    }\n```\n\n----------------------------------------\n\nTITLE: Evaluating RAG Performance Using Multiple Metrics in Python\nDESCRIPTION: This function evaluates RAG pipeline performance by calculating relevance, accuracy, and quality scores. It processes lists of queries and their ground truth data, then returns average scores across all metrics. The function depends on external functions for retrieving chunks and generating responses.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_rag_performance(\n    queries: List[str], \n    ground_truth_chunks: List[str], \n    ground_truth_responses: List[str]\n) -> Dict[str, float]:\n    \"\"\"\n    Evaluate the performance of the RAG pipeline using relevance, accuracy, and response quality metrics.\n\n    Args:\n        queries (List[str]): A list of query strings to evaluate.\n        ground_truth_chunks (List[str]): A list of ground truth text chunks corresponding to the queries.\n        ground_truth_responses (List[str]): A list of ground truth responses corresponding to the queries.\n\n    Returns:\n        Dict[str, float]: A dictionary containing the average relevance, accuracy, and quality scores.\n    \"\"\"\n    # Initialize lists to store scores for each metric\n    relevance_scores: List[float] = []\n    accuracy_scores: List[float] = []\n    quality_scores: List[float] = []\n\n    # Iterate through each query and its corresponding ground truth data\n    for query, ground_truth_chunk, ground_truth_response in zip(queries, ground_truth_chunks, ground_truth_responses):\n        # Retrieve relevant chunks for the query\n        retrieved_chunks: List[str] = retrieve_relevant_chunks(query)\n        \n        # Evaluate the relevance of the retrieved chunks compared to the ground truth chunk\n        relevance: float = evaluate_relevance(retrieved_chunks, [ground_truth_chunk])\n        relevance_scores.append(relevance)\n\n        # Generate a response using the basic RAG pipeline\n        response: str = basic_rag_pipeline(query)\n        \n        # Evaluate the accuracy of the generated response compared to the ground truth response\n        accuracy: float = evaluate_accuracy([response], [ground_truth_response])\n        accuracy_scores.append(accuracy)\n\n        # Evaluate the quality of the generated response\n        quality: float = evaluate_response_quality([response])\n        quality_scores.append(quality)\n\n    # Calculate the average scores for each metric\n    avg_relevance: float = np.mean(relevance_scores)\n    avg_accuracy: float = np.mean(accuracy_scores)\n    avg_quality: float = np.mean(quality_scores)\n\n    # Return the average scores as a dictionary\n    return {\n        \"average_relevance\": avg_relevance,\n        \"average_accuracy\": avg_accuracy,\n        \"average_quality\": avg_quality\n    }\n```\n\n----------------------------------------\n\nTITLE: Hierarchical Retrieval in Python\nDESCRIPTION: This function performs hierarchical retrieval using summary and detailed vector stores. It first retrieves relevant summaries, then uses them to filter and retrieve detailed chunks from relevant pages, enhancing the efficiency and relevance of the search process.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/18_hierarchy_rag.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef retrieve_hierarchically(query, summary_store, detailed_store, k_summaries=3, k_chunks=5):\n    \"\"\"\n    Retrieve information using hierarchical indices.\n    \n    Args:\n        query (str): User query\n        summary_store (SimpleVectorStore): Store of document summaries\n        detailed_store (SimpleVectorStore): Store of detailed chunks\n        k_summaries (int): Number of summaries to retrieve\n        k_chunks (int): Number of chunks to retrieve per summary\n        \n    Returns:\n        List[Dict]: Retrieved chunks with relevance scores\n    \"\"\"\n    print(f\"Performing hierarchical retrieval for query: {query}\")\n    \n    # Create query embedding\n    query_embedding = create_embeddings(query)\n    \n    # First, retrieve relevant summaries\n    summary_results = summary_store.similarity_search(\n        query_embedding, \n        k=k_summaries\n    )\n    \n    print(f\"Retrieved {len(summary_results)} relevant summaries\")\n    \n    # Collect pages from relevant summaries\n    relevant_pages = [result[\"metadata\"][\"page\"] for result in summary_results]\n    \n    # Create a filter function to only keep chunks from relevant pages\n    def page_filter(metadata):\n        return metadata[\"page\"] in relevant_pages\n    \n    # Then, retrieve detailed chunks from only those relevant pages\n    detailed_results = detailed_store.similarity_search(\n        query_embedding, \n        k=k_chunks * len(relevant_pages),\n        filter_func=page_filter\n    )\n    \n    print(f\"Retrieved {len(detailed_results)} detailed chunks from relevant pages\")\n    \n    # For each result, add which summary/page it came from\n    for result in detailed_results:\n        page = result[\"metadata\"][\"page\"]\n        matching_summaries = [s for s in summary_results if s[\"metadata\"][\"page\"] == page]\n        if matching_summaries:\n            result[\"summary\"] = matching_summaries[0][\"text\"]\n    \n    return detailed_results\n```\n\n----------------------------------------\n\nTITLE: Evaluating Adaptive vs Standard Retrieval in Python\nDESCRIPTION: This function compares adaptive retrieval with standard retrieval on a set of test queries. It processes a document, runs both retrieval methods on each query, and compares their performance. If reference answers are provided, it also evaluates the quality of responses against these references.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/12_adaptive_rag.ipynb#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_adaptive_vs_standard(pdf_path, test_queries, reference_answers=None):\n    \"\"\"\n    Compare adaptive retrieval with standard retrieval on a set of test queries.\n    \n    This function processes a document, runs both standard and adaptive retrieval methods\n    on each test query, and compares their performance. If reference answers are provided,\n    it also evaluates the quality of responses against these references.\n    \n    Args:\n        pdf_path (str): Path to PDF document to be processed as the knowledge source\n        test_queries (List[str]): List of test queries to evaluate both retrieval methods\n        reference_answers (List[str], optional): Reference answers for evaluation metrics\n        \n    Returns:\n        Dict: Evaluation results containing individual query results and overall comparison\n    \"\"\"\n    print(\"=== EVALUATING ADAPTIVE VS. STANDARD RETRIEVAL ===\")\n    \n    # Process document to extract text, create chunks and build the vector store\n    chunks, vector_store = process_document(pdf_path)\n    \n    # Initialize collection for storing comparison results\n    results = []\n    \n    # Process each test query with both retrieval methods\n    for i, query in enumerate(test_queries):\n        print(f\"\\n\\nQuery {i+1}: {query}\")\n        \n        # --- Standard retrieval approach ---\n        print(\"\\n--- Standard Retrieval ---\")\n        # Create embedding for the query\n        query_embedding = create_embeddings(query)\n        # Retrieve documents using simple vector similarity\n        standard_docs = vector_store.similarity_search(query_embedding, k=4)\n        # Generate response using a generic approach\n        standard_response = generate_response(query, standard_docs, \"General\")\n        \n        # --- Adaptive retrieval approach ---\n        print(\"\\n--- Adaptive Retrieval ---\")\n        # Classify the query to determine its type (Factual, Analytical, Opinion, Contextual)\n        query_type = classify_query(query)\n        # Retrieve documents using the strategy appropriate for this query type\n        adaptive_docs = adaptive_retrieval(query, vector_store, k=4)\n        # Generate a response tailored to the query type\n        adaptive_response = generate_response(query, adaptive_docs, query_type)\n        \n        # Store complete results for this query\n        result = {\n            \"query\": query,\n            \"query_type\": query_type,\n            \"standard_retrieval\": {\n                \"documents\": standard_docs,\n                \"response\": standard_response\n            },\n            \"adaptive_retrieval\": {\n                \"documents\": adaptive_docs,\n                \"response\": adaptive_response\n            }\n        }\n        \n        # Add reference answer if available for this query\n        if reference_answers and i < len(reference_answers):\n            result[\"reference_answer\"] = reference_answers[i]\n            \n        results.append(result)\n        \n        # Display preview of both responses for quick comparison\n        print(\"\\n--- Responses ---\")\n        print(f\"Standard: {standard_response[:200]}...\")\n        print(f\"Adaptive: {adaptive_response[:200]}...\")\n    \n    # Calculate comparative metrics if reference answers are available\n    if reference_answers:\n        comparison = compare_responses(results)\n        print(\"\\n=== EVALUATION RESULTS ===\")\n        print(comparison)\n    \n    # Return the complete evaluation results\n    return {\n        \"results\": results,\n        \"comparison\": comparison if reference_answers else \"No reference answers provided for evaluation\"\n    }\n```\n\n----------------------------------------\n\nTITLE: Implementing Traditional RAG Function\nDESCRIPTION: Implements a traditional RAG (Retrieval-Augmented Generation) approach that retrieves relevant documents and generates responses based on combined context. Takes a query and vector store as input and returns a generated response.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/13_self_rag.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef traditional_rag(query, vector_store, top_k=3):\n    \"\"\"\n    Implements a traditional RAG approach for comparison.\n    \n    Args:\n        query (str): User query\n        vector_store (SimpleVectorStore): Vector store containing document chunks\n        top_k (int): Number of documents to retrieve\n        \n    Returns:\n        str: Generated response\n    \"\"\"\n    print(f\"\\n=== Running traditional RAG for query: {query} ===\\n\")\n    \n    # Retrieve documents\n    print(\"Retrieving documents...\")\n    query_embedding = create_embeddings(query)  # Create embeddings for the query\n    results = vector_store.similarity_search(query_embedding, k=top_k)  # Search for similar documents\n    print(f\"Retrieved {len(results)} documents\")\n    \n    # Combine contexts from retrieved documents\n    contexts = [result[\"text\"] for result in results]  # Extract text from results\n    combined_context = \"\\n\\n\".join(contexts)  # Combine texts into a single context\n    \n    # Generate response using the combined context\n    print(\"Generating response...\")\n    response = generate_response(query, combined_context)  # Generate response based on the combined context\n    \n    return response\n```\n\n----------------------------------------\n\nTITLE: Evaluating RAG Approaches Function\nDESCRIPTION: Implements a comparison framework between Self-RAG and traditional RAG approaches. Processes test queries, generates responses using both methods, and compares results against reference answers if provided.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/13_self_rag.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_rag_approaches(pdf_path, test_queries, reference_answers=None):\n    \"\"\"\n    Compare Self-RAG with traditional RAG.\n    \n    Args:\n        pdf_path (str): Path to the document\n        test_queries (List[str]): List of test queries\n        reference_answers (List[str], optional): Reference answers for evaluation\n        \n    Returns:\n        dict: Evaluation results\n    \"\"\"\n    print(\"=== Evaluating RAG Approaches ===\")\n    \n    # Process document to create a vector store\n    vector_store = process_document(pdf_path)\n    \n    results = []\n    \n    for i, query in enumerate(test_queries):\n        print(f\"\\nProcessing query {i+1}: {query}\")\n        \n        # Run Self-RAG\n        self_rag_result = self_rag(query, vector_store)  # Get response from Self-RAG\n        self_rag_response = self_rag_result[\"response\"]\n        \n        # Run traditional RAG\n        trad_rag_response = traditional_rag(query, vector_store)  # Get response from traditional RAG\n        \n        # Compare results if reference answer is available\n        reference = reference_answers[i] if reference_answers and i < len(reference_answers) else None\n        comparison = compare_responses(query, self_rag_response, trad_rag_response, reference)  # Compare responses\n        \n        results.append({\n            \"query\": query,\n            \"self_rag_response\": self_rag_response,\n            \"traditional_rag_response\": trad_rag_response,\n            \"reference_answer\": reference,\n            \"comparison\": comparison,\n            \"self_rag_metrics\": self_rag_result[\"metrics\"]\n        })\n    \n    # Generate overall analysis\n    overall_analysis = generate_overall_analysis(results)\n    \n    return {\n        \"results\": results,\n        \"overall_analysis\": overall_analysis\n    }\n```\n\n----------------------------------------\n\nTITLE: Running Proposition Chunking Evaluation in Python for RAG Systems\nDESCRIPTION: This function runs a complete evaluation of proposition chunking vs standard chunking for a given PDF document and set of test queries. It processes the document, builds vector stores, generates responses, and evaluates the results for both approaches.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/14_proposition_chunking.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef run_proposition_chunking_evaluation(pdf_path, test_queries, reference_answers=None):\n    \"\"\"\n    Run a complete evaluation of proposition chunking vs standard chunking.\n    \n    Args:\n        pdf_path (str): Path to the PDF file\n        test_queries (List[str]): List of test queries\n        reference_answers (List[str], optional): Reference answers for queries\n        \n    Returns:\n        Dict: Evaluation results\n    \"\"\"\n    print(\"=== Starting Proposition Chunking Evaluation ===\\n\")\n    \n    # Process document into propositions and chunks\n    chunks, propositions = process_document_into_propositions(pdf_path)\n    \n    # Build vector stores for chunks and propositions\n    chunk_store, prop_store = build_vector_stores(chunks, propositions)\n    \n    # Initialize a list to store results for each query\n    results = []\n    \n    # Run tests for each query\n    for i, query in enumerate(test_queries):\n        print(f\"\\n\\n=== Testing Query {i+1}/{len(test_queries)} ===\")\n        print(f\"Query: {query}\")\n        \n        # Get retrieval results from both chunk-based and proposition-based approaches\n        retrieval_results = compare_retrieval_approaches(query, chunk_store, prop_store)\n        \n        # Generate responses based on the retrieved proposition-based results\n        print(\"\\nGenerating response from proposition-based results...\")\n        prop_response = generate_response(\n            query, \n            retrieval_results[\"proposition_results\"], \n            \"proposition\"\n        )\n        \n        # Generate responses based on the retrieved chunk-based results\n        print(\"Generating response from chunk-based results...\")\n        chunk_response = generate_response(\n            query, \n            retrieval_results[\"chunk_results\"], \n            \"chunk\"\n        )\n        \n        # Get reference answer if available\n        reference = None\n        if reference_answers and i < len(reference_answers):\n            reference = reference_answers[i]\n        \n        # Evaluate the generated responses\n        print(\"\\nEvaluating responses...\")\n        evaluation = evaluate_responses(query, prop_response, chunk_response, reference)\n        \n        # Compile results for the current query\n        query_result = {\n            \"query\": query,\n            \"proposition_results\": retrieval_results[\"proposition_results\"],\n            \"chunk_results\": retrieval_results[\"chunk_results\"],\n            \"proposition_response\": prop_response,\n            \"chunk_response\": chunk_response,\n            \"reference_answer\": reference,\n            \"evaluation\": evaluation\n        }\n        \n        # Append the results to the overall results list\n        results.append(query_result)\n        \n        # Print the responses and evaluation for the current query\n        print(\"\\n=== Proposition-Based Response ===\")\n        print(prop_response)\n        \n        print(\"\\n=== Chunk-Based Response ===\")\n        print(chunk_response)\n        \n        print(\"\\n=== Evaluation ===\")\n        print(evaluation)\n    \n    # Generate overall analysis of the evaluation\n    print(\"\\n\\n=== Generating Overall Analysis ===\")\n    overall_analysis = generate_overall_analysis(results)\n    print(\"\\n\" + overall_analysis)\n    \n    # Return the evaluation results, overall analysis, and counts of propositions and chunks\n    return {\n        \"results\": results,\n        \"overall_analysis\": overall_analysis,\n        \"proposition_count\": len(propositions),\n        \"chunk_count\": len(chunks)\n    }\n```\n\n----------------------------------------\n\nTITLE: Implementing Fusion Retrieval with Vector and BM25 Search in Python\nDESCRIPTION: Function that combines vector-based and BM25 search results using a weighted fusion approach. It normalizes both score types, computes combined scores with a weighting factor alpha, sorts the results, and returns the top k results.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/16_fusion_rag.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef fusion_retrieval(query, chunks, vector_store, bm25_index, k=5, alpha=0.5):\n    \"\"\"\n    Perform fusion retrieval combining vector-based and BM25 search.\n    \n    Args:\n        query (str): Query string\n        chunks (List[Dict]): Original text chunks\n        vector_store (SimpleVectorStore): Vector store\n        bm25_index (BM25Okapi): BM25 index\n        k (int): Number of results to return\n        alpha (float): Weight for vector scores (0-1), where 1-alpha is BM25 weight\n        \n    Returns:\n        List[Dict]: Top k results based on combined scores\n    \"\"\"\n    print(f\"Performing fusion retrieval for query: {query}\")\n    \n    # Define small epsilon to avoid division by zero\n    epsilon = 1e-8\n    \n    # Get vector search results\n    query_embedding = create_embeddings(query)  # Create embedding for the query\n    vector_results = vector_store.similarity_search_with_scores(query_embedding, k=len(chunks))  # Perform vector search\n    \n    # Get BM25 search results\n    bm25_results = bm25_search(bm25_index, chunks, query, k=len(chunks))  # Perform BM25 search\n    \n    # Create dictionaries to map document index to score\n    vector_scores_dict = {result[\"metadata\"][\"index\"]: result[\"similarity\"] for result in vector_results}\n    bm25_scores_dict = {result[\"metadata\"][\"index\"]: result[\"bm25_score\"] for result in bm25_results}\n    \n    # Ensure all documents have scores for both methods\n    all_docs = vector_store.get_all_documents()\n    combined_results = []\n    \n    for i, doc in enumerate(all_docs):\n        vector_score = vector_scores_dict.get(i, 0.0)  # Get vector score or 0 if not found\n        bm25_score = bm25_scores_dict.get(i, 0.0)  # Get BM25 score or 0 if not found\n        combined_results.append({\n            \"text\": doc[\"text\"],\n            \"metadata\": doc[\"metadata\"],\n            \"vector_score\": vector_score,\n            \"bm25_score\": bm25_score,\n            \"index\": i\n        })\n    \n    # Extract scores as arrays\n    vector_scores = np.array([doc[\"vector_score\"] for doc in combined_results])\n    bm25_scores = np.array([doc[\"bm25_score\"] for doc in combined_results])\n    \n    # Normalize scores\n    norm_vector_scores = (vector_scores - np.min(vector_scores)) / (np.max(vector_scores) - np.min(vector_scores) + epsilon)\n    norm_bm25_scores = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores) + epsilon)\n    \n    # Compute combined scores\n    combined_scores = alpha * norm_vector_scores + (1 - alpha) * norm_bm25_scores\n    \n    # Add combined scores to results\n    for i, score in enumerate(combined_scores):\n        combined_results[i][\"combined_score\"] = float(score)\n    \n    # Sort by combined score (descending)\n    combined_results.sort(key=lambda x: x[\"combined_score\"], reverse=True)\n    \n    # Return top k results\n    top_results = combined_results[:k]\n    \n    print(f\"Retrieved {len(top_results)} documents with fusion retrieval\")\n    return top_results\n```\n\n----------------------------------------\n\nTITLE: Generating AI Responses Based on Retrieved Context in Python\nDESCRIPTION: This function generates an AI response based on a system prompt and user message using a specified model. It includes a system prompt that instructs the AI to answer strictly based on the provided context and handles cases where information is insufficient.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/6_doc_augmentation_rag.ipynb#2025-04-23_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n# Define the system prompt for the AI assistant\nsystem_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\n\ndef generate_response(system_prompt, user_message, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Generates a response from the AI model based on the system prompt and user message.\n\n    Args:\n    system_prompt (str): The system prompt to guide the AI's behavior.\n    user_message (str): The user's message or query.\n    model (str): The model to be used for generating the response. Default is \"meta-llama/Llama-2-7B-chat-hf\".\n\n    Returns:\n    dict: The response from the AI model.\n    \"\"\"\n    response = client.chat.completions.create(\n        model=model,\n        temperature=0,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_message}\n        ]\n    )\n    return response\n\n# Create the user prompt based on the top chunks\nuser_prompt = \"\\n\".join([f\"Context {i + 1}:\\n{chunk}\\n=====================================\\n\" for i, chunk in enumerate(top_chunks)])\nuser_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n\n# Generate AI response\nai_response = generate_response(system_prompt, user_prompt)\n```\n\n----------------------------------------\n\nTITLE: Complete RAG Pipeline with Hierarchical Retrieval in Python\nDESCRIPTION: This function implements a complete hierarchical RAG pipeline. It processes documents, creates or loads vector stores, performs hierarchical retrieval, and generates responses. It also includes caching mechanisms to improve efficiency for repeated queries on the same document.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/18_hierarchy_rag.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef hierarchical_rag(query, pdf_path, chunk_size=1000, chunk_overlap=200, \n                    k_summaries=3, k_chunks=5, regenerate=False):\n    \"\"\"\n    Complete hierarchical RAG pipeline.\n    \n    Args:\n        query (str): User query\n        pdf_path (str): Path to the PDF document\n        chunk_size (int): Size of each detailed chunk\n        chunk_overlap (int): Overlap between chunks\n        k_summaries (int): Number of summaries to retrieve\n        k_chunks (int): Number of chunks to retrieve per summary\n        regenerate (bool): Whether to regenerate vector stores\n        \n    Returns:\n        Dict: Results including response and retrieved chunks\n    \"\"\"\n    # Create store filenames for caching\n    summary_store_file = f\"{os.path.basename(pdf_path)}_summary_store.pkl\"\n    detailed_store_file = f\"{os.path.basename(pdf_path)}_detailed_store.pkl\"\n    \n    # Process document and create stores if needed\n    if regenerate or not os.path.exists(summary_store_file) or not os.path.exists(detailed_store_file):\n        print(\"Processing document and creating vector stores...\")\n        # Process the document to create hierarchical indices and vector stores\n        summary_store, detailed_store = process_document_hierarchically(\n            pdf_path, chunk_size, chunk_overlap\n        )\n        \n        # Save the summary store to a file for future use\n        with open(summary_store_file, 'wb') as f:\n            pickle.dump(summary_store, f)\n        \n        # Save the detailed store to a file for future use\n        with open(detailed_store_file, 'wb') as f:\n            pickle.dump(detailed_store, f)\n    else:\n        # Load existing summary store from file\n        print(\"Loading existing vector stores...\")\n        with open(summary_store_file, 'rb') as f:\n            summary_store = pickle.load(f)\n        \n        # Load existing detailed store from file\n        with open(detailed_store_file, 'rb') as f:\n            detailed_store = pickle.load(f)\n    \n    # Retrieve relevant chunks hierarchically using the query\n    retrieved_chunks = retrieve_hierarchically(\n        query, summary_store, detailed_store, k_summaries, k_chunks\n    )\n    \n    # Generate a response based on the retrieved chunks\n    response = generate_response(query, retrieved_chunks)\n    \n    # Return results including the query, response, retrieved chunks, and counts of summaries and detailed chunks\n    return {\n        \"query\": query,\n        \"response\": response,\n        \"retrieved_chunks\": retrieved_chunks,\n        \"summary_count\": len(summary_store.texts),\n        \"detailed_count\": len(detailed_store.texts)\n    }\n```\n\n----------------------------------------\n\nTITLE: Adaptive Retrieval System Implementation in Python\nDESCRIPTION: A function that performs adaptive retrieval by classifying the query type and selecting the appropriate retrieval strategy. It supports factual, analytical, opinion, and contextual query types, with different strategies for each.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/12_adaptive_rag.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef adaptive_retrieval(query, vector_store, k=4, user_context=None):\n    \"\"\"\n    Perform adaptive retrieval by selecting and executing the appropriate strategy.\n    \n    Args:\n        query (str): User query\n        vector_store (SimpleVectorStore): Vector store\n        k (int): Number of documents to retrieve\n        user_context (str): Optional user context for contextual queries\n        \n    Returns:\n        List[Dict]: Retrieved documents\n    \"\"\"\n    # Classify the query to determine its type\n    query_type = classify_query(query)\n    print(f\"Query classified as: {query_type}\")\n    \n    # Select and execute the appropriate retrieval strategy based on the query type\n    if query_type == \"Factual\":\n        # Use the factual retrieval strategy for precise information\n        results = factual_retrieval_strategy(query, vector_store, k)\n    elif query_type == \"Analytical\":\n        # Use the analytical retrieval strategy for comprehensive coverage\n        results = analytical_retrieval_strategy(query, vector_store, k)\n    elif query_type == \"Opinion\":\n        # Use the opinion retrieval strategy for diverse perspectives\n        results = opinion_retrieval_strategy(query, vector_store, k)\n    elif query_type == \"Contextual\":\n        # Use the contextual retrieval strategy, incorporating user context\n        results = contextual_retrieval_strategy(query, vector_store, k, user_context)\n    else:\n        # Default to factual retrieval strategy if classification fails\n        results = factual_retrieval_strategy(query, vector_store, k)\n    \n    return results  # Return the retrieved documents\n```\n\n----------------------------------------\n\nTITLE: Complete RAG Pipeline Implementation with Compression\nDESCRIPTION: Main function implementing the complete RAG pipeline with contextual compression, including document processing, chunk retrieval, compression, and response generation.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/10_contextual_compression.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef rag_with_compression(pdf_path, query, k=10, compression_type=\"selective\", model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Complete RAG pipeline with contextual compression.\n    \n    Args:\n        pdf_path (str): Path to PDF document\n        query (str): User query\n        k (int): Number of chunks to retrieve initially\n        compression_type (str): Type of compression\n        model (str): LLM model to use\n        \n    Returns:\n        dict: Results including query, compressed chunks, and response\n    \"\"\"\n    print(\"\\n=== RAG WITH CONTEXTUAL COMPRESSION ===\")\n    print(f\"Query: {query}\")\n    print(f\"Compression type: {compression_type}\")\n    \n    vector_store = process_document(pdf_path)\n    \n    query_embedding = create_embeddings(query)\n    \n    print(f\"Retrieving top {k} chunks...\")\n    results = vector_store.similarity_search(query_embedding, k=k)\n    retrieved_chunks = [result[\"text\"] for result in results]\n    \n    compressed_results = batch_compress_chunks(retrieved_chunks, query, compression_type, model)\n    compressed_chunks = [result[0] for result in compressed_results]\n    compression_ratios = [result[1] for result in compressed_results]\n    \n    filtered_chunks = [(chunk, ratio) for chunk, ratio in zip(compressed_chunks, compression_ratios) if chunk.strip()]\n    \n    if not filtered_chunks:\n        print(\"Warning: All chunks were compressed to empty strings. Using original chunks.\")\n        filtered_chunks = [(chunk, 0.0) for chunk in retrieved_chunks]\n    else:\n        compressed_chunks, compression_ratios = zip(*filtered_chunks)\n    \n    context = \"\\n\\n---\\n\\n\".join(compressed_chunks)\n    \n    print(\"Generating response based on compressed chunks...\")\n    response = generate_response(query, context, model)\n    \n    result = {\n        \"query\": query,\n        \"original_chunks\": retrieved_chunks,\n        \"compressed_chunks\": compressed_chunks,\n        \"compression_ratios\": compression_ratios,\n        \"context_length_reduction\": f\"{sum(compression_ratios)/len(compression_ratios):.2f}%\",\n        \"response\": response\n    }\n    \n    print(\"\\n=== RESPONSE ===\")\n    print(response)\n    \n    return result\n```\n\n----------------------------------------\n\nTITLE: Comparing Retrieval Methods for RAG in Python\nDESCRIPTION: This function compares different retrieval methods (vector-only, BM25-only, and fusion) for a given query. It runs each method, generates responses, and evaluates them. It requires text chunks, a vector store, a BM25 index, and optional parameters for retrieval and evaluation.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/16_fusion_rag.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef compare_retrieval_methods(query, chunks, vector_store, bm25_index, k=5, alpha=0.5, reference_answer=None):\n    \"\"\"\n    Compare different retrieval methods for a query.\n    \n    Args:\n        query (str): User query\n        chunks (List[Dict]): Text chunks\n        vector_store (SimpleVectorStore): Vector store\n        bm25_index (BM25Okapi): BM25 index\n        k (int): Number of documents to retrieve\n        alpha (float): Weight for vector scores in fusion retrieval\n        reference_answer (str, optional): Reference answer for comparison\n        \n    Returns:\n        Dict: Comparison results\n    \"\"\"\n    print(f\"\\n=== Comparing retrieval methods for query: {query} ===\\n\")\n    \n    # Run vector-only RAG\n    print(\"\\nRunning vector-only RAG...\")\n    vector_result = vector_only_rag(query, vector_store, k)\n    \n    # Run BM25-only RAG\n    print(\"\\nRunning BM25-only RAG...\")\n    bm25_result = bm25_only_rag(query, chunks, bm25_index, k)\n    \n    # Run fusion RAG\n    print(\"\\nRunning fusion RAG...\")\n    fusion_result = answer_with_fusion_rag(query, chunks, vector_store, bm25_index, k, alpha)\n    \n    # Compare responses from different retrieval methods\n    print(\"\\nComparing responses...\")\n    comparison = evaluate_responses(\n        query, \n        vector_result[\"response\"], \n        bm25_result[\"response\"], \n        fusion_result[\"response\"],\n        reference_answer\n    )\n    \n    # Return the comparison results\n    return {\n        \"query\": query,\n        \"vector_result\": vector_result,\n        \"bm25_result\": bm25_result,\n        \"fusion_result\": fusion_result,\n        \"comparison\": comparison\n    }\n```\n\n----------------------------------------\n\nTITLE: Evaluating Multi-Modal vs Text-Only RAG Performance\nDESCRIPTION: Function to compare multi-modal RAG with text-only RAG using test queries and optional reference answers. It processes documents for both approaches and generates detailed comparison results.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/15_multimodel_rag.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_multimodal_vs_textonly(pdf_path, test_queries, reference_answers=None):\n    \"\"\"\n    Compare multi-modal RAG with text-only RAG.\n    \n    Args:\n        pdf_path (str): Path to the PDF file\n        test_queries (List[str]): Test queries\n        reference_answers (List[str], optional): Reference answers\n        \n    Returns:\n        Dict: Evaluation results\n    \"\"\"\n    print(\"=== EVALUATING MULTI-MODAL RAG VS TEXT-ONLY RAG ===\\n\")\n    \n    # Process document for multi-modal RAG\n    print(\"\\nProcessing document for multi-modal RAG...\")\n    mm_vector_store, mm_doc_info = process_document(pdf_path)\n    \n    # Build text-only store\n    print(\"\\nProcessing document for text-only RAG...\")\n    text_vector_store = build_text_only_store(pdf_path)\n    \n    # Run evaluation for each query\n    results = []\n    \n    for i, query in enumerate(test_queries):\n        print(f\"\\n\\n=== Evaluating Query {i+1}: {query} ===\")\n        \n        # Get reference answer if available\n        reference = None\n        if reference_answers and i < len(reference_answers):\n            reference = reference_answers[i]\n        \n        # Run multi-modal RAG\n        print(\"\\nRunning multi-modal RAG...\")\n        mm_result = query_multimodal_rag(query, mm_vector_store)\n        \n        # Run text-only RAG\n        print(\"\\nRunning text-only RAG...\")\n        text_result = query_multimodal_rag(query, text_vector_store)\n        \n        # Compare responses\n        comparison = compare_responses(query, mm_result[\"response\"], text_result[\"response\"], reference)\n        \n        # Add to results\n        results.append({\n            \"query\": query,\n            \"multimodal_response\": mm_result[\"response\"],\n            \"textonly_response\": text_result[\"response\"],\n            \"multimodal_results\": {\n                \"text_count\": mm_result[\"text_results_count\"],\n                \"image_count\": mm_result[\"image_results_count\"]\n            },\n            \"reference_answer\": reference,\n            \"comparison\": comparison\n        })\n    \n    # Generate overall analysis\n    overall_analysis = generate_overall_analysis(results)\n    \n    return {\n        \"results\": results,\n        \"overall_analysis\": overall_analysis,\n        \"multimodal_doc_info\": mm_doc_info\n    }\n```\n\n----------------------------------------\n\nTITLE: Implementing Standard RAG Pipeline in Python\nDESCRIPTION: This function implements a standard non-hierarchical RAG pipeline that extracts text from a PDF, chunks it, creates embeddings, and performs similarity search to retrieve relevant chunks for answering a query.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/18_hierarchy_rag.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef standard_rag(query, pdf_path, chunk_size=1000, chunk_overlap=200, k=15):\n    \"\"\"\n    Standard RAG pipeline without hierarchical retrieval.\n    \n    Args:\n        query (str): User query\n        pdf_path (str): Path to the PDF document\n        chunk_size (int): Size of each chunk\n        chunk_overlap (int): Overlap between chunks\n        k (int): Number of chunks to retrieve\n        \n    Returns:\n        Dict: Results including response and retrieved chunks\n    \"\"\"\n    # Extract pages from the PDF document\n    pages = extract_text_from_pdf(pdf_path)\n    \n    # Create chunks directly from all pages\n    chunks = []\n    for page in pages:\n        # Chunk the text of the page\n        page_chunks = chunk_text(\n            page[\"text\"], \n            page[\"metadata\"], \n            chunk_size, \n            chunk_overlap\n        )\n        # Extend the chunks list with the chunks from the current page\n        chunks.extend(page_chunks)\n    \n    print(f\"Created {len(chunks)} chunks for standard RAG\")\n    \n    # Create a vector store to hold the chunks\n    store = SimpleVectorStore()\n    \n    # Create embeddings for the chunks\n    print(\"Creating embeddings for chunks...\")\n    texts = [chunk[\"text\"] for chunk in chunks]\n    embeddings = create_embeddings(texts)\n    \n    # Add chunks to the vector store\n    for i, chunk in enumerate(chunks):\n        store.add_item(\n            text=chunk[\"text\"],\n            embedding=embeddings[i],\n            metadata=chunk[\"metadata\"]\n        )\n    \n    # Create an embedding for the query\n    query_embedding = create_embeddings(query)\n    \n    # Retrieve the most relevant chunks based on the query embedding\n    retrieved_chunks = store.similarity_search(query_embedding, k=k)\n    print(f\"Retrieved {len(retrieved_chunks)} chunks with standard RAG\")\n    \n    # Generate a response based on the retrieved chunks\n    response = generate_response(query, retrieved_chunks)\n    \n    # Return the results including the query, response, and retrieved chunks\n    return {\n        \"query\": query,\n        \"response\": response,\n        \"retrieved_chunks\": retrieved_chunks\n    }\n```\n\n----------------------------------------\n\nTITLE: Evaluating RAG Methods: RSE vs Standard Top-K Retrieval in Python\nDESCRIPTION: This function compares the performance of Relevant Segment Extraction (RSE) and standard top-k retrieval for RAG. It runs both methods on a given query and document, and optionally evaluates their responses against a reference answer using a language model.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/9_rse.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_methods(pdf_path, query, reference_answer=None):\n    \"\"\"\n    Compare RSE with standard top-k retrieval.\n    \n    Args:\n        pdf_path (str): Path to the document\n        query (str): User query\n        reference_answer (str, optional): Reference answer for evaluation\n    \"\"\"\n    print(\"\\n========= EVALUATION =========\\n\")\n    \n    # Run the RAG with Relevant Segment Extraction (RSE) method\n    rse_result = rag_with_rse(pdf_path, query)\n    \n    # Run the standard top-k retrieval method\n    standard_result = standard_top_k_retrieval(pdf_path, query)\n    \n    # If a reference answer is provided, evaluate the responses\n    if reference_answer:\n        print(\"\\n=== COMPARING RESULTS ===\")\n        \n        # Create an evaluation prompt to compare the responses against the reference answer\n        evaluation_prompt = f\"\"\"\n            Query: {query}\n\n            Reference Answer:\n            {reference_answer}\n\n            Response from Standard Retrieval:\n            {standard_result[\"response\"]}\n\n            Response from Relevant Segment Extraction:\n            {rse_result[\"response\"]}\n\n            Compare these two responses against the reference answer. Which one is:\n            1. More accurate and comprehensive\n            2. Better at addressing the user's query\n            3. Less likely to include irrelevant information\n\n            Explain your reasoning for each point.\n        \"\"\"\n        \n        print(\"Evaluating responses against reference answer...\")\n        \n        # Generate the evaluation using the specified model\n        evaluation = client.chat.completions.create(\n            model=\"meta-llama/Llama-3.2-3B-Instruct\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are an objective evaluator of RAG system responses.\"},\n                {\"role\": \"user\", \"content\": evaluation_prompt}\n            ]\n        )\n        \n        # Print the evaluation results\n        print(\"\\n=== EVALUATION RESULTS ===\")\n        print(evaluation.choices[0].message.content)\n    \n    # Return the results of both methods\n    return {\n        \"rse_result\": rse_result,\n        \"standard_result\": standard_result\n    }\n```\n\n----------------------------------------\n\nTITLE: Complete RAG Workflow with Feedback Collection in Python\nDESCRIPTION: Orchestrates the entire RAG process from document processing to feedback collection. It loads historical feedback, processes documents, optionally fine-tunes the vector index, executes the RAG pipeline, collects user feedback, and stores it for future improvement.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/11_feedback_loop_rag.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef full_rag_workflow(pdf_path, query, feedback_data=None, feedback_file=\"feedback_data.json\", fine_tune=False):\n    \"\"\"\n    Execute a complete RAG workflow with feedback integration for continuous improvement.\n    \n    This function orchestrates the entire Retrieval-Augmented Generation process:\n    1. Load historical feedback data\n    2. Process and chunk the document\n    3. Optionally fine-tune the vector index with prior feedback\n    4. Perform retrieval and generation with feedback-adjusted relevance scores\n    5. Collect new user feedback for future improvement\n    6. Store feedback to enable system learning over time\n    \n    Args:\n        pdf_path (str): Path to the PDF document to be processed\n        query (str): User's natural language query\n        feedback_data (List[Dict], optional): Pre-loaded feedback data, loads from file if None\n        feedback_file (str): Path to the JSON file storing feedback history\n        fine_tune (bool): Whether to enhance the index with successful past Q&A pairs\n        \n    Returns:\n        Dict: Results containing the response and retrieval metadata\n    \"\"\"\n    # Step 1: Load historical feedback for relevance adjustment if not explicitly provided\n    if feedback_data is None:\n        feedback_data = load_feedback_data(feedback_file)\n        print(f\"Loaded {len(feedback_data)} feedback entries from {feedback_file}\")\n    \n    # Step 2: Process document through extraction, chunking and embedding pipeline\n    chunks, vector_store = process_document(pdf_path)\n    \n    # Step 3: Fine-tune the vector index by incorporating high-quality past interactions\n    # This creates enhanced retrievable content from successful Q&A pairs\n    if fine_tune and feedback_data:\n        vector_store = fine_tune_index(vector_store, chunks, feedback_data)\n    \n    # Step 4: Execute core RAG with feedback-aware retrieval\n    # Note: This depends on the rag_with_feedback_loop function which should be defined elsewhere\n    result = rag_with_feedback_loop(query, vector_store, feedback_data)\n    \n    # Step 5: Collect user feedback to improve future performance\n    print(\"\\n=== Would you like to provide feedback on this response? ===\")\n    print(\"Rate relevance (1-5, with 5 being most relevant):\")\n    relevance = input()\n    \n    print(\"Rate quality (1-5, with 5 being highest quality):\")\n    quality = input()\n    \n    print(\"Any comments? (optional, press Enter to skip)\")\n    comments = input()\n    \n    # Step 6: Format feedback into structured data\n    feedback = get_user_feedback(\n        query=query,\n        response=result[\"response\"],\n        relevance=int(relevance),\n        quality=int(quality),\n        comments=comments\n    )\n    \n    # Step 7: Persist feedback to enable continuous system learning\n    store_feedback(feedback, feedback_file)\n    print(\"Feedback recorded. Thank you!\")\n    \n    return result\n```\n\n----------------------------------------\n\nTITLE: Processing Documents into Quality-Checked Propositions in Python\nDESCRIPTION: A comprehensive pipeline that processes PDF documents into high-quality propositions. It extracts text, chunks it, generates propositions for each chunk, evaluates their quality, and filters out those that don't meet specified quality thresholds across accuracy, clarity, completeness, and conciseness dimensions.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/14_proposition_chunking.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef process_document_into_propositions(pdf_path, chunk_size=800, chunk_overlap=100, \n                                      quality_thresholds=None):\n    \"\"\"\n    Process a document into quality-checked propositions.\n    \n    Args:\n        pdf_path (str): Path to the PDF file\n        chunk_size (int): Size of each chunk in characters\n        chunk_overlap (int): Overlap between chunks in characters\n        quality_thresholds (Dict): Threshold scores for proposition quality\n        \n    Returns:\n        Tuple[List[Dict], List[Dict]]: Original chunks and proposition chunks\n    \"\"\"\n    # Set default quality thresholds if not provided\n    if quality_thresholds is None:\n        quality_thresholds = {\n            \"accuracy\": 7,\n            \"clarity\": 7,\n            \"completeness\": 7,\n            \"conciseness\": 7\n        }\n    \n    # Extract text from the PDF file\n    text = extract_text_from_pdf(pdf_path)\n    \n    # Create chunks from the extracted text\n    chunks = chunk_text(text, chunk_size, chunk_overlap)\n    \n    # Initialize a list to store all propositions\n    all_propositions = []\n    \n    print(\"Generating propositions from chunks...\")\n    for i, chunk in enumerate(chunks):\n        print(f\"Processing chunk {i+1}/{len(chunks)}...\")\n        \n        # Generate propositions for the current chunk\n        chunk_propositions = generate_propositions(chunk)\n        print(f\"Generated {len(chunk_propositions)} propositions\")\n        \n        # Process each generated proposition\n        for prop in chunk_propositions:\n            proposition_data = {\n                \"text\": prop,\n                \"source_chunk_id\": chunk[\"chunk_id\"],\n                \"source_text\": chunk[\"text\"]\n            }\n            all_propositions.append(proposition_data)\n    \n    # Evaluate the quality of the generated propositions\n    print(\"\\nEvaluating proposition quality...\")\n    quality_propositions = []\n    \n    for i, prop in enumerate(all_propositions):\n        if i % 10 == 0:  # Status update every 10 propositions\n            print(f\"Evaluating proposition {i+1}/{len(all_propositions)}...\")\n            \n        # Evaluate the quality of the current proposition\n        scores = evaluate_proposition(prop[\"text\"], prop[\"source_text\"])\n        prop[\"quality_scores\"] = scores\n        \n        # Check if the proposition passes the quality thresholds\n        passes_quality = True\n        for metric, threshold in quality_thresholds.items():\n            if scores.get(metric, 0) < threshold:\n                passes_quality = False\n                break\n        \n        if passes_quality:\n            quality_propositions.append(prop)\n        else:\n            print(f\"Proposition failed quality check: {prop['text'][:50]}...\")\n    \n    print(f\"\\nRetained {len(quality_propositions)}/{len(all_propositions)} propositions after quality filtering\")\n    \n    return chunks, quality_propositions\n```\n\n----------------------------------------\n\nTITLE: Generating Responses Based on Retrieved Chunks in RAG\nDESCRIPTION: This function generates a response from an AI model based on retrieved chunks. It uses a system prompt to guide the AI's behavior and a user message that includes the relevant chunks and the query. This is the final step in the RAG process, where the model generates an answer based on the retrieved information.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/5_contextual_chunk_headers_rag.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Define the system prompt for the AI assistant\nsystem_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\n\ndef generate_response(system_prompt, user_message, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Generates a response from the AI model based on the system prompt and user message.\n\n    Args:\n    system_prompt (str): The system prompt to guide the AI's behavior.\n    user_message (str): The user's message or query.\n    model (str): The model to be used for generating the response. Default is \"meta-llama/Llama-2-7B-chat-hf\".\n\n    Returns:\n    dict: The response from the AI model.\n    \"\"\"\n    response = client.chat.completions.create(\n        model=model,\n        temperature=0,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_message}\n        ]\n    )\n    return response\n\n# Create the user prompt based on the top chunks\nuser_prompt = \"\\n\".join([f\"Header: {chunk['header']}\\nContent:\\n{chunk['text']}\" for chunk in top_chunks])\nuser_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Core CRAG Process in Python\nDESCRIPTION: Main function that implements the Corrective RAG process. Handles document retrieval, relevance evaluation, and adaptive knowledge acquisition based on relevance scores. Includes web search integration for low-relevance cases and combines multiple knowledge sources for medium-relevance scenarios.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/20_crag.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef crag_process(query, vector_store, k=3):\n    \"\"\"\n    Run the Corrective RAG process.\n    \n    Args:\n        query (str): User query\n        vector_store (SimpleVectorStore): Vector store with document chunks\n        k (int): Number of initial documents to retrieve\n        \n    Returns:\n        Dict: Process results including response and debug info\n    \"\"\"\n    print(f\"\\n=== Processing query with CRAG: {query} ===\\n\")\n    \n    # Step 1: Create query embedding and retrieve documents\n    print(\"Retrieving initial documents...\")\n    query_embedding = create_embeddings(query)\n    retrieved_docs = vector_store.similarity_search(query_embedding, k=k)\n    \n    # Step 2: Evaluate document relevance\n    print(\"Evaluating document relevance...\")\n    relevance_scores = []\n    for doc in retrieved_docs:\n        score = evaluate_document_relevance(query, doc[\"text\"])\n        relevance_scores.append(score)\n        doc[\"relevance\"] = score\n        print(f\"Document scored {score:.2f} relevance\")\n    \n    # Step 3: Determine action based on best relevance score\n    max_score = max(relevance_scores) if relevance_scores else 0\n    best_doc_idx = relevance_scores.index(max_score) if relevance_scores else -1\n    \n    # Track sources for attribution\n    sources = []\n    final_knowledge = \"\"\n    \n    # Step 4: Execute the appropriate knowledge acquisition strategy\n    if max_score > 0.7:\n        # Case 1: High relevance - Use document directly\n        print(f\"High relevance ({max_score:.2f}) - Using document directly\")\n        best_doc = retrieved_docs[best_doc_idx][\"text\"]\n        final_knowledge = best_doc\n        sources.append({\n            \"title\": \"Document\",\n            \"url\": \"\"\n        })\n        \n    elif max_score < 0.3:\n        # Case 2: Low relevance - Use web search\n        print(f\"Low relevance ({max_score:.2f}) - Performing web search\")\n        web_results, web_sources = perform_web_search(query)\n        final_knowledge = refine_knowledge(web_results)\n        sources.extend(web_sources)\n        \n    else:\n        # Case 3: Medium relevance - Combine document with web search\n        print(f\"Medium relevance ({max_score:.2f}) - Combining document with web search\")\n        best_doc = retrieved_docs[best_doc_idx][\"text\"]\n        refined_doc = refine_knowledge(best_doc)\n        \n        # Get web results\n        web_results, web_sources = perform_web_search(query)\n        refined_web = refine_knowledge(web_results)\n        \n        # Combine knowledge\n        final_knowledge = f\"From document:\\n{refined_doc}\\n\\nFrom web search:\\n{refined_web}\"\n        \n        # Add sources\n        sources.append({\n            \"title\": \"Document\",\n            \"url\": \"\"\n        })\n        sources.extend(web_sources)\n    \n    # Step 5: Generate final response\n    print(\"Generating final response...\")\n    response = generate_response(query, final_knowledge, sources)\n    \n    # Return comprehensive results\n    return {\n        \"query\": query,\n        \"response\": response,\n        \"retrieved_docs\": retrieved_docs,\n        \"relevance_scores\": relevance_scores,\n        \"max_relevance\": max_score,\n        \"final_knowledge\": final_knowledge,\n        \"sources\": sources\n    }\n```\n\n----------------------------------------\n\nTITLE: Evaluating Different Query Transformation Techniques for RAG in Python\nDESCRIPTION: This function evaluates multiple transformation techniques (original, rewrite, step_back, decompose) for the same query against a PDF document. It runs RAG for each technique, collects the responses, and optionally compares them against a reference answer using the compare_responses function.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/7_query_transform.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_transformations(pdf_path, query, reference_answer=None):\n    \"\"\"\n    Evaluate different transformation techniques for the same query.\n    \n    Args:\n        pdf_path (str): Path to PDF document\n        query (str): Query to evaluate\n        reference_answer (str): Optional reference answer for comparison\n        \n    Returns:\n        Dict: Evaluation results\n    \"\"\"\n    # Define the transformation techniques to evaluate\n    transformation_types = [None, \"rewrite\", \"step_back\", \"decompose\"]\n    results = {}\n    \n    # Run RAG with each transformation technique\n    for transformation_type in transformation_types:\n        type_name = transformation_type if transformation_type else \"original\"\n        print(f\"\\n===== Running RAG with {type_name} query =====\")\n        \n        # Get the result for the current transformation type\n        result = rag_with_query_transformation(pdf_path, query, transformation_type)\n        results[type_name] = result\n        \n        # Print the response for the current transformation type\n        print(f\"Response with {type_name} query:\")\n        print(result[\"response\"])\n        print(\"=\" * 50)\n    \n    # Compare results if a reference answer is provided\n    if reference_answer:\n        compare_responses(results, reference_answer)\n    \n    return results\n```\n\n----------------------------------------\n\nTITLE: Complete RAG Pipeline with Feedback Integration in Python\nDESCRIPTION: Implements a full retrieval-augmented generation pipeline that incorporates feedback to improve retrieval quality. It handles embedding creation, document retrieval, relevance score adjustment based on feedback, context building, and response generation.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/11_feedback_loop_rag.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef rag_with_feedback_loop(query, vector_store, feedback_data, k=5, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Complete RAG pipeline incorporating feedback loop.\n    \n    Args:\n        query (str): User query\n        vector_store (SimpleVectorStore): Vector store with document chunks\n        feedback_data (List[Dict]): History of feedback\n        k (int): Number of documents to retrieve\n        model (str): LLM model for response generation\n        \n    Returns:\n        Dict: Results including query, retrieved documents, and response\n    \"\"\"\n    print(f\"\\n=== Processing query with feedback-enhanced RAG ===\")\n    print(f\"Query: {query}\")\n    \n    # Step 1: Create query embedding\n    query_embedding = create_embeddings(query)\n    \n    # Step 2: Perform initial retrieval based on query embedding\n    results = vector_store.similarity_search(query_embedding, k=k)\n    \n    # Step 3: Adjust relevance scores of retrieved documents based on feedback\n    adjusted_results = adjust_relevance_scores(query, results, feedback_data)\n    \n    # Step 4: Extract texts from adjusted results for context building\n    retrieved_texts = [result[\"text\"] for result in adjusted_results]\n    \n    # Step 5: Build context for response generation by concatenating retrieved texts\n    context = \"\\n\\n---\\n\\n\".join(retrieved_texts)\n    \n    # Step 6: Generate response using the context and query\n    print(\"Generating response...\")\n    response = generate_response(query, context, model)\n    \n    # Step 7: Compile the final result\n    result = {\n        \"query\": query,\n        \"retrieved_documents\": adjusted_results,\n        \"response\": response\n    }\n    \n    print(\"\\n=== Response ===\")\n    print(response)\n    \n    return result\n```\n\n----------------------------------------\n\nTITLE: Generating Overall RAG Analysis\nDESCRIPTION: Function that generates comprehensive analysis comparing multi-modal and text-only RAG approaches across multiple test queries using LLaMA 3.2 for evaluation.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/15_multimodel_rag.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef generate_overall_analysis(results):\n    \"\"\"\n    Generate an overall analysis of multi-modal vs text-only RAG.\n    \n    Args:\n        results (List[Dict]): Evaluation results for each query\n        \n    Returns:\n        str: Overall analysis\n    \"\"\"\n    # System prompt for the evaluator\n    system_prompt = \"\"\"You are an expert evaluator of RAG systems. Provide an overall analysis comparing \n    multi-modal RAG (text + images) versus text-only RAG based on multiple test queries.\n\n    Focus on:\n    1. Types of queries where multi-modal RAG outperforms text-only\n    2. Specific advantages of incorporating image information\n    3. Any disadvantages or limitations of the multi-modal approach\n    4. Overall recommendation on when to use each approach\"\"\"\n\n    # Create summary of evaluations\n    evaluations_summary = \"\"\n    for i, result in enumerate(results):\n        evaluations_summary += f\"Query {i+1}: {result['query']}\\n\"\n        evaluations_summary += f\"Multi-modal retrieved {result['multimodal_results']['text_count']} text chunks and {result['multimodal_results']['image_count']} image captions\\n\"\n        evaluations_summary += f\"Comparison summary: {result['comparison'][:200]}...\\n\\n\"\n\n    # User prompt with evaluations summary\n    user_prompt = f\"\"\"Based on the following evaluations of multi-modal vs text-only RAG across {len(results)} queries, \n    provide an overall analysis comparing these two approaches:\n\n    {evaluations_summary}\n\n    Please provide a comprehensive analysis of the relative strengths and weaknesses of multi-modal RAG \n    compared to text-only RAG, with specific attention to how image information contributed (or didn't contribute) to response quality.\"\"\"\n\n    # Generate overall analysis using meta-llama/Llama-3.2-3B-Instruct\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0\n    )\n    \n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Implementing a Basic RAG Pipeline in Python\nDESCRIPTION: Function that orchestrates the Retrieval-Augmented Generation process by retrieving relevant context chunks, constructing a prompt, and generating a response. This represents the core functionality of the RAG system in a single integrated function.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef basic_rag_pipeline(query: str) -> str:\n    \"\"\"\n    Implement the basic Retrieval-Augmented Generation (RAG) pipeline:\n    retrieve relevant chunks, construct a prompt, and generate a response.\n\n    Args:\n        query (str): The input query for which a response is to be generated.\n\n    Returns:\n        str: The generated response from the LLM based on the query and retrieved context.\n    \"\"\"\n    # Step 1: Retrieve the most relevant chunks for the given query\n    relevant_chunks: List[str] = retrieve_relevant_chunks(query)\n    \n    # Step 2: Construct a prompt using the query and the retrieved chunks\n    prompt: str = construct_prompt(query, relevant_chunks)\n    \n    # Step 3: Generate a response from the LLM using the constructed prompt\n    response: str = generate_response(prompt)\n    \n    # Return the generated response\n    return response\n```\n\n----------------------------------------\n\nTITLE: Implementing RL Training Loop for RAG in Python\nDESCRIPTION: Comprehensive implementation of the training loop for RL-enhanced RAG. It initializes parameters, runs multiple episodes, performs RL steps, tracks performance, and compares the RL-enhanced RAG with a simple RAG baseline.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndef training_loop(\n    query_text: str, \n    ground_truth: str, \n    params: Optional[Dict[str, Union[float, int]]] = None\n) -> Tuple[Dict[str, Dict[str, Union[float, str]]], List[float], List[List[str]], Optional[str]]:\n    \"\"\"\n    Implement the training loop for RL-enhanced RAG.\n\n    Args:\n        query_text (str): The input query text for the RAG pipeline.\n        ground_truth (str): The expected correct answer for the query.\n        params (Optional[Dict[str, Union[float, int]]]): Training parameters such as learning rate, \n            number of episodes, and discount factor. If None, default parameters are initialized.\n\n    Returns:\n        Tuple: A tuple containing:\n            - policy (Dict[str, Dict[str, Union[float, str]]]): The updated policy after training.\n            - rewards_history (List[float]): A list of rewards received in each episode.\n            - actions_history (List[List[str]]): A list of actions taken in each episode.\n            - best_response (Optional[str]): The best response generated during training.\n    \"\"\"\n    # Initialize training parameters if not provided\n    if params is None:\n        params = initialize_training_params()\n    \n    # Initialize variables to track progress\n    rewards_history: List[float] = []  # List to store rewards for each episode\n    actions_history: List[List[str]] = []  # List to store actions taken in each episode\n    policy: Dict[str, Dict[str, Union[float, str]]] = {}  # Policy dictionary to store actions and rewards\n    action_space: List[str] = define_action_space()  # Define the action space\n    best_response: Optional[str] = None  # Variable to store the best response\n    best_reward: float = -1  # Initialize the best reward to a very low value\n    \n    # Get initial performance from the simple RAG pipeline for comparison\n    simple_response: str = basic_rag_pipeline(query_text)\n    simple_reward: float = calculate_reward(simple_response, ground_truth)\n    print(f\"Simple RAG reward: {simple_reward:.4f}\")\n\n    # Start the training loop\n    for episode in range(params[\"num_episodes\"]):\n        # Reset the environment with the same query\n        context_chunks: List[str] = retrieve_relevant_chunks(query_text)\n        state: Dict[str, object] = define_state(query_text, context_chunks)\n        episode_reward: float = 0  # Initialize the reward for the current episode\n        episode_actions: List[str] = []  # Initialize the list of actions for the current episode\n        \n        # Maximum number of steps per episode to prevent infinite loops\n        for step in range(10):\n            # Perform a single RL step\n            state, action, reward, response = rl_step(state, action_space, ground_truth)\n            episode_actions.append(action)  # Record the action taken\n            \n            # If a response is generated, end the episode\n            if response:\n                episode_reward = reward  # Update the episode reward\n                \n                # Track the best response and reward\n                if reward > best_reward:\n                    best_reward = reward\n                    best_response = response\n                \n                break  # Exit the loop as the episode ends\n        \n        # Update rewards and actions history\n        rewards_history.append(episode_reward)\n        actions_history.append(episode_actions)\n        \n        # Print progress every 5 episodes\n        if episode % 5 == 0:\n            print(f\"Episode {episode}: Reward = {episode_reward:.4f}, Actions = {episode_actions}\")\n    \n    # Compare the best RL-enhanced RAG reward with the simple RAG reward\n    improvement: float = best_reward - simple_reward\n    print(f\"\\nTraining completed:\")\n    print(f\"Simple RAG reward: {simple_reward:.4f}\")\n    print(f\"Best RL-enhanced RAG reward: {best_reward:.4f}\")\n    print(f\"Improvement: {improvement:.4f} ({improvement * 100:.2f}%)\")\n\n    return policy, rewards_history, actions_history, best_response\n```\n\n----------------------------------------\n\nTITLE: Implementing Standard RAG for Document Retrieval in Python\nDESCRIPTION: A function that implements standard Retrieval-Augmented Generation without compression. It processes a document, generates embeddings, retrieves relevant chunks based on query similarity, and produces a response using an LLM. The function returns the query, retrieved chunks, and the generated response.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/10_contextual_compression.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef standard_rag(pdf_path, query, k=10, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Standard RAG without compression.\n    \n    Args:\n        pdf_path (str): Path to PDF document\n        query (str): User query\n        k (int): Number of chunks to retrieve\n        model (str): LLM model to use\n        \n    Returns:\n        dict: Results including query, chunks, and response\n    \"\"\"\n    print(\"\\n=== STANDARD RAG ===\")\n    print(f\"Query: {query}\")\n    \n    # Process the document to extract text, chunk it, and create embeddings\n    vector_store = process_document(pdf_path)\n    \n    # Create an embedding for the query\n    query_embedding = create_embeddings(query)\n    \n    # Retrieve the top k most similar chunks based on the query embedding\n    print(f\"Retrieving top {k} chunks...\")\n    results = vector_store.similarity_search(query_embedding, k=k)\n    retrieved_chunks = [result[\"text\"] for result in results]\n    \n    # Generate context from the retrieved chunks\n    context = \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n    \n    # Generate a response based on the retrieved chunks\n    print(\"Generating response...\")\n    response = generate_response(query, context, model)\n    \n    # Prepare the result dictionary\n    result = {\n        \"query\": query,\n        \"chunks\": retrieved_chunks,\n        \"response\": response\n    }\n    \n    print(\"\\n=== RESPONSE ===\")\n    print(response)\n    \n    return result\n```\n\n----------------------------------------\n\nTITLE: Generating Hypothetical Document for HyDE in Python\nDESCRIPTION: This function creates a hypothetical document that answers a given query using an LLM. It sets up a system prompt and user prompt to instruct the model to generate an authoritative document of specified length that directly answers the question, then makes an API call to generate the content.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/19_HyDE_rag.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef generate_hypothetical_document(query, desired_length=1000):\n    \"\"\"\n    Generate a hypothetical document that answers the query.\n    \n    Args:\n        query (str): User query\n        desired_length (int): Target length of the hypothetical document\n        \n    Returns:\n        str: Generated hypothetical document\n    \"\"\"\n    # Define the system prompt to instruct the model on how to generate the document\n    system_prompt = f\"\"\"You are an expert document creator. \n    Given a question, generate a detailed document that would directly answer this question.\n    The document should be approximately {desired_length} characters long and provide an in-depth, \n    informative answer to the question. Write as if this document is from an authoritative source\n    on the subject. Include specific details, facts, and explanations.\n    Do not mention that this is a hypothetical document - just write the content directly.\"\"\"\n\n    # Define the user prompt with the query\n    user_prompt = f\"Question: {query}\\n\\nGenerate a document that fully answers this question:\"\n    \n    # Make a request to the OpenAI API to generate the hypothetical document\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",  # Specify the model to use\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n            {\"role\": \"user\", \"content\": user_prompt}  # User message with the query\n        ],\n        temperature=0.1  # Set the temperature for response generation\n    )\n    \n    # Return the generated document content\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Implementing Factual RAG Strategy in Python\nDESCRIPTION: A retrieval strategy optimized for factual queries that focuses on precision. It uses LLM to enhance queries, creates embeddings, performs similarity search, and ranks results based on relevance scores. Requires a vector store and LLM client implementation.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/12_adaptive_rag.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef factual_retrieval_strategy(query, vector_store, k=4):\n    \"\"\"\n    Retrieval strategy for factual queries focusing on precision.\n    \n    Args:\n        query (str): User query\n        vector_store (SimpleVectorStore): Vector store\n        k (int): Number of documents to return\n        \n    Returns:\n        List[Dict]: Retrieved documents\n    \"\"\"\n    print(f\"Executing Factual retrieval strategy for: '{query}'\")\n    \n    # Use LLM to enhance the query for better precision\n    system_prompt = \"\"\"You are an expert at enhancing search queries.\n        Your task is to reformulate the given factual query to make it more precise and \n        specific for information retrieval. Focus on key entities and their relationships.\n\n        Provide ONLY the enhanced query without any explanation.\n    \"\"\"\n\n    user_prompt = f\"Enhance this factual query: {query}\"\n    \n    # Generate the enhanced query using the LLM\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0\n    )\n    \n    # Extract and print the enhanced query\n    enhanced_query = response.choices[0].message.content.strip()\n    print(f\"Enhanced query: {enhanced_query}\")\n    \n    # Create embeddings for the enhanced query\n    query_embedding = create_embeddings(enhanced_query)\n    \n    # Perform initial similarity search to retrieve documents\n    initial_results = vector_store.similarity_search(query_embedding, k=k*2)\n    \n    # Initialize a list to store ranked results\n    ranked_results = []\n    \n    # Score and rank documents by relevance using LLM\n    for doc in initial_results:\n        relevance_score = score_document_relevance(enhanced_query, doc[\"text\"])\n        ranked_results.append({\n            \"text\": doc[\"text\"],\n            \"metadata\": doc[\"metadata\"],\n            \"similarity\": doc[\"similarity\"],\n            \"relevance_score\": relevance_score\n        })\n    \n    # Sort the results by relevance score in descending order\n    ranked_results.sort(key=lambda x: x[\"relevance_score\"], reverse=True)\n    \n    # Return the top k results\n    return ranked_results[:k]\n```\n\n----------------------------------------\n\nTITLE: Implementing Contextual Retrieval Strategy with User Context Integration in Python\nDESCRIPTION: A function that implements a contextual retrieval strategy by integrating user context with queries. It can infer context if not provided, reformulate queries to incorporate context, and score documents based on both query relevance and contextual relevance.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/12_adaptive_rag.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef contextual_retrieval_strategy(query, vector_store, k=4, user_context=None):\n    \"\"\"\n    Retrieval strategy for contextual queries integrating user context.\n    \n    Args:\n        query (str): User query\n        vector_store (SimpleVectorStore): Vector store\n        k (int): Number of documents to return\n        user_context (str): Additional user context\n        \n    Returns:\n        List[Dict]: Retrieved documents\n    \"\"\"\n    print(f\"Executing Contextual retrieval strategy for: '{query}'\")\n    \n    # If no user context provided, try to infer it from the query\n    if not user_context:\n        system_prompt = \"\"\"You are an expert at understanding implied context in questions.\nFor the given query, infer what contextual information might be relevant or implied \nbut not explicitly stated. Focus on what background would help answering this query.\n\nReturn a brief description of the implied context.\"\"\"\n\n        user_prompt = f\"Infer the implied context in this query: {query}\"\n        \n        # Generate the inferred context using the LLM\n        response = client.chat.completions.create(\n            model=\"meta-llama/Llama-3.2-3B-Instruct\",\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt}\n            ],\n            temperature=0.1\n        )\n        \n        # Extract and print the inferred context\n        user_context = response.choices[0].message.content.strip()\n        print(f\"Inferred context: {user_context}\")\n    \n    # Reformulate the query to incorporate context\n    system_prompt = \"\"\"You are an expert at reformulating questions with context.\n    Given a query and some contextual information, create a more specific query that \n    incorporates the context to get more relevant information.\n\n    Return ONLY the reformulated query without explanation.\"\"\"\n\n    user_prompt = f\"\"\"\n    Query: {query}\n    Context: {user_context}\n\n    Reformulate the query to incorporate this context:\"\"\"\n    \n    # Generate the contextualized query using the LLM\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0\n    )\n    \n    # Extract and print the contextualized query\n    contextualized_query = response.choices[0].message.content.strip()\n    print(f\"Contextualized query: {contextualized_query}\")\n    \n    # Retrieve documents based on the contextualized query\n    query_embedding = create_embeddings(contextualized_query)\n    initial_results = vector_store.similarity_search(query_embedding, k=k*2)\n    \n    # Rank documents considering both relevance and user context\n    ranked_results = []\n    \n    for doc in initial_results:\n        # Score document relevance considering the context\n        context_relevance = score_document_context_relevance(query, user_context, doc[\"text\"])\n        ranked_results.append({\n            \"text\": doc[\"text\"],\n            \"metadata\": doc[\"metadata\"],\n            \"similarity\": doc[\"similarity\"],\n            \"context_relevance\": context_relevance\n        })\n    \n    # Sort by context relevance and return top k results\n    ranked_results.sort(key=lambda x: x[\"context_relevance\"], reverse=True)\n    return ranked_results[:k]\n```\n\n----------------------------------------\n\nTITLE: Implementing Complete CRAG Evaluation Pipeline in Python\nDESCRIPTION: This function runs a complete evaluation of CRAG with multiple test queries, comparing it to standard RAG. It processes a PDF document, creates a vector store, and evaluates each query, returning detailed results and an overall analysis.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/20_crag.ipynb#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef run_crag_evaluation(pdf_path, test_queries, reference_answers=None):\n    \"\"\"\n    Run a complete evaluation of CRAG with multiple test queries.\n    \n    Args:\n        pdf_path (str): Path to the PDF document\n        test_queries (List[str]): List of test queries\n        reference_answers (List[str], optional): Reference answers for queries\n        \n    Returns:\n        Dict: Complete evaluation results\n    \"\"\"\n    # Process document and create vector store\n    vector_store = process_document(pdf_path)\n    \n    results = []\n    \n    for i, query in enumerate(test_queries):\n        print(f\"\\n\\n===== Evaluating Query {i+1}/{len(test_queries)} =====\")\n        print(f\"Query: {query}\")\n        \n        # Get reference answer if available\n        reference = None\n        if reference_answers and i < len(reference_answers):\n            reference = reference_answers[i]\n        \n        # Run comparison between CRAG and standard RAG\n        result = compare_crag_vs_standard_rag(query, vector_store, reference)\n        results.append(result)\n        \n        # Display comparison results\n        print(\"\\n=== Comparison ===\")\n        print(result[\"comparison\"])\n    \n    # Generate overall analysis from individual results\n    overall_analysis = generate_overall_analysis(results)\n    \n    return {\n        \"results\": results,\n        \"overall_analysis\": overall_analysis\n    }\n```\n\n----------------------------------------\n\nTITLE: Implementing Standard Top-K Retrieval for RAG in Python\nDESCRIPTION: This function implements a standard RAG approach with top-k retrieval. It processes a PDF document, chunks the text, creates embeddings, retrieves the most relevant chunks based on a query, and generates a response using a language model.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/9_rse.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef standard_top_k_retrieval(pdf_path, query, k=10, chunk_size=800):\n    \"\"\"\n    Standard RAG with top-k retrieval.\n    \n    Args:\n        pdf_path (str): Path to the document\n        query (str): User query\n        k (int): Number of chunks to retrieve\n        chunk_size (int): Size of chunks\n        \n    Returns:\n        Dict: Result with query, chunks, and response\n    \"\"\"\n    print(\"\\n=== STARTING STANDARD TOP-K RETRIEVAL ===\")\n    print(f\"Query: {query}\")\n    \n    # Process the document to extract text, chunk it, and create embeddings\n    chunks, vector_store, doc_info = process_document(pdf_path, chunk_size)\n    \n    # Create an embedding for the query\n    print(\"Creating query embedding and retrieving chunks...\")\n    query_embedding = create_embeddings([query])[0]\n    \n    # Retrieve the top-k most relevant chunks based on the query embedding\n    results = vector_store.search(query_embedding, top_k=k)\n    retrieved_chunks = [result[\"document\"] for result in results]\n    \n    # Format the retrieved chunks into a context string\n    context = \"\\n\\n\".join([\n        f\"CHUNK {i+1}:\\n{chunk}\" \n        for i, chunk in enumerate(retrieved_chunks)\n    ])\n    \n    # Generate a response from the language model using the context\n    response = generate_response(query, context)\n    \n    # Compile the result into a dictionary\n    result = {\n        \"query\": query,\n        \"chunks\": retrieved_chunks,\n        \"response\": response\n    }\n    \n    print(\"\\n=== FINAL RESPONSE ===\")\n    print(response)\n    \n    return result\n```\n\n----------------------------------------\n\nTITLE: Generating Overall Analysis for RAG System Evaluation in Python\nDESCRIPTION: This function generates an overall analysis comparing proposition-based and chunk-based retrieval approaches for RAG systems. It uses an AI model to analyze the results from multiple test queries and provide insights on the strengths and weaknesses of each approach.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/14_proposition_chunking.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef generate_overall_analysis(results):\n    \"\"\"\n    Generate an overall analysis of proposition vs chunk approaches.\n    \n    Args:\n        results (List[Dict]): Results from each test query\n        \n    Returns:\n        str: Overall analysis\n    \"\"\"\n    # System prompt to instruct the AI on how to generate the overall analysis\n    system_prompt = \"\"\"You are an expert at evaluating information retrieval systems.\n    Based on multiple test queries, provide an overall analysis comparing proposition-based retrieval \n    to chunk-based retrieval for RAG (Retrieval-Augmented Generation) systems.\n\n    Focus on:\n    1. When proposition-based retrieval performs better\n    2. When chunk-based retrieval performs better\n    3. The overall strengths and weaknesses of each approach\n    4. Recommendations for when to use each approach\"\"\"\n\n    # Create a summary of evaluations for each query\n    evaluations_summary = \"\"\n    for i, result in enumerate(results):\n        evaluations_summary += f\"Query {i+1}: {result['query']}\\n\"\n        evaluations_summary += f\"Evaluation Summary: {result['evaluation'][:200]}...\\n\\n\"\n\n    # User prompt containing the summary of evaluations\n    user_prompt = f\"\"\"Based on the following evaluations of proposition-based vs chunk-based retrieval across {len(results)} queries, \n    provide an overall analysis comparing these two approaches:\n\n    {evaluations_summary}\n\n    Please provide a comprehensive analysis on the relative strengths and weaknesses of proposition-based \n    and chunk-based retrieval for RAG systems.\"\"\"\n\n    # Generate the overall analysis using the OpenAI client\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0\n    )\n    \n    # Return the generated analysis text\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Generating Overall RAG Analysis Function\nDESCRIPTION: Implements overall analysis generation for comparing Self-RAG and traditional RAG performance across multiple queries. Analyzes performance patterns and provides recommendations for different query types.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/13_self_rag.ipynb#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef generate_overall_analysis(results):\n    \"\"\"\n    Generate an overall analysis of Self-RAG vs traditional RAG.\n    \n    Args:\n        results (List[Dict]): Results from evaluate_rag_approaches\n        \n    Returns:\n        str: Overall analysis\n    \"\"\"\n    system_prompt = \"\"\"You are an expert evaluator of RAG systems. Your task is to provide an overall analysis comparing\n    Self-RAG and Traditional RAG based on multiple test queries.\n\n    Focus your analysis on:\n    1. When Self-RAG performs better and why\n    2. When Traditional RAG performs better and why\n    3. The impact of dynamic retrieval decisions in Self-RAG\n    4. The value of relevance and support evaluation in Self-RAG\n    5. Overall recommendations on which approach to use for different types of queries\"\"\"\n\n    # Prepare a summary of the individual comparisons\n    comparisons_summary = \"\"\n    for i, result in enumerate(results):\n        comparisons_summary += f\"Query {i+1}: {result['query']}\\n\"\n        comparisons_summary += f\"Self-RAG metrics: Retrieval needed: {result['self_rag_metrics']['retrieval_needed']}, \"\n        comparisons_summary += f\"Relevant docs: {result['self_rag_metrics']['relevant_documents']}/{result['self_rag_metrics']['documents_retrieved']}\\n\"\n        comparisons_summary += f\"Comparison summary: {result['comparison'][:200]}...\\n\\n\"\n\n        user_prompt = f\"\"\"Based on the following comparison results from {len(results)} test queries, please provide an overall analysis of\n    Self-RAG versus Traditional RAG:\n\n    {comparisons_summary}\n\n    Please provide your comprehensive analysis.\n    \"\"\"\n\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0\n    )\n    \n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Implementing Sub-query Decomposition Function for RAG Systems in Python\nDESCRIPTION: Defines a function to break down complex queries into simpler sub-queries for comprehensive retrieval. This technique helps address different aspects of a complex question by parsing the model's response into a list of sub-queries.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/7_query_transform.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef decompose_query(original_query, num_subqueries=4, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Decomposes a complex query into simpler sub-queries.\n    \n    Args:\n        original_query (str): The original complex query\n        num_subqueries (int): Number of sub-queries to generate\n        model (str): The model to use for query decomposition\n        \n    Returns:\n        List[str]: A list of simpler sub-queries\n    \"\"\"\n    # Define the system prompt to guide the AI assistant's behavior\n    system_prompt = \"You are an AI assistant specialized in breaking down complex questions. Your task is to decompose complex queries into simpler sub-questions that, when answered together, address the original query.\"\n    \n    # Define the user prompt with the original query to be decomposed\n    user_prompt = f\"\"\"\n    Break down the following complex query into {num_subqueries} simpler sub-queries. Each sub-query should focus on a different aspect of the original question.\n    \n    Original query: {original_query}\n    \n    Generate {num_subqueries} sub-queries, one per line, in this format:\n    1. [First sub-query]\n    2. [Second sub-query]\n    And so on...\n    \"\"\"\n    \n    # Generate the sub-queries using the specified model\n    response = client.chat.completions.create(\n        model=model,\n        temperature=0.2,  # Slightly higher temperature for some variation\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ]\n    )\n    \n    # Process the response to extract sub-queries\n    content = response.choices[0].message.content.strip()\n    \n    # Extract numbered queries using simple parsing\n    lines = content.split(\"\\n\")\n    sub_queries = []\n    \n    for line in lines:\n        if line.strip() and any(line.strip().startswith(f\"{i}.\") for i in range(1, 10)):\n            # Remove the number and leading space\n            query = line.strip()\n            query = query[query.find(\".\")+1:].strip()\n            sub_queries.append(query)\n    \n    return sub_queries\n```\n\n----------------------------------------\n\nTITLE: Main Fusion RAG Answer Generation Function in Python\nDESCRIPTION: Main function that answers a query using fusion RAG. It retrieves documents using the fusion retrieval method, formats the context from retrieved documents, generates a response using an LLM, and returns the query, retrieved documents, and response.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/16_fusion_rag.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef answer_with_fusion_rag(query, chunks, vector_store, bm25_index, k=5, alpha=0.5):\n    \"\"\"\n    Answer a query using fusion RAG.\n    \n    Args:\n        query (str): User query\n        chunks (List[Dict]): Text chunks\n        vector_store (SimpleVectorStore): Vector store\n        bm25_index (BM25Okapi): BM25 index\n        k (int): Number of documents to retrieve\n        alpha (float): Weight for vector scores\n        \n    Returns:\n        Dict: Query results including retrieved documents and response\n    \"\"\"\n    # Retrieve documents using fusion retrieval method\n    retrieved_docs = fusion_retrieval(query, chunks, vector_store, bm25_index, k=k, alpha=alpha)\n    \n    # Format the context from the retrieved documents by joining their text with separators\n    context = \"\\n\\n---\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n    \n    # Generate a response based on the query and the formatted context\n    response = generate_response(query, context)\n    \n    # Return the query, retrieved documents, and the generated response\n    return {\n        \"query\": query,\n        \"retrieved_documents\": retrieved_docs,\n        \"response\": response\n    }\n```\n\n----------------------------------------\n\nTITLE: Evaluating Graph RAG on PDF Documents in Python\nDESCRIPTION: This function evaluates Graph RAG on multiple test queries for a given PDF document. It extracts text, builds a knowledge graph, traverses the graph for relevant information, generates responses, and compares them with reference answers if available.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/17_graph_rag.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_graph_rag(pdf_path, test_queries, reference_answers=None):\n    \"\"\"\n    Evaluate Graph RAG on multiple test queries.\n    \n    Args:\n        pdf_path (str): Path to the PDF document\n        test_queries (List[str]): List of test queries\n        reference_answers (List[str], optional): Reference answers for comparison\n        \n    Returns:\n        Dict: Evaluation results\n    \"\"\"\n    # Extract text from PDF\n    text = extract_text_from_pdf(pdf_path)\n    \n    # Split text into chunks\n    chunks = chunk_text(text)\n    \n    # Build knowledge graph (do this once for all queries)\n    graph, embeddings = build_knowledge_graph(chunks)\n    \n    results = []\n    \n    for i, query in enumerate(test_queries):\n        print(f\"\\n\\n=== Evaluating Query {i+1}/{len(test_queries)} ===\")\n        print(f\"Query: {query}\")\n        \n        # Traverse graph to find relevant information\n        relevant_chunks, traversal_path = traverse_graph(query, graph, embeddings)\n        \n        # Generate response\n        response = generate_response(query, relevant_chunks)\n        \n        # Compare with reference answer if available\n        reference = None\n        comparison = None\n        if reference_answers and i < len(reference_answers):\n            reference = reference_answers[i]\n            comparison = compare_with_reference(response, reference, query)\n        \n        # Append results for the current query\n        results.append({\n            \"query\": query,\n            \"response\": response,\n            \"reference_answer\": reference,\n            \"comparison\": comparison,\n            \"traversal_path_length\": len(traversal_path),\n            \"relevant_chunks_count\": len(relevant_chunks)\n        })\n        \n        # Display results\n        print(f\"\\nResponse: {response}\\n\")\n        if comparison:\n            print(f\"Comparison: {comparison}\\n\")\n    \n    # Return evaluation results and graph statistics\n    return {\n        \"results\": results,\n        \"graph_stats\": {\n            \"nodes\": graph.number_of_nodes(),\n            \"edges\": graph.number_of_edges(),\n            \"avg_degree\": sum(dict(graph.degree()).values()) / graph.number_of_nodes()\n        }\n    }\n```\n\n----------------------------------------\n\nTITLE: Implementing Semantic Search for RAG in Python\nDESCRIPTION: Function to retrieve the most relevant text chunks for a given query using cosine similarity between embeddings. It returns the top k chunks with the highest similarity scores, which will be used as context for the language model.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/1_simple_rag.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef semantic_search(query, text_chunks, embeddings, k=5):\n    \"\"\"\n    Performs semantic search on the text chunks using the given query and embeddings.\n\n    Args:\n    query (str): The query for the semantic search.\n    text_chunks (List[str]): A list of text chunks to search through.\n    embeddings (List[dict]): A list of embeddings for the text chunks.\n    k (int): The number of top relevant text chunks to return. Default is 5.\n\n    Returns:\n    List[str]: A list of the top k most relevant text chunks based on the query.\n    \"\"\"\n    # Create an embedding for the query\n    query_embedding = create_embeddings(query).data[0].embedding\n    similarity_scores = []  # Initialize a list to store similarity scores\n\n    # Calculate similarity scores between the query embedding and each text chunk embedding\n    for i, chunk_embedding in enumerate(embeddings):\n        similarity_score = cosine_similarity(np.array(query_embedding), np.array(chunk_embedding.embedding))\n        similarity_scores.append((i, similarity_score))  # Append the index and similarity score\n\n    # Sort the similarity scores in descending order\n    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n    # Get the indices of the top k most similar text chunks\n    top_indices = [index for index, _ in similarity_scores[:k]]\n    # Return the top k most relevant text chunks\n    return [text_chunks[index] for index in top_indices]\n```\n\n----------------------------------------\n\nTITLE: Assessing Response Support from Context in Python\nDESCRIPTION: This function evaluates how well a response is supported by the given context. It uses the Llama-3.2-3B-Instruct model to determine if facts and claims in the response are backed by the context, returning 'fully supported', 'partially supported', or 'no support'.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/13_self_rag.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef assess_support(response, context):\n    \"\"\"\n    Assesses how well a response is supported by the context.\n    \n    Args:\n        response (str): Generated response\n        context (str): Context text\n        \n    Returns:\n        str: 'fully supported', 'partially supported', or 'no support'\n    \"\"\"\n    # System prompt to instruct the AI on how to evaluate support\n    system_prompt = \"\"\"You are an AI assistant that determines if a response is supported by the given context.\n    Evaluate if the facts, claims, and information in the response are backed by the context.\n    Answer with ONLY one of these three options:\n    - \"Fully supported\": All information in the response is directly supported by the context.\n    - \"Partially supported\": Some information in the response is supported by the context, but some is not.\n    - \"No support\": The response contains significant information not found in or contradicting the context.\n    \"\"\"\n\n    # Truncate context if it is too long to avoid exceeding token limits\n    max_context_length = 2000\n    if len(context) > max_context_length:\n        context = context[:max_context_length] + \"... [truncated]\"\n\n    # User prompt containing the context and the response to be evaluated\n    user_prompt = f\"\"\"Context:\n    {context}\n\n    Response:\n    {response}\n\n    How well is this response supported by the context? Answer with ONLY \"Fully supported\", \"Partially supported\", or \"No support\".\n    \"\"\"\n    \n    # Generate response from the model\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0\n    )\n    \n    # Extract the answer from the model's response and convert to lowercase\n    answer = response.choices[0].message.content.strip().lower()\n    \n    return answer  # Return the support assessment\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG with Query Transformations in Python\nDESCRIPTION: This function performs retrieval with different query transformation techniques: query rewriting, step-back prompting, and query decomposition. Each technique transforms the query in different ways to improve the relevance of retrieved documents.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/7_query_transform.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef transformed_search(query, vector_store, transformation_type, top_k=3):\n    \"\"\"\n    Search using a transformed query.\n    \n    Args:\n        query (str): Original query\n        vector_store (SimpleVectorStore): Vector store to search\n        transformation_type (str): Type of transformation ('rewrite', 'step_back', or 'decompose')\n        top_k (int): Number of results to return\n        \n    Returns:\n        List[Dict]: Search results\n    \"\"\"\n    print(f\"Transformation type: {transformation_type}\")\n    print(f\"Original query: {query}\")\n    \n    results = []\n    \n    if transformation_type == \"rewrite\":\n        # Query rewriting\n        transformed_query = rewrite_query(query)\n        print(f\"Rewritten query: {transformed_query}\")\n        \n        # Create embedding for transformed query\n        query_embedding = create_embeddings(transformed_query)\n        \n        # Search with rewritten query\n        results = vector_store.similarity_search(query_embedding, k=top_k)\n        \n    elif transformation_type == \"step_back\":\n        # Step-back prompting\n        transformed_query = generate_step_back_query(query)\n        print(f\"Step-back query: {transformed_query}\")\n        \n        # Create embedding for transformed query\n        query_embedding = create_embeddings(transformed_query)\n        \n        # Search with step-back query\n        results = vector_store.similarity_search(query_embedding, k=top_k)\n        \n    elif transformation_type == \"decompose\":\n        # Sub-query decomposition\n        sub_queries = decompose_query(query)\n        print(\"Decomposed into sub-queries:\")\n        for i, sub_q in enumerate(sub_queries, 1):\n            print(f\"{i}. {sub_q}\")\n        \n        # Create embeddings for all sub-queries\n        sub_query_embeddings = create_embeddings(sub_queries)\n        \n        # Search with each sub-query and combine results\n        all_results = []\n        for i, embedding in enumerate(sub_query_embeddings):\n            sub_results = vector_store.similarity_search(embedding, k=2)  # Get fewer results per sub-query\n            all_results.extend(sub_results)\n        \n        # Remove duplicates (keep highest similarity score)\n        seen_texts = {}\n        for result in all_results:\n            text = result[\"text\"]\n            if text not in seen_texts or result[\"similarity\"] > seen_texts[text][\"similarity\"]:\n                seen_texts[text] = result\n        \n        # Sort by similarity and take top_k\n        results = sorted(seen_texts.values(), key=lambda x: x[\"similarity\"], reverse=True)[:top_k]\n        \n    else:\n        # Regular search without transformation\n        query_embedding = create_embeddings(query)\n        results = vector_store.similarity_search(query_embedding, k=top_k)\n    \n    return results\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM-based Reranking with OpenAI API in Python\nDESCRIPTION: This function reranks search results using LLM relevance scoring on a scale from 0-10. It uses the OpenAI API to evaluate how well each document answers the user query, providing detailed scoring guidelines to the model to ensure consistent evaluation.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/8_reranker.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef rerank_with_llm(query, results, top_n=3, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Reranks search results using LLM relevance scoring.\n    \n    Args:\n        query (str): User query\n        results (List[Dict]): Initial search results\n        top_n (int): Number of results to return after reranking\n        model (str): Model to use for scoring\n        \n    Returns:\n        List[Dict]: Reranked results\n    \"\"\"\n    print(f\"Reranking {len(results)} documents...\")  # Print the number of documents to be reranked\n    \n    scored_results = []  # Initialize an empty list to store scored results\n    \n    # Define the system prompt for the LLM\n    system_prompt = \"\"\"You are an expert at evaluating document relevance for search queries.\nYour task is to rate documents on a scale from 0 to 10 based on how well they answer the given query.\n\nGuidelines:\n- Score 0-2: Document is completely irrelevant\n- Score 3-5: Document has some relevant information but doesn't directly answer the query\n- Score 6-8: Document is relevant and partially answers the query\n- Score 9-10: Document is highly relevant and directly answers the query\n\nYou MUST respond with ONLY a single integer score between 0 and 10. Do not include ANY other text.\"\"\"\n    \n    # Iterate through each result\n    for i, result in enumerate(results):\n        # Show progress every 5 documents\n        if i % 5 == 0:\n            print(f\"Scoring document {i+1}/{len(results)}...\")\n        \n        # Define the user prompt for the LLM\n        user_prompt = f\"\"\"Query: {query}\n\nDocument:\n{result['text']}\n\nRate this document's relevance to the query on a scale from 0 to 10:\"\"\"\n        \n        # Get the LLM response\n        response = client.chat.completions.create(\n            model=model,\n            temperature=0,\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt}\n            ]\n        )\n        \n        # Extract the score from the LLM response\n        score_text = response.choices[0].message.content.strip()\n        \n        # Use regex to extract the numerical score\n        score_match = re.search(r'\\b(10|[0-9])\\b', score_text)\n        if score_match:\n            score = float(score_match.group(1))\n        else:\n            # If score extraction fails, use similarity score as fallback\n            print(f\"Warning: Could not extract score from response: '{score_text}', using similarity score instead\")\n            score = result[\"similarity\"] * 10\n        \n        # Append the scored result to the list\n        scored_results.append({\n            \"text\": result[\"text\"],\n            \"metadata\": result[\"metadata\"],\n            \"similarity\": result[\"similarity\"],\n            \"relevance_score\": score\n        })\n    \n    # Sort results by relevance score in descending order\n    reranked_results = sorted(scored_results, key=lambda x: x[\"relevance_score\"], reverse=True)\n    \n    # Return the top_n results\n    return reranked_results[:top_n]\n```\n\n----------------------------------------\n\nTITLE: Generating Overall Analysis for Fusion Retrieval in Python\nDESCRIPTION: This function generates an overall analysis of fusion retrieval performance based on the results of multiple test queries. It uses a language model to provide insights on the strengths and weaknesses of different retrieval approaches.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/16_fusion_rag.ipynb#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef generate_overall_analysis(results):\n    \"\"\"\n    Generate an overall analysis of fusion retrieval.\n    \n    Args:\n        results (List[Dict]): Results from evaluating queries\n        \n    Returns:\n        str: Overall analysis\n    \"\"\"\n    # System prompt to guide the evaluation process\n    system_prompt = \"\"\"You are an expert at evaluating information retrieval systems. \n    Based on multiple test queries, provide an overall analysis comparing three retrieval approaches:\n    1. Vector-based retrieval (semantic similarity)\n    2. BM25 keyword retrieval (keyword matching)\n    3. Fusion retrieval (combination of both)\n\n    Focus on:\n    1. Types of queries where each approach performs best\n    2. Overall strengths and weaknesses of each approach\n    3. How fusion retrieval balances the trade-offs\n    4. Recommendations for when to use each approach\"\"\"\n\n    # Create a summary of evaluations for each query\n    evaluations_summary = \"\"\n    for i, result in enumerate(results):\n        evaluations_summary += f\"Query {i+1}: {result['query']}\\n\"\n        evaluations_summary += f\"Comparison Summary: {result['comparison'][:200]}...\\n\\n\"\n\n    # User prompt containing the evaluations summary\n    user_prompt = f\"\"\"Based on the following evaluations of different retrieval methods across {len(results)} queries, \n    provide an overall analysis comparing these three approaches:\n\n    {evaluations_summary}\n\n    Please provide a comprehensive analysis of vector-based, BM25, and fusion retrieval approaches,\n    highlighting when and why fusion retrieval provides advantages over the individual methods.\"\"\"\n\n    # Generate the overall analysis using meta-llama/Llama-3.2-3B-Instruct\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0\n    )\n    \n    # Return the generated analysis content\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Hierarchical Document Processing in Python\nDESCRIPTION: This function processes a PDF document into hierarchical indices, creating summary and detailed vector stores. It extracts text from PDF pages, generates summaries, chunks detailed text, creates embeddings, and populates vector stores for efficient retrieval.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/18_hierarchy_rag.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef process_document_hierarchically(pdf_path, chunk_size=1000, chunk_overlap=200):\n    \"\"\"\n    Process a document into hierarchical indices.\n    \n    Args:\n        pdf_path (str): Path to the PDF file\n        chunk_size (int): Size of each detailed chunk\n        chunk_overlap (int): Overlap between chunks\n        \n    Returns:\n        Tuple[SimpleVectorStore, SimpleVectorStore]: Summary and detailed vector stores\n    \"\"\"\n    # Extract pages from PDF\n    pages = extract_text_from_pdf(pdf_path)\n    \n    # Create summaries for each page\n    print(\"Generating page summaries...\")\n    summaries = []\n    for i, page in enumerate(pages):\n        print(f\"Summarizing page {i+1}/{len(pages)}...\")\n        summary_text = generate_page_summary(page[\"text\"])\n        \n        # Create summary metadata\n        summary_metadata = page[\"metadata\"].copy()\n        summary_metadata.update({\"is_summary\": True})\n        \n        # Append the summary text and metadata to the summaries list\n        summaries.append({\n            \"text\": summary_text,\n            \"metadata\": summary_metadata\n        })\n    \n    # Create detailed chunks for each page\n    detailed_chunks = []\n    for page in pages:\n        # Chunk the text of the page\n        page_chunks = chunk_text(\n            page[\"text\"], \n            page[\"metadata\"], \n            chunk_size, \n            chunk_overlap\n        )\n        # Extend the detailed_chunks list with the chunks from the current page\n        detailed_chunks.extend(page_chunks)\n    \n    print(f\"Created {len(detailed_chunks)} detailed chunks\")\n    \n    # Create embeddings for summaries\n    print(\"Creating embeddings for summaries...\")\n    summary_texts = [summary[\"text\"] for summary in summaries]\n    summary_embeddings = create_embeddings(summary_texts)\n    \n    # Create embeddings for detailed chunks\n    print(\"Creating embeddings for detailed chunks...\")\n    chunk_texts = [chunk[\"text\"] for chunk in detailed_chunks]\n    chunk_embeddings = create_embeddings(chunk_texts)\n    \n    # Create vector stores\n    summary_store = SimpleVectorStore()\n    detailed_store = SimpleVectorStore()\n    \n    # Add summaries to summary store\n    for i, summary in enumerate(summaries):\n        summary_store.add_item(\n            text=summary[\"text\"],\n            embedding=summary_embeddings[i],\n            metadata=summary[\"metadata\"]\n        )\n    \n    # Add chunks to detailed store\n    for i, chunk in enumerate(detailed_chunks):\n        detailed_store.add_item(\n            text=chunk[\"text\"],\n            embedding=chunk_embeddings[i],\n            metadata=chunk[\"metadata\"]\n        )\n    \n    print(f\"Created vector stores with {len(summaries)} summaries and {len(detailed_chunks)} chunks\")\n    return summary_store, detailed_store\n```\n\n----------------------------------------\n\nTITLE: Comparing Compression Techniques for RAG in Python\nDESCRIPTION: A function that compares different contextual compression techniques with standard RAG. It measures compression ratios, context lengths, and evaluates response quality for selective compression, summary compression, and extraction-based compression methods.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/10_contextual_compression.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_compression(pdf_path, query, reference_answer=None, compression_types=[\"selective\", \"summary\", \"extraction\"]):\n    \"\"\"\n    Compare different compression techniques with standard RAG.\n    \n    Args:\n        pdf_path (str): Path to PDF document\n        query (str): User query\n        reference_answer (str): Optional reference answer\n        compression_types (List[str]): Compression types to evaluate\n        \n    Returns:\n        dict: Evaluation results\n    \"\"\"\n    print(\"\\n=== EVALUATING CONTEXTUAL COMPRESSION ===\")\n    print(f\"Query: {query}\")\n    \n    # Run standard RAG without compression\n    standard_result = standard_rag(pdf_path, query)\n    \n    # Dictionary to store results of different compression techniques\n    compression_results = {}\n    \n    # Run RAG with each compression technique\n    for comp_type in compression_types:\n        print(f\"\\nTesting {comp_type} compression...\")\n        compression_results[comp_type] = rag_with_compression(pdf_path, query, compression_type=comp_type)\n    \n    # Gather responses for evaluation\n    responses = {\n        \"standard\": standard_result[\"response\"]\n    }\n    for comp_type in compression_types:\n        responses[comp_type] = compression_results[comp_type][\"response\"]\n    \n    # Evaluate responses if a reference answer is provided\n    if reference_answer:\n        evaluation = evaluate_responses(query, responses, reference_answer)\n        print(\"\\n=== EVALUATION RESULTS ===\")\n        print(evaluation)\n    else:\n        evaluation = \"No reference answer provided for evaluation.\"\n    \n    # Calculate metrics for each compression type\n    metrics = {}\n    for comp_type in compression_types:\n        metrics[comp_type] = {\n            \"avg_compression_ratio\": f\"{sum(compression_results[comp_type]['compression_ratios'])/len(compression_results[comp_type]['compression_ratios']):.2f}%\",\n            \"total_context_length\": len(\"\\n\\n\".join(compression_results[comp_type]['compressed_chunks'])),\n            \"original_context_length\": len(\"\\n\\n\".join(standard_result['chunks']))\n        }\n    \n    # Return the evaluation results, responses, and metrics\n    return {\n        \"query\": query,\n        \"responses\": responses,\n        \"evaluation\": evaluation,\n        \"metrics\": metrics,\n        \"standard_result\": standard_result,\n        \"compression_results\": compression_results\n    }\n```\n\n----------------------------------------\n\nTITLE: Rewriting Query for Better Retrieval in Python\nDESCRIPTION: This function uses a language model to rewrite the original query for better document retrieval. It takes into account the current context chunks and generates a more specific and targeted query to improve the relevance of retrieved information.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef rewrite_query(\n    query: str, \n    context_chunks: List[str], \n    model: str = \"google/gemma-2-2b-it\", \n    max_tokens: int = 100, \n    temperature: float = 0.3\n) -> str:\n    \"\"\"\n    Use the LLM to rewrite the query for better document retrieval.\n\n    Args:\n        query (str): The original query text.\n        context_chunks (List[str]): A list of context chunks retrieved so far.\n        model (str): The model to use for generating the rewritten query. Default is \"google/gemma-2-2b-it\".\n        max_tokens (int): Maximum number of tokens in the rewritten query. Default is 100.\n        temperature (float): Sampling temperature for response diversity. Default is 0.3.\n\n    Returns:\n        str: The rewritten query optimized for document retrieval.\n    \"\"\"\n    # Construct a prompt for the LLM to rewrite the query\n    rewrite_prompt = f\"\"\"\n    You are a query optimization assistant. Your task is to rewrite the given query to make it more effective \n    for retrieving relevant information. The query will be used for document retrieval.\n    \n    Original query: {query}\n    \n    Based on the context retrieved so far:\n    {' '.join(context_chunks[:2]) if context_chunks else 'No context available yet'}\n    \n    Rewrite the query to be more specific and targeted to retrieve better information.\n    Rewritten query:\n    \"\"\"\n    \n    # Use the LLM to generate a rewritten query\n    response = client.chat.completions.create(\n        model=model, # Specify the model to use for generating the response\n        max_tokens=max_tokens, # Maximum number of tokens in the response\n        temperature=temperature, # Sampling temperature for response diversity\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": rewrite_prompt\n            }\n        ]\n    )\n    \n    # Extract and return the rewritten query from the response\n    rewritten_query = response.choices[0].message.content.strip()\n    return rewritten_query\n```\n\n----------------------------------------\n\nTITLE: Processing Documents for Self-RAG in Python\nDESCRIPTION: A comprehensive function that processes a PDF document for use in Self-RAG. It extracts text, chunks it with overlap, creates embeddings for each chunk, and stores everything in a vector store for efficient retrieval during the RAG process.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/13_self_rag.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n    \"\"\"\n    Process a document for Self-RAG.\n\n    Args:\n        pdf_path (str): Path to the PDF file.\n        chunk_size (int): Size of each chunk in characters.\n        chunk_overlap (int): Overlap between chunks in characters.\n\n    Returns:\n        SimpleVectorStore: A vector store containing document chunks and their embeddings.\n    \"\"\"\n    # Extract text from the PDF file\n    print(\"Extracting text from PDF...\")\n    extracted_text = extract_text_from_pdf(pdf_path)\n    \n    # Chunk the extracted text\n    print(\"Chunking text...\")\n    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n    print(f\"Created {len(chunks)} text chunks\")\n    \n    # Create embeddings for each chunk\n    print(\"Creating embeddings for chunks...\")\n    chunk_embeddings = create_embeddings(chunks)\n    \n    # Initialize the vector store\n    store = SimpleVectorStore()\n    \n    # Add each chunk and its embedding to the vector store\n    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n        store.add_item(\n            text=chunk,\n            embedding=embedding,\n            metadata={\"index\": i, \"source\": pdf_path}\n        )\n    \n    print(f\"Added {len(chunks)} chunks to the vector store\")\n    return store\n```\n\n----------------------------------------\n\nTITLE: Evaluating Fusion Retrieval in Python\nDESCRIPTION: This function evaluates fusion retrieval compared to other methods. It processes a PDF document, runs test queries, and compares the results of different retrieval methods. The function returns evaluation results and an overall analysis.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/16_fusion_rag.ipynb#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_fusion_retrieval(pdf_path, test_queries, reference_answers=None, k=5, alpha=0.5):\n    \"\"\"\n    Evaluate fusion retrieval compared to other methods.\n    \n    Args:\n        pdf_path (str): Path to the PDF file\n        test_queries (List[str]): List of test queries\n        reference_answers (List[str], optional): Reference answers\n        k (int): Number of documents to retrieve\n        alpha (float): Weight for vector scores in fusion retrieval\n        \n    Returns:\n        Dict: Evaluation results\n    \"\"\"\n    print(\"=== EVALUATING FUSION RETRIEVAL ===\\n\")\n    \n    # Process the document to extract text, create chunks, and build vector and BM25 indices\n    chunks, vector_store, bm25_index = process_document(pdf_path)\n    \n    # Initialize a list to store results for each query\n    results = []\n    \n    # Iterate over each test query\n    for i, query in enumerate(test_queries):\n        print(f\"\\n\\n=== Evaluating Query {i+1}/{len(test_queries)} ===\")\n        print(f\"Query: {query}\")\n        \n        # Get the reference answer if available\n        reference = None\n        if reference_answers and i < len(reference_answers):\n            reference = reference_answers[i]\n        \n        # Compare retrieval methods for the current query\n        comparison = compare_retrieval_methods(\n            query, \n            chunks, \n            vector_store, \n            bm25_index, \n            k=k, \n            alpha=alpha,\n            reference_answer=reference\n        )\n        \n        # Append the comparison results to the results list\n        results.append(comparison)\n        \n        # Print the responses from different retrieval methods\n        print(\"\\n=== Vector-based Response ===\")\n        print(comparison[\"vector_result\"][\"response\"])\n        \n        print(\"\\n=== BM25 Response ===\")\n        print(comparison[\"bm25_result\"][\"response\"])\n        \n        print(\"\\n=== Fusion Response ===\")\n        print(comparison[\"fusion_result\"][\"response\"])\n        \n        print(\"\\n=== Comparison ===\")\n        print(comparison[\"comparison\"])\n    \n    # Generate an overall analysis of the fusion retrieval performance\n    overall_analysis = generate_overall_analysis(results)\n    \n    # Return the results and overall analysis\n    return {\n        \"results\": results,\n        \"overall_analysis\": overall_analysis\n    }\n```\n\n----------------------------------------\n\nTITLE: Running Complete Self-RAG System with Examples in Python\nDESCRIPTION: This function demonstrates the usage of the complete Self-RAG system with three example queries. It processes a document, creates a vector store, and runs the Self-RAG system for each query. The function returns the results for all three examples, including the final responses and metrics.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/13_self_rag.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef run_self_rag_example():\n    \"\"\"\n    Demonstrates the complete Self-RAG system with examples.\n    \"\"\"\n    # Process document\n    pdf_path = \"data/AI_Information.pdf\"  # Path to the PDF document\n    print(f\"Processing document: {pdf_path}\")\n    vector_store = process_document(pdf_path)  # Process the document and create a vector store\n    \n    # Example 1: Query likely needing retrieval\n    query1 = \"What are the main ethical concerns in AI development?\"\n    print(\"\\n\" + \"=\"*80)\n    print(f\"EXAMPLE 1: {query1}\")\n    result1 = self_rag(query1, vector_store)  # Run Self-RAG for the first query\n    print(\"\\nFinal response:\")\n    print(result1[\"response\"])  # Print the final response for the first query\n    print(\"\\nMetrics:\")\n    print(json.dumps(result1[\"metrics\"], indent=2))  # Print the metrics for the first query\n    \n    # Example 2: Query likely not needing retrieval\n    query2 = \"Can you write a short poem about artificial intelligence?\"\n    print(\"\\n\" + \"=\"*80)\n    print(f\"EXAMPLE 2: {query2}\")\n    result2 = self_rag(query2, vector_store)  # Run Self-RAG for the second query\n    print(\"\\nFinal response:\")\n    print(result2[\"response\"])  # Print the final response for the second query\n    print(\"\\nMetrics:\")\n    print(json.dumps(result2[\"metrics\"], indent=2))  # Print the metrics for the second query\n    \n    # Example 3: Query with some relevance to document but requiring additional knowledge\n    query3 = \"How might AI impact economic growth in developing countries?\"\n    print(\"\\n\" + \"=\"*80)\n    print(f\"EXAMPLE 3: {query3}\")\n    result3 = self_rag(query3, vector_store)  # Run Self-RAG for the third query\n    print(\"\\nFinal response:\")\n    print(result3[\"response\"])  # Print the final response for the third query\n    print(\"\\nMetrics:\")\n    print(json.dumps(result3[\"metrics\"], indent=2))  # Print the metrics for the third query\n    \n    return {\n        \"example1\": result1,\n        \"example2\": result2,\n        \"example3\": result3\n    }\n```\n\n----------------------------------------\n\nTITLE: Determining Retrieval Necessity in Self-RAG with LLM in Python\nDESCRIPTION: A function that decides whether retrieval is necessary for a given query by consulting an LLM (Llama-3.2-3B-Instruct). It uses a system prompt to guide the model in making this decision based on the nature of the query, returning True for factual questions and False for opinions or common knowledge.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/13_self_rag.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef determine_if_retrieval_needed(query):\n    \"\"\"\n    Determines if retrieval is necessary for the given query.\n    \n    Args:\n        query (str): User query\n        \n    Returns:\n        bool: True if retrieval is needed, False otherwise\n    \"\"\"\n    # System prompt to instruct the AI on how to determine if retrieval is necessary\n    system_prompt = \"\"\"You are an AI assistant that determines if retrieval is necessary to answer a query.\n    For factual questions, specific information requests, or questions about events, people, or concepts, answer \"Yes\".\n    For opinions, hypothetical scenarios, or simple queries with common knowledge, answer \"No\".\n    Answer with ONLY \"Yes\" or \"No\".\"\"\"\n\n    # User prompt containing the query\n    user_prompt = f\"Query: {query}\\n\\nIs retrieval necessary to answer this query accurately?\"\n    \n    # Generate response from the model\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0\n    )\n    \n    # Extract the answer from the model's response and convert to lowercase\n    answer = response.choices[0].message.content.strip().lower()\n    \n    # Return True if the answer contains \"yes\", otherwise return False\n    return \"yes\" in answer\n```\n\n----------------------------------------\n\nTITLE: Complete Document Processing Pipeline for Multi-Modal RAG in Python\nDESCRIPTION: This function implements a complete processing pipeline for multi-modal Retrieval-Augmented Generation (RAG). It extracts text and images from a PDF, chunks the text, generates captions for images, creates embeddings, and builds a vector store. The function returns the populated vector store and document information.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/15_multimodel_rag.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n    \"\"\"\n    Process a document for multi-modal RAG.\n    \n    Args:\n        pdf_path (str): Path to the PDF file\n        chunk_size (int): Size of each chunk in characters\n        chunk_overlap (int): Overlap between chunks in characters\n        \n    Returns:\n        Tuple[MultiModalVectorStore, Dict]: Vector store and document info\n    \"\"\"\n    # Create a directory for extracted images\n    image_dir = \"extracted_images\"\n    os.makedirs(image_dir, exist_ok=True)\n    \n    # Extract text and images from the PDF\n    text_data, image_paths = extract_content_from_pdf(pdf_path, image_dir)\n    \n    # Chunk the extracted text\n    chunked_text = chunk_text(text_data, chunk_size, chunk_overlap)\n    \n    # Process the extracted images to generate captions\n    image_data = process_images(image_paths)\n    \n    # Combine all content items (text chunks and image captions)\n    all_items = chunked_text + image_data\n    \n    # Extract content for embedding\n    contents = [item[\"content\"] for item in all_items]\n    \n    # Create embeddings for all content\n    print(\"Creating embeddings for all content...\")\n    embeddings = create_embeddings(contents)\n    \n    # Build the vector store and add items with their embeddings\n    vector_store = MultiModalVectorStore()\n    vector_store.add_items(all_items, embeddings)\n    \n    # Prepare document info with counts of text chunks and image captions\n    doc_info = {\n        \"text_count\": len(chunked_text),\n        \"image_count\": len(image_data),\n        \"total_items\": len(all_items),\n    }\n    \n    # Print summary of added items\n    print(f\"Added {len(all_items)} items to vector store ({len(chunked_text)} text chunks, {len(image_data)} image captions)\")\n    \n    # Return the vector store and document info\n    return vector_store, doc_info\n```\n\n----------------------------------------\n\nTITLE: Implementing Context-Enriched Semantic Search for RAG in Python\nDESCRIPTION: Function that performs context-aware retrieval by finding the most relevant text chunk for a query and including its neighboring chunks. This approach ensures that retrieved information has sufficient context for comprehensive understanding.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/4_context_enriched_rag.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef context_enriched_search(query, text_chunks, embeddings, k=1, context_size=1):\n    \"\"\"\n    Retrieves the most relevant chunk along with its neighboring chunks.\n\n    Args:\n    query (str): Search query.\n    text_chunks (List[str]): List of text chunks.\n    embeddings (List[dict]): List of chunk embeddings.\n    k (int): Number of relevant chunks to retrieve.\n    context_size (int): Number of neighboring chunks to include.\n\n    Returns:\n    List[str]: Relevant text chunks with contextual information.\n    \"\"\"\n    # Convert the query into an embedding vector\n    query_embedding = create_embeddings(query).data[0].embedding\n    similarity_scores = []\n\n    # Compute similarity scores between query and each text chunk embedding\n    for i, chunk_embedding in enumerate(embeddings):\n        # Calculate cosine similarity between the query embedding and current chunk embedding\n        similarity_score = cosine_similarity(np.array(query_embedding), np.array(chunk_embedding.embedding))\n        # Store the index and similarity score as a tuple\n        similarity_scores.append((i, similarity_score))\n\n    # Sort chunks by similarity score in descending order (highest similarity first)\n    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n\n    # Get the index of the most relevant chunk\n    top_index = similarity_scores[0][0]\n\n    # Define the range for context inclusion\n    # Ensure we don't go below 0 or beyond the length of text_chunks\n    start = max(0, top_index - context_size)\n    end = min(len(text_chunks), top_index + context_size + 1)\n\n    # Return the relevant chunk along with its neighboring context chunks\n    return [text_chunks[i] for i in range(start, end)]\n```\n\n----------------------------------------\n\nTITLE: Calculating Chunk Values for Relevance Scoring in RSE Algorithm\nDESCRIPTION: Function that calculates the value of each document chunk by combining relevance scores from vector similarity search with a penalty for irrelevant chunks. This is a key component of the RSE algorithm that helps identify relevant segments.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/9_rse.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef calculate_chunk_values(query, chunks, vector_store, irrelevant_chunk_penalty=0.2):\n    \"\"\"\n    Calculate chunk values by combining relevance and position.\n    \n    Args:\n        query (str): Query text\n        chunks (List[str]): List of document chunks\n        vector_store (SimpleVectorStore): Vector store containing the chunks\n        irrelevant_chunk_penalty (float): Penalty for irrelevant chunks\n        \n    Returns:\n        List[float]: List of chunk values\n    \"\"\"\n    # Create query embedding\n    query_embedding = create_embeddings([query])[0]\n    \n    # Get all chunks with similarity scores\n    num_chunks = len(chunks)\n    results = vector_store.search(query_embedding, top_k=num_chunks)\n    \n    # Create a mapping of chunk_index to relevance score\n    relevance_scores = {result[\"metadata\"][\"chunk_index\"]: result[\"score\"] for result in results}\n    \n    # Calculate chunk values (relevance score minus penalty)\n    chunk_values = []\n    for i in range(num_chunks):\n        # Get relevance score or default to 0 if not in results\n        score = relevance_scores.get(i, 0.0)\n        # Apply penalty to convert to a value where irrelevant chunks have negative value\n        value = score - irrelevant_chunk_penalty\n        chunk_values.append(value)\n    \n    return chunk_values\n```\n\n----------------------------------------\n\nTITLE: Comparing Simple RAG vs RL-Enhanced RAG Performance in Python\nDESCRIPTION: This code executes a comparison between simple RAG and RL-enhanced RAG pipelines using a sample query and expected answer. It calls a function that returns both responses and their similarity scores to the ground truth, allowing for direct comparison.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_37\n\nLANGUAGE: python\nCODE:\n```\n# Compare the performance of the simple RAG pipeline and the RL-enhanced RAG pipeline\n# using the sample query and its expected answer.\n# The function returns:\n# - simple_response: The response generated by the simple RAG pipeline.\n# - rl_response: The best response generated by the RL-enhanced RAG pipeline.\n# - simple_sim: The similarity score of the simple RAG response to the ground truth.\n# - rl_sim: The similarity score of the RL-enhanced RAG response to the ground truth.\nsimple_response, rl_response, simple_sim, rl_sim = compare_rag_approaches(sample_query, expected_answer)\n```\n\n----------------------------------------\n\nTITLE: Generating AI Responses with Context in Python\nDESCRIPTION: Function to generate responses from an AI model based on system and user prompts. Uses a context-aware approach where the user message contains both the retrieved text chunks and the original query.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/1_simple_rag.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Define the system prompt for the AI assistant\nsystem_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\n\ndef generate_response(system_prompt, user_message, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Generates a response from the AI model based on the system prompt and user message.\n\n    Args:\n    system_prompt (str): The system prompt to guide the AI's behavior.\n    user_message (str): The user's message or query.\n    model (str): The model to be used for generating the response. Default is \"meta-llama/Llama-2-7B-chat-hf\".\n\n    Returns:\n    dict: The response from the AI model.\n    \"\"\"\n    response = client.chat.completions.create(\n        model=model,\n        temperature=0,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_message}\n        ]\n    )\n    return response\n\n# Create the user prompt based on the top chunks\nuser_prompt = \"\\n\".join([f\"Context {i + 1}:\\n{chunk}\\n=====================================\\n\" for i, chunk in enumerate(top_chunks)])\nuser_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n\n# Generate AI response\nai_response = generate_response(system_prompt, user_prompt)\n```\n\n----------------------------------------\n\nTITLE: Generating LLM Responses with Context in Python\nDESCRIPTION: Creates responses to user queries by providing the retrieved context to a language model. It constructs appropriate system and user prompts to guide the model in generating relevant answers based only on the provided context.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/11_feedback_loop_rag.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef generate_response(query, context, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Generate a response based on the query and context.\n    \n    Args:\n        query (str): User query\n        context (str): Context text from retrieved documents\n        model (str): LLM model to use\n        \n    Returns:\n        str: Generated response\n    \"\"\"\n    # Define the system prompt to guide the AI's behavior\n    system_prompt = \"\"\"You are a helpful AI assistant. Answer the user's question based only on the provided context. If you cannot find the answer in the context, state that you don't have enough information.\"\"\"\n    \n    # Create the user prompt by combining the context and the query\n    user_prompt = f\"\"\"\n        Context:\n        {context}\n\n        Question: {query}\n\n        Please provide a comprehensive answer based only on the context above.\n    \"\"\"\n    \n    # Call the OpenAI API to generate a response based on the system and user prompts\n    response = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0  # Use temperature=0 for consistent, deterministic responses\n    )\n    \n    # Return the generated response content\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Implementing Standard RAG Pipeline in Python for Comparison\nDESCRIPTION: This function implements a standard RAG approach that directly embeds the query. Unlike HyDE RAG, it doesn't generate a hypothetical document but instead uses the query embedding to retrieve relevant document chunks and optionally generates a response based on those chunks.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/19_HyDE_rag.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef standard_rag(query, vector_store, k=5, should_generate_response=True):\n    \"\"\"\n    Perform standard RAG using direct query embedding.\n    \n    Args:\n        query (str): User query\n        vector_store (SimpleVectorStore): Vector store with document chunks\n        k (int): Number of chunks to retrieve\n        generate_response (bool): Whether to generate a final response\n        \n    Returns:\n        Dict: Results including retrieved chunks\n    \"\"\"\n    print(f\"\\n=== Processing query with Standard RAG: {query} ===\\n\")\n    \n    # Step 1: Create embedding for the query\n    print(\"Creating embedding for query...\")\n    query_embedding = create_embeddings([query])[0]\n    \n    # Step 2: Retrieve similar chunks based on the query embedding\n    print(f\"Retrieving {k} most similar chunks...\")\n    retrieved_chunks = vector_store.similarity_search(query_embedding, k=k)\n    \n    # Prepare the results dictionary\n    results = {\n        \"query\": query,\n        \"retrieved_chunks\": retrieved_chunks\n    }\n    \n    # Step 3: Generate a response if requested\n    if should_generate_response:\n        print(\"Generating final response...\")\n        response = generate_response(query, retrieved_chunks)\n        results[\"response\"] = response\n        \n    return results\n```\n\n----------------------------------------\n\nTITLE: Single RL Step Execution Function in Python\nDESCRIPTION: Function that executes a single step in the RL process, including action selection, execution, reward calculation, and state updates. It handles different types of actions and their corresponding logic.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndef rl_step(state: dict, action_space: List[str], ground_truth: str) -> tuple[dict, str, float, str]:\n    \"\"\"\n    Perform a single RL step: select an action, execute it, and calculate the reward.\n\n    Args:\n        state (dict): The current state of the environment, including query, context, responses, and rewards.\n        action_space (List[str]): The list of possible actions the agent can take.\n        ground_truth (str): The expected correct answer to calculate the reward.\n\n    Returns:\n        tuple: A tuple containing:\n            - state (dict): The updated state after executing the action.\n            - action (str): The action selected by the policy network.\n            - reward (float): The reward received for the action.\n            - response (str): The response generated (if applicable).\n    \"\"\"\n    # Select an action using the policy network\n    action: str = policy_network(state, action_space)\n    response: str = None  # Initialize response as None\n    reward: float = 0  # Initialize reward as 0\n\n    # Execute the selected action\n    if action == \"rewrite_query\":\n        # Rewrite the query to improve retrieval\n        rewritten_query: str = rewrite_query(state[\"original_query\"], state[\"context\"])\n        state[\"current_query\"] = rewritten_query  # Update the current query in the state\n        # Retrieve new context based on the rewritten query\n        new_context: List[str] = retrieve_relevant_chunks(rewritten_query)\n        state[\"context\"] = new_context  # Update the context in the state\n\n    elif action == \"expand_context\":\n        # Expand the context by retrieving additional chunks\n        expanded_context: List[str] = expand_context(state[\"current_query\"], state[\"context\"])\n        state[\"context\"] = expanded_context  # Update the context in the state\n\n    elif action == \"filter_context\":\n        # Filter the context to keep only the most relevant chunks\n        filtered_context: List[str] = filter_context(state[\"current_query\"], state[\"context\"])\n        state[\"context\"] = filtered_context  # Update the context in the state\n\n    elif action == \"generate_response\":\n        # Construct a prompt using the current query and context\n        prompt: str = construct_prompt(state[\"current_query\"], state[\"context\"])\n        # Generate a response using the LLM\n        response: str = generate_response(prompt)\n        # Calculate the reward based on the similarity between the response and the ground truth\n        reward: float = calculate_reward(response, ground_truth)\n        # Update the state with the new response and reward\n        state[\"previous_responses\"].append(response)\n        state[\"previous_rewards\"].append(reward)\n\n    # Return the updated state, selected action, reward, and response\n    return state, action, reward, response\n```\n\n----------------------------------------\n\nTITLE: Comparing Hierarchical and Standard RAG Approaches in Python\nDESCRIPTION: This function executes both hierarchical and standard RAG approaches on the same query and compares their performance. It runs both implementations and returns a comprehensive comparison of the results.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/18_hierarchy_rag.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef compare_approaches(query, pdf_path, reference_answer=None):\n    \"\"\"\n    Compare hierarchical and standard RAG approaches.\n    \n    Args:\n        query (str): User query\n        pdf_path (str): Path to the PDF document\n        reference_answer (str, optional): Reference answer for evaluation\n        \n    Returns:\n        Dict: Comparison results\n    \"\"\"\n    print(f\"\\n=== Comparing RAG approaches for query: {query} ===\")\n    \n    # Run hierarchical RAG\n    print(\"\\nRunning hierarchical RAG...\")\n    hierarchical_result = hierarchical_rag(query, pdf_path)\n    hier_response = hierarchical_result[\"response\"]\n    \n    # Run standard RAG\n    print(\"\\nRunning standard RAG...\")\n    standard_result = standard_rag(query, pdf_path)\n    std_response = standard_result[\"response\"]\n    \n    # Compare results from hierarchical and standard RAG\n    comparison = compare_responses(query, hier_response, std_response, reference_answer)\n    \n    # Return a dictionary with the comparison results\n    return {\n        \"query\": query,  # The original query\n        \"hierarchical_response\": hier_response,  # Response from hierarchical RAG\n        \"standard_response\": std_response,  # Response from standard RAG\n        \"reference_answer\": reference_answer,  # Reference answer for evaluation\n        \"comparison\": comparison,  # Comparison analysis\n        \"hierarchical_chunks_count\": len(hierarchical_result[\"retrieved_chunks\"]),  # Number of chunks retrieved by hierarchical RAG\n        \"standard_chunks_count\": len(standard_result[\"retrieved_chunks\"])  # Number of chunks retrieved by standard RAG\n    }\n```\n\n----------------------------------------\n\nTITLE: Evaluating Feedback Loop Impact on RAG Systems in Python\nDESCRIPTION: This function evaluates the impact of a feedback loop on RAG quality by comparing performance before and after feedback integration. It runs a controlled experiment with two rounds of queries, generates synthetic feedback, and compares results to quantify the feedback impact.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/11_feedback_loop_rag.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_feedback_loop(pdf_path, test_queries, reference_answers=None):\n    \"\"\"\n    Evaluate the impact of feedback loop on RAG quality by comparing performance before and after feedback integration.\n    \n    This function runs a controlled experiment to measure how incorporating feedback affects retrieval and generation:\n    1. First round: Run all test queries with no feedback\n    2. Generate synthetic feedback based on reference answers (if provided)\n    3. Second round: Run the same queries with feedback-enhanced retrieval\n    4. Compare results between rounds to quantify feedback impact\n    \n    Args:\n        pdf_path (str): Path to the PDF document used as the knowledge base\n        test_queries (List[str]): List of test queries to evaluate system performance\n        reference_answers (List[str], optional): Reference/gold standard answers for evaluation\n                                                and synthetic feedback generation\n        \n    Returns:\n        Dict: Evaluation results containing:\n            - round1_results: Results without feedback\n            - round2_results: Results with feedback\n            - comparison: Quantitative comparison metrics between rounds\n    \"\"\"\n    print(\"=== Evaluating Feedback Loop Impact ===\")\n    \n    # Create a temporary feedback file for this evaluation session only\n    temp_feedback_file = \"temp_evaluation_feedback.json\"\n    \n    # Initialize feedback collection (empty at the start)\n    feedback_data = []\n    \n    # ----------------------- FIRST EVALUATION ROUND -----------------------\n    # Run all queries without any feedback influence to establish baseline performance\n    print(\"\\n=== ROUND 1: NO FEEDBACK ===\")\n    round1_results = []\n    \n    for i, query in enumerate(test_queries):\n        print(f\"\\nQuery {i+1}: {query}\")\n        \n        # Process document to create initial vector store\n        chunks, vector_store = process_document(pdf_path)\n        \n        # Execute RAG without feedback influence (empty feedback list)\n        result = rag_with_feedback_loop(query, vector_store, [])\n        round1_results.append(result)\n        \n        # Generate synthetic feedback if reference answers are available\n        # This simulates user feedback for training the system\n        if reference_answers and i < len(reference_answers):\n            # Calculate synthetic feedback scores based on similarity to reference answer\n            similarity_to_ref = calculate_similarity(result[\"response\"], reference_answers[i])\n            # Convert similarity (0-1) to rating scale (1-5)\n            relevance = max(1, min(5, int(similarity_to_ref * 5)))\n            quality = max(1, min(5, int(similarity_to_ref * 5)))\n            \n            # Create structured feedback entry\n            feedback = get_user_feedback(\n                query=query,\n                response=result[\"response\"],\n                relevance=relevance,\n                quality=quality,\n                comments=f\"Synthetic feedback based on reference similarity: {similarity_to_ref:.2f}\"\n            )\n            \n            # Add to in-memory collection and persist to temporary file\n            feedback_data.append(feedback)\n            store_feedback(feedback, temp_feedback_file)\n    \n    # ----------------------- SECOND EVALUATION ROUND -----------------------\n    # Run the same queries with feedback incorporation to measure improvement\n    print(\"\\n=== ROUND 2: WITH FEEDBACK ===\")\n    round2_results = []\n    \n    # Process document and enhance with feedback-derived content\n    chunks, vector_store = process_document(pdf_path)\n    vector_store = fine_tune_index(vector_store, chunks, feedback_data)\n    \n    for i, query in enumerate(test_queries):\n        print(f\"\\nQuery {i+1}: {query}\")\n        \n        # Execute RAG with feedback influence\n        result = rag_with_feedback_loop(query, vector_store, feedback_data)\n        round2_results.append(result)\n    \n    # ----------------------- RESULTS ANALYSIS -----------------------\n    # Compare performance metrics between the two rounds\n    comparison = compare_results(test_queries, round1_results, round2_results, reference_answers)\n    \n    # Clean up temporary evaluation artifacts\n    if os.path.exists(temp_feedback_file):\n        os.remove(temp_feedback_file)\n    \n    return {\n        \"round1_results\": round1_results,\n        \"round2_results\": round2_results,\n        \"comparison\": comparison\n    }\n```\n\n----------------------------------------\n\nTITLE: Building Vector Stores for Chunks and Propositions in Python\nDESCRIPTION: A function that creates and populates vector stores for both traditional chunk-based and proposition-based retrieval approaches. It generates embeddings for each text unit, associates them with appropriate metadata (including quality scores for propositions), and stores them in separate vector stores for retrieval.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/14_proposition_chunking.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef build_vector_stores(chunks, propositions):\n    \"\"\"\n    Build vector stores for both chunk-based and proposition-based approaches.\n    \n    Args:\n        chunks (List[Dict]): Original document chunks\n        propositions (List[Dict]): Quality-filtered propositions\n        \n    Returns:\n        Tuple[SimpleVectorStore, SimpleVectorStore]: Chunk and proposition vector stores\n    \"\"\"\n    # Create vector store for chunks\n    chunk_store = SimpleVectorStore()\n    \n    # Extract chunk texts and create embeddings\n    chunk_texts = [chunk[\"text\"] for chunk in chunks]\n    print(f\"Creating embeddings for {len(chunk_texts)} chunks...\")\n    chunk_embeddings = create_embeddings(chunk_texts)\n    \n    # Add chunks to vector store with metadata\n    chunk_metadata = [{\"chunk_id\": chunk[\"chunk_id\"], \"type\": \"chunk\"} for chunk in chunks]\n    chunk_store.add_items(chunk_texts, chunk_embeddings, chunk_metadata)\n    \n    # Create vector store for propositions\n    prop_store = SimpleVectorStore()\n    \n    # Extract proposition texts and create embeddings\n    prop_texts = [prop[\"text\"] for prop in propositions]\n    print(f\"Creating embeddings for {len(prop_texts)} propositions...\")\n    prop_embeddings = create_embeddings(prop_texts)\n    \n    # Add propositions to vector store with metadata\n    prop_metadata = [\n        {\n            \"type\": \"proposition\", \n            \"source_chunk_id\": prop[\"source_chunk_id\"],\n            \"quality_scores\": prop[\"quality_scores\"]\n        } \n        for prop in propositions\n    ]\n    prop_store.add_items(prop_texts, prop_embeddings, prop_metadata)\n    \n    return chunk_store, prop_store\n```\n\n----------------------------------------\n\nTITLE: Evaluating RAG Responses Against Reference Answers in Python\nDESCRIPTION: A function that evaluates multiple RAG responses against a reference answer. It uses an LLM to analyze the factual accuracy, comprehensiveness, conciseness, and overall quality of each response, then ranks them from best to worst with detailed explanations.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/10_contextual_compression.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_responses(query, responses, reference_answer):\n    \"\"\"\n    Evaluate multiple responses against a reference answer.\n    \n    Args:\n        query (str): User query\n        responses (Dict[str, str]): Dictionary of responses by method\n        reference_answer (str): Reference answer\n        \n    Returns:\n        str: Evaluation text\n    \"\"\"\n    # Define the system prompt to guide the AI's behavior for evaluation\n    system_prompt = \"\"\"You are an objective evaluator of RAG responses. Compare different responses to the same query\n    and determine which is most accurate, comprehensive, and relevant to the query.\"\"\"\n    \n    # Create the user prompt by combining the query and reference answer\n    user_prompt = f\"\"\"\n    Query: {query}\n\n    Reference Answer: {reference_answer}\n\n    \"\"\"\n    \n    # Add each response to the prompt\n    for method, response in responses.items():\n        user_prompt += f\"\\n{method.capitalize()} Response:\\n{response}\\n\"\n    \n    # Add the evaluation criteria to the user prompt\n    user_prompt += \"\"\"\n    Please evaluate these responses based on:\n    1. Factual accuracy compared to the reference\n    2. Comprehensiveness - how completely they answer the query\n    3. Conciseness - whether they avoid irrelevant information\n    4. Overall quality\n\n    Rank the responses from best to worst with detailed explanations.\n    \"\"\"\n    \n    # Generate an evaluation response using the OpenAI API\n    evaluation_response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0\n    )\n    \n    # Return the evaluation text from the response\n    return evaluation_response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Processing Documents for RAG in Python\nDESCRIPTION: Implements a full document processing pipeline for RAG, combining PDF text extraction, chunking, and embedding creation. It returns a vector store populated with document chunks and their embeddings for retrieval.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/19_HyDE_rag.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n    \"\"\"\n    Process a document for RAG.\n    \n    Args:\n        pdf_path (str): Path to the PDF file\n        chunk_size (int): Size of each chunk in characters\n        chunk_overlap (int): Overlap between chunks in characters\n        \n    Returns:\n        SimpleVectorStore: Vector store containing document chunks\n    \"\"\"\n    # Extract text from the PDF file\n    pages = extract_text_from_pdf(pdf_path)\n    \n    # Process each page and create chunks\n    all_chunks = []\n    for page in pages:\n        # Pass the text content (string) to chunk_text, not the dictionary\n        page_chunks = chunk_text(page[\"text\"], chunk_size, chunk_overlap)\n        \n        # Update metadata for each chunk with the page's metadata\n        for chunk in page_chunks:\n            chunk[\"metadata\"].update(page[\"metadata\"])\n        \n        all_chunks.extend(page_chunks)\n    \n    # Create embeddings for the text chunks\n    print(\"Creating embeddings for chunks...\")\n    chunk_texts = [chunk[\"text\"] for chunk in all_chunks]\n    chunk_embeddings = create_embeddings(chunk_texts)\n    \n    # Create a vector store to hold the chunks and their embeddings\n    vector_store = SimpleVectorStore()\n    for i, chunk in enumerate(all_chunks):\n        vector_store.add_item(\n            text=chunk[\"text\"],\n            embedding=chunk_embeddings[i],\n            metadata=chunk[\"metadata\"]\n        )\n    \n    print(f\"Vector store created with {len(all_chunks)} chunks\")\n    return vector_store\n```\n\n----------------------------------------\n\nTITLE: Comparing RAG Responses from Different Query Transformation Techniques in Python\nDESCRIPTION: This function compares responses from different query transformation techniques against a reference answer using an LLM evaluator. It formats the reference answer and responses as a prompt to the model, which then scores and ranks the techniques based on accuracy, completeness, and relevance.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/7_query_transform.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef compare_responses(results, reference_answer, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Compare responses from different query transformation techniques.\n    \n    Args:\n        results (Dict): Results from different transformation techniques\n        reference_answer (str): Reference answer for comparison\n        model (str): Model for evaluation\n    \"\"\"\n    # Define the system prompt to guide the AI assistant's behavior\n    system_prompt = \"\"\"You are an expert evaluator of RAG systems. \n    Your task is to compare different responses generated using various query transformation techniques \n    and determine which technique produced the best response compared to the reference answer.\"\"\"\n    \n    # Prepare the comparison text with the reference answer and responses from each technique\n    comparison_text = f\"\"\"Reference Answer: {reference_answer}\\n\\n\"\"\"\n    \n    for technique, result in results.items():\n        comparison_text += f\"{technique.capitalize()} Query Response:\\n{result['response']}\\n\\n\"\n    \n    # Define the user prompt with the comparison text\n    user_prompt = f\"\"\"\n    {comparison_text}\n    \n    Compare the responses generated by different query transformation techniques to the reference answer.\n    \n    For each technique (original, rewrite, step_back, decompose):\n    1. Score the response from 1-10 based on accuracy, completeness, and relevance\n    2. Identify strengths and weaknesses\n    \n    Then rank the techniques from best to worst and explain which technique performed best overall and why.\n    \"\"\"\n    \n    # Generate the evaluation response using the specified model\n    response = client.chat.completions.create(\n        model=model,\n        temperature=0,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ]\n    )\n    \n    # Print the evaluation results\n    print(\"\\n===== EVALUATION RESULTS =====\")\n    print(response.choices[0].message.content)\n    print(\"=============================\")\n```\n\n----------------------------------------\n\nTITLE: Executing Reranking Evaluation and Displaying Results\nDESCRIPTION: Code that executes the evaluation function to compare standard retrieval results with LLM-based reranking results. It passes the user query, both result sets, and an optional reference answer, then prints the evaluation results.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/8_reranker.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Evaluate the quality of reranked results compared to standard results\nevaluation = evaluate_reranking(\n    query=query,  # The user query\n    standard_results=standard_results,  # Results from standard retrieval\n    reranked_results=llm_results,  # Results from LLM-based reranking\n    reference_answer=reference_answer  # Reference answer for comparison\n)\n\n# Print the evaluation results\nprint(\"\\n=== EVALUATION RESULTS ===\")\nprint(evaluation)\n```\n\n----------------------------------------\n\nTITLE: Implementing Analytical RAG Strategy in Python\nDESCRIPTION: A retrieval strategy for analytical queries focusing on comprehensive coverage. It breaks down complex queries into sub-questions, retrieves documents for each sub-query, and ensures diverse results. Requires a vector store and LLM client implementation.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/12_adaptive_rag.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef analytical_retrieval_strategy(query, vector_store, k=4):\n    \"\"\"\n    Retrieval strategy for analytical queries focusing on comprehensive coverage.\n    \n    Args:\n        query (str): User query\n        vector_store (SimpleVectorStore): Vector store\n        k (int): Number of documents to return\n        \n    Returns:\n        List[Dict]: Retrieved documents\n    \"\"\"\n    print(f\"Executing Analytical retrieval strategy for: '{query}'\")\n    \n    # Define the system prompt to guide the AI in generating sub-questions\n    system_prompt = \"\"\"You are an expert at breaking down complex questions.\n    Generate sub-questions that explore different aspects of the main analytical query.\n    These sub-questions should cover the breadth of the topic and help retrieve \n    comprehensive information.\n\n    Return a list of exactly 3 sub-questions, one per line.\n    \"\"\"\n\n    # Create the user prompt with the main query\n    user_prompt = f\"Generate sub-questions for this analytical query: {query}\"\n    \n    # Generate the sub-questions using the LLM\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0.3\n    )\n    \n    # Extract and clean the sub-questions\n    sub_queries = response.choices[0].message.content.strip().split('\\n')\n    sub_queries = [q.strip() for q in sub_queries if q.strip()]\n    print(f\"Generated sub-queries: {sub_queries}\")\n    \n    # Retrieve documents for each sub-query\n    all_results = []\n    for sub_query in sub_queries:\n        # Create embeddings for the sub-query\n        sub_query_embedding = create_embeddings(sub_query)\n        # Perform similarity search for the sub-query\n        results = vector_store.similarity_search(sub_query_embedding, k=2)\n        all_results.extend(results)\n    \n    # Ensure diversity by selecting from different sub-query results\n    # Remove duplicates (same text content)\n    unique_texts = set()\n    diverse_results = []\n    \n    for result in all_results:\n        if result[\"text\"] not in unique_texts:\n            unique_texts.add(result[\"text\"])\n            diverse_results.append(result)\n    \n    # If we need more results to reach k, add more from initial results\n    if len(diverse_results) < k:\n        # Direct retrieval for the main query\n        main_query_embedding = create_embeddings(query)\n        main_results = vector_store.similarity_search(main_query_embedding, k=k)\n        \n        for result in main_results:\n            if result[\"text\"] not in unique_texts and len(diverse_results) < k:\n                unique_texts.add(result[\"text\"])\n                diverse_results.append(result)\n    \n    # Return the top k diverse results\n    return diverse_results[:k]\n```\n\n----------------------------------------\n\nTITLE: Constructing a Prompt with Context for RAG Pipeline in Python\nDESCRIPTION: Function that combines a user query with retrieved context chunks to create a structured prompt for a language model. It includes a system message to guide the LLM's behavior and formatting to separate context from the question.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef construct_prompt(query: str, context_chunks: List[str]) -> str:\n    \"\"\"\n    Construct a prompt by combining the query with the retrieved context chunks.\n\n    Args:\n        query (str): The query text for which the prompt is being constructed.\n        context_chunks (List[str]): A list of relevant context chunks to include in the prompt.\n\n    Returns:\n        str: The constructed prompt to be used as input for the LLM.\n    \"\"\"\n    # Combine all context chunks into a single string, separated by newlines\n    context = \"\\n\".join(context_chunks)\n    \n    # Define the system message to guide the LLM's behavior\n    system_message = (\n        \"You are a helpful assistant. Only use the provided context to answer the question. \"\n        \"If the context doesn't contain the information needed, say 'I don't have enough information to answer this question.'\"\n    )\n    \n    # Construct the final prompt by combining the system message, context, and query\n    prompt = f\"System: {system_message}\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{query}\\n\\nAnswer:\"\n    \n    return prompt\n```\n\n----------------------------------------\n\nTITLE: Running RAG Evaluation Comparison in Python\nDESCRIPTION: Executes comparison evaluation between HyDE and standard RAG approaches using provided PDF content, test queries and reference answers\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/19_HyDE_rag.ipynb#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nevaluation_results = run_evaluation(\n    pdf_path=pdf_path,\n    test_queries=test_queries,\n    reference_answers=reference_answers\n)\n```\n\n----------------------------------------\n\nTITLE: Processing Documents for Adaptive RAG in Python\nDESCRIPTION: Pipeline function that processes a PDF document for use with adaptive retrieval. It extracts text, chunks it with overlap, creates embeddings for each chunk, and stores them in a vector store with appropriate metadata.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/12_adaptive_rag.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n    \"\"\"\n    Process a document for use with adaptive retrieval.\n\n    Args:\n    pdf_path (str): Path to the PDF file.\n    chunk_size (int): Size of each chunk in characters.\n    chunk_overlap (int): Overlap between chunks in characters.\n\n    Returns:\n    Tuple[List[str], SimpleVectorStore]: Document chunks and vector store.\n    \"\"\"\n    # Extract text from the PDF file\n    print(\"Extracting text from PDF...\")\n    extracted_text = extract_text_from_pdf(pdf_path)\n    \n    # Chunk the extracted text\n    print(\"Chunking text...\")\n    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n    print(f\"Created {len(chunks)} text chunks\")\n    \n    # Create embeddings for the text chunks\n    print(\"Creating embeddings for chunks...\")\n    chunk_embeddings = create_embeddings(chunks)\n    \n    # Initialize the vector store\n    store = SimpleVectorStore()\n    \n    # Add each chunk and its embedding to the vector store with metadata\n    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n        store.add_item(\n            text=chunk,\n            embedding=embedding,\n            metadata={\"index\": i, \"source\": pdf_path}\n        )\n    \n    print(f\"Added {len(chunks)} chunks to the vector store\")\n    \n    # Return the chunks and the vector store\n    return chunks, store\n```\n\n----------------------------------------\n\nTITLE: Rating Response Utility for Queries in Python\nDESCRIPTION: This function rates the utility of a response for a given query on a scale from 1 to 5. It evaluates how well the response answers the query, considering completeness, correctness, and helpfulness using the Llama-3.2-3B-Instruct model.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/13_self_rag.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef rate_utility(query, response):\n    \"\"\"\n    Rates the utility of a response for the query.\n    \n    Args:\n        query (str): User query\n        response (str): Generated response\n        \n    Returns:\n        int: Utility rating from 1 to 5\n    \"\"\"\n    # System prompt to instruct the AI on how to rate the utility of the response\n    system_prompt = \"\"\"You are an AI assistant that rates the utility of a response to a query.\n    Consider how well the response answers the query, its completeness, correctness, and helpfulness.\n    Rate the utility on a scale from 1 to 5, where:\n    - 1: Not useful at all\n    - 2: Slightly useful\n    - 3: Moderately useful\n    - 4: Very useful\n    - 5: Exceptionally useful\n    Answer with ONLY a single number from 1 to 5.\"\"\"\n\n    # User prompt containing the query and the response to be rated\n    user_prompt = f\"\"\"Query: {query}\n    Response:\n    {response}\n\n    Rate the utility of this response on a scale from 1 to 5:\"\"\"\n    \n    # Generate the utility rating using the OpenAI client\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0\n    )\n    \n    # Extract the rating from the model's response\n    rating = response.choices[0].message.content.strip()\n    \n    # Extract just the number from the rating\n    rating_match = re.search(r'[1-5]', rating)\n    if rating_match:\n        return int(rating_match.group())  # Return the extracted rating as an integer\n    \n    return 3  # Default to middle rating if parsing fails\n```\n\n----------------------------------------\n\nTITLE: Processing Document for Fusion Retrieval in Python\nDESCRIPTION: Function that processes a PDF document for fusion retrieval by extracting text, cleaning it, chunking it, creating embeddings, initializing a vector store, and creating a BM25 index. Returns all components needed for the retrieval pipeline.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/16_fusion_rag.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n    \"\"\"\n    Process a document for fusion retrieval.\n    \n    Args:\n        pdf_path (str): Path to the PDF file\n        chunk_size (int): Size of each chunk in characters\n        chunk_overlap (int): Overlap between chunks in characters\n        \n    Returns:\n        Tuple[List[Dict], SimpleVectorStore, BM25Okapi]: Chunks, vector store, and BM25 index\n    \"\"\"\n    # Extract text from the PDF file\n    text = extract_text_from_pdf(pdf_path)\n    \n    # Clean the extracted text to remove extra whitespace and special characters\n    cleaned_text = clean_text(text)\n    \n    # Split the cleaned text into overlapping chunks\n    chunks = chunk_text(cleaned_text, chunk_size, chunk_overlap)\n    \n    # Extract the text content from each chunk for embedding creation\n    chunk_texts = [chunk[\"text\"] for chunk in chunks]\n    print(\"Creating embeddings for chunks...\")\n    \n    # Create embeddings for the chunk texts\n    embeddings = create_embeddings(chunk_texts)\n    \n    # Initialize the vector store\n    vector_store = SimpleVectorStore()\n    \n    # Add the chunks and their embeddings to the vector store\n    vector_store.add_items(chunks, embeddings)\n    print(f\"Added {len(chunks)} items to vector store\")\n    \n    # Create a BM25 index from the chunks\n    bm25_index = create_bm25_index(chunks)\n    \n    # Return the chunks, vector store, and BM25 index\n    return chunks, vector_store, bm25_index\n```\n\n----------------------------------------\n\nTITLE: Adjusting Relevance Scores Based on Feedback for RAG in Python\nDESCRIPTION: This function adjusts document relevance scores based on historical feedback to improve retrieval quality in RAG systems. It analyzes past user feedback, calculates score modifiers, and re-ranks the results accordingly.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/11_feedback_loop_rag.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef adjust_relevance_scores(query, results, feedback_data):\n    \"\"\"\n    Adjust document relevance scores based on historical feedback to improve retrieval quality.\n    \n    This function analyzes past user feedback to dynamically adjust the relevance scores of \n    retrieved documents. It identifies feedback that is relevant to the current query context,\n    calculates score modifiers based on relevance ratings, and re-ranks the results accordingly.\n    \n    Args:\n        query (str): Current user query\n        results (List[Dict]): Retrieved documents with their original similarity scores\n        feedback_data (List[Dict]): Historical feedback containing user ratings\n        \n    Returns:\n        List[Dict]: Results with adjusted relevance scores, sorted by the new scores\n    \"\"\"\n    # If no feedback data available, return original results unchanged\n    if not feedback_data:\n        return results\n    \n    print(\"Adjusting relevance scores based on feedback history...\")\n    \n    # Process each retrieved document\n    for i, result in enumerate(results):\n        document_text = result[\"text\"]\n        relevant_feedback = []\n        \n        # Find relevant feedback for this specific document and query combination\n        # by querying the LLM to assess relevance of each historical feedback item\n        for feedback in feedback_data:\n            is_relevant = assess_feedback_relevance(query, document_text, feedback)\n            if is_relevant:\n                relevant_feedback.append(feedback)\n        \n        # Apply score adjustments if relevant feedback exists\n        if relevant_feedback:\n            # Calculate average relevance rating from all applicable feedback entries\n            # Feedback relevance is on a 1-5 scale (1=not relevant, 5=highly relevant)\n            avg_relevance = sum(f['relevance'] for f in relevant_feedback) / len(relevant_feedback)\n            \n            # Convert the average relevance to a score modifier in range 0.5-1.5\n            # - Scores below 3/5 will reduce the original similarity (modifier < 1.0)\n            # - Scores above 3/5 will increase the original similarity (modifier > 1.0)\n            modifier = 0.5 + (avg_relevance / 5.0)\n            \n            # Apply the modifier to the original similarity score\n            original_score = result[\"similarity\"]\n            adjusted_score = original_score * modifier\n            \n            # Update the result dictionary with new scores and feedback metadata\n            result[\"original_similarity\"] = original_score  # Preserve the original score\n            result[\"similarity\"] = adjusted_score           # Update the primary score\n            result[\"relevance_score\"] = adjusted_score      # Update the relevance score\n            result[\"feedback_applied\"] = True               # Flag that feedback was applied\n            result[\"feedback_count\"] = len(relevant_feedback)  # Number of feedback entries used\n            \n            # Log the adjustment details\n            print(f\"  Document {i+1}: Adjusted score from {original_score:.4f} to {adjusted_score:.4f} based on {len(relevant_feedback)} feedback(s)\")\n    \n    # Re-sort results by adjusted scores to ensure higher quality matches appear first\n    results.sort(key=lambda x: x[\"similarity\"], reverse=True)\n    \n    return results\n```\n\n----------------------------------------\n\nTITLE: Comparing RAG Results Using AI-Generated Analysis in Python\nDESCRIPTION: This function compares results from two rounds of RAG (with and without feedback). It uses an AI model to generate a comparison analysis for each query, focusing on how the feedback loop has impacted response quality. The function returns detailed comparisons for each query.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/11_feedback_loop_rag.ipynb#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef compare_results(queries, round1_results, round2_results, reference_answers=None):\n    \"\"\"\n    Compare results from two rounds of RAG.\n    \n    Args:\n        queries (List[str]): Test queries\n        round1_results (List[Dict]): Results from round 1\n        round2_results (List[Dict]): Results from round 2\n        reference_answers (List[str], optional): Reference answers\n        \n    Returns:\n        str: Comparison analysis\n    \"\"\"\n    print(\"\\n=== COMPARING RESULTS ===\")\n    \n    # System prompt to guide the AI's evaluation behavior\n    system_prompt = \"\"\"You are an expert evaluator of RAG systems. Compare responses from two versions:\n        1. Standard RAG: No feedback used\n        2. Feedback-enhanced RAG: Uses a feedback loop to improve retrieval\n\n        Analyze which version provides better responses in terms of:\n        - Relevance to the query\n        - Accuracy of information\n        - Completeness\n        - Clarity and conciseness\n    \"\"\"\n\n    comparisons = []\n    \n    # Iterate over each query and its corresponding results from both rounds\n    for i, (query, r1, r2) in enumerate(zip(queries, round1_results, round2_results)):\n        # Create a prompt for comparing the responses\n        comparison_prompt = f\"\"\"\n        Query: {query}\n\n        Standard RAG Response:\n        {r1[\"response\"]}\n\n        Feedback-enhanced RAG Response:\n        {r2[\"response\"]}\n        \"\"\"\n\n        # Include reference answer if available\n        if reference_answers and i < len(reference_answers):\n            comparison_prompt += f\"\"\"\n            Reference Answer:\n            {reference_answers[i]}\n            \"\"\"\n\n        comparison_prompt += \"\"\"\n        Compare these responses and explain which one is better and why.\n        Focus specifically on how the feedback loop has (or hasn't) improved the response quality.\n        \"\"\"\n\n        # Call the OpenAI API to generate a comparison analysis\n        response = client.chat.completions.create(\n            model=\"meta-llama/Llama-3.2-3B-Instruct\",\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": comparison_prompt}\n            ],\n            temperature=0\n        )\n        \n        # Append the comparison analysis to the results\n        comparisons.append({\n            \"query\": query,\n            \"analysis\": response.choices[0].message.content\n        })\n        \n        # Print a snippet of the analysis for each query\n        print(f\"\\nQuery {i+1}: {query}\")\n        print(f\"Analysis: {response.choices[0].message.content[:200]}...\")\n    \n    return comparisons\n```\n\n----------------------------------------\n\nTITLE: Processing Documents with Question Augmentation in Python\nDESCRIPTION: Main function to process PDF documents by extracting text, chunking it, generating questions for each chunk, and building a vector store. Includes embedding creation for both chunks and generated questions.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/6_doc_augmentation_rag.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef process_document(pdf_path, chunk_size=1000, chunk_overlap=200, questions_per_chunk=5):\n    \"\"\"\n    Process a document with question augmentation.\n\n    Args:\n    pdf_path (str): Path to the PDF file.\n    chunk_size (int): Size of each text chunk in characters.\n    chunk_overlap (int): Overlap between chunks in characters.\n    questions_per_chunk (int): Number of questions to generate per chunk.\n\n    Returns:\n    Tuple[List[str], SimpleVectorStore]: Text chunks and vector store.\n    \"\"\"\n    print(\"Extracting text from PDF...\")\n    extracted_text = extract_text_from_pdf(pdf_path)\n    \n    print(\"Chunking text...\")\n    text_chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n    print(f\"Created {len(text_chunks)} text chunks\")\n    \n    vector_store = SimpleVectorStore()\n    \n    print(\"Processing chunks and generating questions...\")\n    for i, chunk in enumerate(tqdm(text_chunks, desc=\"Processing Chunks\")):\n        # Create embedding for the chunk itself\n        chunk_embedding_response = create_embeddings(chunk)\n        chunk_embedding = chunk_embedding_response.data[0].embedding\n        \n        # Add the chunk to the vector store\n        vector_store.add_item(\n            text=chunk,\n            embedding=chunk_embedding,\n            metadata={\"type\": \"chunk\", \"index\": i}\n        )\n        \n        # Generate questions for this chunk\n        questions = generate_questions(chunk, num_questions=questions_per_chunk)\n        \n        # Create embeddings for each question and add to vector store\n        for j, question in enumerate(questions):\n            question_embedding_response = create_embeddings(question)\n            question_embedding = question_embedding_response.data[0].embedding\n            \n            # Add the question to the vector store\n            vector_store.add_item(\n                text=question,\n                embedding=question_embedding,\n                metadata={\"type\": \"question\", \"chunk_index\": i, \"original_chunk\": chunk}\n            )\n    \n    return text_chunks, vector_store\n```\n\n----------------------------------------\n\nTITLE: Evaluating CRAG Response Quality in Python\nDESCRIPTION: This function evaluates the quality of a CRAG response using GPT-4. It assesses relevance, accuracy, completeness, clarity, and source quality, returning a JSON object with scores and explanations.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/20_crag.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_crag_response(query, response, reference_answer=None):\n    \"\"\"\n    Evaluate the quality of a CRAG response.\n    \n    Args:\n        query (str): User query\n        response (str): Generated response\n        reference_answer (str, optional): Reference answer for comparison\n        \n    Returns:\n        Dict: Evaluation metrics\n    \"\"\"\n    # System prompt for the evaluation criteria\n    system_prompt = \"\"\"\n    You are an expert at evaluating the quality of responses to questions.\n    Please evaluate the provided response based on the following criteria:\n    \n    1. Relevance (0-10): How directly does the response address the query?\n    2. Accuracy (0-10): How factually correct is the information?\n    3. Completeness (0-10): How thoroughly does the response answer all aspects of the query?\n    4. Clarity (0-10): How clear and easy to understand is the response?\n    5. Source Quality (0-10): How well does the response cite relevant sources?\n    \n    Return your evaluation as a JSON object with scores for each criterion and a brief explanation for each score.\n    Also include an \"overall_score\" (0-10) and a brief \"summary\" of your evaluation.\n    \"\"\"\n    \n    # User prompt with the query and response to be evaluated\n    user_prompt = f\"\"\"\n    Query: {query}\n    \n    Response to evaluate:\n    {response}\n    \"\"\"\n    \n    # Include reference answer in the prompt if provided\n    if reference_answer:\n        user_prompt += f\"\"\"\n    Reference answer (for comparison):\n    {reference_answer}\n    \"\"\"\n    \n    try:\n        # Request evaluation from the GPT-4 model\n        evaluation_response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt}\n            ],\n            response_format={\"type\": \"json_object\"},\n            temperature=0\n        )\n        \n        # Parse the evaluation response\n        evaluation = json.loads(evaluation_response.choices[0].message.content)\n        return evaluation\n    except Exception as e:\n        # Handle any errors during the evaluation process\n        print(f\"Error evaluating response: {e}\")\n        return {\n            \"error\": str(e),\n            \"overall_score\": 0,\n            \"summary\": \"Evaluation failed due to an error.\"\n        }\n```\n\n----------------------------------------\n\nTITLE: Implementing Simple Vector Store with NumPy\nDESCRIPTION: A basic vector store implementation using NumPy arrays to store embeddings and their associated text and metadata. Includes functionality for adding items and performing similarity search using cosine similarity.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/6_doc_augmentation_rag.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleVectorStore:\n    \"\"\"\n    A simple vector store implementation using NumPy.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize the vector store.\n        \"\"\"\n        self.vectors = []\n        self.texts = []\n        self.metadata = []\n    \n    def add_item(self, text, embedding, metadata=None):\n        \"\"\"\n        Add an item to the vector store.\n\n        Args:\n        text (str): The original text.\n        embedding (List[float]): The embedding vector.\n        metadata (dict, optional): Additional metadata.\n        \"\"\"\n        self.vectors.append(np.array(embedding))\n        self.texts.append(text)\n        self.metadata.append(metadata or {})\n    \n    def similarity_search(self, query_embedding, k=5):\n        \"\"\"\n        Find the most similar items to a query embedding.\n\n        Args:\n        query_embedding (List[float]): Query embedding vector.\n        k (int): Number of results to return.\n\n        Returns:\n        List[Dict]: Top k most similar items with their texts and metadata.\n        \"\"\"\n        if not self.vectors:\n            return []\n        \n        # Convert query embedding to numpy array\n        query_vector = np.array(query_embedding)\n        \n        # Calculate similarities using cosine similarity\n        similarities = []\n        for i, vector in enumerate(self.vectors):\n            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n            similarities.append((i, similarity))\n        \n        # Sort by similarity (descending)\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        \n        # Return top k results\n        results = []\n        for i in range(min(k, len(similarities))):\n            idx, score = similarities[i]\n            results.append({\n                \"text\": self.texts[idx],\n                \"metadata\": self.metadata[idx],\n                \"similarity\": score\n            })\n        \n        return results\n```\n\n----------------------------------------\n\nTITLE: Context Preparation for Response Generation in Python\nDESCRIPTION: Function to prepare a unified context from search results by combining relevant chunks and questions for response generation.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/6_doc_augmentation_rag.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef prepare_context(search_results):\n    \"\"\"\n    Prepares a unified context from search results for response generation.\n\n    Args:\n    search_results (List[Dict]): Results from semantic search.\n\n    Returns:\n    str: Combined context string.\n    \"\"\"\n    # Extract unique chunks referenced in the results\n    chunk_indices = set()\n    context_chunks = []\n    \n    # First add direct chunk matches\n    for result in search_results:\n        if result[\"metadata\"][\"type\"] == \"chunk\":\n            chunk_indices.add(result[\"metadata\"][\"index\"])\n            context_chunks.append(f\"Chunk {result['metadata']['index']}:\\n{result['text']}\")\n    \n    # Then add chunks referenced by questions\n    for result in search_results:\n        if result[\"metadata\"][\"type\"] == \"question\":\n            chunk_idx = result[\"metadata\"][\"chunk_index\"]\n            if chunk_idx not in chunk_indices:\n                chunk_indices.add(chunk_idx)\n                context_chunks.append(f\"Chunk {chunk_idx} (referenced by question '{result['text']}'):\\n{result['metadata']['original_chunk']}\")\n    \n    # Combine all context chunks\n    full_context = \"\\n\\n\".join(context_chunks)\n    return full_context\n```\n\n----------------------------------------\n\nTITLE: Retrieval Approaches Comparison\nDESCRIPTION: Function to compare chunk-based and proposition-based retrieval approaches. Takes a query and searches both stores, displaying and returning the results.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/14_proposition_chunking.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef compare_retrieval_approaches(query, chunk_store, prop_store, k=5):\n    \"\"\"\n    Compare chunk-based and proposition-based retrieval for a query.\n    \n    Args:\n        query (str): User query\n        chunk_store (SimpleVectorStore): Chunk-based vector store\n        prop_store (SimpleVectorStore): Proposition-based vector store\n        k (int): Number of results to retrieve from each store\n        \n    Returns:\n        Dict: Comparison results\n    \"\"\"\n    print(f\"\\n=== Query: {query} ===\")\n    \n    # Retrieve results from the proposition-based vector store\n    print(\"\\nRetrieving with proposition-based approach...\")\n    prop_results = retrieve_from_store(query, prop_store, k)\n    \n    # Retrieve results from the chunk-based vector store\n    print(\"Retrieving with chunk-based approach...\")\n    chunk_results = retrieve_from_store(query, chunk_store, k)\n    \n    # Display proposition-based results\n    print(\"\\n=== Proposition-Based Results ===\")\n    for i, result in enumerate(prop_results):\n        print(f\"{i+1}) {result['text']} (Score: {result['similarity']:.4f})\")\n    \n    # Display chunk-based results\n    print(\"\\n=== Chunk-Based Results ===\")\n    for i, result in enumerate(chunk_results):\n        # Truncate text to keep the output manageable\n        truncated_text = result['text'][:150] + \"...\" if len(result['text']) > 150 else result['text']\n        print(f\"{i+1}) {truncated_text} (Score: {result['similarity']:.4f})\")\n    \n    # Return the comparison results\n    return {\n        \"query\": query,\n        \"proposition_results\": prop_results,\n        \"chunk_results\": chunk_results\n    }\n```\n\n----------------------------------------\n\nTITLE: Defining Action Space for RL Agent in Python\nDESCRIPTION: This function defines the set of possible actions that the reinforcement learning agent can take. The actions include rewriting the query, expanding the context, filtering the context, and generating a response based on the current state.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef define_action_space() -> List[str]:\n    \"\"\"\n    Define the set of possible actions the reinforcement learning agent can take.\n    \n    Actions include:\n    - rewrite_query: Reformulate the original query to improve retrieval\n    - expand_context: Retrieve additional context chunks\n    - filter_context: Remove irrelevant context chunks\n    - generate_response: Generate a response based on current query and context\n    \n    Returns:\n        List[str]: A list of available actions.\n    \"\"\"\n\n    # Define the set of actions the agent can take\n    actions = [\"rewrite_query\", \"expand_context\", \"filter_context\", \"generate_response\"]\n    return actions\n```\n\n----------------------------------------\n\nTITLE: Running Multi-Query Evaluation of RAG Approaches in Python\nDESCRIPTION: This function executes a comprehensive evaluation of both RAG approaches using multiple test queries. It processes each query through both approaches and collects the results for overall analysis.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/18_hierarchy_rag.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef run_evaluation(pdf_path, test_queries, reference_answers=None):\n    \"\"\"\n    Run a complete evaluation with multiple test queries.\n    \n    Args:\n        pdf_path (str): Path to the PDF document\n        test_queries (List[str]): List of test queries\n        reference_answers (List[str], optional): Reference answers for queries\n        \n    Returns:\n        Dict: Evaluation results\n    \"\"\"\n    results = []  # Initialize an empty list to store results\n    \n    # Iterate over each query in the test queries\n    for i, query in enumerate(test_queries):\n        print(f\"Query: {query}\")  # Print the current query\n        \n        # Get reference answer if available\n        reference = None\n        if reference_answers and i < len(reference_answers):\n            reference = reference_answers[i]  # Retrieve the reference answer for the current query\n        \n        # Compare hierarchical and standard RAG approaches\n        result = compare_approaches(query, pdf_path, reference)\n        results.append(result)  # Append the result to the results list\n    \n    # Generate overall analysis of the evaluation results\n    overall_analysis = generate_overall_analysis(results)\n    \n    return {\n        \"results\": results,  # Return the individual results\n        \"overall_analysis\": overall_analysis  # Return the overall analysis\n    }\n```\n\n----------------------------------------\n\nTITLE: Generating Responses with LLM using RSE Context\nDESCRIPTION: Function that generates a response to the user's query using the extracted relevant segments as context. It creates a structured prompt for the language model that includes the context and query, along with instructions on how to use the provided information.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/9_rse.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef generate_response(query, context, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Generate a response based on the query and context.\n    \n    Args:\n        query (str): User query\n        context (str): Context text from relevant segments\n        model (str): LLM model to use\n        \n    Returns:\n        str: Generated response\n    \"\"\"\n    print(\"Generating response using relevant segments as context...\")\n    \n    # Define the system prompt to guide the AI's behavior\n    system_prompt = \"\"\"You are a helpful assistant that answers questions based on the provided context.\n    The context consists of document segments that have been retrieved as relevant to the user's query.\n    Use the information from these segments to provide a comprehensive and accurate answer.\n    If the context doesn't contain relevant information to answer the question, say so clearly.\"\"\"\n    \n    # Create the user prompt by combining the context and the query\n    user_prompt = f\"\"\"\nContext:\n{context}\n\nQuestion: {query}\n\nPlease provide a helpful answer based on the context provided.\n\"\"\"\n    \n    # Generate the response using the specified model\n    response = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0\n    )\n    \n    # Return the generated response content\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Document Relevance Evaluation with GPT-3.5\nDESCRIPTION: Function that evaluates document relevance to a query using OpenAI's GPT-3.5-turbo model. Returns a relevance score between 0 and 1 based on the model's assessment. Includes error handling and default fallback values.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/20_crag.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_document_relevance(query, document):\n    \"\"\"\n    Evaluate the relevance of a document to a query.\n    \n    Args:\n        query (str): User query\n        document (str): Document text\n        \n    Returns:\n        float: Relevance score (0-1)\n    \"\"\"\n    # Define the system prompt to instruct the model on how to evaluate relevance\n    system_prompt = \"\"\"\n    You are an expert at evaluating document relevance. \n    Rate how relevant the given document is to the query on a scale from 0 to 1.\n    0 means completely irrelevant, 1 means perfectly relevant.\n    Provide ONLY the score as a float between 0 and 1.\n    \"\"\"\n    \n    # Define the user prompt with the query and document\n    user_prompt = f\"Query: {query}\\n\\nDocument: {document}\"\n    \n    try:\n        # Make a request to the OpenAI API to evaluate the relevance\n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",  # Specify the model to use\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n                {\"role\": \"user\", \"content\": user_prompt}  # User message with the query and document\n            ],\n            temperature=0,  # Set the temperature for response generation\n            max_tokens=5  # Very short response needed\n        )\n        \n        # Extract the score from the response\n        score_text = response.choices[0].message.content.strip()\n        # Use regex to find the float value in the response\n        score_match = re.search(r'(\\d+(\\.\\d+)?)', score_text)\n        if score_match:\n            return float(score_match.group(1))  # Return the extracted score as a float\n        return 0.5  # Default to middle value if parsing fails\n    \n    except Exception as e:\n        # Print the error message and return a default value on error\n        print(f\"Error evaluating document relevance: {e}\")\n        return 0.5  # Default to middle value on error\n```\n\n----------------------------------------\n\nTITLE: Generating Overall Analysis for CRAG vs RAG Evaluation in Python\nDESCRIPTION: This function generates an overall analysis of evaluation results comparing CRAG to standard RAG. It uses GPT-4 to analyze the results, focusing on the strengths and weaknesses of each approach and providing recommendations for their use.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/20_crag.ipynb#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef generate_overall_analysis(results):\n    \"\"\"\n    Generate an overall analysis of evaluation results.\n    \n    Args:\n        results (List[Dict]): Results from individual query evaluations\n        \n    Returns:\n        str: Overall analysis\n    \"\"\"\n    # System prompt for the analysis\n    system_prompt = \"\"\"\n    You are an expert at evaluating information retrieval and response generation systems.\n    Based on multiple test queries, provide an overall analysis comparing CRAG (Corrective RAG) \n    with standard RAG.\n    \n    Focus on:\n    1. When CRAG performs better and why\n    2. When standard RAG performs better and why\n    3. The overall strengths and weaknesses of each approach\n    4. Recommendations for when to use each approach\n    \"\"\"\n    \n    # Create summary of evaluations\n    evaluations_summary = \"\"\n    for i, result in enumerate(results):\n        evaluations_summary += f\"Query {i+1}: {result['query']}\\n\"\n        if 'crag_evaluation' in result and 'overall_score' in result['crag_evaluation']:\n            crag_score = result['crag_evaluation'].get('overall_score', 'N/A')\n            evaluations_summary += f\"CRAG score: {crag_score}\\n\"\n        if 'standard_evaluation' in result and 'overall_score' in result['standard_evaluation']:\n            std_score = result['standard_evaluation'].get('overall_score', 'N/A')\n            evaluations_summary += f\"Standard RAG score: {std_score}\\n\"\n        evaluations_summary += f\"Comparison summary: {result['comparison'][:200]}...\\n\\n\"\n    \n    # User prompt for the analysis\n    user_prompt = f\"\"\"\n    Based on the following evaluations comparing CRAG vs standard RAG across {len(results)} queries, \n    provide an overall analysis of these two approaches:\n    \n    {evaluations_summary}\n    \n    Please provide a comprehensive analysis of the relative strengths and weaknesses of CRAG \n    compared to standard RAG, focusing on when and why one approach outperforms the other.\n    \"\"\"\n    \n    try:\n        # Generate the overall analysis using GPT-4\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt}\n            ],\n            temperature=0\n        )\n        \n        return response.choices[0].message.content.strip()\n    except Exception as e:\n        print(f\"Error generating overall analysis: {e}\")\n        return f\"Error generating overall analysis: {str(e)}\"\n```\n\n----------------------------------------\n\nTITLE: Performing Semantic Search in RAG with CCH\nDESCRIPTION: This function implements semantic search to find the most relevant text chunks for a user query. It uses cosine similarity to compare query embeddings with both text and header embeddings of chunks, incorporating the CCH technique to improve retrieval accuracy in RAG.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/5_contextual_chunk_headers_rag.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef semantic_search(query, chunks, k=5):\n    \"\"\"\n    Searches for the most relevant chunks based on a query.\n\n    Args:\n    query (str): User query.\n    chunks (List[dict]): List of text chunks with embeddings.\n    k (int): Number of top results.\n\n    Returns:\n    List[dict]: Top-k most relevant chunks.\n    \"\"\"\n    # Create an embedding for the query\n    query_embedding = create_embeddings(query)\n\n    similarities = []  # Initialize a list to store similarity scores\n    \n    # Iterate through each chunk to calculate similarity scores\n    for chunk in chunks:\n        # Compute cosine similarity between query embedding and chunk text embedding\n        sim_text = cosine_similarity(np.array(query_embedding), np.array(chunk[\"embedding\"]))\n        # Compute cosine similarity between query embedding and chunk header embedding\n        sim_header = cosine_similarity(np.array(query_embedding), np.array(chunk[\"header_embedding\"]))\n        # Calculate the average similarity score\n        avg_similarity = (sim_text + sim_header) / 2\n        # Append the chunk and its average similarity score to the list\n        similarities.append((chunk, avg_similarity))\n\n    # Sort the chunks based on similarity scores in descending order\n    similarities.sort(key=lambda x: x[1], reverse=True)\n    # Return the top-k most relevant chunks\n    return [x[0] for x in similarities[:k]]\n```\n\n----------------------------------------\n\nTITLE: Running Adaptive RAG Evaluation in Python\nDESCRIPTION: This code snippet demonstrates how to run the evaluation comparing adaptive vs standard retrieval. It processes each query using both methods and compares the results, highlighting where adaptive strategies provide improved outcomes.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/12_adaptive_rag.ipynb#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# Run the evaluation comparing adaptive vs standard retrieval\n# This will process each query using both methods and compare the results\nevaluation_results = evaluate_adaptive_vs_standard(\n    pdf_path=pdf_path,                  # Source document for knowledge extraction\n    test_queries=test_queries,          # List of test queries to evaluate\n    reference_answers=reference_answers  # Optional ground truth for comparison\n)\n\n# The results will show a detailed comparison between standard retrieval and \n# adaptive retrieval performance across different query types, highlighting\n# where adaptive strategies provide improved outcomes\nprint(evaluation_results[\"comparison\"])\n```\n\n----------------------------------------\n\nTITLE: Formatting Segments for LLM Context\nDESCRIPTION: Function that formats the reconstructed text segments into a structured context string for the language model. It adds headers with segment indices and chunk ranges to help the model understand the structure of the provided context.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/9_rse.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef format_segments_for_context(segments):\n    \"\"\"\n    Format segments into a context string for the LLM.\n    \n    Args:\n        segments (List[Dict]): List of segment dictionaries\n        \n    Returns:\n        str: Formatted context text\n    \"\"\"\n    context = []  # Initialize an empty list to store the formatted context\n    \n    for i, segment in enumerate(segments):\n        # Create a header for each segment with its index and chunk range\n        segment_header = f\"SEGMENT {i+1} (Chunks {segment['segment_range'][0]}-{segment['segment_range'][1]-1}):\"\n        context.append(segment_header)  # Add the segment header to the context list\n        context.append(segment['text'])  # Add the segment text to the context list\n        context.append(\"-\" * 80)  # Add a separator line for readability\n    \n    # Join all elements in the context list with double newlines and return the result\n    return \"\\n\\n\".join(context)\n```\n\n----------------------------------------\n\nTITLE: Comparing RAG Responses with LLM Evaluation\nDESCRIPTION: Function that uses LLaMA 3.2 to compare and evaluate responses from multi-modal and text-only RAG systems. It considers accuracy, completeness, relevance, and unique visual information.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/15_multimodel_rag.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef compare_responses(query, mm_response, text_response, reference=None):\n    \"\"\"\n    Compare multi-modal and text-only responses.\n    \n    Args:\n        query (str): User query\n        mm_response (str): Multi-modal response\n        text_response (str): Text-only response\n        reference (str, optional): Reference answer\n        \n    Returns:\n        str: Comparison analysis\n    \"\"\"\n    # System prompt for the evaluator\n    system_prompt = \"\"\"You are an expert evaluator comparing two RAG systems:\n    1. Multi-modal RAG: Retrieves from both text and image captions\n    2. Text-only RAG: Retrieves only from text\n\n    Evaluate which response better answers the query based on:\n    - Accuracy and correctness\n    - Completeness of information\n    - Relevance to the query\n    - Unique information from visual elements (for multi-modal)\"\"\"\n\n    # User prompt with query and responses\n    user_prompt = f\"\"\"Query: {query}\n\n    Multi-modal RAG Response:\n    {mm_response}\n\n    Text-only RAG Response:\n    {text_response}\n    \"\"\"\n\n    if reference:\n        user_prompt += f\"\"\"\n    Reference Answer:\n    {reference}\n    \"\"\"\n\n        user_prompt += \"\"\"\n    Compare these responses and explain which one better answers the query and why.\n    Note any specific information that came from images in the multi-modal response.\n    \"\"\"\n\n    # Generate comparison using meta-llama/Llama-3.2-3B-Instruct\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0\n    )\n    \n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Implementing a Simple Vector Store for Similarity Search\nDESCRIPTION: Defines a SimpleVectorStore class that stores text documents with their embedding vectors and provides similarity search functionality using cosine similarity. It supports adding individual items or batches and retrieving documents with similarity scores.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/16_fusion_rag.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleVectorStore:\n    \"\"\"\n    A simple vector store implementation using NumPy.\n    \"\"\"\n    def __init__(self):\n        self.vectors = []  # List to store embedding vectors\n        self.texts = []  # List to store text content\n        self.metadata = []  # List to store metadata\n    \n    def add_item(self, text, embedding, metadata=None):\n        \"\"\"\n        Add an item to the vector store.\n        \n        Args:\n            text (str): The text content\n            embedding (List[float]): The embedding vector\n            metadata (Dict, optional): Additional metadata\n        \"\"\"\n        self.vectors.append(np.array(embedding))  # Append the embedding vector\n        self.texts.append(text)  # Append the text content\n        self.metadata.append(metadata or {})  # Append the metadata (or empty dict if None)\n    \n    def add_items(self, items, embeddings):\n        \"\"\"\n        Add multiple items to the vector store.\n        \n        Args:\n            items (List[Dict]): List of text items\n            embeddings (List[List[float]]): List of embedding vectors\n        \"\"\"\n        for i, (item, embedding) in enumerate(zip(items, embeddings)):\n            self.add_item(\n                text=item[\"text\"],  # Extract text from item\n                embedding=embedding,  # Use corresponding embedding\n                metadata={**item.get(\"metadata\", {}), \"index\": i}  # Merge item metadata with index\n            )\n    \n    def similarity_search_with_scores(self, query_embedding, k=5):\n        \"\"\"\n        Find the most similar items to a query embedding with similarity scores.\n        \n        Args:\n            query_embedding (List[float]): Query embedding vector\n            k (int): Number of results to return\n            \n        Returns:\n            List[Tuple[Dict, float]]: Top k most similar items with scores\n        \"\"\"\n        if not self.vectors:\n            return []  # Return empty list if no vectors are stored\n        \n        # Convert query embedding to numpy array\n        query_vector = np.array(query_embedding)\n        \n        # Calculate similarities using cosine similarity\n        similarities = []\n        for i, vector in enumerate(self.vectors):\n            similarity = cosine_similarity([query_vector], [vector])[0][0]  # Compute cosine similarity\n            similarities.append((i, similarity))  # Append index and similarity score\n        \n        # Sort by similarity (descending)\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        \n        # Return top k results with scores\n        results = []\n        for i in range(min(k, len(similarities))):\n            idx, score = similarities[i]\n            results.append({\n                \"text\": self.texts[idx],  # Retrieve text by index\n                \"metadata\": self.metadata[idx],  # Retrieve metadata by index\n                \"similarity\": float(score)  # Add similarity score\n            })\n        \n        return results\n    \n    def get_all_documents(self):\n        \"\"\"\n        Get all documents in the store.\n        \n        Returns:\n            List[Dict]: All documents\n        \"\"\"\n        return [{\"text\": text, \"metadata\": meta} for text, meta in zip(self.texts, self.metadata)]  # Combine texts and metadata\n```\n\n----------------------------------------\n\nTITLE: Generating Overall Analysis of RAG Evaluation Results in Python\nDESCRIPTION: This function produces a comprehensive analysis of evaluation results across multiple queries. It uses a language model to analyze patterns in hierarchical vs standard RAG performance and provide recommendations.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/18_hierarchy_rag.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef generate_overall_analysis(results):\n    \"\"\"\n    Generate an overall analysis of the evaluation results.\n    \n    Args:\n        results (List[Dict]): Results from individual query evaluations\n        \n    Returns:\n        str: Overall analysis\n    \"\"\"\n    # Define the system prompt to instruct the model on how to evaluate the results\n    system_prompt = \"\"\"You are an expert at evaluating information retrieval systems.\nBased on multiple test queries, provide an overall analysis comparing hierarchical RAG \nwith standard RAG.\n\nFocus on:\n1. When hierarchical retrieval performs better and why\n2. When standard retrieval performs better and why\n3. The overall strengths and weaknesses of each approach\n4. Recommendations for when to use each approach\"\"\"\n\n    # Create a summary of the evaluations\n    evaluations_summary = \"\"\n    for i, result in enumerate(results):\n        evaluations_summary += f\"Query {i+1}: {result['query']}\\n\"\n        evaluations_summary += f\"Hierarchical chunks: {result['hierarchical_chunks_count']}, Standard chunks: {result['standard_chunks_count']}\\n\"\n        evaluations_summary += f\"Comparison summary: {result['comparison'][:200]}...\\n\\n\"\n\n    # Define the user prompt with the evaluations summary\n    user_prompt = f\"\"\"Based on the following evaluations comparing hierarchical vs standard RAG across {len(results)} queries, \nprovide an overall analysis of these two approaches:\n\n{evaluations_summary}\n\nPlease provide a comprehensive analysis of the relative strengths and weaknesses of hierarchical RAG \ncompared to standard RAG, with specific focus on retrieval quality and response generation.\"\"\"\n\n    # Make a request to the OpenAI API to generate the overall analysis\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n            {\"role\": \"user\", \"content\": user_prompt}  # User message with the evaluations summary\n        ],\n        temperature=0  # Set the temperature for response generation\n    )\n    \n    # Return the generated overall analysis\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Running RAG Evaluation with Validation Data in Python\nDESCRIPTION: This code snippet demonstrates how to load validation data from a JSON file, extract a query and reference answer, and run the evaluation of RAG methods using a specific PDF document.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/9_rse.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Load the validation data from a JSON file\nwith open('data/val.json') as f:\n    data = json.load(f)\n\n# Extract the first query from the validation data\nquery = data[0]['question']\n\n# Extract the reference answer from the validation data\nreference_answer = data[0]['ideal_answer']\n\n# pdf_path\npdf_path = \"data/AI_Information.pdf\"\n\n# Run evaluation\nresults = evaluate_methods(pdf_path, query, reference_answer)\n```\n\n----------------------------------------\n\nTITLE: Implementing Semantic Search on Text Chunks in Python\nDESCRIPTION: This function performs semantic search on text chunks using embeddings. It creates an embedding for the query, calculates similarity scores between the query and text chunks, and returns the top k most relevant chunks based on these scores.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/6_doc_augmentation_rag.ipynb#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef semantic_search(query, text_chunks, embeddings, k=5):\n    \"\"\"\n    Performs semantic search on the text chunks using the given query and embeddings.\n\n    Args:\n    query (str): The query for the semantic search.\n    text_chunks (List[str]): A list of text chunks to search through.\n    embeddings (List[dict]): A list of embeddings for the text chunks.\n    k (int): The number of top relevant text chunks to return. Default is 5.\n\n    Returns:\n    List[str]: A list of the top k most relevant text chunks based on the query.\n    \"\"\"\n    # Create an embedding for the query\n    query_embedding = create_embeddings(query).data[0].embedding\n    similarity_scores = []  # Initialize a list to store similarity scores\n\n    # Calculate similarity scores between the query embedding and each text chunk embedding\n    for i, chunk_embedding in enumerate(embeddings):\n        similarity_score = cosine_similarity(np.array(query_embedding), np.array(chunk_embedding.embedding))\n        similarity_scores.append((i, similarity_score))  # Append the index and similarity score\n\n    # Sort the similarity scores in descending order\n    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n    # Get the indices of the top k most similar text chunks\n    top_indices = [index for index, _ in similarity_scores[:k]]\n    # Return the top k most relevant text chunks\n    return [text_chunks[index] for index in top_indices]\n```\n\n----------------------------------------\n\nTITLE: Implementing Opinion RAG Strategy in Python\nDESCRIPTION: A retrieval strategy for opinion queries focusing on diverse perspectives. It identifies different viewpoints using LLM, retrieves documents for each viewpoint, and ensures representation of multiple perspectives. Requires a vector store and LLM client implementation.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/12_adaptive_rag.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef opinion_retrieval_strategy(query, vector_store, k=4):\n    \"\"\"\n    Retrieval strategy for opinion queries focusing on diverse perspectives.\n    \n    Args:\n        query (str): User query\n        vector_store (SimpleVectorStore): Vector store\n        k (int): Number of documents to return\n        \n    Returns:\n        List[Dict]: Retrieved documents\n    \"\"\"\n    print(f\"Executing Opinion retrieval strategy for: '{query}'\")\n    \n    # Define the system prompt to guide the AI in identifying different perspectives\n    system_prompt = \"\"\"You are an expert at identifying different perspectives on a topic.\n        For the given query about opinions or viewpoints, identify different perspectives \n        that people might have on this topic.\n\n        Return a list of exactly 3 different viewpoint angles, one per line.\n    \"\"\"\n\n    # Create the user prompt with the main query\n    user_prompt = f\"Identify different perspectives on: {query}\"\n    \n    # Generate the different perspectives using the LLM\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0.3\n    )\n    \n    # Extract and clean the viewpoints\n    viewpoints = response.choices[0].message.content.strip().split('\\n')\n    viewpoints = [v.strip() for v in viewpoints if v.strip()]\n    print(f\"Identified viewpoints: {viewpoints}\")\n    \n    # Retrieve documents representing each viewpoint\n    all_results = []\n    for viewpoint in viewpoints:\n        # Combine the main query with the viewpoint\n        combined_query = f\"{query} {viewpoint}\"\n        # Create embeddings for the combined query\n        viewpoint_embedding = create_embeddings(combined_query)\n        # Perform similarity search for the combined query\n        results = vector_store.similarity_search(viewpoint_embedding, k=2)\n        \n        # Mark results with the viewpoint they represent\n        for result in results:\n            result[\"viewpoint\"] = viewpoint\n        \n        # Add the results to the list of all results\n        all_results.extend(results)\n    \n    # Select a diverse range of opinions\n    # Ensure we get at least one document from each viewpoint if possible\n    selected_results = []\n    for viewpoint in viewpoints:\n        # Filter documents by viewpoint\n        viewpoint_docs = [r for r in all_results if r.get(\"viewpoint\") == viewpoint]\n        if viewpoint_docs:\n            selected_results.append(viewpoint_docs[0])\n    \n    # Fill remaining slots with highest similarity docs\n    remaining_slots = k - len(selected_results)\n    if remaining_slots > 0:\n        # Sort remaining docs by similarity\n        remaining_docs = [r for r in all_results if r not in selected_results]\n        remaining_docs.sort(key=lambda x: x[\"similarity\"], reverse=True)\n        selected_results.extend(remaining_docs[:remaining_slots])\n    \n    # Return the top k results\n    return selected_results[:k]\n```\n\n----------------------------------------\n\nTITLE: Finding Optimal Text Segments in RSE Algorithm\nDESCRIPTION: Function that implements a variant of the maximum sum subarray algorithm to identify the best continuous segments of text based on chunk values. The algorithm respects constraints on segment length while maximizing the total value of included chunks.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/9_rse.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef find_best_segments(chunk_values, max_segment_length=20, total_max_length=30, min_segment_value=0.2):\n    \"\"\"\n    Find the best segments using a variant of the maximum sum subarray algorithm.\n    \n    Args:\n        chunk_values (List[float]): Values for each chunk\n        max_segment_length (int): Maximum length of a single segment\n        total_max_length (int): Maximum total length across all segments\n        min_segment_value (float): Minimum value for a segment to be considered\n        \n    Returns:\n        List[Tuple[int, int]]: List of (start, end) indices for best segments\n    \"\"\"\n    print(\"Finding optimal continuous text segments...\")\n    \n    best_segments = []\n    segment_scores = []\n    total_included_chunks = 0\n    \n    # Keep finding segments until we hit our limits\n    while total_included_chunks < total_max_length:\n        best_score = min_segment_value  # Minimum threshold for a segment\n        best_segment = None\n        \n        # Try each possible starting position\n        for start in range(len(chunk_values)):\n            # Skip if this start position is already in a selected segment\n            if any(start >= s[0] and start < s[1] for s in best_segments):\n                continue\n                \n            # Try each possible segment length\n            for length in range(1, min(max_segment_length, len(chunk_values) - start) + 1):\n                end = start + length\n                \n                # Skip if end position is already in a selected segment\n                if any(end > s[0] and end <= s[1] for s in best_segments):\n                    continue\n                \n                # Calculate segment value as sum of chunk values\n                segment_value = sum(chunk_values[start:end])\n                \n                # Update best segment if this one is better\n                if segment_value > best_score:\n                    best_score = segment_value\n                    best_segment = (start, end)\n        \n        # If we found a good segment, add it\n        if best_segment:\n            best_segments.append(best_segment)\n            segment_scores.append(best_score)\n            total_included_chunks += best_segment[1] - best_segment[0]\n            print(f\"Found segment {best_segment} with score {best_score:.4f}\")\n        else:\n            # No more good segments to find\n            break\n    \n    # Sort segments by their starting position for readability\n    best_segments = sorted(best_segments, key=lambda x: x[0])\n    \n    return best_segments, segment_scores\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Vector Index with Feedback in Python\nDESCRIPTION: Enhances a vector store using high-quality feedback data. The function identifies successful Q&A pairs, creates new retrieval items from them, and adds them to the vector store with boosted relevance weights to improve retrieval quality over time.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/11_feedback_loop_rag.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef fine_tune_index(current_store, chunks, feedback_data):\n    \"\"\"\n    Enhance vector store with high-quality feedback to improve retrieval quality over time.\n    \n    This function implements a continuous learning process by:\n    1. Identifying high-quality feedback (highly rated Q&A pairs)\n    2. Creating new retrieval items from successful interactions\n    3. Adding these to the vector store with boosted relevance weights\n    \n    Args:\n        current_store (SimpleVectorStore): Current vector store containing original document chunks\n        chunks (List[str]): Original document text chunks \n        feedback_data (List[Dict]): Historical user feedback with relevance and quality ratings\n        \n    Returns:\n        SimpleVectorStore: Enhanced vector store containing both original chunks and feedback-derived content\n    \"\"\"\n    print(\"Fine-tuning index with high-quality feedback...\")\n    \n    # Filter for only high-quality responses (both relevance and quality rated 4 or 5)\n    # This ensures we only learn from the most successful interactions\n    good_feedback = [f for f in feedback_data if f['relevance'] >= 4 and f['quality'] >= 4]\n    \n    if not good_feedback:\n        print(\"No high-quality feedback found for fine-tuning.\")\n        return current_store  # Return original store unchanged if no good feedback exists\n    \n    # Initialize new store that will contain both original and enhanced content\n    new_store = SimpleVectorStore()\n    \n    # First transfer all original document chunks with their existing metadata\n    for i in range(len(current_store.texts)):\n        new_store.add_item(\n            text=current_store.texts[i],\n            embedding=current_store.vectors[i],\n            metadata=current_store.metadata[i].copy()  # Use copy to prevent reference issues\n        )\n    \n    # Create and add enhanced content from good feedback\n    for feedback in good_feedback:\n        # Format a new document that combines the question and its high-quality answer\n        # This creates retrievable content that directly addresses user queries\n        enhanced_text = f\"Question: {feedback['query']}\\nAnswer: {feedback['response']}\"\n        \n        # Generate embedding vector for this new synthetic document\n        embedding = create_embeddings(enhanced_text)\n        \n        # Add to vector store with special metadata that identifies its origin and importance\n        new_store.add_item(\n            text=enhanced_text,\n            embedding=embedding,\n            metadata={\n                \"type\": \"feedback_enhanced\",  # Mark as derived from feedback\n                \"query\": feedback[\"query\"],   # Store original query for reference\n                \"relevance_score\": 1.2,       # Boost initial relevance to prioritize these items\n                \"feedback_count\": 1,          # Track feedback incorporation\n                \"original_feedback\": feedback # Preserve complete feedback record\n            }\n        )\n        \n        print(f\"Added enhanced content from feedback: {feedback['query'][:50]}...\")\n    \n    # Log summary statistics about the enhancement\n    print(f\"Fine-tuned index now has {len(new_store.texts)} items (original: {len(chunks)})\")\n    return new_store\n```\n\n----------------------------------------\n\nTITLE: Web Search Orchestration\nDESCRIPTION: Function that combines query rewriting and web search functionality. Coordinates the process of improving queries before performing the search to get better results.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/20_crag.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef perform_web_search(query):\n    \"\"\"\n    Perform web search with query rewriting.\n    \n    Args:\n        query (str): Original user query\n        \n    Returns:\n        Tuple[str, List[Dict]]: Search results text and source metadata\n    \"\"\"\n    # Rewrite the query to improve search results\n    rewritten_query = rewrite_search_query(query)\n    print(f\"Rewritten search query: {rewritten_query}\")\n    \n    # Perform the web search using the rewritten query\n    results_text, sources = duck_duck_go_search(rewritten_query)\n    \n    # Return the search results text and source metadata\n    return results_text, sources\n```\n\n----------------------------------------\n\nTITLE: Evaluating Reranking Quality in RAG Systems\nDESCRIPTION: Function to evaluate the quality of reranked results compared to standard results. It uses an LLM to assess which retrieval method provides better context and more accurate answers, with optional reference answer support.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/8_reranker.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_reranking(query, standard_results, reranked_results, reference_answer=None):\n    \"\"\"\n    Evaluates the quality of reranked results compared to standard results.\n    \n    Args:\n        query (str): User query\n        standard_results (Dict): Results from standard retrieval\n        reranked_results (Dict): Results from reranked retrieval\n        reference_answer (str, optional): Reference answer for comparison\n        \n    Returns:\n        str: Evaluation output\n    \"\"\"\n    # Define the system prompt for the AI evaluator\n    system_prompt = \"\"\"You are an expert evaluator of RAG systems.\n    Compare the retrieved contexts and responses from two different retrieval methods.\n    Assess which one provides better context and a more accurate, comprehensive answer.\"\"\"\n    \n    # Prepare the comparison text with truncated contexts and responses\n    comparison_text = f\"\"\"Query: {query}\n\nStandard Retrieval Context:\n{standard_results['context'][:1000]}... [truncated]\n\nStandard Retrieval Answer:\n{standard_results['response']}\n\nReranked Retrieval Context:\n{reranked_results['context'][:1000]}... [truncated]\n\nReranked Retrieval Answer:\n{reranked_results['response']}\"\"\"\n\n    # If a reference answer is provided, include it in the comparison text\n    if reference_answer:\n        comparison_text += f\"\"\"\n        \nReference Answer:\n{reference_answer}\"\"\"\n\n    # Create the user prompt for the AI evaluator\n    user_prompt = f\"\"\"\n{comparison_text}\n\nPlease evaluate which retrieval method provided:\n1. More relevant context\n2. More accurate answer\n3. More comprehensive answer\n4. Better overall performance\n\nProvide a detailed analysis with specific examples.\n\"\"\"\n    \n    # Generate the evaluation response using the specified model\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n        temperature=0,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ]\n    )\n    \n    # Return the evaluation output\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Classifying Queries by Type for Adaptive RAG in Python\nDESCRIPTION: Function to classify user queries into one of four categories (Factual, Analytical, Opinion, or Contextual) using a language model. This classification determines which retrieval strategy will be used in the adaptive RAG system.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/12_adaptive_rag.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef classify_query(query, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Classify a query into one of four categories: Factual, Analytical, Opinion, or Contextual.\n    \n    Args:\n        query (str): User query\n        model (str): LLM model to use\n        \n    Returns:\n        str: Query category\n    \"\"\"\n    # Define the system prompt to guide the AI's classification\n    system_prompt = \"\"\"You are an expert at classifying questions. \n        Classify the given query into exactly one of these categories:\n        - Factual: Queries seeking specific, verifiable information.\n        - Analytical: Queries requiring comprehensive analysis or explanation.\n        - Opinion: Queries about subjective matters or seeking diverse viewpoints.\n        - Contextual: Queries that depend on user-specific context.\n\n        Return ONLY the category name, without any explanation or additional text.\n    \"\"\"\n\n    # Create the user prompt with the query to be classified\n    user_prompt = f\"Classify this query: {query}\"\n    \n    # Generate the classification response from the AI model\n    response = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0\n    )\n    \n    # Extract and strip the category from the response\n    category = response.choices[0].message.content.strip()\n    \n    # Define the list of valid categories\n    valid_categories = [\"Factual\", \"Analytical\", \"Opinion\", \"Contextual\"]\n    \n    # Ensure the returned category is valid\n    for valid in valid_categories:\n        if valid in category:\n            return valid\n    \n    # Default to \"Factual\" if classification fails\n    return \"Factual\"\n```\n\n----------------------------------------\n\nTITLE: Response Evaluation System\nDESCRIPTION: Function to evaluate and compare responses from proposition-based and chunk-based approaches using an LLM evaluator.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/14_proposition_chunking.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_responses(query, prop_response, chunk_response, reference_answer=None):\n    \"\"\"\n    Evaluate and compare responses from both approaches.\n    \n    Args:\n        query (str): User query\n        prop_response (str): Response from proposition-based approach\n        chunk_response (str): Response from chunk-based approach\n        reference_answer (str, optional): Reference answer for comparison\n        \n    Returns:\n        str: Evaluation analysis\n    \"\"\"\n    # System prompt to instruct the AI on how to evaluate the responses\n    system_prompt = \"\"\"You are an expert evaluator of information retrieval systems. \n    Compare the two responses to the same query, one generated from proposition-based retrieval \n    and the other from chunk-based retrieval.\n\n    Evaluate them based on:\n    1. Accuracy: Which response provides more factually correct information?\n    2. Relevance: Which response better addresses the specific query?\n    3. Conciseness: Which response is more concise while maintaining completeness?\n    4. Clarity: Which response is easier to understand?\n\n    Be specific about the strengths and weaknesses of each approach.\"\"\"\n\n    # User prompt containing the query and the responses to be compared\n    user_prompt = f\"\"\"Query: {query}\n\n    Response from Proposition-Based Retrieval:\n    {prop_response}\n\n    Response from Chunk-Based Retrieval:\n    {chunk_response}\"\"\"\n\n    # If a reference answer is provided, include it in the user prompt for factual checking\n    if reference_answer:\n        user_prompt += f\"\"\"\n\n    Reference Answer (for factual checking):\n    {reference_answer}\"\"\"\n\n    # Add the final instruction to the user prompt\n    user_prompt += \"\"\"\n    Please provide a detailed comparison of these two responses, highlighting which approach performed better and why.\"\"\"\n\n    # Generate the evaluation analysis using the OpenAI client\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0\n    )\n    \n    # Return the generated evaluation analysis\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Expanding Context with Additional Chunks in Python\nDESCRIPTION: This function expands the context by retrieving additional chunks based on the query. It filters out duplicates from the current context and limits the number of new chunks added to a specified top_k value, ensuring a diverse and relevant context expansion.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndef expand_context(query: str, current_chunks: List[str], top_k: int = 3) -> List[str]:\n    \"\"\"\n    Expand the context by retrieving additional chunks.\n\n    Args:\n        query (str): The query text for which additional context is needed.\n        current_chunks (List[str]): The current list of context chunks.\n        top_k (int): The number of additional chunks to retrieve. Default is 3.\n\n    Returns:\n        List[str]: The expanded list of context chunks including new unique chunks.\n    \"\"\"\n    # Retrieve more chunks than currently available\n    additional_chunks = retrieve_relevant_chunks(query, top_k=top_k + len(current_chunks))\n    \n    # Filter out chunks that are already in the current context\n    new_chunks = []\n    for chunk in additional_chunks:\n        if chunk not in current_chunks:\n            new_chunks.append(chunk)\n    \n    # Add new unique chunks to the current context, limited to top_k\n    expanded_context = current_chunks + new_chunks[:top_k]\n    return expanded_context\n```\n\n----------------------------------------\n\nTITLE: Multi-Query Evaluation Runner\nDESCRIPTION: Function to run a complete evaluation using multiple test queries. Processes documents, creates vector store and evaluates both RAG approaches across all test queries.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/19_HyDE_rag.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef run_evaluation(pdf_path, test_queries, reference_answers=None, chunk_size=1000, chunk_overlap=200):\n    \"\"\"\n    Run a complete evaluation with multiple test queries.\n    \n    Args:\n        pdf_path (str): Path to the PDF document\n        test_queries (List[str]): List of test queries\n        reference_answers (List[str], optional): Reference answers for queries\n        chunk_size (int): Size of each chunk in characters\n        chunk_overlap (int): Overlap between chunks in characters\n        \n    Returns:\n        Dict: Evaluation results\n    \"\"\"\n    # Process document and create vector store\n    vector_store = process_document(pdf_path, chunk_size, chunk_overlap)\n    \n    results = []\n    \n    for i, query in enumerate(test_queries):\n        print(f\"\\n\\n===== Evaluating Query {i+1}/{len(test_queries)} =====\")\n        print(f\"Query: {query}\")\n        \n        # Get reference answer if available\n        reference = None\n        if reference_answers and i < len(reference_answers):\n            reference = reference_answers[i]\n        \n        # Compare approaches\n        result = compare_approaches(query, vector_store, reference)\n        results.append(result)\n    \n    # Generate overall analysis\n    overall_analysis = generate_overall_analysis(results)\n    \n    return {\n        \"results\": results,\n        \"overall_analysis\": overall_analysis\n    }\n```\n\n----------------------------------------\n\nTITLE: Generating Page Summaries for Hierarchical RAG in Python\nDESCRIPTION: This function generates concise summaries of page content using the OpenAI API. It's designed for hierarchical RAG systems, creating detailed summaries that capture main topics and key information while being more concise than the original text.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/18_hierarchy_rag.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef generate_page_summary(page_text):\n    \"\"\"\n    Generate a concise summary of a page.\n    \n    Args:\n        page_text (str): Text content of the page\n        \n    Returns:\n        str: Generated summary\n    \"\"\"\n    # Define the system prompt to instruct the summarization model\n    system_prompt = \"\"\"You are an expert summarization system.\n    Create a detailed summary of the provided text. \n    Focus on capturing the main topics, key information, and important facts.\n    Your summary should be comprehensive enough to understand what the page contains\n    but more concise than the original.\"\"\"\n\n    # Truncate input text if it exceeds the maximum token limit\n    max_tokens = 6000\n    truncated_text = page_text[:max_tokens] if len(page_text) > max_tokens else page_text\n\n    # Make a request to the OpenAI API to generate the summary\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",  # Specify the model to use\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n            {\"role\": \"user\", \"content\": f\"Please summarize this text:\\n\\n{truncated_text}\"}  # User message with the text to summarize\n        ],\n        temperature=0.3  # Set the temperature for response generation\n    )\n    \n    # Return the generated summary content\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Comparing Simple RAG vs RL-Enhanced RAG in Python\nDESCRIPTION: This function compares the outputs of simple RAG versus RL-enhanced RAG for a given query. It generates responses using both approaches, calculates similarity scores, trains the RL model, and visualizes the results.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndef compare_rag_approaches(query_text: str, ground_truth: str) -> Tuple[str, str, float, float]:\n    \"\"\"\n    Compare the outputs of simple RAG versus RL-enhanced RAG.\n\n    Args:\n        query_text (str): The input query text for the RAG pipeline.\n        ground_truth (str): The expected correct answer for the query.\n\n    Returns:\n        Tuple[str, str, float, float]: A tuple containing:\n            - simple_response (str): The response generated by the simple RAG pipeline.\n            - best_rl_response (str): The best response generated by the RL-enhanced RAG pipeline.\n            - simple_similarity (float): The similarity score of the simple RAG response to the ground truth.\n            - rl_similarity (float): The similarity score of the RL-enhanced RAG response to the ground truth.\n    \"\"\"\n    print(\"=\" * 80)\n    print(f\"Query: {query_text}\")\n    print(\"=\" * 80)\n    \n    # Step 1: Generate a response using the simple RAG pipeline\n    # The basic RAG pipeline retrieves relevant chunks and generates a response without reinforcement learning.\n    simple_response: str = basic_rag_pipeline(query_text)\n    # Calculate the similarity score between the simple RAG response and the ground truth.\n    simple_similarity: float = calculate_reward(simple_response, ground_truth)\n    \n    print(\"\\nSimple RAG Output:\")\n    print(\"-\" * 40)\n    print(simple_response)\n    print(f\"Similarity to ground truth: {simple_similarity:.4f}\")\n    \n    # Step 2: Train the RL-enhanced RAG model\n    print(\"\\nTraining RL-enhanced RAG model...\")\n    # Initialize training parameters (e.g., learning rate, number of episodes, discount factor).\n    params: Dict[str, float | int] = initialize_training_params()\n    # Set the number of episodes to a smaller value for demonstration purposes.\n    params[\"num_episodes\"] = 5\n    \n    # Run the training loop for the RL-enhanced RAG model.\n    # This loop trains the model to optimize its responses using reinforcement learning.\n    _, rewards_history, actions_history, best_rl_response = training_loop(\n        query_text, ground_truth, params\n    )\n    \n    # If no response was generated during training, generate one using the current query and context.\n    if best_rl_response is None:\n        # Retrieve relevant chunks for the query.\n        context_chunks: List[str] = retrieve_relevant_chunks(query_text)\n        # Construct a prompt using the query and retrieved context.\n        prompt: str = construct_prompt(query_text, context_chunks)\n        # Generate a response using the language model.\n        best_rl_response: str = generate_response(prompt)\n    \n    # Calculate the similarity score between the RL-enhanced RAG response and the ground truth.\n    rl_similarity: float = calculate_reward(best_rl_response, ground_truth)\n    \n    print(\"\\nRL-enhanced RAG Output:\")\n    print(\"-\" * 40)\n    print(best_rl_response)\n    print(f\"Similarity to ground truth: {rl_similarity:.4f}\")\n    \n    # Step 3: Evaluate and compare the results\n    # Calculate the improvement in similarity score achieved by the RL-enhanced RAG model.\n    improvement: float = rl_similarity - simple_similarity\n    \n    print(\"\\nEvaluation Results:\")\n    print(\"-\" * 40)\n    print(f\"Simple RAG similarity to ground truth: {simple_similarity:.4f}\")\n    print(f\"RL-enhanced RAG similarity to ground truth: {rl_similarity:.4f}\")\n    print(f\"Improvement: {improvement * 100:.2f}%\")\n    \n    # Step 4: Plot the reward history (if there are enough episodes and matplotlib is available)\n    if len(rewards_history) > 1:\n        try:\n            import matplotlib.pyplot as plt\n            # Create a plot to visualize the reward history during RL training.\n            plt.figure(figsize=(10, 6))\n            plt.plot(rewards_history)\n            plt.title('Reward History During RL Training')\n            plt.xlabel('Episode')\n            plt.ylabel('Reward')\n            plt.grid(True)\n            plt.show()\n        except ImportError:\n            # If matplotlib is not available, print a message instead of plotting.\n            print(\"Matplotlib not available for plotting rewards\")\n    \n    # Return the results: responses and similarity scores for both approaches.\n    return simple_response, best_rl_response, simple_similarity, rl_similarity\n```\n\n----------------------------------------\n\nTITLE: Comparing Responses in Adaptive RAG Evaluation\nDESCRIPTION: This function compares standard and adaptive responses against reference answers. It uses an AI-based evaluation system to analyze the strengths and weaknesses of each approach, considering factors like accuracy, relevance, and comprehensiveness.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/12_adaptive_rag.ipynb#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef compare_responses(results):\n    \"\"\"\n    Compare standard and adaptive responses against reference answers.\n    \n    Args:\n        results (List[Dict]): Results containing both types of responses\n        \n    Returns:\n        str: Comparison analysis\n    \"\"\"\n    # Define the system prompt to guide the AI in comparing responses\n    comparison_prompt = \"\"\"You are an expert evaluator of information retrieval systems.\n    Compare the standard retrieval and adaptive retrieval responses for each query.\n    Consider factors like accuracy, relevance, comprehensiveness, and alignment with the reference answer.\n    Provide a detailed analysis of the strengths and weaknesses of each approach.\"\"\"\n    \n    # Initialize the comparison text with a header\n    comparison_text = \"# Evaluation of Standard vs. Adaptive Retrieval\\n\\n\"\n    \n    # Iterate through each result to compare responses\n    for i, result in enumerate(results):\n        # Skip if there is no reference answer for the query\n        if \"reference_answer\" not in result:\n            continue\n            \n        # Add query details to the comparison text\n        comparison_text += f\"## Query {i+1}: {result['query']}\\n\"\n        comparison_text += f\"*Query Type: {result['query_type']}*\\n\\n\"\n        comparison_text += f\"**Reference Answer:**\\n{result['reference_answer']}\\n\\n\"\n        \n        # Add standard retrieval response to the comparison text\n        comparison_text += f\"**Standard Retrieval Response:**\\n{result['standard_retrieval']['response']}\\n\\n\"\n        \n        # Add adaptive retrieval response to the comparison text\n        comparison_text += f\"**Adaptive Retrieval Response:**\\n{result['adaptive_retrieval']['response']}\\n\\n\"\n        \n        # Create the user prompt for the AI to compare the responses\n        user_prompt = f\"\"\"\n        Reference Answer: {result['reference_answer']}\n        \n        Standard Retrieval Response: {result['standard_retrieval']['response']}\n        \n        Adaptive Retrieval Response: {result['adaptive_retrieval']['response']}\n        \n        Provide a detailed comparison of the two responses.\n        \"\"\"\n        \n        # Generate the comparison analysis using the OpenAI client\n        response = client.chat.completions.create(\n            model=\"meta-llama/Llama-3.2-3B-Instruct\",\n            messages=[\n                {\"role\": \"system\", \"content\": comparison_prompt},\n                {\"role\": \"user\", \"content\": user_prompt}\n            ],\n            temperature=0.2\n        )\n        \n        # Add the AI's comparison analysis to the comparison text\n        comparison_text += f\"**Comparison Analysis:**\\n{response.choices[0].message.content}\\n\\n\"\n    \n    return comparison_text  # Return the complete comparison analysis\n```\n\n----------------------------------------\n\nTITLE: Implementing a Simple Multi-Modal Vector Store in Python\nDESCRIPTION: This class implements a simple vector store for multi-modal content. It provides methods to add individual and multiple items, and perform similarity search using cosine similarity. The store maintains separate lists for vectors, contents, and metadata.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/15_multimodel_rag.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass MultiModalVectorStore:\n    \"\"\"\n    A simple vector store implementation for multi-modal content.\n    \"\"\"\n    def __init__(self):\n        # Initialize lists to store vectors, contents, and metadata\n        self.vectors = []\n        self.contents = []\n        self.metadata = []\n    \n    def add_item(self, content, embedding, metadata=None):\n        \"\"\"\n        Add an item to the vector store.\n        \n        Args:\n            content (str): The content (text or image caption)\n            embedding (List[float]): The embedding vector\n            metadata (Dict, optional): Additional metadata\n        \"\"\"\n        # Append the embedding vector, content, and metadata to their respective lists\n        self.vectors.append(np.array(embedding))\n        self.contents.append(content)\n        self.metadata.append(metadata or {})\n    \n    def add_items(self, items, embeddings):\n        \"\"\"\n        Add multiple items to the vector store.\n        \n        Args:\n            items (List[Dict]): List of content items\n            embeddings (List[List[float]]): List of embedding vectors\n        \"\"\"\n        # Loop through items and embeddings and add each to the vector store\n        for item, embedding in zip(items, embeddings):\n            self.add_item(\n                content=item[\"content\"],\n                embedding=embedding,\n                metadata=item.get(\"metadata\", {})\n            )\n    \n    def similarity_search(self, query_embedding, k=5):\n        \"\"\"\n        Find the most similar items to a query embedding.\n        \n        Args:\n            query_embedding (List[float]): Query embedding vector\n            k (int): Number of results to return\n            \n        Returns:\n            List[Dict]: Top k most similar items\n        \"\"\"\n        # Return an empty list if there are no vectors in the store\n        if not self.vectors:\n            return []\n        \n        # Convert query embedding to numpy array\n        query_vector = np.array(query_embedding)\n        \n        # Calculate similarities using cosine similarity\n        similarities = []\n        for i, vector in enumerate(self.vectors):\n            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n            similarities.append((i, similarity))\n        \n        # Sort by similarity (descending)\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        \n        # Return top k results\n        results = []\n        for i in range(min(k, len(similarities))):\n            idx, score = similarities[i]\n            results.append({\n                \"content\": self.contents[idx],\n                \"metadata\": self.metadata[idx],\n                \"similarity\": float(score)  # Convert to float for JSON serialization\n            })\n        \n        return results\n```\n\n----------------------------------------\n\nTITLE: Context-Aware Document Relevance Scoring in Python\nDESCRIPTION: A function that scores document relevance considering both the query and user context. It uses an LLM to evaluate how well the document addresses the query within the given context, returning a 0-10 score.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/12_adaptive_rag.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef score_document_context_relevance(query, context, document, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Score document relevance considering both query and context.\n    \n    Args:\n        query (str): User query\n        context (str): User context\n        document (str): Document text\n        model (str): LLM model\n        \n    Returns:\n        float: Relevance score from 0-10\n    \"\"\"\n    # System prompt to instruct the model on how to rate relevance considering context\n    system_prompt = \"\"\"You are an expert at evaluating document relevance considering context.\n        Rate the document on a scale from 0 to 10 based on how well it addresses the query\n        when considering the provided context, where:\n        0 = Completely irrelevant\n        10 = Perfectly addresses the query in the given context\n\n        Return ONLY a numerical score between 0 and 10, nothing else.\n    \"\"\"\n\n    # Truncate document if it's too long\n    doc_preview = document[:1500] + \"...\" if len(document) > 1500 else document\n    \n    # User prompt containing the query, context, and document preview\n    user_prompt = f\"\"\"\n    Query: {query}\n    Context: {context}\n\n    Document: {doc_preview}\n\n    Relevance score considering context (0-10):\n    \"\"\"\n    \n    # Generate response from the model\n    response = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0\n    )\n    \n    # Extract the score from the model's response\n    score_text = response.choices[0].message.content.strip()\n    \n    # Extract numeric score using regex\n    match = re.search(r'(\\d+(\\.\\d+)?)', score_text)\n    if match:\n        score = float(match.group(1))\n        return min(10, max(0, score))  # Ensure score is within 0-10\n    else:\n        # Default score if extraction fails\n        return 5.0\n```\n\n----------------------------------------\n\nTITLE: Filtering Context Chunks Based on Query Relevance in Python\nDESCRIPTION: Function that filters context chunks by calculating relevance scores using embeddings and cosine similarity. It keeps the top 5 most relevant chunks for a given query.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndef filter_context(query: str, context_chunks: List[str]) -> List[str]:\n    \"\"\"\n    Filter the context to keep only the most relevant chunks.\n\n    Args:\n        query (str): The query text for which relevance is calculated.\n        context_chunks (List[str]): The list of context chunks to filter.\n\n    Returns:\n        List[str]: A filtered list of the most relevant context chunks.\n    \"\"\"\n    if not context_chunks:\n        return []\n        \n    # Generate embeddings for the query and each chunk\n    query_embedding = generate_embeddings([query])[0]\n    chunk_embeddings = [generate_embeddings([chunk])[0] for chunk in context_chunks]\n    \n    # Calculate relevance scores for each chunk\n    relevance_scores = []\n    for chunk_embedding in chunk_embeddings:\n        score = cosine_similarity(query_embedding, chunk_embedding)\n        relevance_scores.append(score)\n    \n    # Sort chunks by relevance scores in descending order\n    sorted_chunks = [x for _, x in sorted(zip(relevance_scores, context_chunks), reverse=True)]\n    \n    # Keep the top 5 most relevant chunks or fewer if less than 5 are available\n    filtered_chunks = sorted_chunks[:min(5, len(sorted_chunks))]\n    \n    return filtered_chunks\n```\n\n----------------------------------------\n\nTITLE: Evaluating RAG Response Quality Using LLM-based Comparison in Python\nDESCRIPTION: This function uses a language model to compare responses from hierarchical and standard RAG approaches. It evaluates responses based on accuracy, comprehensiveness, coherence, and use of page references.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/18_hierarchy_rag.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef compare_responses(query, hierarchical_response, standard_response, reference=None):\n    \"\"\"\n    Compare responses from hierarchical and standard RAG.\n    \n    Args:\n        query (str): User query\n        hierarchical_response (str): Response from hierarchical RAG\n        standard_response (str): Response from standard RAG\n        reference (str, optional): Reference answer\n        \n    Returns:\n        str: Comparison analysis\n    \"\"\"\n    # Define the system prompt to instruct the model on how to evaluate the responses\n    system_prompt = \"\"\"You are an expert evaluator of information retrieval systems. \nCompare the two responses to the same query, one generated using hierarchical retrieval\nand the other using standard retrieval.\n\nEvaluate them based on:\n1. Accuracy: Which response provides more factually correct information?\n2. Comprehensiveness: Which response better covers all aspects of the query?\n3. Coherence: Which response has better logical flow and organization?\n4. Page References: Does either response make better use of page references?\n\nBe specific in your analysis of the strengths and weaknesses of each approach.\"\"\"\n\n    # Create the user prompt with the query and both responses\n    user_prompt = f\"\"\"Query: {query}\n\nResponse from Hierarchical RAG:\n{hierarchical_response}\n\nResponse from Standard RAG:\n{standard_response}\"\"\"\n\n    # If a reference answer is provided, include it in the user prompt\n    if reference:\n        user_prompt += f\"\"\"\n\nReference Answer:\n{reference}\"\"\"\n\n    # Add the final instruction to the user prompt\n    user_prompt += \"\"\"\n\nPlease provide a detailed comparison of these two responses, highlighting which approach performed better and why.\"\"\"\n\n    # Make a request to the OpenAI API to generate the comparison analysis\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n            {\"role\": \"user\", \"content\": user_prompt}  # User message with the query and responses\n        ],\n        temperature=0  # Set the temperature for response generation\n    )\n    \n    # Return the generated comparison analysis\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Visualizing Text Compression Results in Python\nDESCRIPTION: Function that creates a detailed visualization of text compression results, showing original vs compressed chunks, compression ratios, and summary statistics. Takes evaluation results as input and displays formatted comparisons for each compression technique, including length metrics and average compression ratios.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/10_contextual_compression.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef visualize_compression_results(evaluation_results):\n    \"\"\"\n    Visualize the results of different compression techniques.\n    \n    Args:\n        evaluation_results (Dict): Results from evaluate_compression function\n    \"\"\"\n    # Extract the query and standard chunks from the evaluation results\n    query = evaluation_results[\"query\"]\n    standard_chunks = evaluation_results[\"standard_result\"][\"chunks\"]\n    \n    # Print the query\n    print(f\"Query: {query}\")\n    print(\"\\n\" + \"=\"*80 + \"\\n\")\n    \n    # Get a sample chunk to visualize (using the first chunk)\n    original_chunk = standard_chunks[0]\n    \n    # Iterate over each compression type and show a comparison\n    for comp_type in evaluation_results[\"compression_results\"].keys():\n        compressed_chunks = evaluation_results[\"compression_results\"][comp_type][\"compressed_chunks\"]\n        compression_ratios = evaluation_results[\"compression_results\"][comp_type][\"compression_ratios\"]\n        \n        # Get the corresponding compressed chunk and its compression ratio\n        compressed_chunk = compressed_chunks[0]\n        compression_ratio = compression_ratios[0]\n        \n        print(f\"\\n=== {comp_type.upper()} COMPRESSION EXAMPLE ===\\n\")\n        \n        # Show the original chunk (truncated if too long)\n        print(\"ORIGINAL CHUNK:\")\n        print(\"-\" * 40)\n        if len(original_chunk) > 800:\n            print(original_chunk[:800] + \"... [truncated]\")\n        else:\n            print(original_chunk)\n        print(\"-\" * 40)\n        print(f\"Length: {len(original_chunk)} characters\\n\")\n        \n        # Show the compressed chunk\n        print(\"COMPRESSED CHUNK:\")\n        print(\"-\" * 40)\n        print(compressed_chunk)\n        print(\"-\" * 40)\n        print(f\"Length: {len(compressed_chunk)} characters\")\n        print(f\"Compression ratio: {compression_ratio:.2f}%\\n\")\n        \n        # Show overall statistics for this compression type\n        avg_ratio = sum(compression_ratios) / len(compression_ratios)\n        print(f\"Average compression across all chunks: {avg_ratio:.2f}%\")\n        print(f\"Total context length reduction: {evaluation_results['metrics'][comp_type]['avg_compression_ratio']}\")\n        print(\"=\" * 80)\n    \n    # Show a summary table of compression techniques\n    print(\"\\n=== COMPRESSION SUMMARY ===\\n\")\n    print(f\"{'Technique':<15} {'Avg Ratio':<15} {'Context Length':<15} {'Original Length':<15}\")\n    print(\"-\" * 60)\n    \n    # Print the metrics for each compression type\n    for comp_type, metrics in evaluation_results[\"metrics\"].items():\n        print(f\"{comp_type:<15} {metrics['avg_compression_ratio']:<15} {metrics['total_context_length']:<15} {metrics['original_context_length']:<15}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Response Generation with Context in Python\nDESCRIPTION: This function generates a comprehensive response to a user query using retrieved context. It uses the OpenAI API with a specified model, where the system prompt instructs the AI to answer based only on the provided context and acknowledge when information is insufficient.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/8_reranker.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef generate_response(query, context, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Generates a response based on the query and context.\n    \n    Args:\n        query (str): User query\n        context (str): Retrieved context\n        model (str): Model to use for response generation\n        \n    Returns:\n        str: Generated response\n    \"\"\"\n    # Define the system prompt to guide the AI's behavior\n    system_prompt = \"You are a helpful AI assistant. Answer the user's question based only on the provided context. If you cannot find the answer in the context, state that you don't have enough information.\"\n    \n    # Create the user prompt by combining the context and query\n    user_prompt = f\"\"\"\n        Context:\n        {context}\n\n        Question: {query}\n\n        Please provide a comprehensive answer based only on the context above.\n    \"\"\"\n    \n    # Generate the response using the specified model\n    response = client.chat.completions.create(\n        model=model,\n        temperature=0,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ]\n    )\n    \n    # Return the generated response content\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Comparing RAG Responses Function\nDESCRIPTION: Implements response comparison logic between Self-RAG and traditional RAG outputs using an LLM evaluator. Analyzes responses based on relevance, factual correctness, completeness, and focus.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/13_self_rag.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef compare_responses(query, self_rag_response, trad_rag_response, reference=None):\n    \"\"\"\n    Compare responses from Self-RAG and traditional RAG.\n    \n    Args:\n        query (str): User query\n        self_rag_response (str): Response from Self-RAG\n        trad_rag_response (str): Response from traditional RAG\n        reference (str, optional): Reference answer\n        \n    Returns:\n        str: Comparison analysis\n    \"\"\"\n    system_prompt = \"\"\"You are an expert evaluator of RAG systems. Your task is to compare responses from two different RAG approaches:\n1. Self-RAG: A dynamic approach that decides if retrieval is needed and evaluates information relevance and response quality\n2. Traditional RAG: Always retrieves documents and uses them to generate a response\n\nCompare the responses based on:\n- Relevance to the query\n- Factual correctness\n- Completeness and informativeness\n- Conciseness and focus\"\"\"\n\n    user_prompt = f\"\"\"Query: {query}\n\nResponse from Self-RAG:\n{self_rag_response}\n\nResponse from Traditional RAG:\n{trad_rag_response}\n\"\"\"\n\n    if reference:\n        user_prompt += f\"\"\"\nReference Answer (for factual checking):\n{reference}\n\"\"\"\n\n    user_prompt += \"\"\"\nCompare these responses and explain which one is better and why.\nFocus on accuracy, relevance, completeness, and quality.\n\"\"\"\n\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",  # Using a stronger model for evaluation\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0\n    )\n    \n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Evaluating RAG Responses Using LLM in Python\nDESCRIPTION: This function evaluates responses from different RAG methods using a language model. It generates a detailed comparison of the responses based on relevance, factual correctness, comprehensiveness, and clarity. It uses the meta-llama/Llama-3.2-3B-Instruct model for evaluation.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/16_fusion_rag.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_responses(query, vector_response, bm25_response, fusion_response, reference_answer=None):\n    \"\"\"\n    Evaluate the responses from different retrieval methods.\n    \n    Args:\n        query (str): User query\n        vector_response (str): Response from vector-only RAG\n        bm25_response (str): Response from BM25-only RAG\n        fusion_response (str): Response from fusion RAG\n        reference_answer (str, optional): Reference answer\n        \n    Returns:\n        str: Evaluation of responses\n    \"\"\"\n    # System prompt for the evaluator to guide the evaluation process\n    system_prompt = \"\"\"You are an expert evaluator of RAG systems. Compare responses from three different retrieval approaches:\n    1. Vector-based retrieval: Uses semantic similarity for document retrieval\n    2. BM25 keyword retrieval: Uses keyword matching for document retrieval\n    3. Fusion retrieval: Combines both vector and keyword approaches\n\n    Evaluate the responses based on:\n    - Relevance to the query\n    - Factual correctness\n    - Comprehensiveness\n    - Clarity and coherence\"\"\"\n\n    # User prompt containing the query and responses\n    user_prompt = f\"\"\"Query: {query}\n\n    Vector-based response:\n    {vector_response}\n\n    BM25 keyword response:\n    {bm25_response}\n\n    Fusion response:\n    {fusion_response}\n    \"\"\"\n\n    # Add reference answer to the prompt if provided\n    if reference_answer:\n        user_prompt += f\"\"\"\n            Reference answer:\n            {reference_answer}\n        \"\"\"\n\n    # Add instructions for detailed comparison to the user prompt\n    user_prompt += \"\"\"\n    Please provide a detailed comparison of these three responses. Which approach performed best for this query and why?\n    Be specific about the strengths and weaknesses of each approach for this particular query.\n    \"\"\"\n\n    # Generate the evaluation using meta-llama/Llama-3.2-3B-Instruct\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",  # Specify the model to use\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the evaluator\n            {\"role\": \"user\", \"content\": user_prompt}  # User message with query and responses\n        ],\n        temperature=0  # Set the temperature for response generation\n    )\n    \n    # Return the generated evaluation content\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Querying with Context-Enriched Retrieval in Python\nDESCRIPTION: Code that demonstrates context-enriched retrieval by loading a validation dataset, extracting a query, and retrieving relevant text chunks with context. It shows how to include neighboring chunks around the most relevant one for improved completeness.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/4_context_enriched_rag.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Load the validation dataset from a JSON file\nwith open('data/val.json') as f:\n    data = json.load(f)\n\n# Extract the first question from the dataset to use as our query\nquery = data[0]['question']\n\n# Retrieve the most relevant chunk and its neighboring chunks for context\n# Parameters:\n# - query: The question we're searching for\n# - text_chunks: Our text chunks extracted from the PDF\n# - response.data: The embeddings of our text chunks\n# - k=1: Return the top match\n# - context_size=1: Include 1 chunk before and after the top match for context\ntop_chunks = context_enriched_search(query, text_chunks, response.data, k=1, context_size=1)\n\n# Print the query for reference\nprint(\"Query:\", query)\n# Print each retrieved chunk with a heading and separator\nfor i, chunk in enumerate(top_chunks):\n    print(f\"Context {i + 1}:\\n{chunk}\\n=====================================\")\n```\n\n----------------------------------------\n\nTITLE: Running RAG Evaluation Example\nDESCRIPTION: Demonstrates the usage of the RAG evaluation framework with sample test queries and reference answers. Shows how to set up and execute the comparison between Self-RAG and traditional RAG approaches.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/13_self_rag.ipynb#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# Path to the AI information document\npdf_path = \"data/AI_Information.pdf\"\n\n# Define test queries covering different query types to test Self-RAG's adaptive retrieval\ntest_queries = [\n    \"What are the main ethical concerns in AI development?\",        # Document-focused query\n    # \"How does explainable AI improve trust in AI systems?\",         # Document-focused query\n    # \"Write a poem about artificial intelligence\",                   # Creative query, doesn't need retrieval\n    # \"Will superintelligent AI lead to human obsolescence?\"          # Speculative query, partial retrieval needed\n]\n\n# Reference answers for more objective evaluation\nreference_answers = [\n    \"The main ethical concerns in AI development include bias and fairness, privacy, transparency, accountability, safety, and the potential for misuse or harmful applications.\",\n    # \"Explainable AI improves trust by making AI decision-making processes transparent and understandable to users, helping them verify fairness, identify potential biases, and better understand AI limitations.\",\n    # \"A quality poem about artificial intelligence should creatively explore themes of AI's capabilities, limitations, relationship with humanity, potential futures, or philosophical questions about consciousness and intelligence.\",\n    # \"Views on superintelligent AI's impact on human relevance vary widely. Some experts warn of potential risks if AI surpasses human capabilities across domains, possibly leading to economic displacement or loss of human agency. Others argue humans will remain relevant through complementary skills, emotional intelligence, and by defining AI's purpose. Most experts agree that thoughtful governance and human-centered design are essential regardless of the outcome.\"\n]\n\n# Run the evaluation comparing Self-RAG with traditional RAG approaches\nevaluation_results = evaluate_rag_approaches(\n    pdf_path=pdf_path,                  # Source document containing AI information\n    test_queries=test_queries,          # List of AI-related test queries\n    reference_answers=reference_answers  # Ground truth answers for evaluation\n)\n\n# Print the overall comparative analysis\nprint(\"\\n=== OVERALL ANALYSIS ===\\n\")\nprint(evaluation_results[\"overall_analysis\"])\n```\n\n----------------------------------------\n\nTITLE: Evaluating Document Relevance for RAG in Python\nDESCRIPTION: This function evaluates whether a given context document is relevant to a user query. It uses the Llama-3.2-3B-Instruct model to determine if the document contains information that would be helpful in answering the query, returning either 'relevant' or 'irrelevant'.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/13_self_rag.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_relevance(query, context):\n    \"\"\"\n    Evaluates the relevance of a context to the query.\n    \n    Args:\n        query (str): User query\n        context (str): Context text\n        \n    Returns:\n        str: 'relevant' or 'irrelevant'\n    \"\"\"\n    # System prompt to instruct the AI on how to determine document relevance\n    system_prompt = \"\"\"You are an AI assistant that determines if a document is relevant to a query.\n    Consider whether the document contains information that would be helpful in answering the query.\n    Answer with ONLY \"Relevant\" or \"Irrelevant\".\"\"\"\n\n    # Truncate context if it is too long to avoid exceeding token limits\n    max_context_length = 2000\n    if len(context) > max_context_length:\n        context = context[:max_context_length] + \"... [truncated]\"\n\n    # User prompt containing the query and the document content\n    user_prompt = f\"\"\"Query: {query}\n    Document content:\n    {context}\n\n    Is this document relevant to the query? Answer with ONLY \"Relevant\" or \"Irrelevant\".\n    \"\"\"\n    \n    # Generate response from the model\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0\n    )\n    \n    # Extract the answer from the model's response and convert to lowercase\n    answer = response.choices[0].message.content.strip().lower()\n    \n    return answer  # Return the relevance evaluation\n```\n\n----------------------------------------\n\nTITLE: Creating Embeddings for Text Chunks with OpenAI in Python\nDESCRIPTION: Function that converts text into numerical vectors (embeddings) using the OpenAI API. These embeddings represent the semantic meaning of text and enable efficient similarity comparisons for retrieval.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/4_context_enriched_rag.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n    \"\"\"\n    Creates embeddings for the given text using the specified OpenAI model.\n\n    Args:\n    text (str): The input text for which embeddings are to be created.\n    model (str): The model to be used for creating embeddings. Default is \"BAAI/bge-en-icl\".\n\n    Returns:\n    dict: The response from the OpenAI API containing the embeddings.\n    \"\"\"\n    # Create embeddings for the input text using the specified model\n    response = client.embeddings.create(\n        model=model,\n        input=text\n    )\n\n    return response  # Return the response containing the embeddings\n\n# Create embeddings for the text chunks\nresponse = create_embeddings(text_chunks)\n```\n\n----------------------------------------\n\nTITLE: Comparing CRAG and Standard RAG Responses in Python\nDESCRIPTION: This function compares CRAG and standard RAG responses using GPT-4. It evaluates accuracy, relevance, completeness, clarity, and source attribution quality, providing a detailed analysis of which approach performed better for the specific query.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/20_crag.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef compare_responses(query, crag_response, standard_response, reference_answer=None):\n    \"\"\"\n    Compare CRAG and standard RAG responses.\n    \n    Args:\n        query (str): User query\n        crag_response (str): CRAG response\n        standard_response (str): Standard RAG response\n        reference_answer (str, optional): Reference answer\n        \n    Returns:\n        str: Comparison analysis\n    \"\"\"\n    # System prompt for comparing the two approaches\n    system_prompt = \"\"\"\n    You are an expert evaluator comparing two response generation approaches:\n    \n    1. CRAG (Corrective RAG): A system that evaluates document relevance and dynamically switches to web search when needed.\n    2. Standard RAG: A system that directly retrieves documents based on embedding similarity and uses them for response generation.\n    \n    Compare the responses from these two systems based on:\n    - Accuracy and factual correctness\n    - Relevance to the query\n    - Completeness of the answer\n    - Clarity and organization\n    - Source attribution quality\n    \n    Explain which approach performed better for this specific query and why.\n    \"\"\"\n    \n    # User prompt with the query and responses to be compared\n    user_prompt = f\"\"\"\n    Query: {query}\n    \n    CRAG Response:\n    {crag_response}\n    \n    Standard RAG Response:\n    {standard_response}\n    \"\"\"\n    \n    # Include reference answer in the prompt if provided\n    if reference_answer:\n        user_prompt += f\"\"\"\n    Reference Answer:\n    {reference_answer}\n    \"\"\"\n    \n    try:\n        # Request comparison from the GPT-4 model\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt}\n            ],\n            temperature=0\n        )\n        \n        # Return the comparison analysis\n        return response.choices[0].message.content.strip()\n    except Exception as e:\n        # Handle any errors during the comparison process\n        print(f\"Error comparing responses: {e}\")\n        return f\"Error comparing responses: {str(e)}\"\n```\n\n----------------------------------------\n\nTITLE: Implementing BM25-Only RAG in Python\nDESCRIPTION: This function performs RAG using only BM25-based retrieval. It uses a BM25 index to retrieve relevant documents, formats the context, and generates a response. It requires a list of text chunks and a BM25 index, and uses a specified number of documents (k) for retrieval.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/16_fusion_rag.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef bm25_only_rag(query, chunks, bm25_index, k=5):\n    \"\"\"\n    Answer a query using only BM25-based RAG.\n    \n    Args:\n        query (str): User query\n        chunks (List[Dict]): Text chunks\n        bm25_index (BM25Okapi): BM25 index\n        k (int): Number of documents to retrieve\n        \n    Returns:\n        Dict: Query results\n    \"\"\"\n    # Retrieve documents using BM25 search\n    retrieved_docs = bm25_search(bm25_index, chunks, query, k=k)\n    \n    # Format the context from the retrieved documents by joining their text with separators\n    context = \"\\n\\n---\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n    \n    # Generate a response based on the query and the formatted context\n    response = generate_response(query, context)\n    \n    # Return the query, retrieved documents, and the generated response\n    return {\n        \"query\": query,\n        \"retrieved_documents\": retrieved_docs,\n        \"response\": response\n    }\n```\n\n----------------------------------------\n\nTITLE: Running a Query on Extracted Chunks in RAG\nDESCRIPTION: This snippet demonstrates how to run a query on the extracted and embedded chunks. It loads validation data, performs semantic search to retrieve the top 2 most relevant chunks, and prints the results. This is a crucial step in the RAG process to find relevant information for answering queries.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/5_contextual_chunk_headers_rag.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Load validation data\nwith open('data/val.json') as f:\n    data = json.load(f)\n\nquery = data[0]['question']\n\n# Retrieve the top 2 most relevant text chunks\ntop_chunks = semantic_search(query, embeddings, k=2)\n\n# Print the results\nprint(\"Query:\", query)\nfor i, chunk in enumerate(top_chunks):\n    print(f\"Header {i+1}: {chunk['header']}\")\n    print(f\"Content:\\n{chunk['text']}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Semantic Search Implementation in Python\nDESCRIPTION: Function to perform semantic search on the vector store using query embeddings. Returns the top k most relevant items based on similarity scores.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/6_doc_augmentation_rag.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef semantic_search(query, vector_store, k=5):\n    \"\"\"\n    Performs semantic search using the query and vector store.\n\n    Args:\n    query (str): The search query.\n    vector_store (SimpleVectorStore): The vector store to search in.\n    k (int): Number of results to return.\n\n    Returns:\n    List[Dict]: Top k most relevant items.\n    \"\"\"\n    # Create embedding for the query\n    query_embedding_response = create_embeddings(query)\n    query_embedding = query_embedding_response.data[0].embedding\n    \n    # Search the vector store\n    results = vector_store.similarity_search(query_embedding, k=k)\n    \n    return results\n```\n\n----------------------------------------\n\nTITLE: Implementing Query Rewriting Function for RAG Systems in Python\nDESCRIPTION: Defines a function to rewrite a user query to make it more specific and detailed, enhancing retrieval precision. The function uses the OpenAI API to generate the rewritten query based on a system prompt that guides the AI assistant to improve search queries.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/7_query_transform.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef rewrite_query(original_query, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Rewrites a query to make it more specific and detailed for better retrieval.\n    \n    Args:\n        original_query (str): The original user query\n        model (str): The model to use for query rewriting\n        \n    Returns:\n        str: The rewritten query\n    \"\"\"\n    # Define the system prompt to guide the AI assistant's behavior\n    system_prompt = \"You are an AI assistant specialized in improving search queries. Your task is to rewrite user queries to be more specific, detailed, and likely to retrieve relevant information.\"\n    \n    # Define the user prompt with the original query to be rewritten\n    user_prompt = f\"\"\"\n    Rewrite the following query to make it more specific and detailed. Include relevant terms and concepts that might help in retrieving accurate information.\n    \n    Original query: {original_query}\n    \n    Rewritten query:\n    \"\"\"\n    \n    # Generate the rewritten query using the specified model\n    response = client.chat.completions.create(\n        model=model,\n        temperature=0.0,  # Low temperature for deterministic output\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ]\n    )\n    \n    # Return the rewritten query, stripping any leading/trailing whitespace\n    return response.choices[0].message.content.strip()\n```\n\n----------------------------------------\n\nTITLE: Evaluating AI Response Quality in Python\nDESCRIPTION: Compares an AI-generated response against an ideal answer using another AI model. The evaluation system assigns scores of 0 (incorrect), 0.5 (partially correct), or 1 (very close to ideal), based on the alignment between the generated response and the expected answer.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/2_semantic_chunking.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Define the system prompt for the evaluation system\nevaluate_system_prompt = \"You are an intelligent evaluation system tasked with assessing the AI assistant's responses. If the AI assistant's response is very close to the true response, assign a score of 1. If the response is incorrect or unsatisfactory in relation to the true response, assign a score of 0. If the response is partially aligned with the true response, assign a score of 0.5.\"\n\n# Create the evaluation prompt by combining the user query, AI response, true response, and evaluation system prompt\nevaluation_prompt = f\"User Query: {query}\\nAI Response:\\n{ai_response.choices[0].message.content}\\nTrue Response: {data[0]['ideal_answer']}\\n{evaluate_system_prompt}\"\n\n# Generate the evaluation response using the evaluation system prompt and evaluation prompt\nevaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n\n# Print the evaluation response\nprint(evaluation_response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Running Fusion Retrieval Evaluation in Python\nDESCRIPTION: This code snippet demonstrates how to use the fusion retrieval evaluation pipeline. It sets up the necessary parameters, including the path to a PDF document, test queries, and reference answers, then runs the evaluation and prints the overall analysis.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/16_fusion_rag.ipynb#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# Path to PDF document\n# Path to PDF document containing AI information for knowledge retrieval testing\npdf_path = \"data/AI_Information.pdf\"\n\n# Define a single AI-related test query\ntest_queries = [\n    \"What are the main applications of transformer models in natural language processing?\"  # AI-specific query\n]\n\n# Optional reference answer\nreference_answers = [\n    \"Transformer models have revolutionized natural language processing with applications including machine translation, text summarization, question answering, sentiment analysis, and text generation. They excel at capturing long-range dependencies in text and have become the foundation for models like BERT, GPT, and T5.\",\n]\n\n# Set parameters\nk = 5  # Number of documents to retrieve\nalpha = 0.5  # Weight for vector scores (0.5 means equal weight between vector and BM25)\n\n# Run evaluation\nevaluation_results = evaluate_fusion_retrieval(\n    pdf_path=pdf_path,\n    test_queries=test_queries,\n    reference_answers=reference_answers,\n    k=k,\n    alpha=alpha\n)\n\n# Print overall analysis\nprint(\"\\n\\n=== OVERALL ANALYSIS ===\\n\")\nprint(evaluation_results[\"overall_analysis\"])\n```\n\n----------------------------------------\n\nTITLE: Evaluating Relevance of Retrieved Chunks in Python\nDESCRIPTION: This function evaluates the relevance of retrieved chunks by comparing them to ground truth chunks using cosine similarity of their embeddings.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_relevance(retrieved_chunks: List[str], ground_truth_chunks: List[str]) -> float:\n    \"\"\"\n    Evaluate the relevance of retrieved chunks by comparing them to ground truth chunks.\n\n    Args:\n        retrieved_chunks (List[str]): A list of text chunks retrieved by the system.\n        ground_truth_chunks (List[str]): A list of ground truth text chunks for comparison.\n\n    Returns:\n        float: The average relevance score between the retrieved chunks and the ground truth chunks.\n    \"\"\"\n    relevance_scores: List[float] = []  # Initialize a list to store relevance scores\n\n    # Iterate through pairs of retrieved and ground truth chunks\n    for retrieved, ground_truth in zip(retrieved_chunks, ground_truth_chunks):\n        # Calculate the cosine similarity between the embeddings of the retrieved and ground truth chunks\n        relevance: float = cosine_similarity(\n            generate_embeddings([retrieved])[0],\n            generate_embeddings([ground_truth])[0]\n        )\n        # Append the relevance score to the list\n        relevance_scores.append(relevance)\n\n    # Return the average relevance score\n    return np.mean(relevance_scores)\n```\n\n----------------------------------------\n\nTITLE: Assessing Feedback Relevance for RAG in Python\nDESCRIPTION: This function uses an LLM to determine if a past feedback entry is relevant to the current query and document in a RAG system. It helps filter which past feedback should influence the current retrieval process.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/11_feedback_loop_rag.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef assess_feedback_relevance(query, doc_text, feedback):\n    \"\"\"\n    Use LLM to assess if a past feedback entry is relevant to the current query and document.\n    \n    This function helps determine which past feedback should influence the current retrieval\n    by sending the current query, past query+feedback, and document content to an LLM\n    for relevance assessment.\n    \n    Args:\n        query (str): Current user query that needs information retrieval\n        doc_text (str): Text content of the document being evaluated\n        feedback (Dict): Previous feedback data containing 'query' and 'response' keys\n        \n    Returns:\n        bool: True if the feedback is deemed relevant to current query/document, False otherwise\n    \"\"\"\n    # Define system prompt instructing the LLM to make binary relevance judgments only\n    system_prompt = \"\"\"You are an AI system that determines if a past feedback is relevant to a current query and document.\n    Answer with ONLY 'yes' or 'no'. Your job is strictly to determine relevance, not to provide explanations.\"\"\"\n\n    # Construct user prompt with current query, past feedback data, and truncated document content\n    user_prompt = f\"\"\"\n    Current query: {query}\n    Past query that received feedback: {feedback['query']}\n    Document content: {doc_text[:500]}... [truncated]\n    Past response that received feedback: {feedback['response'][:500]}... [truncated]\n\n    Is this past feedback relevant to the current query and document? (yes/no)\n    \"\"\"\n\n    # Call the LLM API with zero temperature for deterministic output\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0  # Use temperature=0 for consistent, deterministic responses\n    )\n    \n    # Extract and normalize the response to determine relevance\n    answer = response.choices[0].message.content.strip().lower()\n    return 'yes' in answer  # Return True if the answer contains 'yes'\n```\n\n----------------------------------------\n\nTITLE: Evaluating CRAG with Test Queries in Python\nDESCRIPTION: This code snippet demonstrates how to use the CRAG evaluation pipeline with a specific PDF document and test queries. It sets up the evaluation parameters and runs the full comparison between CRAG and standard RAG, printing the overall analysis.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/20_crag.ipynb#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# Path to the AI information PDF document\npdf_path = \"data/AI_Information.pdf\"\n\n# Run comprehensive evaluation with multiple AI-related queries\ntest_queries = [\n    \"How does machine learning differ from traditional programming?\",\n]\n\n# Optional reference answers for better quality evaluation\nreference_answers = [\n    \"Machine learning differs from traditional programming by having computers learn patterns from data rather than following explicit instructions. In traditional programming, developers write specific rules for the computer to follow, while in machine learning\",\n]\n\n# Run the full evaluation comparing CRAG vs standard RAG\nevaluation_results = run_crag_evaluation(pdf_path, test_queries, reference_answers)\nprint(\"\\n=== Overall Analysis of CRAG vs Standard RAG ===\")\nprint(evaluation_results[\"overall_analysis\"])\n```\n\n----------------------------------------\n\nTITLE: Comparing Generated Responses with Reference Answers in Python\nDESCRIPTION: This function compares a generated response with a reference answer using an AI model. It evaluates the correctness, completeness, and relevance of the generated response to the query, providing a brief analysis of how well it matches the reference.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/17_graph_rag.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef compare_with_reference(response, reference, query):\n    \"\"\"\n    Compare generated response with reference answer.\n    \n    Args:\n        response (str): Generated response\n        reference (str): Reference answer\n        query (str): Original query\n        \n    Returns:\n        str: Comparison analysis\n    \"\"\"\n    # System message to instruct the model on how to compare the responses\n    system_message = \"\"\"Compare the AI-generated response with the reference answer.\nEvaluate based on: correctness, completeness, and relevance to the query.\nProvide a brief analysis (2-3 sentences) of how well the generated response matches the reference.\"\"\"\n\n    # Construct the prompt with the query, AI-generated response, and reference answer\n    prompt = f\"\"\"\nQuery: {query}\n\nAI-generated response:\n{response}\n\nReference answer:\n{reference}\n\nHow well does the AI response match the reference?\n\"\"\"\n\n    # Make a request to the OpenAI API to generate the comparison analysis\n    comparison = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_message},  # System message to guide the assistant\n            {\"role\": \"user\", \"content\": prompt}  # User message with the prompt\n        ],\n        temperature=0.0  # Set the temperature for response generation\n    )\n    \n    # Return the generated comparison analysis\n    return comparison.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Implementing Single Chunk Compression with LLM\nDESCRIPTION: Function to compress individual text chunks using different compression strategies (selective, summary, or extraction) via LLM. Supports customizable models and returns both compressed text and compression ratio.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/10_contextual_compression.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef compress_chunk(chunk, query, compression_type=\"selective\", model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Compress a retrieved chunk by keeping only the parts relevant to the query.\n    \n    Args:\n        chunk (str): Text chunk to compress\n        query (str): User query\n        compression_type (str): Type of compression (\"selective\", \"summary\", or \"extraction\")\n        model (str): LLM model to use\n        \n    Returns:\n        str: Compressed chunk\n    \"\"\"\n    # Define system prompts for different compression approaches\n    if compression_type == \"selective\":\n        system_prompt = \"\"\"You are an expert at information filtering. \n        Your task is to analyze a document chunk and extract ONLY the sentences or paragraphs that are directly \n        relevant to the user's query. Remove all irrelevant content.\n\n        Your output should:\n        1. ONLY include text that helps answer the query\n        2. Preserve the exact wording of relevant sentences (do not paraphrase)\n        3. Maintain the original order of the text\n        4. Include ALL relevant content, even if it seems redundant\n        5. EXCLUDE any text that isn't relevant to the query\n\n        Format your response as plain text with no additional comments.\"\"\"\n    elif compression_type == \"summary\":\n        system_prompt = \"\"\"You are an expert at summarization. \n        Your task is to create a concise summary of the provided chunk that focuses ONLY on \n        information relevant to the user's query.\n\n        Your output should:\n        1. Be brief but comprehensive regarding query-relevant information\n        2. Focus exclusively on information related to the query\n        3. Omit irrelevant details\n        4. Be written in a neutral, factual tone\n\n        Format your response as plain text with no additional comments.\"\"\"\n    else:  # extraction\n        system_prompt = \"\"\"You are an expert at information extraction.\n        Your task is to extract ONLY the exact sentences from the document chunk that contain information relevant \n        to answering the user's query.\n\n        Your output should:\n        1. Include ONLY direct quotes of relevant sentences from the original text\n        2. Preserve the original wording (do not modify the text)\n        3. Include ONLY sentences that directly relate to the query\n        4. Separate extracted sentences with newlines\n        5. Do not add any commentary or additional text\n\n        Format your response as plain text with no additional comments.\"\"\"\n\n    # Define the user prompt with the query and document chunk\n    user_prompt = f\"\"\"\n        Query: {query}\n\n        Document Chunk:\n        {chunk}\n\n        Extract only the content relevant to answering this query.\n    \"\"\"\n    \n    # Generate a response using the OpenAI API\n    response = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0\n    )\n    \n    # Extract the compressed chunk from the response\n    compressed_chunk = response.choices[0].message.content.strip()\n    \n    # Calculate compression ratio\n    original_length = len(chunk)\n    compressed_length = len(compressed_chunk)\n    compression_ratio = (original_length - compressed_length) / original_length * 100\n    \n    return compressed_chunk, compression_ratio\n```\n\n----------------------------------------\n\nTITLE: Evaluating RAG Feedback Loop in Python\nDESCRIPTION: This snippet evaluates a RAG feedback loop using predefined test queries and reference answers. It calls the evaluate_feedback_loop function with the PDF path, test queries, and reference answers as inputs.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/11_feedback_loop_rag.ipynb#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nevaluation_results = evaluate_feedback_loop(\n    pdf_path=pdf_path,\n    test_queries=test_queries,\n    reference_answers=reference_answers\n)\n```\n\n----------------------------------------\n\nTITLE: Response Generation with Context in Python\nDESCRIPTION: This function generates a response based on the user query and retrieved chunks. It formats the context with page numbers, uses a system message to guide the AI assistant, and leverages the OpenAI API to generate a contextually relevant response.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/18_hierarchy_rag.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef generate_response(query, retrieved_chunks):\n    \"\"\"\n    Generate a response based on the query and retrieved chunks.\n    \n    Args:\n        query (str): User query\n        retrieved_chunks (List[Dict]): Retrieved chunks from hierarchical search\n        \n    Returns:\n        str: Generated response\n    \"\"\"\n    # Extract text from chunks and prepare context parts\n    context_parts = []\n    \n    for i, chunk in enumerate(retrieved_chunks):\n        page_num = chunk[\"metadata\"][\"page\"]  # Get the page number from metadata\n        context_parts.append(f\"[Page {page_num}]: {chunk['text']}\")  # Format the chunk text with page number\n    \n    # Combine all context parts into a single context string\n    context = \"\\n\\n\".join(context_parts)\n    \n    # Define the system message to guide the AI assistant\n    system_message = \"\"\"You are a helpful AI assistant answering questions based on the provided context.\nUse the information from the context to answer the user's question accurately.\nIf the context doesn't contain relevant information, acknowledge that.\nInclude page numbers when referencing specific information.\"\"\"\n\n    # Generate the response using the OpenAI API\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",  # Specify the model to use\n        messages=[\n            {\"role\": \"system\", \"content\": system_message},  # System message to guide the assistant\n            {\"role\": \"user\", \"content\": f\"Context:\\n\\n{context}\\n\\nQuestion: {query}\"}  # User message with context and query\n        ],\n        temperature=0.2  # Set the temperature for response generation\n    )\n    \n    # Return the generated response content\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Chunking Extracted Text with Overlap\nDESCRIPTION: Function to divide the extracted text into smaller, overlapping chunks to improve retrieval accuracy. It takes the text, chunk size, and overlap size as parameters.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/10_contextual_compression.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef chunk_text(text, n=1000, overlap=200):\n    \"\"\"\n    Chunks the given text into segments of n characters with overlap.\n\n    Args:\n    text (str): The text to be chunked.\n    n (int): The number of characters in each chunk.\n    overlap (int): The number of overlapping characters between chunks.\n\n    Returns:\n    List[str]: A list of text chunks.\n    \"\"\"\n    chunks = []  # Initialize an empty list to store the chunks\n    \n    # Loop through the text with a step size of (n - overlap)\n    for i in range(0, len(text), n - overlap):\n        # Append a chunk of text from index i to i + n to the chunks list\n        chunks.append(text[i:i + n])\n\n    return chunks  # Return the list of text chunks\n```\n\n----------------------------------------\n\nTITLE: Evaluating Proposition Quality in Python\nDESCRIPTION: A function that assesses the quality of generated propositions by comparing them to the original text. It uses an LLM to evaluate each proposition on four criteria: accuracy, clarity, completeness, and conciseness, returning numerical scores (1-10) for each dimension.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/14_proposition_chunking.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_proposition(proposition, original_text):\n    \"\"\"\n    Evaluate a proposition's quality based on accuracy, clarity, completeness, and conciseness.\n    \n    Args:\n        proposition (str): The proposition to evaluate\n        original_text (str): The original text for comparison\n        \n    Returns:\n        Dict: Scores for each evaluation dimension\n    \"\"\"\n    # System prompt to instruct the AI on how to evaluate the proposition\n    system_prompt = \"\"\"You are an expert at evaluating the quality of propositions extracted from text.\n    Rate the given proposition on the following criteria (scale 1-10):\n\n    - Accuracy: How well the proposition reflects information in the original text\n    - Clarity: How easy it is to understand the proposition without additional context\n    - Completeness: Whether the proposition includes necessary details (dates, qualifiers, etc.)\n    - Conciseness: Whether the proposition is concise without losing important information\n\n    The response must be in valid JSON format with numerical scores for each criterion:\n    {\"accuracy\": X, \"clarity\": X, \"completeness\": X, \"conciseness\": X}\n    \"\"\"\n\n    # User prompt containing the proposition and the original text\n    user_prompt = f\"\"\"Proposition: {proposition}\n\n    Original Text: {original_text}\n\n    Please provide your evaluation scores in JSON format.\"\"\"\n\n    # Generate response from the model\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        response_format={\"type\": \"json_object\"},\n        temperature=0\n    )\n    \n    # Parse the JSON response\n    try:\n        scores = json.loads(response.choices[0].message.content.strip())\n        return scores\n    except json.JSONDecodeError:\n        # Fallback if JSON parsing fails\n        return {\n            \"accuracy\": 5,\n            \"clarity\": 5,\n            \"completeness\": 5,\n            \"conciseness\": 5\n        }\n```\n\n----------------------------------------\n\nTITLE: Comparing CRAG vs Standard RAG in Python\nDESCRIPTION: This function compares the performance of CRAG against standard RAG for a given query. It runs both approaches, evaluates their responses, and provides a detailed comparison of the results.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/20_crag.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef compare_crag_vs_standard_rag(query, vector_store, reference_answer=None):\n    \"\"\"\n    Compare CRAG against standard RAG for a query.\n    \n    Args:\n        query (str): User query\n        vector_store (SimpleVectorStore): Vector store with document chunks\n        reference_answer (str, optional): Reference answer for comparison\n        \n    Returns:\n        Dict: Comparison results\n    \"\"\"\n    # Run CRAG process\n    print(\"\\n=== Running CRAG ===\")\n    crag_result = crag_process(query, vector_store)\n    crag_response = crag_result[\"response\"]\n    \n    # Run standard RAG (directly retrieve and respond)\n    print(\"\\n=== Running standard RAG ===\")\n    query_embedding = create_embeddings(query)\n    retrieved_docs = vector_store.similarity_search(query_embedding, k=3)\n    combined_text = \"\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n    standard_sources = [{\"title\": \"Document\", \"url\": \"\"}]\n    standard_response = generate_response(query, combined_text, standard_sources)\n    \n    # Evaluate both approaches\n    print(\"\\n=== Evaluating CRAG response ===\")\n    crag_eval = evaluate_crag_response(query, crag_response, reference_answer)\n    \n    print(\"\\n=== Evaluating standard RAG response ===\")\n    standard_eval = evaluate_crag_response(query, standard_response, reference_answer)\n    \n    # Compare approaches\n    print(\"\\n=== Comparing approaches ===\")\n    comparison = compare_responses(query, crag_response, standard_response, reference_answer)\n    \n    return {\n        \"query\": query,\n        \"crag_response\": crag_response,\n        \"standard_response\": standard_response,\n        \"reference_answer\": reference_answer,\n        \"crag_evaluation\": crag_eval,\n        \"standard_evaluation\": standard_eval,\n        \"comparison\": comparison\n    }\n```\n\n----------------------------------------\n\nTITLE: Evaluating AI Response Quality in Python\nDESCRIPTION: Evaluates the quality of an AI-generated response by comparing it to an ideal answer. Uses a scoring system where 1 indicates a perfect match, 0.5 a partial match, and 0 an incorrect answer.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/1_simple_rag.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Define the system prompt for the evaluation system\nevaluate_system_prompt = \"You are an intelligent evaluation system tasked with assessing the AI assistant's responses. If the AI assistant's response is very close to the true response, assign a score of 1. If the response is incorrect or unsatisfactory in relation to the true response, assign a score of 0. If the response is partially aligned with the true response, assign a score of 0.5.\"\n\n# Create the evaluation prompt by combining the user query, AI response, true response, and evaluation system prompt\nevaluation_prompt = f\"User Query: {query}\\nAI Response:\\n{ai_response.choices[0].message.content}\\nTrue Response: {data[0]['ideal_answer']}\\n{evaluate_system_prompt}\"\n\n# Generate the evaluation response using the evaluation system prompt and evaluation prompt\nevaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n\n# Print the evaluation response\nprint(evaluation_response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Calculating Reward for RL Agent Response in Python\nDESCRIPTION: This function calculates a reward value by comparing the generated response to the ground truth using cosine similarity between their embeddings. It determines how close the response is to the expected answer, with higher values indicating greater similarity.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndef calculate_reward(response: str, ground_truth: str) -> float:\n    \"\"\"\n    Calculate a reward value by comparing the generated response to the ground truth.\n    \n    Uses cosine similarity between the embeddings of the response and ground truth\n    to determine how close the response is to the expected answer.\n    \n    Args:\n        response (str): The generated response from the RAG pipeline.\n        ground_truth (str): The expected correct answer.\n    \n    Returns:\n        float: A reward value between -1 and 1, where higher values indicate \n               greater similarity to the ground truth.\n    \"\"\"\n    # Generate embeddings for both the response and ground truth\n    response_embedding = generate_embeddings([response])[0]\n    ground_truth_embedding = generate_embeddings([ground_truth])[0]\n    \n    # Calculate cosine similarity between the embeddings as the reward\n    similarity = cosine_similarity(response_embedding, ground_truth_embedding)\n    return similarity\n```\n\n----------------------------------------\n\nTITLE: Creating and Processing Evaluation Prompts for RAG System in Python\nDESCRIPTION: This code creates an evaluation prompt by combining multiple elements including user query, AI response, ideal answer, and evaluation system prompt. It then generates an evaluation using this prompt and prints the result to assess the AI system's performance against ground truth.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/4_context_enriched_rag.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Create the evaluation prompt by combining the user query, AI response, true response, and evaluation system prompt\nevaluation_prompt = f\"User Query: {query}\\nAI Response:\\n{ai_response.choices[0].message.content}\\nTrue Response: {data[0]['ideal_answer']}\\n{evaluate_system_prompt}\"\n\n# Generate the evaluation response using the evaluation system prompt and evaluation prompt\nevaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n\n# Print the evaluation response\nprint(evaluation_response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Generating Atomic Propositions from Text Chunks in Python\nDESCRIPTION: A function that breaks down text chunks into simple, self-contained propositions using an LLM (Llama-3.2-3B-Instruct). It processes the output to ensure each proposition expresses a single fact, is context-independent, uses full entity names, and includes relevant qualifiers.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/14_proposition_chunking.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef generate_propositions(chunk):\n    \"\"\"\n    Generate atomic, self-contained propositions from a text chunk.\n    \n    Args:\n        chunk (Dict): Text chunk with content and metadata\n        \n    Returns:\n        List[str]: List of generated propositions\n    \"\"\"\n    # System prompt to instruct the AI on how to generate propositions\n    system_prompt = \"\"\"Please break down the following text into simple, self-contained propositions. \n    Ensure that each proposition meets the following criteria:\n\n    1. Express a Single Fact: Each proposition should state one specific fact or claim.\n    2. Be Understandable Without Context: The proposition should be self-contained, meaning it can be understood without needing additional context.\n    3. Use Full Names, Not Pronouns: Avoid pronouns or ambiguous references; use full entity names.\n    4. Include Relevant Dates/Qualifiers: If applicable, include necessary dates, times, and qualifiers to make the fact precise.\n    5. Contain One Subject-Predicate Relationship: Focus on a single subject and its corresponding action or attribute, without conjunctions or multiple clauses.\n\n    Output ONLY the list of propositions without any additional text or explanations.\"\"\"\n\n    # User prompt containing the text chunk to be converted into propositions\n    user_prompt = f\"Text to convert into propositions:\\n\\n{chunk['text']}\"\n    \n    # Generate response from the model\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",  # Using a stronger model for accurate proposition generation\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0\n    )\n    \n    # Extract propositions from the response\n    raw_propositions = response.choices[0].message.content.strip().split('\\n')\n    \n    # Clean up propositions (remove numbering, bullets, etc.)\n    clean_propositions = []\n    for prop in raw_propositions:\n        # Remove numbering (1., 2., etc.) and bullet points\n        cleaned = re.sub(r'^\\s*(\\d+\\.|-|\\*)\\s*', '', prop).strip()\n        if cleaned and len(cleaned) > 10:  # Simple filter for empty or very short propositions\n            clean_propositions.append(cleaned)\n    \n    return clean_propositions\n```\n\n----------------------------------------\n\nTITLE: Search Query Rewriting with GPT-3.5\nDESCRIPTION: Function that uses GPT-3.5-turbo to rewrite search queries for better results. Transforms user queries into more effective search terms by focusing on keywords and removing unnecessary words.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/20_crag.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef rewrite_search_query(query):\n    \"\"\"\n    Rewrite a query to be more suitable for web search.\n    \n    Args:\n        query (str): Original query\n        \n    Returns:\n        str: Rewritten query\n    \"\"\"\n    # Define the system prompt to instruct the model on how to rewrite the query\n    system_prompt = \"\"\"\n    You are an expert at creating effective search queries.\n    Rewrite the given query to make it more suitable for a web search engine.\n    Focus on keywords and facts, remove unnecessary words, and make it concise.\n    \"\"\"\n    \n    try:\n        # Make a request to the OpenAI API to rewrite the query\n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",  # Specify the model to use\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n                {\"role\": \"user\", \"content\": f\"Original query: {query}\\n\\nRewritten query:\"}  # User message with the original query\n            ],\n            temperature=0.3,  # Set the temperature for response generation\n            max_tokens=50  # Limit the response length\n        )\n        \n        # Return the rewritten query from the response\n        return response.choices[0].message.content.strip()\n    except Exception as e:\n        # Print the error message and return the original query on error\n        print(f\"Error rewriting search query: {e}\")\n        return query  # Return original query on error\n```\n\n----------------------------------------\n\nTITLE: Evaluating AI Response Quality Against Expected Answers in Python\nDESCRIPTION: This snippet defines a system prompt for evaluating AI responses against expected answers. It creates an evaluation prompt that includes the query, AI response, and true response, then generates an evaluation score (0, 0.5, or 1) based on how well the AI response matches the expected answer.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/6_doc_augmentation_rag.ipynb#2025-04-23_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# Define the system prompt for the evaluation system\nevaluate_system_prompt = \"You are an intelligent evaluation system tasked with assessing the AI assistant's responses. If the AI assistant's response is very close to the true response, assign a score of 1. If the response is incorrect or unsatisfactory in relation to the true response, assign a score of 0. If the response is partially aligned with the true response, assign a score of 0.5.\"\n\n# Create the evaluation prompt by combining the user query, AI response, true response, and evaluation system prompt\nevaluation_prompt = f\"User Query: {query}\\nAI Response:\\n{ai_response.choices[0].message.content}\\nTrue Response: {data[0]['ideal_answer']}\\n{evaluate_system_prompt}\"\n\n# Generate the evaluation response using the evaluation system prompt and evaluation prompt\nevaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n\n# Print the evaluation response\nprint(evaluation_response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Processing PDF Text for RAG in Python\nDESCRIPTION: Extracts text from a PDF file, chunks it into segments of 1000 characters with 200 character overlap, and displays information about the chunks. This is the data preparation phase for the RAG pipeline.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/1_simple_rag.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Define the path to the PDF file\npdf_path = \"data/AI_Information.pdf\"\n\n# Extract text from the PDF file\nextracted_text = extract_text_from_pdf(pdf_path)\n\n# Chunk the extracted text into segments of 1000 characters with an overlap of 200 characters\ntext_chunks = chunk_text(extracted_text, 1000, 200)\n\n# Print the number of text chunks created\nprint(\"Number of text chunks:\", len(text_chunks))\n\n# Print the first text chunk\nprint(\"\\nFirst text chunk:\")\nprint(text_chunks[0])\n```\n\n----------------------------------------\n\nTITLE: Creating Embeddings for Text and Headers in RAG\nDESCRIPTION: This function creates embeddings for given text using a specified model. It's used to generate numerical representations of both chunk text and headers, which is crucial for semantic search in the RAG process.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/5_contextual_chunk_headers_rag.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n    \"\"\"\n    Creates embeddings for the given text.\n\n    Args:\n    text (str): The input text to be embedded.\n    model (str): The embedding model to be used. Default is \"BAAI/bge-en-icl\".\n\n    Returns:\n    dict: The response containing the embedding for the input text.\n    \"\"\"\n    # Create embeddings using the specified model and input text\n    response = client.embeddings.create(\n        model=model,\n        input=text\n    )\n    # Return the embedding from the response\n    return response.data[0].embedding\n```\n\n----------------------------------------\n\nTITLE: Detailed Response Comparison Function\nDESCRIPTION: Function to compare responses from HyDE and standard RAG using an LLM evaluator. Analyzes accuracy, relevance, completeness and clarity of responses.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/19_HyDE_rag.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef compare_responses(query, hyde_response, standard_response, reference=None):\n    \"\"\"\n    Compare responses from HyDE and standard RAG.\n    \n    Args:\n        query (str): User query\n        hyde_response (str): Response from HyDE RAG\n        standard_response (str): Response from standard RAG\n        reference (str, optional): Reference answer\n        \n    Returns:\n        str: Comparison analysis\n    \"\"\"\n    system_prompt = \"\"\"You are an expert evaluator of information retrieval systems.\nCompare the two responses to the same query, one generated using HyDE (Hypothetical Document Embedding) \nand the other using standard RAG with direct query embedding.\n\nEvaluate them based on:\n1. Accuracy: Which response provides more factually correct information?\n2. Relevance: Which response better addresses the query?\n3. Completeness: Which response provides more thorough coverage of the topic?\n4. Clarity: Which response is better organized and easier to understand?\n\nBe specific about the strengths and weaknesses of each approach.\"\"\"\n\n    user_prompt = f\"\"\"Query: {query}\n\nResponse from HyDE RAG:\n{hyde_response}\n\nResponse from Standard RAG:\n{standard_response}\"\"\"\n\n    if reference:\n        user_prompt += f\"\"\"\n\nReference Answer:\n{reference}\"\"\"\n\n    user_prompt += \"\"\"\n\nPlease provide a detailed comparison of these two responses, highlighting which approach performed better and why.\"\"\"\n\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0\n    )\n    \n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Testing and Comparing Different Retrieval Methods in Python\nDESCRIPTION: This code demonstrates the evaluation of different retrieval and reranking methods using a test query. It processes a document into a vector store, then compares standard retrieval (no reranking) with LLM-based reranking to highlight the differences in response quality.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/8_reranker.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Process document\nvector_store = process_document(pdf_path)\n\n# Example query\nquery = \"Does AI have the potential to transform the way we live and work?\"\n\n# Compare different methods\nprint(\"Comparing retrieval methods...\")\n\n# 1. Standard retrieval (no reranking)\nprint(\"\\n=== STANDARD RETRIEVAL ===\")\nstandard_results = rag_with_reranking(query, vector_store, reranking_method=\"none\")\nprint(f\"\\nQuery: {query}\")\nprint(f\"\\nResponse:\\n{standard_results['response']}\")\n\n# 2. LLM-based reranking\nprint(\"\\n=== LLM-BASED RERANKING ===\")\nllm_results = rag_with_reranking(query, vector_store, reranking_method=\"llm\")\nprint(f\"\\nQuery: {query}\")\nprint(f\"\\nResponse:\\n{llm_results['response']}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Response Evaluation Function in Python\nDESCRIPTION: This function evaluates the quality of an AI-generated response based on faithfulness and relevancy. It uses the defined prompt templates, sends requests to an AI model, and returns scores for both criteria.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/3_chunk_size_selector.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_response(question, response, true_answer):\n        \"\"\"\n        Evaluates the quality of an AI-generated response based on faithfulness and relevancy.\n\n        Args:\n        question (str): The user's original question.\n        response (str): The AI-generated response being evaluated.\n        true_answer (str): The correct answer used as ground truth.\n\n        Returns:\n        Tuple[float, float]: A tuple containing (faithfulness_score, relevancy_score).\n                                                Each score is one of: 1.0 (full), 0.5 (partial), or 0.0 (none).\n        \"\"\"\n        # Format the evaluation prompts\n        faithfulness_prompt = FAITHFULNESS_PROMPT_TEMPLATE.format(\n                question=question, \n                response=response, \n                true_answer=true_answer,\n                full=SCORE_FULL,\n                partial=SCORE_PARTIAL,\n                none=SCORE_NONE\n        )\n        \n        relevancy_prompt = RELEVANCY_PROMPT_TEMPLATE.format(\n                question=question, \n                response=response,\n                full=SCORE_FULL,\n                partial=SCORE_PARTIAL,\n                none=SCORE_NONE\n        )\n\n        # Request faithfulness evaluation from the model\n        faithfulness_response = client.chat.completions.create(\n               model=\"meta-llama/Llama-3.2-3B-Instruct\",\n                temperature=0,\n                messages=[\n                        {\"role\": \"system\", \"content\": \"You are an objective evaluator. Return ONLY the numerical score.\"},\n                        {\"role\": \"user\", \"content\": faithfulness_prompt}\n                ]\n        )\n        \n        # Request relevancy evaluation from the model\n        relevancy_response = client.chat.completions.create(\n                model=\"meta-llama/Llama-3.2-3B-Instruct\",\n                temperature=0,\n                messages=[\n                        {\"role\": \"system\", \"content\": \"You are an objective evaluator. Return ONLY the numerical score.\"},\n                        {\"role\": \"user\", \"content\": relevancy_prompt}\n                ]\n        )\n        \n        # Extract scores and handle potential parsing errors\n        try:\n                faithfulness_score = float(faithfulness_response.choices[0].message.content.strip())\n        except ValueError:\n                print(\"Warning: Could not parse faithfulness score, defaulting to 0\")\n                faithfulness_score = 0.0\n                \n        try:\n                relevancy_score = float(relevancy_response.choices[0].message.content.strip())\n        except ValueError:\n                print(\"Warning: Could not parse relevancy score, defaulting to 0\")\n                relevancy_score = 0.0\n\n        return faithfulness_score, relevancy_score\n```\n\n----------------------------------------\n\nTITLE: Processing Documents into Vector Store in Python\nDESCRIPTION: This function processes a PDF document into a vector store. It extracts text from the PDF, chunks the text, creates embeddings for each chunk, and adds them to a SimpleVectorStore instance for efficient retrieval in the CRAG system.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/20_crag.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n    \"\"\"\n    Process a document into a vector store.\n    \n    Args:\n        pdf_path (str): Path to the PDF file\n        chunk_size (int): Size of each chunk in characters\n        chunk_overlap (int): Overlap between chunks in characters\n        \n    Returns:\n        SimpleVectorStore: Vector store containing document chunks\n    \"\"\"\n    # Extract text from the PDF file\n    text = extract_text_from_pdf(pdf_path)\n    \n    # Split the extracted text into chunks with specified size and overlap\n    chunks = chunk_text(text, chunk_size, chunk_overlap)\n    \n    # Create embeddings for each chunk of text\n    print(\"Creating embeddings for chunks...\")\n    chunk_texts = [chunk[\"text\"] for chunk in chunks]\n    chunk_embeddings = create_embeddings(chunk_texts)\n    \n    # Initialize a new vector store\n    vector_store = SimpleVectorStore()\n    \n    # Add the chunks and their embeddings to the vector store\n    vector_store.add_items(chunks, chunk_embeddings)\n    \n    print(f\"Vector store created with {len(chunks)} chunks\")\n    return vector_store\n```\n\n----------------------------------------\n\nTITLE: Chunking Text with Overlap for RAG Processing in Python\nDESCRIPTION: Function to divide a long text into smaller, overlapping chunks. This technique improves retrieval accuracy by ensuring that context spanning chunk boundaries is preserved in at least one chunk.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/12_adaptive_rag.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef chunk_text(text, n, overlap):\n    \"\"\"\n    Chunks the given text into segments of n characters with overlap.\n\n    Args:\n    text (str): The text to be chunked.\n    n (int): The number of characters in each chunk.\n    overlap (int): The number of overlapping characters between chunks.\n\n    Returns:\n    List[str]: A list of text chunks.\n    \"\"\"\n    chunks = []  # Initialize an empty list to store the chunks\n    \n    # Loop through the text with a step size of (n - overlap)\n    for i in range(0, len(text), n - overlap):\n        # Append a chunk of text from index i to i + n to the chunks list\n        chunks.append(text[i:i + n])\n\n    return chunks  # Return the list of text chunks\n```\n\n----------------------------------------\n\nTITLE: Overall Analysis Generator\nDESCRIPTION: Function to generate comprehensive analysis of evaluation results across multiple queries. Compares strengths and weaknesses of both approaches and provides usage recommendations.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/19_HyDE_rag.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef generate_overall_analysis(results):\n    \"\"\"\n    Generate an overall analysis of the evaluation results.\n    \n    Args:\n        results (List[Dict]): Results from individual query evaluations\n        \n    Returns:\n        str: Overall analysis\n    \"\"\"\n    system_prompt = \"\"\"You are an expert at evaluating information retrieval systems.\nBased on multiple test queries, provide an overall analysis comparing HyDE RAG (using hypothetical document embedding)\nwith standard RAG (using direct query embedding).\n\nFocus on:\n1. When HyDE performs better and why\n2. When standard RAG performs better and why\n3. The types of queries that benefit most from HyDE\n4. The overall strengths and weaknesses of each approach\n5. Recommendations for when to use each approach\"\"\"\n\n    # Create summary of evaluations\n    evaluations_summary = \"\"\n    for i, result in enumerate(results):\n        evaluations_summary += f\"Query {i+1}: {result['query']}\\n\"\n        evaluations_summary += f\"Comparison summary: {result['comparison'][:200]}...\\n\\n\"\n\n    user_prompt = f\"\"\"Based on the following evaluations comparing HyDE vs standard RAG across {len(results)} queries, \nprovide an overall analysis of these two approaches:\n\n{evaluations_summary}\n\nPlease provide a comprehensive analysis of the relative strengths and weaknesses of HyDE compared to standard RAG,\nfocusing on when and why one approach outperforms the other.\"\"\"\n\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0\n    )\n    \n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Chunking Text with Overlap for Improved Retrieval in Python\nDESCRIPTION: Function that divides a large text into smaller, overlapping chunks. This approach helps maintain context between chunks and improves retrieval accuracy by ensuring important information isn't split across chunk boundaries.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/4_context_enriched_rag.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef chunk_text(text, n, overlap):\n    \"\"\"\n    Chunks the given text into segments of n characters with overlap.\n\n    Args:\n    text (str): The text to be chunked.\n    n (int): The number of characters in each chunk.\n    overlap (int): The number of overlapping characters between chunks.\n\n    Returns:\n    List[str]: A list of text chunks.\n    \"\"\"\n    chunks = []  # Initialize an empty list to store the chunks\n    \n    # Loop through the text with a step size of (n - overlap)\n    for i in range(0, len(text), n - overlap):\n        # Append a chunk of text from index i to i + n to the chunks list\n        chunks.append(text[i:i + n])\n\n    return chunks  # Return the list of text chunks\n```\n\n----------------------------------------\n\nTITLE: Chunking Text for Document Processing in Python\nDESCRIPTION: This function splits a large text into smaller chunks of a specified size with overlapping characters between chunks. This is crucial for processing long documents in RAG systems, as it ensures context continuity between chunks.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/7_query_transform.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef chunk_text(text, n=1000, overlap=200):\n    \"\"\"\n    Chunks the given text into segments of n characters with overlap.\n\n    Args:\n    text (str): The text to be chunked.\n    n (int): The number of characters in each chunk.\n    overlap (int): The number of overlapping characters between chunks.\n\n    Returns:\n    List[str]: A list of text chunks.\n    \"\"\"\n    chunks = []  # Initialize an empty list to store the chunks\n    \n    # Loop through the text with a step size of (n - overlap)\n    for i in range(0, len(text), n - overlap):\n        # Append a chunk of text from index i to i + n to the chunks list\n        chunks.append(text[i:i + n])\n\n    return chunks  # Return the list of text chunks\n```\n\n----------------------------------------\n\nTITLE: Processing Documents for RAG in Python\nDESCRIPTION: This function processes a PDF document for RAG by extracting text, chunking it, creating embeddings for each chunk, and storing them in a vector store. It handles the complete document preprocessing pipeline needed for retrieval.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/7_query_transform.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n    \"\"\"\n    Process a document for RAG.\n\n    Args:\n    pdf_path (str): Path to the PDF file.\n    chunk_size (int): Size of each chunk in characters.\n    chunk_overlap (int): Overlap between chunks in characters.\n\n    Returns:\n    SimpleVectorStore: A vector store containing document chunks and their embeddings.\n    \"\"\"\n    print(\"Extracting text from PDF...\")\n    extracted_text = extract_text_from_pdf(pdf_path)\n    \n    print(\"Chunking text...\")\n    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n    print(f\"Created {len(chunks)} text chunks\")\n    \n    print(\"Creating embeddings for chunks...\")\n    # Create embeddings for all chunks at once for efficiency\n    chunk_embeddings = create_embeddings(chunks)\n    \n    # Create vector store\n    store = SimpleVectorStore()\n    \n    # Add chunks to vector store\n    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n        store.add_item(\n            text=chunk,\n            embedding=embedding,\n            metadata={\"index\": i, \"source\": pdf_path}\n        )\n    \n    print(f\"Added {len(chunks)} chunks to the vector store\")\n    return store\n```\n\n----------------------------------------\n\nTITLE: Generating Responses Based on Retrieved Context in Python\nDESCRIPTION: This function generates a response to a user query using an LLM based on the retrieved context from the RAG system. It uses a structured prompt to guide the model to answer based solely on the provided context.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/7_query_transform.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef generate_response(query, context, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Generates a response based on the query and retrieved context.\n    \n    Args:\n        query (str): User query\n        context (str): Retrieved context\n        model (str): The model to use for response generation\n        \n    Returns:\n        str: Generated response\n    \"\"\"\n    # Define the system prompt to guide the AI assistant's behavior\n    system_prompt = \"You are a helpful AI assistant. Answer the user's question based only on the provided context. If you cannot find the answer in the context, state that you don't have enough information.\"\n    \n    # Define the user prompt with the context and query\n    user_prompt = f\"\"\"\n        Context:\n        {context}\n\n        Question: {query}\n\n        Please provide a comprehensive answer based only on the context above.\n    \"\"\"\n    \n    # Generate the response using the specified model\n    response = client.chat.completions.create(\n        model=model,\n        temperature=0,  # Low temperature for deterministic output\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ]\n    )\n    \n    # Return the generated response, stripping any leading/trailing whitespace\n    return response.choices[0].message.content.strip()\n```\n\n----------------------------------------\n\nTITLE: Evaluating Accuracy of Generated Responses in Python\nDESCRIPTION: This function evaluates the accuracy of generated responses by comparing them to ground truth responses using cosine similarity of their embeddings.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_accuracy(responses: List[str], ground_truth_responses: List[str]) -> float:\n    \"\"\"\n    Evaluate the accuracy of generated responses by comparing them to ground truth responses.\n\n    Args:\n        responses (List[str]): A list of generated responses to evaluate.\n        ground_truth_responses (List[str]): A list of ground truth responses to compare against.\n\n    Returns:\n        float: The average accuracy score, calculated as the mean cosine similarity \n               between the embeddings of the generated responses and the ground truth responses.\n    \"\"\"\n    accuracy_scores: List[float] = []  # Initialize a list to store accuracy scores\n\n    # Iterate through each pair of generated response and ground truth response\n    for response, ground_truth in zip(responses, ground_truth_responses):\n        # Calculate the cosine similarity between the embeddings of the response and ground truth\n        accuracy: float = cosine_similarity(\n            generate_embeddings([response])[0],\n            generate_embeddings([ground_truth])[0]\n        )\n        # Append the accuracy score to the list\n        accuracy_scores.append(accuracy)\n\n    # Return the mean of the accuracy scores\n    return np.mean(accuracy_scores)\n```\n\n----------------------------------------\n\nTITLE: Reconstructing Text Segments from Chunk Indices\nDESCRIPTION: Function that reconstructs continuous text segments from individual chunks based on the indices of the best segments identified by the algorithm. It joins chunks together and maintains metadata about the segment ranges.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/9_rse.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef reconstruct_segments(chunks, best_segments):\n    \"\"\"\n    Reconstruct text segments based on chunk indices.\n    \n    Args:\n        chunks (List[str]): List of all document chunks\n        best_segments (List[Tuple[int, int]]): List of (start, end) indices for segments\n        \n    Returns:\n        List[str]: List of reconstructed text segments\n    \"\"\"\n    reconstructed_segments = []  # Initialize an empty list to store the reconstructed segments\n    \n    for start, end in best_segments:\n        # Join the chunks in this segment to form the complete segment text\n        segment_text = \" \".join(chunks[start:end])\n        # Append the segment text and its range to the reconstructed_segments list\n        reconstructed_segments.append({\n            \"text\": segment_text,\n            \"segment_range\": (start, end),\n        })\n    \n    return reconstructed_segments  # Return the list of reconstructed text segments\n```\n\n----------------------------------------\n\nTITLE: Generating AI Response Based on Retrieved Context in Python\nDESCRIPTION: Function that generates a response to a user query based on retrieved context. It defines system and user prompts, calls the LLM (Llama-3.2-3B-Instruct) through an API, and returns the generated response.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/16_fusion_rag.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef generate_response(query, context):\n    \"\"\"\n    Generate a response based on the query and context.\n    \n    Args:\n        query (str): User query\n        context (str): Context from retrieved documents\n        \n    Returns:\n        str: Generated response\n    \"\"\"\n    # Define the system prompt to guide the AI assistant\n    system_prompt = \"\"\"You are a helpful AI assistant. Answer the user's question based on the provided context. \n    If the context doesn't contain relevant information to answer the question fully, acknowledge this limitation.\"\"\"\n\n    # Format the user prompt with the context and query\n    user_prompt = f\"\"\"Context:\n    {context}\n\n    Question: {query}\n\n    Please answer the question based on the provided context.\"\"\"\n\n    # Generate the response using the OpenAI API\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",  # Specify the model to use\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n            {\"role\": \"user\", \"content\": user_prompt}  # User message with context and query\n        ],\n        temperature=0.1  # Set the temperature for response generation\n    )\n    \n    # Return the generated response\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Calculating Cosine Similarity Between Vectors in Python\nDESCRIPTION: Function that computes the cosine similarity between two vectors. This metric is used to determine how similar two text embeddings are, with values closer to 1 indicating higher similarity.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/4_context_enriched_rag.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef cosine_similarity(vec1, vec2):\n    \"\"\"\n    Calculates the cosine similarity between two vectors.\n\n    Args:\n    vec1 (np.ndarray): The first vector.\n    vec2 (np.ndarray): The second vector.\n\n    Returns:\n    float: The cosine similarity between the two vectors.\n    \"\"\"\n    # Compute the dot product of the two vectors and divide by the product of their norms\n    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n```\n\n----------------------------------------\n\nTITLE: Generating Questions from Text Chunks using LLM\nDESCRIPTION: Function that generates relevant questions from text chunks using an LLM. This is a key enhancement in document augmentation RAG, creating questions that help improve retrieval by anticipating potential user queries.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/6_doc_augmentation_rag.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef generate_questions(text_chunk, num_questions=5, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Generates relevant questions that can be answered from the given text chunk.\n\n    Args:\n    text_chunk (str): The text chunk to generate questions from.\n    num_questions (int): Number of questions to generate.\n    model (str): The model to use for question generation.\n\n    Returns:\n    List[str]: List of generated questions.\n    \"\"\"\n    # Define the system prompt to guide the AI's behavior\n    system_prompt = \"You are an expert at generating relevant questions from text. Create concise questions that can be answered using only the provided text. Focus on key information and concepts.\"\n    \n    # Define the user prompt with the text chunk and the number of questions to generate\n    user_prompt = f\"\"\"\n    Based on the following text, generate {num_questions} different questions that can be answered using only this text:\n\n    {text_chunk}\n    \n    Format your response as a numbered list of questions only, with no additional text.\n    \"\"\"\n    \n    # Generate questions using the OpenAI API\n    response = client.chat.completions.create(\n        model=model,\n        temperature=0.7,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ]\n    )\n    \n    # Extract and clean questions from the response\n    questions_text = response.choices[0].message.content.strip()\n    questions = []\n    \n    # Extract questions using regex pattern matching\n    for line in questions_text.split('\\n'):\n        # Remove numbering and clean up whitespace\n        cleaned_line = re.sub(r'^\\d+\\.\\s*', '', line.strip())\n        if cleaned_line and cleaned_line.endswith('?'):\n            questions.append(cleaned_line)\n    \n    return questions\n```\n\n----------------------------------------\n\nTITLE: Implementing In-Memory Vector Store in Python\nDESCRIPTION: This function adds embeddings and their corresponding text chunks to an in-memory vector store implemented as a dictionary. Each entry in the store contains an embedding and its associated text chunk.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Initialize an in-memory vector store as a dictionary\n# The keys will be unique identifiers (integers), and the values will be dictionaries containing embeddings and corresponding text chunks\nvector_store: dict[int, dict[str, object]] = {}\n\n# Function to add embeddings and corresponding text chunks to the vector store\ndef add_to_vector_store(embeddings: np.ndarray, chunks: List[str]) -> None:\n    \"\"\"\n    Add embeddings and their corresponding text chunks to the vector store.\n\n    Args:\n        embeddings (np.ndarray): A NumPy array containing the embeddings to add.\n        chunks (List[str]): A list of text chunks corresponding to the embeddings.\n\n    Returns:\n        None\n    \"\"\"\n    # Iterate over embeddings and chunks simultaneously\n    for embedding, chunk in zip(embeddings, chunks):\n        # Add each embedding and its corresponding chunk to the vector store\n        # Use the current length of the vector store as the unique key\n        vector_store[len(vector_store)] = {\"embedding\": embedding, \"chunk\": chunk}\n```\n\n----------------------------------------\n\nTITLE: Chunking Text with Contextual Headers for RAG\nDESCRIPTION: This function chunks text into smaller segments and generates headers for each chunk. It uses the generate_chunk_header function to create contextual headers, which is a key component of the CCH technique in RAG.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/5_contextual_chunk_headers_rag.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef chunk_text_with_headers(text, n, overlap):\n    \"\"\"\n    Chunks text into smaller segments and generates headers.\n\n    Args:\n    text (str): The full text to be chunked.\n    n (int): The chunk size in characters.\n    overlap (int): Overlapping characters between chunks.\n\n    Returns:\n    List[dict]: A list of dictionaries with 'header' and 'text' keys.\n    \"\"\"\n    chunks = []  # Initialize an empty list to store chunks\n\n    # Iterate through the text with the specified chunk size and overlap\n    for i in range(0, len(text), n - overlap):\n        chunk = text[i:i + n]  # Extract a chunk of text\n        header = generate_chunk_header(chunk)  # Generate a header for the chunk using LLM\n        chunks.append({\"header\": header, \"text\": chunk})  # Append the header and chunk to the list\n\n    return chunks  # Return the list of chunks with headers\n```\n\n----------------------------------------\n\nTITLE: Evaluating Graph RAG on Sample PDF Document in Python\nDESCRIPTION: This script demonstrates how to use the Graph RAG evaluation function on a sample PDF document. It processes the document, answers a test query, and performs a formal evaluation with a predefined test query and reference answer, displaying summary statistics.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/17_graph_rag.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Path to the PDF document containing AI information\npdf_path = \"data/AI_Information.pdf\"\n\n# Define an AI-related query for testing Graph RAG\nquery = \"What are the key applications of transformers in natural language processing?\"\n\n# Execute the Graph RAG pipeline to process the document and answer the query\nresults = graph_rag_pipeline(pdf_path, query)\n\n# Print the response generated from the Graph RAG system\nprint(\"\\n=== ANSWER ===\")\nprint(results[\"response\"])\n\n# Define a test query and reference answer for formal evaluation\ntest_queries = [\n    \"How do transformers handle sequential data compared to RNNs?\"\n]\n\n# Reference answer for evaluation purposes\nreference_answers = [\n    \"Transformers handle sequential data differently from RNNs by using self-attention mechanisms instead of recurrent connections. This allows transformers to process all tokens in parallel rather than sequentially, capturing long-range dependencies more efficiently and enabling better parallelization during training. Unlike RNNs, transformers don't suffer from vanishing gradient problems with long sequences.\"\n]\n\n# Run formal evaluation of the Graph RAG system with the test query\nevaluation = evaluate_graph_rag(pdf_path, test_queries, reference_answers)\n\n# Print evaluation summary statistics\nprint(\"\\n=== EVALUATION SUMMARY ===\")\nprint(f\"Graph nodes: {evaluation['graph_stats']['nodes']}\")\nprint(f\"Graph edges: {evaluation['graph_stats']['edges']}\")\nfor i, result in enumerate(evaluation['results']):\n    print(f\"\\nQuery {i+1}: {result['query']}\")\n    print(f\"Path length: {result['traversal_path_length']}\")\n    print(f\"Chunks used: {result['relevant_chunks_count']}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing a Simple Vector Store in Python using NumPy\nDESCRIPTION: A basic vector store implementation for managing document chunks and their embeddings. It provides functionality to add items with their embeddings and perform similarity searches using cosine similarity, with optional filtering based on metadata.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/13_self_rag.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleVectorStore:\n    \"\"\"\n    A simple vector store implementation using NumPy.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize the vector store.\n        \"\"\"\n        self.vectors = []  # List to store embedding vectors\n        self.texts = []  # List to store original texts\n        self.metadata = []  # List to store metadata for each text\n    \n    def add_item(self, text, embedding, metadata=None):\n        \"\"\"\n        Add an item to the vector store.\n\n        Args:\n        text (str): The original text.\n        embedding (List[float]): The embedding vector.\n        metadata (dict, optional): Additional metadata.\n        \"\"\"\n        self.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\n        self.texts.append(text)  # Add the original text to texts list\n        self.metadata.append(metadata or {})  # Add metadata to metadata list, default to empty dict if None\n    \n    def similarity_search(self, query_embedding, k=5, filter_func=None):\n        \"\"\"\n        Find the most similar items to a query embedding.\n\n        Args:\n        query_embedding (List[float]): Query embedding vector.\n        k (int): Number of results to return.\n        filter_func (callable, optional): Function to filter results.\n\n        Returns:\n        List[Dict]: Top k most similar items with their texts and metadata.\n        \"\"\"\n        if not self.vectors:\n            return []  # Return empty list if no vectors are stored\n        \n        # Convert query embedding to numpy array\n        query_vector = np.array(query_embedding)\n        \n        # Calculate similarities using cosine similarity\n        similarities = []\n        for i, vector in enumerate(self.vectors):\n            # Apply filter if provided\n            if filter_func and not filter_func(self.metadata[i]):\n                continue\n                \n            # Calculate cosine similarity\n            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n            similarities.append((i, similarity))  # Append index and similarity score\n        \n        # Sort by similarity (descending)\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        \n        # Return top k results\n        results = []\n        for i in range(min(k, len(similarities))):\n            idx, score = similarities[i]\n            results.append({\n                \"text\": self.texts[idx],  # Add the text\n                \"metadata\": self.metadata[idx],  # Add the metadata\n                \"similarity\": score  # Add the similarity score\n            })\n        \n        return results  # Return the list of top k results\n```\n\n----------------------------------------\n\nTITLE: Implementing Vector-Only RAG in Python\nDESCRIPTION: This function performs RAG using only vector-based retrieval. It creates a query embedding, retrieves similar documents, formats the context, and generates a response. It requires a vector store and uses a specified number of documents (k) for retrieval.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/16_fusion_rag.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef vector_only_rag(query, vector_store, k=5):\n    \"\"\"\n    Answer a query using only vector-based RAG.\n    \n    Args:\n        query (str): User query\n        vector_store (SimpleVectorStore): Vector store\n        k (int): Number of documents to retrieve\n        \n    Returns:\n        Dict: Query results\n    \"\"\"\n    # Create query embedding\n    query_embedding = create_embeddings(query)\n    \n    # Retrieve documents using vector-based similarity search\n    retrieved_docs = vector_store.similarity_search_with_scores(query_embedding, k=k)\n    \n    # Format the context from the retrieved documents by joining their text with separators\n    context = \"\\n\\n---\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n    \n    # Generate a response based on the query and the formatted context\n    response = generate_response(query, context)\n    \n    # Return the query, retrieved documents, and the generated response\n    return {\n        \"query\": query,\n        \"retrieved_documents\": retrieved_docs,\n        \"response\": response\n    }\n```\n\n----------------------------------------\n\nTITLE: Implementing a Simple Vector Store for RAG Systems in Python\nDESCRIPTION: Defines a SimpleVectorStore class that demonstrates how query transformations integrate with retrieval. This implementation uses NumPy for vector operations and includes methods for adding items and performing similarity searches using cosine similarity.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/7_query_transform.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleVectorStore:\n    \"\"\"\n    A simple vector store implementation using NumPy.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize the vector store.\n        \"\"\"\n        self.vectors = []  # List to store embedding vectors\n        self.texts = []  # List to store original texts\n        self.metadata = []  # List to store metadata for each text\n    \n    def add_item(self, text, embedding, metadata=None):\n        \"\"\"\n        Add an item to the vector store.\n\n        Args:\n        text (str): The original text.\n        embedding (List[float]): The embedding vector.\n        metadata (dict, optional): Additional metadata.\n        \"\"\"\n        self.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\n        self.texts.append(text)  # Add the original text to texts list\n        self.metadata.append(metadata or {})  # Add metadata to metadata list, use empty dict if None\n    \n    def similarity_search(self, query_embedding, k=5):\n        \"\"\"\n        Find the most similar items to a query embedding.\n\n        Args:\n        query_embedding (List[float]): Query embedding vector.\n        k (int): Number of results to return.\n\n        Returns:\n        List[Dict]: Top k most similar items with their texts and metadata.\n        \"\"\"\n        if not self.vectors:\n            return []  # Return empty list if no vectors are stored\n        \n        # Convert query embedding to numpy array\n        query_vector = np.array(query_embedding)\n        \n        # Calculate similarities using cosine similarity\n        similarities = []\n        for i, vector in enumerate(self.vectors):\n            # Compute cosine similarity between query vector and stored vector\n            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n            similarities.append((i, similarity))  # Append index and similarity score\n        \n        # Sort by similarity (descending)\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        \n        # Return top k results\n        results = []\n        for i in range(min(k, len(similarities))):\n            idx, score = similarities[i]\n            results.append({\n                \"text\": self.texts[idx],  # Add the corresponding text\n                \"metadata\": self.metadata[idx],  # Add the corresponding metadata\n                \"similarity\": score  # Add the similarity score\n            })\n        \n        return results  # Return the list of top k similar items\n```\n\n----------------------------------------\n\nTITLE: Generating Responses from Retrieved Context in Python\nDESCRIPTION: Function that generates a response to a user query based on the retrieved context chunks from graph traversal. It combines context texts, handles length limitations, and generates a response using a large language model (Llama-3.2) with appropriate system prompting.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/17_graph_rag.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef generate_response(query, context_chunks):\n    \"\"\"\n    Generate a response using the retrieved context.\n    \n    Args:\n        query (str): The user's question\n        context_chunks (List[Dict]): Relevant chunks from graph traversal\n        \n    Returns:\n        str: Generated response\n    \"\"\"\n    # Extract text from each chunk in the context\n    context_texts = [chunk[\"text\"] for chunk in context_chunks]\n    \n    # Combine the extracted texts into a single context string, separated by \"---\"\n    combined_context = \"\\n\\n---\\n\\n\".join(context_texts)\n    \n    # Define the maximum allowed length for the context (OpenAI limit)\n    max_context = 14000\n    \n    # Truncate the combined context if it exceeds the maximum length\n    if len(combined_context) > max_context:\n        combined_context = combined_context[:max_context] + \"... [truncated]\"\n    \n    # Define the system message to guide the AI assistant\n    system_message = \"\"\"You are a helpful AI assistant. Answer the user's question based on the provided context.\nIf the information is not in the context, say so. Refer to specific parts of the context in your answer when possible.\"\"\"\n\n    # Generate the response using the OpenAI API\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",  # Specify the model to use\n        messages=[\n            {\"role\": \"system\", \"content\": system_message},  # System message to guide the assistant\n            {\"role\": \"user\", \"content\": f\"Context:\\n{combined_context}\\n\\nQuestion: {query}\"}  # User message with context and query\n        ],\n        temperature=0.2  # Set the temperature for response generation\n    )\n    \n    # Return the generated response content\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings with OpenAI API\nDESCRIPTION: Function to create embeddings for given text using the OpenAI API. It handles both single string and list inputs, and returns the corresponding embeddings.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/10_contextual_compression.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef create_embeddings(text,  model=\"BAAI/bge-en-icl\"):\n    \"\"\"\n    Creates embeddings for the given text.\n\n    Args:\n    text (str or List[str]): The input text(s) for which embeddings are to be created.\n    model (str): The model to be used for creating embeddings.\n\n    Returns:\n    List[float] or List[List[float]]: The embedding vector(s).\n    \"\"\"\n    # Handle both string and list inputs by ensuring input_text is always a list\n    input_text = text if isinstance(text, list) else [text]\n    \n    # Create embeddings for the input text using the specified model\n    response = client.embeddings.create(\n        model=model,\n        input=input_text\n    )\n    \n    # If the input was a single string, return just the first embedding\n    if isinstance(text, str):\n        return response.data[0].embedding\n    \n    # Otherwise, return all embeddings for the list of input texts\n    return [item.embedding for item in response.data]\n```\n\n----------------------------------------\n\nTITLE: Evaluating AI Responses with Different Chunk Sizes in Python\nDESCRIPTION: This snippet demonstrates the usage of the evaluate_response function to assess AI-generated responses with different chunk sizes (256 and 128). It prints the faithfulness and relevancy scores for each chunk size.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/3_chunk_size_selector.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# True answer for the first validation data\ntrue_answer = data[3]['ideal_answer']\n\n# Evaluate response for chunk size 256 and 128\nfaithfulness, relevancy = evaluate_response(query, ai_responses_dict[256], true_answer)\nfaithfulness2, relevancy2 = evaluate_response(query, ai_responses_dict[128], true_answer)\n\n# print the evaluation scores\nprint(f\"Faithfulness Score (Chunk Size 256): {faithfulness}\")\nprint(f\"Relevancy Score (Chunk Size 256): {relevancy}\")\n\nprint(f\"\\n\")\n\nprint(f\"Faithfulness Score (Chunk Size 128): {faithfulness2}\")\nprint(f\"Relevancy Score (Chunk Size 128): {relevancy2}\")\n```\n\n----------------------------------------\n\nTITLE: Creating Text Embeddings with OpenAI API in Python\nDESCRIPTION: Function to generate embeddings for text using the OpenAI API with a specified model. Embeddings convert text into numerical vectors that capture semantic meaning, enabling similarity-based retrieval.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/1_simple_rag.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n    \"\"\"\n    Creates embeddings for the given text using the specified OpenAI model.\n\n    Args:\n    text (str): The input text for which embeddings are to be created.\n    model (str): The model to be used for creating embeddings. Default is \"BAAI/bge-en-icl\".\n\n    Returns:\n    dict: The response from the OpenAI API containing the embeddings.\n    \"\"\"\n    # Create embeddings for the input text using the specified model\n    response = client.embeddings.create(\n        model=model,\n        input=text\n    )\n\n    return response  # Return the response containing the embeddings\n\n# Create embeddings for the text chunks\nresponse = create_embeddings(text_chunks)\n```\n\n----------------------------------------\n\nTITLE: Embedding Generation Function\nDESCRIPTION: Function to create embeddings for input text using the specified OpenAI model. Handles both single string and list inputs.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/8_reranker.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n    \"\"\"\n    Creates embeddings for the given text using the specified OpenAI model.\n\n    Args:\n    text (str): The input text for which embeddings are to be created.\n    model (str): The model to be used for creating embeddings.\n\n    Returns:\n    List[float]: The embedding vector.\n    \"\"\"\n    # Handle both string and list inputs by converting string input to a list\n    input_text = text if isinstance(text, list) else [text]\n    \n    # Create embeddings for the input text using the specified model\n    response = client.embeddings.create(\n        model=model,\n        input=input_text\n    )\n    \n    # If input was a string, return just the first embedding\n    if isinstance(text, str):\n        return response.data[0].embedding\n    \n    # Otherwise, return all embeddings as a list of vectors\n    return [item.embedding for item in response.data]\n```\n\n----------------------------------------\n\nTITLE: Response Generation Using LLM\nDESCRIPTION: Function to generate responses using a Llama LLM model based on retrieved results. Supports both proposition and chunk-based approaches.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/14_proposition_chunking.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef generate_response(query, results, result_type=\"proposition\"):\n    \"\"\"\n    Generate a response based on retrieved results.\n    \n    Args:\n        query (str): User query\n        results (List[Dict]): Retrieved items\n        result_type (str): Type of results ('proposition' or 'chunk')\n        \n    Returns:\n        str: Generated response\n    \"\"\"\n    # Combine retrieved texts into a single context string\n    context = \"\\n\\n\".join([result[\"text\"] for result in results])\n    \n    # System prompt to instruct the AI on how to generate the response\n    system_prompt = f\"\"\"You are an AI assistant answering questions based on retrieved information.\nYour answer should be based on the following {result_type}s that were retrieved from a knowledge base.\nIf the retrieved information doesn't answer the question, acknowledge this limitation.\"\"\"\n\n    # User prompt containing the query and the retrieved context\n    user_prompt = f\"\"\"Query: {query}\n\nRetrieved {result_type}s:\n{context}\n\nPlease answer the query based on the retrieved information.\"\"\"\n\n    # Generate the response using the OpenAI client\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0.2\n    )\n    \n    # Return the generated response text\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Policy Network Implementation for Action Selection in Python\nDESCRIPTION: Implementation of a policy network that uses epsilon-greedy strategy to select actions based on the current state. It balances between exploration and exploitation while choosing actions.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndef policy_network(state: dict, action_space: List[str], epsilon: float = 0.2) -> str:\n    \"\"\"\n    Define a policy network to select an action based on the current state using an epsilon-greedy strategy.\n\n    Args:\n        state (dict): The current state of the environment, including query, context, responses, and rewards.\n        action_space (List[str]): The list of possible actions the agent can take.\n        epsilon (float): The probability of choosing a random action for exploration. Default is 0.2.\n\n    Returns:\n        str: The selected action from the action space.\n    \"\"\"\n    # Use epsilon-greedy strategy: random exploration vs. exploitation\n    if np.random.random() < epsilon:\n        # Exploration: randomly select an action from the action space\n        action = np.random.choice(action_space)\n    else:\n        # Exploitation: select the best action based on the current state using a simple heuristic\n\n        # If there are no previous responses, prioritize rewriting the query\n        if len(state[\"previous_responses\"]) == 0:\n            action = \"rewrite_query\"\n        # If there are previous responses but the rewards are low, try expanding the context\n        elif state[\"previous_rewards\"] and max(state[\"previous_rewards\"]) < 0.7:\n            action = \"expand_context\"\n        # If the context has too many chunks, try filtering the context\n        elif len(state[\"context\"]) > 5:\n            action = \"filter_context\"\n        # Otherwise, generate a response\n        else:\n            action = \"generate_response\"\n    \n    return action\n```\n\n----------------------------------------\n\nTITLE: Generating Chunk Headers Using LLM for CCH\nDESCRIPTION: This function uses a language model to generate descriptive headers for text chunks. It takes a chunk of text as input and returns a concise title, which is crucial for creating contextual chunk headers in the RAG process.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/5_contextual_chunk_headers_rag.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef generate_chunk_header(chunk, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Generates a title/header for a given text chunk using an LLM.\n\n    Args:\n    chunk (str): The text chunk to summarize as a header.\n    model (str): The model to be used for generating the header. Default is \"meta-llama/Llama-3.2-3B-Instruct\".\n\n    Returns:\n    str: Generated header/title.\n    \"\"\"\n    # Define the system prompt to guide the AI's behavior\n    system_prompt = \"Generate a concise and informative title for the given text.\"\n    \n    # Generate a response from the AI model based on the system prompt and text chunk\n    response = client.chat.completions.create(\n        model=model,\n        temperature=0,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": chunk}\n        ]\n    )\n\n    # Return the generated header/title, stripping any leading/trailing whitespace\n    return response.choices[0].message.content.strip()\n```\n\n----------------------------------------\n\nTITLE: User Feedback Collection and Storage Implementation\nDESCRIPTION: Implements functions for collecting, formatting, storing, and loading user feedback data using JSON format.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/11_feedback_loop_rag.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef get_user_feedback(query, response, relevance, quality, comments=\"\"):\n    \"\"\"\n    Format user feedback in a dictionary.\n    \n    Args:\n        query (str): User's query\n        response (str): System's response\n        relevance (int): Relevance score (1-5)\n        quality (int): Quality score (1-5)\n        comments (str): Optional feedback comments\n        \n    Returns:\n        Dict: Formatted feedback\n    \"\"\"\n    return {\n        \"query\": query,\n        \"response\": response,\n        \"relevance\": int(relevance),\n        \"quality\": int(quality),\n        \"comments\": comments,\n        \"timestamp\": datetime.now().isoformat()\n    }\n```\n\n----------------------------------------\n\nTITLE: Implementing a Simple Vector Store for RAG in Python\nDESCRIPTION: A basic vector store implementation using NumPy for managing document chunks and their embeddings. It supports adding items and performing similarity searches based on cosine similarity.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/12_adaptive_rag.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleVectorStore:\n    \"\"\"\n    A simple vector store implementation using NumPy.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize the vector store.\n        \"\"\"\n        self.vectors = []  # List to store embedding vectors\n        self.texts = []  # List to store original texts\n        self.metadata = []  # List to store metadata for each text\n    \n    def add_item(self, text, embedding, metadata=None):\n        \"\"\"\n        Add an item to the vector store.\n\n        Args:\n        text (str): The original text.\n        embedding (List[float]): The embedding vector.\n        metadata (dict, optional): Additional metadata.\n        \"\"\"\n        self.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\n        self.texts.append(text)  # Add the original text to texts list\n        self.metadata.append(metadata or {})  # Add metadata to metadata list, default to empty dict if None\n    \n    def similarity_search(self, query_embedding, k=5, filter_func=None):\n        \"\"\"\n        Find the most similar items to a query embedding.\n\n        Args:\n        query_embedding (List[float]): Query embedding vector.\n        k (int): Number of results to return.\n        filter_func (callable, optional): Function to filter results.\n\n        Returns:\n        List[Dict]: Top k most similar items with their texts and metadata.\n        \"\"\"\n        if not self.vectors:\n            return []  # Return empty list if no vectors are stored\n        \n        # Convert query embedding to numpy array\n        query_vector = np.array(query_embedding)\n        \n        # Calculate similarities using cosine similarity\n        similarities = []\n        for i, vector in enumerate(self.vectors):\n            # Apply filter if provided\n            if filter_func and not filter_func(self.metadata[i]):\n                continue\n                \n            # Calculate cosine similarity\n            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n            similarities.append((i, similarity))  # Append index and similarity score\n        \n        # Sort by similarity (descending)\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        \n        # Return top k results\n        results = []\n        for i in range(min(k, len(similarities))):\n            idx, score = similarities[i]\n            results.append({\n                \"text\": self.texts[idx],  # Add the text\n                \"metadata\": self.metadata[idx],  # Add the metadata\n                \"similarity\": score  # Add the similarity score\n            })\n        \n        return results  # Return the list of top k results\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from PDF Documents in Python\nDESCRIPTION: Function to extract text content from a PDF file using PyMuPDF (fitz) library. It iterates through each page in the PDF, extracts the text, and returns the combined content.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/1_simple_rag.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef extract_text_from_pdf(pdf_path):\n    \"\"\"\n    Extracts text from a PDF file and prints the first `num_chars` characters.\n\n    Args:\n    pdf_path (str): Path to the PDF file.\n\n    Returns:\n    str: Extracted text from the PDF.\n    \"\"\"\n    # Open the PDF file\n    mypdf = fitz.open(pdf_path)\n    all_text = \"\"  # Initialize an empty string to store the extracted text\n\n    # Iterate through each page in the PDF\n    for page_num in range(mypdf.page_count):\n        page = mypdf[page_num]  # Get the page\n        text = page.get_text(\"text\")  # Extract text from the page\n        all_text += text  # Append the extracted text to the all_text string\n\n    return all_text  # Return the extracted text\n```\n\n----------------------------------------\n\nTITLE: Evaluating AI Response Quality in Python\nDESCRIPTION: Code that defines a system prompt for evaluating the quality of AI responses. The prompt establishes a scoring system to assess how well the AI's answer aligns with the expected response.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/4_context_enriched_rag.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Define the system prompt for the evaluation system\nevaluate_system_prompt = \"You are an intelligent evaluation system tasked with assessing the AI assistant's responses. If the AI assistant's response is very close to the true response, assign a score of 1. If the response is incorrect or unsatisfactory in relation to the true response, assign a score of 0. If the response is partially aligned with the true response, assign a score of 0.5.\"\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from PDF Documents\nDESCRIPTION: Function to extract text content from PDF files using PyMuPDF library, processing the document page by page and concatenating the results.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/14_proposition_chunking.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef extract_text_from_pdf(pdf_path):\n    \"\"\"\n    Extracts text from a PDF file and prints the first `num_chars` characters.\n\n    Args:\n    pdf_path (str): Path to the PDF file.\n\n    Returns:\n    str: Extracted text from the PDF.\n    \"\"\"\n    # Open the PDF file\n    mypdf = fitz.open(pdf_path)\n    all_text = \"\"  # Initialize an empty string to store the extracted text\n\n    # Iterate through each page in the PDF\n    for page_num in range(mypdf.page_count):\n        page = mypdf[page_num]  # Get the page\n        text = page.get_text(\"text\")  # Extract text from the page\n        all_text += text  # Append the extracted text to the all_text string\n\n    return all_text  # Return the extracted text\n```\n\n----------------------------------------\n\nTITLE: Creating Embeddings with OpenAI API in Python\nDESCRIPTION: Defines a function to create embeddings for a list of texts using the OpenAI API. It handles batching to comply with API limits and processes large collections of text efficiently.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/19_HyDE_rag.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef create_embeddings(texts, model=\"BAAI/bge-en-icl\"):\n    \"\"\"\n    Create embeddings for the given texts.\n    \n    Args:\n        texts (List[str]): Input texts\n        model (str): Embedding model name\n        \n    Returns:\n        List[List[float]]: Embedding vectors\n    \"\"\"\n    # Handle empty input\n    if not texts:\n        return []\n        \n    # Process in batches if needed (OpenAI API limits)\n    batch_size = 100\n    all_embeddings = []\n    \n    # Iterate over the input texts in batches\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i + batch_size]  # Get the current batch of texts\n        \n        # Create embeddings for the current batch\n        response = client.embeddings.create(\n            model=model,\n            input=batch\n        )\n        \n        # Extract embeddings from the response\n        batch_embeddings = [item.embedding for item in response.data]\n        all_embeddings.extend(batch_embeddings)  # Add the batch embeddings to the list\n    \n    return all_embeddings  # Return all embeddings\n```\n\n----------------------------------------\n\nTITLE: Vector Store Implementation\nDESCRIPTION: Implements a simple vector store class using NumPy for managing document chunks and their embeddings. Includes methods for adding items and performing similarity searches.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/11_feedback_loop_rag.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleVectorStore:\n    \"\"\"\n    A simple vector store implementation using NumPy.\n    \n    This class provides an in-memory storage and retrieval system for \n    embedding vectors and their corresponding text chunks and metadata.\n    It supports basic similarity search functionality using cosine similarity.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize the vector store with empty lists for vectors, texts, and metadata.\n        \n        The vector store maintains three parallel lists:\n        - vectors: NumPy arrays of embedding vectors\n        - texts: Original text chunks corresponding to each vector\n        - metadata: Optional metadata dictionaries for each item\n        \"\"\"\n        self.vectors = []  # List to store embedding vectors\n        self.texts = []    # List to store original text chunks\n        self.metadata = [] # List to store metadata for each text chunk\n    \n    def add_item(self, text, embedding, metadata=None):\n        \"\"\"\n        Add an item to the vector store.\n\n        Args:\n            text (str): The original text chunk to store.\n            embedding (List[float]): The embedding vector representing the text.\n            metadata (dict, optional): Additional metadata for the text chunk,\n                                      such as source, timestamp, or relevance scores.\n        \"\"\"\n        self.vectors.append(np.array(embedding))  # Convert and store the embedding\n        self.texts.append(text)                   # Store the original text\n        self.metadata.append(metadata or {})      # Store metadata (empty dict if None)\n    \n    def similarity_search(self, query_embedding, k=5, filter_func=None):\n        \"\"\"\n        Find the most similar items to a query embedding using cosine similarity.\n\n        Args:\n            query_embedding (List[float]): Query embedding vector to compare against stored vectors.\n            k (int): Number of most similar results to return.\n            filter_func (callable, optional): Function to filter results based on metadata.\n                                             Takes metadata dict as input and returns boolean.\n\n        Returns:\n            List[Dict]: Top k most similar items, each containing:\n                - text: The original text\n                - metadata: Associated metadata\n                - similarity: Raw cosine similarity score\n                - relevance_score: Either metadata-based relevance or calculated similarity\n                \n        Note: Returns empty list if no vectors are stored or none pass the filter.\n        \"\"\"\n        if not self.vectors:\n            return []  # Return empty list if vector store is empty\n        \n        # Convert query embedding to numpy array for vector operations\n        query_vector = np.array(query_embedding)\n        \n        # Calculate cosine similarity between query and each stored vector\n        similarities = []\n        for i, vector in enumerate(self.vectors):\n            # Skip items that don't pass the filter criteria\n            if filter_func and not filter_func(self.metadata[i]):\n                continue\n                \n            # Calculate cosine similarity: dot product / (norm1 * norm2)\n            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n            similarities.append((i, similarity))  # Store index and similarity score\n        \n        # Sort results by similarity score in descending order\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        \n        # Construct result dictionaries for the top k matches\n        results = []\n        for i in range(min(k, len(similarities))):\n            idx, score = similarities[i]\n            results.append({\n                \"text\": self.texts[idx],\n                \"metadata\": self.metadata[idx],\n                \"similarity\": score,\n                # Use pre-existing relevance score from metadata if available, otherwise use similarity\n                \"relevance_score\": self.metadata[idx].get(\"relevance_score\", score)\n            })\n        \n        return results\n```\n\n----------------------------------------\n\nTITLE: Processing Documents for RAG with Feedback Awareness in Python\nDESCRIPTION: This function handles the complete document processing pipeline for RAG, including text extraction from PDF, text chunking, embedding creation, and storage in a vector database with metadata for feedback-based improvements.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/11_feedback_loop_rag.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n    \"\"\"\n    Process a document for RAG (Retrieval Augmented Generation) with feedback loop.\n    This function handles the complete document processing pipeline:\n    1. Text extraction from PDF\n    2. Text chunking with overlap\n    3. Embedding creation for chunks\n    4. Storage in vector database with metadata\n\n    Args:\n    pdf_path (str): Path to the PDF file to process.\n    chunk_size (int): Size of each text chunk in characters.\n    chunk_overlap (int): Number of overlapping characters between consecutive chunks.\n\n    Returns:\n    Tuple[List[str], SimpleVectorStore]: A tuple containing:\n        - List of document chunks\n        - Populated vector store with embeddings and metadata\n    \"\"\"\n    # Step 1: Extract raw text content from the PDF document\n    print(\"Extracting text from PDF...\")\n    extracted_text = extract_text_from_pdf(pdf_path)\n    \n    # Step 2: Split text into manageable, overlapping chunks for better context preservation\n    print(\"Chunking text...\")\n    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n    print(f\"Created {len(chunks)} text chunks\")\n    \n    # Step 3: Generate vector embeddings for each text chunk\n    print(\"Creating embeddings for chunks...\")\n    chunk_embeddings = create_embeddings(chunks)\n    \n    # Step 4: Initialize the vector database to store chunks and their embeddings\n    store = SimpleVectorStore()\n    \n    # Step 5: Add each chunk with its embedding to the vector store\n    # Include metadata for feedback-based improvements\n    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n        store.add_item(\n            text=chunk,\n            embedding=embedding,\n            metadata={\n                \"index\": i,                # Position in original document\n                \"source\": pdf_path,        # Source document path\n                \"relevance_score\": 1.0,    # Initial relevance score (will be updated with feedback)\n                \"feedback_count\": 0        # Counter for feedback received on this chunk\n            }\n        )\n    \n    print(f\"Added {len(chunks)} chunks to the vector store\")\n    return chunks, store\n```\n\n----------------------------------------\n\nTITLE: Loading and Evaluating RAG with Validation Data in Python\nDESCRIPTION: Code snippet for loading validation data from a JSON file and testing the RAG pipeline with a sample query. This section demonstrates how to evaluate the pipeline's performance by comparing its responses with expected answers.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# Open the validation data file in read mode and load its content as a dictionary\nwith open('data/val_rl.json', 'r') as file:\n    validation_data = json.load(file)\n\n# Test the basic RAG pipeline with a sample query\nsample_query = validation_data['basic_factual_questions'][0]['question']  # Extract the query text\nexpected_answer = validation_data['basic_factual_questions'][0]['answer']  # Extract the ground truth answer\n\n# print the sample query and expected answer\nprint(f\"Sample Query: {sample_query}\\n\")\nprint(f\"Expected Answer: {expected_answer}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Simple Vector Store for RAG in Python\nDESCRIPTION: This class implements a simple vector store using NumPy for RAG systems. It provides methods to add items (text, embedding, and metadata) and perform similarity searches using cosine similarity, with optional filtering.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/18_hierarchy_rag.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleVectorStore:\n    \"\"\"\n    A simple vector store implementation using NumPy.\n    \"\"\"\n    def __init__(self):\n        self.vectors = []  # List to store vector embeddings\n        self.texts = []  # List to store text content\n        self.metadata = []  # List to store metadata\n    \n    def add_item(self, text, embedding, metadata=None):\n        \"\"\"\n        Add an item to the vector store.\n        \n        Args:\n            text (str): Text content\n            embedding (List[float]): Vector embedding\n            metadata (Dict, optional): Additional metadata\n        \"\"\"\n        self.vectors.append(np.array(embedding))  # Append the embedding as a numpy array\n        self.texts.append(text)  # Append the text content\n        self.metadata.append(metadata or {})  # Append the metadata or an empty dict if None\n    \n    def similarity_search(self, query_embedding, k=5, filter_func=None):\n        \"\"\"\n        Find the most similar items to a query embedding.\n        \n        Args:\n            query_embedding (List[float]): Query embedding vector\n            k (int): Number of results to return\n            filter_func (callable, optional): Function to filter results\n            \n        Returns:\n            List[Dict]: Top k most similar items\n        \"\"\"\n        if not self.vectors:\n            return []  # Return an empty list if there are no vectors\n        \n        # Convert query embedding to numpy array\n        query_vector = np.array(query_embedding)\n        \n        # Calculate similarities using cosine similarity\n        similarities = []\n        for i, vector in enumerate(self.vectors):\n            # Skip if doesn't pass the filter\n            if filter_func and not filter_func(self.metadata[i]):\n                continue\n                \n            # Calculate cosine similarity\n            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n            similarities.append((i, similarity))  # Append index and similarity score\n        \n        # Sort by similarity (descending)\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        \n        # Return top k results\n        results = []\n        for i in range(min(k, len(similarities))):\n            idx, score = similarities[i]\n            results.append({\n                \"text\": self.texts[idx],  # Add the text content\n                \"metadata\": self.metadata[idx],  # Add the metadata\n                \"similarity\": float(score)  # Add the similarity score\n            })\n        \n        return results  # Return the list of top k results\n```\n\n----------------------------------------\n\nTITLE: DuckDuckGo Web Search Implementation\nDESCRIPTION: Function that performs web searches using DuckDuckGo's API with fallback to SerpAPI. Returns search results and source metadata. Includes error handling and backup search functionality.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/20_crag.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef duck_duck_go_search(query, num_results=3):\n    \"\"\"\n    Perform a web search using DuckDuckGo.\n    \n    Args:\n        query (str): Search query\n        num_results (int): Number of results to return\n        \n    Returns:\n        Tuple[str, List[Dict]]: Combined search results text and source metadata\n    \"\"\"\n    # Encode the query for URL\n    encoded_query = quote_plus(query)\n    \n    # DuckDuckGo search API endpoint (unofficial)\n    url = f\"https://api.duckduckgo.com/?q={encoded_query}&format=json\"\n    \n    try:\n        # Perform the web search request\n        response = requests.get(url, headers={\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n        })\n        data = response.json()\n        \n        # Initialize variables to store results text and sources\n        results_text = \"\"\n        sources = []\n        \n        # Add abstract if available\n        if data.get(\"AbstractText\"):\n            results_text += f\"{data['AbstractText']}\\n\\n\"\n            sources.append({\n                \"title\": data.get(\"AbstractSource\", \"Wikipedia\"),\n                \"url\": data.get(\"AbstractURL\", \"\")\n            })\n        \n        # Add related topics\n        for topic in data.get(\"RelatedTopics\", [])[:num_results]:\n            if \"Text\" in topic and \"FirstURL\" in topic:\n                results_text += f\"{topic['Text']}\\n\\n\"\n                sources.append({\n                    \"title\": topic.get(\"Text\", \"\").split(\" - \")[0],\n                    \"url\": topic.get(\"FirstURL\", \"\")\n                })\n        \n        return results_text, sources\n    \n    except Exception as e:\n        # Print error message if the main search fails\n        print(f\"Error performing web search: {e}\")\n        \n        # Fallback to a backup search API\n        try:\n            backup_url = f\"https://serpapi.com/search.json?q={encoded_query}&engine=duckduckgo\"\n            response = requests.get(backup_url)\n            data = response.json()\n            \n            # Initialize variables to store results text and sources\n            results_text = \"\"\n            sources = []\n            \n            # Extract results from the backup API\n            for result in data.get(\"organic_results\", [])[:num_results]:\n                results_text += f\"{result.get('title', '')}: {result.get('snippet', '')}\\n\\n\"\n                sources.append({\n                    \"title\": result.get(\"title\", \"\"),\n                    \"url\": result.get(\"link\", \"\")\n                })\n            \n            return results_text, sources\n        except Exception as backup_error:\n            # Print error message if the backup search also fails\n            print(f\"Backup search also failed: {backup_error}\")\n            return \"Failed to retrieve search results.\", []\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from PDF using PyMuPDF in Python\nDESCRIPTION: Function that extracts text from a PDF file using the PyMuPDF library. It iterates through each page of the PDF, extracts the text, and combines it into a single string.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/4_context_enriched_rag.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef extract_text_from_pdf(pdf_path):\n    \"\"\"\n    Extracts text from a PDF file and prints the first `num_chars` characters.\n\n    Args:\n    pdf_path (str): Path to the PDF file.\n\n    Returns:\n    str: Extracted text from the PDF.\n    \"\"\"\n    # Open the PDF file\n    mypdf = fitz.open(pdf_path)\n    all_text = \"\"  # Initialize an empty string to store the extracted text\n\n    # Iterate through each page in the PDF\n    for page_num in range(mypdf.page_count):\n        page = mypdf[page_num]  # Get the page\n        text = page.get_text(\"text\")  # Extract text from the page\n        all_text += text  # Append the extracted text to the all_text string\n\n    return all_text  # Return the extracted text\n```\n\n----------------------------------------\n\nTITLE: Testing Hierarchical RAG Query Processing in Python\nDESCRIPTION: Python code that demonstrates testing of a hierarchical RAG system through example queries and formal evaluation. The code includes running a test query about transformer models, comparing results against reference answers, and evaluating performance differences between hierarchical and standard RAG approaches.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/18_hierarchy_rag.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Example query about AI for testing the hierarchical RAG approach\nquery = \"What are the key applications of transformer models in natural language processing?\"\nresult = hierarchical_rag(query, pdf_path)\n\nprint(\"\\n=== Response ===\")\nprint(result[\"response\"])\n\n# Test query for formal evaluation (using only one query as requested)\ntest_queries = [\n    \"How do transformers handle sequential data compared to RNNs?\"\n]\n\n# Reference answer for the test query to enable comparison\nreference_answers = [\n    \"Transformers handle sequential data differently from RNNs by using self-attention mechanisms instead of recurrent connections. This allows transformers to process all tokens in parallel rather than sequentially, capturing long-range dependencies more efficiently and enabling better parallelization during training. Unlike RNNs, transformers don't suffer from vanishing gradient problems with long sequences.\"\n]\n\n# Run the evaluation comparing hierarchical and standard RAG approaches\nevaluation_results = run_evaluation(\n    pdf_path=pdf_path,\n    test_queries=test_queries,\n    reference_answers=reference_answers\n)\n\n# Print the overall analysis of the comparison\nprint(\"\\n=== OVERALL ANALYSIS ===\")\nprint(evaluation_results[\"overall_analysis\"])\n```\n\n----------------------------------------\n\nTITLE: Creating Text Embeddings using AI Models in Python\nDESCRIPTION: This function creates embeddings for text using a specified AI model (default is BAAI/bge-en-icl). It sends the text to the client's embeddings API and returns the response containing the vector representations.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/6_doc_augmentation_rag.ipynb#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n    \"\"\"\n    Creates embeddings for the given text using the specified OpenAI model.\n\n    Args:\n    text (str): The input text for which embeddings are to be created.\n    model (str): The model to be used for creating embeddings. Default is \"BAAI/bge-en-icl\".\n\n    Returns:\n    dict: The response from the OpenAI API containing the embeddings.\n    \"\"\"\n    # Create embeddings for the input text using the specified model\n    response = client.embeddings.create(\n        model=model,\n        input=text\n    )\n\n    return response  # Return the response containing the embeddings\n\n# Create embeddings for the text chunks\nresponse = create_embeddings(text_chunks)\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from PDF Documents in Python\nDESCRIPTION: Defines a function to extract text content from PDF files using PyMuPDF with page separation. It filters out pages with minimal text and returns a list of pages with their text content and metadata.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/19_HyDE_rag.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef extract_text_from_pdf(pdf_path):\n    \"\"\"\n    Extract text content from a PDF file with page separation.\n    \n    Args:\n        pdf_path (str): Path to the PDF file\n        \n    Returns:\n        List[Dict]: List of pages with text content and metadata\n    \"\"\"\n    print(f\"Extracting text from {pdf_path}...\")  # Print the path of the PDF being processed\n    pdf = fitz.open(pdf_path)  # Open the PDF file using PyMuPDF\n    pages = []  # Initialize an empty list to store the pages with text content\n    \n    # Iterate over each page in the PDF\n    for page_num in range(len(pdf)):\n        page = pdf[page_num]  # Get the current page\n        text = page.get_text()  # Extract text from the current page\n        \n        # Skip pages with very little text (less than 50 characters)\n        if len(text.strip()) > 50:\n            # Append the page text and metadata to the list\n            pages.append({\n                \"text\": text,\n                \"metadata\": {\n                    \"source\": pdf_path,  # Source file path\n                    \"page\": page_num + 1  # Page number (1-based index)\n                }\n            })\n    \n    print(f\"Extracted {len(pages)} pages with content\")  # Print the number of pages extracted\n    return pages  # Return the list of pages with text content and metadata\n```\n\n----------------------------------------\n\nTITLE: Generating Final Response from Retrieved Chunks in Python\nDESCRIPTION: This function uses an LLM to generate a final response based on the original query and retrieved document chunks. It concatenates the text from all retrieved chunks to create a context, then prompts the model to answer the question based on this context using the Llama-3.2-3B-Instruct model.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/19_HyDE_rag.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef generate_response(query, relevant_chunks):\n    \"\"\"\n    Generate a final response based on the query and relevant chunks.\n    \n    Args:\n        query (str): User query\n        relevant_chunks (List[Dict]): Retrieved relevant chunks\n        \n    Returns:\n        str: Generated response\n    \"\"\"\n    # Concatenate the text from the chunks to create context\n    context = \"\\n\\n\".join([chunk[\"text\"] for chunk in relevant_chunks])\n    \n    # Generate response using OpenAI API\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer the question based on the provided context.\"},\n            {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {query}\"}\n        ],\n        temperature=0.5,\n        max_tokens=500\n    )\n    \n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Document Relevance Scoring using LLM in Python\nDESCRIPTION: A function that uses an LLM to score the relevance of a document to a specific query. It returns a numeric score between 0-10 indicating how well the document addresses the query.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/12_adaptive_rag.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef score_document_relevance(query, document, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Score document relevance to a query using LLM.\n    \n    Args:\n        query (str): User query\n        document (str): Document text\n        model (str): LLM model\n        \n    Returns:\n        float: Relevance score from 0-10\n    \"\"\"\n    # System prompt to instruct the model on how to rate relevance\n    system_prompt = \"\"\"You are an expert at evaluating document relevance.\n        Rate the relevance of a document to a query on a scale from 0 to 10, where:\n        0 = Completely irrelevant\n        10 = Perfectly addresses the query\n\n        Return ONLY a numerical score between 0 and 10, nothing else.\n    \"\"\"\n\n    # Truncate document if it's too long\n    doc_preview = document[:1500] + \"...\" if len(document) > 1500 else document\n    \n    # User prompt containing the query and document preview\n    user_prompt = f\"\"\"\n        Query: {query}\n\n        Document: {doc_preview}\n\n        Relevance score (0-10):\n    \"\"\"\n    \n    # Generate response from the model\n    response = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0\n    )\n    \n    # Extract the score from the model's response\n    score_text = response.choices[0].message.content.strip()\n    \n    # Extract numeric score using regex\n    match = re.search(r'(\\d+(\\.\\d+)?)', score_text)\n    if match:\n        score = float(match.group(1))\n        return min(10, max(0, score))  # Ensure score is within 0-10\n    else:\n        # Default score if extraction fails\n        return 5.0\n```\n\n----------------------------------------\n\nTITLE: Implementing Simple Vector Store in Python\nDESCRIPTION: Creates a simple vector store class using NumPy for storing and retrieving document embeddings. It provides methods to add items with their embeddings and perform similarity search based on cosine similarity.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/19_HyDE_rag.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleVectorStore:\n    \"\"\"\n    A simple vector store implementation using NumPy.\n    \"\"\"\n    def __init__(self):\n        self.vectors = []  # List to store vector embeddings\n        self.texts = []  # List to store text content\n        self.metadata = []  # List to store metadata\n    \n    def add_item(self, text, embedding, metadata=None):\n        \"\"\"\n        Add an item to the vector store.\n        \n        Args:\n            text (str): Text content\n            embedding (List[float]): Vector embedding\n            metadata (Dict, optional): Additional metadata\n        \"\"\"\n        self.vectors.append(np.array(embedding))  # Append the embedding as a numpy array\n        self.texts.append(text)  # Append the text content\n        self.metadata.append(metadata or {})  # Append the metadata or an empty dict if None\n    \n    def similarity_search(self, query_embedding, k=5, filter_func=None):\n        \"\"\"\n        Find the most similar items to a query embedding.\n        \n        Args:\n            query_embedding (List[float]): Query embedding vector\n            k (int): Number of results to return\n            filter_func (callable, optional): Function to filter results\n            \n        Returns:\n            List[Dict]: Top k most similar items\n        \"\"\"\n        if not self.vectors:\n            return []  # Return an empty list if there are no vectors\n        \n        # Convert query embedding to numpy array\n        query_vector = np.array(query_embedding)\n        \n        # Calculate similarities using cosine similarity\n        similarities = []\n        for i, vector in enumerate(self.vectors):\n            # Skip if doesn't pass the filter\n            if filter_func and not filter_func(self.metadata[i]):\n                continue\n                \n            # Calculate cosine similarity\n            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n            similarities.append((i, similarity))  # Append index and similarity score\n        \n        # Sort by similarity (descending)\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        \n        # Return top k results\n        results = []\n        for i in range(min(k, len(similarities))):\n            idx, score = similarities[i]\n            results.append({\n                \"text\": self.texts[idx],  # Add the text content\n                \"metadata\": self.metadata[idx],  # Add the metadata\n                \"similarity\": float(score)  # Add the similarity score\n            })\n        \n        return results  # Return the list of top k results\n```\n\n----------------------------------------\n\nTITLE: Simple Vector Store Implementation\nDESCRIPTION: A lightweight vector store class using NumPy for storing and searching document embeddings with metadata support.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/9_rse.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleVectorStore:\n    \"\"\"\n    A lightweight vector store implementation using NumPy.\n    \"\"\"\n    def __init__(self, dimension=1536):\n        \"\"\"\n        Initialize the vector store.\n        \n        Args:\n            dimension (int): Dimension of embeddings\n        \"\"\"\n        self.dimension = dimension\n        self.vectors = []\n        self.documents = []\n        self.metadata = []\n    \n    def add_documents(self, documents, vectors=None, metadata=None):\n        \"\"\"\n        Add documents to the vector store.\n        \n        Args:\n            documents (List[str]): List of document chunks\n            vectors (List[List[float]], optional): List of embedding vectors\n            metadata (List[Dict], optional): List of metadata dictionaries\n        \"\"\"\n        if vectors is None:\n            vectors = [None] * len(documents)\n        \n        if metadata is None:\n            metadata = [{} for _ in range(len(documents))]\n        \n        for doc, vec, meta in zip(documents, vectors, metadata):\n            self.documents.append(doc)\n            self.vectors.append(vec)\n            self.metadata.append(meta)\n    \n    def search(self, query_vector, top_k=5):\n        \"\"\"\n        Search for most similar documents.\n        \n        Args:\n            query_vector (List[float]): Query embedding vector\n            top_k (int): Number of results to return\n            \n        Returns:\n            List[Dict]: List of results with documents, scores, and metadata\n        \"\"\"\n        if not self.vectors or not self.documents:\n            return []\n        \n        # Convert query vector to numpy array\n        query_array = np.array(query_vector)\n        \n        # Calculate similarities\n        similarities = []\n        for i, vector in enumerate(self.vectors):\n            if vector is not None:\n                # Compute cosine similarity\n                similarity = np.dot(query_array, vector) / (\n                    np.linalg.norm(query_array) * np.linalg.norm(vector)\n                )\n                similarities.append((i, similarity))\n        \n        # Sort by similarity (descending)\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        \n        # Get top-k results\n        results = []\n        for i, score in similarities[:top_k]:\n            results.append({\n                \"document\": self.documents[i],\n                \"score\": float(score),\n                \"metadata\": self.metadata[i]\n            })\n        \n        return results\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for CCH RAG Implementation\nDESCRIPTION: This snippet imports necessary libraries for implementing Contextual Chunk Headers in Retrieval-Augmented Generation. It includes libraries for data manipulation, OpenAI API interaction, PDF processing, and progress tracking.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/5_contextual_chunk_headers_rag.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport numpy as np\nimport json\nfrom openai import OpenAI\nimport fitz\nfrom tqdm import tqdm\n```\n\n----------------------------------------\n\nTITLE: Executing Proposition Chunking Evaluation for AI Information Document in Python\nDESCRIPTION: This code snippet demonstrates how to execute the proposition chunking evaluation on an AI information document. It defines test queries, reference answers, and runs the evaluation pipeline, printing the overall analysis at the end.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/14_proposition_chunking.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Path to the AI information document that will be processed\npdf_path = \"data/AI_Information.pdf\"\n\n# Define test queries covering different aspects of AI to evaluate proposition chunking\ntest_queries = [\n    \"What are the main ethical concerns in AI development?\",\n    # \"How does explainable AI improve trust in AI systems?\",\n    # \"What are the key challenges in developing fair AI systems?\",\n    # \"What role does human oversight play in AI safety?\"\n]\n\n# Reference answers for more thorough evaluation and comparison of results\n# These provide a ground truth to measure the quality of generated responses\nreference_answers = [\n    \"The main ethical concerns in AI development include bias and fairness, privacy, transparency, accountability, safety, and the potential for misuse or harmful applications.\",\n    # \"Explainable AI improves trust by making AI decision-making processes transparent and understandable to users, helping them verify fairness, identify potential biases, and better understand AI limitations.\",\n    # \"Key challenges in developing fair AI systems include addressing data bias, ensuring diverse representation in training data, creating transparent algorithms, defining fairness across different contexts, and balancing competing fairness criteria.\",\n    # \"Human oversight plays a critical role in AI safety by monitoring system behavior, verifying outputs, intervening when necessary, setting ethical boundaries, and ensuring AI systems remain aligned with human values and intentions throughout their operation.\"\n]\n\n# Run the evaluation\nevaluation_results = run_proposition_chunking_evaluation(\n    pdf_path=pdf_path,\n    test_queries=test_queries,\n    reference_answers=reference_answers\n)\n\n# Print the overall analysis\nprint(\"\\n\\n=== Overall Analysis ===\")\nprint(evaluation_results[\"overall_analysis\"])\n```\n\n----------------------------------------\n\nTITLE: Generating AI Response using RAG in Python\nDESCRIPTION: This snippet shows how to generate an AI response using a system prompt and user query. It utilizes a function called generate_response, which likely interfaces with an AI model or API.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/5_contextual_chunk_headers_rag.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Generate AI response\nai_response = generate_response(system_prompt, user_prompt)\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search in Vector Store in Python\nDESCRIPTION: This function performs a similarity search in the vector store based on a query embedding. It computes cosine similarity between the query and stored embeddings, then returns the top-k most similar chunks.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef similarity_search(query_embedding: np.ndarray, top_k: int = 5) -> List[str]:\n    \"\"\"\n    Perform similarity search in the vector store and return the top_k most similar chunks.\n\n    Args:\n        query_embedding (np.ndarray): The embedding vector of the query.\n        top_k (int): The number of most similar chunks to retrieve. Default is 5.\n\n    Returns:\n        List[str]: A list of the top_k most similar text chunks.\n    \"\"\"\n    similarities = []  # Initialize a list to store similarity scores and corresponding keys\n\n    # Iterate through all items in the vector store\n    for key, value in vector_store.items():\n        # Compute the cosine similarity between the query embedding and the stored embedding\n        similarity = cosine_similarity(query_embedding, value[\"embedding\"])\n        # Append the key and similarity score as a tuple to the list\n        similarities.append((key, similarity))\n\n    # Sort the list of similarities in descending order based on the similarity score\n    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n\n    # Retrieve the top_k most similar chunks based on their keys\n    return [vector_store[key][\"chunk\"] for key, _ in similarities[:top_k]]\n```\n\n----------------------------------------\n\nTITLE: Implementing Response Generation with OpenAI API in Python\nDESCRIPTION: Function that generates responses using the OpenAI API. Takes a query, knowledge base, and sources as input, formats them into appropriate prompts, and uses GPT-4 to generate comprehensive responses with source attribution.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/20_crag.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef generate_response(query, knowledge, sources):\n    \"\"\"\n    Generate a response based on the query and knowledge.\n    \n    Args:\n        query (str): User query\n        knowledge (str): Knowledge to base the response on\n        sources (List[Dict]): List of sources with title and URL\n        \n    Returns:\n        str: Generated response\n    \"\"\"\n    # Format sources for inclusion in prompt\n    sources_text = \"\"\n    for source in sources:\n        title = source.get(\"title\", \"Unknown Source\")\n        url = source.get(\"url\", \"\")\n        if url:\n            sources_text += f\"- {title}: {url}\\n\"\n        else:\n            sources_text += f\"- {title}\\n\"\n    \n    # Define the system prompt to instruct the model on how to generate the response\n    system_prompt = \"\"\"\n    You are a helpful AI assistant. Generate a comprehensive, informative response to the query based on the provided knowledge.\n    Include all relevant information while keeping your answer clear and concise.\n    If the knowledge doesn't fully answer the query, acknowledge this limitation.\n    Include source attribution at the end of your response.\n    \"\"\"\n    \n    # Define the user prompt with the query, knowledge, and sources\n    user_prompt = f\"\"\"\n    Query: {query}\n    \n    Knowledge:\n    {knowledge}\n    \n    Sources:\n    {sources_text}\n    \n    Please provide an informative response to the query based on this information.\n    Include the sources at the end of your response.\n    \"\"\"\n    \n    try:\n        # Make a request to the OpenAI API to generate the response\n        response = client.chat.completions.create(\n            model=\"gpt-4\",  # Using GPT-4 for high-quality responses\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt}\n            ],\n            temperature=0.2\n        )\n        \n        # Return the generated response\n        return response.choices[0].message.content.strip()\n    except Exception as e:\n        # Print the error message and return an error response\n        print(f\"Error generating response: {e}\")\n        return f\"I apologize, but I encountered an error while generating a response to your query: '{query}'. The error was: {str(e)}\"\n```\n\n----------------------------------------\n\nTITLE: Generating Responses with Optional Context in Python\nDESCRIPTION: This function generates a response to a user query with optional context support. When context is provided, the response is based on that information; otherwise, it generates a response from the model's knowledge using the Llama-3.2-3B-Instruct model with a temperature of 0.2.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/13_self_rag.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef generate_response(query, context=None):\n    \"\"\"\n    Generates a response based on the query and optional context.\n    \n    Args:\n        query (str): User query\n        context (str, optional): Context text\n        \n    Returns:\n        str: Generated response\n    \"\"\"\n    # System prompt to instruct the AI on how to generate a helpful response\n    system_prompt = \"\"\"You are a helpful AI assistant. Provide a clear, accurate, and informative response to the query.\"\"\"\n    \n    # Create the user prompt based on whether context is provided\n    if context:\n        user_prompt = f\"\"\"Context:\n        {context}\n\n        Query: {query}\n\n        Please answer the query based on the provided context.\n        \"\"\"\n    else:\n        user_prompt = f\"\"\"Query: {query}\n        \n        Please answer the query to the best of your ability.\"\"\"\n    \n    # Generate the response using the OpenAI client\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0.2\n    )\n    \n    # Return the generated response text\n    return response.choices[0].message.content.strip()\n```\n\n----------------------------------------\n\nTITLE: Implementing Simple Vector Store with NumPy\nDESCRIPTION: A simple vector store implementation using NumPy for storing and retrieving embeddings. It includes methods for adding items and performing similarity searches.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/10_contextual_compression.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleVectorStore:\n    \"\"\"\n    A simple vector store implementation using NumPy.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize the vector store.\n        \"\"\"\n        self.vectors = []  # List to store embedding vectors\n        self.texts = []  # List to store original texts\n        self.metadata = []  # List to store metadata for each text\n    \n    def add_item(self, text, embedding, metadata=None):\n        \"\"\"\n        Add an item to the vector store.\n\n        Args:\n        text (str): The original text.\n        embedding (List[float]): The embedding vector.\n        metadata (dict, optional): Additional metadata.\n        \"\"\"\n        self.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\n        self.texts.append(text)  # Add the original text to texts list\n        self.metadata.append(metadata or {})  # Add metadata to metadata list, use empty dict if None\n    \n    def similarity_search(self, query_embedding, k=5):\n        \"\"\"\n        Find the most similar items to a query embedding.\n\n        Args:\n        query_embedding (List[float]): Query embedding vector.\n        k (int): Number of results to return.\n\n        Returns:\n        List[Dict]: Top k most similar items with their texts and metadata.\n        \"\"\"\n        if not self.vectors:\n            return []  # Return empty list if no vectors are stored\n        \n        # Convert query embedding to numpy array\n        query_vector = np.array(query_embedding)\n        \n        # Calculate similarities using cosine similarity\n        similarities = []\n        for i, vector in enumerate(self.vectors):\n            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n            similarities.append((i, similarity))  # Append index and similarity score\n        \n        # Sort by similarity (descending)\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        \n        # Return top k results\n        results = []\n        for i in range(min(k, len(similarities))):\n            idx, score = similarities[i]\n            results.append({\n                \"text\": self.texts[idx],  # Add the text corresponding to the index\n                \"metadata\": self.metadata[idx],  # Add the metadata corresponding to the index\n                \"similarity\": score  # Add the similarity score\n            })\n        \n        return results  # Return the list of top k results\n```\n\n----------------------------------------\n\nTITLE: Implementing Cosine Similarity for Semantic Search in RAG\nDESCRIPTION: This function calculates the cosine similarity between two vectors. It's a key component in semantic search, used to measure the similarity between query embeddings and chunk embeddings in the RAG process.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/5_contextual_chunk_headers_rag.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef cosine_similarity(vec1, vec2):\n    \"\"\"\n    Computes cosine similarity between two vectors.\n\n    Args:\n    vec1 (np.ndarray): First vector.\n    vec2 (np.ndarray): Second vector.\n\n    Returns:\n    float: Cosine similarity score.\n    \"\"\"\n    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client for API Access in Python\nDESCRIPTION: Sets up the OpenAI client with a custom base URL (Nebius) and retrieves the API key from environment variables. This client will be used for generating embeddings and AI responses.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/1_simple_rag.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the OpenAI client with the base URL and API key\nclient = OpenAI(\n    base_url=\"https://api.studio.nebius.com/v1/\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Feedback Impact in Python\nDESCRIPTION: This snippet extracts comparison data from evaluation results and prints out the analysis of feedback impact for each query. It also compares response lengths across different rounds as a proxy for completeness, calculating average lengths and percentage changes between rounds.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/11_feedback_loop_rag.ipynb#2025-04-23_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n# Extract the comparison data which contains the analysis of feedback impact\ncomparisons = evaluation_results['comparison']\n\n# Print out the analysis results to visualize feedback impact\nprint(\"\\n=== FEEDBACK IMPACT ANALYSIS ===\\n\")\nfor i, comparison in enumerate(comparisons):\n    print(f\"Query {i+1}: {comparison['query']}\")\n    print(f\"\\nAnalysis of feedback impact:\")\n    print(comparison['analysis'])\n    print(\"\\n\" + \"-\"*50 + \"\\n\")\n\n# Additionally, we can compare some metrics between rounds\nround_responses = [evaluation_results[f'round{round_num}_results'] for round_num in range(1, len(evaluation_results) - 1)]\nresponse_lengths = [[len(r[\"response\"]) for r in round] for round in round_responses]\n\nprint(\"\\nResponse length comparison (proxy for completeness):\")\navg_lengths = [sum(lengths) / len(lengths) for lengths in response_lengths]\nfor round_num, avg_len in enumerate(avg_lengths, start=1):\n    print(f\"Round {round_num}: {avg_len:.1f} chars\")\n\nif len(avg_lengths) > 1:\n    changes = [(avg_lengths[i] - avg_lengths[i-1]) / avg_lengths[i-1] * 100 for i in range(1, len(avg_lengths))]\n    for round_num, change in enumerate(changes, start=2):\n        print(f\"Change from Round {round_num-1} to Round {round_num}: {change:.1f}%\")\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Multi-Modal RAG in Python\nDESCRIPTION: This code snippet imports necessary libraries for implementing a Multi-Modal RAG system. It includes libraries for file handling, image processing, API interactions, and data manipulation.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/15_multimodel_rag.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport io\nimport numpy as np\nimport json\nimport fitz\nfrom PIL import Image\nfrom openai import OpenAI\nimport base64\nimport re\nimport tempfile\nimport shutil\n```\n\n----------------------------------------\n\nTITLE: Tracking RL Training Progress in Python\nDESCRIPTION: Function to track the training progress by storing rewards for each episode and printing progress periodically. This helps monitor how the model is learning and improving over time during training.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ndef track_progress(\n    episode: int, \n    reward: float, \n    rewards_history: List[float]\n) -> List[float]:\n    \"\"\"\n    Track the training progress by storing rewards for each episode.\n\n    Args:\n        episode (int): The current episode number.\n        reward (float): The reward received in the current episode.\n        rewards_history (List[float]): A list to store the rewards for all episodes.\n\n    Returns:\n        List[float]: The updated rewards history.\n    \"\"\"\n    # Append the current reward to the rewards history\n    rewards_history.append(reward)\n    \n    # Print progress every 10 episodes\n    print(f\"Episode {episode}: Reward = {reward}\")\n    \n    return rewards_history\n```\n\n----------------------------------------\n\nTITLE: Creating Text Embeddings for RAG with OpenAI API in Python\nDESCRIPTION: Function to generate embeddings for text inputs using OpenAI's API. It handles both single strings and lists of strings, returning appropriate embedding vectors for use in the vector store.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/12_adaptive_rag.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n    \"\"\"\n    Creates embeddings for the given text.\n\n    Args:\n    text (str or List[str]): The input text(s) for which embeddings are to be created.\n    model (str): The model to be used for creating embeddings.\n\n    Returns:\n    List[float] or List[List[float]]: The embedding vector(s).\n    \"\"\"\n    # Handle both string and list inputs by converting string input to a list\n    input_text = text if isinstance(text, list) else [text]\n    \n    # Create embeddings for the input text using the specified model\n    response = client.embeddings.create(\n        model=model,\n        input=input_text\n    )\n    \n    # If the input was a single string, return just the first embedding\n    if isinstance(text, str):\n        return response.data[0].embedding\n    \n    # Otherwise, return all embeddings for the list of texts\n    return [item.embedding for item in response.data]\n```\n\n----------------------------------------\n\nTITLE: Retrieving Relevant Text Chunks Based on Query Similarity in Python\nDESCRIPTION: Implements a function to retrieve the most relevant text chunks for a given query using cosine similarity between embeddings. The function returns the top-k chunks with highest similarity scores.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/3_chunk_size_selector.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef retrieve_relevant_chunks(query, text_chunks, chunk_embeddings, k=5):\n    \"\"\"\n    Retrieves the top-k most relevant text chunks.\n    \n    Args:\n    query (str): User query.\n    text_chunks (List[str]): List of text chunks.\n    chunk_embeddings (List[np.ndarray]): Embeddings of text chunks.\n    k (int): Number of top chunks to return.\n    \n    Returns:\n    List[str]: Most relevant text chunks.\n    \"\"\"\n    # Generate an embedding for the query - pass query as a list and get first item\n    query_embedding = create_embeddings([query])[0]\n    \n    # Calculate cosine similarity between the query embedding and each chunk embedding\n    similarities = [cosine_similarity(query_embedding, emb) for emb in chunk_embeddings]\n    \n    # Get the indices of the top-k most similar chunks\n    top_indices = np.argsort(similarities)[-k:][::-1]\n    \n    # Return the top-k most relevant text chunks\n    return [text_chunks[i] for i in top_indices]\n```\n\n----------------------------------------\n\nTITLE: Chunking Text with Metadata Preservation for RAG in Python\nDESCRIPTION: This function splits text into overlapping chunks while preserving metadata. It's designed for RAG systems, creating chunks of specified size with overlap, and maintaining original metadata along with chunk-specific information.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/18_hierarchy_rag.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef chunk_text(text, metadata, chunk_size=1000, overlap=200):\n    \"\"\"\n    Split text into overlapping chunks while preserving metadata.\n    \n    Args:\n        text (str): Input text to chunk\n        metadata (Dict): Metadata to preserve\n        chunk_size (int): Size of each chunk in characters\n        overlap (int): Overlap between chunks in characters\n        \n    Returns:\n        List[Dict]: List of text chunks with metadata\n    \"\"\"\n    chunks = []  # Initialize an empty list to store the chunks\n    \n    # Iterate over the text with the specified chunk size and overlap\n    for i in range(0, len(text), chunk_size - overlap):\n        chunk_text = text[i:i + chunk_size]  # Extract the chunk of text\n        \n        # Skip very small chunks (less than 50 characters)\n        if chunk_text and len(chunk_text.strip()) > 50:\n            # Create a copy of metadata and add chunk-specific info\n            chunk_metadata = metadata.copy()\n            chunk_metadata.update({\n                \"chunk_index\": len(chunks),  # Index of the chunk\n                \"start_char\": i,  # Start character index of the chunk\n                \"end_char\": i + len(chunk_text),  # End character index of the chunk\n                \"is_summary\": False  # Flag indicating this is not a summary\n            })\n            \n            # Append the chunk with its metadata to the list\n            chunks.append({\n                \"text\": chunk_text,\n                \"metadata\": chunk_metadata\n            })\n    \n    return chunks  # Return the list of chunks with metadata\n```\n\n----------------------------------------\n\nTITLE: Creating Embeddings in Python for RAG\nDESCRIPTION: This function creates embeddings for the provided text using a specified model. It handles both single string and list inputs, returning either a single embedding vector or a list of embedding vectors accordingly.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/7_query_transform.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n    \"\"\"\n    Creates embeddings for the given text using the specified OpenAI model.\n\n    Args:\n    text (str): The input text for which embeddings are to be created.\n    model (str): The model to be used for creating embeddings.\n\n    Returns:\n    List[float]: The embedding vector.\n    \"\"\"\n    # Handle both string and list inputs by converting string input to a list\n    input_text = text if isinstance(text, list) else [text]\n    \n    # Create embeddings for the input text using the specified model\n    response = client.embeddings.create(\n        model=model,\n        input=input_text\n    )\n    \n    # If input was a string, return just the first embedding\n    if isinstance(text, str):\n        return response.data[0].embedding\n    \n    # Otherwise, return all embeddings as a list of vectors\n    return [item.embedding for item in response.data]\n```\n\n----------------------------------------\n\nTITLE: Generating Image Captions with OpenAI Vision in Python\nDESCRIPTION: This function generates captions for images using OpenAI's vision capabilities. It takes an image file path, encodes the image, and sends a request to the OpenAI API to generate a detailed caption. It includes error handling for file not found and other exceptions.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/15_multimodel_rag.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef generate_image_caption(image_path):\n    \"\"\"\n    Generate a caption for an image using OpenAI's vision capabilities.\n    \n    Args:\n        image_path (str): Path to the image file\n        \n    Returns:\n        str: Generated caption\n    \"\"\"\n    # Check if the file exists and is an image\n    if not os.path.exists(image_path):\n        return \"Error: Image file not found\"\n    \n    try:\n        # Open and validate the image\n        Image.open(image_path)\n        \n        # Encode the image to base64\n        base64_image = encode_image(image_path)\n        \n        # Create the API request to generate the caption\n        response = client.chat.completions.create(\n            model=\"llava-hf/llava-1.5-7b-hf\", # Use the llava-1.5-7b model\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are an assistant specialized in describing images from academic papers. \"\n                    \"Provide detailed captions for the image that capture key information. \"\n                    \"If the image contains charts, tables, or diagrams, describe their content and purpose clearly. \"\n                    \"Your caption should be optimized for future retrieval when people ask questions about this content.\"\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"text\", \"text\": \"Describe this image in detail, focusing on its academic content:\"},\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\n                                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n                            }\n                        }\n                    ]\n                }\n            ],\n            max_tokens=300\n        )\n        \n        # Extract the caption from the response\n        caption = response.choices[0].message.content\n        return caption\n    \n    except Exception as e:\n        # Return an error message if an exception occurs\n        return f\"Error generating caption: {str(e)}\"\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI API Client for Embeddings and Responses in Python\nDESCRIPTION: This code initializes the OpenAI client to generate embeddings and responses. It uses the Nebius API base URL and retrieves the API key from environment variables.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/15_multimodel_rag.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the OpenAI client with the base URL and API key\nclient = OpenAI(\n    base_url=\"https://api.studio.nebius.com/v1/\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluation Execution in Python\nDESCRIPTION: Code to execute the evaluation process by comparing the generated response against a reference answer and producing an evaluation score with justification.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/6_doc_augmentation_rag.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Get reference answer from validation data\nreference_answer = data[0]['ideal_answer']\n\n# Evaluate the response\nevaluation = evaluate_response(query, response_text, reference_answer)\n\nprint(\"\\nEvaluation:\")\nprint(evaluation)\n```\n\n----------------------------------------\n\nTITLE: Configuring Reference Answers for RAG Evaluation in Python\nDESCRIPTION: Sets up reference answer list containing detailed explanations about neural network architectures for use in evaluating RAG performance\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/19_HyDE_rag.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nreference_answers = [\n    \"Neural network architecture significantly impacts AI performance through factors like depth (number of layers), width (neurons per layer), connectivity patterns, and activation functions. Different architectures like CNNs, RNNs, and Transformers are optimized for specific tasks such as image recognition, sequence processing, and natural language understanding respectively.\",\n]\n```\n\n----------------------------------------\n\nTITLE: Evaluating AI Response against Ground Truth in Python\nDESCRIPTION: This code evaluates the AI-generated response by comparing it to a ground truth answer. It uses a separate evaluation system prompt and assigns a score based on the accuracy of the response. The evaluation function returns a score of 0, 0.5, or 1.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/5_contextual_chunk_headers_rag.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Define evaluation system prompt\nevaluate_system_prompt = \"\"\"You are an intelligent evaluation system. \nAssess the AI assistant's response based on the provided context. \n- Assign a score of 1 if the response is very close to the true answer. \n- Assign a score of 0.5 if the response is partially correct. \n- Assign a score of 0 if the response is incorrect.\nReturn only the score (0, 0.5, or 1).\"\"\"\n\n# Extract the ground truth answer from validation data\ntrue_answer = data[0]['ideal_answer']\n\n# Construct evaluation prompt\nevaluation_prompt = f\"\"\"\nUser Query: {query}\nAI Response: {ai_response}\nTrue Answer: {true_answer}\n{evaluate_system_prompt}\n\"\"\"\n\n# Generate evaluation score\nevaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n\n# Print the evaluation score\nprint(\"Evaluation Score:\", evaluation_response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Relevant Document Chunks for Query in Python\nDESCRIPTION: This function retrieves the most relevant document chunks for a given query text. It generates an embedding for the query, performs a similarity search, and returns the top-k most relevant chunks.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef retrieve_relevant_chunks(query_text: str, top_k: int = 5) -> List[str]:\n    \"\"\"\n    Retrieve the most relevant document chunks for a given query text.\n\n    Args:\n        query_text (str): The query text for which relevant chunks are to be retrieved.\n        top_k (int): The number of most relevant chunks to retrieve. Default is 5.\n\n    Returns:\n        List[str]: A list of the top_k most relevant text chunks.\n    \"\"\"\n    # Generate embedding for the query text using the embedding model\n    query_embedding = generate_embeddings([query_text])[0]\n    \n    # Perform similarity search to find the most relevant chunks\n    relevant_chunks = similarity_search(query_embedding, top_k=top_k)\n    \n    # Return the list of relevant chunks\n    return relevant_chunks\n```\n\n----------------------------------------\n\nTITLE: Running a Semantic Search Query on Text Chunks in Python\nDESCRIPTION: This snippet loads validation data from a JSON file, extracts a query, and performs semantic search to find the top 2 most relevant text chunks for the query. It then prints the query and the retrieved chunks for analysis.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/6_doc_augmentation_rag.ipynb#2025-04-23_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# Load the validation data from a JSON file\nwith open('data/val.json') as f:\n    data = json.load(f)\n\n# Extract the first query from the validation data\nquery = data[0]['question']\n\n# Perform semantic search to find the top 2 most relevant text chunks for the query\ntop_chunks = semantic_search(query, text_chunks, response.data, k=2)\n\n# Print the query\nprint(\"Query:\", query)\n\n# Print the top 2 most relevant text chunks\nfor i, chunk in enumerate(top_chunks):\n    print(f\"Context {i + 1}:\\n{chunk}\\n=====================================\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Semantic Search with Cosine Similarity in Python\nDESCRIPTION: Defines a function to perform semantic search using cosine similarity to find the most relevant text chunks for a given query.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/2_semantic_chunking.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef semantic_search(query, text_chunks, chunk_embeddings, k=5):\n    \"\"\"\n    Finds the most relevant text chunks for a query.\n\n    Args:\n    query (str): Search query.\n    text_chunks (List[str]): List of text chunks.\n    chunk_embeddings (List[np.ndarray]): List of chunk embeddings.\n    k (int): Number of top results to return.\n\n    Returns:\n    List[str]: Top-k relevant chunks.\n    \"\"\"\n    # Generate an embedding for the query\n    query_embedding = get_embedding(query)\n    \n    # Calculate cosine similarity between the query embedding and each chunk embedding\n    similarities = [cosine_similarity(query_embedding, emb) for emb in chunk_embeddings]\n    \n    # Get the indices of the top-k most similar chunks\n    top_indices = np.argsort(similarities)[-k:][::-1]\n    \n    # Return the top-k most relevant text chunks\n    return [text_chunks[i] for i in top_indices]\n```\n\n----------------------------------------\n\nTITLE: Creating Embeddings with OpenAI API in Python\nDESCRIPTION: A function to generate embeddings for text inputs using the OpenAI API. It handles both single strings and lists of strings, returning the corresponding embeddings. The function uses the BAAI/bge-en-icl model by default.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/13_self_rag.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n    \"\"\"\n    Creates embeddings for the given text.\n\n    Args:\n    text (str or List[str]): The input text(s) for which embeddings are to be created.\n    model (str): The model to be used for creating embeddings.\n\n    Returns:\n    List[float] or List[List[float]]: The embedding vector(s).\n    \"\"\"\n    # Handle both string and list inputs by converting string input to a list\n    input_text = text if isinstance(text, list) else [text]\n    \n    # Create embeddings for the input text using the specified model\n    response = client.embeddings.create(\n        model=model,\n        input=input_text\n    )\n    \n    # If the input was a single string, return just the first embedding\n    if isinstance(text, str):\n        return response.data[0].embedding\n    \n    # Otherwise, return all embeddings for the list of texts\n    return [item.embedding for item in response.data]\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Corrective RAG Implementation in Python\nDESCRIPTION: This snippet imports necessary libraries for the Corrective RAG implementation, including OpenAI, PyMuPDF for PDF processing, and various utility libraries.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/20_crag.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport numpy as np\nimport json\nimport fitz  # PyMuPDF\nfrom openai import OpenAI\nimport requests\nfrom typing import List, Dict, Tuple, Any\nimport re\nfrom urllib.parse import quote_plus\nimport time\n```\n\n----------------------------------------\n\nTITLE: Traversing Knowledge Graph for Query Processing in Python\nDESCRIPTION: Function to traverse a knowledge graph using similarity-based starting points and priority-based breadth-first search to find relevant information for a user query. It calculates embeddings similarity, selects top-k starting nodes, and explores the graph up to a maximum depth.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/17_graph_rag.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef traverse_graph(query, graph, embeddings, top_k=5, max_depth=3):\n    \"\"\"\n    Traverse the knowledge graph to find relevant information for the query.\n    \n    Args:\n        query (str): The user's question\n        graph (nx.Graph): The knowledge graph\n        embeddings (List): List of node embeddings\n        top_k (int): Number of initial nodes to consider\n        max_depth (int): Maximum traversal depth\n        \n    Returns:\n        List[Dict]: Relevant information from graph traversal\n    \"\"\"\n    print(f\"Traversing graph for query: {query}\")\n    \n    # Get query embedding\n    query_embedding = create_embeddings(query)\n    \n    # Calculate similarity between query and all nodes\n    similarities = []\n    for i, node_embedding in enumerate(embeddings):\n        similarity = np.dot(query_embedding, node_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(node_embedding))\n        similarities.append((i, similarity))\n    \n    # Sort by similarity (descending)\n    similarities.sort(key=lambda x: x[1], reverse=True)\n    \n    # Get top-k most similar nodes as starting points\n    starting_nodes = [node for node, _ in similarities[:top_k]]\n    print(f\"Starting traversal from {len(starting_nodes)} nodes\")\n    \n    # Initialize traversal\n    visited = set()  # Set to keep track of visited nodes\n    traversal_path = []  # List to store the traversal path\n    results = []  # List to store the results\n    \n    # Use a priority queue for traversal\n    queue = []\n    for node in starting_nodes:\n        heapq.heappush(queue, (-similarities[node][1], node))  # Negative for max-heap\n    \n    # Traverse the graph using a modified breadth-first search with priority\n    while queue and len(results) < (top_k * 3):  # Limit results to top_k * 3\n        _, node = heapq.heappop(queue)\n        \n        if node in visited:\n            continue\n        \n        # Mark as visited\n        visited.add(node)\n        traversal_path.append(node)\n        \n        # Add current node's text to results\n        results.append({\n            \"text\": graph.nodes[node][\"text\"],\n            \"concepts\": graph.nodes[node][\"concepts\"],\n            \"node_id\": node\n        })\n        \n        # Explore neighbors if we haven't reached max depth\n        if len(traversal_path) < max_depth:\n            neighbors = [(neighbor, graph[node][neighbor][\"weight\"]) \n                        for neighbor in graph.neighbors(node)\n                        if neighbor not in visited]\n            \n            # Add neighbors to queue based on edge weight\n            for neighbor, weight in sorted(neighbors, key=lambda x: x[1], reverse=True):\n                heapq.heappush(queue, (-weight, neighbor))\n    \n    print(f\"Graph traversal found {len(results)} relevant chunks\")\n    return results, traversal_path\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI API Client for RAG in Python\nDESCRIPTION: Initializes the OpenAI client with a custom base URL and API key. This setup is used for generating embeddings and responses in the RAG system.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/12_adaptive_rag.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the OpenAI client with the base URL and API key\nclient = OpenAI(\n    base_url=\"https://api.studio.nebius.com/v1/\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n)\n```\n\n----------------------------------------\n\nTITLE: Splitting Documents into Chunks for RAG in Python\nDESCRIPTION: Implements a function to split documents into smaller chunks of a specified size. Uses word-based chunking with a default size of 30 words per chunk.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef split_into_chunks(documents: List[str], chunk_size: int = 30) -> List[str]:\n    \"\"\"\n    Split documents into smaller chunks of specified size.\n\n    Args:\n        documents (List[str]): A list of document strings to be split into chunks.\n        chunk_size (int): The maximum number of words in each chunk. Default is 100.\n\n    Returns:\n        List[str]: A list of chunks, where each chunk is a string containing up to `chunk_size` words.\n    \"\"\"\n    chunks = []  # Initialize an empty list to store the chunks\n    for doc in documents:  # Iterate through each document\n        words = doc.split()  # Split the document into words\n        # Create chunks of the specified size\n        for i in range(0, len(words), chunk_size):\n            chunk = \" \".join(words[i:i + chunk_size])  # Join words to form a chunk\n            chunks.append(chunk)  # Add the chunk to the list\n    return chunks  # Return the list of chunks\n```\n\n----------------------------------------\n\nTITLE: Performing BM25 Search in Python\nDESCRIPTION: Function that searches a BM25 index with a query string. It tokenizes the query, retrieves BM25 scores for the tokens against the indexed documents, and returns the top k results with their scores and metadata.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/16_fusion_rag.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef bm25_search(bm25, chunks, query, k=5):\n    \"\"\"\n    Search the BM25 index with a query.\n    \n    Args:\n        bm25 (BM25Okapi): BM25 index\n        chunks (List[Dict]): List of text chunks\n        query (str): Query string\n        k (int): Number of results to return\n        \n    Returns:\n        List[Dict]: Top k results with scores\n    \"\"\"\n    # Tokenize the query by splitting it into individual words\n    query_tokens = query.split()\n    \n    # Get BM25 scores for the query tokens against the indexed documents\n    scores = bm25.get_scores(query_tokens)\n    \n    # Initialize an empty list to store results with their scores\n    results = []\n    \n    # Iterate over the scores and corresponding chunks\n    for i, score in enumerate(scores):\n        # Create a copy of the metadata to avoid modifying the original\n        metadata = chunks[i].get(\"metadata\", {}).copy()\n        # Add index to metadata\n        metadata[\"index\"] = i\n        \n        results.append({\n            \"text\": chunks[i][\"text\"],\n            \"metadata\": metadata,  # Add metadata with index\n            \"bm25_score\": float(score)\n        })\n    \n    # Sort the results by BM25 score in descending order\n    results.sort(key=lambda x: x[\"bm25_score\"], reverse=True)\n    \n    # Return the top k results\n    return results[:k]\n```\n\n----------------------------------------\n\nTITLE: Generating Responses Using OpenAI Chat Model in Python\nDESCRIPTION: Function that sends constructed prompts to the OpenAI API to generate responses. It includes configurable parameters for model selection, token limits, and generation settings such as temperature, top_p, and top_k for controlling response characteristics.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef generate_response(\n    prompt: str,\n    model: str = \"google/gemma-2-2b-it\",\n    max_tokens: int = 512,\n    temperature: float = 1,\n    top_p: float = 0.9,\n    top_k: int = 50\n) -> str:\n    \"\"\"\n    Generate a response from the OpenAI chat model based on the constructed prompt.\n\n    Args:\n        prompt (str): The input prompt to provide to the chat model.\n        model (str): The model to use for generating the response. Default is \"google/gemma-2-2b-it\".\n        max_tokens (int): Maximum number of tokens in the response. Default is 512.\n        temperature (float): Sampling temperature for response diversity. Default is 0.5.\n        top_p (float): Probability mass for nucleus sampling. Default is 0.9.\n        top_k (int): Number of highest probability tokens to consider. Default is 50.\n\n    Returns:\n        str: The generated response from the chat model.\n    \"\"\"\n    # Use the OpenAI client to create a chat completion\n    response = client.chat.completions.create(\n        model=model,  # Specify the model to use for generating the response\n        max_tokens=max_tokens,  # Maximum number of tokens in the response\n        temperature=temperature,  # Sampling temperature for response diversity\n        top_p=top_p,  # Probability mass for nucleus sampling\n        extra_body={  # Additional parameters for the request\n            \"top_k\": top_k  # Number of highest probability tokens to consider\n        },\n        messages=[  # List of messages to provide context for the chat model\n            {\n                \"role\": \"user\",  # Role of the message sender (user in this case)\n                \"content\": [  # Content of the message\n                    {\n                        \"type\": \"text\",  # Type of content (text in this case)\n                        \"text\": prompt  # The actual prompt text\n                    }\n                ]\n            }\n        ]\n    )\n    # Return the content of the first choice in the response\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Chunking Text into Overlapping Segments\nDESCRIPTION: Implements a function to split text into overlapping chunks of a specified size. Each chunk is stored with metadata about its position in the original text, facilitating document retrieval and context reconstruction.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/16_fusion_rag.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef chunk_text(text, chunk_size=1000, chunk_overlap=200):\n    \"\"\"\n    Split text into overlapping chunks.\n    \n    Args:\n        text (str): Input text to chunk\n        chunk_size (int): Size of each chunk in characters\n        chunk_overlap (int): Overlap between chunks in characters\n        \n    Returns:\n        List[Dict]: List of chunks with text and metadata\n    \"\"\"\n    chunks = []  # Initialize an empty list to store chunks\n    \n    # Iterate over the text with the specified chunk size and overlap\n    for i in range(0, len(text), chunk_size - chunk_overlap):\n        chunk = text[i:i + chunk_size]  # Extract a chunk of the specified size\n        if chunk:  # Ensure we don't add empty chunks\n            chunk_data = {\n                \"text\": chunk,  # The chunk text\n                \"metadata\": {\n                    \"start_char\": i,  # Start character index of the chunk\n                    \"end_char\": i + len(chunk)  # End character index of the chunk\n                }\n            }\n            chunks.append(chunk_data)  # Add the chunk data to the list\n    \n    print(f\"Created {len(chunks)} text chunks\")  # Print the number of created chunks\n    return chunks  # Return the list of chunks\n```\n\n----------------------------------------\n\nTITLE: Creating Embeddings for RAG Text Processing in Python\nDESCRIPTION: This function creates embeddings for given texts using a specified model. It handles batch processing to comply with API limits and returns a list of embedding vectors for use in RAG systems.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/18_hierarchy_rag.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef create_embeddings(texts, model=\"BAAI/bge-en-icl\"):\n    \"\"\"\n    Create embeddings for the given texts.\n    \n    Args:\n        texts (List[str]): Input texts\n        model (str): Embedding model name\n        \n    Returns:\n        List[List[float]]: Embedding vectors\n    \"\"\"\n    # Handle empty input\n    if not texts:\n        return []\n        \n    # Process in batches if needed (OpenAI API limits)\n    batch_size = 100\n    all_embeddings = []\n    \n    # Iterate over the input texts in batches\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i + batch_size]  # Get the current batch of texts\n        \n        # Create embeddings for the current batch\n        response = client.embeddings.create(\n            model=model,\n            input=batch\n        )\n        \n        # Extract embeddings from the response\n        batch_embeddings = [item.embedding for item in response.data]\n        all_embeddings.extend(batch_embeddings)  # Add the batch embeddings to the list\n    \n    return all_embeddings  # Return all embeddings\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client for API Access in Python\nDESCRIPTION: Initializes the OpenAI client with a custom base URL (Nebius API) and an API key retrieved from environment variables. This client is used for generating embeddings and retrieving model completions.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/13_self_rag.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the OpenAI client with the base URL and API key\nclient = OpenAI(\n    base_url=\"https://api.studio.nebius.com/v1/\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Keyword-based Reranking for Search Results in Python\nDESCRIPTION: A simple alternative reranking method based on keyword matching and position. It calculates relevance scores using a combination of vector similarity, keyword presence, keyword position, and keyword frequency, providing a lightweight alternative to LLM-based reranking.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/8_reranker.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef rerank_with_keywords(query, results, top_n=3):\n    \"\"\"\n    A simple alternative reranking method based on keyword matching and position.\n    \n    Args:\n        query (str): User query\n        results (List[Dict]): Initial search results\n        top_n (int): Number of results to return after reranking\n        \n    Returns:\n        List[Dict]: Reranked results\n    \"\"\"\n    # Extract important keywords from the query\n    keywords = [word.lower() for word in query.split() if len(word) > 3]\n    \n    scored_results = []  # Initialize a list to store scored results\n    \n    for result in results:\n        document_text = result[\"text\"].lower()  # Convert document text to lowercase\n        \n        # Base score starts with vector similarity\n        base_score = result[\"similarity\"] * 0.5\n        \n        # Initialize keyword score\n        keyword_score = 0\n        for keyword in keywords:\n            if keyword in document_text:\n                # Add points for each keyword found\n                keyword_score += 0.1\n                \n                # Add more points if keyword appears near the beginning\n                first_position = document_text.find(keyword)\n                if first_position < len(document_text) / 4:  # In the first quarter of the text\n                    keyword_score += 0.1\n                \n                # Add points for keyword frequency\n                frequency = document_text.count(keyword)\n                keyword_score += min(0.05 * frequency, 0.2)  # Cap at 0.2\n        \n        # Calculate the final score by combining base score and keyword score\n        final_score = base_score + keyword_score\n        \n        # Append the scored result to the list\n        scored_results.append({\n            \"text\": result[\"text\"],\n            \"metadata\": result[\"metadata\"],\n            \"similarity\": result[\"similarity\"],\n            \"relevance_score\": final_score\n        })\n    \n    # Sort results by final relevance score in descending order\n    reranked_results = sorted(scored_results, key=lambda x: x[\"relevance_score\"], reverse=True)\n    \n    # Return the top_n results\n    return reranked_results[:top_n]\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client for API Interaction\nDESCRIPTION: This snippet sets up the OpenAI client for API interactions. It initializes the client with a custom base URL and retrieves the API key from environment variables, enabling the use of OpenAI's services for embedding generation and response creation.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/5_contextual_chunk_headers_rag.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the OpenAI client with the base URL and API key\nclient = OpenAI(\n    base_url=\"https://api.studio.nebius.com/v1/\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Query Transformation Techniques for RAG Systems in Python\nDESCRIPTION: Applies the three query transformation techniques to an example query about AI's impact on job automation. This demonstrates how each technique modifies the original query differently to enhance retrieval performance.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/7_query_transform.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Example query\noriginal_query = \"What are the impacts of AI on job automation and employment?\"\n\n# Apply query transformations\nprint(\"Original Query:\", original_query)\n\n# Query Rewriting\nrewritten_query = rewrite_query(original_query)\nprint(\"\\n1. Rewritten Query:\")\nprint(rewritten_query)\n\n# Step-back Prompting\nstep_back_query = generate_step_back_query(original_query)\nprint(\"\\n2. Step-back Query:\")\nprint(step_back_query)\n\n# Sub-query Decomposition\nsub_queries = decompose_query(original_query, num_subqueries=4)\nprint(\"\\n3. Sub-queries:\")\nfor i, query in enumerate(sub_queries, 1):\n    print(f\"   {i}. {query}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Simple Vector Store in Python\nDESCRIPTION: This class implements a simple vector store using NumPy. It provides methods to add items, perform similarity searches, and manage embeddings for efficient information retrieval in the CRAG system.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/20_crag.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleVectorStore:\n    \"\"\"\n    A simple vector store implementation using NumPy.\n    \"\"\"\n    def __init__(self):\n        # Initialize lists to store vectors, texts, and metadata\n        self.vectors = []\n        self.texts = []\n        self.metadata = []\n    \n    def add_item(self, text, embedding, metadata=None):\n        \"\"\"\n        Add an item to the vector store.\n        \n        Args:\n            text (str): The text content\n            embedding (List[float]): The embedding vector\n            metadata (Dict, optional): Additional metadata\n        \"\"\"\n        # Append the embedding, text, and metadata to their respective lists\n        self.vectors.append(np.array(embedding))\n        self.texts.append(text)\n        self.metadata.append(metadata or {})\n    \n    def add_items(self, items, embeddings):\n        \"\"\"\n        Add multiple items to the vector store.\n        \n        Args:\n            items (List[Dict]): List of items with text and metadata\n            embeddings (List[List[float]]): List of embedding vectors\n        \"\"\"\n        # Iterate over items and embeddings and add them to the store\n        for i, (item, embedding) in enumerate(zip(items, embeddings)):\n            self.add_item(\n                text=item[\"text\"],\n                embedding=embedding,\n                metadata=item.get(\"metadata\", {})\n            )\n    \n    def similarity_search(self, query_embedding, k=5):\n        \"\"\"\n        Find the most similar items to a query embedding.\n        \n        Args:\n            query_embedding (List[float]): Query embedding vector\n            k (int): Number of results to return\n            \n        Returns:\n            List[Dict]: Top k most similar items\n        \"\"\"\n        # Return an empty list if there are no vectors in the store\n        if not self.vectors:\n            return []\n        \n        # Convert query embedding to numpy array\n        query_vector = np.array(query_embedding)\n        \n        # Calculate similarities using cosine similarity\n        similarities = []\n        for i, vector in enumerate(self.vectors):\n            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n            similarities.append((i, similarity))\n        \n        # Sort by similarity (descending)\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        \n        # Return top k results\n        results = []\n        for i in range(min(k, len(similarities))):\n            idx, score = similarities[i]\n            results.append({\n                \"text\": self.texts[idx],\n                \"metadata\": self.metadata[idx],\n                \"similarity\": float(score)\n            })\n        \n        return results\n```\n\n----------------------------------------\n\nTITLE: Knowledge Refinement with GPT-3.5\nDESCRIPTION: Function that uses GPT-3.5-turbo to extract and refine key information from text. Transforms verbose text into concise bullet points focusing on the most relevant facts and important details.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/20_crag.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef refine_knowledge(text):\n    \"\"\"\n    Extract and refine key information from text.\n    \n    Args:\n        text (str): Input text to refine\n        \n    Returns:\n        str: Refined key points from the text\n    \"\"\"\n    # Define the system prompt to instruct the model on how to extract key information\n    system_prompt = \"\"\"\n    Extract the key information from the following text as a set of clear, concise bullet points.\n    Focus on the most relevant facts and important details.\n    Format your response as a bulleted list with each point on a new line starting with \"• \".\n    \"\"\"\n    \n    try:\n        # Make a request to the OpenAI API to refine the text\n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",  # Specify the model to use\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n                {\"role\": \"user\", \"content\": f\"Text to refine:\\n\\n{text}\"}  # User message with the text to refine\n            ],\n            temperature=0.3  # Set the temperature for response generation\n        )\n        \n        # Return the refined key points from the response\n        return response.choices[0].message.content.strip()\n    except Exception as e:\n        # Print the error message and return the original text on error\n        print(f\"Error refining knowledge: {e}\")\n        return text  # Return original text on error\n```\n\n----------------------------------------\n\nTITLE: Text Chunking Implementation\nDESCRIPTION: Function to split text into non-overlapping chunks of specified size. Designed specifically for RSE to maintain proper segment reconstruction.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/9_rse.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef chunk_text(text, chunk_size=800, overlap=0):\n    \"\"\"\n    Split text into non-overlapping chunks.\n    For RSE, we typically want non-overlapping chunks so we can reconstruct segments properly.\n    \n    Args:\n        text (str): Input text to chunk\n        chunk_size (int): Size of each chunk in characters\n        overlap (int): Overlap between chunks in characters\n        \n    Returns:\n        List[str]: List of text chunks\n    \"\"\"\n    chunks = []\n    \n    # Simple character-based chunking\n    for i in range(0, len(text), chunk_size - overlap):\n        chunk = text[i:i + chunk_size]\n        if chunk:  # Ensure we don't add empty chunks\n            chunks.append(chunk)\n    \n    return chunks\n```\n\n----------------------------------------\n\nTITLE: Chunking Text for Efficient Retrieval in Python\nDESCRIPTION: This function splits a large text into smaller, overlapping chunks for efficient retrieval and processing in RAG systems. It creates chunks with specified size and overlap, maintaining context across chunk boundaries.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/20_crag.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef chunk_text(text, chunk_size=1000, overlap=200):\n    \"\"\"\n    Split text into overlapping chunks for efficient retrieval and processing.\n    \n    This function divides a large text into smaller, manageable chunks with\n    specified overlap between consecutive chunks. Chunking is critical for RAG\n    systems as it allows for more precise retrieval of relevant information.\n    \n    Args:\n        text (str): Input text to be chunked\n        chunk_size (int): Maximum size of each chunk in characters\n        overlap (int): Number of overlapping characters between consecutive chunks\n                       to maintain context across chunk boundaries\n        \n    Returns:\n        List[Dict]: List of text chunks, each containing:\n                   - text: The chunk content\n                   - metadata: Dictionary with positional information and source type\n    \"\"\"\n    chunks = []\n    \n    # Iterate through the text with a sliding window approach\n    # Moving by (chunk_size - overlap) ensures proper overlap between chunks\n    for i in range(0, len(text), chunk_size - overlap):\n        # Extract the current chunk, limited by chunk_size\n        chunk_text = text[i:i + chunk_size]\n        \n        # Only add non-empty chunks\n        if chunk_text:\n            chunks.append({\n                \"text\": chunk_text,  # The actual text content\n                \"metadata\": {\n                    \"start_pos\": i,  # Starting position in the original text\n                    \"end_pos\": i + len(chunk_text),  # Ending position\n                    \"source_type\": \"document\"  # Indicates the source of this text\n                }\n            })\n    \n    print(f\"Created {len(chunks)} text chunks\")\n    return chunks\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Hierarchical RAG Implementation in Python\nDESCRIPTION: This snippet imports necessary Python libraries for implementing hierarchical indexing in RAG systems. It includes libraries for file handling, numerical operations, JSON processing, PDF text extraction, and OpenAI API integration.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/18_hierarchy_rag.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport numpy as np\nimport json\nimport fitz\nfrom openai import OpenAI\nimport re\nimport pickle\n```\n\n----------------------------------------\n\nTITLE: Implementing Step-back Prompting Function for RAG Systems in Python\nDESCRIPTION: Defines a function to generate broader, more general queries to retrieve useful contextual background information. This technique is useful for providing broader context around specific queries using the OpenAI API.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/7_query_transform.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef generate_step_back_query(original_query, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Generates a more general 'step-back' query to retrieve broader context.\n    \n    Args:\n        original_query (str): The original user query\n        model (str): The model to use for step-back query generation\n        \n    Returns:\n        str: The step-back query\n    \"\"\"\n    # Define the system prompt to guide the AI assistant's behavior\n    system_prompt = \"You are an AI assistant specialized in search strategies. Your task is to generate broader, more general versions of specific queries to retrieve relevant background information.\"\n    \n    # Define the user prompt with the original query to be generalized\n    user_prompt = f\"\"\"\n    Generate a broader, more general version of the following query that could help retrieve useful background information.\n    \n    Original query: {original_query}\n    \n    Step-back query:\n    \"\"\"\n    \n    # Generate the step-back query using the specified model\n    response = client.chat.completions.create(\n        model=model,\n        temperature=0.1,  # Slightly higher temperature for some variation\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ]\n    )\n    \n    # Return the step-back query, stripping any leading/trailing whitespace\n    return response.choices[0].message.content.strip()\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for HyDE RAG Implementation in Python\nDESCRIPTION: Imports necessary Python libraries for implementing Hypothetical Document Embedding (HyDE) for Retrieval Augmented Generation, including OpenAI API, NumPy for vector operations, and PyMuPDF for PDF processing.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/19_HyDE_rag.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport numpy as np\nimport json\nimport fitz\nfrom openai import OpenAI\nimport re\nimport matplotlib.pyplot as plt\n```\n\n----------------------------------------\n\nTITLE: Implementing Batch Compression for Multiple Chunks\nDESCRIPTION: Function to compress multiple text chunks in batch, tracking compression progress and calculating overall compression ratio.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/10_contextual_compression.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef batch_compress_chunks(chunks, query, compression_type=\"selective\", model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Compress multiple chunks individually.\n    \n    Args:\n        chunks (List[str]): List of text chunks to compress\n        query (str): User query\n        compression_type (str): Type of compression (\"selective\", \"summary\", or \"extraction\")\n        model (str): LLM model to use\n        \n    Returns:\n        List[Tuple[str, float]]: List of compressed chunks with compression ratios\n    \"\"\"\n    print(f\"Compressing {len(chunks)} chunks...\")\n    results = []\n    total_original_length = 0\n    total_compressed_length = 0\n    \n    for i, chunk in enumerate(chunks):\n        print(f\"Compressing chunk {i+1}/{len(chunks)}...\")\n        compressed_chunk, compression_ratio = compress_chunk(chunk, query, compression_type, model)\n        results.append((compressed_chunk, compression_ratio))\n        \n        total_original_length += len(chunk)\n        total_compressed_length += len(compressed_chunk)\n    \n    overall_ratio = (total_original_length - total_compressed_length) / total_original_length * 100\n    print(f\"Overall compression ratio: {overall_ratio:.2f}%\")\n    \n    return results\n```\n\n----------------------------------------\n\nTITLE: Splitting Text into Semantic Chunks in Python\nDESCRIPTION: Implements a function to split sentences into semantic chunks based on computed breakpoints and applies it to create text chunks.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/2_semantic_chunking.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef split_into_chunks(sentences, breakpoints):\n    \"\"\"\n    Splits sentences into semantic chunks.\n\n    Args:\n    sentences (List[str]): List of sentences.\n    breakpoints (List[int]): Indices where chunking should occur.\n\n    Returns:\n    List[str]: List of text chunks.\n    \"\"\"\n    chunks = []  # Initialize an empty list to store the chunks\n    start = 0  # Initialize the start index\n\n    # Iterate through each breakpoint to create chunks\n    for bp in breakpoints:\n        # Append the chunk of sentences from start to the current breakpoint\n        chunks.append(\". \".join(sentences[start:bp + 1]) + \".\")\n        start = bp + 1  # Update the start index to the next sentence after the breakpoint\n\n    # Append the remaining sentences as the last chunk\n    chunks.append(\". \".join(sentences[start:]))\n    return chunks  # Return the list of chunks\n\n# Create chunks using the split_into_chunks function\ntext_chunks = split_into_chunks(sentences, breakpoints)\n\n# Print the number of chunks created\nprint(f\"Number of semantic chunks: {len(text_chunks)}\")\n\n# Print the first chunk to verify the result\nprint(\"\\nFirst text chunk:\")\nprint(text_chunks[0])\n```\n\n----------------------------------------\n\nTITLE: Generating AI Responses with Retrieved Context in Python\nDESCRIPTION: Code that defines a function to generate responses using a language model based on retrieved context. It includes a system prompt that instructs the AI to strictly answer based on the provided context, preventing hallucinations.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/4_context_enriched_rag.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Define the system prompt for the AI assistant\nsystem_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\n\ndef generate_response(system_prompt, user_message, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Generates a response from the AI model based on the system prompt and user message.\n\n    Args:\n    system_prompt (str): The system prompt to guide the AI's behavior.\n    user_message (str): The user's message or query.\n    model (str): The model to be used for generating the response. Default is \"meta-llama/Llama-2-7B-chat-hf\".\n\n    Returns:\n    dict: The response from the AI model.\n    \"\"\"\n    response = client.chat.completions.create(\n        model=model,\n        temperature=0,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_message}\n        ]\n    )\n    return response\n\n# Create the user prompt based on the top chunks\nuser_prompt = \"\\n\".join([f\"Context {i + 1}:\\n{chunk}\\n=====================================\\n\" for i, chunk in enumerate(top_chunks)])\nuser_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n\n# Generate AI response\nai_response = generate_response(system_prompt, user_prompt)\n```\n\n----------------------------------------\n\nTITLE: Chunking Text with Configurable Size and Overlap in Python\nDESCRIPTION: Implements a function to split text into chunks of specified size with controlled overlap, then creates chunks of different sizes for comparison. This approach helps optimize retrieval performance in RAG systems.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/3_chunk_size_selector.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef chunk_text(text, n, overlap):\n    \"\"\"\n    Splits text into overlapping chunks.\n\n    Args:\n    text (str): The text to be chunked.\n    n (int): Number of characters per chunk.\n    overlap (int): Overlapping characters between chunks.\n\n    Returns:\n    List[str]: A list of text chunks.\n    \"\"\"\n    chunks = []  # Initialize an empty list to store the chunks\n    for i in range(0, len(text), n - overlap):\n        # Append a chunk of text from the current index to the index + chunk size\n        chunks.append(text[i:i + n])\n    \n    return chunks  # Return the list of text chunks\n\n# Define different chunk sizes to evaluate\nchunk_sizes = [128, 256, 512]\n\n# Create a dictionary to store text chunks for each chunk size\ntext_chunks_dict = {size: chunk_text(extracted_text, size, size // 5) for size in chunk_sizes}\n\n# Print the number of chunks created for each chunk size\nfor size, chunks in text_chunks_dict.items():\n    print(f\"Chunk Size: {size}, Number of Chunks: {len(chunks)}\")\n```\n\n----------------------------------------\n\nTITLE: Defining State Representation for RL Agent in Python\nDESCRIPTION: This function defines the state representation for the reinforcement learning agent, including the original query, context chunks, rewritten query, and history of responses and rewards. It encapsulates all relevant information needed for the agent to make decisions.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef define_state(\n    query: str, \n    context_chunks: List[str], \n    rewritten_query: str = None, \n    previous_responses: List[str] = None, \n    previous_rewards: List[float] = None\n) -> dict:\n    \"\"\"\n    Define the state representation for the reinforcement learning agent.\n    \n    Args:\n        query (str): The original user query.\n        context_chunks (List[str]): Retrieved context chunks from the knowledge base.\n        rewritten_query (str, optional): A reformulated version of the original query.\n        previous_responses (List[str], optional): List of previously generated responses.\n        previous_rewards (List[float], optional): List of rewards received for previous actions.\n    \n    Returns:\n        dict: A dictionary representing the current state with all relevant information.\n    \"\"\"\n    state = {\n        \"original_query\": query,                                    # The initial query from the user\n        \"current_query\": rewritten_query if rewritten_query else query,  # Current version of the query (may be rewritten)\n        \"context\": context_chunks,                                 # Retrieved context chunks from the knowledge base\n        \"previous_responses\": previous_responses if previous_responses else [],  # History of generated responses\n        \"previous_rewards\": previous_rewards if previous_rewards else []         # History of received rewards\n    }\n    return state\n```\n\n----------------------------------------\n\nTITLE: Implementing Text Chunking with Overlap\nDESCRIPTION: Function to split text into overlapping chunks with specified size and overlap, returning chunks with metadata including position and ID.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/14_proposition_chunking.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef chunk_text(text, chunk_size=800, overlap=100):\n    \"\"\"\n    Split text into overlapping chunks.\n    \n    Args:\n        text (str): Input text to chunk\n        chunk_size (int): Size of each chunk in characters\n        overlap (int): Overlap between chunks in characters\n        \n    Returns:\n        List[Dict]: List of chunk dictionaries with text and metadata\n    \"\"\"\n    chunks = []  # Initialize an empty list to store the chunks\n    \n    # Iterate over the text with the specified chunk size and overlap\n    for i in range(0, len(text), chunk_size - overlap):\n        chunk = text[i:i + chunk_size]  # Extract a chunk of the specified size\n        if chunk:  # Ensure we don't add empty chunks\n            chunks.append({\n                \"text\": chunk,  # The chunk text\n                \"chunk_id\": len(chunks) + 1,  # Unique ID for the chunk\n                \"start_char\": i,  # Starting character index of the chunk\n                \"end_char\": i + len(chunk)  # Ending character index of the chunk\n            })\n    \n    print(f\"Created {len(chunks)} text chunks\")  # Print the number of created chunks\n    return chunks  # Return the list of chunks\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Document Augmentation RAG\nDESCRIPTION: Imports necessary Python libraries including fitz for PDF processing, numpy for vector operations, OpenAI for language model access, and tqdm for progress tracking.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/6_doc_augmentation_rag.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport fitz\nimport os\nimport numpy as np\nimport json\nfrom openai import OpenAI\nimport re\nfrom tqdm import tqdm\n```\n\n----------------------------------------\n\nTITLE: Vector Store Query Function\nDESCRIPTION: Function to retrieve relevant items from a vector store based on a query. Uses embeddings for similarity search and returns top k results.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/14_proposition_chunking.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef retrieve_from_store(query, vector_store, k=5):\n    \"\"\"\n    Retrieve relevant items from a vector store based on query.\n    \n    Args:\n        query (str): User query\n        vector_store (SimpleVectorStore): Vector store to search\n        k (int): Number of results to retrieve\n        \n    Returns:\n        List[Dict]: Retrieved items with scores and metadata\n    \"\"\"\n    # Create query embedding\n    query_embedding = create_embeddings(query)\n    \n    # Search vector store for the top k most similar items\n    results = vector_store.similarity_search(query_embedding, k=k)\n    \n    return results\n```\n\n----------------------------------------\n\nTITLE: Running and Visualizing RAG Pipeline Results in Python\nDESCRIPTION: Code that executes the RAG pipeline on a sample query and displays the results with formatting for easy comparison with ground truth answers. This helps in manually analyzing the pipeline's performance and identifying areas for improvement.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# Print a message to indicate the start of the RAG pipeline\nprint(\"🔍 Running the Retrieval-Augmented Generation (RAG) pipeline...\")\nprint(f\"📥 Query: {sample_query}\\n\")\n\n# Run the RAG pipeline and get the response\nresponse = basic_rag_pipeline(sample_query)\n\n# Print the response with better formatting\nprint(\"🤖 AI Response:\")\nprint(\"-\" * 50)\nprint(response.strip())\nprint(\"-\" * 50)\n\n# Print the ground truth answer for comparison\nprint(\"✅ Ground Truth Answer:\")\nprint(\"-\" * 50)\nprint(expected_answer)\nprint(\"-\" * 50)\n```\n\n----------------------------------------\n\nTITLE: Implementing Response Generation with LLM\nDESCRIPTION: Function to generate responses using an LLM based on compressed context and user query. Includes system prompts for controlling response behavior.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/10_contextual_compression.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef generate_response(query, context, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Generate a response based on the query and context.\n    \n    Args:\n        query (str): User query\n        context (str): Context text from compressed chunks\n        model (str): LLM model to use\n        \n    Returns:\n        str: Generated response\n    \"\"\"\n    system_prompt = \"\"\"You are a helpful AI assistant. Answer the user's question based only on the provided context.\n    If you cannot find the answer in the context, state that you don't have enough information.\"\"\"\n            \n    user_prompt = f\"\"\"\n        Context:\n        {context}\n\n        Question: {query}\n\n        Please provide a comprehensive answer based only on the context above.\n    \"\"\"\n    \n    response = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=0\n    )\n    \n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Chunking Text with Overlap for RAG Processing\nDESCRIPTION: Function that splits a long text into smaller, overlapping chunks of a specified size. This chunking approach helps maintain context across chunk boundaries and improves retrieval accuracy.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/6_doc_augmentation_rag.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef chunk_text(text, n, overlap):\n    \"\"\"\n    Chunks the given text into segments of n characters with overlap.\n\n    Args:\n    text (str): The text to be chunked.\n    n (int): The number of characters in each chunk.\n    overlap (int): The number of overlapping characters between chunks.\n\n    Returns:\n    List[str]: A list of text chunks.\n    \"\"\"\n    chunks = []  # Initialize an empty list to store the chunks\n    \n    # Loop through the text with a step size of (n - overlap)\n    for i in range(0, len(text), n - overlap):\n        # Append a chunk of text from index i to i + n to the chunks list\n        chunks.append(text[i:i + n])\n\n    return chunks  # Return the list of text chunks\n```\n\n----------------------------------------\n\nTITLE: Extracting Text and Images from PDF in Python\nDESCRIPTION: This function extracts both text and images from a PDF file. It processes each page, extracting text content and saving images to a specified directory. It returns lists of extracted text data and image paths with associated metadata.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/15_multimodel_rag.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef extract_content_from_pdf(pdf_path, output_dir=None):\n    \"\"\"\n    Extract both text and images from a PDF file.\n    \n    Args:\n        pdf_path (str): Path to the PDF file\n        output_dir (str, optional): Directory to save extracted images\n        \n    Returns:\n        Tuple[List[Dict], List[Dict]]: Text data and image data\n    \"\"\"\n    # Create a temporary directory for images if not provided\n    temp_dir = None\n    if output_dir is None:\n        temp_dir = tempfile.mkdtemp()\n        output_dir = temp_dir\n    else:\n        os.makedirs(output_dir, exist_ok=True)\n        \n    text_data = []  # List to store extracted text data\n    image_paths = []  # List to store paths of extracted images\n    \n    print(f\"Extracting content from {pdf_path}...\")\n    \n    try:\n        with fitz.open(pdf_path) as pdf_file:\n            # Loop through every page in the PDF\n            for page_number in range(len(pdf_file)):\n                page = pdf_file[page_number]\n                \n                # Extract text from the page\n                text = page.get_text().strip()\n                if text:\n                    text_data.append({\n                        \"content\": text,\n                        \"metadata\": {\n                            \"source\": pdf_path,\n                            \"page\": page_number + 1,\n                            \"type\": \"text\"\n                        }\n                    })\n                \n                # Extract images from the page\n                image_list = page.get_images(full=True)\n                for img_index, img in enumerate(image_list):\n                    xref = img[0]  # XREF of the image\n                    base_image = pdf_file.extract_image(xref)\n                    \n                    if base_image:\n                        image_bytes = base_image[\"image\"]\n                        image_ext = base_image[\"ext\"]\n                        \n                        # Save the image to the output directory\n                        img_filename = f\"page_{page_number+1}_img_{img_index+1}.{image_ext}\"\n                        img_path = os.path.join(output_dir, img_filename)\n                        \n                        with open(img_path, \"wb\") as img_file:\n                            img_file.write(image_bytes)\n                        \n                        image_paths.append({\n                            \"path\": img_path,\n                            \"metadata\": {\n                                \"source\": pdf_path,\n                                \"page\": page_number + 1,\n                                \"image_index\": img_index + 1,\n                                \"type\": \"image\"\n                            }\n                        })\n        \n        print(f\"Extracted {len(text_data)} text segments and {len(image_paths)} images\")\n        return text_data, image_paths\n    \n    except Exception as e:\n        print(f\"Error extracting content: {e}\")\n        if temp_dir and os.path.exists(temp_dir):\n            shutil.rmtree(temp_dir)\n        raise\n```\n\n----------------------------------------\n\nTITLE: Implementing Cosine Similarity for Vector Comparison in Python\nDESCRIPTION: Function to calculate cosine similarity between two vectors, a key metric for measuring semantic similarity between embeddings. This is used to determine relevance in semantic search for RAG systems.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/1_simple_rag.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef cosine_similarity(vec1, vec2):\n    \"\"\"\n    Calculates the cosine similarity between two vectors.\n\n    Args:\n    vec1 (np.ndarray): The first vector.\n    vec2 (np.ndarray): The second vector.\n\n    Returns:\n    float: The cosine similarity between the two vectors.\n    \"\"\"\n    # Compute the dot product of the two vectors and divide by the product of their norms\n    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n```\n\n----------------------------------------\n\nTITLE: Implementing Cosine Similarity for Vector Comparison in Python\nDESCRIPTION: Defines a function to calculate cosine similarity between two vectors, used to measure similarity between query embeddings and document embeddings in the retrieval process.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/3_chunk_size_selector.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef cosine_similarity(vec1, vec2):\n    \"\"\"\n    Computes cosine similarity between two vectors.\n\n    Args:\n    vec1 (np.ndarray): First vector.\n    vec2 (np.ndarray): Second vector.\n\n    Returns:\n    float: Cosine similarity score.\n    \"\"\"\n\n    # Compute the dot product of the two vectors\n    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n```\n\n----------------------------------------\n\nTITLE: Setting Up Adaptive RAG Evaluation in Python\nDESCRIPTION: This code snippet demonstrates how to set up and run the adaptive RAG evaluation system. It defines the path to the knowledge source document, test queries covering different query types, and reference answers for thorough evaluation.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/12_adaptive_rag.ipynb#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# Path to your knowledge source document\n# This PDF file contains the information that the RAG system will use\npdf_path = \"data/AI_Information.pdf\"\n\n# Define test queries covering different query types to demonstrate \n# how adaptive retrieval handles various query intentions\ntest_queries = [\n    \"What is Explainable AI (XAI)?\",                                              # Factual query - seeking definition/specific information\n    # \"How do AI ethics and governance frameworks address potential societal impacts?\",  # Analytical query - requiring comprehensive analysis\n    # \"Is AI development moving too fast for proper regulation?\",                   # Opinion query - seeking diverse perspectives\n    # \"How might explainable AI help in healthcare decisions?\",                     # Contextual query - benefits from context-awareness\n]\n\n# Reference answers for more thorough evaluation\n# These can be used to objectively assess response quality against a known standard\nreference_answers = [\n    \"Explainable AI (XAI) aims to make AI systems transparent and understandable by providing clear explanations of how decisions are made. This helps users trust and effectively manage AI technologies.\",\n    # \"AI ethics and governance frameworks address potential societal impacts by establishing guidelines and principles to ensure AI systems are developed and used responsibly. These frameworks focus on fairness, accountability, transparency, and the protection of human rights to mitigate risks and promote beneficial output.5.\",\n    # \"Opinions on whether AI development is moving too fast for proper regulation vary. Some argue that rapid advancements outpace regulatory efforts, leading to potential risks and ethical concerns. Others believe that innovation should continue at its current pace, with regulations evolving alongside to address emerging challenges.\",\n    # \"Explainable AI can significantly aid healthcare decisions by providing transparent and understandable insights into AI-driven recommendations. This transparency helps healthcare professionals trust AI systems, make informed decisions, and improve patient output by understanding the rationale behind AI suggestions.\"\n]\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings with OpenAI API\nDESCRIPTION: Implements a function to create embeddings for text using the OpenAI API. It handles both single text inputs and lists of texts, processing them in batches to respect API limits.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/16_fusion_rag.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef create_embeddings(texts, model=\"BAAI/bge-en-icl\"):\n    \"\"\"\n    Create embeddings for the given texts.\n    \n    Args:\n        texts (str or List[str]): Input text(s)\n        model (str): Embedding model name\n        \n    Returns:\n        List[List[float]]: Embedding vectors\n    \"\"\"\n    # Handle both string and list inputs\n    input_texts = texts if isinstance(texts, list) else [texts]\n    \n    # Process in batches if needed (OpenAI API limits)\n    batch_size = 100\n    all_embeddings = []\n    \n    # Iterate over the input texts in batches\n    for i in range(0, len(input_texts), batch_size):\n        batch = input_texts[i:i + batch_size]  # Get the current batch of texts\n        \n        # Create embeddings for the current batch\n        response = client.embeddings.create(\n            model=model,\n            input=batch\n        )\n        \n        # Extract embeddings from the response\n        batch_embeddings = [item.embedding for item in response.data]\n        all_embeddings.extend(batch_embeddings)  # Add the batch embeddings to the list\n    \n    # If input was a string, return just the first embedding\n    if isinstance(texts, str):\n        return all_embeddings[0]\n    \n    # Otherwise return all embeddings\n    return all_embeddings\n```\n\n----------------------------------------\n\nTITLE: Creating Text Embeddings with OpenAI API in Python\nDESCRIPTION: Defines a function to generate embeddings for text chunks using OpenAI's API. The function returns embeddings as numpy arrays, which are then created for all text chunks of different sizes.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/3_chunk_size_selector.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom tqdm import tqdm\n\ndef create_embeddings(texts, model=\"BAAI/bge-en-icl\"):\n    \"\"\"\n    Generates embeddings for a list of texts.\n\n    Args:\n    texts (List[str]): List of input texts.\n    model (str): Embedding model.\n\n    Returns:\n    List[np.ndarray]: List of numerical embeddings.\n    \"\"\"\n    # Create embeddings using the specified model\n    response = client.embeddings.create(model=model, input=texts)\n    # Convert the response to a list of numpy arrays and return\n    return [np.array(embedding.embedding) for embedding in response.data]\n\n# Generate embeddings for each chunk size\n# Iterate over each chunk size and its corresponding chunks in the text_chunks_dict\nchunk_embeddings_dict = {size: create_embeddings(chunks) for size, chunks in tqdm(text_chunks_dict.items(), desc=\"Generating Embeddings\")}\n```\n\n----------------------------------------\n\nTITLE: Chunking Text Content for RAG in Python\nDESCRIPTION: This function splits text data into overlapping chunks. It takes text data extracted from a PDF, chunk size, and overlap as parameters. It returns a list of chunked text data with updated metadata including chunk index and count.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/15_multimodel_rag.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef chunk_text(text_data, chunk_size=1000, overlap=200):\n    \"\"\"\n    Split text data into overlapping chunks.\n    \n    Args:\n        text_data (List[Dict]): Text data extracted from PDF\n        chunk_size (int): Size of each chunk in characters\n        overlap (int): Overlap between chunks in characters\n        \n    Returns:\n        List[Dict]: Chunked text data\n    \"\"\"\n    chunked_data = []  # Initialize an empty list to store chunked data\n    \n    for item in text_data:\n        text = item[\"content\"]  # Extract the text content\n        metadata = item[\"metadata\"]  # Extract the metadata\n        \n        # Skip if text is too short\n        if len(text) < chunk_size / 2:\n            chunked_data.append({\n                \"content\": text,\n                \"metadata\": metadata\n            })\n            continue\n        \n        # Create chunks with overlap\n        chunks = []\n        for i in range(0, len(text), chunk_size - overlap):\n            chunk = text[i:i + chunk_size]  # Extract a chunk of the specified size\n            if chunk:  # Ensure we don't add empty chunks\n                chunks.append(chunk)\n        \n        # Add each chunk with updated metadata\n        for i, chunk in enumerate(chunks):\n            chunk_metadata = metadata.copy()  # Copy the original metadata\n            chunk_metadata[\"chunk_index\"] = i  # Add chunk index to metadata\n            chunk_metadata[\"chunk_count\"] = len(chunks)  # Add total chunk count to metadata\n            \n            chunked_data.append({\n                \"content\": chunk,  # The chunk text\n                \"metadata\": chunk_metadata  # The updated metadata\n            })\n    \n    print(f\"Created {len(chunked_data)} text chunks\")  # Print the number of created chunks\n    return chunked_data  # Return the list of chunked data\n```\n\n----------------------------------------\n\nTITLE: Calculating Text Similarity Using Embeddings in Python\nDESCRIPTION: This function calculates the semantic similarity between two texts using embeddings. It generates embeddings for both texts, converts them to numpy arrays, and calculates the cosine similarity between the vectors.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/11_feedback_loop_rag.ipynb#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef calculate_similarity(text1, text2):\n    \"\"\"\n    Calculate semantic similarity between two texts using embeddings.\n    \n    Args:\n        text1 (str): First text\n        text2 (str): Second text\n        \n    Returns:\n        float: Similarity score between 0 and 1\n    \"\"\"\n    # Generate embeddings for both texts\n    embedding1 = create_embeddings(text1)\n    embedding2 = create_embeddings(text2)\n    \n    # Convert embeddings to numpy arrays\n    vec1 = np.array(embedding1)\n    vec2 = np.array(embedding2)\n    \n    # Calculate cosine similarity between the two vectors\n    similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n    \n    return similarity\n```\n\n----------------------------------------\n\nTITLE: Results Visualization Function\nDESCRIPTION: Function to create visual comparisons between HyDE and standard RAG results using matplotlib. Displays query, hypothetical document, and retrieved chunks side by side.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/19_HyDE_rag.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef visualize_results(query, hyde_result, standard_result):\n    \"\"\"\n    Visualize the results of HyDE and standard RAG approaches.\n    \n    Args:\n        query (str): User query\n        hyde_result (Dict): Results from HyDE RAG\n        standard_result (Dict): Results from standard RAG\n    \"\"\"\n    # Create a figure with 3 subplots\n    fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n    \n    # Plot the query in the first subplot\n    axs[0].text(0.5, 0.5, f\"Query:\\n\\n{query}\", \n                horizontalalignment='center', verticalalignment='center',\n                fontsize=12, wrap=True)\n    axs[0].axis('off')  # Hide the axis for the query plot\n    \n    # Plot the hypothetical document in the second subplot\n    hypothetical_doc = hyde_result[\"hypothetical_document\"]\n    # Shorten the hypothetical document if it's too long\n    shortened_doc = hypothetical_doc[:500] + \"...\" if len(hypothetical_doc) > 500 else hypothetical_doc\n    axs[1].text(0.5, 0.5, f\"Hypothetical Document:\\n\\n{shortened_doc}\", \n                horizontalalignment='center', verticalalignment='center',\n                fontsize=10, wrap=True)\n    axs[1].axis('off')  # Hide the axis for the hypothetical document plot\n    \n    # Plot comparison of retrieved chunks in the third subplot\n    # Shorten each chunk text for better visualization\n    hyde_chunks = [chunk[\"text\"][:100] + \"...\" for chunk in hyde_result[\"retrieved_chunks\"]]\n    std_chunks = [chunk[\"text\"][:100] + \"...\" for chunk in standard_result[\"retrieved_chunks\"]]\n    \n    # Prepare the comparison text\n    comparison_text = \"Retrieved by HyDE:\\n\\n\"\n    for i, chunk in enumerate(hyde_chunks):\n        comparison_text += f\"{i+1}. {chunk}\\n\\n\"\n    \n    comparison_text += \"\\nRetrieved by Standard RAG:\\n\\n\"\n    for i, chunk in enumerate(std_chunks):\n        comparison_text += f\"{i+1}. {chunk}\\n\\n\"\n    \n    # Plot the comparison text in the third subplot\n    axs[2].text(0.5, 0.5, comparison_text, \n                horizontalalignment='center', verticalalignment='center',\n                fontsize=8, wrap=True)\n    axs[2].axis('off')  # Hide the axis for the comparison plot\n    \n    # Adjust layout to prevent overlap\n    plt.tight_layout()\n    # Display the plot\n    plt.show()\n```\n\n----------------------------------------\n\nTITLE: Evaluating Response Quality in Python\nDESCRIPTION: This function evaluates the quality of responses using a simple heuristic based on response length. It normalizes the length and caps the score at 1.0.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_response_quality(responses: List[str]) -> float:\n    \"\"\"\n    Evaluate the quality of responses using a heuristic or external model.\n\n    Args:\n        responses (List[str]): A list of generated responses to evaluate.\n\n    Returns:\n        float: The average quality score of the responses, ranging from 0 to 1.\n    \"\"\"\n    quality_scores: List[float] = []  # Initialize a list to store quality scores for each response\n\n    for response in responses:\n        # Example heuristic: Calculate a quality score based on response length\n        # Normalize the length by a maximum of 100 words and cap the score at 1.0\n        quality: float = len(response.split()) / 100\n        quality_scores.append(min(quality, 1.0))  # Append the capped quality score to the list\n\n    # Return the average quality score across all responses\n    return np.mean(quality_scores)\n```\n\n----------------------------------------\n\nTITLE: Processing Multiple Images and Generating Captions in Python\nDESCRIPTION: This function processes a list of image paths, generates captions for each image, and returns a list of image data including captions and metadata. It uses the generate_image_caption function and provides progress updates during processing.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/15_multimodel_rag.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef process_images(image_paths):\n    \"\"\"\n    Process all images and generate captions.\n    \n    Args:\n        image_paths (List[Dict]): Paths to extracted images\n        \n    Returns:\n        List[Dict]: Image data with captions\n    \"\"\"\n    image_data = []  # Initialize an empty list to store image data with captions\n    \n    print(f\"Generating captions for {len(image_paths)} images...\")  # Print the number of images to process\n    for i, img_item in enumerate(image_paths):\n        print(f\"Processing image {i+1}/{len(image_paths)}...\")  # Print the current image being processed\n        img_path = img_item[\"path\"]  # Get the image path\n        metadata = img_item[\"metadata\"]  # Get the image metadata\n        \n        # Generate caption for the image\n        caption = generate_image_caption(img_path)\n        \n        # Add the image data with caption to the list\n        image_data.append({\n            \"content\": caption,  # The generated caption\n            \"metadata\": metadata,  # The image metadata\n            \"image_path\": img_path  # The path to the image\n        })\n    \n    return image_data  # Return the list of image data with captions\n```\n\n----------------------------------------\n\nTITLE: Embedding Creation Function\nDESCRIPTION: Implements a function to create embeddings for text using OpenAI's API. Supports both single string and list inputs.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/11_feedback_loop_rag.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n    \"\"\"\n    Creates embeddings for the given text.\n\n    Args:\n    text (str or List[str]): The input text(s) for which embeddings are to be created.\n    model (str): The model to be used for creating embeddings.\n\n    Returns:\n    List[float] or List[List[float]]: The embedding vector(s).\n    \"\"\"\n    # Convert single string to list for uniform processing\n    input_text = text if isinstance(text, list) else [text]\n    \n    # Call the OpenAI API to generate embeddings for all input texts\n    response = client.embeddings.create(\n        model=model,\n        input=input_text\n    )\n    \n    # For single string input, return just the first embedding vector\n    if isinstance(text, str):\n        return response.data[0].embedding\n    \n    # For list input, return a list of all embedding vectors\n    return [item.embedding for item in response.data]\n```\n\n----------------------------------------\n\nTITLE: Creating BM25 Index from Text Chunks in Python\nDESCRIPTION: Function that creates a BM25 index from a list of text chunks. It extracts text from chunks, tokenizes the documents by splitting on whitespace, and builds a BM25Okapi index for efficient retrieval.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/16_fusion_rag.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef create_bm25_index(chunks):\n    \"\"\"\n    Create a BM25 index from the given chunks.\n    \n    Args:\n        chunks (List[Dict]): List of text chunks\n        \n    Returns:\n        BM25Okapi: A BM25 index\n    \"\"\"\n    # Extract text from each chunk\n    texts = [chunk[\"text\"] for chunk in chunks]\n    \n    # Tokenize each document by splitting on whitespace\n    tokenized_docs = [text.split() for text in texts]\n    \n    # Create the BM25 index using the tokenized documents\n    bm25 = BM25Okapi(tokenized_docs)\n    \n    # Print the number of documents in the BM25 index\n    print(f\"Created BM25 index with {len(texts)} documents\")\n    \n    return bm25\n```\n\n----------------------------------------\n\nTITLE: Implementing Semantic Chunking Methods in Python\nDESCRIPTION: Defines a function to compute chunking breakpoints based on similarity drops using three methods: percentile, standard deviation, and interquartile range.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/2_semantic_chunking.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef compute_breakpoints(similarities, method=\"percentile\", threshold=90):\n    \"\"\"\n    Computes chunking breakpoints based on similarity drops.\n\n    Args:\n    similarities (List[float]): List of similarity scores between sentences.\n    method (str): 'percentile', 'standard_deviation', or 'interquartile'.\n    threshold (float): Threshold value (percentile for 'percentile', std devs for 'standard_deviation').\n\n    Returns:\n    List[int]: Indices where chunk splits should occur.\n    \"\"\"\n    # Determine the threshold value based on the selected method\n    if method == \"percentile\":\n        # Calculate the Xth percentile of the similarity scores\n        threshold_value = np.percentile(similarities, threshold)\n    elif method == \"standard_deviation\":\n        # Calculate the mean and standard deviation of the similarity scores\n        mean = np.mean(similarities)\n        std_dev = np.std(similarities)\n        # Set the threshold value to mean minus X standard deviations\n        threshold_value = mean - (threshold * std_dev)\n    elif method == \"interquartile\":\n        # Calculate the first and third quartiles (Q1 and Q3)\n        q1, q3 = np.percentile(similarities, [25, 75])\n        # Set the threshold value using the IQR rule for outliers\n        threshold_value = q1 - 1.5 * (q3 - q1)\n    else:\n        # Raise an error if an invalid method is provided\n        raise ValueError(\"Invalid method. Choose 'percentile', 'standard_deviation', or 'interquartile'.\")\n\n    # Identify indices where similarity drops below the threshold value\n    return [i for i, sim in enumerate(similarities) if sim < threshold_value]\n\n# Compute breakpoints using the percentile method with a threshold of 90\nbreakpoints = compute_breakpoints(similarities, method=\"percentile\", threshold=90)\n```\n\n----------------------------------------\n\nTITLE: Implementing Cosine Similarity for Vector Comparison in Python\nDESCRIPTION: This function calculates the cosine similarity between two vectors, which is used to measure the similarity between text embeddings. It computes the dot product of the vectors divided by the product of their norms.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/6_doc_augmentation_rag.ipynb#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef cosine_similarity(vec1, vec2):\n    \"\"\"\n    Calculates the cosine similarity between two vectors.\n\n    Args:\n    vec1 (np.ndarray): The first vector.\n    vec2 (np.ndarray): The second vector.\n\n    Returns:\n    float: The cosine similarity between the two vectors.\n    \"\"\"\n    # Compute the dot product of the two vectors and divide by the product of their norms\n    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n```\n\n----------------------------------------\n\nTITLE: Creating Embeddings using OpenAI API in Python\nDESCRIPTION: This function creates vector embeddings for text inputs using OpenAI's embedding models. It handles both single string and list inputs, processes texts in batches to avoid API rate limits, and returns embedding vectors.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/20_crag.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef create_embeddings(texts, model=\"text-embedding-3-small\"):\n    \"\"\"\n    Create vector embeddings for text inputs using OpenAI's embedding models.\n    \n    Embeddings are dense vector representations of text that capture semantic meaning,\n    allowing for similarity comparisons. In RAG systems, embeddings are essential\n    for matching queries with relevant document chunks.\n    \n    Args:\n        texts (str or List[str]): Input text(s) to be embedded. Can be a single string\n                                  or a list of strings.\n        model (str): The embedding model name to use. Defaults to \"text-embedding-3-small\".\n        \n    Returns:\n        List[List[float]]: If input is a list, returns a list of embedding vectors.\n                          If input is a single string, returns a single embedding vector.\n    \"\"\"\n    # Handle both single string and list inputs by converting single strings to a list\n    input_texts = texts if isinstance(texts, list) else [texts]\n    \n    # Process in batches to avoid API rate limits and payload size restrictions\n    # OpenAI API typically has limits on request size and rate\n    batch_size = 100\n    all_embeddings = []\n    \n    # Process each batch of texts\n    for i in range(0, len(input_texts), batch_size):\n        # Extract the current batch of texts\n        batch = input_texts[i:i + batch_size]\n        \n        # Make API call to generate embeddings for the current batch\n        response = client.embeddings.create(\n            model=model,\n            input=batch\n        )\n        \n        # Extract the embedding vectors from the response\n        batch_embeddings = [item.embedding for item in response.data]\n        all_embeddings.extend(batch_embeddings)\n    \n    # If the original input was a single string, return just the first embedding\n    if isinstance(texts, str):\n        return all_embeddings[0]\n    \n    # Otherwise return the full list of embeddings\n    return all_embeddings\n```\n\n----------------------------------------\n\nTITLE: Visualizing Knowledge Graph Traversal in Python\nDESCRIPTION: Function to visualize a knowledge graph and the traversal path used to answer a query. It uses NetworkX and matplotlib for visualization, highlighting traversal path nodes in different colors, and adjusting edge widths based on weights.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/17_graph_rag.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef visualize_graph_traversal(graph, traversal_path):\n    \"\"\"\n    Visualize the knowledge graph and the traversal path.\n    \n    Args:\n        graph (nx.Graph): The knowledge graph\n        traversal_path (List): List of nodes in traversal order\n    \"\"\"\n    plt.figure(figsize=(12, 10))  # Set the figure size\n    \n    # Define node colors, default to light blue\n    node_color = ['lightblue'] * graph.number_of_nodes()\n    \n    # Highlight traversal path nodes in light green\n    for node in traversal_path:\n        node_color[node] = 'lightgreen'\n    \n    # Highlight start node in green and end node in red\n    if traversal_path:\n        node_color[traversal_path[0]] = 'green'\n        node_color[traversal_path[-1]] = 'red'\n    \n    # Create positions for all nodes using spring layout\n    pos = nx.spring_layout(graph, k=0.5, iterations=50, seed=42)\n    \n    # Draw the graph nodes\n    nx.draw_networkx_nodes(graph, pos, node_color=node_color, node_size=500, alpha=0.8)\n    \n    # Draw edges with width proportional to weight\n    for u, v, data in graph.edges(data=True):\n        weight = data.get('weight', 1.0)\n        nx.draw_networkx_edges(graph, pos, edgelist=[(u, v)], width=weight*2, alpha=0.6)\n    \n    # Draw traversal path with red dashed lines\n    traversal_edges = [(traversal_path[i], traversal_path[i+1]) \n                      for i in range(len(traversal_path)-1)]\n    \n    nx.draw_networkx_edges(graph, pos, edgelist=traversal_edges, \n                          width=3, alpha=0.8, edge_color='red', \n                          style='dashed', arrows=True)\n    \n    # Add labels with the first concept for each node\n    labels = {}\n    for node in graph.nodes():\n        concepts = graph.nodes[node]['concepts']\n        label = concepts[0] if concepts else f\"Node {node}\"\n        labels[node] = f\"{node}: {label}\"\n    \n    nx.draw_networkx_labels(graph, pos, labels=labels, font_size=8)\n    \n    plt.title(\"Knowledge Graph with Traversal Path\")  # Set the plot title\n    plt.axis('off')  # Turn off the axis\n    plt.tight_layout()  # Adjust layout\n    plt.show()  # Display the plot\n```\n\n----------------------------------------\n\nTITLE: Creating Text Embeddings for Vector Search\nDESCRIPTION: Function that generates embeddings for text using the specified model through the OpenAI API. These embeddings are used to represent both text chunks and generated questions in the vector space.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/6_doc_augmentation_rag.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n    \"\"\"\n    Creates embeddings for the given text using the specified OpenAI model.\n\n    Args:\n    text (str): The input text for which embeddings are to be created.\n    model (str): The model to be used for creating embeddings.\n\n    Returns:\n    dict: The response from the OpenAI API containing the embeddings.\n    \"\"\"\n    # Create embeddings for the input text using the specified model\n    response = client.embeddings.create(\n        model=model,\n        input=text\n    )\n\n    return response  # Return the response containing the embeddings\n```\n\n----------------------------------------\n\nTITLE: Response Evaluation Implementation in Python\nDESCRIPTION: Function to evaluate AI-generated responses against reference answers using an LLM. Includes scoring criteria and detailed evaluation metrics.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/6_doc_augmentation_rag.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_response(query, response, reference_answer, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Evaluates the AI response against a reference answer.\n    \n    Args:\n    query (str): The user's question.\n    response (str): The AI-generated response.\n    reference_answer (str): The reference/ideal answer.\n    model (str): Model to use for evaluation.\n    \n    Returns:\n    str: Evaluation feedback.\n    \"\"\"\n    # Define the system prompt for the evaluation system\n    evaluate_system_prompt = \"\"\"You are an intelligent evaluation system tasked with assessing AI responses.\n            \n        Compare the AI assistant's response to the true/reference answer, and evaluate based on:\n        1. Factual correctness - Does the response contain accurate information?\n        2. Completeness - Does it cover all important aspects from the reference?\n        3. Relevance - Does it directly address the question?\n\n        Assign a score from 0 to 1:\n        - 1.0: Perfect match in content and meaning\n        - 0.8: Very good, with minor omissions/differences\n        - 0.6: Good, covers main points but misses some details\n        - 0.4: Partial answer with significant omissions\n        - 0.2: Minimal relevant information\n        - 0.0: Incorrect or irrelevant\n\n        Provide your score with justification.\n    \"\"\"\n            \n    # Create the evaluation prompt\n    evaluation_prompt = f\"\"\"\n        User Query: {query}\n\n        AI Response:\n        {response}\n\n        Reference Answer:\n        {reference_answer}\n\n        Please evaluate the AI response against the reference answer.\n    \"\"\"\n    \n    # Generate evaluation\n    eval_response = client.chat.completions.create(\n        model=model,\n        temperature=0,\n        messages=[\n            {\"role\": \"system\", \"content\": evaluate_system_prompt},\n            {\"role\": \"user\", \"content\": evaluation_prompt}\n        ]\n    )\n    \n    return eval_response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Saving RAG Comparison Results to JSON in Python\nDESCRIPTION: This code saves the results of comparing simple and RL-enhanced RAG pipelines to a JSON file. It creates a structured dictionary with the query, ground truth, responses from both pipelines, similarity scores, and the improvement percentage, then writes the data to 'rl_rag_results.json'.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_38\n\nLANGUAGE: python\nCODE:\n```\n# Save the results for later comparison\nresults = {\n    \"query\": query_text,  # The input query text\n    \"ground_truth\": expected_answer,  # The expected correct answer for the query\n    \"simple_rag\": {\n        \"response\": simple_response,  # The response generated by the simple RAG pipeline\n        \"similarity\": float(simple_sim)  # The similarity score of the simple RAG response to the ground truth\n    },\n    \"rl_rag\": {\n        \"response\": rl_response,  # The response generated by the RL-enhanced RAG pipeline\n        \"similarity\": float(rl_sim)  # The similarity score of the RL-enhanced RAG response to the ground truth\n    },\n    \"improvement\": float(rl_sim - simple_sim)  # The improvement in similarity score achieved by RL-enhanced RAG\n}\n\n# Save the results to a JSON file for future reference\nwith open('rl_rag_results.json', 'w') as f:\n    json.dump(results, f, indent=2)  # Write the results dictionary to the file with indentation for readability\n\n# Print a confirmation message to indicate that the results have been saved\nprint(\"\\nResults saved to rl_rag_results.json\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Keyword-based Reranking in RAG System\nDESCRIPTION: Code that implements keyword-based reranking for a RAG system. It prints the section header, applies the reranking method to the query using the vector store, and displays both the query and the response.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/8_reranker.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nprint(\"\\n=== KEYWORD-BASED RERANKING ===\")\nkeyword_results = rag_with_reranking(query, vector_store, reranking_method=\"keywords\")\nprint(f\"\\nQuery: {query}\")\nprint(f\"\\nResponse:\\n{keyword_results['response']}\")\n```\n\n----------------------------------------\n\nTITLE: Building Text-Only Vector Store for RAG\nDESCRIPTION: Function to construct a text-only vector store from a PDF document. It extracts text content, chunks it, creates embeddings, and builds a vector store for comparison purposes.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/15_multimodel_rag.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef build_text_only_store(pdf_path, chunk_size=1000, chunk_overlap=200):\n    \"\"\"\n    Build a text-only vector store for comparison.\n    \n    Args:\n        pdf_path (str): Path to the PDF file\n        chunk_size (int): Size of each chunk in characters\n        chunk_overlap (int): Overlap between chunks in characters\n        \n    Returns:\n        MultiModalVectorStore: Text-only vector store\n    \"\"\"\n    # Extract text from PDF (reuse function but ignore images)\n    text_data, _ = extract_content_from_pdf(pdf_path, None)\n    \n    # Chunk text\n    chunked_text = chunk_text(text_data, chunk_size, chunk_overlap)\n    \n    # Extract content for embedding\n    contents = [item[\"content\"] for item in chunked_text]\n    \n    # Create embeddings\n    print(\"Creating embeddings for text-only content...\")\n    embeddings = create_embeddings(contents)\n    \n    # Build vector store\n    vector_store = MultiModalVectorStore()\n    vector_store.add_items(chunked_text, embeddings)\n    \n    print(f\"Added {len(chunked_text)} text items to text-only vector store\")\n    return vector_store\n```\n\n----------------------------------------\n\nTITLE: Running RAG Evaluation Example\nDESCRIPTION: Example code showing how to run the evaluation framework on a specific PDF document with test queries and reference answers.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/15_multimodel_rag.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Path to your PDF document\npdf_path = \"data/attention_is_all_you_need.pdf\"\n\n# Define test queries targeting both text and visual content\ntest_queries = [\n    \"What is the BLEU score of the Transformer (base model)?\",\n]\n\n# Optional reference answers for evaluation\nreference_answers = [\n    \"The Transformer (base model) achieves a BLEU score of 27.3 on the WMT 2014 English-to-German translation task and 38.1 on the WMT 2014 English-to-French translation task.\",\n]\n\n# Run evaluation\nevaluation_results = evaluate_multimodal_vs_textonly(\n    pdf_path=pdf_path,\n    test_queries=test_queries,\n    reference_answers=reference_answers\n)\n\n# Print overall analysis\nprint(\"\\n=== OVERALL ANALYSIS ===\\n\")\nprint(evaluation_results[\"overall_analysis\"])\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from PDF Files with Page Separation in Python\nDESCRIPTION: This function extracts text content from a PDF file, separating it by pages. It uses PyMuPDF to process the PDF, skips pages with minimal content, and returns a list of pages with text and metadata.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/18_hierarchy_rag.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef extract_text_from_pdf(pdf_path):\n    \"\"\"\n    Extract text content from a PDF file with page separation.\n    \n    Args:\n        pdf_path (str): Path to the PDF file\n        \n    Returns:\n        List[Dict]: List of pages with text content and metadata\n    \"\"\"\n    print(f\"Extracting text from {pdf_path}...\")  # Print the path of the PDF being processed\n    pdf = fitz.open(pdf_path)  # Open the PDF file using PyMuPDF\n    pages = []  # Initialize an empty list to store the pages with text content\n    \n    # Iterate over each page in the PDF\n    for page_num in range(len(pdf)):\n        page = pdf[page_num]  # Get the current page\n        text = page.get_text()  # Extract text from the current page\n        \n        # Skip pages with very little text (less than 50 characters)\n        if len(text.strip()) > 50:\n            # Append the page text and metadata to the list\n            pages.append({\n                \"text\": text,\n                \"metadata\": {\n                    \"source\": pdf_path,  # Source file path\n                    \"page\": page_num + 1  # Page number (1-based index)\n                }\n            })\n    \n    print(f\"Extracted {len(pages)} pages with content\")  # Print the number of pages extracted\n    return pages  # Return the list of pages with text content and metadata\n```\n\n----------------------------------------\n\nTITLE: Defining Faithfulness Evaluation Prompt Template in Python\nDESCRIPTION: This snippet defines a template for prompting an AI model to evaluate the faithfulness of a response compared to a true answer. It uses a strict scoring system with predefined values for full, partial, and no faithfulness.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/3_chunk_size_selector.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nFAITHFULNESS_PROMPT_TEMPLATE = \"\"\"\nEvaluate the faithfulness of the AI response compared to the true answer.\nUser Query: {question}\nAI Response: {response}\nTrue Answer: {true_answer}\n\nFaithfulness measures how well the AI response aligns with facts in the true answer, without hallucinations.\n\nINSTRUCTIONS:\n- Score STRICTLY using only these values:\n    * {full} = Completely faithful, no contradictions with true answer\n    * {partial} = Partially faithful, minor contradictions\n    * {none} = Not faithful, major contradictions or hallucinations\n- Return ONLY the numerical score ({full}, {partial}, or {none}) with no explanation or additional text.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Creating Embeddings for Text Using OpenAI API in Python\nDESCRIPTION: This function creates embeddings for a list of texts using the OpenAI API. It processes texts in batches to handle API limits and returns a list of embedding vectors. The function uses the specified embedding model and handles empty input gracefully.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/15_multimodel_rag.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef create_embeddings(texts, model=\"BAAI/bge-en-icl\"):\n    \"\"\"\n    Create embeddings for the given texts.\n    \n    Args:\n        texts (List[str]): Input texts\n        model (str): Embedding model name\n        \n    Returns:\n        List[List[float]]: Embedding vectors\n    \"\"\"\n    # Handle empty input\n    if not texts:\n        return []\n        \n    # Process in batches if needed (OpenAI API limits)\n    batch_size = 100\n    all_embeddings = []\n    \n    # Iterate over the input texts in batches\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i + batch_size]  # Get the current batch of texts\n        \n        # Create embeddings for the current batch\n        response = client.embeddings.create(\n            model=model,\n            input=batch\n        )\n        \n        # Extract embeddings from the response\n        batch_embeddings = [item.embedding for item in response.data]\n        all_embeddings.extend(batch_embeddings)  # Add the batch embeddings to the list\n    \n    return all_embeddings  # Return all embeddings\n```\n\n----------------------------------------\n\nTITLE: Running Query Transformation Evaluation with Sample Data in Python\nDESCRIPTION: This snippet demonstrates how to execute the evaluation of query transformations using a sample PDF document and validation data. It loads a query and reference answer from a JSON file, then runs the evaluation process to compare different transformation techniques.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/7_query_transform.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Load the validation data from a JSON file\nwith open('data/val.json') as f:\n    data = json.load(f)\n\n# Extract the first query from the validation data\nquery = data[0]['question']\n\n# Extract the reference answer from the validation data\nreference_answer = data[0]['ideal_answer']\n\n# pdf_path\npdf_path = \"data/AI_Information.pdf\"\n\n# Run evaluation\nevaluation_results = evaluate_transformations(pdf_path, query, reference_answer)\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings for Text Chunks in Python\nDESCRIPTION: This function generates embeddings for a list of text chunks using batching. It processes chunks in batches of a specified size and returns a NumPy array of embeddings.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef generate_embeddings(chunks: List[str], batch_size: int = 10) -> np.ndarray:\n    \"\"\"\n    Generate embeddings for all text chunks in batches.\n\n    Args:\n        chunks (List[str]): A list of text chunks to generate embeddings for.\n        batch_size (int): The number of chunks to process in each batch. Default is 10.\n\n    Returns:\n        np.ndarray: A NumPy array containing embeddings for all chunks.\n    \"\"\"\n    all_embeddings = []  # Initialize an empty list to store all embeddings\n\n    # Iterate through the chunks in batches\n    for i in range(0, len(chunks), batch_size):\n        # Extract the current batch of chunks\n        batch = chunks[i:i + batch_size]\n        # Generate embeddings for the current batch\n        embeddings = generate_embeddings_batch(batch)\n        # Extend the list of all embeddings with the embeddings from the current batch\n        all_embeddings.extend(embeddings)\n\n    # Convert the list of embeddings to a NumPy array and return it\n    return np.array(all_embeddings)\n```\n\n----------------------------------------\n\nTITLE: Vector Store Implementation\nDESCRIPTION: Simple vector store class implementation using NumPy for storing and searching embeddings. Includes methods for adding items and performing similarity search using cosine similarity.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/8_reranker.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleVectorStore:\n    \"\"\"\n    A simple vector store implementation using NumPy.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize the vector store.\n        \"\"\"\n        self.vectors = []  # List to store embedding vectors\n        self.texts = []  # List to store original texts\n        self.metadata = []  # List to store metadata for each text\n    \n    def add_item(self, text, embedding, metadata=None):\n        \"\"\"\n        Add an item to the vector store.\n\n        Args:\n        text (str): The original text.\n        embedding (List[float]): The embedding vector.\n        metadata (dict, optional): Additional metadata.\n        \"\"\"\n        self.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\n        self.texts.append(text)  # Add the original text to texts list\n        self.metadata.append(metadata or {})  # Add metadata to metadata list, use empty dict if None\n    \n    def similarity_search(self, query_embedding, k=5):\n        \"\"\"\n        Find the most similar items to a query embedding.\n\n        Args:\n        query_embedding (List[float]): Query embedding vector.\n        k (int): Number of results to return.\n\n        Returns:\n        List[Dict]: Top k most similar items with their texts and metadata.\n        \"\"\"\n        if not self.vectors:\n            return []  # Return empty list if no vectors are stored\n        \n        # Convert query embedding to numpy array\n        query_vector = np.array(query_embedding)\n        \n        # Calculate similarities using cosine similarity\n        similarities = []\n        for i, vector in enumerate(self.vectors):\n            # Compute cosine similarity between query vector and stored vector\n            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n            similarities.append((i, similarity))  # Append index and similarity score\n        \n        # Sort by similarity (descending)\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        \n        # Return top k results\n        results = []\n        for i in range(min(k, len(similarities))):\n            idx, score = similarities[i]\n            results.append({\n                \"text\": self.texts[idx],  # Add the corresponding text\n                \"metadata\": self.metadata[idx],  # Add the corresponding metadata\n                \"similarity\": score  # Add the similarity score\n            })\n        \n        return results  # Return the list of top k similar items\n```\n\n----------------------------------------\n\nTITLE: Generating AI Response with Context in Python\nDESCRIPTION: Creates a user prompt by joining multiple context chunks with a user query, then passes this combined prompt to a response generation function. This implements the prompt construction phase of a RAG (Retrieval-Augmented Generation) system.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/2_semantic_chunking.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Create the user prompt based on the top chunks\nuser_prompt = \"\\n\".join([f\"Context {i + 1}:\\n{chunk}\\n=====================================\\n\" for i, chunk in enumerate(top_chunks)])\nuser_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n\n# Generate AI response\nai_response = generate_response(system_prompt, user_prompt)\n```\n\n----------------------------------------\n\nTITLE: Performing Semantic Search on Validation Data in Python\nDESCRIPTION: Loads validation data from a JSON file, extracts a query, and performs semantic search to find the top 2 most relevant text chunks.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/2_semantic_chunking.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Load the validation data from a JSON file\nwith open('data/val.json') as f:\n    data = json.load(f)\n\n# Extract the first query from the validation data\nquery = data[0]['question']\n\n# Get top 2 relevant chunks\ntop_chunks = semantic_search(query, text_chunks, chunk_embeddings, k=2)\n\n# Print the query\nprint(f\"Query: {query}\")\n\n# Print the top 2 most relevant text chunks\nfor i, chunk in enumerate(top_chunks):\n    print(f\"Context {i+1}:\\n{chunk}\\n{'='*40}\")\n```\n\n----------------------------------------\n\nTITLE: Processing PDF and Chunking Text for RAG\nDESCRIPTION: This snippet demonstrates the process of extracting text from a PDF file, chunking it with contextual headers, and printing a sample chunk. It uses a chunk size of 1000 characters with an overlap of 200 characters, which are typical parameters for RAG text processing.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/5_contextual_chunk_headers_rag.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Define the PDF file path\npdf_path = \"data/AI_Information.pdf\"\n\n# Extract text from the PDF file\nextracted_text = extract_text_from_pdf(pdf_path)\n\n# Chunk the extracted text with headers\n# We use a chunk size of 1000 characters and an overlap of 200 characters\ntext_chunks = chunk_text_with_headers(extracted_text, 1000, 200)\n\n# Print a sample chunk with its generated header\nprint(\"Sample Chunk:\")\nprint(\"Header:\", text_chunks[0]['header'])\nprint(\"Content:\", text_chunks[0]['text'])\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from PDF Files in Python\nDESCRIPTION: This function extracts text from a PDF file using the PyMuPDF (fitz) library. It iterates through each page of the PDF, extracts the text, and concatenates it into a single string.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/7_query_transform.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef extract_text_from_pdf(pdf_path):\n    \"\"\"\n    Extracts text from a PDF file.\n\n    Args:\n    pdf_path (str): Path to the PDF file.\n\n    Returns:\n    str: Extracted text from the PDF.\n    \"\"\"\n    # Open the PDF file\n    mypdf = fitz.open(pdf_path)\n    all_text = \"\"  # Initialize an empty string to store the extracted text\n\n    # Iterate through each page in the PDF\n    for page_num in range(mypdf.page_count):\n        page = mypdf[page_num]  # Get the page\n        text = page.get_text(\"text\")  # Extract text from the page\n        all_text += text  # Append the extracted text to the all_text string\n\n    return all_text  # Return the extracted text\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings for RAG in Python\nDESCRIPTION: Defines a function to generate embeddings for a batch of text chunks using the OpenAI client. Uses the 'BAAI/bge-en-icl' model for embedding generation.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef generate_embeddings_batch(chunks_batch: List[str], model: str = \"BAAI/bge-en-icl\") -> List[List[float]]:\n    \"\"\"\n    Generate embeddings for a batch of text chunks using the OpenAI client.\n\n    Args:\n        chunks_batch (List[str]): A batch of text chunks to generate embeddings for.\n        model (str): The model to use for embedding generation. Default is \"BAAI/bge-en-icl\".\n\n    Returns:\n        List[List[float]]: A list of embeddings, where each embedding is a list of floats.\n    \"\"\"\n    # Use the OpenAI client to create embeddings for the input batch\n    response = client.embeddings.create(\n        model=model,  # Specify the model to use for embedding generation\n        input=chunks_batch  # Provide the batch of text chunks as input\n    )\n    # Extract embeddings from the response and return them\n    embeddings = [item.embedding for item in response.data]\n    return embeddings\n```\n\n----------------------------------------\n\nTITLE: Calculating Cosine Similarity Between Sentences in Python\nDESCRIPTION: Implements a function to compute cosine similarity between two vectors and uses it to calculate similarity between consecutive sentences in the text.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/2_semantic_chunking.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef cosine_similarity(vec1, vec2):\n    \"\"\"\n    Computes cosine similarity between two vectors.\n\n    Args:\n    vec1 (np.ndarray): First vector.\n    vec2 (np.ndarray): Second vector.\n\n    Returns:\n    float: Cosine similarity.\n    \"\"\"\n    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n\n# Compute similarity between consecutive sentences\nsimilarities = [cosine_similarity(embeddings[i], embeddings[i + 1]) for i in range(len(embeddings) - 1)]\n```\n\n----------------------------------------\n\nTITLE: Extracting and Chunking Text from PDF Files in Python\nDESCRIPTION: This snippet extracts text from a PDF file and chunks it into segments of 1000 characters with an overlap of 200 characters. It then prints the number of chunks created and displays the first chunk as an example.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/6_doc_augmentation_rag.ipynb#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# Extract text from the PDF file\nextracted_text = extract_text_from_pdf(pdf_path)\n\n# Chunk the extracted text into segments of 1000 characters with an overlap of 200 characters\ntext_chunks = chunk_text(extracted_text, 1000, 200)\n\n# Print the number of text chunks created\nprint(\"Number of text chunks:\", len(text_chunks))\n\n# Print the first text chunk\nprint(\"\\nFirst text chunk:\")\nprint(text_chunks[0])\n```\n\n----------------------------------------\n\nTITLE: Loading Validation Data in Python\nDESCRIPTION: This code snippet loads validation data from a JSON file for evaluating reranking quality. It extracts the first query and reference answer from the validation data and specifies the path to a PDF file that will be used for testing the RAG pipeline.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/8_reranker.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Load the validation data from a JSON file\nwith open('data/val.json') as f:\n    data = json.load(f)\n\n# Extract the first query from the validation data\nquery = data[0]['question']\n\n# Extract the reference answer from the validation data\nreference_answer = data[0]['ideal_answer']\n\n# pdf_path\npdf_path = \"data/AI_Information.pdf\"\n```\n\n----------------------------------------\n\nTITLE: Updating RL Policy Based on Reward in Python\nDESCRIPTION: Function to update the policy based on the reward received for a given state-action pair. This implementation stores the action taken and the reward received for each query in the policy dictionary, which could be extended with more sophisticated RL algorithms.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndef update_policy(\n    policy: Dict[str, Dict[str, Union[float, str]]], \n    state: Dict[str, object], \n    action: str, \n    reward: float, \n    learning_rate: float\n) -> Dict[str, Dict[str, Union[float, str]]]:\n    \"\"\"\n    Update the policy based on the reward received.\n\n    Args:\n        policy (Dict[str, Dict[str, Union[float, str]]]): The current policy to be updated.\n        state (Dict[str, object]): The current state of the environment.\n        action (str): The action taken by the agent.\n        reward (float): The reward received for the action.\n        learning_rate (float): The learning rate for updating the policy.\n\n    Returns:\n        Dict[str, Dict[str, Union[float, str]]]: The updated policy.\n    \"\"\"\n    # Example: Simple policy update (to be replaced with a proper RL algorithm)\n    policy[state[\"query\"]] = {\n        \"action\": action,  # Store the action taken\n        \"reward\": reward   # Store the reward received\n    }\n    return policy\n```\n\n----------------------------------------\n\nTITLE: Generating Text Embeddings with OpenAI\nDESCRIPTION: Function to create embeddings for input texts using OpenAI's API, supporting both single and batch processing with specified model.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/14_proposition_chunking.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef create_embeddings(texts, model=\"BAAI/bge-en-icl\"):\n    \"\"\"\n    Create embeddings for the given texts.\n    \n    Args:\n        texts (str or List[str]): Input text(s)\n        model (str): Embedding model name\n        \n    Returns:\n        List[List[float]]: Embedding vector(s)\n    \"\"\"\n    # Handle both string and list inputs\n    input_texts = texts if isinstance(texts, list) else [texts]\n    \n    # Process in batches if needed (OpenAI API limits)\n    batch_size = 100\n    all_embeddings = []\n    \n    # Iterate over the input texts in batches\n    for i in range(0, len(input_texts), batch_size):\n        batch = input_texts[i:i + batch_size]  # Get the current batch of texts\n        \n        # Create embeddings for the current batch\n        response = client.embeddings.create(\n            model=model,\n            input=batch\n        )\n        \n        # Extract embeddings from the response\n        batch_embeddings = [item.embedding for item in response.data]\n        all_embeddings.extend(batch_embeddings)  # Add the batch embeddings to the list\n    \n    # If input was a single string, return just the first embedding\n    if isinstance(texts, str):\n        return all_embeddings[0]\n    \n    # Otherwise, return all embeddings\n    return all_embeddings\n```\n\n----------------------------------------\n\nTITLE: Defining Relevancy Evaluation Prompt Template in Python\nDESCRIPTION: This snippet defines a template for prompting an AI model to evaluate the relevancy of a response to a user query. It uses a strict scoring system with predefined values for full, partial, and no relevancy.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/3_chunk_size_selector.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nRELEVANCY_PROMPT_TEMPLATE = \"\"\"\nEvaluate the relevancy of the AI response to the user query.\nUser Query: {question}\nAI Response: {response}\n\nRelevancy measures how well the response addresses the user's question.\n\nINSTRUCTIONS:\n- Score STRICTLY using only these values:\n    * {full} = Completely relevant, directly addresses the query\n    * {partial} = Partially relevant, addresses some aspects\n    * {none} = Not relevant, fails to address the query\n- Return ONLY the numerical score ({full}, {partial}, or {none}) with no explanation or additional text.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Creating Sentence-Level Embeddings with OpenAI in Python\nDESCRIPTION: Defines a function to create embeddings for given text using OpenAI API. It then splits the extracted text into sentences and generates embeddings for each sentence.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/2_semantic_chunking.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef get_embedding(text, model=\"BAAI/bge-en-icl\"):\n    \"\"\"\n    Creates an embedding for the given text using OpenAI.\n\n    Args:\n    text (str): Input text.\n    model (str): Embedding model name.\n\n    Returns:\n    np.ndarray: The embedding vector.\n    \"\"\"\n    response = client.embeddings.create(model=model, input=text)\n    return np.array(response.data[0].embedding)\n\n# Splitting text into sentences (basic split)\nsentences = extracted_text.split(\". \")\n\n# Generate embeddings for each sentence\nembeddings = [get_embedding(sentence) for sentence in sentences]\n\nprint(f\"Generated {len(embeddings)} sentence embeddings.\")\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from PDF Files using PyMuPDF\nDESCRIPTION: Function that extracts text content from a PDF file by iterating through each page and concatenating the extracted text. Uses the fitz (PyMuPDF) library to handle PDF parsing.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/6_doc_augmentation_rag.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef extract_text_from_pdf(pdf_path):\n    \"\"\"\n    Extracts text from a PDF file and prints the first `num_chars` characters.\n\n    Args:\n    pdf_path (str): Path to the PDF file.\n\n    Returns:\n    str: Extracted text from the PDF.\n    \"\"\"\n    # Open the PDF file\n    mypdf = fitz.open(pdf_path)\n    all_text = \"\"  # Initialize an empty string to store the extracted text\n\n    # Iterate through each page in the PDF\n    for page_num in range(mypdf.page_count):\n        page = mypdf[page_num]  # Get the page\n        text = page.get_text(\"text\")  # Extract text from the page\n        all_text += text  # Append the extracted text to the all_text string\n\n    return all_text  # Return the extracted text\n```\n\n----------------------------------------\n\nTITLE: Executing Data Preprocessing for RAG in Python\nDESCRIPTION: Demonstrates the execution of data preprocessing steps including loading documents, splitting into chunks, and preprocessing the chunks. Prints sample output for verification.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Specify the directory path containing the text files\ndirectory_path = \"data\"\n\n# Load all text documents from the specified directory\ndocuments = load_documents(directory_path)\n\n# Split the loaded documents into smaller chunks of text\nchunks = split_into_chunks(documents)\n\n# Preprocess the chunks (e.g., lowercasing, removing special characters)\npreprocessed_chunks = preprocess_chunks(chunks)\n\n# Print the first 2 preprocessed chunks, displaying only the first 200 characters of each chunk\nfor i in range(2):\n    # Use slicing to limit the output to the first 200\n    print(f\"Chunk {i+1}: {preprocessed_chunks[i][:50]} ... \")\n    print(\"-\" * 50)  # Print a separator line\n```\n\n----------------------------------------\n\nTITLE: Generating AI Responses Based on Retrieved Chunks in Python\nDESCRIPTION: Defines a function to generate an AI response using OpenAI's chat completion API based on retrieved chunks and a query. The system is instructed to answer strictly based on the provided context for evaluation.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/3_chunk_size_selector.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Define the system prompt for the AI assistant\nsystem_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\n\ndef generate_response(query, system_prompt, retrieved_chunks, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Generates an AI response based on retrieved chunks.\n\n    Args:\n    query (str): User query.\n    retrieved_chunks (List[str]): List of retrieved text chunks.\n    model (str): AI model.\n\n    Returns:\n    str: AI-generated response.\n    \"\"\"\n    # Combine retrieved chunks into a single context string\n    context = \"\\n\".join([f\"Context {i+1}:\\n{chunk}\" for i, chunk in enumerate(retrieved_chunks)])\n    \n    # Create the user prompt by combining the context and the query\n    user_prompt = f\"{context}\\n\\nQuestion: {query}\"\n\n    # Generate the AI response using the specified model\n    response = client.chat.completions.create(\n        model=model,\n        temperature=0,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ]\n    )\n\n    # Return the content of the AI response\n    return response.choices[0].message.content\n\n# Generate AI responses for each chunk size\nai_responses_dict = {size: generate_response(query, system_prompt, retrieved_chunks_dict[size]) for size in chunk_sizes}\n\n# Print the response for chunk size 256\nprint(ai_responses_dict[256])\n```\n\n----------------------------------------\n\nTITLE: Response Generation Execution in Python\nDESCRIPTION: Code to execute the response generation process by preparing context from search results and generating a response using the LLM.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/6_doc_augmentation_rag.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Prepare context from search results\ncontext = prepare_context(search_results)\n\n# Generate response\nresponse_text = generate_response(query, context)\n\nprint(\"\\nQuery:\", query)\nprint(\"\\nResponse:\")\nprint(response_text)\n```\n\n----------------------------------------\n\nTITLE: Executing and Printing Basic RAG Pipeline Results in Python\nDESCRIPTION: This code runs a basic RAG pipeline on a sample query, displaying the query, AI response, and ground truth answer for comparison. It formats the output with clear separators to make the comparison easy to read.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_36\n\nLANGUAGE: python\nCODE:\n```\n# Print a message to indicate the start of the RAG pipeline\nprint(\"🔍 Running the Retrieval-Augmented Generation (RAG) pipeline...\")\nprint(f\"📥 Query: {sample_query}\\n\")\n\n# Run the RAG pipeline and get the response\nresponse = basic_rag_pipeline(sample_query)\n\n# Print the response with better formatting\nprint(\"🤖 AI Response:\")\nprint(\"-\" * 50)\nprint(response.strip())\nprint(\"-\" * 50)\n\n# Print the ground truth answer for comparison\nprint(\"✅ Ground Truth Answer:\")\nprint(\"-\" * 50)\nprint(expected_answer)\nprint(\"-\" * 50)\n```\n\n----------------------------------------\n\nTITLE: Chunking Text with Overlap in Python\nDESCRIPTION: Function to break text into smaller chunks with specified overlap between consecutive chunks. This technique is crucial for RAG systems to maintain context across chunk boundaries and improve retrieval accuracy.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/1_simple_rag.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef chunk_text(text, n, overlap):\n    \"\"\"\n    Chunks the given text into segments of n characters with overlap.\n\n    Args:\n    text (str): The text to be chunked.\n    n (int): The number of characters in each chunk.\n    overlap (int): The number of overlapping characters between chunks.\n\n    Returns:\n    List[str]: A list of text chunks.\n    \"\"\"\n    chunks = []  # Initialize an empty list to store the chunks\n    \n    # Loop through the text with a step size of (n - overlap)\n    for i in range(0, len(text), n - overlap):\n        # Append a chunk of text from index i to i + n to the chunks list\n        chunks.append(text[i:i + n])\n\n    return chunks  # Return the list of text chunks\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client for API Access\nDESCRIPTION: Sets up the OpenAI client with the Nebius API base URL and retrieves the API key from environment variables. This client will be used for both embedding generation and question generation.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/6_doc_augmentation_rag.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the OpenAI client with the base URL and API key\nclient = OpenAI(\n    base_url=\"https://api.studio.nebius.com/v1/\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Evaluation Scoring Constants for Response Assessment in Python\nDESCRIPTION: Defines constants for a scoring system to evaluate AI responses based on faithfulness and relevancy. The system uses three levels: full match, partial match, and no match.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/3_chunk_size_selector.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Define evaluation scoring system constants\nSCORE_FULL = 1.0     # Complete match or fully satisfactory\nSCORE_PARTIAL = 0.5  # Partial match or somewhat satisfactory\nSCORE_NONE = 0.0     # No match or unsatisfactory\n```\n\n----------------------------------------\n\nTITLE: Text Chunking Implementation\nDESCRIPTION: Creates a function to split large text into smaller, overlapping chunks for better retrieval accuracy. Allows specification of chunk size and overlap amount.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/11_feedback_loop_rag.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef chunk_text(text, n, overlap):\n    \"\"\"\n    Chunks the given text into segments of n characters with overlap.\n\n    Args:\n    text (str): The text to be chunked.\n    n (int): The number of characters in each chunk.\n    overlap (int): The number of overlapping characters between chunks.\n\n    Returns:\n    List[str]: A list of text chunks.\n    \"\"\"\n    chunks = []  # Initialize an empty list to store the chunks\n    \n    # Loop through the text with a step size of (n - overlap)\n    for i in range(0, len(text), n - overlap):\n        # Append a chunk of text from index i to i + n to the chunks list\n        chunks.append(text[i:i + n])\n\n    return chunks  # Return the list of text chunks\n```\n\n----------------------------------------\n\nTITLE: Feedback Data Loading Function\nDESCRIPTION: Implements a function to load stored feedback data from a JSON file, with error handling for missing files.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/11_feedback_loop_rag.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef load_feedback_data(feedback_file=\"feedback_data.json\"):\n    \"\"\"\n    Load feedback data from file.\n    \n    Args:\n        feedback_file (str): Path to feedback file\n        \n    Returns:\n        List[Dict]: List of feedback entries\n    \"\"\"\n    feedback_data = []\n    try:\n        with open(feedback_file, \"r\") as f:\n            for line in f:\n                if line.strip():\n                    feedback_data.append(json.loads(line.strip()))\n    except FileNotFoundError:\n        print(\"No feedback data file found. Starting with empty feedback.\")\n    \n    return feedback_data\n```\n\n----------------------------------------\n\nTITLE: Testing Retrieval with a Sample Query in Python\nDESCRIPTION: Loads validation data from a JSON file, extracts a sample query, and retrieves relevant chunks for different chunk sizes. The code demonstrates how chunk size affects retrieval results.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/3_chunk_size_selector.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Load the validation data from a JSON file\nwith open('data/val.json') as f:\n    data = json.load(f)\n\n# Extract the first query from the validation data\nquery = data[3]['question']\n\n# Retrieve relevant chunks for each chunk size\nretrieved_chunks_dict = {size: retrieve_relevant_chunks(query, text_chunks_dict[size], chunk_embeddings_dict[size]) for size in chunk_sizes}\n\n# Print retrieved chunks for chunk size 256\nprint(retrieved_chunks_dict[256])\n```\n\n----------------------------------------\n\nTITLE: Chunking Text with Overlap in Python\nDESCRIPTION: Function that divides a large text into smaller chunks with specified overlap between consecutive chunks. This is essential for processing long documents effectively in retrieval systems while maintaining context across chunk boundaries.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/13_self_rag.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef chunk_text(text, n, overlap):\n    \"\"\"\n    Chunks the given text into segments of n characters with overlap.\n\n    Args:\n    text (str): The text to be chunked.\n    n (int): The number of characters in each chunk.\n    overlap (int): The number of overlapping characters between chunks.\n\n    Returns:\n    List[str]: A list of text chunks.\n    \"\"\"\n    chunks = []  # Initialize an empty list to store the chunks\n    \n    # Loop through the text with a step size of (n - overlap)\n    for i in range(0, len(text), n - overlap):\n        # Append a chunk of text from index i to i + n to the chunks list\n        chunks.append(text[i:i + n])\n\n    return chunks  # Return the list of text chunks\n```\n\n----------------------------------------\n\nTITLE: Printing RAG Evaluation Analysis Results in Python\nDESCRIPTION: Outputs the overall analysis results comparing performance between different RAG approaches across test queries\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/19_HyDE_rag.ipynb#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nprint(\"\\n=== OVERALL ANALYSIS ===\")\nprint(evaluation_results[\"overall_analysis\"])\n```\n\n----------------------------------------\n\nTITLE: Training Parameters Initialization in Python\nDESCRIPTION: Function that initializes training parameters for the RL system, including learning rate, number of episodes, and discount factor for future rewards.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndef initialize_training_params() -> Dict[str, Union[float, int]]:\n    \"\"\"\n    Initialize training parameters such as learning rate, number of episodes, and discount factor.\n\n    Returns:\n        Dict[str, Union[float, int]]: A dictionary containing the initialized training parameters.\n    \"\"\"\n    params = {\n        \"learning_rate\": 0.01,  # Learning rate for policy updates\n        \"num_episodes\": 100,   # Total number of training episodes\n        \"discount_factor\": 0.99  # Discount factor for future rewards\n    }\n    return params\n```\n\n----------------------------------------\n\nTITLE: Text Chunking Implementation\nDESCRIPTION: Function to split large text into smaller, overlapping chunks for better retrieval performance. Implements overlapping to maintain context across chunk boundaries.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/8_reranker.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef chunk_text(text, n, overlap):\n    \"\"\"\n    Chunks the given text into segments of n characters with overlap.\n\n    Args:\n    text (str): The text to be chunked.\n    n (int): The number of characters in each chunk.\n    overlap (int): The number of overlapping characters between chunks.\n\n    Returns:\n    List[str]: A list of text chunks.\n    \"\"\"\n    chunks = []  # Initialize an empty list to store the chunks\n    \n    # Loop through the text with a step size of (n - overlap)\n    for i in range(0, len(text), n - overlap):\n        # Append a chunk of text from index i to i + n to the chunks list\n        chunks.append(text[i:i + n])\n\n    return chunks  # Return the list of text chunks\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings for Chunks in RAG\nDESCRIPTION: This snippet demonstrates the process of generating embeddings for each text chunk and its header. It uses the create_embeddings function to create numerical representations for both the chunk text and its header, which are essential for semantic search in RAG.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/5_contextual_chunk_headers_rag.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Generate embeddings for each chunk\nembeddings = []  # Initialize an empty list to store embeddings\n\n# Iterate through each text chunk with a progress bar\nfor chunk in tqdm(text_chunks, desc=\"Generating embeddings\"):\n    # Create an embedding for the chunk's text\n    text_embedding = create_embeddings(chunk[\"text\"])\n    # Create an embedding for the chunk's header\n    header_embedding = create_embeddings(chunk[\"header\"])\n    # Append the chunk's header, text, and their embeddings to the list\n    embeddings.append({\"header\": chunk[\"header\"], \"text\": chunk[\"text\"], \"embedding\": text_embedding, \"header_embedding\": header_embedding})\n```\n\n----------------------------------------\n\nTITLE: Running Full RAG Workflow in Python (Commented Out)\nDESCRIPTION: This commented-out section demonstrates how to run a full RAG workflow interactively. It includes loading accumulated feedback, getting user input, and running the full_rag_workflow function with fine-tuning enabled.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/11_feedback_loop_rag.ipynb#2025-04-23_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# # Run an interactive example\n# print(\"\\n\\n=== INTERACTIVE EXAMPLE ===\")\n# print(\"Enter your query about AI:\")\n# user_query = input()\n\n# # Load accumulated feedback\n# all_feedback = load_feedback_data()\n\n# # Run full workflow\n# result = full_rag_workflow(\n#     pdf_path=pdf_path,\n#     query=user_query,\n#     feedback_data=all_feedback,\n#     fine_tune=True\n# )\n```\n\n----------------------------------------\n\nTITLE: Creating Embeddings for Semantic Chunks in Python\nDESCRIPTION: Defines a function to create embeddings for each text chunk using the previously defined get_embedding function.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/2_semantic_chunking.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef create_embeddings(text_chunks):\n    \"\"\"\n    Creates embeddings for each text chunk.\n\n    Args:\n    text_chunks (List[str]): List of text chunks.\n\n    Returns:\n    List[np.ndarray]: List of embedding vectors.\n    \"\"\"\n    # Generate embeddings for each text chunk using the get_embedding function\n    return [get_embedding(chunk) for chunk in text_chunks]\n\n# Create chunk embeddings using the create_embeddings function\nchunk_embeddings = create_embeddings(text_chunks)\n```\n\n----------------------------------------\n\nTITLE: Processing PDF and Creating Text Chunks in Python\nDESCRIPTION: Code that loads a PDF, extracts its text, and segments it into overlapping chunks. It also prints the number of chunks created and shows the first chunk as an example.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/4_context_enriched_rag.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Define the path to the PDF file\npdf_path = \"data/AI_Information.pdf\"\n\n# Extract text from the PDF file\nextracted_text = extract_text_from_pdf(pdf_path)\n\n# Chunk the extracted text into segments of 1000 characters with an overlap of 200 characters\ntext_chunks = chunk_text(extracted_text, 1000, 200)\n\n# Print the number of text chunks created\nprint(\"Number of text chunks:\", len(text_chunks))\n\n# Print the first text chunk\nprint(\"\\nFirst text chunk:\")\nprint(text_chunks[0])\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from PDF File in Python\nDESCRIPTION: Defines a function to extract text from a PDF file using PyMuPDF library. It iterates through each page, extracts text, and concatenates it into a single string.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/2_semantic_chunking.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef extract_text_from_pdf(pdf_path):\n    \"\"\"\n    Extracts text from a PDF file.\n\n    Args:\n    pdf_path (str): Path to the PDF file.\n\n    Returns:\n    str: Extracted text from the PDF.\n    \"\"\"\n    # Open the PDF file\n    mypdf = fitz.open(pdf_path)\n    all_text = \"\"  # Initialize an empty string to store the extracted text\n    \n    # Iterate through each page in the PDF\n    for page in mypdf:\n        # Extract text from the current page and add spacing\n        all_text += page.get_text(\"text\") + \" \"\n\n    # Return the extracted text, stripped of leading/trailing whitespace\n    return all_text.strip()\n\n# Define the path to the PDF file\npdf_path = \"data/AI_Information.pdf\"\n\n# Extract text from the PDF file\nextracted_text = extract_text_from_pdf(pdf_path)\n\n# Print the first 500 characters of the extracted text\nprint(extracted_text[:500])\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Python Environment\nDESCRIPTION: Code example showing how to set the OpenAI API key as an environment variable within a Python script or notebook. This is a prerequisite for running the RAG notebooks.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/README.md#2025-04-23_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_NEBIUS_AI_API_KEY\"\n```\n\n----------------------------------------\n\nTITLE: Computing Cosine Similarity Between Vectors in Python\nDESCRIPTION: This function calculates the cosine similarity between two vectors using NumPy. It computes the dot product of the vectors and their magnitudes, then returns the ratio as the similarity score.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n    \"\"\"\n    Compute the cosine similarity between two vectors.\n\n    Args:\n        vec1 (np.ndarray): The first vector.\n        vec2 (np.ndarray): The second vector.\n\n    Returns:\n        float: The cosine similarity between the two vectors, ranging from -1 to 1.\n    \"\"\"\n    # Compute the dot product of the two vectors\n    dot_product = np.dot(vec1, vec2)\n    # Compute the magnitude (norm) of the first vector\n    norm_vec1 = np.linalg.norm(vec1)\n    # Compute the magnitude (norm) of the second vector\n    norm_vec2 = np.linalg.norm(vec2)\n    # Return the cosine similarity as the ratio of the dot product to the product of the norms\n    return dot_product / (norm_vec1 * norm_vec2)\n```\n\n----------------------------------------\n\nTITLE: Document Processing Initialization in Python\nDESCRIPTION: Code to initialize document processing by setting the PDF path and calling the process_document function with specific parameters for chunk size, overlap, and questions per chunk.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/6_doc_augmentation_rag.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Define the path to the PDF file\npdf_path = \"data/AI_Information.pdf\"\n\n# Process the document (extract text, create chunks, generate questions, build vector store)\ntext_chunks, vector_store = process_document(\n    pdf_path, \n    chunk_size=1000, \n    chunk_overlap=200, \n    questions_per_chunk=3\n)\n\nprint(f\"Vector store contains {len(vector_store.texts)} items\")\n```\n\n----------------------------------------\n\nTITLE: Running RAG with Compression on AI Ethics Document in Python\nDESCRIPTION: An implementation example showing how to run the compression evaluation system on a document about AI ethics. It specifies a query about ethical concerns in AI decision-making, provides a reference answer for evaluation, and compares three compression techniques: selective, summary, and extraction.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/10_contextual_compression.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Path to the PDF document containing information on AI ethics  \npdf_path = \"data/AI_Information.pdf\" \n\n# Query to extract relevant information from the document  \nquery = \"What are the ethical concerns surrounding the use of AI in decision-making?\"  \n\n# Optional reference answer for evaluation  \nreference_answer = \"\"\"  \nThe use of AI in decision-making raises several ethical concerns.  \n- Bias in AI models can lead to unfair or discriminatory outcomes, especially in critical areas like hiring, lending, and law enforcement.  \n- Lack of transparency and explainability in AI-driven decisions makes it difficult for individuals to challenge unfair outcomes.  \n- Privacy risks arise as AI systems process vast amounts of personal data, often without explicit consent.  \n- The potential for job displacement due to automation raises social and economic concerns.  \n- AI decision-making may also concentrate power in the hands of a few large tech companies, leading to accountability challenges.  \n- Ensuring fairness, accountability, and transparency in AI systems is essential for ethical deployment.  \n\"\"\"  \n\n# Run evaluation with different compression techniques  \n# Compression types:  \n# - \"selective\": Retains key details while omitting less relevant parts  \n# - \"summary\": Provides a concise version of the information  \n# - \"extraction\": Extracts relevant sentences verbatim from the document  \nresults = evaluate_compression(  \n    pdf_path=pdf_path,  \n    query=query,  \n    reference_answer=reference_answer,  \n    compression_types=[\"selective\", \"summary\", \"extraction\"]  \n)\n```\n\n----------------------------------------\n\nTITLE: Saving Embeddings to JSON File in Python\nDESCRIPTION: This function saves a NumPy array of embeddings to a JSON file. It opens the specified file in write mode with UTF-8 encoding and uses json.dump() to save the embeddings as a list.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef save_embeddings(embeddings: np.ndarray, output_file: str) -> None:\n    \"\"\"\n    Save embeddings to a JSON file.\n\n    Args:\n        embeddings (np.ndarray): A NumPy array containing the embeddings to save.\n        output_file (str): The path to the output JSON file where embeddings will be saved.\n\n    Returns:\n        None\n    \"\"\"\n    # Open the specified file in write mode with UTF-8 encoding\n    with open(output_file, 'w', encoding='utf-8') as file:\n        # Convert the NumPy array to a list and save it as JSON\n        json.dump(embeddings.tolist(), file)\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from PDF File using PyMuPDF\nDESCRIPTION: Function to extract text from a PDF file using the PyMuPDF library. It iterates through each page of the PDF and concatenates the extracted text.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/10_contextual_compression.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef extract_text_from_pdf(pdf_path):\n    \"\"\"\n    Extracts text from a PDF file and prints the first `num_chars` characters.\n\n    Args:\n    pdf_path (str): Path to the PDF file.\n\n    Returns:\n    str: Extracted text from the PDF.\n    \"\"\"\n    # Open the PDF file\n    mypdf = fitz.open(pdf_path)\n    all_text = \"\"  # Initialize an empty string to store the extracted text\n\n    # Iterate through each page in the PDF\n    for page_num in range(mypdf.page_count):\n        page = mypdf[page_num]  # Get the page\n        text = page.get_text(\"text\")  # Extract text from the page\n        all_text += text  # Append the extracted text to the all_text string\n\n    return all_text  # Return the extracted text\n```\n\n----------------------------------------\n\nTITLE: Chunking Text with Overlap in Python\nDESCRIPTION: Implements a function to split text into overlapping chunks of specified size. Each chunk includes metadata about its position in the original text, which is useful for context retrieval and reconstruction.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/19_HyDE_rag.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef chunk_text(text, chunk_size=1000, overlap=200):\n    \"\"\"\n    Split text into overlapping chunks.\n    \n    Args:\n        text (str): Input text to chunk\n        chunk_size (int): Size of each chunk in characters\n        overlap (int): Overlap between chunks in characters\n        \n    Returns:\n        List[Dict]: List of chunks with metadata\n    \"\"\"\n    chunks = []  # Initialize an empty list to store the chunks\n    \n    # Iterate over the text in steps of (chunk_size - overlap)\n    for i in range(0, len(text), chunk_size - overlap):\n        chunk_text = text[i:i + chunk_size]  # Extract the chunk of text\n        if chunk_text:  # Ensure we don't add empty chunks\n            chunks.append({\n                \"text\": chunk_text,  # Add the chunk text\n                \"metadata\": {\n                    \"start_pos\": i,  # Start position of the chunk in the original text\n                    \"end_pos\": i + len(chunk_text)  # End position of the chunk in the original text\n                }\n            })\n    \n    print(f\"Created {len(chunks)} text chunks\")  # Print the number of chunks created\n    return chunks  # Return the list of chunks with metadata\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from PDF Documents in Python\nDESCRIPTION: Defines a function to extract text from a PDF file using the PyMuPDF (fitz) library and applies it to a sample PDF. The function processes each page and concatenates the text with spacing.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/3_chunk_size_selector.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef extract_text_from_pdf(pdf_path):\n    \"\"\"\n    Extracts text from a PDF file.\n\n    Args:\n    pdf_path (str): Path to the PDF file.\n\n    Returns:\n    str: Extracted text from the PDF.\n    \"\"\"\n    # Open the PDF file\n    mypdf = fitz.open(pdf_path)\n    all_text = \"\"  # Initialize an empty string to store the extracted text\n    \n    # Iterate through each page in the PDF\n    for page in mypdf:\n        # Extract text from the current page and add spacing\n        all_text += page.get_text(\"text\") + \" \"\n\n    # Return the extracted text, stripped of leading/trailing whitespace\n    return all_text.strip()\n\n# Define the path to the PDF file\npdf_path = \"data/AI_Information.pdf\"\n\n# Extract text from the PDF file\nextracted_text = extract_text_from_pdf(pdf_path)\n\n# Print the first 500 characters of the extracted text\nprint(extracted_text[:500])\n```\n\n----------------------------------------\n\nTITLE: Text Embedding Generation\nDESCRIPTION: Function to create embeddings for text chunks using specified model with batch processing support.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/9_rse.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef create_embeddings(texts, model=\"BAAI/bge-en-icl\"):\n    \"\"\"\n    Generate embeddings for texts.\n    \n    Args:\n        texts (List[str]): List of texts to embed\n        model (str): Embedding model to use\n        \n    Returns:\n        List[List[float]]: List of embedding vectors\n    \"\"\"\n    if not texts:\n        return []  # Return an empty list if no texts are provided\n        \n    # Process in batches if the list is long\n    batch_size = 100  # Adjust based on your API limits\n    all_embeddings = []  # Initialize a list to store all embeddings\n    \n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i + batch_size]  # Get the current batch of texts\n        \n        # Create embeddings for the current batch using the specified model\n        response = client.embeddings.create(\n            input=batch,\n            model=model\n        )\n        \n        # Extract embeddings from the response\n        batch_embeddings = [item.embedding for item in response.data]\n        all_embeddings.extend(batch_embeddings)  # Add the batch embeddings to the list\n        \n    return all_embeddings  # Return the list of all embeddings\n```\n\n----------------------------------------\n\nTITLE: Feedback Storage Function\nDESCRIPTION: Implements a function to store user feedback in a JSON file for persistence.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/11_feedback_loop_rag.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef store_feedback(feedback, feedback_file=\"feedback_data.json\"):\n    \"\"\"\n    Store feedback in a JSON file.\n    \n    Args:\n        feedback (Dict): Feedback data\n        feedback_file (str): Path to feedback file\n    \"\"\"\n    with open(feedback_file, \"a\") as f:\n        json.dump(feedback, f)\n        f.write(\"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Cleaning Text for Processing\nDESCRIPTION: Defines a function to clean text by removing extra whitespace, special characters, and fixing common OCR issues. This preprocessing step improves the quality of embeddings and keyword search.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/16_fusion_rag.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef clean_text(text):\n    \"\"\"\n    Clean text by removing extra whitespace and special characters.\n    \n    Args:\n        text (str): Input text\n        \n    Returns:\n        str: Cleaned text\n    \"\"\"\n    # Replace multiple whitespace characters (including newlines and tabs) with a single space\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Fix common OCR issues by replacing tab and newline characters with a space\n    text = text.replace('\\\\t', ' ')\n    text = text.replace('\\\\n', ' ')\n    \n    # Remove any leading or trailing whitespace and ensure single spaces between words\n    text = ' '.join(text.split())\n    \n    return text\n```\n\n----------------------------------------\n\nTITLE: Implementing Simple Vector Store\nDESCRIPTION: Custom vector store implementation using NumPy for managing document chunks and their embeddings, including similarity search functionality.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/14_proposition_chunking.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleVectorStore:\n    \"\"\"\n    A simple vector store implementation using NumPy.\n    \"\"\"\n    def __init__(self):\n        # Initialize lists to store vectors, texts, and metadata\n        self.vectors = []\n        self.texts = []\n        self.metadata = []\n    \n    def add_item(self, text, embedding, metadata=None):\n        \"\"\"\n        Add an item to the vector store.\n        \n        Args:\n            text (str): The text content\n            embedding (List[float]): The embedding vector\n            metadata (Dict, optional): Additional metadata\n        \"\"\"\n        # Append the embedding, text, and metadata to their respective lists\n        self.vectors.append(np.array(embedding))\n        self.texts.append(text)\n        self.metadata.append(metadata or {})\n    \n    def add_items(self, texts, embeddings, metadata_list=None):\n        \"\"\"\n        Add multiple items to the vector store.\n        \n        Args:\n            texts (List[str]): List of text contents\n            embeddings (List[List[float]]): List of embedding vectors\n            metadata_list (List[Dict], optional): List of metadata dictionaries\n        \"\"\"\n        # If no metadata list is provided, create an empty dictionary for each text\n        if metadata_list is None:\n            metadata_list = [{} for _ in range(len(texts))]\n        \n        # Add each text, embedding, and metadata to the store\n        for text, embedding, metadata in zip(texts, embeddings, metadata_list):\n            self.add_item(text, embedding, metadata)\n    \n    def similarity_search(self, query_embedding, k=5):\n        \"\"\"\n        Find the most similar items to a query embedding.\n        \n        Args:\n            query_embedding (List[float]): Query embedding vector\n            k (int): Number of results to return\n            \n        Returns:\n            List[Dict]: Top k most similar items\n        \"\"\"\n        # Return an empty list if there are no vectors in the store\n        if not self.vectors:\n            return []\n        \n        # Convert query embedding to a numpy array\n        query_vector = np.array(query_embedding)\n        \n        # Calculate similarities using cosine similarity\n        similarities = []\n        for i, vector in enumerate(self.vectors):\n            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n            similarities.append((i, similarity))\n        \n        # Sort by similarity in descending order\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        \n        # Collect the top k results\n        results = []\n        for i in range(min(k, len(similarities))):\n            idx, score = similarities[i]\n            results.append({\n                \"text\": self.texts[idx],\n                \"metadata\": self.metadata[idx],\n                \"similarity\": float(score)  # Convert to float for JSON serialization\n            })\n        \n        return results\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client for API Access in Python\nDESCRIPTION: This code snippet initializes the OpenAI client with a custom base URL and API key retrieved from environment variables. It's used to generate embeddings and responses in the CRAG system.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/20_crag.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the OpenAI client with the base URL and API key\nclient = OpenAI(\n    base_url=\"https://api.studio.nebius.com/v1/\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Chunks for RAG in Python\nDESCRIPTION: Implements a function to preprocess all text chunks using the previously defined preprocess_text function. Applies preprocessing to each chunk in the list.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef preprocess_chunks(chunks: List[str]) -> List[str]:\n    \"\"\"\n    Apply preprocessing to all text chunks.\n\n    Args:\n        chunks (List[str]): A list of text chunks to preprocess.\n\n    Returns:\n        List[str]: A list of preprocessed text chunks.\n    \"\"\"\n    # Apply the preprocess_text function to each chunk in the list\n    return [preprocess_text(chunk) for chunk in chunks]\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from PDF for RAG Processing\nDESCRIPTION: This function extracts text from a PDF file. It iterates through each page of the PDF, extracts the text content, and combines it into a single string. This is a crucial step in preparing document data for RAG processing.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/5_contextual_chunk_headers_rag.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef extract_text_from_pdf(pdf_path):\n    \"\"\"\n    Extracts text from a PDF file and prints the first `num_chars` characters.\n\n    Args:\n    pdf_path (str): Path to the PDF file.\n\n    Returns:\n    str: Extracted text from the PDF.\n    \"\"\"\n    # Open the PDF file\n    mypdf = fitz.open(pdf_path)\n    all_text = \"\"  # Initialize an empty string to store the extracted text\n\n    # Iterate through each page in the PDF\n    for page_num in range(mypdf.page_count):\n        page = mypdf[page_num]  # Get the page\n        text = page.get_text(\"text\")  # Extract text from the page\n        all_text += text  # Append the extracted text to the all_text string\n\n    return all_text  # Return the extracted text\n```\n\n----------------------------------------\n\nTITLE: OpenAI Client Configuration\nDESCRIPTION: Initializes the OpenAI client with custom base URL and API key for generating embeddings and responses.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/11_feedback_loop_rag.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the OpenAI client with the base URL and API key\nclient = OpenAI(\n    base_url=\"https://api.studio.nebius.com/v1/\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n)\n```\n\n----------------------------------------\n\nTITLE: PDF Text Extraction Function\nDESCRIPTION: Implements a function to extract text content from PDF files using PyMuPDF library. Processes the PDF page by page and concatenates all text content.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/11_feedback_loop_rag.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef extract_text_from_pdf(pdf_path):\n    \"\"\"\n    Extracts text from a PDF file and prints the first `num_chars` characters.\n\n    Args:\n    pdf_path (str): Path to the PDF file.\n\n    Returns:\n    str: Extracted text from the PDF.\n    \"\"\"\n    # Open the PDF file\n    mypdf = fitz.open(pdf_path)\n    all_text = \"\"  # Initialize an empty string to store the extracted text\n\n    # Iterate through each page in the PDF\n    for page_num in range(mypdf.page_count):\n        page = mypdf[page_num]  # Get the page\n        text = page.get_text(\"text\")  # Extract text from the page\n        all_text += text  # Append the extracted text to the all_text string\n\n    return all_text  # Return the extracted text\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client in Python\nDESCRIPTION: Sets up the OpenAI client by initializing it with the Nebius API base URL and retrieving the API key from environment variables. This client will be used to generate embeddings and responses.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/19_HyDE_rag.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the OpenAI client with the base URL and API key\nclient = OpenAI(\n    base_url=\"https://api.studio.nebius.com/v1/\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n)\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from PDF Files using PyMuPDF in Python\nDESCRIPTION: Function to extract text content from a PDF file using the PyMuPDF (fitz) library. It iterates through each page in the PDF and combines all text into a single string.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/12_adaptive_rag.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef extract_text_from_pdf(pdf_path):\n    \"\"\"\n    Extracts text from a PDF file and prints the first `num_chars` characters.\n\n    Args:\n    pdf_path (str): Path to the PDF file.\n\n    Returns:\n    str: Extracted text from the PDF.\n    \"\"\"\n    # Open the PDF file\n    mypdf = fitz.open(pdf_path)\n    all_text = \"\"  # Initialize an empty string to store the extracted text\n\n    # Iterate through each page in the PDF\n    for page_num in range(mypdf.page_count):\n        page = mypdf[page_num]  # Get the page\n        text = page.get_text(\"text\")  # Extract text from the page\n        all_text += text  # Append the extracted text to the all_text string\n\n    return all_text  # Return the extracted text\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from PDF Files\nDESCRIPTION: Defines a function to extract text content from PDF files using PyMuPDF (fitz). It processes each page in the document and concatenates the extracted text into a single string.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/16_fusion_rag.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef extract_text_from_pdf(pdf_path):\n    \"\"\"\n    Extract text content from a PDF file.\n    \n    Args:\n        pdf_path (str): Path to the PDF file\n        \n    Returns:\n        str: Extracted text content\n    \"\"\"\n    print(f\"Extracting text from {pdf_path}...\")  # Print the path of the PDF being processed\n    pdf_document = fitz.open(pdf_path)  # Open the PDF file using PyMuPDF\n    text = \"\"  # Initialize an empty string to store the extracted text\n    \n    # Iterate through each page in the PDF\n    for page_num in range(pdf_document.page_count):\n        page = pdf_document[page_num]  # Get the page object\n        text += page.get_text()  # Extract text from the page and append to the text string\n    \n    return text  # Return the extracted text content\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client for API Access in Python\nDESCRIPTION: Code that initializes the OpenAI client with a base URL and API key from environment variables. This client will be used for generating embeddings and AI responses.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/4_context_enriched_rag.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the OpenAI client with the base URL and API key\nclient = OpenAI(\n    base_url=\"https://api.studio.nebius.com/v1/\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Fusion Retrieval System\nDESCRIPTION: Sets up the required Python libraries for implementing a fusion retrieval system, including numpy for vector operations, rank_bm25 for keyword search, OpenAI for embeddings, and PyMuPDF for PDF processing.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/16_fusion_rag.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport numpy as np\nfrom rank_bm25 import BM25Okapi\nimport fitz\nfrom openai import OpenAI\nimport re\nimport json\nimport time\nfrom sklearn.metrics.pairwise import cosine_similarity\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from PDF Files in Python using PyMuPDF\nDESCRIPTION: Function that extracts text content from a PDF file using the PyMuPDF (fitz) library. It iterates through each page in the PDF and combines the extracted text into a single string.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/13_self_rag.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef extract_text_from_pdf(pdf_path):\n    \"\"\"\n    Extracts text from a PDF file and prints the first `num_chars` characters.\n\n    Args:\n    pdf_path (str): Path to the PDF file.\n\n    Returns:\n    str: Extracted text from the PDF.\n    \"\"\"\n    # Open the PDF file\n    mypdf = fitz.open(pdf_path)\n    all_text = \"\"  # Initialize an empty string to store the extracted text\n\n    # Iterate through each page in the PDF\n    for page_num in range(mypdf.page_count):\n        page = mypdf[page_num]  # Get the page\n        text = page.get_text(\"text\")  # Extract text from the page\n        all_text += text  # Append the extracted text to the all_text string\n\n    return all_text  # Return the extracted text\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI API Client\nDESCRIPTION: Initialization of OpenAI client with custom base URL and API key from environment variables.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/14_proposition_chunking.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the OpenAI client with the base URL and API key\nclient = OpenAI(\n    base_url=\"https://api.studio.nebius.com/v1/\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n)\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from PDF Files in Python\nDESCRIPTION: This function uses PyMuPDF to extract text content from a PDF file. It iterates through each page of the PDF and concatenates the extracted text.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/20_crag.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef extract_text_from_pdf(pdf_path):\n    \"\"\"\n    Extract text content from a PDF file.\n    \n    Args:\n        pdf_path (str): Path to the PDF file\n        \n    Returns:\n        str: Extracted text content\n    \"\"\"\n    print(f\"Extracting text from {pdf_path}...\")\n    \n    # Open the PDF file\n    pdf = fitz.open(pdf_path)\n    text = \"\"\n    \n    # Iterate through each page in the PDF\n    for page_num in range(len(pdf)):\n        page = pdf[page_num]\n        # Extract text from the current page and append it to the text variable\n        text += page.get_text()\n    \n    return text\n```\n\n----------------------------------------\n\nTITLE: Defining Test Queries and Reference Answers for RAG Evaluation in Python\nDESCRIPTION: This code snippet defines test queries and reference answers for evaluating a RAG system. It includes a path to the AI document, a list of test queries, and corresponding reference answers. Some queries and answers are commented out to reduce the number of tests.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/11_feedback_loop_rag.ipynb#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# AI Document Path\npdf_path = \"data/AI_Information.pdf\"\n\n# Define test queries\ntest_queries = [\n    \"What is a neural network and how does it function?\",\n\n    #################################################################################\n    ### Commented out queries to reduce the number of queries for testing purposes ###\n    \n    # \"Describe the process and applications of reinforcement learning.\",\n    # \"What are the main applications of natural language processing in today's technology?\",\n    # \"Explain the impact of overfitting in machine learning models and how it can be mitigated.\"\n]\n\n# Define reference answers for evaluation\nreference_answers = [\n    \"A neural network is a series of algorithms that attempt to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates. It consists of layers of nodes, with each node representing a neuron. Neural networks function by adjusting the weights of connections between nodes based on the error of the output compared to the expected result.\",\n\n    ############################################################################################\n    #### Commented out reference answers to reduce the number of queries for testing purposes ###\n\n#     \"Reinforcement learning is a type of machine learning where an agent learns to make decisions by performing actions in an environment to maximize cumulative reward. It involves exploration, exploitation, and learning from the consequences of actions. Applications include robotics, game playing, and autonomous vehicles.\",\n#     \"The main applications of natural language processing in today's technology include machine translation, sentiment analysis, chatbots, information retrieval, text summarization, and speech recognition. NLP enables machines to understand and generate human language, facilitating human-computer interaction.\",\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client for Query Transformations in Python\nDESCRIPTION: Sets up the OpenAI client with the Nebius API base URL and retrieves the API key from environment variables. This client will be used to generate embeddings and responses for the query transformation techniques.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/7_query_transform.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the OpenAI client with the base URL and API key\nclient = OpenAI(\n    base_url=\"https://api.studio.nebius.com/v1/\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n)\n```\n\n----------------------------------------\n\nTITLE: Query Execution and Result Processing in Python\nDESCRIPTION: Code to execute a query against the vector store and process the results, organizing them by type (chunks and questions) and displaying them with similarity scores.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/6_doc_augmentation_rag.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Load the validation data from a JSON file\nwith open('data/val.json') as f:\n    data = json.load(f)\n\n# Extract the first query from the validation data\nquery = data[0]['question']\n\n# Perform semantic search to find relevant content\nsearch_results = semantic_search(query, vector_store, k=5)\n\nprint(\"Query:\", query)\nprint(\"\\nSearch Results:\")\n\n# Organize results by type\nchunk_results = []\nquestion_results = []\n\nfor result in search_results:\n    if result[\"metadata\"][\"type\"] == \"chunk\":\n        chunk_results.append(result)\n    else:\n        question_results.append(result)\n\n# Print chunk results first\nprint(\"\\nRelevant Document Chunks:\")\nfor i, result in enumerate(chunk_results):\n    print(f\"Context {i + 1} (similarity: {result['similarity']:.4f}):\")\n    print(result[\"text\"][:300] + \"...\")\n    print(\"=====================================\")\n\n# Then print question matches\nprint(\"\\nMatched Questions:\")\nfor i, result in enumerate(question_results):\n    print(f\"Question {i + 1} (similarity: {result['similarity']:.4f}):\")\n    print(result[\"text\"])\n    chunk_idx = result[\"metadata\"][\"chunk_index\"]\n    print(f\"From chunk {chunk_idx}\")\n    print(\"=====================================\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Required Libraries for RAG Implementation\nDESCRIPTION: Imports necessary Python libraries for PDF processing, numerical operations, and OpenAI API integration.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/14_proposition_chunking.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport numpy as np\nimport json\nimport fitz\nfrom openai import OpenAI\nimport re\n```\n\n----------------------------------------\n\nTITLE: PDF Text Extraction Function\nDESCRIPTION: Function to extract text content from PDF files using PyMuPDF (fitz) library. Processes each page sequentially and combines the text.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/9_rse.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef extract_text_from_pdf(pdf_path):\n    \"\"\"\n    Extracts text from a PDF file and prints the first `num_chars` characters.\n\n    Args:\n    pdf_path (str): Path to the PDF file.\n\n    Returns:\n    str: Extracted text from the PDF.\n    \"\"\"\n    # Open the PDF file\n    mypdf = fitz.open(pdf_path)\n    all_text = \"\"  # Initialize an empty string to store the extracted text\n\n    # Iterate through each page in the PDF\n    for page_num in range(mypdf.page_count):\n        page = mypdf[page_num]  # Get the page\n        text = page.get_text(\"text\")  # Extract text from the page\n        all_text += text  # Append the extracted text to the all_text string\n\n    return all_text  # Return the extracted text\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI API Client for Embeddings and Responses in Python\nDESCRIPTION: This code initializes the OpenAI client with a custom base URL and API key for generating embeddings and responses. The API key is retrieved from environment variables for security.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/18_hierarchy_rag.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the OpenAI client with the base URL and API key\nclient = OpenAI(\n    base_url=\"https://api.studio.nebius.com/v1/\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for RAG System in Python\nDESCRIPTION: Imports necessary libraries including fitz for PDF processing, numpy for vector operations, and OpenAI client for embeddings and response generation.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/4_context_enriched_rag.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport fitz\nimport os\nimport numpy as np\nimport json\nfrom openai import OpenAI\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for RAG Implementation in Python\nDESCRIPTION: Imports the necessary Python libraries for implementing a RAG system. These include fitz for PDF processing, os for environment variables, numpy for numerical operations, json for data handling, and OpenAI for API access.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/1_simple_rag.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport fitz\nimport os\nimport numpy as np\nimport json\nfrom openai import OpenAI\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Text for RAG in Python\nDESCRIPTION: Defines a function to preprocess text by converting to lowercase and removing special characters. This step is optional in the RAG pipeline.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef preprocess_text(text: str) -> str:\n    \"\"\"\n    Preprocess the input text by converting it to lowercase and removing special characters.\n\n    Args:\n        text (str): The input text to preprocess.\n\n    Returns:\n        str: The preprocessed text with only alphanumeric characters and spaces.\n    \"\"\"\n    # Convert the text to lowercase\n    text = text.lower()\n    # Remove special characters, keeping only alphanumeric characters and spaces\n    text = ''.join(char for char in text if char.isalnum() or char.isspace())\n    return text\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Self-RAG Implementation in Python\nDESCRIPTION: Imports necessary libraries for the Self-RAG implementation, including os for environment variables, numpy for vector operations, json for data handling, fitz (PyMuPDF) for PDF processing, and OpenAI for embeddings and completions.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/13_self_rag.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport numpy as np\nimport json\nimport fitz\nfrom openai import OpenAI\nimport re\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client for Embeddings Generation\nDESCRIPTION: Sets up the OpenAI client with the Nebius API base URL and retrieves the API key from environment variables. This client will be used to generate embeddings for vector search.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/16_fusion_rag.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the OpenAI client with the base URL and API key\nclient = OpenAI(\n    base_url=\"https://api.studio.nebius.com/v1/\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for RAG Pipeline in Python\nDESCRIPTION: Imports necessary libraries for PDF processing, data manipulation, and API interaction with OpenAI for a RAG system.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/3_chunk_size_selector.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport fitz\nimport os\nimport numpy as np\nimport json\nfrom openai import OpenAI\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client for RAG in Python\nDESCRIPTION: Sets up the OpenAI client with custom base URL and API key for generating responses and embeddings. Uses environment variables for secure API key storage.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclient = OpenAI(\n    base_url=\"https://api.studio.nebius.com/v1/\",  # Base URL for (eg. ollama api, anyother llm api provider)\n    api_key= os.environ[\"OPENAI_API_KEY\"]  # API key for authentication \n)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for RSE-RAG\nDESCRIPTION: Initial setup importing necessary Python libraries for PDF processing, numerical operations, and OpenAI integration.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/9_rse.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport fitz\nimport os\nimport numpy as np\nimport json\nfrom openai import OpenAI\nimport re\n```\n\n----------------------------------------\n\nTITLE: OpenAI API Client Initialization\nDESCRIPTION: Configuration of the OpenAI client with custom base URL and API key from environment variables.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/8_reranker.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the OpenAI client with the base URL and API key\nclient = OpenAI(\n    base_url=\"https://api.studio.nebius.com/v1/\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI API Client in Python\nDESCRIPTION: Sets up the OpenAI client with a custom base URL and API key retrieved from environment variables for generating embeddings and responses.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/3_chunk_size_selector.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the OpenAI client with the base URL and API key\nclient = OpenAI(\n    base_url=\"https://api.studio.nebius.com/v1/\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI API Client in Python\nDESCRIPTION: Sets up the OpenAI client with the base URL and API key for generating embeddings and responses.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/2_semantic_chunking.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the OpenAI client with the base URL and API key\nclient = OpenAI(\n    base_url=\"https://api.studio.nebius.com/v1/\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI API Client\nDESCRIPTION: Set up the OpenAI client with the base URL and API key for generating embeddings and responses.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/10_contextual_compression.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the OpenAI client with the base URL and API key\nclient = OpenAI(\n    base_url=\"https://api.studio.nebius.com/v1/\",\n    api_key= os.environ.get(\"OPENAI_API_KEY\") # Use your OpenAI API key\n)\n```\n\n----------------------------------------\n\nTITLE: Executing Semantic Search Query on Text Chunks in Python\nDESCRIPTION: Code for running a semantic search query on previously extracted text chunks. It loads a validation query from a JSON file, retrieves the top 2 most relevant chunks, and displays the results.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/1_simple_rag.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Load the validation data from a JSON file\nwith open('data/val.json') as f:\n    data = json.load(f)\n\n# Extract the first query from the validation data\nquery = data[0]['question']\n\n# Perform semantic search to find the top 2 most relevant text chunks for the query\ntop_chunks = semantic_search(query, text_chunks, response.data, k=2)\n\n# Print the query\nprint(\"Query:\", query)\n\n# Print the top 2 most relevant text chunks\nfor i, chunk in enumerate(top_chunks):\n    print(f\"Context {i + 1}:\\n{chunk}\\n=====================================\")\n```\n\n----------------------------------------\n\nTITLE: Encoding Images as Base64 in Python\nDESCRIPTION: This function encodes an image file as a base64 string. It takes a file path as input, reads the image in binary mode, and returns the base64 encoded string representation of the image.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/15_multimodel_rag.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef encode_image(image_path):\n    \"\"\"\n    Encode an image file as base64.\n    \n    Args:\n        image_path (str): Path to the image file\n        \n    Returns:\n        str: Base64 encoded image\n    \"\"\"\n    # Open the image file in binary read mode\n    with open(image_path, \"rb\") as image_file:\n        # Read the image file and encode it to base64\n        encoded_image = base64.b64encode(image_file.read())\n        # Decode the base64 bytes to a string and return\n        return encoded_image.decode('utf-8')\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Adaptive RAG System in Python\nDESCRIPTION: Imports necessary libraries for PDF processing, vector operations, and OpenAI API integration to build an adaptive retrieval system.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/12_adaptive_rag.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport numpy as np\nimport json\nimport fitz\nfrom openai import OpenAI\nimport re\n```\n\n----------------------------------------\n\nTITLE: PDF File Path Definition in Python\nDESCRIPTION: Simple code snippet defining the path to the PDF file for processing.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/6_doc_augmentation_rag.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Define the path to the PDF file\npdf_path = \"data/AI_Information.pdf\"\n```\n\n----------------------------------------\n\nTITLE: Executing Compression Results Visualization\nDESCRIPTION: Example code showing how to call the visualization function with results data to display compression comparisons.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/10_contextual_compression.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Visualize the compression results\nvisualize_compression_results(results)\n```\n\n----------------------------------------\n\nTITLE: OpenAI Client Initialization\nDESCRIPTION: Configuration of OpenAI client with custom base URL and API key for generating embeddings and responses.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/9_rse.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the OpenAI client with the base URL and API key\nclient = OpenAI(\n    base_url=\"https://api.studio.nebius.com/v1/\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Semantic Chunking in Python\nDESCRIPTION: Imports necessary libraries for PDF text extraction, numerical operations, JSON handling, and OpenAI API integration.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/2_semantic_chunking.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport fitz\nimport os\nimport numpy as np\nimport json\nfrom openai import OpenAI\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with pip\nDESCRIPTION: Command for installing the required Python dependencies for the RAG techniques from the requirements.txt file.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/README.md#2025-04-23_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for RAG System Enhancement\nDESCRIPTION: Import necessary libraries for PDF processing, numerical operations, and OpenAI API integration.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/10_contextual_compression.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport fitz\nimport os\nimport numpy as np\nimport json\nfrom openai import OpenAI\n```\n\n----------------------------------------\n\nTITLE: Cloning the Repository\nDESCRIPTION: Bash commands for cloning the repository from GitHub and navigating to the project directory to get started with the RAG techniques.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/README.md#2025-04-23_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\ngit clone https://github.com/FareedKhan-dev/all-rag-techniques.git\ncd all-rag-techniques\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for RAG System\nDESCRIPTION: Imports necessary Python libraries for PDF processing, numerical operations, and OpenAI API integration.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/8_reranker.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport fitz\nimport os\nimport numpy as np\nimport json\nfrom openai import OpenAI\nimport re\n```\n\n----------------------------------------\n\nTITLE: Loading Documents for RAG in Python\nDESCRIPTION: Defines a function to load all .txt files from a specified directory. Returns a list of document contents as strings.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef load_documents(directory_path: str) -> List[str]:\n    \"\"\"\n    Load all text documents from the specified directory.\n\n    Args:\n        directory_path (str): Path to the directory containing text files.\n\n    Returns:\n        List[str]: A list of strings, where each string is the content of a text file.\n    \"\"\"\n    documents = []  # Initialize an empty list to store document contents\n    for filename in os.listdir(directory_path):  # Iterate through all files in the directory\n        if filename.endswith(\".txt\"):  # Check if the file has a .txt extension\n            # Open the file in read mode with UTF-8 encoding and append its content to the list\n            with open(os.path.join(directory_path, filename), 'r', encoding='utf-8') as file:\n                documents.append(file.read())\n    return documents  # Return the list of document contents\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Windows\nDESCRIPTION: Command for setting the OpenAI API key as an environment variable in Windows systems, required for accessing the AI models.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/README.md#2025-04-23_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\nsetx OPENAI_API_KEY \"YOUR_NEBIUS_AI_API_KEY\"  # On Windows\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for RAG with RL in Python\nDESCRIPTION: Imports necessary Python libraries for implementing RAG with RL, including OpenAI API, numpy, and json. Sets up the environment for the project.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/21_rag_with_rl.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom openai import OpenAI\nimport numpy as np\nimport json\nfrom typing import Dict, List, Tuple, Optional, Union\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for RAG Query Transformations in Python\nDESCRIPTION: Imports necessary libraries for implementing query transformation techniques in Python. The imports include fitz for PDF processing, os for environment variables, numpy for vector operations, json for data formatting, and OpenAI for API access.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/7_query_transform.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport fitz\nimport os\nimport numpy as np\nimport json\nfrom openai import OpenAI\n```\n\n----------------------------------------\n\nTITLE: Initializing Required Libraries for RAG System\nDESCRIPTION: Imports necessary Python libraries for PDF processing, numerical operations, JSON handling, OpenAI integration, and datetime functionality.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/11_feedback_loop_rag.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport fitz\nimport os\nimport numpy as np\nimport json\nfrom openai import OpenAI\nfrom datetime import datetime\n```\n\n----------------------------------------\n\nTITLE: PDF Text Extraction Function\nDESCRIPTION: Function to extract text content from PDF files using PyMuPDF library. Processes the PDF page by page and concatenates the extracted text.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/8_reranker.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef extract_text_from_pdf(pdf_path):\n    \"\"\"\n    Extracts text from a PDF file and prints the first `num_chars` characters.\n\n    Args:\n    pdf_path (str): Path to the PDF file.\n\n    Returns:\n    str: Extracted text from the PDF.\n    \"\"\"\n    # Open the PDF file\n    mypdf = fitz.open(pdf_path)\n    all_text = \"\"  # Initialize an empty string to store the extracted text\n\n    # Iterate through each page in the PDF\n    for page_num in range(mypdf.page_count):\n        page = mypdf[page_num]  # Get the page\n        text = page.get_text(\"text\")  # Extract text from the page\n        all_text += text  # Append the extracted text to the all_text string\n\n    return all_text  # Return the extracted text\n```\n\n----------------------------------------\n\nTITLE: Response Generation with LLM in Python\nDESCRIPTION: Function to generate responses using an LLM based on the query and prepared context. Uses system and user prompts to ensure responses are based solely on provided context.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/6_doc_augmentation_rag.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef generate_response(query, context, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Generates a response based on the query and context.\n\n    Args:\n    query (str): User's question.\n    context (str): Context information retrieved from the vector store.\n    model (str): Model to use for response generation.\n\n    Returns:\n    str: Generated response.\n    \"\"\"\n    system_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\n    \n    user_prompt = f\"\"\"\n        Context:\n        {context}\n\n        Question: {query}\n\n        Please answer the question based only on the context provided above. Be concise and accurate.\n    \"\"\"\n    \n    response = client.chat.completions.create(\n        model=model,\n        temperature=0,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ]\n    )\n    \n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Generating AI Responses Based on Retrieved Chunks in Python\nDESCRIPTION: Defines a function to generate responses from an AI model based on a system prompt and user message, using the OpenAI API.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/2_semantic_chunking.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Define the system prompt for the AI assistant\nsystem_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\n\ndef generate_response(system_prompt, user_message, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Generates a response from the AI model based on the system prompt and user message.\n\n    Args:\n    system_prompt (str): The system prompt to guide the AI's behavior.\n    user_message (str): The user's message or query.\n    model (str): The model to be used for generating the response. Default is \"meta-llama/Llama-2-7B-chat-hf\".\n\n    Returns:\n    dict: The response from the AI model.\n    \"\"\"\n    response = client.chat.completions.create(\n        model=model,\n        temperature=0,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_message}\n        ]\n    )\n    return response\n```\n\n----------------------------------------\n\nTITLE: Representing Quantum Superposition State\nDESCRIPTION: Mathematical representation of a qubit's superposition state using Dirac notation, where α and β are complex probability amplitudes satisfying the normalization condition.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/data/quantum.txt#2025-04-23_snippet_0\n\nLANGUAGE: mathematics\nCODE:\n```\n|ψ⟩ = α|0⟩ + β|1⟩\n```\n\n----------------------------------------\n\nTITLE: Representing Bell State Entanglement\nDESCRIPTION: Mathematical representation of a Bell state showing quantum entanglement between two qubits in a maximally entangled state.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/data/quantum.txt#2025-04-23_snippet_1\n\nLANGUAGE: mathematics\nCODE:\n```\n|Φ⁺⟩ = (|00⟩ + |11⟩)/√2\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Terminal (Unix)\nDESCRIPTION: Command for setting the OpenAI API key as an environment variable in Unix-based systems, required for accessing the AI models.\nSOURCE: https://github.com/fareedkhan-dev/all-rag-techniques/blob/main/README.md#2025-04-23_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\nexport OPENAI_API_KEY='YOUR_NEBIUS_AI_API_KEY'\n```"
  }
]
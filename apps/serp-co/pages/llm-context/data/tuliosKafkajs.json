[
  {
    "owner": "tulios",
    "repo": "kafkajs",
    "content": "TITLE: Implementing a Kafka Consumer with SSL and SASL Authentication in JavaScript\nDESCRIPTION: This example shows how to create a Kafka consumer with SSL certificate validation and SASL authentication using the scram-sha-256 mechanism. It connects securely to a Kafka broker, subscribes to a topic, and processes messages individually while properly handling errors and shutdown signals.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/ConsumerExample.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst fs = require('fs')\nconst ip = require('ip')\n\nconst { Kafka, logLevel } = require('../index')\n\nconst host = process.env.HOST_IP || ip.address()\n\nconst kafka = new Kafka({\n  logLevel: logLevel.INFO,\n  brokers: [`${host}:9094`],\n  clientId: 'example-consumer',\n  ssl: {\n    rejectUnauthorized: true\n  },\n  sasl: {\n    mechanism: 'scram-sha-256',\n    username: 'test',\n    password: 'testtest',\n  },\n})\n\nconst topic = 'topic-test'\nconst consumer = kafka.consumer({ groupId: 'test-group' })\n\nconst run = async () => {\n  await consumer.connect()\n  await consumer.subscribe({ topic })\n  await consumer.run({\n    // eachBatch: async ({ batch }) => {\n    //   console.log(batch)\n    // },\n    eachMessage: async ({ topic, partition, message }) => {\n      const prefix = `${topic}[${partition} | ${message.offset}] / ${message.timestamp}`\n      console.log(`- ${prefix} ${message.key}#${message.value}`)\n    },\n  })\n}\n\nrun().catch(e => console.error(`[example/consumer] ${e.message}`, e))\n\nconst errorTypes = ['unhandledRejection', 'uncaughtException']\nconst signalTraps = ['SIGTERM', 'SIGINT', 'SIGUSR2']\n\nerrorTypes.map(type => {\n  process.on(type, async e => {\n    try {\n      console.log(`process.on ${type}`)\n      console.error(e)\n      await consumer.disconnect()\n      process.exit(0)\n    } catch (_) {\n      process.exit(1)\n    }\n  })\n})\n\nsignalTraps.map(type => {\n  process.once(type, async () => {\n    try {\n      await consumer.disconnect()\n    } finally {\n      process.kill(process.pid, type)\n    }\n  })\n})\n```\n\n----------------------------------------\n\nTITLE: Processing Kafka Messages with eachBatch\nDESCRIPTION: Demonstrates how to use the eachBatch handler to process Kafka messages in batches, including usage of utility functions like resolveOffset and heartbeat.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Consuming.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nawait consumer.run({\n    eachBatch: async ({ batch, resolveOffset, heartbeat, isRunning, isStale }) => {\n        for (let message of batch.messages) {\n            console.log({\n                topic: batch.topic,\n                partition: batch.partition,\n                highWatermark: batch.highWatermark,\n                message: {\n                    offset: message.offset,\n                    key: message.key.toString(),\n                    value: message.value.toString(),\n                    headers: message.headers,\n                }\n            })\n\n            await resolveOffset(message.offset)\n            await heartbeat()\n        }\n    },\n})\n```\n\n----------------------------------------\n\nTITLE: Consuming Messages with KafkaJS\nDESCRIPTION: Creates a consumer instance with a group ID, connects to Kafka, subscribes to a topic, and processes incoming messages.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/GettingStarted.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst consumer = kafka.consumer({ groupId: 'test-group' })\n\nawait consumer.connect()\nawait consumer.subscribe({ topic: 'test-topic', fromBeginning: true })\n\nawait consumer.run({\n  eachMessage: async ({ topic, partition, message }) => {\n    console.log({\n      value: message.value.toString(),\n    })\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing Kafka Producer with KafkaJS in JavaScript\nDESCRIPTION: This snippet shows how to set up a Kafka producer, create and send messages, and handle errors and signals. It uses the KafkaJS library to interact with Kafka, sends compressed messages at regular intervals, and implements proper shutdown procedures.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/ProducerExample.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst fs = require('fs')\nconst ip = require('ip')\n\nconst { Kafka, CompressionTypes, logLevel } = require('../index')\n\nconst host = process.env.HOST_IP || ip.address()\n\nconst kafka = new Kafka({\n  logLevel: logLevel.DEBUG,\n  brokers: [`${host}:9092`],\n  clientId: 'example-producer',\n})\n\nconst topic = 'topic-test'\nconst producer = kafka.producer()\n\nconst getRandomNumber = () => Math.round(Math.random(10) * 1000)\nconst createMessage = num => ({\n  key: `key-${num}`,\n  value: `value-${num}-${new Date().toISOString()}`,\n})\n\nconst sendMessage = () => {\n  return producer\n    .send({\n      topic,\n      compression: CompressionTypes.GZIP,\n      messages: Array(getRandomNumber())\n        .fill()\n        .map(_ => createMessage(getRandomNumber())),\n    })\n    .then(console.log)\n    .catch(e => console.error(`[example/producer] ${e.message}`, e))\n}\n\nconst run = async () => {\n  await producer.connect()\n  setInterval(sendMessage, 3000)\n}\n\nrun().catch(e => console.error(`[example/producer] ${e.message}`, e))\n\nconst errorTypes = ['unhandledRejection', 'uncaughtException']\nconst signalTraps = ['SIGTERM', 'SIGINT', 'SIGUSR2']\n\nerrorTypes.map(type => {\n  process.on(type, async () => {\n    try {\n      console.log(`process.on ${type}`)\n      await producer.disconnect()\n      process.exit(0)\n    } catch (_) {\n      process.exit(1)\n    }\n  })\n})\n\nsignalTraps.map(type => {\n  process.once(type, async () => {\n    try {\n      await producer.disconnect()\n    } finally {\n      process.kill(process.pid, type)\n    }\n  })\n})\n```\n\n----------------------------------------\n\nTITLE: Basic KafkaJS Producer and Consumer Example\nDESCRIPTION: Demonstrates how to set up a basic Kafka producer and consumer using KafkaJS, including connecting to brokers, sending messages, and consuming messages from a topic.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/README.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Kafka } = require('kafkajs')\n\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092']\n})\n\nconst producer = kafka.producer()\nconst consumer = kafka.consumer({ groupId: 'test-group' })\n\nconst run = async () => {\n  // Producing\n  await producer.connect()\n  await producer.send({\n    topic: 'test-topic',\n    messages: [\n      { value: 'Hello KafkaJS user!' },\n    ],\n  })\n\n  // Consuming\n  await consumer.connect()\n  await consumer.subscribe({ topic: 'test-topic', fromBeginning: true })\n\n  await consumer.run({\n    eachMessage: async ({ topic, partition, message }) => {\n      console.log({\n        partition,\n        offset: message.offset,\n        value: message.value.toString(),\n      })\n    },\n  })\n}\n\nrun().catch(console.error)\n```\n\n----------------------------------------\n\nTITLE: Initializing KafkaJS Client with Broker List\nDESCRIPTION: Creates a basic KafkaJS client by specifying a client ID and an array of broker addresses. The brokers list is used to bootstrap the client and load initial metadata.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Configuration.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Kafka } = require('kafkajs')\n\n// Create the client with the broker list\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092']\n})\n```\n\n----------------------------------------\n\nTITLE: Consuming Messages with KafkaJS\nDESCRIPTION: Creates a consumer with a group ID, connects to Kafka, subscribes to a topic, and processes incoming messages. This shows how to consume and process messages from a Kafka topic.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/GettingStarted.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst consumer = kafka.consumer({ groupId: 'test-group' })\n\nawait consumer.connect()\nawait consumer.subscribe({ topic: 'test-topic' })\n\nawait consumer.run({\n  eachMessage: async ({ topic, partition, message }) => {\n    console.log({\n      value: message.value.toString(),\n    })\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Sending Messages to Kafka Topic\nDESCRIPTION: Demonstrates how to connect to Kafka and send multiple messages with keys to a specific topic.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Producing.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n\nawait producer.connect()\nawait producer.send({\n    topic: 'topic-name',\n    messages: [\n        { key: 'key1', value: 'hello world' },\n        { key: 'key2', value: 'hey hey!' }\n    ],\n})\n```\n\n----------------------------------------\n\nTITLE: Manually Committing Offsets in KafkaJS Consumer\nDESCRIPTION: Demonstrates how to manually commit offsets using the consumer.commitOffsets method. This is useful when autoCommit is disabled and you need fine-grained control over offset management.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Consuming.md#2025-04-14_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconsumer.run({\n    autoCommit: false,\n    eachMessage: async ({ topic, partition, message }) => {\n        // Process the message somehow\n    },\n})\n\nconsumer.commitOffsets([\n  { topic: 'topic-A', partition: 0, offset: '1' },\n  { topic: 'topic-A', partition: 1, offset: '3' },\n  { topic: 'topic-B', partition: 0, offset: '2' }\n])\n```\n\n----------------------------------------\n\nTITLE: Consuming Messages with KafkaJS\nDESCRIPTION: Creates a consumer with a group ID, connects to Kafka, subscribes to a topic, and processes incoming messages using a callback function.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/GettingStarted.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst consumer = kafka.consumer({ groupId: 'test-group' })\n\nawait consumer.connect()\nawait consumer.subscribe({ topic: 'test-topic' })\n\nawait consumer.run({\n  eachMessage: async ({ topic, partition, message }) => {\n    console.log({\n      value: message.value.toString(),\n    })\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Processing Messages with eachBatch Handler\nDESCRIPTION: Advanced batch processing implementation using eachBatch handler with utilities for offset management, heartbeat control, and batch processing control.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Consuming.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nawait consumer.run({\n    eachBatchAutoResolve: true,\n    eachBatch: async ({\n        batch,\n        resolveOffset,\n        heartbeat,\n        commitOffsetsIfNecessary,\n        uncommittedOffsets,\n        isRunning,\n        isStale,\n        pause,\n    }) => {\n        for (let message of batch.messages) {\n            console.log({\n                topic: batch.topic,\n                partition: batch.partition,\n                highWatermark: batch.highWatermark,\n                message: {\n                    offset: message.offset,\n                    key: message.key.toString(),\n                    value: message.value.toString(),\n                    headers: message.headers,\n                }\n            })\n\n            resolveOffset(message.offset)\n            await heartbeat()\n        }\n    },\n})\n```\n\n----------------------------------------\n\nTITLE: Producing Messages with KafkaJS\nDESCRIPTION: Creates a producer, connects to Kafka, sends a message to a specific topic, and then disconnects.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/GettingStarted.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n\nawait producer.connect()\nawait producer.send({\n  topic: 'test-topic',\n  messages: [\n    { value: 'Hello KafkaJS user!' },\n  ],\n})\n\nawait producer.disconnect()\n```\n\n----------------------------------------\n\nTITLE: Installing KafkaJS with NPM\nDESCRIPTION: Command to install KafkaJS package using the NPM package manager.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/GettingStarted.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install kafkajs\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Kafka Topics\nDESCRIPTION: Examples of subscribing to single or multiple topics, with option to start from beginning of topic or use regular expressions for topic matching.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Consuming.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait consumer.connect()\n\nawait consumer.subscribe({ topics: ['topic-A'] })\n\n// You can subscribe to multiple topics at once\nawait consumer.subscribe({ topics: ['topic-B', 'topic-C'] })\n\n// It's possible to start from the beginning of the topic\nawait consumer.subscribe({ topics: ['topic-D'], fromBeginning: true })\n```\n\n----------------------------------------\n\nTITLE: Initializing KafkaJS Admin Client in JavaScript\nDESCRIPTION: Creates a new Kafka instance and initializes the admin client. Demonstrates how to connect and disconnect from the admin client.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Admin.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst kafka = new Kafka(...)\nconst admin = kafka.admin()\n\n// remember to connect and disconnect when you are done\nawait admin.connect()\nawait admin.disconnect()\n```\n\n----------------------------------------\n\nTITLE: Producing and Consuming JSON Messages with KafkaJS\nDESCRIPTION: This snippet demonstrates how to produce and consume JSON messages using KafkaJS. It shows sending a message with a key-value pair and parsing the received message in the consumer.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/KafkaIntro.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nawait producer.send({\n  topic,\n  messages: [{\n    key: 'my-key',\n    value: JSON.stringify({ some: 'data' })\n  }]\n})\n\nconst eachMessage = async ({ /*topic, partition,*/ message }) => {\n  // From Kafka's perspective, both key and value are just bytes\n  // so we need to parse them.\n  console.log({\n    key: message.key.toString(),\n    value: JSON.parse(message.value.toString())\n  })\n\n  /**\n   * { key: 'my-key', value: { some: 'data' } }\n   */\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Class-based Kafka Consumer in TypeScript\nDESCRIPTION: A TypeScript implementation of a Kafka consumer using a class-based approach. This example shows both single message and batch processing methods, with proper type definitions and error handling.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/ConsumerExample.md#2025-04-14_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Consumer, ConsumerSubscribeTopics, EachBatchPayload, Kafka, EachMessagePayload } from 'kafkajs'\n\nexport default class ExampleConsumer {\n  private kafkaConsumer: Consumer\n  private messageProcessor: ExampleMessageProcessor\n\n  public constructor(messageProcessor: ExampleMessageProcessor) {\n    this.messageProcessor = messageProcessor\n    this.kafkaConsumer = this.createKafkaConsumer()\n  }\n\n  public async startConsumer(): Promise<void> {\n    const topic: ConsumerSubscribeTopics = {\n      topics: ['example-topic'],\n      fromBeginning: false\n    }\n\n    try {\n      await this.kafkaConsumer.connect()\n      await this.kafkaConsumer.subscribe(topic)\n\n      await this.kafkaConsumer.run({\n        eachMessage: async (messagePayload: EachMessagePayload) => {\n          const { topic, partition, message } = messagePayload\n          const prefix = `${topic}[${partition} | ${message.offset}] / ${message.timestamp}`\n          console.log(`- ${prefix} ${message.key}#${message.value}`)\n        }\n      })\n    } catch (error) {\n      console.log('Error: ', error)\n    }\n  }\n\n  public async startBatchConsumer(): Promise<void> {\n    const topic: ConsumerSubscribeTopics = {\n      topics: ['example-topic'],\n      fromBeginning: false\n    }\n\n    try {\n      await this.kafkaConsumer.connect()\n      await this.kafkaConsumer.subscribe(topic)\n      await this.kafkaConsumer.run({\n        eachBatch: async (eachBatchPayload: EachBatchPayload) => {\n          const { batch } = eachBatchPayload\n          for (const message of batch.messages) {\n            const prefix = `${batch.topic}[${batch.partition} | ${message.offset}] / ${message.timestamp}`\n            console.log(`- ${prefix} ${message.key}#${message.value}`) \n          }\n        }\n      })\n    } catch (error) {\n      console.log('Error: ', error)\n    }\n  }\n\n  public async shutdown(): Promise<void> {\n    await this.kafkaConsumer.disconnect()\n  }\n\n  private createKafkaConsumer(): Consumer {\n    const kafka = new Kafka({ \n      clientId: 'client-id',\n      brokers: ['example.kafka.broker:9092']\n    })\n    const consumer = kafka.consumer({ groupId: 'consumer-group' })\n    return consumer\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Basic Kafka Consumer in JavaScript\nDESCRIPTION: This snippet demonstrates how to create a basic Kafka consumer using KafkaJS. It connects to a local Kafka broker, subscribes to a topic, and processes messages. The consumer also includes error handling and graceful shutdown procedures.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/ConsumerExample.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst ip = require('ip')\n\nconst { Kafka, logLevel } = require('../index')\n\nconst host = process.env.HOST_IP || ip.address()\n\nconst kafka = new Kafka({\n  logLevel: logLevel.INFO,\n  brokers: [`${host}:9092`],\n  clientId: 'example-consumer',\n})\n\nconst topic = 'topic-test'\nconst consumer = kafka.consumer({ groupId: 'test-group' })\n\nconst run = async () => {\n  await consumer.connect()\n  await consumer.subscribe({ topic, fromBeginning: true })\n  await consumer.run({\n    // eachBatch: async ({ batch }) => {\n    //   console.log(batch)\n    // },\n    eachMessage: async ({ topic, partition, message }) => {\n      const prefix = `${topic}[${partition} | ${message.offset}] / ${message.timestamp}`\n      console.log(`- ${prefix} ${message.key}#${message.value}`)\n    },\n  })\n}\n\nrun().catch(e => console.error(`[example/consumer] ${e.message}`, e))\n\nconst errorTypes = ['unhandledRejection', 'uncaughtException']\nconst signalTraps = ['SIGTERM', 'SIGINT', 'SIGUSR2']\n\nerrorTypes.map(type => {\n  process.on(type, async e => {\n    try {\n      console.log(`process.on ${type}`)\n      console.error(e)\n      await consumer.disconnect()\n      process.exit(0)\n    } catch (_) {\n      process.exit(1)\n    }\n  })\n})\n\nsignalTraps.map(type => {\n  process.once(type, async () => {\n    try {\n      await consumer.disconnect()\n    } finally {\n      process.kill(process.pid, type)\n    }\n  })\n})\n```\n\n----------------------------------------\n\nTITLE: Batch Processing with Staleness Check\nDESCRIPTION: Example showing batch processing with checks for running state and staleness, including proper offset resolution and heartbeat handling.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Consuming.md#2025-04-14_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconsumer.run({\n    eachBatchAutoResolve: false,\n    eachBatch: async ({ batch, resolveOffset, heartbeat, isRunning, isStale }) => {\n        for (let message of batch.messages) {\n            if (!isRunning() || isStale()) break\n            await processMessage(message)\n            resolveOffset(message.offset)\n            await heartbeat()\n        }\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing a Basic Kafka Producer in JavaScript with KafkaJS\nDESCRIPTION: This snippet demonstrates how to create a Kafka producer using KafkaJS. It includes configuration, message creation, sending messages with compression, and error handling. The producer sends random messages at regular intervals and handles various error types and signal traps.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/ProducerExample.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst ip = require('ip')\n\nconst { Kafka, CompressionTypes, logLevel } = require('kafkajs')\n\nconst host = process.env.HOST_IP || ip.address()\n\nconst kafka = new Kafka({\n  logLevel: logLevel.DEBUG,\n  brokers: [`${host}:9092`],\n  clientId: 'example-producer',\n})\n\nconst topic = 'topic-test'\nconst producer = kafka.producer()\n\nconst getRandomNumber = () => Math.round(Math.random(10) * 1000)\nconst createMessage = num => ({\n  key: `key-${num}`,\n  value: `value-${num}-${new Date().toISOString()}`,\n})\n\nconst sendMessage = () => {\n  return producer\n    .send({\n      topic,\n      compression: CompressionTypes.GZIP,\n      messages: Array(getRandomNumber())\n        .fill()\n        .map(_ => createMessage(getRandomNumber())),\n    })\n    .then(console.log)\n    .catch(e => console.error(`[example/producer] ${e.message}`, e))\n}\n\nconst run = async () => {\n  await producer.connect()\n  setInterval(sendMessage, 3000)\n}\n\nrun().catch(e => console.error(`[example/producer] ${e.message}`, e))\n\nconst errorTypes = ['unhandledRejection', 'uncaughtException']\nconst signalTraps = ['SIGTERM', 'SIGINT', 'SIGUSR2']\n\nerrorTypes.forEach(type => {\n  process.on(type, async () => {\n    try {\n      console.log(`process.on ${type}`)\n      await producer.disconnect()\n      process.exit(0)\n    } catch (_) {\n      process.exit(1)\n    }\n  })\n})\n\nsignalTraps.forEach(type => {\n  process.once(type, async () => {\n    try {\n      await producer.disconnect()\n    } finally {\n      process.kill(process.pid, type)\n    }\n  })\n})\n```\n\n----------------------------------------\n\nTITLE: Sending Messages in a KafkaJS Transaction in JavaScript\nDESCRIPTION: This code snippet shows how to send messages within a KafkaJS transaction. It demonstrates initializing a transaction, sending messages, and committing or aborting the transaction based on the operation's success.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/Transactions.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst transaction = await producer.transaction()\n\ntry {\n  await transaction.send({ topic, messages })\n\n  await transaction.commit()\n} catch (e) {\n  await transaction.abort()\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Consumer with SSL and SASL Authentication in JavaScript\nDESCRIPTION: This example demonstrates how to set up a Kafka consumer with SSL and SASL authentication using the scram-sha-256 mechanism. It configures secure connections to a Kafka broker, subscribes to a topic, and processes messages with proper authentication credentials.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/ConsumerExample.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst fs = require('fs')\nconst ip = require('ip')\n\nconst { Kafka, logLevel } = require('../index')\n\nconst host = process.env.HOST_IP || ip.address()\n\nconst kafka = new Kafka({\n  logLevel: logLevel.INFO,\n  brokers: [`${host}:9094`],\n  clientId: 'example-consumer',\n  ssl: {\n    rejectUnauthorized: true\n  },\n  sasl: {\n    mechanism: 'scram-sha-256',\n    username: 'test',\n    password: 'testtest',\n  },\n})\n\nconst topic = 'topic-test'\nconst consumer = kafka.consumer({ groupId: 'test-group' })\n\nconst run = async () => {\n  await consumer.connect()\n  await consumer.subscribe({ topic, fromBeginning: true })\n  await consumer.run({\n    // eachBatch: async ({ batch }) => {\n    //   console.log(batch)\n    // },\n    eachMessage: async ({ topic, partition, message }) => {\n      const prefix = `${topic}[${partition} | ${message.offset}] / ${message.timestamp}`\n      console.log(`- ${prefix} ${message.key}#${message.value}`)\n    },\n  })\n}\n\nrun().catch(e => console.error(`[example/consumer] ${e.message}`, e))\n\nconst errorTypes = ['unhandledRejection', 'uncaughtException']\nconst signalTraps = ['SIGTERM', 'SIGINT', 'SIGUSR2']\n\nerrorTypes.map(type => {\n  process.on(type, async e => {\n    try {\n      console.log(`process.on ${type}`)\n      console.error(e)\n      await consumer.disconnect()\n      process.exit(0)\n    } catch (_) {\n      process.exit(1)\n    }\n  })\n})\n\nsignalTraps.map(type => {\n  process.once(type, async () => {\n    try {\n      await consumer.disconnect()\n    } finally {\n      process.kill(process.pid, type)\n    }\n  })\n})\n```\n\n----------------------------------------\n\nTITLE: Consuming Messages with KafkaJS\nDESCRIPTION: Creates a consumer with a group ID, connects to Kafka, subscribes to a topic, and processes incoming messages.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/GettingStarted.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst consumer = kafka.consumer({ groupId: 'test-group' })\n\nawait consumer.connect()\nawait consumer.subscribe({ topic: 'test-topic', fromBeginning: true })\n\nawait consumer.run({\n  eachMessage: async ({ topic, partition, message }) => {\n    console.log({\n      value: message.value.toString(),\n    })\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Sending Messages to Kafka Topic\nDESCRIPTION: Shows how to connect a producer and send messages to a Kafka topic. Includes examples of sending multiple messages with different keys.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.15.0/Producing.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n\nawait producer.connect()\nawait producer.send({\n    topic: 'topic-name',\n    messages: [\n        { key: 'key1', value: 'hello world' },\n        { key: 'key2', value: 'hey hey!' }\n    ],\n})\n```\n\n----------------------------------------\n\nTITLE: Sending Messages to Specific Partitions in KafkaJS\nDESCRIPTION: Demonstrates how to send messages to specific partitions in a Kafka topic using the KafkaJS producer.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Producing.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n\nawait producer.connect()\nawait producer.send({\n    topic: 'topic-name',\n    messages: [\n        { key: 'key1', value: 'hello world', partition: 0 },\n        { key: 'key2', value: 'hey hey!', partition: 1 }\n    ],\n})\n```\n\n----------------------------------------\n\nTITLE: Sending Offsets in a KafkaJS Transaction\nDESCRIPTION: Shows how to send consumer offsets as part of a transaction using the sendOffsets method. This is used in consume-transform-produce patterns to ensure that the offsets are committed only if the transaction succeeds.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Transactions.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nawait transaction.sendOffsets({\n  consumerGroupId, topics\n})\n```\n\n----------------------------------------\n\nTITLE: Sending Messages with Headers in KafkaJS\nDESCRIPTION: Shows how to include message headers when sending messages to Kafka, which allows for additional metadata to be attached to each message.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.15.0/Producing.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nawait producer.send({\n    topic: 'topic-name',\n    messages: [{\n        key: 'key1',\n        value: 'hello world',\n        headers: {\n            'correlation-id': '2bfb68bb-893a-423b-a7fa-7b568cad5b67',\n            'system-id': 'my-system'\n        }\n    }]\n})\n```\n\n----------------------------------------\n\nTITLE: Basic KafkaJS Consumer Implementation - JavaScript\nDESCRIPTION: Demonstrates setting up a basic Kafka consumer with error handling and graceful shutdown. Includes connection setup, topic subscription, and message processing with console logging.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/ConsumerExample.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst ip = require('ip')\n\nconst { Kafka, logLevel } = require('kafkajs')\n\nconst host = process.env.HOST_IP || ip.address()\n\nconst kafka = new Kafka({\n  logLevel: logLevel.INFO,\n  brokers: [`${host}:9092`],\n  clientId: 'example-consumer',\n})\n\nconst topic = 'topic-test'\nconst consumer = kafka.consumer({ groupId: 'test-group' })\n\nconst run = async () => {\n  await consumer.connect()\n  await consumer.subscribe({ topic, fromBeginning: true })\n  await consumer.run({\n    // eachBatch: async ({ batch }) => {\n    //   console.log(batch)\n    // },\n    eachMessage: async ({ topic, partition, message }) => {\n      const prefix = `${topic}[${partition} | ${message.offset}] / ${message.timestamp}`\n      console.log(`- ${prefix} ${message.key}#${message.value}`)\n    },\n  })\n}\n\nrun().catch(e => console.error(`[example/consumer] ${e.message}`, e))\n\nconst errorTypes = ['unhandledRejection', 'uncaughtException']\nconst signalTraps = ['SIGTERM', 'SIGINT', 'SIGUSR2']\n\nerrorTypes.forEach(type => {\n  process.on(type, async e => {\n    try {\n      console.log(`process.on ${type}`)\n      console.error(e)\n      await consumer.disconnect()\n      process.exit(0)\n    } catch (_) {\n      process.exit(1)\n    }\n  })\n})\n\nsignalTraps.forEach(type => {\n  process.once(type, async () => {\n    try {\n      await consumer.disconnect()\n    } finally {\n      process.kill(process.pid, type)\n    }\n  })\n})\n```\n\n----------------------------------------\n\nTITLE: Creating a Kafka Producer Factory in TypeScript with KafkaJS\nDESCRIPTION: This snippet shows how to create a ProducerFactory class in TypeScript using KafkaJS. It encapsulates producer creation, connection management, and message sending. The class provides methods for starting, shutting down, and sending batches of messages in a custom format.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.4/ProducerExample.md#2025-04-14_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Kafka, Message, Producer, ProducerBatch, TopicMessages } from 'kafkajs'\n\ninterface CustomMessageFormat { a: string }\n\nexport default class ProducerFactory {\n  private producer: Producer\n\n  constructor() {\n    this.producer = this.createProducer()\n  }\n\n  public async start(): Promise<void> {\n    try {\n      await this.producer.connect()\n    } catch (error) {\n      console.log('Error connecting the producer: ', error)\n    }\n  }\n\n  public async shutdown(): Promise<void> {\n    await this.producer.disconnect()\n  }\n\n  public async sendBatch(messages: Array<CustomMessageFormat>): Promise<void> {\n    const kafkaMessages: Array<Message> = messages.map((message) => {\n      return {\n        value: JSON.stringify(message)\n      }\n    })\n\n    const topicMessages: TopicMessages = {\n      topic: 'producer-topic',\n      messages: kafkaMessages\n    }\n\n    const batch: ProducerBatch = {\n      topicMessages: [topicMessages]\n    }\n\n    await this.producer.sendBatch(batch)\n  }\n\n  private createProducer() : Producer {\n    const kafka = new Kafka({\n      clientId: 'producer-client',\n      brokers: ['localhost:9092'],\n    })\n\n    return kafka.producer()\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Basic Kafka Producer in JavaScript with KafkaJS\nDESCRIPTION: This snippet demonstrates how to set up a Kafka producer using KafkaJS. It includes configuration, message creation, sending messages with compression, and error handling. The producer sends random messages at regular intervals and implements graceful shutdown on various signals.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.4/ProducerExample.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst ip = require('ip')\n\nconst { Kafka, CompressionTypes, logLevel } = require('kafkajs')\n\nconst host = process.env.HOST_IP || ip.address()\n\nconst kafka = new Kafka({\n  logLevel: logLevel.DEBUG,\n  brokers: [`${host}:9092`],\n  clientId: 'example-producer',\n})\n\nconst topic = 'topic-test'\nconst producer = kafka.producer()\n\nconst getRandomNumber = () => Math.round(Math.random(10) * 1000)\nconst createMessage = num => ({\n  key: `key-${num}`,\n  value: `value-${num}-${new Date().toISOString()}`,\n})\n\nconst sendMessage = () => {\n  return producer\n    .send({\n      topic,\n      compression: CompressionTypes.GZIP,\n      messages: Array(getRandomNumber())\n        .fill()\n        .map(_ => createMessage(getRandomNumber())),\n    })\n    .then(console.log)\n    .catch(e => console.error(`[example/producer] ${e.message}`, e))\n}\n\nconst run = async () => {\n  await producer.connect()\n  setInterval(sendMessage, 3000)\n}\n\nrun().catch(e => console.error(`[example/producer] ${e.message}`, e))\n\nconst errorTypes = ['unhandledRejection', 'uncaughtException']\nconst signalTraps = ['SIGTERM', 'SIGINT', 'SIGUSR2']\n\nerrorTypes.forEach(type => {\n  process.on(type, async () => {\n    try {\n      console.log(`process.on ${type}`)\n      await producer.disconnect()\n      process.exit(0)\n    } catch (_) {\n      process.exit(1)\n    }\n  })\n})\n\nsignalTraps.forEach(type => {\n  process.once(type, async () => {\n    try {\n      await producer.disconnect()\n    } finally {\n      process.kill(process.pid, type)\n    }\n  })\n})\n```\n\n----------------------------------------\n\nTITLE: Sending Messages with KafkaJS Producer\nDESCRIPTION: Shows how to connect a producer and send messages to a Kafka topic. Includes examples of sending multiple messages with different keys.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Producing.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n\nawait producer.connect()\nawait producer.send({\n    topic: 'topic-name',\n    messages: [\n        { key: 'key1', value: 'hello world' },\n        { key: 'key2', value: 'hey hey!' }\n    ],\n})\n```\n\n----------------------------------------\n\nTITLE: Basic Kafka Consumer Implementation with KafkaJS\nDESCRIPTION: Demonstrates how to set up a basic Kafka consumer using KafkaJS. The consumer connects to a local Kafka broker, subscribes to a topic, and processes messages individually. Includes error handling and graceful shutdown logic.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/ConsumerExample.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst fs = require('fs')\nconst ip = require('ip')\n\nconst { Kafka, logLevel } = require('../index')\n\nconst host = process.env.HOST_IP || ip.address()\n\nconst kafka = new Kafka({\n  logLevel: logLevel.INFO,\n  brokers: [`${host}:9092`],\n  clientId: 'example-consumer',\n})\n\nconst topic = 'topic-test'\nconst consumer = kafka.consumer({ groupId: 'test-group' })\n\nconst run = async () => {\n  await consumer.connect()\n  await consumer.subscribe({ topic })\n  await consumer.run({\n    // eachBatch: async ({ batch }) => {\n    //   console.log(batch)\n    // },\n    eachMessage: async ({ topic, partition, message }) => {\n      const prefix = `${topic}[${partition} | ${message.offset}] / ${message.timestamp}`\n      console.log(`- ${prefix} ${message.key}#${message.value}`)\n    },\n  })\n}\n\nrun().catch(e => console.error(`[example/consumer] ${e.message}`, e))\n\nconst errorTypes = ['unhandledRejection', 'uncaughtException']\nconst signalTraps = ['SIGTERM', 'SIGINT', 'SIGUSR2']\n\nerrorTypes.map(type => {\n  process.on(type, async e => {\n    try {\n      console.log(`process.on ${type}`)\n      console.error(e)\n      await consumer.disconnect()\n      process.exit(0)\n    } catch (_) {\n      process.exit(1)\n    }\n  })\n})\n\nsignalTraps.map(type => {\n  process.once(type, async () => {\n    try {\n      await consumer.disconnect()\n    } finally {\n      process.kill(process.pid, type)\n    }\n  })\n})\n```\n\n----------------------------------------\n\nTITLE: Initializing KafkaJS Client\nDESCRIPTION: Creates a Kafka client instance by specifying a client ID and an array of broker addresses. This is the first step in establishing a connection to a Kafka cluster.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/GettingStarted.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Kafka } = require('kafkajs')\n\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092']\n})\n```\n\n----------------------------------------\n\nTITLE: Basic KafkaJS Consumer Implementation in JavaScript\nDESCRIPTION: Demonstrates setting up a basic Kafka consumer with error handling and graceful shutdown. Creates a consumer that connects to a local Kafka broker, subscribes to a topic, and processes messages individually.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.0/ConsumerExample.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst ip = require('ip')\n\nconst { Kafka, logLevel } = require('kafkajs')\n\nconst host = process.env.HOST_IP || ip.address()\n\nconst kafka = new Kafka({\n  logLevel: logLevel.INFO,\n  brokers: [`${host}:9092`],\n  clientId: 'example-consumer',\n})\n\nconst topic = 'topic-test'\nconst consumer = kafka.consumer({ groupId: 'test-group' })\n\nconst run = async () => {\n  await consumer.connect()\n  await consumer.subscribe({ topic, fromBeginning: true })\n  await consumer.run({\n    // eachBatch: async ({ batch }) => {\n    //   console.log(batch)\n    // },\n    eachMessage: async ({ topic, partition, message }) => {\n      const prefix = `${topic}[${partition} | ${message.offset}] / ${message.timestamp}`\n      console.log(`- ${prefix} ${message.key}#${message.value}`)\n    },\n  })\n}\n\nrun().catch(e => console.error(`[example/consumer] ${e.message}`, e))\n\nconst errorTypes = ['unhandledRejection', 'uncaughtException']\nconst signalTraps = ['SIGTERM', 'SIGINT', 'SIGUSR2']\n\nerrorTypes.forEach(type => {\n  process.on(type, async e => {\n    try {\n      console.log(`process.on ${type}`)\n      console.error(e)\n      await consumer.disconnect()\n      process.exit(0)\n    } catch (_) {\n      process.exit(1)\n    }\n  })\n})\n\nsignalTraps.forEach(type => {\n  process.once(type, async () => {\n    try {\n      await consumer.disconnect()\n    } finally {\n      process.kill(process.pid, type)\n    }\n  })\n})\n```\n\n----------------------------------------\n\nTITLE: Sending Messages with Headers in KafkaJS\nDESCRIPTION: Demonstrates how to send a message with headers using the KafkaJS producer. Headers allow messages to carry extra metadata.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Producing.md#2025-04-14_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nawait producer.send({\n    topic: 'topic-name',\n    messages: [{\n        key: 'key1',\n        value: 'hello world',\n        headers: {\n            'correlation-id': '2bfb68bb-893a-423b-a7fa-7b568cad5b67',\n            'system-id': 'my-system',\n        }\n    }]\n})\n```\n\n----------------------------------------\n\nTITLE: Sending Messages with Headers in KafkaJS\nDESCRIPTION: Demonstrates how to include message headers when sending messages to Kafka using KafkaJS. Headers allow for additional metadata to be attached to messages.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/Producing.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nawait producer.send({\n    topic: 'topic-name',\n    messages: [{\n        key: 'key1',\n        value: 'hello world',\n        headers: {\n            'correlation-id': '2bfb68bb-893a-423b-a7fa-7b568cad5b67',\n            'system-id': 'my-system'\n        }\n    }]\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing a Kafka Consumer with SSL and SASL Authentication in JavaScript\nDESCRIPTION: This snippet shows how to create a Kafka consumer with SSL and SASL authentication using KafkaJS. It connects to a Kafka broker using SSL and SASL (scram-sha-256 mechanism), subscribes to a topic, and processes messages. The consumer includes error handling and graceful shutdown procedures.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/ConsumerExample.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst ip = require('ip')\n\nconst { Kafka, logLevel } = require('../index')\n\nconst host = process.env.HOST_IP || ip.address()\n\nconst kafka = new Kafka({\n  logLevel: logLevel.INFO,\n  brokers: [`${host}:9094`],\n  clientId: 'example-consumer',\n  ssl: {\n    rejectUnauthorized: true\n  },\n  sasl: {\n    mechanism: 'scram-sha-256',\n    username: 'test',\n    password: 'testtest',\n  },\n})\n\nconst topic = 'topic-test'\nconst consumer = kafka.consumer({ groupId: 'test-group' })\n\nconst run = async () => {\n  await consumer.connect()\n  await consumer.subscribe({ topic, fromBeginning: true })\n  await consumer.run({\n    // eachBatch: async ({ batch }) => {\n    //   console.log(batch)\n    // },\n    eachMessage: async ({ topic, partition, message }) => {\n      const prefix = `${topic}[${partition} | ${message.offset}] / ${message.timestamp}`\n      console.log(`- ${prefix} ${message.key}#${message.value}`)\n    },\n  })\n}\n\nrun().catch(e => console.error(`[example/consumer] ${e.message}`, e))\n\nconst errorTypes = ['unhandledRejection', 'uncaughtException']\nconst signalTraps = ['SIGTERM', 'SIGINT', 'SIGUSR2']\n\nerrorTypes.map(type => {\n  process.on(type, async e => {\n    try {\n      console.log(`process.on ${type}`)\n      console.error(e)\n      await consumer.disconnect()\n      process.exit(0)\n    } catch (_) {\n      process.exit(1)\n    }\n  })\n})\n\nsignalTraps.map(type => {\n  process.once(type, async () => {\n    try {\n      await consumer.disconnect()\n    } finally {\n      process.kill(process.pid, type)\n    }\n  })\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing a Basic Kafka Consumer in JavaScript\nDESCRIPTION: This example demonstrates how to create a basic Kafka consumer that connects to a broker, subscribes to a topic, and processes messages one by one. It includes proper error handling and graceful shutdown on process termination.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/ConsumerExample.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst ip = require('ip')\n\nconst { Kafka, logLevel } = require('kafkajs')\n\nconst host = process.env.HOST_IP || ip.address()\n\nconst kafka = new Kafka({\n  logLevel: logLevel.INFO,\n  brokers: [`${host}:9092`],\n  clientId: 'example-consumer',\n})\n\nconst topic = 'topic-test'\nconst consumer = kafka.consumer({ groupId: 'test-group' })\n\nconst run = async () => {\n  await consumer.connect()\n  await consumer.subscribe({ topic, fromBeginning: true })\n  await consumer.run({\n    // eachBatch: async ({ batch }) => {\n    //   console.log(batch)\n    // },\n    eachMessage: async ({ topic, partition, message }) => {\n      const prefix = `${topic}[${partition} | ${message.offset}] / ${message.timestamp}`\n      console.log(`- ${prefix} ${message.key}#${message.value}`)\n    },\n  })\n}\n\nrun().catch(e => console.error(`[example/consumer] ${e.message}`, e))\n\nconst errorTypes = ['unhandledRejection', 'uncaughtException']\nconst signalTraps = ['SIGTERM', 'SIGINT', 'SIGUSR2']\n\nerrorTypes.forEach(type => {\n  process.on(type, async e => {\n    try {\n      console.log(`process.on ${type}`)\n      console.error(e)\n      await consumer.disconnect()\n      process.exit(0)\n    } catch (_) {\n      process.exit(1)\n    }\n  })\n})\n\nsignalTraps.forEach(type => {\n  process.once(type, async () => {\n    try {\n      await consumer.disconnect()\n    } finally {\n      process.kill(process.pid, type)\n    }\n  })\n})\n```\n\n----------------------------------------\n\nTITLE: Initializing Basic KafkaJS Client\nDESCRIPTION: Basic setup of KafkaJS client with broker list and client ID configuration. This is the minimum required configuration to create a Kafka client instance.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.14.0/Configuration.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Kafka } = require('kafkajs')\n\n// Create the client with the broker list\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092']\n})\n```\n\n----------------------------------------\n\nTITLE: Executing Kafka Transaction\nDESCRIPTION: Demonstrates how to send messages within a transaction and handle commit/abort scenarios. The transaction ensures all messages are either committed or rolled back atomically.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Transactions.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst transaction = await producer.transaction()\n\ntry {\n  await transaction.send({ topic, messages })\n\n  await transaction.commit()\n} catch (e) {\n  await transaction.abort()\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Transactional Producer in KafkaJS\nDESCRIPTION: Sets up a KafkaJS producer with the necessary configuration for transactional operations, including setting a transactional ID, limiting in-flight requests to 1, and enabling idempotence for exactly-once-semantics.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Transactions.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst client = new Kafka({\n  clientId: 'transactional-client',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n})\nconst producer = client.producer({\n  transactionalId: 'my-transactional-producer',\n  maxInFlightRequests: 1,\n  idempotent: true\n})\n```\n\n----------------------------------------\n\nTITLE: Producing a Message with KafkaJS\nDESCRIPTION: JavaScript code to create a producer, connect to Kafka, send a message to a topic, and then disconnect.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/GettingStarted.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n\nawait producer.connect()\nawait producer.send({\n  topic: 'test-topic',\n  messages: [\n    { value: 'Hello KafkaJS user!' },\n  ],\n})\n\nawait producer.disconnect()\n```\n\n----------------------------------------\n\nTITLE: Installing KafkaJS with NPM\nDESCRIPTION: Command to install KafkaJS package using NPM package manager.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/GettingStarted.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install kafkajs\n```\n\n----------------------------------------\n\nTITLE: eachBatch Consumer Handler\nDESCRIPTION: Advanced batch processing implementation with utilities for offset resolution, heartbeats, and running state checks.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Consuming.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nawait consumer.run({\n    eachBatch: async ({ batch, resolveOffset, heartbeat, isRunning, isStale }) => {\n        for (let message of batch.messages) {\n            console.log({\n                topic: batch.topic,\n                partition: batch.partition,\n                highWatermark: batch.highWatermark,\n                message: {\n                    offset: message.offset,\n                    key: message.key.toString(),\n                    value: message.value.toString(),\n                    headers: message.headers,\n                }\n            })\n\n            resolveOffset(message.offset)\n            await heartbeat()\n        }\n    },\n})\n```\n\n----------------------------------------\n\nTITLE: Producing to Multiple Kafka Topics with KafkaJS\nDESCRIPTION: Shows how to use the sendBatch method to produce messages to multiple Kafka topics simultaneously.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Producing.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst topicMessages = [\n  {\n    topic: 'topic-a',\n    messages: [{ key: 'key', value: 'hello topic-a' }],\n  },\n  {\n    topic: 'topic-b',\n    messages: [{ key: 'key', value: 'hello topic-b' }],\n  },\n  {\n    topic: 'topic-c',\n    messages: [\n      {\n        key: 'key',\n        value: 'hello topic-c',\n        headers: {\n          'correlation-id': '2bfb68bb-893a-423b-a7fa-7b568cad5b67',\n        },\n      }\n    ],\n  }\n]\nawait producer.sendBatch({ topicMessages })\n```\n\n----------------------------------------\n\nTITLE: Configuring KafkaJS Producer for Transactions in JavaScript\nDESCRIPTION: Sets up a KafkaJS client and producer with the necessary configuration for transactional messaging, including maxInFlightRequests and idempotent settings for exactly-once semantics.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Transactions.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst client = new Kafka({\n  clientId: 'transactional-client',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n})\nconst producer = client.producer({ maxInFlightRequests: 1, idempotent: true })\n```\n\n----------------------------------------\n\nTITLE: Sending Offsets in a KafkaJS Transaction\nDESCRIPTION: Shows how to include consumer offsets in a transaction using the sendOffsets method. This is crucial for consume-transform-produce patterns to ensure exactly-once processing, as the offsets will only be committed if the transaction succeeds.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Transactions.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nawait transaction.sendOffsets({\n  consumerGroupId, topics\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing a Kafka Consumer with SSL and SASL Authentication\nDESCRIPTION: This example shows how to create a Kafka consumer with SSL certificate validation and SASL authentication using the scram-sha-256 mechanism. It demonstrates secure connections to Kafka brokers with proper credential handling.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/ConsumerExample.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst ip = require('ip')\n\nconst { Kafka, logLevel } = require('../index')\n\nconst host = process.env.HOST_IP || ip.address()\n\nconst kafka = new Kafka({\n  logLevel: logLevel.INFO,\n  brokers: [`${host}:9094`],\n  clientId: 'example-consumer',\n  ssl: {\n    rejectUnauthorized: true\n  },\n  sasl: {\n    mechanism: 'scram-sha-256',\n    username: 'test',\n    password: 'testtest',\n  },\n})\n\nconst topic = 'topic-test'\nconst consumer = kafka.consumer({ groupId: 'test-group' })\n\nconst run = async () => {\n  await consumer.connect()\n  await consumer.subscribe({ topic, fromBeginning: true })\n  await consumer.run({\n    // eachBatch: async ({ batch }) => {\n    //   console.log(batch)\n    // },\n    eachMessage: async ({ topic, partition, message }) => {\n      const prefix = `${topic}[${partition} | ${message.offset}] / ${message.timestamp}`\n      console.log(`- ${prefix} ${message.key}#${message.value}`)\n    },\n  })\n}\n\nrun().catch(e => console.error(`[example/consumer] ${e.message}`, e))\n\nconst errorTypes = ['unhandledRejection', 'uncaughtException']\nconst signalTraps = ['SIGTERM', 'SIGINT', 'SIGUSR2']\n\nerrorTypes.forEach(type => {\n  process.on(type, async e => {\n    try {\n      console.log(`process.on ${type}`)\n      console.error(e)\n      await consumer.disconnect()\n      process.exit(0)\n    } catch (_) {\n      process.exit(1)\n    }\n  })\n})\n\nsignalTraps.forEach(type => {\n  process.once(type, async () => {\n    try {\n      await consumer.disconnect()\n    } finally {\n      process.kill(process.pid, type)\n    }\n  })\n})\n```\n\n----------------------------------------\n\nTITLE: Initializing KafkaJS Client with Basic Configuration\nDESCRIPTION: Creates a basic KafkaJS client instance with minimum required configuration including clientId and broker list.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Configuration.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Kafka } = require('kafkajs')\n\n// Create the client with the broker list\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092']\n})\n```\n\n----------------------------------------\n\nTITLE: Producing and Consuming JSON Messages with KafkaJS\nDESCRIPTION: Demonstrates how to send and receive JSON messages using KafkaJS. The producer serializes a JavaScript object to JSON before sending, while the consumer parses the received message buffer back into a JavaScript object.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/KafkaIntro.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nawait producer.send({\n  topic,\n  messages: [{\n    key: 'my-key',\n    value: JSON.stringify({ some: 'data' })\n  }]\n})\n\nconst eachMessage = async ({ /*topic, partition,*/ message }) => {\n  // From Kafka's perspective, both key and value are just bytes\n  // so we need to parse them.\n  console.log({\n    key: message.key.toString(),\n    value: JSON.parse(message.value.toString())\n  })\n\n  /**\n   * { key: 'my-key', value: { some: 'data' } }\n   */\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing JSON Message Format in Kafka using JavaScript\nDESCRIPTION: Demonstrates how to send and receive JSON messages in Kafka using producer and consumer implementations. Shows conversion between Buffer and JSON formats, including key-value message handling and parsing.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/KafkaIntro.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nawait producer.send({\n  topic,\n  messages: [{\n    key: 'my-key',\n    value: JSON.stringify({ some: 'data' })\n  }]\n})\n\nconst eachMessage = async ({ /*topic, partition,*/ message }) => {\n  // From Kafka's perspective, both key and value are just bytes\n  // so we need to parse them.\n  console.log({\n    key: message.key.toString(),\n    value: JSON.parse(message.value.toString())\n  })\n\n  /**\n   * { key: 'my-key', value: { some: 'data' } }\n   */\n}\n```\n\n----------------------------------------\n\nTITLE: Producing Messages with KafkaJS\nDESCRIPTION: Creates a producer instance, connects to Kafka, sends a message to a specific topic, and then disconnects.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/GettingStarted.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n\nawait producer.connect()\nawait producer.send({\n  topic: 'test-topic',\n  messages: [\n    { value: 'Hello KafkaJS user!' },\n  ],\n})\n\nawait producer.disconnect()\n```\n\n----------------------------------------\n\nTITLE: Creating a Kafka Producer in JavaScript\nDESCRIPTION: Demonstrates how to create a basic Kafka producer and a producer with custom options using KafkaJS.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/Producing.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer({\n    allowAutoTopicCreation: false,\n    transactionTimeout: 30000\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Socket Factory for KafkaJS in JavaScript\nDESCRIPTION: This code snippet demonstrates how to create a custom socket factory function for KafkaJS. It includes logic for handling both SSL and non-SSL connections, sets a keep-alive interval, and shows how to use the custom factory when initializing a Kafka client.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Configuration.md#2025-04-14_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Kafka } = require('kafkajs')\n\n// Example socket factory setting a custom TTL\nconst net = require('net')\nconst tls = require('tls')\n\nconst myCustomSocketFactory = ({ host, port, ssl, onConnect }) => {\n  const socket = ssl\n    ? tls.connect(\n        Object.assign({ host, port }, ssl),\n        onConnect\n      )\n    : net.connect(\n        { host, port },\n        onConnect\n      )\n\n  socket.setKeepAlive(true, 30000)\n\n  return socket\n}\n\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  socketFactory: myCustomSocketFactory,\n})\n```\n\n----------------------------------------\n\nTITLE: Creating Basic KafkaJS Producer\nDESCRIPTION: Initialize a basic Kafka producer instance using KafkaJS client.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Producing.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Compose for Kafka and Zookeeper\nDESCRIPTION: Docker compose configuration that sets up a development Kafka environment with a single broker and Zookeeper. The configuration enables auto-creation of topics and specifies ports for both services.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/DockerLocal.md#2025-04-14_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nversion: '2'\nservices:\n  zookeeper:\n    image: wurstmeister/zookeeper:latest\n    ports:\n      - \"2181:2181\"\n  kafka:\n    image: wurstmeister/kafka:2.11-1.1.1\n    ports:\n      - \"9092:9092\"\n    links:\n      - zookeeper\n    environment:\n      KAFKA_ADVERTISED_HOST_NAME: ${HOST_IP}\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'\n      KAFKA_DELETE_TOPIC_ENABLE: 'true'\n      KAFKA_CREATE_TOPICS: \"topic-test:1:1\"\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n```\n\n----------------------------------------\n\nTITLE: Sending Offsets in Transaction\nDESCRIPTION: Shows the structure for sending consumer offsets as part of a transaction, which is necessary for consume-transform-produce patterns. The offsets are committed only if the transaction succeeds.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Transactions.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nawait transaction.sendOffsets({\n  consumerGroupId, topics\n})\n```\n\n----------------------------------------\n\nTITLE: Producing Messages with KafkaJS\nDESCRIPTION: Creates a producer, connects to Kafka, sends a message to a specified topic, and then disconnects. Shows the basic message production workflow.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/GettingStarted.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n\nawait producer.connect()\nawait producer.send({\n  topic: 'test-topic',\n  messages: [\n    { value: 'Hello KafkaJS user!' },\n  ],\n})\n\nawait producer.disconnect()\n```\n\n----------------------------------------\n\nTITLE: Using GZIP Compression with KafkaJS Producer\nDESCRIPTION: Shows how to enable GZIP compression when sending messages using KafkaJS producer. GZIP compression is built into KafkaJS core functionality.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.4/Producing.md#2025-04-14_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst { CompressionTypes } = require('kafkajs')\n\nasync () => {\n  await producer.send({\n    topic: 'topic-name',\n    compression: CompressionTypes.GZIP,\n    messages: [\n        { key: 'key1', value: 'hello world' },\n        { key: 'key2', value: 'hey hey!' }\n    ],\n  })\n}\n```\n\n----------------------------------------\n\nTITLE: Sending Messages with KafkaJS Producer in JavaScript\nDESCRIPTION: Illustrates how to connect a producer and send messages to a Kafka topic. It includes examples of sending multiple messages with different keys.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.4/Producing.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n\nawait producer.connect()\nawait producer.send({\n    topic: 'topic-name',\n    messages: [\n        { key: 'key1', value: 'hello world' },\n        { key: 'key2', value: 'hey hey!' }\n    ],\n})\n```\n\n----------------------------------------\n\nTITLE: Sending Messages in a KafkaJS Transaction in JavaScript\nDESCRIPTION: Demonstrates how to initialize a transaction, send messages, and either commit or abort the transaction based on the outcome. This pattern ensures that messages are only committed if the entire transaction succeeds.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Transactions.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst  transaction = await producer.transaction()\n\ntry {\n  await transaction.send({ topic, messages })\n\n  await transaction.commit()\n} catch (e) {\n  await transaction.abort()\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring KafkaJS Producer for Transactions in JavaScript\nDESCRIPTION: This snippet demonstrates how to configure a KafkaJS producer client with the necessary settings for transactional messaging, including setting a transactional ID, maximum in-flight requests, and enabling idempotence.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/Transactions.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst client = new Kafka({\n  clientId: 'transactional-client',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n})\nconst producer = client.producer({\n  transactionalId: 'my-transactional-producer',\n  maxInFlightRequests: 1,\n  idempotent: true\n})\n```\n\n----------------------------------------\n\nTITLE: Connecting to Kafka Cluster using KafkaJS\nDESCRIPTION: JavaScript code snippet demonstrating how to connect to the local Kafka cluster using KafkaJS. It includes SSL and SASL configuration for secure communication.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/DevelopmentEnvironment.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst fs = require('fs')\nconst ip = require('ip')\n\nconst { Kafka, CompressionTypes, logLevel } = require('./index')\n\nconst host = process.env.HOST_IP || ip.address()\n\nconst kafka = new Kafka({\n  logLevel: logLevel.DEBUG,\n  brokers: [`${host}:9094`, `${host}:9097`, `${host}:9100`],\n  clientId: 'example-producer',\n  ssl: {\n    servername: 'localhost',\n    rejectUnauthorized: false,\n    ca: [fs.readFileSync('./testHelpers/certs/cert-signed', 'utf-8')],\n  },\n  sasl: {\n    mechanism: 'plain',\n    username: 'test',\n    password: 'testtest',\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing a Basic Kafka Consumer with KafkaJS in JavaScript\nDESCRIPTION: This example shows how to implement a basic Kafka consumer for a local development environment. It connects to a broker, subscribes to a topic, and processes messages individually. The code also includes proper error handling and graceful shutdown on process termination.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/ConsumerExample.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst fs = require('fs')\nconst ip = require('ip')\n\nconst { Kafka, logLevel } = require('../index')\n\nconst host = process.env.HOST_IP || ip.address()\n\nconst kafka = new Kafka({\n  logLevel: logLevel.INFO,\n  brokers: [`${host}:9092`],\n  clientId: 'example-consumer',\n})\n\nconst topic = 'topic-test'\nconst consumer = kafka.consumer({ groupId: 'test-group' })\n\nconst run = async () => {\n  await consumer.connect()\n  await consumer.subscribe({ topic, fromBeginning: true })\n  await consumer.run({\n    // eachBatch: async ({ batch }) => {\n    //   console.log(batch)\n    // },\n    eachMessage: async ({ topic, partition, message }) => {\n      const prefix = `${topic}[${partition} | ${message.offset}] / ${message.timestamp}`\n      console.log(`- ${prefix} ${message.key}#${message.value}`)\n    },\n  })\n}\n\nrun().catch(e => console.error(`[example/consumer] ${e.message}`, e))\n\nconst errorTypes = ['unhandledRejection', 'uncaughtException']\nconst signalTraps = ['SIGTERM', 'SIGINT', 'SIGUSR2']\n\nerrorTypes.map(type => {\n  process.on(type, async e => {\n    try {\n      console.log(`process.on ${type}`)\n      console.error(e)\n      await consumer.disconnect()\n      process.exit(0)\n    } catch (_) {\n      process.exit(1)\n    }\n  })\n})\n\nsignalTraps.map(type => {\n  process.once(type, async () => {\n    try {\n      await consumer.disconnect()\n    } finally {\n      process.kill(process.pid, type)\n    }\n  })\n})\n```\n\n----------------------------------------\n\nTITLE: Instantiating KafkaJS Client\nDESCRIPTION: Code snippet to create a new Kafka instance with KafkaJS, specifying a client ID and broker addresses.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/GettingStarted.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Kafka } = require('kafkajs')\n\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n})\n```\n\n----------------------------------------\n\nTITLE: Sending Messages in a KafkaJS Transaction\nDESCRIPTION: Demonstrates how to initialize a transaction, send messages within it, and then commit or abort based on the result. The transaction is created from a producer instance and handles error cases by aborting.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Transactions.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst  transaction = await producer.transaction()\n\ntry {\n  await transaction.send({ topic, messages })\n\n  await transaction.commit()\n} catch (e) {\n  await transaction.abort()\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Kafka Consumer\nDESCRIPTION: Initializes a Kafka consumer with a specified consumer group ID.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Consuming.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst consumer = kafka.consumer({ groupId: 'my-group' })\n```\n\n----------------------------------------\n\nTITLE: Configuring SSL/TLS for KafkaJS Client\nDESCRIPTION: Sets up secure TLS connections for a KafkaJS client by configuring SSL options. This example shows how to provide custom CA certificates, client key, and certificate files for TLS authentication.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Configuration.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst fs = require('fs')\n\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  ssl: {\n    rejectUnauthorized: false,\n    ca: [fs.readFileSync('/my/custom/ca.crt', 'utf-8')],\n    key: fs.readFileSync('/my/custom/client-key.pem', 'utf-8'),\n    cert: fs.readFileSync('/my/custom/client-cert.pem', 'utf-8')\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Compression Codec for KafkaJS Producer\nDESCRIPTION: This snippet illustrates how to implement a custom compression codec for KafkaJS. It defines a codec object with compress and decompress functions, and then adds it to the CompressionCodecs object.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Producing.md#2025-04-14_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nconst MyCustomSnappyCodec = {\n    async compress(encoder) {\n        return someCompressFunction(encoder.buffer)\n    },\n\n    async decompress(buffer) {\n        return someDecompressFunction(buffer)\n    }\n}\n\nconst { CompressionTypes, CompressionCodecs } = require('kafkajs')\nCompressionCodecs[CompressionTypes.Snappy] = MyCustomSnappyCodec\n```\n\n----------------------------------------\n\nTITLE: Initializing KafkaJS Client\nDESCRIPTION: Creates a new KafkaJS client instance by specifying a client ID and an array of broker addresses.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/GettingStarted.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Kafka } = require('kafkajs')\n\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092']\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Compression Codec in KafkaJS\nDESCRIPTION: Demonstrates how to implement a custom compression codec for KafkaJS using existing libraries. The example shows a custom Snappy codec implementation.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Producing.md#2025-04-14_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nconst MyCustomSnappyCodec = {\n    async compress(encoder) {\n        return someCompressFunction(encoder.buffer)\n    },\n\n    async decompress(buffer) {\n        return someDecompressFunction(buffer)\n    }\n}\n\nconst { CompressionTypes, CompressionCodecs } = require('kafkajs')\nCompressionCodecs[CompressionTypes.Snappy] = () => MyCustomSnappyCodec\n\nawait producer.send({\n    topic: 'topic-name',\n    compression: CompressionTypes.Snappy,\n    messages: [\n        { key: 'key1', value: 'hello world' },\n        { key: 'key2', value: 'hey hey!' }\n    ],\n})\n```\n\n----------------------------------------\n\nTITLE: Producing Messages with KafkaJS\nDESCRIPTION: Creates a producer, connects to Kafka, sends a message to a specified topic, and then disconnects. This demonstrates the basic pattern for producing messages to Kafka.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/GettingStarted.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n\nawait producer.connect()\nawait producer.send({\n  topic: 'test-topic',\n  messages: [\n    { value: 'Hello KafkaJS user!' },\n  ],\n})\n\nawait producer.disconnect()\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Authentication Mechanism in KafkaJS\nDESCRIPTION: This snippet demonstrates how to implement a custom authentication mechanism in KafkaJS using a plugin. It provides the structure for defining a custom SASL mechanism and authentication provider.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Configuration.md#2025-04-14_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\n{ \n  sasl: { \n      mechanism: <mechanism name>,\n      authenticationProvider: ({ host, port, logger, saslAuthenticate }) => { authenticate: () => Promise<void> }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Processing Messages with eachMessage Handler\nDESCRIPTION: Implementation of message processing using eachMessage handler, which processes one message at a time and includes heartbeat and pause functionality.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Consuming.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nawait consumer.run({\n    eachMessage: async ({ topic, partition, message, heartbeat, pause }) => {\n        console.log({\n            key: message.key.toString(),\n            value: message.value.toString(),\n            headers: message.headers,\n        })\n    },\n})\n```\n\n----------------------------------------\n\nTITLE: Initializing KafkaJS Client with Broker List\nDESCRIPTION: Creates a new Kafka client instance with a specified client ID and a list of seed brokers for initial connection and metadata loading.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Configuration.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Kafka } = require('kafkajs')\n\n// Create the client with the broker list\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092']\n})\n```\n\n----------------------------------------\n\nTITLE: Creating a KafkaJS Consumer\nDESCRIPTION: Demonstrates how to create a KafkaJS consumer with a specific group ID.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Consuming.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst consumer = kafka.consumer({ groupId: 'my-group' })\n```\n\n----------------------------------------\n\nTITLE: Handling JSON Messages with KafkaJS\nDESCRIPTION: Demonstrates how to send and receive JSON messages using KafkaJS. Shows message production with key-value pairs and message consumption with proper Buffer to string conversion and JSON parsing.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/KafkaIntro.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nawait producer.send({\n  topic,\n  messages: [{\n    key: 'my-key',\n    value: JSON.stringify({ some: 'data' })\n  }]\n})\n\nconst eachMessage = async ({ /*topic, partition,*/ message }) => {\n  // From Kafka's perspective, both key and value are just bytes\n  // so we need to parse them.\n  console.log({\n    key: message.key.toString(),\n    value: JSON.parse(message.value.toString())\n  })\n\n  /**\n   * { key: 'my-key', value: { some: 'data' } }\n   */\n}\n```\n\n----------------------------------------\n\nTITLE: Sending Messages to Kafka Topics in JavaScript\nDESCRIPTION: Shows how to connect to Kafka and send messages to a topic using the producer's send method. Includes examples with and without specified partitions.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/Producing.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n\nawait producer.connect()\nawait producer.send({\n    topic: 'topic-name',\n    messages: [\n        { key: 'key1', value: 'hello world' },\n        { key: 'key2', value: 'hey hey!' }\n    ],\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n\nawait producer.connect()\nawait producer.send({\n    topic: 'topic-name',\n    messages: [\n        { key: 'key1', value: 'hello world', partition: 0 },\n        { key: 'key2', value: 'hey hey!', partition: 1 }\n    ],\n})\n```\n\n----------------------------------------\n\nTITLE: Creating Kafka Topics Configuration\nDESCRIPTION: Demonstrates the structure for creating topics including partitions, replication factors, and custom configurations.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Admin.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.createTopics({\n    validateOnly: <boolean>,\n    waitForLeaders: <boolean>\n    timeout: <Number>,\n    topics: <ITopicConfig[]>,\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    topic: <String>,\n    numPartitions: <Number>,     // default: 1\n    replicationFactor: <Number>, // default: 1\n    replicaAssignment: <Array>,  // Example: [{ partition: 0, replicas: [0,1,2] }] - default: []\n    configEntries: <Array>       // Example: [{ name: 'cleanup.policy', value: 'compact' }] - default: []\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring ZSTD Compression in KafkaJS Producer\nDESCRIPTION: This snippet shows how to set up Zstandard (ZSTD) compression for use with KafkaJS. It involves installing the @kafkajs/zstd package, importing the necessary modules, and registering the ZSTD codec with KafkaJS.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.15.0/Producing.md#2025-04-14_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nconst { CompressionTypes, CompressionCodecs } = require('kafkajs')\nconst ZstdCodec = require('@kafkajs/zstd')\n\nCompressionCodecs[CompressionTypes.ZSTD] = ZstdCodec()\n```\n\n----------------------------------------\n\nTITLE: Producing to Multiple Topics with KafkaJS\nDESCRIPTION: Shows how to use the sendBatch method to produce messages to multiple topics simultaneously. This can be useful for scenarios like topic migration.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Producing.md#2025-04-14_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst topicMessages = [\n  {\n    topic: 'topic-a',\n    messages: [{ key: 'key', value: 'hello topic-a' }],\n  },\n  {\n    topic: 'topic-b',\n    messages: [{ key: 'key', value: 'hello topic-b' }],\n  },\n  {\n    topic: 'topic-c',\n    messages: [\n      {\n        key: 'key',\n        value: 'hello topic-c',\n        headers: {\n          'correlation-id': '2bfb68bb-893a-423b-a7fa-7b568cad5b67',\n        },\n      }\n    ],\n  }\n]\nawait producer.sendBatch({ topicMessages })\n```\n\n----------------------------------------\n\nTITLE: Setting Consumer Group Offsets with KafkaJS Admin Client\nDESCRIPTION: Illustrates how to manually set consumer group offsets for specific partitions using the setOffsets method.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Admin.md#2025-04-14_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.setOffsets({\n    groupId: <String>,\n    topic: <String>,\n    partitions: <SeekEntry[]>,\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    partition: <Number>,\n    offset: <String>,\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.setOffsets({\n    groupId: 'my-consumer-group',\n    topic: 'custom-topic',\n    partitions: [\n        { partition: 0, offset: '35' },\n        { partition: 3, offset: '19' },\n    ]\n})\n```\n\n----------------------------------------\n\nTITLE: Producing to Multiple Topics with KafkaJS\nDESCRIPTION: Shows how to use sendBatch to produce messages to multiple topics simultaneously. This can be useful for scenarios like migrating between topics.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.4/Producing.md#2025-04-14_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst topicMessages = [\n  {\n    topic: 'topic-a',\n    messages: [{ key: 'key', value: 'hello topic-a' }],\n  },\n  {\n    topic: 'topic-b',\n    messages: [{ key: 'key', value: 'hello topic-b' }],\n  },\n  {\n    topic: 'topic-c',\n    messages: [\n      {\n        key: 'key',\n        value: 'hello topic-c',\n        headers: {\n          'correlation-id': '2bfb68bb-893a-423b-a7fa-7b568cad5b67',\n        },\n      }\n    ],\n  }\n]\nawait producer.sendBatch({ topicMessages })\n```\n\n----------------------------------------\n\nTITLE: Configuring PLAIN/SCRAM Authentication for KafkaJS\nDESCRIPTION: Sets up PLAIN or SCRAM-SHA authentication for Kafka. The example demonstrates how to configure username and password authentication with SSL encryption. Includes options for authentication timeout and reauthentication threshold.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Configuration.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  // authenticationTimeout: 1000,\n  // reauthenticationThreshold: 10000,\n  ssl: true,\n  sasl: {\n    mechanism: 'plain', // scram-sha-256 or scram-sha-512\n    username: 'my-username',\n    password: 'my-password'\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: KafkaJS Consumer with SSL and SASL Authentication\nDESCRIPTION: Demonstrates configuring a Kafka consumer with SSL certificate validation and SASL authentication using scram-sha-256 mechanism. Includes error handling and graceful shutdown patterns.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.0/ConsumerExample.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst ip = require('ip')\n\nconst { Kafka, logLevel } = require('../index')\n\nconst host = process.env.HOST_IP || ip.address()\n\nconst kafka = new Kafka({\n  logLevel: logLevel.INFO,\n  brokers: [`${host}:9094`],\n  clientId: 'example-consumer',\n  ssl: {\n    rejectUnauthorized: true\n  },\n  sasl: {\n    mechanism: 'scram-sha-256',\n    username: 'test',\n    password: 'testtest',\n  },\n})\n\nconst topic = 'topic-test'\nconst consumer = kafka.consumer({ groupId: 'test-group' })\n\nconst run = async () => {\n  await consumer.connect()\n  await consumer.subscribe({ topic, fromBeginning: true })\n  await consumer.run({\n    // eachBatch: async ({ batch }) => {\n    //   console.log(batch)\n    // },\n    eachMessage: async ({ topic, partition, message }) => {\n      const prefix = `${topic}[${partition} | ${message.offset}] / ${message.timestamp}`\n      console.log(`- ${prefix} ${message.key}#${message.value}`)\n    },\n  })\n}\n\nrun().catch(e => console.error(`[example/consumer] ${e.message}`, e))\n\nconst errorTypes = ['unhandledRejection', 'uncaughtException']\nconst signalTraps = ['SIGTERM', 'SIGINT', 'SIGUSR2']\n\nerrorTypes.forEach(type => {\n  process.on(type, async e => {\n    try {\n      console.log(`process.on ${type}`)\n      console.error(e)\n      await consumer.disconnect()\n      process.exit(0)\n    } catch (_) {\n      process.exit(1)\n    }\n  })\n})\n\nsignalTraps.forEach(type => {\n  process.once(type, async () => {\n    try {\n      await consumer.disconnect()\n    } finally {\n      process.kill(process.pid, type)\n    }\n  })\n})\n```\n\n----------------------------------------\n\nTITLE: Secured Kafka Consumer with SSL and SASL Authentication\nDESCRIPTION: Shows how to configure a Kafka consumer with SSL and SASL authentication using scram-sha-256 mechanism. Includes the same consumer functionality as the basic example but with added security configurations.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/ConsumerExample.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst fs = require('fs')\nconst ip = require('ip')\n\nconst { Kafka, logLevel } = require('../index')\n\nconst host = process.env.HOST_IP || ip.address()\n\nconst kafka = new Kafka({\n  logLevel: logLevel.INFO,\n  brokers: [`${host}:9094`],\n  clientId: 'example-consumer',\n  ssl: {\n    rejectUnauthorized: true\n  },\n  sasl: {\n    mechanism: 'scram-sha-256',\n    username: 'test',\n    password: 'testtest',\n  },\n})\n\nconst topic = 'topic-test'\nconst consumer = kafka.consumer({ groupId: 'test-group' })\n\nconst run = async () => {\n  await consumer.connect()\n  await consumer.subscribe({ topic })\n  await consumer.run({\n    // eachBatch: async ({ batch }) => {\n    //   console.log(batch)\n    // },\n    eachMessage: async ({ topic, partition, message }) => {\n      const prefix = `${topic}[${partition} | ${message.offset}] / ${message.timestamp}`\n      console.log(`- ${prefix} ${message.key}#${message.value}`)\n    },\n  })\n}\n\nrun().catch(e => console.error(`[example/consumer] ${e.message}`, e))\n\nconst errorTypes = ['unhandledRejection', 'uncaughtException']\nconst signalTraps = ['SIGTERM', 'SIGINT', 'SIGUSR2']\n\nerrorTypes.map(type => {\n  process.on(type, async e => {\n    try {\n      console.log(`process.on ${type}`)\n      console.error(e)\n      await consumer.disconnect()\n      process.exit(0)\n    } catch (_) {\n      process.exit(1)\n    }\n  })\n})\n\nsignalTraps.map(type => {\n  process.once(type, async () => {\n    try {\n      await consumer.disconnect()\n    } finally {\n      process.kill(process.pid, type)\n    }\n  })\n})\n```\n\n----------------------------------------\n\nTITLE: Creating a KafkaJS Producer\nDESCRIPTION: Demonstrates how to create a basic KafkaJS producer instance.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Producing.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n```\n\n----------------------------------------\n\nTITLE: Implementing Pause and Resume in KafkaJS Consumer\nDESCRIPTION: Demonstrates how to pause and resume topic consumption when an external dependency is overloaded. Includes error handling and automatic resumption after a timeout.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Consuming.md#2025-04-14_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nawait consumer.connect()\nawait consumer.subscribe({ topic: 'jobs' })\n\nawait consumer.run({ eachMessage: async ({ topic, message }) => {\n    try {\n        await sendToDependency(message)\n    } catch (e) {\n        if (e instanceof TooManyRequestsError) {\n            consumer.pause([{ topic }])\n            setTimeout(() => consumer.resume([{ topic }]), e.retryAfter * 1000)\n        }\n\n        throw e\n    }\n}})\n```\n\n----------------------------------------\n\nTITLE: Sending Messages to a Single Kafka Topic\nDESCRIPTION: Shows how to connect a producer and send messages to a specific Kafka topic. It includes examples of sending messages with and without defined partitions.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Producing.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n\nawait producer.connect()\nawait producer.send({\n    topic: 'topic-name',\n    messages: [\n        { key: 'key1', value: 'hello world' },\n        { key: 'key2', value: 'hey hey!' }\n    ],\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n\nawait producer.connect()\nawait producer.send({\n    topic: 'topic-name',\n    messages: [\n        { key: 'key1', value: 'hello world', partition: 0 },\n        { key: 'key2', value: 'hey hey!', partition: 1 }\n    ],\n})\n```\n\n----------------------------------------\n\nTITLE: Deleting Kafka Groups with KafkaJS Admin API\nDESCRIPTION: This snippet demonstrates how to use the deleteGroups method to remove groups by their groupIds. It includes an example of error handling for partial failures when deleting multiple groups.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Admin.md#2025-04-14_snippet_17\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.deleteGroups([groupId])\n```\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.deleteGroups(['group-test'])\n```\n\nLANGUAGE: javascript\nCODE:\n```\ntry {\n    await admin.deleteGroups(['a', 'b', 'c'])\n} catch (error) {\n  // error.name 'KafkaJSDeleteGroupsError'\n  // error.groups = [{\n  //   groupId: a\n  //   error: KafkaJSProtocolError\n  // }]\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Partitions with KafkaJS Admin Client\nDESCRIPTION: Demonstrates how to create partitions using the createPartitions method. Includes options for validation, timeout, and partition assignments.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Admin.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.createPartitions({\n    validateOnly: <boolean>,\n    timeout: <Number>,\n    topicPartitions: <TopicPartition[]>,\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    topic: <String>,\n    count: <Number>,     // partition count\n    assignments: <Array<Array<Number>>> // Example: [[0,1],[1,2],[2,0]]\n}\n```\n\n----------------------------------------\n\nTITLE: Sending Messages to Kafka Topic\nDESCRIPTION: Example of connecting producer and sending multiple messages to a Kafka topic with different keys.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Producing.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n\nawait producer.connect()\nawait producer.send({\n    topic: 'topic-name',\n    messages: [\n        { key: 'key1', value: 'hello world' },\n        { key: 'key2', value: 'hey hey!' }\n    ],\n})\n```\n\n----------------------------------------\n\nTITLE: Fetching Topic Metadata with KafkaJS Admin Client\nDESCRIPTION: Shows how to fetch topic metadata using the fetchTopicMetadata method. Includes options for specifying topics and structures for returned metadata.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Admin.md#2025-04-14_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.fetchTopicMetadata({ topics: <Array<String>> })\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    topics: <Array<TopicMetadata>>,\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    name: <String>,\n    partitions: <Array<PartitionMetadata>> // default: 1\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    partitionErrorCode: <Number>, // default: 0\n    partitionId: <Number>,\n    leader: <Number>,\n    replicas: <Array<Number>>,\n    isr: <Array<Number>>,\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.fetchTopicMetadata()\n```\n\n----------------------------------------\n\nTITLE: Sending Messages with Headers in KafkaJS\nDESCRIPTION: Demonstrates how to send a message with headers using the KafkaJS producer. Headers allow messages to carry extra metadata.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Producing.md#2025-04-14_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nawait producer.send({\n    topic: 'topic-name',\n    messages: [{\n        key: 'key1',\n        value: 'hello world',\n        headers: {\n            'correlation-id': '2bfb68bb-893a-423b-a7fa-7b568cad5b67',\n            'system-id': 'my-system',\n        }\n    }]\n})\n```\n\n----------------------------------------\n\nTITLE: Consuming Messages with KafkaJS\nDESCRIPTION: JavaScript code to create a consumer, connect to Kafka, subscribe to a topic, and process incoming messages.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/GettingStarted.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst consumer = kafka.consumer({ groupId: 'test-group' })\n\nawait consumer.connect()\nawait consumer.subscribe({ topic: 'test-topic', fromBeginning: true })\n\nawait consumer.run({\n  eachMessage: async ({ topic, partition, message }) => {\n    console.log({\n      value: message.value.toString(),\n    })\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing KafkaJS Producer with Error Handling and Compression\nDESCRIPTION: Demonstrates setting up a Kafka producer that sends compressed messages to a topic every 3 seconds. Includes configuration for compression, error handling, and graceful shutdown on process termination. Uses GZIP compression and generates random messages with unique keys.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/ProducerExample.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst ip = require('ip')\n\nconst { Kafka, CompressionTypes, logLevel } = require('../index')\n\nconst host = process.env.HOST_IP || ip.address()\n\nconst kafka = new Kafka({\n  logLevel: logLevel.DEBUG,\n  brokers: [`${host}:9092`],\n  clientId: 'example-producer',\n})\n\nconst topic = 'topic-test'\nconst producer = kafka.producer()\n\nconst getRandomNumber = () => Math.round(Math.random(10) * 1000)\nconst createMessage = num => ({\n  key: `key-${num}`,\n  value: `value-${num}-${new Date().toISOString()}`,\n})\n\nconst sendMessage = () => {\n  return producer\n    .send({\n      topic,\n      compression: CompressionTypes.GZIP,\n      messages: Array(getRandomNumber())\n        .fill()\n        .map(_ => createMessage(getRandomNumber())),\n    })\n    .then(console.log)\n    .catch(e => console.error(`[example/producer] ${e.message}`, e))\n}\n\nconst run = async () => {\n  await producer.connect()\n  setInterval(sendMessage, 3000)\n}\n\nrun().catch(e => console.error(`[example/producer] ${e.message}`, e))\n\nconst errorTypes = ['unhandledRejection', 'uncaughtException']\nconst signalTraps = ['SIGTERM', 'SIGINT', 'SIGUSR2']\n\nerrorTypes.map(type => {\n  process.on(type, async () => {\n    try {\n      console.log(`process.on ${type}`)\n      await producer.disconnect()\n      process.exit(0)\n    } catch (_) {\n      process.exit(1)\n    }\n  })\n})\n\nsignalTraps.map(type => {\n  process.once(type, async () => {\n    try {\n      await producer.disconnect()\n    } finally {\n      process.kill(process.pid, type)\n    }\n  })\n})\n```\n\n----------------------------------------\n\nTITLE: KafkaJS Consumer Options Configuration\nDESCRIPTION: Comprehensive consumer configuration options including group management, network timeouts, partition assignment, and data fetch parameters.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Consuming.md#2025-04-14_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nkafka.consumer({\n  groupId: <String>,\n  partitionAssigners: <Array>,\n  sessionTimeout: <Number>,\n  rebalanceTimeout: <Number>,\n  heartbeatInterval: <Number>,\n  metadataMaxAge: <Number>,\n  allowAutoTopicCreation: <Boolean>,\n  maxBytesPerPartition: <Number>,\n  minBytes: <Number>,\n  maxBytes: <Number>,\n  maxWaitTimeInMs: <Number>,\n  retry: <Object>,\n})\n```\n\n----------------------------------------\n\nTITLE: TypeScript KafkaJS Producer Class Implementation\nDESCRIPTION: A TypeScript class implementation of a Kafka producer with batch sending capability. Includes proper typing, connection management, and message format conversion. The implementation provides a clean interface for sending batched messages to Kafka topics.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/ProducerExample.md#2025-04-14_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Kafka, Message, Producer, ProducerBatch, TopicMessages } from 'kafkajs'\n\ninterface CustomMessageFormat { a: string }\n\nexport default class ProducerFactory {\n  private producer: Producer\n\n  constructor() {\n    this.producer = this.createProducer()\n  }\n\n  public async start(): Promise<void> {\n    try {\n      await this.producer.connect()\n    } catch (error) {\n      console.log('Error connecting the producer: ', error)\n    }\n  }\n\n  public async shutdown(): Promise<void> {\n    await this.producer.disconnect()\n  }\n\n  public async sendBatch(messages: Array<CustomMessageFormat>): Promise<void> {\n    const kafkaMessages: Array<Message> = messages.map((message) => {\n      return {\n        value: JSON.stringify(message)\n      }\n    })\n\n    const topicMessages: TopicMessages = {\n      topic: 'producer-topic',\n      messages: kafkaMessages\n    }\n\n    const batch: ProducerBatch = {\n      topicMessages: [topicMessages]\n    }\n\n    await this.producer.sendBatch(batch)\n  }\n\n  private createProducer() : Producer {\n    const kafka = new Kafka({\n      clientId: 'producer-client',\n      brokers: ['localhost:9092'],\n    })\n\n    return kafka.producer()\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic KafkaJS Producer in JavaScript\nDESCRIPTION: Demonstrates how to create a simple KafkaJS producer without any additional options.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.4/Producing.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n```\n\n----------------------------------------\n\nTITLE: Sending Messages to a Kafka Topic in JavaScript\nDESCRIPTION: Connects to Kafka and sends multiple messages with different keys to a specified topic.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Producing.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n\nawait producer.connect()\nawait producer.send({\n    topic: 'topic-name',\n    messages: [\n        { key: 'key1', value: 'hello world' },\n        { key: 'key2', value: 'hey hey!' }\n    ],\n})\n```\n\n----------------------------------------\n\nTITLE: Creating a KafkaJS Producer with Options in JavaScript\nDESCRIPTION: Shows how to create a KafkaJS producer with specific options, such as disabling auto topic creation and setting a transaction timeout.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.4/Producing.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer({\n    allowAutoTopicCreation: false,\n    transactionTimeout: 30000\n})\n```\n\n----------------------------------------\n\nTITLE: Fetching Topic Metadata with KafkaJS Admin Client\nDESCRIPTION: Shows how to fetch topic metadata using the admin client's fetchTopicMetadata method. Includes structures for TopicsMetadata, TopicMetadata, and PartitionMetadata.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Admin.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.fetchTopicMetadata({ topics: <Array<String>> })\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    topics: <Array<TopicMetadata>>,\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    topic: <String>,\n    partitions: <Array<PartitionMetadata>> // default: 1\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    partitionErrorCode: <Number>, // default: 0\n    partitionId: <Number>,\n    leader: <Number>,\n    replicas: <Array<Number>>,\n    isr: <Array<Number>>,\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.fetchTopicMetadata()\n```\n\n----------------------------------------\n\nTITLE: Describing Kafka Configurations\nDESCRIPTION: Retrieves configuration for specified Kafka resources using the admin.describeConfigs() method. Supports querying both all configs and specific config parameters for resources like topics.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/Admin.md#2025-04-14_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.describeConfigs({\n  includeSynonyms: <boolean>,\n  resources: <ResourceConfigQuery[]>\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst { ConfigResourceTypes } = require('kafkajs')\n\nawait admin.describeConfigs({\n  includeSynonyms: false,\n  resources: [\n    {\n      type: ConfigResourceTypes.TOPIC,\n      name: 'topic-name'\n    }\n  ]\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Partitioner in KafkaJS\nDESCRIPTION: Demonstrates how to create and use a custom partitioner function for message distribution in KafkaJS.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Producing.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst MyPartitioner = () => {\n    return ({ topic, partitionMetadata, message }) => {\n        // select a partition based on some logic\n        // return the partition number\n        return 0\n    }\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\nkafka.producer({ createPartitioner: MyPartitioner })\n```\n\n----------------------------------------\n\nTITLE: Describing Kafka Configs with KafkaJS Admin API\nDESCRIPTION: This snippet demonstrates how to use the describeConfigs method to retrieve configuration for specified resources. It shows examples of returning all configs and specific configs for a given resource.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Admin.md#2025-04-14_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.describeConfigs({\n  includeSynonyms: <boolean>,\n  resources: <ResourceConfigQuery[]>\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst { ConfigResourceTypes } = require('kafkajs')\n\nawait admin.describeConfigs({\n  includeSynonyms: false,\n  resources: [\n    {\n      type: ConfigResourceTypes.TOPIC,\n      name: 'topic-name'\n    }\n  ]\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst { ConfigResourceTypes } = require('kafkajs')\n\nawait admin.describeConfigs({\n  includeSynonyms: false,\n  resources: [\n    {\n      type: ConfigResourceTypes.TOPIC,\n      name: 'topic-name',\n      configNames: ['cleanup.policy']\n    }\n  ]\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Partitioner in KafkaJS\nDESCRIPTION: Shows how to create and use a custom partitioner function for message distribution in KafkaJS producer. The partitioner can implement custom logic for partition selection.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Producing.md#2025-04-14_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst MyPartitioner = () => {\n    return ({ topic, partitionMetadata, message }) => {\n        // select a partition based on some logic\n        // return the partition number\n        return 0\n    }\n}\n\nkafka.producer({ createPartitioner: MyPartitioner })\n```\n\n----------------------------------------\n\nTITLE: Processing Kafka Messages with eachMessage\nDESCRIPTION: Shows how to use the eachMessage handler to process Kafka messages one at a time, including accessing message key, value, and headers.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Consuming.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nawait consumer.run({\n    eachMessage: async ({ topic, partition, message }) => {\n        console.log({\n            key: message.key.toString(),\n            value: message.value.toString(),\n            headers: message.headers,\n        })\n    },\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Partitioner in KafkaJS\nDESCRIPTION: Explains how to create and use a custom partitioner function for the KafkaJS producer. The partitioner determines which partition a message should be sent to.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Producing.md#2025-04-14_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst MyPartitioner = () => {\n    return ({ topic, partitionMetadata, message }) => {\n        // select a partition based on some logic\n        // return the partition number\n        return 0\n    }\n}\n\nkafka.producer({ createPartitioner: MyPartitioner })\n```\n\n----------------------------------------\n\nTITLE: Kafka Producer Send Method Signature in JavaScript\nDESCRIPTION: Defines the full method signature for the producer.send() function, showing all available options including topic, messages, acknowledgment level, timeout, and compression.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Producing.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nawait producer.send({\n    topic: <String>,\n    messages: <Message[]>,\n    acks: <Number>,\n    timeout: <Number>,\n    compression: <CompressionTypes>,\n})\n```\n\n----------------------------------------\n\nTITLE: Creating a Kafka Producer Factory in TypeScript with KafkaJS\nDESCRIPTION: This snippet showcases a TypeScript implementation of a ProducerFactory class using KafkaJS. It includes methods for starting and shutting down the producer, as well as sending batches of messages. The class encapsulates the producer creation and provides a structured approach to managing Kafka producers.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/ProducerExample.md#2025-04-14_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Kafka, logCreator, logLevel, Producer, ProducerBatch } from 'kafkajs'\n\ninterface CustomMessageFormat { a: string }\n\nexport default class ProducerFactory {\n  private producer: Producer\n\n  constructor() {\n    this.producer = this.createProducer()\n  }\n\n  public async start(): Promise<void> {\n    try {\n      await this.producer.connect()\n    } catch (error) {\n      console.log('Error connecting the producer: ', error)\n    }\n  }\n\n  public async shutdown(): Promise<void> {\n    await this.producer.disconnect()\n  }\n\n  public async sendBatch(messages: Array<CustomMessageFormat>): Promise<void> {\n    const kafkaMessages: Array<Message> = messages.map((message) => {\n      return {\n        value: JSON.stringify(message)\n      }\n    })\n\n    const topicMessages: TopicMessages = {\n      topic: 'producer-topic',\n      messages: kafkaMessages\n    }\n\n    const batch: ProducerBatch = {\n      topicMessages: [topicMessages]\n    }\n\n    await this.producer.sendBatch(batch)\n  }\n\n  private createProducer() : Producer {\n    const kafka = new Kafka({\n      clientId: 'producer-client',\n      brokers: ['localhost:9092'],\n    })\n\n    return kafka.producer()\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Producing and Consuming JSON Messages with KafkaJS in JavaScript\nDESCRIPTION: This snippet demonstrates how to send and receive JSON messages using KafkaJS. It shows how to stringify JSON data when producing messages and how to parse the message buffer back into JSON when consuming.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/KafkaIntro.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nawait producer.send({\n  topic,\n  messages: [{\n    key: 'my-key',\n    value: JSON.stringify({ some: 'data' })\n  }]\n})\n\nconst eachMessage = async ({ /*topic, partition,*/ message }) => {\n  // From Kafka's perspective, both key and value are just bytes\n  // so we need to parse them.\n  console.log({\n    key: message.key.toString(),\n    value: JSON.parse(message.value.toString())\n  })\n\n  /**\n   * { key: 'my-key', value: { some: 'data' } }\n   */\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing KafkaJS Client\nDESCRIPTION: Creates a new KafkaJS client instance by specifying a client ID and an array of broker addresses.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/GettingStarted.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Kafka } = require('kafkajs')\n\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092']\n})\n```\n\n----------------------------------------\n\nTITLE: Listing Kafka Topics using KafkaJS Admin Client\nDESCRIPTION: Uses the listTopics method to retrieve an array of all existing topic names in the Kafka cluster.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Admin.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.listTopics()\n// [ 'topic-1', 'topic-2', 'topic-3', ... ]\n```\n\n----------------------------------------\n\nTITLE: Sending Messages to Specific Partitions\nDESCRIPTION: Shows how to send messages to specific partitions within a Kafka topic by explicitly setting the partition number.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Producing.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n\nawait producer.connect()\nawait producer.send({\n    topic: 'topic-name',\n    messages: [\n        { key: 'key1', value: 'hello world', partition: 0 },\n        { key: 'key2', value: 'hey hey!', partition: 1 }\n    ],\n})\n```\n\n----------------------------------------\n\nTITLE: Describing Kafka ACLs with KafkaJS Admin API\nDESCRIPTION: This snippet demonstrates how to use the describeAcls method to retrieve information about Access Control Lists (ACLs) for Kafka resources based on specified criteria.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Admin.md#2025-04-14_snippet_21\n\nLANGUAGE: javascript\nCODE:\n```\nconst {\n  AclResourceTypes,\n  AclOperationTypes,\n  AclPermissionTypes,\n  ResourcePatternTypes,\n} = require('kafkajs')\n\nawait admin.describeAcls({\n  resourceName: 'topic-name,\n  resourceType: AclResourceTypes.TOPIC,\n  host: '*',\n  permissionType: AclPermissionTypes.ALLOW,\n  operation: AclOperationTypes.ANY,\n  resourcePatternTypeFilter: ResourcePatternTypes.LITERAL,\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Partitioner in KafkaJS\nDESCRIPTION: Demonstrates how to create and use a custom partitioner function to control message partition assignment in KafkaJS.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/Producing.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst MyPartitioner = () => {\n    return ({ topic, partitionMetadata, message }) => {\n        // select a partition based on some logic\n        // return the partition number\n        return 0\n    }\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\nkafka.producer({ createPartitioner: MyPartitioner })\n```\n\n----------------------------------------\n\nTITLE: Describing and Altering Configs\nDESCRIPTION: Methods for retrieving and modifying configuration for Kafka resources.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Admin.md#2025-04-14_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst { ResourceTypes } = require('kafkajs')\n\nawait admin.describeConfigs({\n  includeSynonyms: false,\n  resources: [\n    {\n      type: ResourceTypes.TOPIC,\n      name: 'topic-name'\n    }\n  ]\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst { ResourceTypes } = require('kafkajs')\n\nawait admin.alterConfigs({\n    resources: [{\n        type: ResourceTypes.TOPIC,\n        name: 'topic-name',\n        configEntries: [{ name: 'cleanup.policy', value: 'compact' }]\n    }]\n})\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Kafka Topics\nDESCRIPTION: Shows how to connect a consumer and subscribe to multiple topics, including an option to start from the beginning of a topic.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Consuming.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait consumer.connect()\n\n// Subscribe can be called several times\nawait consumer.subscribe({ topic: 'topic-A' })\nawait consumer.subscribe({ topic: 'topic-B' })\n\n// It's possible to start from the beginning:\n// await consumer.subscribe({ topic: 'topic-C', fromBeginning: true })\n```\n\n----------------------------------------\n\nTITLE: Winston Logger Integration with KafkaJS\nDESCRIPTION: Example of creating a custom log creator using the Winston logging library. Includes a log level mapping function and configuration for console and file transports.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/CustomLogger.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst { logLevel } = require('kafkajs')\nconst winston = require('winston')\nconst toWinstonLogLevel = level => switch(level) {\n    case logLevel.ERROR:\n    case logLevel.NOTHING:\n        return 'error'\n    case logLevel.WARN:\n        return 'warn'\n    case logLevel.INFO:\n        return 'info'\n    case logLevel.DEBUG:\n        return 'debug'\n}\n\nconst WinstonLogCreator = logLevel => {\n    const logger = winston.createLogger({\n        level: toWinstonLogLevel(logLevel),\n        transports: [\n            new winston.transports.Console(),\n            new winston.transports.File({ filename: 'myapp.log' })\n        ]\n    })\n\n    return ({ namespace, level, label, log }) => {\n        const { message, ...extra } = log\n        logger.log({\n            level: toWinstonLogLevel(level),\n            message,\n            extra,\n        })\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Partition-Aware Concurrency in KafkaJS\nDESCRIPTION: Shows how to configure concurrent processing of messages from multiple partitions using the partitionsConsumedConcurrently option.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Consuming.md#2025-04-14_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconsumer.run({\n    partitionsConsumedConcurrently: 3, // Default: 1\n    eachMessage: async ({ topic, partition, message }) => {\n        // This will be called up to 3 times concurrently\n    },\n})\n```\n\n----------------------------------------\n\nTITLE: Sending Messages with Headers\nDESCRIPTION: Example of sending Kafka messages with custom headers for additional metadata.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Producing.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nawait producer.send({\n    topic: 'topic-name',\n    messages: [{\n        key: 'key1',\n        value: 'hello world',\n        headers: {\n            'correlation-id': '2bfb68bb-893a-423b-a7fa-7b568cad5b67',\n            'system-id': 'my-system'\n        }\n    }]\n})\n```\n\n----------------------------------------\n\nTITLE: Creating a KafkaJS Producer\nDESCRIPTION: Demonstrates how to create a producer instance using the KafkaJS client.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.15.0/Producing.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n```\n\n----------------------------------------\n\nTITLE: TypeScript Consumer Implementation with Batch Processing\nDESCRIPTION: Implements a Kafka consumer in TypeScript with both message-by-message and batch processing capabilities. Includes proper type definitions and class-based structure with connection management.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.0/ConsumerExample.md#2025-04-14_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Consumer, ConsumerSubscribeTopics, EachBatchPayload, Kafka, EachMessagePayload } from 'kafkajs'\n\nexport default class ExampleConsumer {\n  private kafkaConsumer: Consumer\n  private messageProcessor: ExampleMessageProcessor\n\n  public constructor(messageProcessor: ExampleMessageProcessor) {\n    this.messageProcessor = messageProcessor\n    this.kafkaConsumer = this.createKafkaConsumer()\n  }\n\n  public async startConsumer(): Promise<void> {\n    const topic: ConsumerSubscribeTopics = {\n      topics: ['example-topic'],\n      fromBeginning: false\n    }\n\n    try {\n      await this.kafkaConsumer.connect()\n      await this.kafkaConsumer.subscribe(topic)\n\n      await this.kafkaConsumer.run({\n        eachMessage: async (messagePayload: EachMessagePayload) => {\n          const { topic, partition, message } = messagePayload\n          const prefix = `${topic}[${partition} | ${message.offset}] / ${message.timestamp}`\n          console.log(`- ${prefix} ${message.key}#${message.value}`)\n        }\n      })\n    } catch (error) {\n      console.log('Error: ', error)\n    }\n  }\n\n  public async startBatchConsumer(): Promise<void> {\n    const topic: ConsumerSubscribeTopics = {\n      topics: ['example-topic'],\n      fromBeginning: false\n    }\n\n    try {\n      await this.kafkaConsumer.connect()\n      await this.kafkaConsumer.subscribe(topic)\n      await this.kafkaConsumer.run({\n        eachBatch: async (eachBatchPayload: EachBatchPayload) => {\n          const { batch } = eachBatchPayload\n          for (const message of batch.messages) {\n            const prefix = `${batch.topic}[${batch.partition} | ${message.offset}] / ${message.timestamp}`\n            console.log(`- ${prefix} ${message.key}#${message.value}`) \n          }\n        }\n      })\n    } catch (error) {\n      console.log('Error: ', error)\n    }\n  }\n\n  public async shutdown(): Promise<void> {\n    await this.kafkaConsumer.disconnect()\n  }\n\n  private createKafkaConsumer(): Consumer {\n    const kafka = new Kafka({ \n      clientId: 'client-id',\n      brokers: ['example.kafka.broker:9092']\n    })\n    const consumer = kafka.consumer({ groupId: 'consumer-group' })\n    return consumer\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Producing Messages to Multiple Kafka Topics\nDESCRIPTION: Demonstrates how to use the sendBatch method to produce messages to multiple Kafka topics simultaneously.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Producing.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst topicMessages = [\n  {\n    topic: 'topic-a',\n    messages: [{ key: 'key', value: 'hello topic-a' }],\n  },\n  {\n    topic: 'topic-b',\n    messages: [{ key: 'key', value: 'hello topic-b' }],\n  },\n  {\n    topic: 'topic-c',\n    messages: [\n      {\n        key: 'key',\n        value: 'hello topic-c',\n        headers: {\n          'correlation-id': '2bfb68bb-893a-423b-a7fa-7b568cad5b67',\n        },\n      }\n    ],\n  }\n]\nawait producer.sendBatch({ topicMessages })\n```\n\n----------------------------------------\n\nTITLE: Creating Kafka Topics\nDESCRIPTION: Creates new topics with specified configuration including partitions and replication factors. Returns true if successful, false if topic exists.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/Admin.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.createTopics({\n    validateOnly: <boolean>,\n    waitForLeaders: <boolean>\n    timeout: <Number>,\n    topics: <ITopicConfig[]>,\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    topic: <String>,\n    numPartitions: <Number>,     // default: 1\n    replicationFactor: <Number>, // default: 1\n    replicaAssignment: <Array>,  // Example: [{ partition: 0, replicas: [0,1,2] }] - default: []\n    configEntries: <Array>       // Example: [{ name: 'cleanup.policy', value: 'compact' }] - default: []\n}\n```\n\n----------------------------------------\n\nTITLE: Manual Offset Committing in KafkaJS Consumer\nDESCRIPTION: Example showing how to manually commit offsets using the consumer.commitOffsets method. Demonstrates disabling autoCommit and committing offsets for multiple topic partitions.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Consuming.md#2025-04-14_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconsumer.run({\n    autoCommit: false,\n    eachMessage: async ({ topic, partition, message }) => {\n        // Process the message somehow\n    },\n})\n\nconsumer.commitOffsets([\n  { topic: 'topic-A', partition: 0, offset: '1' },\n  { topic: 'topic-A', partition: 1, offset: '3' },\n  { topic: 'topic-B', partition: 0, offset: '2' }\n])\n```\n\n----------------------------------------\n\nTITLE: Configuring SASL Authentication with AWS IAM\nDESCRIPTION: Sets up SASL authentication for KafkaJS using AWS IAM credentials. Requires authorizationIdentity, accessKeyId, secretAccessKey, and optionally sessionToken.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Configuration.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  // authenticationTimeout: 1000,\n  sasl: {\n    mechanism: 'aws',\n    authorizationIdentity: 'AIDAIOSFODNN7EXAMPLE', // UserId or RoleId\n    accessKeyId: 'AKIAIOSFODNN7EXAMPLE',\n    secretAccessKey: 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',\n    sessionToken: 'WHArYt8i5vfQUrIU5ZbMLCbjcAiv/Eww6eL9tgQMJp6QFNEXAMPLETOKEN' // Optional\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Initializing and Connecting KafkaJS Admin Client\nDESCRIPTION: Creates a new Kafka instance and initializes the admin client. Demonstrates how to connect and disconnect from the admin client.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Admin.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst kafka = new Kafka(...)\nconst admin = kafka.admin()\n\n// remember to connect and disconnect when you are done\nawait admin.connect()\nawait admin.disconnect()\n```\n\n----------------------------------------\n\nTITLE: Retry Mechanism Configuration\nDESCRIPTION: Setup of retry mechanism parameters including retry times, factors, and maximum attempts. Used for handling connection and API call retries.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.14.0/Configuration.md#2025-04-14_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  retry: {\n    initialRetryTime: 100,\n    retries: 8\n  }\n})\n```\n\n----------------------------------------\n\nTITLE: Batch Processing with Manual Control\nDESCRIPTION: Example of batch processing with manual control of message resolution and handling of consumer state.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Consuming.md#2025-04-14_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconsumer.run({\n    eachBatchAutoResolve: false,\n    eachBatch: async ({ batch, resolveOffset, heartbeat, isRunning, isStale }) => {\n        for (let message of batch.messages) {\n            if (!isRunning() || isStale()) break\n            await processMessage(message)\n            resolveOffset(message.offset)\n            await heartbeat()\n        }\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring KafkaJS Client with Custom Logger in JavaScript\nDESCRIPTION: This snippet shows how to configure a KafkaJS client with a custom log creator. It sets the client ID, brokers, log level, and applies the custom log creator.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/CustomLogger.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst kafka = new Kafka({\n    clientId: 'my-app',\n    brokers: ['kafka1:9092', 'kafka2:9092'],\n    logLevel: logLevel.ERROR,\n    logCreator: WinstonLogCreator\n})\n```\n\n----------------------------------------\n\nTITLE: Resetting Consumer Group Offsets by Timestamp with KafkaJS Admin Client\nDESCRIPTION: Demonstrates how to reset consumer group offsets to a specific timestamp by combining fetchTopicOffsetsByTimestamp and setOffsets methods.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Admin.md#2025-04-14_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.setOffsets({ groupId, topic, partitions: await admin.fetchTopicOffsetsByTimestamp(topic, timestamp) })\n```\n\n----------------------------------------\n\nTITLE: Creating Topics with KafkaJS Admin Client\nDESCRIPTION: Shows how to create Kafka topics using the admin client. The method returns true if the topic was created successfully or false if it already exists.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Admin.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.createTopics({\n    validateOnly: <boolean>,\n    waitForLeaders: <boolean>\n    timeout: <Number>,\n    topics: <ITopicConfig[]>,\n})\n```\n\n----------------------------------------\n\nTITLE: Pausing and Resuming Kafka Consumer in JavaScript\nDESCRIPTION: Demonstrates how to pause and resume consuming from a topic when an external dependency is overloaded. It uses a try-catch block to handle errors and implements a timeout for resuming consumption.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Consuming.md#2025-04-14_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nawait consumer.connect()\nawait consumer.subscribe({ topic: 'jobs' })\n\nawait consumer.run({ eachMessage: async ({ topic, message }) => {\n    try {\n        await sendToDependency(message)\n    } catch (e) {\n        if (e instanceof TooManyRequestsError) {\n            consumer.pause([{ topic }])\n            setTimeout(() => consumer.resume([{ topic }]), e.retryAfter * 1000)\n        }\n\n        throw e\n    }\n}})\n```\n\n----------------------------------------\n\nTITLE: KafkaJS Producer Send Method Signature\nDESCRIPTION: Illustrates the full signature of the send method in KafkaJS producer, including options for topic, messages, acknowledgments, timeout, and compression.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Producing.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nawait producer.send({\n    topic: <String>,\n    messages: <Message[]>,\n    acks: <Number>,\n    timeout: <Number>,\n    compression: <CompressionTypes>,\n})\n```\n\n----------------------------------------\n\nTITLE: Listing Kafka Groups using KafkaJS Admin API\nDESCRIPTION: Retrieves a list of all available groups on the Kafka broker using the admin.listGroups method.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Admin.md#2025-04-14_snippet_15\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.listGroups()\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    groups: [\n        {groupId: 'testgroup', protocolType: 'consumer'}\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Partitioner in KafkaJS\nDESCRIPTION: Demonstrates how to create and use a custom partitioner function with KafkaJS producer. The partitioner determines which partition a message should be sent to.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.4/Producing.md#2025-04-14_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst MyPartitioner = () => {\n    return ({ topic, partitionMetadata, message }) => {\n        // select a partition based on some logic\n        // return the partition number\n        return 0\n    }\n}\n\nkafka.producer({ createPartitioner: MyPartitioner })\n```\n\n----------------------------------------\n\nTITLE: Setting Log Level for KafkaJS Client\nDESCRIPTION: Configures the logging level for the KafkaJS client. This example sets the log level to ERROR at initialization and then demonstrates how to change it for specific components.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Configuration.md#2025-04-14_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Kafka, logLevel } = require('kafkajs')\n\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  logLevel: logLevel.ERROR\n})\nkafka.logger().setLogLevel(logLevel.WARN)\n\nconst producer = kafka.producer(...)\nproducer.logger().setLogLevel(logLevel.INFO)\n\nconst consumer = kafka.consumer(...)\nconsumer.logger().setLogLevel(logLevel.DEBUG)\n\nconst admin = kafka.admin(...)\nadmin.logger().setLogLevel(logLevel.NOTHING)\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Kafka Topics\nDESCRIPTION: Examples of subscribing to individual topics and configuring options like starting from beginning. Shows multiple subscription patterns.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Consuming.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait consumer.connect()\n\nawait consumer.subscribe({ topic: 'topic-A' })\n\n// Subscribe can be called several times\nawait consumer.subscribe({ topic: 'topic-B')\nawait consumer.subscribe({ topic: 'topic-C')\n\n// It's possible to start from the beginning of the topic\nawait consumer.subscribe({ topic: 'topic-D', fromBeginning: true })\n```\n\n----------------------------------------\n\nTITLE: Setting Log Level in KafkaJS\nDESCRIPTION: This example demonstrates how to set the log level when creating a new Kafka instance. It also shows how to override the log level for individual components like producer, consumer, and admin.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Configuration.md#2025-04-14_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Kafka, logLevel } = require('kafkajs')\n\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  logLevel: logLevel.ERROR\n})\nkafka.logger().setLogLevel(logLevel.WARN)\n\nconst producer = kafka.producer(...)\nproducer.logger().setLogLevel(logLevel.INFO)\n\nconst consumer = kafka.consumer(...)\nconsumer.logger().setLogLevel(logLevel.DEBUG)\n\nconst admin = kafka.admin(...)\nadmin.logger().setLogLevel(logLevel.NOTHING)\n```\n\n----------------------------------------\n\nTITLE: Topic Subscription Configuration in KafkaJS\nDESCRIPTION: Examples of subscribing to Kafka topics with fromBeginning configuration to control initial offset behavior when starting consumption.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Consuming.md#2025-04-14_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nawait consumer.subscribe({ topic: 'test-topic', fromBeginning: true })\nawait consumer.subscribe({ topic: 'other-topic', fromBeginning: false })\n```\n\n----------------------------------------\n\nTITLE: Altering Kafka Configurations using KafkaJS Admin API\nDESCRIPTION: Updates configuration for specified Kafka resources using the admin.alterConfigs method. Supports different resource types and config entries with validation options.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Admin.md#2025-04-14_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.alterConfigs({\n    validateOnly: false,\n    resources: <ResourceConfig[]>\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    type: <ResourceType>,\n    name: <String>,\n    configEntries: <ResourceConfigEntry[]>\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    name: <String>,\n    value: <String>\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst { ResourceTypes } = require('kafkajs')\n\nawait admin.alterConfigs({\n    resources: [{\n        type: ResourceTypes.TOPIC,\n        name: 'topic-name',\n        configEntries: [{ name: 'cleanup.policy', value: 'compact' }]\n    }]\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    resources: [{\n        errorCode: 0,\n        errorMessage: null,\n        resourceName: 'topic-name',\n        resourceType: 2,\n    }],\n    throttleTime: 0,\n}\n```\n\n----------------------------------------\n\nTITLE: Producing to Multiple Topics with KafkaJS\nDESCRIPTION: Demonstrates how to send messages to multiple topics simultaneously using the sendBatch method in KafkaJS producer.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Producing.md#2025-04-14_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst topicMessages = [\n  {\n    topic: 'topic-a',\n    messages: [{ key: 'key', value: 'hello topic-a' }],\n  },\n  {\n    topic: 'topic-b',\n    messages: [{ key: 'key', value: 'hello topic-b' }],\n  },\n  {\n    topic: 'topic-c',\n    messages: [\n      {\n        key: 'key',\n        value: 'hello topic-c',\n        headers: {\n          'correlation-id': '2bfb68bb-893a-423b-a7fa-7b568cad5b67',\n        },\n      }\n    ],\n  }\n]\nawait producer.sendBatch({ topicMessages })\n```\n\n----------------------------------------\n\nTITLE: Example of Altering Topic Configuration in KafkaJS\nDESCRIPTION: Complete example showing how to change a topic's cleanup policy to 'compact' using the alterConfigs method.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Admin.md#2025-04-14_snippet_23\n\nLANGUAGE: javascript\nCODE:\n```\nconst { ResourceTypes } = require('kafkajs')\n\nawait admin.alterConfigs({\n    resources: [{\n        type: ResourceTypes.TOPIC,\n        name: 'topic-name',\n        configEntries: [{ name: 'cleanup.policy', value: 'compact' }]\n    }]\n})\n```\n\n----------------------------------------\n\nTITLE: Initializing KafkaJS Admin Client\nDESCRIPTION: Basic setup and connection of the KafkaJS admin client instance. Shows how to create, connect and disconnect the admin client.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/Admin.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst kafka = new Kafka(...)\nconst admin = kafka.admin()\n\n// remember to connect and disconnect when you are done\nawait admin.connect()\nawait admin.disconnect()\n```\n\n----------------------------------------\n\nTITLE: Configuring Dynamic Broker Discovery in KafkaJS\nDESCRIPTION: Demonstrates how to set up dynamic broker discovery using an async function that resolves to a broker array, useful for fetching seed brokers from external sources like Confluent REST Proxy.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Configuration.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: async () => {\n    // Example getting brokers from Confluent REST Proxy\n    const clusterResponse = await fetch('https://kafka-rest:8082/v3/clusters', {\n      headers: 'application/vnd.api+json',\n    }).then(response => response.json())\n    const clusterUrl = clusterResponse.data[0].links.self\n\n    const brokersResponse = await fetch(`${clusterUrl}/brokers`, {\n      headers: 'application/vnd.api+json',\n    }).then(response => response.json())\n\n    const brokers = brokersResponse.data.map(broker => {\n      const { host, port } = broker.attributes\n      return `${host}:${port}`\n    })\n\n    return brokers\n  }\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka and Zookeeper with Docker Compose\nDESCRIPTION: This Docker Compose configuration sets up a Kafka broker and Zookeeper for development purposes. It exposes necessary ports and configures environment variables for Kafka.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/DockerLocal.md#2025-04-14_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nversion: '2'\nservices:\n  zookeeper:\n    image: wurstmeister/zookeeper:latest\n    ports:\n      - \"2181:2181\"\n  kafka:\n    image: wurstmeister/kafka:2.11-1.1.1\n    ports:\n      - \"9092:9092\"\n    links:\n      - zookeeper\n    environment:\n      KAFKA_ADVERTISED_HOST_NAME: ${HOST_IP}\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'\n      KAFKA_DELETE_TOPIC_ENABLE: 'true'\n      KAFKA_CREATE_TOPICS: \"topic-test:1:1\"\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n```\n\n----------------------------------------\n\nTITLE: Fetching Consumer Group Offsets with KafkaJS Admin Client\nDESCRIPTION: Shows how to fetch consumer group offsets for a topic using the admin client's fetchOffsets method.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Admin.md#2025-04-14_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.fetchOffsets({ groupId, topic })\n// [\n//   { partition: 0, offset: '31004' },\n//   { partition: 1, offset: '54312' },\n//   { partition: 2, offset: '32103' },\n//   { partition: 3, offset: '28' },\n// ]\n```\n\n----------------------------------------\n\nTITLE: Resetting Consumer Group Offsets in KafkaJS\nDESCRIPTION: Method to reset consumer group offsets to either the latest (default) or earliest offset. The consumer group must have no running instances when performing the reset.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Admin.md#2025-04-14_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.resetOffsets({ groupId, topic }) // latest by default\n// await admin.resetOffsets({ groupId, topic, earliest: true })\n```\n\n----------------------------------------\n\nTITLE: Configuring Retry Mechanism for KafkaJS Client\nDESCRIPTION: Customizes the exponential backoff retry mechanism used for connections and API calls. This example sets a shorter initial retry time and increases the maximum number of retry attempts.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Configuration.md#2025-04-14_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  retry: {\n    initialRetryTime: 100,\n    retries: 8\n  }\n})\n```\n\n----------------------------------------\n\nTITLE: Fetching Consumer Group Offsets with KafkaJS Admin Client\nDESCRIPTION: Demonstrates how to fetch consumer group offsets for specified topics using the fetchOffsets method. Includes options for resolving offsets without starting a consumer.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Admin.md#2025-04-14_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.fetchOffsets({ groupId, topics: ['topic1', 'topic2'] })\n// [\n//   {\n//     topic: 'topic1',\n//     partitions: [\n//       { partition: 0, offset: '31004' },\n//       { partition: 1, offset: '54312' },\n//       { partition: 2, offset: '32103' },\n//       { partition: 3, offset: '28' },\n//     ],\n//   },\n//   {\n//     topic: 'topic2',\n//     partitions: [\n//       { partition: 0, offset: '1234' },\n//       { partition: 1, offset: '4567' },\n//     ],\n//   },\n// ]\n```\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.resetOffsets({ groupId, topic })\nawait admin.fetchOffsets({ groupId, topics: [topic], resolveOffsets: false })\n// [\n//   { partition: 0, offset: '-1' },\n//   { partition: 1, offset: '-1' },\n//   { partition: 2, offset: '-1' },\n//   { partition: 3, offset: '-1' },\n// ]\n\nawait admin.resetOffsets({ groupId, topic })\nawait admin.fetchOffsets({ groupId, topics: [topic], resolveOffsets: true })\n// [\n//   { partition: 0, offset: '31004' },\n//   { partition: 1, offset: '54312' },\n//   { partition: 2, offset: '32103' },\n//   { partition: 3, offset: '28' },\n// ]\n```\n\n----------------------------------------\n\nTITLE: Configuring AutoCommit Threshold in KafkaJS Consumer\nDESCRIPTION: Shows how to configure automatic committing of offsets after processing a specified number of messages using the autoCommitThreshold option.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Consuming.md#2025-04-14_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconsumer.run({\n  autoCommitThreshold: 100,\n  // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring JavaCompatiblePartitioner in KafkaJS\nDESCRIPTION: This snippet demonstrates how to use the JavaCompatiblePartitioner in KafkaJS. This partitioner is compatible with the default partitioner in the Java Kafka client, which can be important for meeting co-partitioning requirements when joining multiple topics.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/Producing.md#2025-04-14_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Partitioners } = require('kafkajs')\nkafka.producer({ createPartitioner: Partitioners.JavaCompatiblePartitioner })\n```\n\n----------------------------------------\n\nTITLE: Describing Kafka Configs with KafkaJS Admin Client\nDESCRIPTION: Illustrates how to retrieve configuration for specified resources using the describeConfigs method, including examples for fetching all configs and specific configs for a topic.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Admin.md#2025-04-14_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.describeConfigs({\n  includeSynonyms: <boolean>,\n  resources: <ResourceConfigQuery[]>\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    type: <ResourceType>,\n    name: <String>,\n    configNames: <String[]>\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst { ResourceTypes } = require('kafkajs')\n\nawait admin.describeConfigs({\n  includeSynonyms: false,\n  resources: [\n    {\n      type: ResourceTypes.TOPIC,\n      name: 'topic-name'\n    }\n  ]\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst { ResourceTypes } = require('kafkajs')\n\nawait admin.describeConfigs({\n  includeSynonyms: false,\n  resources: [\n    {\n      type: ResourceTypes.TOPIC,\n      name: 'topic-name',\n      configNames: ['cleanup.policy']\n    }\n  ]\n})\n```\n\n----------------------------------------\n\nTITLE: Winston Logger Integration with KafkaJS\nDESCRIPTION: Example implementation of a custom logger using Winston, including log level mapping and configuration of console and file transports.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.4/CustomLogger.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst { logLevel } = require('kafkajs')\nconst winston = require('winston')\nconst toWinstonLogLevel = level => {\n    switch(level) {\n        case logLevel.ERROR:\n        case logLevel.NOTHING:\n            return 'error'\n        case logLevel.WARN:\n            return 'warn'\n        case logLevel.INFO:\n            return 'info'\n        case logLevel.DEBUG:\n            return 'debug'\n    }\n}\n\nconst WinstonLogCreator = logLevel => {\n    const logger = winston.createLogger({\n        level: toWinstonLogLevel(logLevel),\n        transports: [\n            new winston.transports.Console(),\n            new winston.transports.File({ filename: 'myapp.log' })\n        ]\n    })\n\n    return ({ namespace, level, label, log }) => {\n        const { message, ...extra } = log\n        logger.log({\n            level: toWinstonLogLevel(level),\n            message,\n            extra,\n        })\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Deleting Topics with KafkaJS Admin Client\nDESCRIPTION: Example showing how to delete Kafka topics. Topic deletion is disabled by default in Apache Kafka versions prior to 1.0.0 and needs to be enabled in server configuration.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Admin.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.deleteTopics({\n    topics: <String[]>,\n    timeout: <Number>,\n})\n```\n\n----------------------------------------\n\nTITLE: Producing to Multiple Kafka Topics with KafkaJS\nDESCRIPTION: Shows how to send messages to multiple Kafka topics in a single batch operation using the sendBatch method of KafkaJS producer.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/Producing.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst topicMessages = [\n  {\n    topic: 'topic-a',\n    messages: [{ key: 'key', value: 'hello topic-a' }],\n  },\n  {\n    topic: 'topic-b',\n    messages: [{ key: 'key', value: 'hello topic-b' }],\n  },\n  {\n    topic: 'topic-c',\n    messages: [\n      {\n        key: 'key',\n        value: 'hello topic-c',\n        headers: {\n          'correlation-id': '2bfb68bb-893a-423b-a7fa-7b568cad5b67',\n        },\n      }\n    ],\n  }\n]\nawait producer.sendBatch({ topicMessages })\n```\n\n----------------------------------------\n\nTITLE: Configuring SASL Authentication with PLAIN/SCRAM Mechanism\nDESCRIPTION: Sets up SASL authentication for KafkaJS using PLAIN or SCRAM-SHA mechanisms. Requires username and password for authentication.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Configuration.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  // authenticationTimeout: 1000,\n  sasl: {\n    mechanism: 'plain', // scram-sha-256 or scram-sha-512\n    username: 'my-username',\n    password: 'my-password'\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Creating Topics with KafkaJS Admin Client\nDESCRIPTION: Shows how to create topics using the admin client's createTopics method. Includes the structure of the ITopicConfig object and available options.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Admin.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.createTopics({\n    validateOnly: <boolean>,\n    waitForLeaders: <boolean>\n    timeout: <Number>,\n    topics: <ITopicConfig[]>,\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    topic: <String>,\n    numPartitions: <Number>,     // default: 1\n    replicationFactor: <Number>, // default: 1\n    replicaAssignment: <Array>,  // Example: [{ partition: 0, replicas: [0,1,2] }] - default: []\n    configEntries: <Array>       // Example: [{ name: 'cleanup.policy', value: 'compact' }] - default: []\n}\n```\n\n----------------------------------------\n\nTITLE: Altering Kafka Configurations\nDESCRIPTION: Updates configuration for specified Kafka resources using admin.alterConfigs(). Allows modification of resource settings like topic cleanup policies.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/Admin.md#2025-04-14_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.alterConfigs({\n    validateOnly: false,\n    resources: <ResourceConfig[]>\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst { ConfigResourceTypes } = require('kafkajs')\n\nawait admin.alterConfigs({\n    resources: [{\n        type: ConfigResourceTypes.TOPIC,\n        name: 'topic-name',\n        configEntries: [{ name: 'cleanup.policy', value: 'compact' }]\n    }]\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS IAM SASL Authentication for KafkaJS\nDESCRIPTION: Sets up SASL authentication using AWS IAM credentials. This requires the broker to have STACK's Kafka AWS IAM LoginModule or a compatible alternative installed. Requires providing the AWS authorization identity, access key ID, and secret access key.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Configuration.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  // authenticationTimeout: 1000,\n  // reauthenticationThreshold: 10000,\n  sasl: {\n    mechanism: 'aws',\n    authorizationIdentity: 'AIDAIOSFODNN7EXAMPLE', // UserId or RoleId\n    accessKeyId: 'AKIAIOSFODNN7EXAMPLE',\n    secretAccessKey: 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',\n    sessionToken: 'WHArYt8i5vfQUrIU5ZbMLCbjcAiv/Eww6eL9tgQMJp6QFNEXAMPLETOKEN' // Optional\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Auto-Commit Configuration Examples\nDESCRIPTION: Examples of configuring automatic offset commits using intervals and thresholds.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Consuming.md#2025-04-14_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconsumer.run({\n  autoCommitInterval: 5000,\n  // ...\n})\n\nconsumer.run({\n  autoCommitThreshold: 100,\n  // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Producing to Multiple Topics with KafkaJS\nDESCRIPTION: Shows how to use the sendBatch method to produce messages to multiple topics simultaneously. This can be useful for scenarios like topic migration.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Producing.md#2025-04-14_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst topicMessages = [\n  {\n    topic: 'topic-a',\n    messages: [{ key: 'key', value: 'hello topic-a' }],\n  },\n  {\n    topic: 'topic-b',\n    messages: [{ key: 'key', value: 'hello topic-b' }],\n  },\n  {\n    topic: 'topic-c',\n    messages: [\n      {\n        key: 'key',\n        value: 'hello topic-c',\n        headers: {\n          'correlation-id': '2bfb68bb-893a-423b-a7fa-7b568cad5b67',\n        },\n      }\n    ],\n  }\n]\nawait producer.sendBatch({ topicMessages })\n```\n\n----------------------------------------\n\nTITLE: Configuring KafkaJS Client with Custom Logger\nDESCRIPTION: Shows how to initialize a Kafka client with a custom logger configuration.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.4/CustomLogger.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst kafka = new Kafka({\n    clientId: 'my-app',\n    brokers: ['kafka1:9092', 'kafka2:9092'],\n    logLevel: logLevel.ERROR,\n    logCreator: WinstonLogCreator\n})\n```\n\n----------------------------------------\n\nTITLE: Custom Partitioner Implementation\nDESCRIPTION: Example of implementing a custom partitioner for message distribution control.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Producing.md#2025-04-14_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst MyPartitioner = () => {\n    return ({ topic, partitionMetadata, message }) => {\n        // select a partition based on some logic\n        // return the partition number\n        return 0\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Kafka Producer with Custom Options in JavaScript\nDESCRIPTION: Initializes a Kafka producer with specific configuration options, including disabling automatic topic creation and setting transaction timeout to 30 seconds.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Producing.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer({\n    allowAutoTopicCreation: false,\n    transactionTimeout: 30000\n})\n```\n\n----------------------------------------\n\nTITLE: Fetching Kafka Topic Metadata with KafkaJS Admin Client\nDESCRIPTION: Demonstrates how to fetch metadata for specific topics or all topics using the fetchTopicMetadata method.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Admin.md#2025-04-14_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.fetchTopicMetadata({ topics: <Array<String>> })\n```\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.fetchTopicMetadata()\n```\n\n----------------------------------------\n\nTITLE: Setting Consumer Group Offsets in KafkaJS\nDESCRIPTION: Method to manually set consumer group offsets to specific values for specified partitions of a topic.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Admin.md#2025-04-14_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.setOffsets({\n    groupId: <String>,\n    topic: <String>,\n    partitions: <SeekEntry[]>,\n})\n```\n\n----------------------------------------\n\nTITLE: Initializing a KafkaJS Client with Basic Configuration\nDESCRIPTION: Creates a Kafka client instance with minimal configuration by providing a client ID and broker list. The brokers are used as seed brokers to bootstrap the client and load initial metadata.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Configuration.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Kafka } = require('kafkajs')\n\n// Create the client with the broker list\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092']\n})\n```\n\n----------------------------------------\n\nTITLE: AWS IAM Authentication Configuration\nDESCRIPTION: Setup for AWS IAM authentication with access keys and optional session token. Required for using AWS IAM credentials with Kafka clusters.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.14.0/Configuration.md#2025-04-14_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  // authenticationTimeout: 1000,\n  // reauthenticationThreshold: 10000,\n  ssl: true,\n  sasl: {\n    mechanism: 'aws',\n    authorizationIdentity: 'AIDAIOSFODNN7EXAMPLE', // UserId or RoleId\n    accessKeyId: 'AKIAIOSFODNN7EXAMPLE',\n    secretAccessKey: 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',\n    sessionToken: 'WHArYt8i5vfQUrIU5ZbMLCbjcAiv/Eww6eL9tgQMJp6QFNEXAMPLETOKEN' // Optional\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Resetting Consumer Group Offsets with KafkaJS Admin Client\nDESCRIPTION: Demonstrates how to reset consumer group offsets to the earliest or latest offset using the admin client's resetOffsets method.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Admin.md#2025-04-14_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.resetOffsets({ groupId, topic }) // latest by default\n// await admin.resetOffsets({ groupId, topic, earliest: true })\n```\n\n----------------------------------------\n\nTITLE: SSL Configuration Setup\nDESCRIPTION: Configuration of SSL/TLS security settings using custom certificates and keys. Shows how to set up secure connections with custom CA certificates and client certificates.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.14.0/Configuration.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst fs = require('fs')\n\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  ssl: {\n    rejectUnauthorized: false,\n    ca: [fs.readFileSync('/my/custom/ca.crt', 'utf-8')],\n    key: fs.readFileSync('/my/custom/client-key.pem', 'utf-8'),\n    cert: fs.readFileSync('/my/custom/client-cert.pem', 'utf-8')\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Describing Kafka Groups with KafkaJS Admin API\nDESCRIPTION: This snippet shows how to use the describeGroups method to get detailed information about consumer groups by their groupIds. It also includes information on decoding memberMetadata and memberAssignment.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Admin.md#2025-04-14_snippet_16\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.describeGroups([ 'testgroup' ])\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Log Creator in KafkaJS\nDESCRIPTION: Demonstrates the basic structure of a custom log creator function that receives a log level and returns a log function to handle the actual logging.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/CustomLogger.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst MyLogCreator = logLevel => ({ namespace, level, label, log }) => {\n    // Example:\n    // const { timestamp, logger, message, ...others } = log\n    // console.log(`${label} [${namespace}] ${message} ${JSON.stringify(others)}`)\n}\n```\n\n----------------------------------------\n\nTITLE: Sending Messages with Headers in KafkaJS\nDESCRIPTION: Demonstrates how to send a message with headers using KafkaJS producer. Headers allow messages to carry extra metadata.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.4/Producing.md#2025-04-14_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nawait producer.send({\n    topic: 'topic-name',\n    messages: [{\n        key: 'key1',\n        value: 'hello world',\n        headers: {\n            'correlation-id': '2bfb68bb-893a-423b-a7fa-7b568cad5b67',\n            'system-id': 'my-system',\n        }\n    }]\n})\n```\n\n----------------------------------------\n\nTITLE: Log Entry Structure Example in JavaScript\nDESCRIPTION: Shows the structure of a log entry object in KafkaJS, including properties like level, label, timestamp, logger name, message, and other custom keys.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/CustomLogger.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    level: 4,\n    label: 'INFO', // NOTHING, ERROR, WARN, INFO, or DEBUG\n    timestamp: '2017-12-29T13:39:54.575Z',\n    logger: 'kafkajs',\n    message: 'Started',\n    // ... any other extra key provided to the log function\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring SSL for KafkaJS Client\nDESCRIPTION: Sets up SSL/TLS configuration for secure connections with the Kafka brokers. This example shows how to provide custom certificates and keys. The options are passed directly to Node.js tls.connect.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Configuration.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst fs = require('fs')\n\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  ssl: {\n    rejectUnauthorized: false,\n    ca: [fs.readFileSync('/my/custom/ca.crt', 'utf-8')],\n    key: fs.readFileSync('/my/custom/client-key.pem', 'utf-8'),\n    cert: fs.readFileSync('/my/custom/client-cert.pem', 'utf-8')\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring SSL/TLS Security in KafkaJS\nDESCRIPTION: Demonstrates SSL configuration for KafkaJS client using custom certificates and keys. Options are passed to Node.js tls.connect.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Configuration.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst fs = require('fs')\n\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  ssl: {\n    rejectUnauthorized: false,\n    ca: [fs.readFileSync('/my/custom/ca.crt', 'utf-8')],\n    key: fs.readFileSync('/my/custom/client-key.pem', 'utf-8'),\n    cert: fs.readFileSync('/my/custom/client-cert.pem', 'utf-8')\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring SSL for KafkaJS Client\nDESCRIPTION: Sets up SSL/TLS secure connections for the KafkaJS client by providing certificates and keys. The options are passed directly to Node.js's tls.connect method.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Configuration.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst fs = require('fs')\n\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  ssl: {\n    rejectUnauthorized: false,\n    ca: [fs.readFileSync('/my/custom/ca.crt', 'utf-8')],\n    key: fs.readFileSync('/my/custom/client-key.pem', 'utf-8'),\n    cert: fs.readFileSync('/my/custom/client-cert.pem', 'utf-8')\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Log Creator for KafkaJS\nDESCRIPTION: This snippet shows the general structure of a custom log creator function for KafkaJS. The function takes a log level and returns a log handler function that processes log entries.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/CustomLogger.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst MyLogCreator = logLevel => ({ namespace, level, label, log }) => {\n    // Example:\n    // const { timestamp, logger, message, ...others } = log\n    // console.log(`${label} [${namespace}] ${message} ${JSON.stringify(others)}`)\n}\n```\n\n----------------------------------------\n\nTITLE: Sending Messages to Specific Partitions in Kafka using JavaScript\nDESCRIPTION: Demonstrates sending messages to explicitly defined partitions in a Kafka topic, allowing precise control over message placement.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Producing.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n\nawait producer.connect()\nawait producer.send({\n    topic: 'topic-name',\n    messages: [\n        { key: 'key1', value: 'hello world', partition: 0 },\n        { key: 'key2', value: 'hey hey!', partition: 1 }\n    ],\n})\n```\n\n----------------------------------------\n\nTITLE: Fetching Topic Offsets with KafkaJS Admin Client\nDESCRIPTION: Demonstrates how to fetch the most recent offsets for a topic using the fetchTopicOffsets method.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Admin.md#2025-04-14_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.fetchTopicOffsets(topic)\n// [\n//   { partition: 0, offset: '31004', high: '31004', low: '421' },\n//   { partition: 1, offset: '54312', high: '54312', low: '3102' },\n//   { partition: 2, offset: '32103', high: '32103', low: '518' },\n//   { partition: 3, offset: '28', high: '28', low: '0' },\n// ]\n```\n\n----------------------------------------\n\nTITLE: Creating Kafka Topics with KafkaJS Admin Client\nDESCRIPTION: Demonstrates how to create new topics using the createTopics method, including options for validation, timeout, and topic configuration.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Admin.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.createTopics({\n    validateOnly: <boolean>,\n    waitForLeaders: <boolean>\n    timeout: <Number>,\n    topics: <ITopicConfig[]>,\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    topic: <String>,\n    numPartitions: <Number>,     // default: 1\n    replicationFactor: <Number>, // default: 1\n    replicaAssignment: <Array>,  // Example: [{ partition: 0, replicas: [0,1,2] }] - default: []\n    configEntries: <Array>       // Example: [{ name: 'cleanup.policy', value: 'compact' }] - default: []\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Winston Logger for KafkaJS\nDESCRIPTION: This example demonstrates how to implement a custom logger for KafkaJS using the Winston logging library. It includes a function to convert KafkaJS log levels to Winston log levels and a log creator that configures Winston.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/CustomLogger.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst { logLevel } = require('kafkajs')\nconst winston = require('winston')\nconst toWinstonLogLevel = level => switch(level) {\n    case logLevel.ERROR:\n    case logLevel.NOTHING:\n        return 'error'\n    case logLevel.WARN:\n        return 'warn'\n    case logLevel.INFO:\n        return 'info'\n    case logLevel.DEBUG:\n        return 'debug'\n}\n\nconst WinstonLogCreator = logLevel => {\n    const logger = winston.createLogger({\n        level: toWinstonLogLevel(logLevel),\n        transports: [\n            new winston.transports.Console(),\n            new winston.transports.File({ filename: 'myapp.log' })\n        ]\n    })\n\n    return ({ namespace, level, label, log }) => {\n        const { message, ...extra } = log\n        logger.log({\n            level: toWinstonLogLevel(level),\n            message,\n            extra,\n        })\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Sending Messages with Custom Compression in KafkaJS\nDESCRIPTION: This snippet shows how to send messages using a custom compression codec in KafkaJS. It uses the producer's send method with the compression option set to the custom codec.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Producing.md#2025-04-14_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nawait producer.send({\n    topic: 'topic-name',\n    compression: CompressionTypes.Snappy,\n    messages: [\n        { key: 'key1', value: 'hello world' },\n        { key: 'key2', value: 'hey hey!' }\n    ],\n})\n```\n\n----------------------------------------\n\nTITLE: OAUTHBEARER Authentication Setup\nDESCRIPTION: Implementation of OAUTHBEARER authentication mechanism with a custom token provider function. Demonstrates how to integrate OAuth authentication with KafkaJS.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.14.0/Configuration.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  // authenticationTimeout: 1000,\n  // reauthenticationThreshold: 10000,\n  ssl: true,\n  sasl: {\n    mechanism: 'oauthbearer',\n    oauthBearerProvider: async () => {\n      // Use an unsecured token...\n      const token = jwt.sign({ sub: 'test' }, 'abc', { algorithm: 'none' })\n\n      // ...or, more realistically, grab the token from some OAuth endpoint\n\n      return {\n        value: token\n      }\n    }\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Deleting Kafka Topics\nDESCRIPTION: Deletes specified topics from the Kafka cluster. Requires delete.topic.enable=true for Kafka versions prior to 1.0.0.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/Admin.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.deleteTopics({\n    topics: <String[]>,\n    timeout: <Number>,\n})\n```\n\n----------------------------------------\n\nTITLE: KafkaJS Client Configuration\nDESCRIPTION: JavaScript code showing how to configure and initialize a KafkaJS client with SSL and SASL authentication\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/DevelopmentEnvironment.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst fs = require('fs')\nconst ip = require('ip')\n\nconst { Kafka, CompressionTypes, logLevel } = require('./index')\n\nconst host = process.env.HOST_IP || ip.address()\n\nconst kafka = new Kafka({\n  logLevel: logLevel.DEBUG,\n  brokers: [`${host}:9094`, `${host}:9097`, `${host}:9100`],\n  clientId: 'example-producer',\n  ssl: {\n    servername: 'localhost',\n    rejectUnauthorized: false,\n    ca: [fs.readFileSync('./testHelpers/certs/cert-signed', 'utf-8')],\n  },\n  sasl: {\n    mechanism: 'plain',\n    username: 'test',\n    password: 'testtest',\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Fetching Kafka Topic Offsets by Timestamp with KafkaJS Admin Client\nDESCRIPTION: Illustrates how to fetch topic offsets based on a specified timestamp using the fetchTopicOffsetsByTimestamp method.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Admin.md#2025-04-14_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.fetchTopicOffsetsByTimestamp(topic, timestamp)\n// [\n//   { partition: 0, offset: '3244' },\n//   { partition: 1, offset: '3113' },\n// ]\n```\n\n----------------------------------------\n\nTITLE: VS Code JavaScript Configuration\nDESCRIPTION: JSConfig configuration for better Visual Studio Code integration with type hints and module resolution\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/DevelopmentEnvironment.md#2025-04-14_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"compilerOptions\": {\n    \"baseUrl\": \".\",\n    \"module\": \"commonjs\",\n    \"target\": \"es6\",\n    \"paths\": {\n      \"testHelpers\": [\"./testHelpers\"]\n    }\n  },\n  \"include\": [\n    \"src\",\n    \"testHelpers\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring PLAIN/SCRAM SASL Authentication for KafkaJS\nDESCRIPTION: Implements SASL authentication using PLAIN or SCRAM-SHA mechanisms. This configuration requires providing a username and password, with optional timeout settings for authentication requests.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Configuration.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  // authenticationTimeout: 1000,\n  // reauthenticationThreshold: 10000,\n  sasl: {\n    mechanism: 'plain', // scram-sha-256 or scram-sha-512\n    username: 'my-username',\n    password: 'my-password'\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Example of Setting Consumer Group Offsets in KafkaJS\nDESCRIPTION: Complete example showing how to set consumer group offsets for specific partitions of a topic to custom offset values.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Admin.md#2025-04-14_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.setOffsets({\n    groupId: 'my-consumer-group',\n    topic: 'custom-topic',\n    partitions: [\n        { partition: 0, offset: '35' },\n        { partition: 3, offset: '19' },\n    ]\n})\n```\n\n----------------------------------------\n\nTITLE: Fetching Consumer Group Offsets with KafkaJS Admin Client\nDESCRIPTION: Demonstrates how to retrieve consumer group offsets for a specific topic using the fetchOffsets method.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Admin.md#2025-04-14_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.fetchOffsets({ groupId, topic })\n// [\n//   { partition: 0, offset: '31004' },\n//   { partition: 1, offset: '54312' },\n//   { partition: 2, offset: '32103' },\n//   { partition: 3, offset: '28' },\n// ]\n```\n\n----------------------------------------\n\nTITLE: Fetching Topic Offsets in KafkaJS\nDESCRIPTION: Method to retrieve the most recent offset for each partition of a specified topic, returning partition information with high, low, and current offset values.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Admin.md#2025-04-14_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.fetchTopicOffsets(topic)\n// [\n//   { partition: 0, offset: '31004', high: '31004', low: '421' },\n//   { partition: 1, offset: '54312', high: '54312', low: '3102' },\n//   { partition: 2, offset: '32103', high: '32103', low: '518' },\n//   { partition: 3, offset: '28', high: '28', low: '0' },\n// ]\n```\n\n----------------------------------------\n\nTITLE: Fetching Topic Offsets by Timestamp with KafkaJS Admin Client\nDESCRIPTION: Shows how to fetch topic offsets by timestamp using the fetchTopicOffsetsByTimestamp method. Returns the earliest offset on each partition where the message's timestamp is greater than or equal to the given timestamp.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Admin.md#2025-04-14_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.fetchTopicOffsetsByTimestamp(topic, timestamp)\n// [\n//   { partition: 0, offset: '3244' },\n//   { partition: 1, offset: '3113' },\n// ]\n```\n\n----------------------------------------\n\nTITLE: Managing Access Control Lists (ACLs)\nDESCRIPTION: Methods for creating, deleting, and describing ACLs to manage access permissions for Kafka resources.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/Admin.md#2025-04-14_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst acl = [\n  {\n    resourceType: AclResourceTypes.TOPIC,\n    resourceName: 'topic-name',\n    resourcePatternType: ResourcePatternTypes.LITERAL,\n    principal: 'User:bob',\n    host: '*',\n    operation: AclOperationTypes.ALL,\n    permissionType: AclPermissionTypes.DENY,\n  }\n]\n\nawait admin.createAcls({ acl })\n```\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.deleteAcls({ filters: [acl] })\n```\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.describeAcls({\n  resourceName: 'topic-name',\n  resourceType: AclResourceTypes.TOPIC,\n  host: '*',\n  permissionType: AclPermissionTypes.ALLOW,\n  operation: AclOperationTypes.ANY,\n  resourcePatternTypeFilter: ResourcePatternTypes.LITERAL,\n})\n```\n\n----------------------------------------\n\nTITLE: Sending Compressed Messages with Custom Codec in KafkaJS Producer\nDESCRIPTION: This code shows how to send messages using a custom compression codec with the KafkaJS producer. It uses the CompressionTypes.Snappy option when calling the send method.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Producing.md#2025-04-14_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nawait producer.send({\n    topic: 'topic-name',\n    compression: CompressionTypes.Snappy,\n    messages: [\n        { key: 'key1', value: 'hello world' },\n        { key: 'key2', value: 'hey hey!' }\n    ],\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Socket Factory for KafkaJS\nDESCRIPTION: Demonstrates how to create and configure a custom socket factory for KafkaJS with support for SSL connections and custom keep-alive settings.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.14.0/Configuration.md#2025-04-14_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Kafka } = require('kafkajs')\n\n// Example socket factory setting a custom TTL\nconst net = require('net')\nconst tls = require('tls')\n\nconst myCustomSocketFactory = ({ host, port, ssl, onConnect }) => {\n  const socket = ssl\n    ? tls.connect(\n        Object.assign({ host, port }, ssl),\n        onConnect\n      )\n    : net.connect(\n        { host, port },\n        onConnect\n      )\n\n  socket.setKeepAlive(true, 30000)\n\n  return socket\n}\n\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  socketFactory: myCustomSocketFactory,\n})\n```\n\n----------------------------------------\n\nTITLE: Creating Kafka Partitions\nDESCRIPTION: Creates additional partitions for existing topics with optional broker assignments.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/Admin.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.createPartitions({\n    validateOnly: <boolean>,\n    timeout: <Number>,\n    topicPartitions: <TopicPartition[]>,\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    topic: <String>,\n    count: <Number>,     // partition count\n    assignments: <Array<Array<Number>>> // Example: [[0,1],[1,2],[2,0]]\n}\n```\n\n----------------------------------------\n\nTITLE: Docker Compose Status Output\nDESCRIPTION: Example output of docker-compose ps command showing the running Kafka and Zookeeper containers\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/DevelopmentEnvironment.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n$ docker-compose -f docker-compose.2_3.yml ps\nWARNING: The HOST_IP variable is not set. Defaulting to a blank string.\n      Name                    Command               State                                   Ports\n----------------------------------------------------------------------------------------------------------------------------------\nkafkajs_kafka1_1   start-kafka.sh                   Up      0.0.0.0:9092->9092/tcp, 0.0.0.0:9093->9093/tcp, 0.0.0.0:9094->9094/tcp\nkafkajs_kafka2_1   start-kafka.sh                   Up      0.0.0.0:9095->9095/tcp, 0.0.0.0:9096->9096/tcp, 0.0.0.0:9097->9097/tcp\nkafkajs_kafka3_1   start-kafka.sh                   Up      0.0.0.0:9098->9098/tcp, 0.0.0.0:9099->9099/tcp, 0.0.0.0:9100->9100/tcp\nkafkajs_zk_1       /bin/sh -c /usr/sbin/sshd  ...   Up      0.0.0.0:2181->2181/tcp, 22/tcp, 2888/tcp, 3888/tcp\n```\n\n----------------------------------------\n\nTITLE: Describing Kafka Groups using KafkaJS Admin API\nDESCRIPTION: Retrieves detailed information about specific consumer groups by their groupIds, including member information and group state.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Admin.md#2025-04-14_snippet_16\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.describeGroups([ 'testgroup' ])\n```\n\n----------------------------------------\n\nTITLE: Running KafkaJS Tests in Docker Environment\nDESCRIPTION: Commands to set up the development environment and run tests for KafkaJS using Docker and yarn. This includes starting the Docker environment, creating SCRAM credentials, and running tests locally with watch mode.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/ContributionGuide.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nyarn test\n# or\n./scripts/dockerComposeUp.sh\n./scripts/createScramCredentials.sh\nyarn test:local:watch\n```\n\n----------------------------------------\n\nTITLE: Seeking to Specific Offset in KafkaJS Consumer\nDESCRIPTION: Shows how to use the 'seek' method to move the offset position in a topic/partition. This method must be called after the consumer is initialized and running.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Consuming.md#2025-04-14_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\nawait consumer.connect()\nawait consumer.subscribe({ topic: 'example' })\n\n// you don't need to await consumer#run\nconsumer.run({ eachMessage: async ({ topic, message }) => true })\nconsumer.seek({ topic: 'example', partition: 0, offset: 12384 })\n```\n\n----------------------------------------\n\nTITLE: Fetching All Configs for a Topic in KafkaJS\nDESCRIPTION: Example showing how to retrieve all configuration settings for a specific Kafka topic using the ResourceTypes.TOPIC type.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Admin.md#2025-04-14_snippet_17\n\nLANGUAGE: javascript\nCODE:\n```\nconst { ResourceTypes } = require('kafkajs')\n\nawait admin.describeConfigs({\n  includeSynonyms: false,\n  resources: [\n    {\n      type: ResourceTypes.TOPIC,\n      name: 'topic-name'\n    }\n  ]\n})\n```\n\n----------------------------------------\n\nTITLE: Fetching Kafka Topic Offsets with KafkaJS Admin Client\nDESCRIPTION: Shows how to retrieve the most recent offsets for a topic using the fetchTopicOffsets method.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Admin.md#2025-04-14_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.fetchTopicOffsets(topic)\n// [\n//   { partition: 0, offset: '31004', high: '31004', low: '421' },\n//   { partition: 1, offset: '54312', high: '54312', low: '3102' },\n//   { partition: 2, offset: '32103', high: '32103', low: '518' },\n//   { partition: 3, offset: '28', high: '28', low: '0' },\n// ]\n```\n\n----------------------------------------\n\nTITLE: Concurrent Partition Processing\nDESCRIPTION: Configuration for concurrent processing of messages from multiple partitions using partitionsConsumedConcurrently option.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Consuming.md#2025-04-14_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconsumer.run({\n    partitionsConsumedConcurrently: 3, // Default: 1\n    eachMessage: async ({ topic, partition, message }) => {\n        // This will be called up to 3 times concurrently\n    },\n})\n```\n\n----------------------------------------\n\nTITLE: Sending Offsets in a KafkaJS Transaction in JavaScript\nDESCRIPTION: This snippet illustrates how to send offsets as part of a KafkaJS transaction. This is useful in consume-transform-produce loops where offsets should only be committed if the transaction succeeds.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/Transactions.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nawait transaction.sendOffsets({\n  consumerGroupId, topics\n})\n```\n\n----------------------------------------\n\nTITLE: Managing Consumer Group Offsets\nDESCRIPTION: Functions for fetching, resetting, and setting consumer group offsets for topics.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Admin.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.setOffsets({\n    groupId: <String>,\n    topic: <String>,\n    partitions: <SeekEntry[]>,\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    partition: <Number>,\n    offset: <String>,\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring KafkaJS with a Custom Logger\nDESCRIPTION: Shows how to initialize a Kafka client with a custom log creator. The example sets the client ID, brokers, log level, and assigns the custom WinstonLogCreator.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/CustomLogger.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst kafka = new Kafka({\n    clientId: 'my-app',\n    brokers: ['kafka1:9092', 'kafka2:9092'],\n    logLevel: logLevel.ERROR,\n    logCreator: WinstonLogCreator\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing Proxy Support in KafkaJS\nDESCRIPTION: This code snippet shows how to implement proxy support in KafkaJS using a custom socket factory. It uses the proxy-chain library to create a tunnel for the connection.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Configuration.md#2025-04-14_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nconst tls = require('tls')\nconst net = require('net')\nconst { createTunnel, closeTunnel } = require('proxy-chain')\n\nconst socketFactory = ({ host, port, ssl, onConnect }) => {\n  const socket = ssl ? new tls.TLSSocket() : new net.Socket()\n\n  createTunnel(process.env.HTTP_PROXY, `${host}:${port}`)\n    .then((tunnelAddress) => {\n      const [tunnelHost, tunnelPort] = tunnelAddress.split(':')\n      socket.setKeepAlive(true, 60000)\n      socket.connect(\n        Object.assign({ host: tunnelHost, port: tunnelPort, servername: host }, ssl),\n        onConnect\n      )\n\n      socket.on('close', () => {\n        closeTunnel(tunnelServer, true)\n      })\n    })\n    .catch(error => socket.emit('error', error))\n\n  return socket\n}\n```\n\n----------------------------------------\n\nTITLE: Describing Cluster with KafkaJS Admin Client\nDESCRIPTION: Demonstrates how to get information about the broker cluster using the describeCluster method. This is useful for monitoring and operations purposes.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Admin.md#2025-04-14_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.describeCluster()\n// {\n//   brokers: [\n//     { nodeId: 0, host: 'localhost', port: 9092 }\n//   ],\n//   controller: 0,\n//   clusterId: 'f8QmWTB8SQSLE6C99G4qzA'\n// }\n```\n\n----------------------------------------\n\nTITLE: Installing KafkaJS with NPM\nDESCRIPTION: Command to install KafkaJS package using NPM package manager.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/GettingStarted.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install kafkajs\n```\n\n----------------------------------------\n\nTITLE: SASL PLAIN/SCRAM Authentication Configuration\nDESCRIPTION: Sets up SASL authentication using PLAIN or SCRAM mechanisms with username/password credentials and SSL enabled.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Configuration.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  // authenticationTimeout: 1000,\n  // reauthenticationThreshold: 10000,\n  ssl: true,\n  sasl: {\n    mechanism: 'plain', // scram-sha-256 or scram-sha-512\n    username: 'my-username',\n    password: 'my-password'\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Installing KafkaJS with Yarn\nDESCRIPTION: Command to install KafkaJS package using Yarn package manager.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/GettingStarted.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nyarn add kafkajs\n```\n\n----------------------------------------\n\nTITLE: Configuring fromBeginning Option for KafkaJS Consumer\nDESCRIPTION: Shows how to configure the fromBeginning option when subscribing to topics with the KafkaJS consumer. This option determines whether to start consuming from the earliest or latest offset when no valid offset is found.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Consuming.md#2025-04-14_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nawait consumer.subscribe({ topic: 'test-topic', fromBeginning: true })\nawait consumer.subscribe({ topic: 'other-topic', fromBeginning: false })\n```\n\n----------------------------------------\n\nTITLE: Sending Messages with Headers in KafkaJS\nDESCRIPTION: Illustrates how to include message headers when sending messages to Kafka using KafkaJS.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Producing.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nawait producer.send({\n    topic: 'topic-name',\n    messages: [{\n        key: 'key1',\n        value: 'hello world',\n        headers: {\n            'correlation-id': '2bfb68bb-893a-423b-a7fa-7b568cad5b67',\n            'system-id': 'my-system'\n        }\n    }]\n})\n```\n\n----------------------------------------\n\nTITLE: Sending Messages with Headers in KafkaJS\nDESCRIPTION: Shows how to include message headers when sending messages using the KafkaJS producer. Headers can carry extra metadata with each message.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Producing.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nawait producer.send({\n    topic: 'topic-name',\n    messages: [{\n        key: 'key1',\n        value: 'hello world',\n        headers: {\n            'correlation-id': '2bfb68bb-893a-423b-a7fa-7b568cad5b67',\n            'system-id': 'my-system'\n        }\n    }]\n})\n```\n\n----------------------------------------\n\nTITLE: Pausing and Resuming Specific Partitions in KafkaJS Consumer\nDESCRIPTION: Shows how to pause and resume specific partitions of a topic, allowing for finer-grained control over consumption. This is useful when processing messages concurrently across partitions.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Consuming.md#2025-04-14_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nconsumer.run({\n    partitionsConsumedConcurrently: 3, // Default: 1\n    eachMessage: async ({ topic, partition, message }) => {\n      // This will be called up to 3 times concurrently\n        try {\n            await sendToDependency(message)\n        } catch (e) {\n            if (e instanceof TooManyRequestsError) {\n                consumer.pause([{ topic, partitions: [partition] }])\n                // Other partitions will keep fetching and processing, until if / when\n                // they also get throttled\n                setTimeout(() => {\n                    consumer.resume([{ topic, partitions: [partition] }])\n                    // Other partitions that are paused will continue to be paused\n                }, e.retryAfter * 1000)\n            }\n\n            throw e\n        }\n    },\n})\n```\n\n----------------------------------------\n\nTITLE: Managing Topic Records\nDESCRIPTION: Deletes records from topics up to specified offsets using admin.deleteTopicRecords(). Supports deletion from earliest offset to target offset per partition.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/Admin.md#2025-04-14_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.deleteTopicRecords({\n    topic: 'custom-topic',\n    partitions: [\n        { partition: 0, offset: '30' },\n        { partition: 3, offset: '-1' },\n    ]\n})\n```\n\n----------------------------------------\n\nTITLE: Setting Consumer Group Offsets with KafkaJS Admin Client\nDESCRIPTION: Demonstrates how to set consumer group offsets to specific values using the setOffsets method. Includes structure for specifying partitions and offsets.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Admin.md#2025-04-14_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.setOffsets({\n    groupId: <String>,\n    topic: <String>,\n    partitions: <SeekEntry[]>,\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    partition: <Number>,\n    offset: <String>,\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.setOffsets({\n    groupId: 'my-consumer-group',\n    topic: 'custom-topic',\n    partitions: [\n        { partition: 0, offset: '35' },\n        { partition: 3, offset: '19' },\n    ]\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Partition Assigner in KafkaJS\nDESCRIPTION: Shows how to create a custom partition assigner function that implements the required interface for use with KafkaJS consumer. It includes methods for assigning partitions and defining the protocol.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Consuming.md#2025-04-14_snippet_16\n\nLANGUAGE: javascript\nCODE:\n```\nconst { AssignerProtocol: { MemberAssignment, MemberMetadata } } = require('kafkajs')\n\nconst MyPartitionAssigner = ({ cluster }) => ({\n    name: 'MyPartitionAssigner',\n    version: 1,\n    async assign({ members, topics }) {\n        // perform assignment\n        return myCustomAssignmentArray.map(memberId => ({\n            memberId,\n            memberAssignment: MemberAssignment.encode({\n                version: this.version,\n                assignment: assignment[memberId],\n            })\n        }))\n    },\n    protocol({ topics }) {\n        return {\n            name: this.name,\n            metadata: MemberMetadata.encode({\n            version: this.version,\n            topics,\n            }),\n        }\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring KafkaJS Log Level\nDESCRIPTION: Demonstrates how to set the log level when initializing a new Kafka instance. Supports different log levels: NOTHING, ERROR, WARN, INFO, and DEBUG.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.14.0/Configuration.md#2025-04-14_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Kafka, logLevel } = require('kafkajs')\n\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  logLevel: logLevel.ERROR\n})\n```\n\n----------------------------------------\n\nTITLE: Pausing and Resuming Kafka Consumer per Partition in JavaScript\nDESCRIPTION: Shows how to pause and resume consumption on a per-partition basis, allowing for finer-grained control and isolation of message processing. It uses concurrent partition processing and handles errors for each partition separately.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Consuming.md#2025-04-14_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nconsumer.run({\n    partitionsConsumedConcurrently: 3, // Default: 1\n    eachMessage: async ({ topic, partition, message }) => {\n      // This will be called up to 3 times concurrently\n        try {\n            await sendToDependency(message)\n        } catch (e) {\n            if (e instanceof TooManyRequestsError) {\n                consumer.pause([{ topic, partitions: [partition] }])\n                // Other partitions will keep fetching and processing, until if / when\n                // they also get throttled\n                setTimeout(() => {\n                    consumer.resume([{ topic, partitions: [partition] }])\n                    // Other partitions that are paused will continue to be paused\n                }, e.retryAfter * 1000)\n            }\n\n            throw e\n        }\n    },\n})\n```\n\n----------------------------------------\n\nTITLE: Defining AVRO Schema in AVDL Format\nDESCRIPTION: This snippet demonstrates how to define a simple AVRO schema using AVDL (Avro IDL) format. It defines a namespace and a protocol with a single record containing one string field.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/KafkaIntro.md#2025-04-14_snippet_1\n\nLANGUAGE: avdl\nCODE:\n```\n@namespace(\"com.kafkajs.fixtures\")\nprotocol SimpleProto {\n  record Simple {\n    string foo;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Subscribing to KafkaJS Events Example\nDESCRIPTION: Demonstrates how to subscribe to KafkaJS events using the consumer.on() method and handle event cleanup.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/InstrumentationEvents.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst { HEARTBEAT } = consumer.events\nconst removeListener = consumer.on(HEARTBEAT, e => console.log(`heartbeat at ${e.timestamp}`))\n\n// Remove the listener by invoking removeListener()\n```\n\n----------------------------------------\n\nTITLE: Defining AVRO Schema in AVDL Format\nDESCRIPTION: Example of an AVRO schema definition in AVDL format that creates a Simple record type with a string field 'foo' under the namespace 'com.kafkajs.fixtures'.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/KafkaIntro.md#2025-04-14_snippet_1\n\nLANGUAGE: avdl\nCODE:\n```\n@namespace(\"com.kafkajs.fixtures\")\nprotocol SimpleProto {\n  record Simple {\n    string foo;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing OAUTHBEARER Authentication in KafkaJS\nDESCRIPTION: Shows how to configure OAUTHBEARER authentication in KafkaJS, including an async function to provide OAuth bearer tokens for authentication requests.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Configuration.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  // authenticationTimeout: 10000,\n  // reauthenticationThreshold: 10000,\n  ssl: true,\n  sasl: {\n    mechanism: 'oauthbearer',\n    oauthBearerProvider: async () => {\n      // Use an unsecured token...\n      const token = jwt.sign({ sub: 'test' }, 'abc', { algorithm: 'none' })\n\n      // ...or, more realistically, grab the token from some OAuth endpoint\n\n      return {\n        value: token\n      }\n    }\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Listing Kafka Groups with KafkaJS Admin API\nDESCRIPTION: This snippet demonstrates how to use the listGroups method to retrieve a list of groups available on the broker.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Admin.md#2025-04-14_snippet_15\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.listGroups()\n```\n\n----------------------------------------\n\nTITLE: Resetting Consumer Group Offsets with KafkaJS Admin Client\nDESCRIPTION: Shows how to reset consumer group offsets to the earliest or latest offset using the resetOffsets method.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Admin.md#2025-04-14_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.resetOffsets({ groupId, topic }) // latest by default\n// await admin.resetOffsets({ groupId, topic, earliest: true })\n```\n\n----------------------------------------\n\nTITLE: Accessing Paused Topic Partitions in KafkaJS Consumer\nDESCRIPTION: Shows how to access the list of paused topic partitions using the paused method of the consumer.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Consuming.md#2025-04-14_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\nconst pausedTopicPartitions = consumer.paused()\n\nfor (const topicPartitions of pausedTopicPartitions) {\n  const { topic, partitions } = topicPartitions\n  console.log({ topic, partitions })\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Consumer Group Offsets with KafkaJS Admin Client\nDESCRIPTION: Shows how to set consumer group offsets to specific values using the admin client's setOffsets method. Includes the structure for SeekEntry.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Admin.md#2025-04-14_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.setOffsets({\n    groupId: <String>,\n    topic: <String>,\n    partitions: <SeekEntry[]>,\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    partition: <Number>,\n    offset: <String>,\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.setOffsets({\n    groupId: 'my-consumer-group',\n    topic: 'custom-topic',\n    partitions: [\n        { partition: 0, offset: '35' },\n        { partition: 3, offset: '19' },\n    ]\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Compression Codec in KafkaJS\nDESCRIPTION: This example shows how to implement a custom compression codec in KafkaJS. It defines a codec object with compress and decompress functions, and then registers it with KafkaJS.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/Producing.md#2025-04-14_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst MyCustomSnappyCodec = {\n    async compress(encoder) {\n        return someCompressFunction(encoder.buffer)\n    },\n\n    async decompress(buffer) {\n        return someDecompressFunction(buffer)\n    }\n}\n\nconst { CompressionTypes, CompressionCodecs } = require('kafkajs')\nCompressionCodecs[CompressionTypes.Snappy] = () => MyCustomSnappyCodec\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Partition Assigner in KafkaJS Consumer\nDESCRIPTION: Demonstrates how to add a custom partition assigner to the list of assigners when creating a KafkaJS consumer. It's important to keep the default assigner for compatibility with older consumers.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Consuming.md#2025-04-14_snippet_17\n\nLANGUAGE: javascript\nCODE:\n```\nconst { PartitionAssigners: { roundRobin } } = require('kafkajs')\n\nkafka.consumer({\n    groupId: 'my-group',\n    partitionAssigners: [\n        MyPartitionAssigner,\n        roundRobin\n    ]\n})\n```\n\n----------------------------------------\n\nTITLE: Setting KafkaJS Log Level via Environment Variable\nDESCRIPTION: Demonstrates setting the log level using the KAFKAJS_LOG_LEVEL environment variable, which takes precedence over programmatic configuration.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Configuration.md#2025-04-14_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\nKAFKAJS_LOG_LEVEL=info node code.js\n```\n\n----------------------------------------\n\nTITLE: Configuring LZ4 Compression in KafkaJS\nDESCRIPTION: Demonstrates how to set up and use LZ4 compression for message production in KafkaJS using the kafkajs-lz4 package.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Producing.md#2025-04-14_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nconst { CompressionTypes, CompressionCodecs } = require('kafkajs')\nconst LZ4 = require('kafkajs-lz4')\n\nCompressionCodecs[CompressionTypes.LZ4] = new LZ4().codec\n```\n\n----------------------------------------\n\nTITLE: Handling Duplicate Headers in KafkaJS Consumer\nDESCRIPTION: Demonstrates how to handle message headers that may contain multiple values for the same key.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/MigrationGuide-2-0-0.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst participants = Array.isArray(message.headers[\"participants\"])\n    ? message.headers[\"participants\"].map(participant => participant.toString()).join(\", \")\n    : message.headers[\"participants\"].toString()\n```\n\n----------------------------------------\n\nTITLE: Fetching Topic Metadata\nDESCRIPTION: Retrieves metadata for specified topics including partition information, leaders, and replicas.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/Admin.md#2025-04-14_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.fetchTopicMetadata({ topics: <Array<String>> })\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    topics: <Array<TopicMetadata>>,\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    name: <String>,\n    partitions: <Array<PartitionMetadata>> // default: 1\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    partitionErrorCode: <Number>, // default: 0\n    partitionId: <Number>,\n    leader: <Number>,\n    replicas: <Array<Number>>,\n    isr: <Array<Number>>,\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Partitioner in KafkaJS\nDESCRIPTION: Shows how to use the new default Java-compatible partitioner explicitly or implicitly.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/MigrationGuide-2-0-0.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// Rely on the default partitioner being compatible with the Java partitioner\nkafka.producer()\n\n// Or explicitly use the default partitioner\nconst { Partitioners } = require('kafkajs')\nkafka.producer({ createPartitioner: Partitioners.DefaultPartitioner })\n```\n\n----------------------------------------\n\nTITLE: AWS IAM Authentication Configuration\nDESCRIPTION: Sets up AWS IAM authentication using access keys and optional session token for AWS MSK clusters.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Configuration.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  // authenticationTimeout: 1000,\n  // reauthenticationThreshold: 10000,\n  ssl: true,\n  sasl: {\n    mechanism: 'aws',\n    authorizationIdentity: 'AIDAIOSFODNN7EXAMPLE', // UserId or RoleId\n    accessKeyId: 'AKIAIOSFODNN7EXAMPLE',\n    secretAccessKey: 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',\n    sessionToken: 'WHArYt8i5vfQUrIU5ZbMLCbjcAiv/Eww6eL9tgQMJp6QFNEXAMPLETOKEN' // Optional\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Describing Consumer Group in KafkaJS\nDESCRIPTION: Demonstrates how to retrieve metadata for a configured consumer group including members, protocol, and state information.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Consuming.md#2025-04-14_snippet_19\n\nLANGUAGE: javascript\nCODE:\n```\nconst data = await consumer.describeGroup()\n// {\n//  errorCode: 0,\n//  groupId: 'consumer-group-id-f104efb0e1044702e5f6',\n//  members: [\n//    {\n//      clientHost: '/172.19.0.1',\n//      clientId: 'test-3e93246fe1f4efa7380a',\n//      memberAssignment: Buffer,\n//      memberId: 'test-3e93246fe1f4efa7380a-ff87d06d-5c87-49b8-a1f1-c4f8e3ffe7eb',\n//      memberMetadata: Buffer,\n//    },\n//  ],\n//  protocol: 'RoundRobinAssigner',\n//  protocolType: 'consumer',\n//  state: 'Stable',\n// },\n```\n\n----------------------------------------\n\nTITLE: Initializing KafkaJS Client\nDESCRIPTION: Creates a new Kafka client instance by specifying a client ID and an array of broker addresses.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/GettingStarted.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Kafka } = require('kafkajs')\n\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n})\n```\n\n----------------------------------------\n\nTITLE: Checking Paused Topic Partitions in KafkaJS\nDESCRIPTION: Demonstrates how to retrieve and inspect the list of currently paused topic partitions using the paused() method.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Consuming.md#2025-04-14_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nconst pausedTopicPartitions = consumer.paused()\n\nfor (const topicPartitions of pausedTopicPartitions) {\n  const { topic, partitions } = topicPartitions\n  console.log({ topic, partitions })\n}\n```\n\n----------------------------------------\n\nTITLE: Event Listener Implementation in KafkaJS\nDESCRIPTION: Demonstrates how to set up an event listener for consumer heartbeat events and remove the listener when needed.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/InstrumentationEvents.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst { HEARTBEAT } = consumer.events\nconst removeListener = consumer.on(HEARTBEAT, e => console.log(`heartbeat at ${e.timestamp}`))\n\n// Remove the listener by invoking removeListener()\n```\n\n----------------------------------------\n\nTITLE: Sending Compressed Messages with Custom Codec in KafkaJS Producer\nDESCRIPTION: This snippet shows how to use a custom compression codec when sending messages with the KafkaJS producer. It demonstrates using the CompressionTypes enum to specify the custom Snappy compression for the messages.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.15.0/Producing.md#2025-04-14_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nawait producer.send({\n    topic: 'topic-name',\n    compression: CompressionTypes.Snappy,\n    messages: [\n        { key: 'key1', value: 'hello world' },\n        { key: 'key2', value: 'hey hey!' }\n    ],\n})\n```\n\n----------------------------------------\n\nTITLE: OAuth Bearer Authentication Configuration\nDESCRIPTION: Configures OAuth bearer token authentication with a custom token provider function that returns JWT tokens.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Configuration.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  // authenticationTimeout: 1000,\n  // reauthenticationThreshold: 10000,\n  ssl: true,\n  sasl: {\n    mechanism: 'oauthbearer',\n    oauthBearerProvider: async () => {\n      // Use an unsecured token...\n      const token = jwt.sign({ sub: 'test' }, 'abc', { algorithm: 'none' })\n\n      // ...or, more realistically, grab the token from some OAuth endpoint\n\n      return {\n        value: token\n      }\n    }\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Producer Send Method Signature\nDESCRIPTION: Complete method signature for the producer.send() function showing all available options.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Producing.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nawait producer.send({\n    topic: <String>,\n    messages: <Message[]>,\n    acks: <Number>,\n    timeout: <Number>,\n    compression: <CompressionTypes>,\n})\n```\n\n----------------------------------------\n\nTITLE: Starting Kafka with Docker Compose\nDESCRIPTION: Shell commands to set the HOST_IP environment variable and start the Docker containers defined in the compose file. The HOST_IP is required for the Kafka broker to advertise itself correctly.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/DockerLocal.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nexport HOST_IP=$(ifconfig | grep -E \"([0-9]{1,3}\\.){3}[0-9]{1,3}\" | grep -v 127.0.0.1 | awk '{ print $2 }' | cut -f2 -d: | head -n1)\ndocker-compose up\n```\n\n----------------------------------------\n\nTITLE: Pre-release Package.json Configuration\nDESCRIPTION: Example of the additional kafkajs metadata added to package.json in pre-release versions, including commit SHA and comparison URL.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/PreReleases.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  // package.json\n  \"kafkajs\": {\n    \"sha\": \"43e325e18133b8d6c1c80f8e95ef8610c44ec631\",\n    \"compare\": \"https://github.com/tulios/kafkajs/compare/v1.9.3...43e325e18133b8d6c1c80f8e95ef8610c44ec631\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring SSL for KafkaJS Client\nDESCRIPTION: Sets up SSL configuration for the KafkaJS client, including custom CA certificates, client key, and client certificate. This enhances security for Kafka connections.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Configuration.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst fs = require('fs')\n\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  ssl: {\n    rejectUnauthorized: false,\n    ca: [fs.readFileSync('/my/custom/ca.crt', 'utf-8')],\n    key: fs.readFileSync('/my/custom/client-key.pem', 'utf-8'),\n    cert: fs.readFileSync('/my/custom/client-cert.pem', 'utf-8')\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Installing LZ4 Compression Package for KafkaJS\nDESCRIPTION: This shell command installs the kafkajs-lz4 package, which provides LZ4 compression support for KafkaJS.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Producing.md#2025-04-14_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\nnpm install kafkajs-lz4\n# yarn add kafkajs-lz4\n```\n\n----------------------------------------\n\nTITLE: Configuring Retry Mechanism for KafkaJS Client\nDESCRIPTION: Sets up retry configuration for connection and API calls to Kafka. Options include maximum retry time, initial retry time, randomization factor, exponential multiplier, number of retries, and max in-flight requests.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Configuration.md#2025-04-14_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  retry: {\n    initialRetryTime: 100,\n    retries: 8\n  }\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring LZ4 Compression in KafkaJS Producer\nDESCRIPTION: This example shows how to set up and use LZ4 compression in KafkaJS. It requires installing the 'kafkajs-lz4' package and registering the LZ4 codec with KafkaJS.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/Producing.md#2025-04-14_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst { CompressionTypes, CompressionCodecs } = require('kafkajs')\nconst LZ4 = require('kafkajs-lz4')\n\nCompressionCodecs[CompressionTypes.LZ4] = new LZ4().codec\n```\n\n----------------------------------------\n\nTITLE: Describing Kafka Cluster with KafkaJS Admin Client\nDESCRIPTION: Shows how to retrieve information about the Kafka broker cluster using the describeCluster method.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Admin.md#2025-04-14_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.describeCluster()\n// {\n//   brokers: [\n//     { nodeId: 0, host: 'localhost', port: 9092 }\n//   ],\n//   controller: 0,\n//   clusterId: 'f8QmWTB8SQSLE6C99G4qzA'\n// }\n```\n\n----------------------------------------\n\nTITLE: Describing Consumer Group in KafkaJS\nDESCRIPTION: Demonstrates how to use the 'describeGroup' method to retrieve metadata for the configured consumer group. This feature is marked as experimental and may change in future versions.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Consuming.md#2025-04-14_snippet_17\n\nLANGUAGE: javascript\nCODE:\n```\nconst data = await consumer.describeGroup()\n// {\n//  errorCode: 0,\n//  groupId: 'consumer-group-id-f104efb0e1044702e5f6',\n//  members: [\n//    {\n//      clientHost: '/172.19.0.1',\n//      clientId: 'test-3e93246fe1f4efa7380a',\n//      memberAssignment: Buffer,\n//      memberId: 'test-3e93246fe1f4efa7380a-ff87d06d-5c87-49b8-a1f1-c4f8e3ffe7eb',\n//      memberMetadata: Buffer,\n//    },\n//  ],\n//  protocol: 'RoundRobinAssigner',\n//  protocolType: 'consumer',\n//  state: 'Stable',\n// },\n```\n\n----------------------------------------\n\nTITLE: Fetching Topic Offsets with KafkaJS Admin Client\nDESCRIPTION: Demonstrates how to fetch the most recent offsets for a topic using the admin client's fetchTopicOffsets method.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Admin.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.fetchTopicOffsets(topic)\n// [\n//   { partition: 0, offset: '31004', high: '31004', low: '421' },\n//   { partition: 1, offset: '54312', high: '54312', low: '3102' },\n//   { partition: 2, offset: '32103', high: '32103', low: '518' },\n//   { partition: 3, offset: '28', high: '28', low: '0' },\n// ]\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Compression Codec in KafkaJS\nDESCRIPTION: Explains how to implement a custom compression codec for KafkaJS using existing libraries. The codec must provide async compress and decompress functions.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Producing.md#2025-04-14_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nconst MyCustomSnappyCodec = {\n    async compress(encoder) {\n        return someCompressFunction(encoder.buffer)\n    },\n\n    async decompress(buffer) {\n        return someDecompressFunction(buffer)\n    }\n}\n\nconst { CompressionTypes, CompressionCodecs } = require('kafkajs')\nCompressionCodecs[CompressionTypes.Snappy] = () => MyCustomSnappyCodec\n```\n\n----------------------------------------\n\nTITLE: Using Java-Compatible Partitioner in KafkaJS\nDESCRIPTION: Shows how to use the JavaCompatiblePartitioner for compatibility with Java Kafka clients.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Producing.md#2025-04-14_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Partitioners } = require('kafkajs')\nkafka.producer({ createPartitioner: Partitioners.JavaCompatiblePartitioner })\n```\n\n----------------------------------------\n\nTITLE: Producer Send Method Signature\nDESCRIPTION: Demonstrates the complete method signature for sending messages, including all available options like topic, messages array, acknowledgments, timeout, and compression settings.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Producing.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nawait producer.send({\n    topic: <String>,\n    messages: <Message[]>,\n    acks: <Number>,\n    timeout: <Number>,\n    compression: <CompressionTypes>,\n})\n```\n\n----------------------------------------\n\nTITLE: Starting Kafka Development Environment\nDESCRIPTION: This shell command exports the host IP address and starts the Docker Compose setup for Kafka and Zookeeper. It uses ifconfig to determine the host IP, which is required for proper Kafka broker connectivity.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/DockerLocal.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nexport HOST_IP=$(ifconfig | grep -E \"([0-9]{1,3}\\.){3}[0-9]{1,3}\" | grep -v 127.0.0.1 | awk '{ print $2 }' | cut -f2 -d: | head -n1)\ndocker-compose up\n```\n\n----------------------------------------\n\nTITLE: Installing LZ4 Compression Package for KafkaJS\nDESCRIPTION: This snippet shows the command to install the kafkajs-lz4 package, which provides LZ4 compression support for KafkaJS.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Producing.md#2025-04-14_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\nnpm install kafkajs-lz4\n# yarn add kafkajs-lz4\n```\n\n----------------------------------------\n\nTITLE: Implementing OAuth Bearer Provider with Token Refresh in TypeScript\nDESCRIPTION: Provides a detailed implementation of an OAuth bearer provider function in TypeScript, handling token retrieval, caching, and automatic refreshing using the simple-oauth2 library.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Configuration.md#2025-04-14_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AccessToken, ClientCredentials } from 'simple-oauth2'\ninterface OauthBearerProviderOptions {\n  clientId: string;\n  clientSecret: string;\n  host: string;\n  path: string;\n  refreshThresholdMs: number;\n}\n\nconst oauthBearerProvider = (options: OauthBearerProviderOptions) => {\n  const client = new ClientCredentials({\n    client: {\n      id: options.clientId,\n      secret: options.clientSecret\n    },\n    auth: {\n      tokenHost: options.host,\n      tokenPath: options.path\n    }\n  });\n\n  let tokenPromise: Promise<string>;\n  let accessToken: AccessToken;\n\n  async function refreshToken() {\n    try {\n      if (accessToken == null) {\n        accessToken = await client.getToken({})\n      }\n\n      if (accessToken.expired(options.refreshThresholdMs / 1000)) {\n        accessToken = await accessToken.refresh()\n      }\n\n      const nextRefresh = accessToken.token.expires_in * 1000 - options.refreshThresholdMs;\n      setTimeout(() => {\n        tokenPromise = refreshToken()\n      }, nextRefresh);\n\n      return accessToken.token.access_token;\n    } catch (error) {\n      accessToken = null;\n      throw error;\n    }\n  }\n\n  tokenPromise = refreshToken();\n\n  return async function () {\n    return {\n      value: await tokenPromise\n    }\n  }\n};\n\nconst kafka = new Kafka({\n  // ... other required options\n  sasl: {\n    mechanism: 'oauthbearer',\n    oauthBearerProvider: oauthBearerProvider({\n      clientId: 'oauth-client-id',\n      clientSecret: 'oauth-client-secret',\n      host: 'https://my-oauth-server.com',\n      path: '/oauth/token',\n      // Refresh the token 15 seconds before it expires\n      refreshThreshold: 15000,\n    }),\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Compression Codec for KafkaJS\nDESCRIPTION: This snippet shows how to implement a custom compression codec for KafkaJS. It defines an object with compress and decompress functions that can be used with any compression library.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Producing.md#2025-04-14_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nconst MyCustomSnappyCodec = {\n    async compress(encoder) {\n        return someCompressFunction(encoder.buffer)\n    },\n\n    async decompress(buffer) {\n        return someDecompressFunction(buffer)\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Snappy Compression in KafkaJS\nDESCRIPTION: Demonstrates how to set up and use Snappy compression with KafkaJS producer. Requires installing the 'kafkajs-snappy' package separately.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.4/Producing.md#2025-04-14_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst {  CompressionTypes, CompressionCodecs } = require('kafkajs')\nconst SnappyCodec = require('kafkajs-snappy')\n\nCompressionCodecs[CompressionTypes.Snappy] = SnappyCodec\n```\n\n----------------------------------------\n\nTITLE: Initializing and Managing KafkaJS Admin Client\nDESCRIPTION: Example showing how to create an admin client instance, connect to Kafka and disconnect when operations are complete. The admin client provides cluster operations functionality.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Admin.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst kafka = new Kafka(...)\nconst admin = kafka.admin()\n\n// remember to connect and disconnect when you are done\nawait admin.connect()\nawait admin.disconnect()\n```\n\n----------------------------------------\n\nTITLE: Describing Consumer Group in KafkaJS\nDESCRIPTION: Shows how to use the describeGroup method to retrieve metadata for the configured consumer group. This feature is marked as experimental and may change in future versions.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Consuming.md#2025-04-14_snippet_18\n\nLANGUAGE: javascript\nCODE:\n```\nconst data = await consumer.describeGroup()\n// {\n//  errorCode: 0,\n//  groupId: 'consumer-group-id-f104efb0e1044702e5f6',\n//  members: [\n//    {\n//      clientHost: '/172.19.0.1',\n//      clientId: 'test-3e93246fe1f4efa7380a',\n//      memberAssignment: Buffer,\n//      memberId: 'test-3e93246fe1f4efa7380a-ff87d06d-5c87-49b8-a1f1-c4f8e3ffe7eb',\n//      memberMetadata: Buffer,\n//    },\n//  ],\n//  protocol: 'RoundRobinAssigner',\n//  protocolType: 'consumer',\n//  state: 'Stable',\n// },\n```\n\n----------------------------------------\n\nTITLE: Producing to Multiple Kafka Topics\nDESCRIPTION: Demonstrates how to use the sendBatch method to produce messages to multiple Kafka topics simultaneously, which can be useful for topic migration or multi-topic publishing scenarios.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.15.0/Producing.md#2025-04-14_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst topicMessages = [\n  {\n    topic: 'topic-a',\n    messages: [{ key: 'key', value: 'hello topic-a' }],\n  },\n  {\n    topic: 'topic-b',\n    messages: [{ key: 'key', value: 'hello topic-b' }],\n  },\n  {\n    topic: 'topic-c',\n    messages: [\n      {\n        key: 'key',\n        value: 'hello topic-c',\n        headers: {\n          'correlation-id': '2bfb68bb-893a-423b-a7fa-7b568cad5b67',\n        },\n      }\n    ],\n  }\n]\nawait producer.sendBatch({ topicMessages })\n```\n\n----------------------------------------\n\nTITLE: Compressing Messages with GZIP in KafkaJS\nDESCRIPTION: Demonstrates how to use GZIP compression when sending messages with the KafkaJS producer. GZIP is the default compression codec included with KafkaJS.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Producing.md#2025-04-14_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst { CompressionTypes } = require('kafkajs')\n\nasync () => {\n  await producer.send({\n    topic: 'topic-name',\n    compression: CompressionTypes.GZIP,\n    messages: [\n        { key: 'key1', value: 'hello world' },\n        { key: 'key2', value: 'hey hey!' }\n    ],\n  })\n}\n```\n\n----------------------------------------\n\nTITLE: Using Legacy Partitioner in KafkaJS\nDESCRIPTION: Demonstrates how to use the LegacyPartitioner in KafkaJS for backwards compatibility with versions prior to 2.0.0.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Producing.md#2025-04-14_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Partitioners } = require('kafkajs')\nkafka.producer({ createPartitioner: Partitioners.LegacyPartitioner })\n```\n\n----------------------------------------\n\nTITLE: Batch Sending to Multiple Topics\nDESCRIPTION: Example of sending messages to multiple topics in a single batch operation.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Producing.md#2025-04-14_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst topicMessages = [\n  {\n    topic: 'topic-a',\n    messages: [{ key: 'key', value: 'hello topic-a' }],\n  },\n  {\n    topic: 'topic-b',\n    messages: [{ key: 'key', value: 'hello topic-b' }],\n  },\n  {\n    topic: 'topic-c',\n    messages: [\n      {\n        key: 'key',\n        value: 'hello topic-c',\n        headers: {\n          'correlation-id': '2bfb68bb-893a-423b-a7fa-7b568cad5b67',\n        },\n      }\n    ],\n  }\n]\nawait producer.sendBatch({ topicMessages })\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Compression Codec in KafkaJS\nDESCRIPTION: Shows how to implement a custom compression codec for KafkaJS producer. This example demonstrates creating a custom Snappy codec and registering it with KafkaJS.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.4/Producing.md#2025-04-14_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nconst MyCustomSnappyCodec = {\n    async compress(encoder) {\n        return someCompressFunction(encoder.buffer)\n    },\n\n    async decompress(buffer) {\n        return someDecompressFunction(buffer)\n    }\n}\n\nconst { CompressionTypes, CompressionCodecs } = require('kafkajs')\nCompressionCodecs[CompressionTypes.Snappy] = () => MyCustomSnappyCodec\n\nawait producer.send({\n    topic: 'topic-name',\n    compression: CompressionTypes.Snappy,\n    messages: [\n        { key: 'key1', value: 'hello world' },\n        { key: 'key2', value: 'hey hey!' }\n    ],\n})\n```\n\n----------------------------------------\n\nTITLE: Updating Sidebar Configuration in Docusaurus\nDESCRIPTION: Example of how to update the sidebar.json file to include a newly created document in the 'Getting Started' category of documentation.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/README.md#2025-04-14_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n// Add newly-created-doc to the Getting Started category of docs\n{\n  \"docs\": {\n    \"Getting Started\": [\n      \"quick-start\",\n      \"newly-created-doc\" // new doc here\n    ],\n    ...\n  },\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Topic Configuration in KafkaJS\nDESCRIPTION: Structure of the ITopicConfig object used when creating topics. Includes topic name, partition count, replication factor, replica assignment, and configuration entries.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Admin.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    topic: <String>,\n    numPartitions: <Number>,     // default: 1\n    replicationFactor: <Number>, // default: 1\n    replicaAssignment: <Array>,  // Example: [{ partition: 0, replicas: [0,1,2] }] - default: []\n    configEntries: <Array>       // Example: [{ name: 'cleanup.policy', value: 'compact' }] - default: []\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Navigation Links in Docusaurus\nDESCRIPTION: Example of how to modify the siteConfig.js file to add various types of links (docs, custom pages, external links) to the site's top navigation bar.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/README.md#2025-04-14_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  headerLinks: [\n    ...\n    /* you can add docs */\n    { doc: 'my-examples', label: 'Examples' },\n    /* you can add custom pages */\n    { page: 'help', label: 'Help' },\n    /* you can add external links */\n    { href: 'https://github.com/facebook/Docusaurus', label: 'GitHub' },\n    ...\n  ],\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Request Timeout for KafkaJS Client\nDESCRIPTION: Configures the maximum time in milliseconds to wait for a successful API request to Kafka. The default value is 30000ms (30 seconds).\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Configuration.md#2025-04-14_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  requestTimeout: 25000\n})\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Compression Codec in KafkaJS\nDESCRIPTION: This snippet demonstrates how to register a custom compression codec in KafkaJS. It imports the necessary modules and assigns the custom codec to the CompressionCodecs object.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Producing.md#2025-04-14_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\nconst { CompressionTypes, CompressionCodecs } = require('kafkajs')\nCompressionCodecs[CompressionTypes.Snappy] = MyCustomSnappyCodec\n```\n\n----------------------------------------\n\nTITLE: Setting Up ZSTD Compression in KafkaJS\nDESCRIPTION: Demonstrates how to configure and use Zstandard (ZSTD) compression with KafkaJS producer. Requires installing the '@kafkajs/zstd' package separately.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.4/Producing.md#2025-04-14_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nconst { CompressionTypes, CompressionCodecs } = require('kafkajs')\nconst ZstdCodec = require('@kafkajs/zstd')\n\nCompressionCodecs[CompressionTypes.ZSTD] = ZstdCodec()\n```\n\n----------------------------------------\n\nTITLE: Initializing KafkaJS Admin Client\nDESCRIPTION: Shows how to create and connect to a Kafka admin client instance. Includes basic connection and disconnection operations.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Admin.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst kafka = new Kafka(...)\nconst admin = kafka.admin()\n\n// remember to connect and disconnect when you are done\nawait admin.connect()\nawait admin.disconnect()\n```\n\n----------------------------------------\n\nTITLE: SASL Plain/SCRAM Authentication\nDESCRIPTION: Configuration for SASL authentication using PLAIN or SCRAM mechanisms with username and password credentials. Includes SSL configuration for secure transmission.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.14.0/Configuration.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  // authenticationTimeout: 1000,\n  // reauthenticationThreshold: 10000,\n  ssl: true,\n  sasl: {\n    mechanism: 'plain', // scram-sha-256 or scram-sha-512\n    username: 'my-username',\n    password: 'my-password'\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Event Listener Implementation in KafkaJS\nDESCRIPTION: Example showing how to add and remove event listeners for KafkaJS consumer events. Uses the HEARTBEAT event as an example.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.15.0/InstrumentationEvents.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst { HEARTBEAT } = consumer.events\nconst removeListener = consumer.on(HEARTBEAT, e => console.log(`heartbeat at ${e.timestamp}`))\n\n// Remove the listener by invoking removeListener()\n```\n\n----------------------------------------\n\nTITLE: Configuring Snappy Compression for KafkaJS\nDESCRIPTION: This code snippet demonstrates how to configure Snappy compression for KafkaJS. It imports the necessary modules and adds the Snappy codec to the CompressionCodecs object.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Producing.md#2025-04-14_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst {  CompressionTypes, CompressionCodecs } = require('kafkajs')\nconst SnappyCodec = require('kafkajs-snappy')\n\nCompressionCodecs[CompressionTypes.Snappy] = SnappyCodec\n```\n\n----------------------------------------\n\nTITLE: Creating Topics with Specific Partition Count and Replication Factor in KafkaJS v2.0.0\nDESCRIPTION: Demonstrates how to create topics with specific partition count and replication factor in KafkaJS v2.0.0, which now respects cluster defaults.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/MigrationGuide-2-0-0.md#2025-04-14_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.createTopics({\n  topics: [{ topic: 'topic-name', numPartitions: 1, replicationFactor: 1 }]\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Partitioner in KafkaJS\nDESCRIPTION: Demonstrates how to create and use a custom partitioner function with the KafkaJS producer. The partitioner determines which partition a message should be sent to.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Producing.md#2025-04-14_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst MyPartitioner = () => {\n    return ({ topic, partitionMetadata, message }) => {\n        // select a partition based on some logic\n        // return the partition number\n        return 0\n    }\n}\n\nkafka.producer({ createPartitioner: MyPartitioner })\n```\n\n----------------------------------------\n\nTITLE: Pausing and Resuming Consumption in KafkaJS Consumer\nDESCRIPTION: Demonstrates how to pause and resume consumption from a topic when an external dependency is overloaded. It uses a timeout to resume consumption after a specified delay.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Consuming.md#2025-04-14_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nawait consumer.connect()\nawait consumer.subscribe({ topics: ['jobs'] })\n\nawait consumer.run({ eachMessage: async ({ topic, message }) => {\n    try {\n        await sendToDependency(message)\n    } catch (e) {\n        if (e instanceof TooManyRequestsError) {\n            consumer.pause([{ topic }])\n            setTimeout(() => consumer.resume([{ topic }]), e.retryAfter * 1000)\n        }\n\n        throw e\n    }\n}})\n```\n\n----------------------------------------\n\nTITLE: Listening for KafkaJS Consumer Events (JavaScript)\nDESCRIPTION: This snippet demonstrates how to listen for a HEARTBEAT event from a KafkaJS consumer. It shows how to add an event listener and how to remove it when no longer needed.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/InstrumentationEvents.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst { HEARTBEAT } = consumer.events\nconst removeListener = consumer.on(HEARTBEAT, e => console.log(`heartbeat at ${e.timestamp}`))\n\n// Remove the listener by invoking removeListener()\n```\n\n----------------------------------------\n\nTITLE: Sending GZIP Compressed Messages with KafkaJS Producer\nDESCRIPTION: This snippet demonstrates how to send GZIP compressed messages using a KafkaJS producer. It imports the CompressionTypes enum and sets the compression option in the send method.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Producing.md#2025-04-14_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst { CompressionTypes } = require('kafkajs')\n\nasync () => {\n  await producer.send({\n    topic: 'topic-name',\n    compression: CompressionTypes.GZIP,\n    messages: [\n        { key: 'key1', value: 'hello world' },\n        { key: 'key2', value: 'hey hey!' }\n    ],\n  })\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring LZ4 Compression for KafkaJS\nDESCRIPTION: This snippet demonstrates how to configure LZ4 compression in KafkaJS. It imports the necessary modules and assigns the LZ4 codec to the CompressionCodecs object.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Producing.md#2025-04-14_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nconst { CompressionTypes, CompressionCodecs } = require('kafkajs')\nconst LZ4 = require('kafkajs-lz4')\n\nCompressionCodecs[CompressionTypes.LZ4] = new LZ4().codec\n```\n\n----------------------------------------\n\nTITLE: Deleting Kafka Topic Records with KafkaJS Admin API\nDESCRIPTION: This snippet shows how to use the deleteTopicRecords method to remove records from a selected topic up to a specified offset for given partitions.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Admin.md#2025-04-14_snippet_18\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.deleteTopicRecords({\n    topic: <String>,\n    partitions: <SeekEntry[]>,\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.deleteTopicRecords({\n    topic: 'custom-topic',\n    partitions: [\n        { partition: 0, offset: '30' }, // delete up to and including offset 29\n        { partition: 3, offset: '-1' }, // delete all available records on this partition\n    ]\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom 'Simon' Authentication Mechanism in KafkaJS\nDESCRIPTION: Complete example implementation of a fictional 'simon' authentication mechanism that demonstrates the authentication flow. The example shows how to encode/decode authentication bytes and handle the authentication communication with the Kafka broker.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/CustomAuthenticationMechanism.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst simonAuthenticator = says = ({ host, port, logger, saslAuthenticate }) => {\n    const INT32_SIZE = 4\n  \n    const request = {\n      /**\n       * Encodes the value for `auth_bytes` in SaslAuthenticate request\n       * @see https://kafka.apache.org/protocol.html#The_Messages_SaslAuthenticate\n       * \n       * In this example, we are just sending `says` as a string,\n       * with the length of the string in bytes prepended as an int32\n       **/\n      encode: () => {\n        const byteLength = Buffer.byteLength(says, 'utf8')\n        const buf = Buffer.alloc(INT32_SIZE + byteLength)\n        buf.writeUInt32BE(byteLength, 0)\n        buf.write(says, INT32_SIZE, byteLength, 'utf8')\n        return buf\n      },\n    }\n    const response = {\n      /**\n       * Decodes the `auth_bytes` in SaslAuthenticate response\n       * @see https://kafka.apache.org/protocol.html#The_Messages_SaslAuthenticate\n       * \n       * This is essentially the reverse of `request.encode`, where\n       * we read the length of the string as an int32 and then read\n       * that many bytes\n       */\n      decode: rawData => {\n        const byteLength = rawData.readInt32BE(0)\n        return rawData.slice(INT32_SIZE, INT32_SIZE + byteLength)\n      },\n      /**\n       * The return value from `response.decode` is passed into\n       * this function, which is responsible for interpreting\n       * the data. In this case, we just turn the buffer back\n       * into a string\n       */\n      parse: data => {\n        return data.toString()\n      },\n    }\n    return {\n      /**\n       * This function is responsible for orchestrating the authentication flow.\n       * Essentially we will send a SaslAuthenticate request with the\n       * value of `sasl.says` to the broker, and expect to\n       * get the same value back.\n       * \n       * Other authentication methods may do any other operations they\n       * like, but communication with the brokers goes through\n       * the SaslAuthenticate request.\n       */\n      authenticate: async () => {\n        if (says == null) {\n          throw new Error('SASL Simon: Invalid \"says\"')\n        }\n        const broker = `${host}:${port}`\n        try {\n          logger.info('Authenticate with SASL Simon', { broker })\n          const authenticateResponse = await saslAuthenticate({ request, response })\n  \n          const saidSimon = says.startsWith(\"Simon says \")\n          const expectedResponse = saidSimon ? says : \"\"\n          if (authenticateResponse !== expectedResponse) {\n              throw new Error(\"Mismatching response from broker\")\n          }\n          logger.info('SASL Simon authentication successful', { broker })\n        } catch (e) {\n          const error = new Error(\n            `SASL Simon authentication failed: ${e.message}`\n          )\n          logger.error(error.message, { broker })\n          throw error\n        }\n      },\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Deleting Topics with KafkaJS Admin Client\nDESCRIPTION: Demonstrates how to delete topics using the admin client's deleteTopics method. Includes a note about enabling topic deletion in Kafka versions prior to 1.0.0.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Admin.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.deleteTopics({\n    topics: <String[]>,\n    timeout: <Number>,\n})\n```\n\nLANGUAGE: yml\nCODE:\n```\ndelete.topic.enable=true\n```\n\n----------------------------------------\n\nTITLE: Installing Snappy Compression Package for KafkaJS\nDESCRIPTION: This snippet shows the command to install the kafkajs-snappy package, which provides Snappy compression support for KafkaJS.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Producing.md#2025-04-14_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nnpm install kafkajs-snappy\n# yarn add kafkajs-snappy\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Instrumentation Events in KafkaJS\nDESCRIPTION: Demonstrates how to subscribe to instrumentation events in KafkaJS using the consumer.on method. The example shows subscribing to the HEARTBEAT event and logging the timestamp.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/InstrumentationEvents.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst { HEARTBEAT } = consumer.events\nconst removeListener = consumer.on(HEARTBEAT, e => console.log(`heartbeat at ${e.timestamp}`))\n\n// Remove the listener by invoking removeListener()\n```\n\n----------------------------------------\n\nTITLE: Configuring Snappy Compression for KafkaJS Producer\nDESCRIPTION: This snippet shows how to configure Snappy compression for KafkaJS. It requires installing the kafkajs-snappy package and then setting up the Snappy codec in the CompressionCodecs object.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Producing.md#2025-04-14_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst {  CompressionTypes, CompressionCodecs } = require('kafkajs')\nconst SnappyCodec = require('kafkajs-snappy')\n\nCompressionCodecs[CompressionTypes.Snappy] = SnappyCodec\n```\n\n----------------------------------------\n\nTITLE: Initializing KafkaJS Client with Basic Configuration\nDESCRIPTION: Creates a new KafkaJS client with a clientId and a list of seed brokers. The brokers are used to bootstrap the client and load initial metadata.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Configuration.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Kafka } = require('kafkajs')\n\n// Create the client with the broker list\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092']\n})\n```\n\n----------------------------------------\n\nTITLE: Sending GZIP Compressed Messages with KafkaJS Producer\nDESCRIPTION: This snippet demonstrates how to send GZIP compressed messages using the KafkaJS producer. It imports the CompressionTypes enum and sets the compression type to GZIP when sending messages.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Producing.md#2025-04-14_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst { CompressionTypes } = require('kafkajs')\n\nasync () => {\n  await producer.send({\n    topic: 'topic-name',\n    compression: CompressionTypes.GZIP,\n    messages: [\n        { key: 'key1', value: 'hello world' },\n        { key: 'key2', value: 'hey hey!' }\n    ],\n  })\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Topics with KafkaJS Admin Client\nDESCRIPTION: Demonstrates how to create topics using the createTopics method. Includes options for validation, waiting for leaders, and timeout. Returns true if successful, false if the topic already exists.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Admin.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.createTopics({\n    validateOnly: <boolean>,\n    waitForLeaders: <boolean>\n    timeout: <Number>,\n    topics: <ITopicConfig[]>,\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    topic: <String>,\n    numPartitions: <Number>,     // default: -1 (uses broker `num.partitions` configuration)\n    replicationFactor: <Number>, // default: -1 (uses broker `default.replication.factor` configuration)\n    replicaAssignment: <Array>,  // Example: [{ partition: 0, replicas: [0,1,2] }] - default: []\n    configEntries: <Array>       // Example: [{ name: 'cleanup.policy', value: 'compact' }] - default: []\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring ZSTD Compression in KafkaJS\nDESCRIPTION: Shows how to set up and use Zstandard compression for message production in KafkaJS using the @kafkajs/zstd package.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Producing.md#2025-04-14_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nconst { CompressionTypes, CompressionCodecs } = require('kafkajs')\nconst ZstdCodec = require('@kafkajs/zstd')\n\nCompressionCodecs[CompressionTypes.ZSTD] = ZstdCodec()\n```\n\n----------------------------------------\n\nTITLE: Deleting Kafka ACLs with KafkaJS Admin API\nDESCRIPTION: This snippet shows how to use the deleteAcls method to remove Access Control Lists (ACLs) for Kafka resources based on specified filters.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Admin.md#2025-04-14_snippet_20\n\nLANGUAGE: javascript\nCODE:\n```\nconst {\n  AclResourceTypes,\n  AclOperationTypes,\n  AclPermissionTypes,\n  ResourcePatternTypes,\n} = require('kafkajs')\n\nconst acl = {\n  resourceName: 'topic-name,\n  resourceType: AclResourceTypes.TOPIC,\n  host: '*',\n  permissionType: AclPermissionTypes.ALLOW,\n  operation: AclOperationTypes.ANY,\n  resourcePatternType: ResourcePatternTypes.LITERAL,\n}\n\nawait admin.deleteAcls({ filters: [acl] })\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Authentication in KafkaJS\nDESCRIPTION: Basic configuration structure for setting up a custom authentication mechanism in KafkaJS. The mechanism name must match what's configured in the Kafka broker's sasl.enabled.mechanisms property.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/CustomAuthenticationMechanism.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n{ \n  sasl: { \n      mechanism: <mechanism name>,\n      authenticationProvider: ({ host, port, logger, saslAuthenticate }) => { authenticate: () => Promise<void> }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Deleting Kafka Topics with KafkaJS Admin Client\nDESCRIPTION: Shows how to delete topics using the deleteTopics method, specifying the topics to delete and an optional timeout.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Admin.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.deleteTopics({\n    topics: <String[]>,\n    timeout: <Number>,\n})\n```\n\n----------------------------------------\n\nTITLE: KafkaJS Producer Send Method Signature\nDESCRIPTION: Describes the full signature of the producer's send method, including optional parameters for acks, timeout, and compression.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.15.0/Producing.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nawait producer.send({\n    topic: <String>,\n    messages: <Message[]>,\n    acks: <Number>,\n    timeout: <Number>,\n    compression: <CompressionTypes>,\n})\n```\n\n----------------------------------------\n\nTITLE: Subscribing to KafkaJS Consumer Events\nDESCRIPTION: Example showing how to register an event listener for a consumer heartbeat event. The listener callback receives an event object and logs the timestamp. The method returns a function to remove the listener when needed.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/InstrumentationEvents.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst { HEARTBEAT } = consumer.events\nconst removeListener = consumer.on(HEARTBEAT, e => console.log(`heartbeat at ${e.timestamp}`))\n\n// Remove the listener by invoking removeListener()\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Compression Codec for KafkaJS\nDESCRIPTION: This snippet shows how to implement a custom compression codec for KafkaJS. It defines an object with compress and decompress methods, which can be used to implement any compression algorithm.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Producing.md#2025-04-14_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nconst MyCustomSnappyCodec = {\n    async compress(encoder) {\n        return someCompressFunction(encoder.buffer)\n    },\n\n    async decompress(buffer) {\n        return someDecompressFunction(buffer)\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Sending Messages to Specific Partitions with KafkaJS Producer in JavaScript\nDESCRIPTION: Demonstrates how to send messages to specific partitions in a Kafka topic using the KafkaJS producer.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.4/Producing.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n\nawait producer.connect()\nawait producer.send({\n    topic: 'topic-name',\n    messages: [\n        { key: 'key1', value: 'hello world', partition: 0 },\n        { key: 'key2', value: 'hey hey!', partition: 1 }\n    ],\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring Snappy Compression for KafkaJS\nDESCRIPTION: This snippet demonstrates how to configure Snappy compression for KafkaJS. It imports the necessary modules and adds the Snappy codec to the CompressionCodecs object.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Producing.md#2025-04-14_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst {  CompressionTypes, CompressionCodecs } = require('kafkajs')\nconst SnappyCodec = require('kafkajs-snappy')\n\nCompressionCodecs[CompressionTypes.Snappy] = SnappyCodec\n```\n\n----------------------------------------\n\nTITLE: Creating Kafka Partitions with KafkaJS Admin Client\nDESCRIPTION: Illustrates the use of createPartitions method to add partitions to existing topics, including options for validation and timeout.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Admin.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.createPartitions({\n    validateOnly: <boolean>,\n    timeout: <Number>,\n    topicPartitions: <TopicPartition[]>,\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    topic: <String>,\n    count: <Number>,     // partition count\n    assignments: <Array<Array<Number>>> // Example: [[0,1],[1,2],[2,0]] \n}\n```\n\n----------------------------------------\n\nTITLE: Installing Snappy Compression Package for KafkaJS\nDESCRIPTION: This snippet shows how to install the kafkajs-snappy package using npm or yarn. This package is required to use Snappy compression with KafkaJS.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Producing.md#2025-04-14_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nnpm install kafkajs-snappy\n# yarn add kafkajs-snappy\n```\n\n----------------------------------------\n\nTITLE: Deleting Kafka Groups using KafkaJS Admin API\nDESCRIPTION: Deletes specified Kafka consumer groups by their groupIds. Only groups with no connected consumers can be deleted.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Admin.md#2025-04-14_snippet_17\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.deleteGroups([groupId])\n```\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.deleteGroups(['group-test'])\n```\n\nLANGUAGE: javascript\nCODE:\n```\n[\n    {groupId: 'testgroup', errorCode: 'consumer'}\n]\n```\n\nLANGUAGE: javascript\nCODE:\n```\ntry {\n    await admin.deleteGroups(['a', 'b', 'c'])\n} catch (error) {\n  // error.name 'KafkaJSDeleteGroupsError'\n  // error.groups = [{\n  //   groupId: a\n  //   error: KafkaJSProtocolError\n  // }]\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Socket Factory in KafkaJS\nDESCRIPTION: This snippet demonstrates how to implement a custom socket factory for KafkaJS. It includes an example of setting a custom TTL and shows how to use the custom socket factory when creating a new Kafka instance.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Configuration.md#2025-04-14_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Kafka } = require('kafkajs')\n\n// Example socket factory setting a custom TTL\nconst net = require('net')\nconst tls = require('tls')\n\nconst myCustomSocketFactory = ({ host, port, ssl, onConnect }) => {\n  const socket = ssl\n    ? tls.connect(\n        Object.assign({ host, port }, ssl),\n        onConnect\n      )\n    : net.connect(\n        { host, port },\n        onConnect\n      )\n\n  socket.setKeepAlive(true, 30000)\n\n  return socket\n}\n\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  socketFactory: myCustomSocketFactory,\n})\n```\n\n----------------------------------------\n\nTITLE: Sending Compressed Messages with GZIP in KafkaJS Producer\nDESCRIPTION: This code shows how to send messages using GZIP compression with the KafkaJS producer. It demonstrates importing the CompressionTypes enum and using it in the send method to specify GZIP compression for the messages.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.15.0/Producing.md#2025-04-14_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst { CompressionTypes } = require('kafkajs')\n\nasync () => {\n  await producer.send({\n    topic: 'topic-name',\n    compression: CompressionTypes.GZIP,\n    messages: [\n        { key: 'key1', value: 'hello world' },\n        { key: 'key2', value: 'hey hey!' }\n    ],\n  })\n}\n```\n\n----------------------------------------\n\nTITLE: Sending Messages with Headers in KafkaJS\nDESCRIPTION: Demonstrates how to include message headers when sending messages to Kafka using KafkaJS.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Producing.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nawait producer.send({\n    topic: 'topic-name',\n    messages: [{\n        key: 'key1',\n        value: 'hello world',\n        headers: {\n            'correlation-id': '2bfb68bb-893a-423b-a7fa-7b568cad5b67',\n            'system-id': 'my-system'\n        }\n    }]\n})\n```\n\n----------------------------------------\n\nTITLE: Instrumentation Event Structure in KafkaJS\nDESCRIPTION: Defines the structure of an instrumentation event in KafkaJS. Each event includes an id, type, timestamp, and payload object.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/InstrumentationEvents.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  id: <Number>,\n  type: <String>,\n  timestamp: <Number>,\n  payload: <Object>\n}\n```\n\n----------------------------------------\n\nTITLE: Installing LZ4 Compression Package for KafkaJS\nDESCRIPTION: This snippet shows how to install the kafkajs-lz4 package using npm or yarn. This package is required to use LZ4 compression with KafkaJS.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Producing.md#2025-04-14_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\nnpm install kafkajs-lz4\n# yarn add kafkajs-lz4\n```\n\n----------------------------------------\n\nTITLE: Resetting Consumer Group Offsets with KafkaJS Admin Client\nDESCRIPTION: Shows how to reset consumer group offsets to the earliest or latest offset using the resetOffsets method. The consumer group must have no running instances when performing the reset.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Admin.md#2025-04-14_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.resetOffsets({ groupId, topic }) // latest by default\n// await admin.resetOffsets({ groupId, topic, earliest: true })\n```\n\n----------------------------------------\n\nTITLE: Sending Compressed Messages with Custom Codec in KafkaJS\nDESCRIPTION: This snippet shows how to send compressed messages using a custom codec in KafkaJS. It uses the producer.send method with the specified compression type and messages.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Producing.md#2025-04-14_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nawait producer.send({\n    topic: 'topic-name',\n    compression: CompressionTypes.Snappy,\n    messages: [\n        { key: 'key1', value: 'hello world' },\n        { key: 'key2', value: 'hey hey!' }\n    ],\n})\n```\n\n----------------------------------------\n\nTITLE: Altering Configs with KafkaJS Admin Client\nDESCRIPTION: Shows how to update the configuration for specified resources using the admin client's alterConfigs method. Includes structures for ResourceConfig and ResourceConfigEntry, and an example of usage.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Admin.md#2025-04-14_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.alterConfigs({\n    validateOnly: false,\n    resources: <ResourceConfig[]>\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    type: <ResourceType>,\n    name: <String>,\n    configEntries: <ResourceConfigEntry[]>\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    name: <String>,\n    value: <String>\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst { ResourceTypes } = require('kafkajs')\n\nawait admin.alterConfigs({\n    resources: [{\n        type: ResourceTypes.TOPIC,\n        name: 'topic-name',\n        configEntries: [{ name: 'cleanup.policy', value: 'compact' }]\n    }]\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring ZSTD Compression in KafkaJS Producer\nDESCRIPTION: This snippet demonstrates how to set up and use ZSTD (Zstandard) compression in KafkaJS. It requires installing the '@kafkajs/zstd' package and registering the ZSTD codec with KafkaJS.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/Producing.md#2025-04-14_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst { CompressionTypes, CompressionCodecs } = require('kafkajs')\nconst ZstdCodec = require('@kafkajs/zstd')\n\nCompressionCodecs[CompressionTypes.ZSTD] = ZstdCodec()\n```\n\n----------------------------------------\n\nTITLE: Deleting Kafka Topics\nDESCRIPTION: Shows how to delete topics from the Kafka cluster with timeout configuration.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Admin.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.deleteTopics({\n    topics: <String[]>,\n    timeout: <Number>,\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Compression Codec for KafkaJS\nDESCRIPTION: This snippet shows how to implement a custom compression codec for KafkaJS. It defines an object with compress and decompress methods, which can be used to implement any compression algorithm.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Producing.md#2025-04-14_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nconst MyCustomSnappyCodec = {\n    async compress(encoder) {\n        return someCompressFunction(encoder.buffer)\n    },\n\n    async decompress(buffer) {\n        return someDecompressFunction(buffer)\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring GZIP Compression in KafkaJS Producer\nDESCRIPTION: This example shows how to use GZIP compression when sending messages with a KafkaJS producer. GZIP is the default compression codec included in KafkaJS core functionality.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/Producing.md#2025-04-14_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst { CompressionTypes } = require('kafkajs')\n\nasync () => {\n  await producer.send({\n    topic: 'topic-name',\n    compression: CompressionTypes.GZIP,\n    messages: [\n        { key: 'key1', value: 'hello world' },\n        { key: 'key2', value: 'hey hey!' }\n    ],\n  })\n}\n```\n\n----------------------------------------\n\nTITLE: Sending Messages with Specified Partitions\nDESCRIPTION: Demonstration of sending messages to specific partitions within a Kafka topic.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Producing.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n\nawait producer.connect()\nawait producer.send({\n    topic: 'topic-name',\n    messages: [\n        { key: 'key1', value: 'hello world', partition: 0 },\n        { key: 'key2', value: 'hey hey!', partition: 1 }\n    ],\n})\n```\n\n----------------------------------------\n\nTITLE: KafkaJS Instrumentation Event Structure\nDESCRIPTION: Shows the structure of a KafkaJS instrumentation event object containing id, type, timestamp and payload properties.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/InstrumentationEvents.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  id: <Number>,\n  type: <String>,\n  timestamp: <Number>,\n  payload: <Object>\n}\n```\n\n----------------------------------------\n\nTITLE: Subscribing with Regular Expression\nDESCRIPTION: Demonstrates how to subscribe to topics using regular expression pattern matching.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Consuming.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nawait consumer.connect()\nawait consumer.subscribe({ topics: [/topic-(eu|us)-.*/i] })\n```\n\n----------------------------------------\n\nTITLE: Sending Messages to a Single Kafka Topic\nDESCRIPTION: Shows how to connect a producer and send messages to a single Kafka topic. Includes examples of sending messages with keys and to specific partitions.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Producing.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n\nawait producer.connect()\nawait producer.send({\n    topic: 'topic-name',\n    messages: [\n        { key: 'key1', value: 'hello world' },\n        { key: 'key2', value: 'hey hey!' }\n    ],\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n\nawait producer.connect()\nawait producer.send({\n    topic: 'topic-name',\n    messages: [\n        { key: 'key1', value: 'hello world', partition: 0 },\n        { key: 'key2', value: 'hey hey!', partition: 1 }\n    ],\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS IAM Authentication for KafkaJS\nDESCRIPTION: Sets up AWS IAM authentication for Kafka. This shows how to configure authentication using AWS credentials including authorization identity, access key, secret access key, and optional session token.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Configuration.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  // authenticationTimeout: 1000,\n  // reauthenticationThreshold: 10000,\n  ssl: true,\n  sasl: {\n    mechanism: 'aws',\n    authorizationIdentity: 'AIDAIOSFODNN7EXAMPLE', // UserId or RoleId\n    accessKeyId: 'AKIAIOSFODNN7EXAMPLE',\n    secretAccessKey: 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',\n    sessionToken: 'WHArYt8i5vfQUrIU5ZbMLCbjcAiv/Eww6eL9tgQMJp6QFNEXAMPLETOKEN' // Optional\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Altering Configs with KafkaJS Admin Client\nDESCRIPTION: Method to update configuration for specified Kafka resources with options to validate without applying changes.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Admin.md#2025-04-14_snippet_20\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.alterConfigs({\n    validateOnly: false,\n    resources: <ResourceConfig[]>\n})\n```\n\n----------------------------------------\n\nTITLE: Deleting Topics with KafkaJS Admin Client\nDESCRIPTION: Shows how to delete topics using the deleteTopics method. Includes options for specifying topics and timeout.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Admin.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.deleteTopics({\n    topics: <String[]>,\n    timeout: <Number>,\n})\n```\n\n----------------------------------------\n\nTITLE: Fetching Topic Metadata\nDESCRIPTION: Retrieves metadata for specified topics including partition information, leaders, and replicas.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Admin.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.fetchTopicMetadata({ topics: <Array<String>> })\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    topics: <Array<TopicMetadata>>,\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    topic: <String>,\n    partitions: <Array<PartitionMetadata>> // default: 1\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    partitionErrorCode: <Number>, // default: 0\n    partitionId: <Number>,\n    leader: <Number>,\n    replicas: <Array<Number>>,\n    isr: <Array<Number>>,\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Winston Logger with KafkaJS\nDESCRIPTION: Complete example showing how to create a custom log creator using Winston, including log level conversion and configuring transports for console and file output.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/CustomLogger.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst { logLevel } = require('kafkajs')\nconst winston = require('winston')\nconst toWinstonLogLevel = level => switch(level) {\n    case logLevel.ERROR:\n    case logLevel.NOTHING:\n        return 'error'\n    case logLevel.WARN:\n        return 'warn'\n    case logLevel.INFO:\n        return 'info'\n    case logLevel.DEBUG:\n        return 'debug'\n}\n\nconst WinstonLogCreator = logLevel => {\n    const logger = winston.createLogger({\n        level: toWinstonLogLevel(logLevel),\n        transports: [\n            new winston.transports.Console(),\n            new winston.transports.File({ filename: 'myapp.log' })\n        ]\n    })\n\n    return ({ namespace, level, label, log }) => {\n        const { message, ...extra } = log\n        logger.log({\n            level: toWinstonLogLevel(level),\n            message,\n            extra,\n        })\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Partitioner in KafkaJS\nDESCRIPTION: Explains how to create and use a custom partitioner function to control message partition assignment in Kafka. The example shows the structure of a custom partitioner and how to assign it to a producer.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.15.0/Producing.md#2025-04-14_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst MyPartitioner = () => {\n    return ({ topic, partitionMetadata, message }) => {\n        // select a partition based on some logic\n        // return the partition number\n        return 0\n    }\n}\n\nkafka.producer({ createPartitioner: MyPartitioner })\n```\n\n----------------------------------------\n\nTITLE: Concurrent Partition Processing\nDESCRIPTION: Configuration for concurrent processing of messages from multiple partitions while maintaining order within each partition.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Consuming.md#2025-04-14_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconsumer.run({\n    partitionsConsumedConcurrently: 3, // Default: 1\n    eachMessage: async ({ topic, partition, message }) => {\n        // This will be called up to 3 times concurrently\n    },\n})\n```\n\n----------------------------------------\n\nTITLE: KafkaJS Instrumentation Event Structure\nDESCRIPTION: The structure of instrumentation events in KafkaJS. Events contain an ID, type, timestamp, and a payload object with event-specific data.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/InstrumentationEvents.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  id: <Number>,\n  type: <String>,\n  timestamp: <Number>,\n  payload: <Object>\n}\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Multiple Kafka Topics Using RegExp\nDESCRIPTION: Demonstrates how to subscribe to multiple topics at once using a regular expression pattern.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Consuming.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nawait consumer.connect()\nawait consumer.subscribe({ topic: /topic-(eu|us)-.*/i })\n```\n\n----------------------------------------\n\nTITLE: KafkaJS Instrumentation Event Structure (JavaScript)\nDESCRIPTION: This snippet outlines the structure of a KafkaJS instrumentation event. It includes fields for id, type, timestamp, and payload, which are common to all instrumentation events.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/InstrumentationEvents.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  id: <Number>,\n  type: <String>,\n  timestamp: <Number>,\n  payload: <Object>\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Socket Factory in KafkaJS\nDESCRIPTION: Example of creating a custom socket factory function that implements custom TTL settings and supports both SSL and non-SSL connections. The socket factory must return an object compatible with net.Socket.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Configuration.md#2025-04-14_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Kafka } = require('kafkajs')\n\n// Example socket factory setting a custom TTL\nconst net = require('net')\nconst tls = require('tls')\n\nconst myCustomSocketFactory = ({ host, port, ssl, onConnect }) => {\n  const socket = ssl\n    ? tls.connect(\n        Object.assign({ host, port }, ssl),\n        onConnect\n      )\n    : net.connect(\n        { host, port },\n        onConnect\n      )\n\n  socket.setKeepAlive(true, 30000)\n\n  return socket\n}\n\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  socketFactory: myCustomSocketFactory,\n})\n```\n\n----------------------------------------\n\nTITLE: Offset Topic Structure Definition\nDESCRIPTION: Defines the data structure required for specifying topics and their partitions when sending offsets in a transaction.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Transactions.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n[{\n  topic: <String>,\n  partitions: [{\n    partition: <Number>,\n    offset: <String>\n  }]\n}]\n```\n\n----------------------------------------\n\nTITLE: Topics Structure for Sending Offsets in KafkaJS\nDESCRIPTION: Defines the data structure required for the topics parameter when sending offsets within a transaction. The structure includes topic names and arrays of partition/offset pairs that should be included in the transaction.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Transactions.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n[{\n  topic: <String>,\n  partitions: [{\n    partition: <Number>,\n    offset: <String>\n  }]\n}]\n```\n\n----------------------------------------\n\nTITLE: Defining Topics Structure for KafkaJS Transaction Offsets in JavaScript\nDESCRIPTION: Illustrates the structure of the 'topics' object used in the sendOffsets method of a KafkaJS transaction. This structure specifies the topic, partitions, and offsets to be included in the transaction.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Transactions.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n[{\n  topic: <String>,\n  partitions: [{\n    partition: <Number>,\n    offset: <String>\n  }]\n}]\n```\n\n----------------------------------------\n\nTITLE: Configuring Default (Java Compatible) Partitioner in KafkaJS v2.0.0\nDESCRIPTION: Shows how to explicitly configure the new default partitioner, which is compatible with the Java partitioner, in KafkaJS v2.0.0.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/MigrationGuide-2-0-0.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// Rely on the default partitioner being compatible with the Java partitioner\nkafka.producer()\n\n// Or explicitly use the default partitioner\nconst { Partitioners } = require('kafkajs')\nkafka.producer({ createPartitioner: Partitioners.DefaultPartitioner })\n```\n\n----------------------------------------\n\nTITLE: Using Custom Compression Codec in KafkaJS Producer\nDESCRIPTION: This snippet demonstrates how to use a custom compression codec (in this case, Snappy) when sending messages with a KafkaJS producer.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/Producing.md#2025-04-14_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nawait producer.send({\n    topic: 'topic-name',\n    compression: CompressionTypes.Snappy,\n    messages: [\n        { key: 'key1', value: 'hello world' },\n        { key: 'key2', value: 'hey hey!' }\n    ],\n})\n```\n\n----------------------------------------\n\nTITLE: Overriding Log Levels for KafkaJS Components\nDESCRIPTION: Demonstrates how to override log levels after instantiation for individual components like the Kafka client, producer, consumer, and admin. Each component's logger can have its own log level.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Configuration.md#2025-04-14_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Kafka, logLevel } = require('kafkajs')\n\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  logLevel: logLevel.ERROR\n})\nkafka.logger().setLogLevel(logLevel.WARN)\n\nconst producer = kafka.producer(...)\nproducer.logger().setLogLevel(logLevel.INFO)\n\nconst consumer = kafka.consumer(...)\nconsumer.logger().setLogLevel(logLevel.DEBUG)\n\nconst admin = kafka.admin(...)\nadmin.logger().setLogLevel(logLevel.NOTHING)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Partitioner in KafkaJS\nDESCRIPTION: Shows how to create and use a custom partitioner function for message distribution in KafkaJS.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Producing.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst MyPartitioner = () => {\n    return ({ topic, partitionMetadata, message }) => {\n        // select a partition based on some logic\n        // return the partition number\n        return 0\n    }\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\nkafka.producer({ createPartitioner: MyPartitioner })\n```\n\n----------------------------------------\n\nTITLE: Partition-Level Pause and Resume in KafkaJS\nDESCRIPTION: Shows how to implement granular pause/resume control at the partition level, useful for concurrent processing and handling partition-specific throttling.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Consuming.md#2025-04-14_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nconsumer.run({\n    partitionsConsumedConcurrently: 3, // Default: 1\n    eachMessage: async ({ topic, partition, message }) => {\n      // This will be called up to 3 times concurrently\n        try {\n            await sendToDependency(message)\n        } catch (e) {\n            if (e instanceof TooManyRequestsError) {\n                consumer.pause([{ topic, partitions: [partition] }])\n                // Other partitions will keep fetching and processing, until if / when\n                // they also get throttled\n                setTimeout(() => {\n                    consumer.resume([{ topic, partitions: [partition] }])\n                    // Other partitions that are paused will continue to be paused\n                }, e.retryAfter * 1000)\n            }\n\n            throw e\n        }\n    },\n})\n```\n\n----------------------------------------\n\nTITLE: Accessing Loggers After KafkaJS Instance Creation\nDESCRIPTION: Demonstrates how to access namespaced loggers for Kafka client, consumer, producer, and admin after instantiation using the logger() method.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/CustomLogger.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst client = new Kafka( ... )\nclient.logger().info( ... )\n\nconst consumer = kafka.consumer( ... )\nconsumer.logger().info( ... )\n\nconst producer = kafka.producer( ... )\nproducer.logger().info( ... )\n\nconst admin = kafka.admin( ... )\nadmin.logger().info( ... )\n```\n\n----------------------------------------\n\nTITLE: Auto-commit Configuration Examples\nDESCRIPTION: Examples of configuring auto-commit behavior using interval and threshold settings.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Consuming.md#2025-04-14_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconsumer.run({\n  autoCommitInterval: 5000,\n  // ...\n})\n\nconsumer.run({\n  autoCommitThreshold: 100,\n  // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring LZ4 Compression in KafkaJS Producer\nDESCRIPTION: This code demonstrates how to set up LZ4 compression for use with KafkaJS. It involves installing the kafkajs-lz4 package, importing the necessary modules, and registering the LZ4 codec with KafkaJS.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.15.0/Producing.md#2025-04-14_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst { CompressionTypes, CompressionCodecs } = require('kafkajs')\nconst LZ4 = require('kafkajs-lz4')\n\nCompressionCodecs[CompressionTypes.LZ4] = new LZ4().codec\n```\n\n----------------------------------------\n\nTITLE: Managing Consumer Groups\nDESCRIPTION: Methods for listing, describing, and deleting consumer groups. Includes operations to view group details and remove inactive consumer groups.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/Admin.md#2025-04-14_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.listGroups()\n```\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.describeGroups([ 'testgroup' ])\n```\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.deleteGroups(['group-test'])\n```\n\n----------------------------------------\n\nTITLE: Configuring Legacy Partitioner in KafkaJS v2.0.0\nDESCRIPTION: Demonstrates how to use the LegacyPartitioner to maintain the same behavior as the previous default partitioner in KafkaJS v2.0.0.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/MigrationGuide-2-0-0.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Partitioners } = require('kafkajs')\nkafka.producer({ createPartitioner: Partitioners.LegacyPartitioner })\n```\n\n----------------------------------------\n\nTITLE: Initializing KafkaJS Admin Client\nDESCRIPTION: Creates a new Kafka instance and initializes the admin client. Demonstrates how to connect and disconnect from the admin client.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Admin.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst kafka = new Kafka(...)\nconst admin = kafka.admin()\n\n// remember to connect and disconnect when you are done\nawait admin.connect()\nawait admin.disconnect()\n```\n\n----------------------------------------\n\nTITLE: Sending GZIP Compressed Messages with KafkaJS Producer\nDESCRIPTION: This code shows how to send GZIP compressed messages using the KafkaJS producer. It uses the CompressionTypes.GZIP option when calling the send method.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Producing.md#2025-04-14_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst { CompressionTypes } = require('kafkajs')\n\nasync () => {\n  await producer.send({\n    topic: 'topic-name',\n    compression: CompressionTypes.GZIP,\n    messages: [\n        { key: 'key1', value: 'hello world' },\n        { key: 'key2', value: 'hey hey!' }\n    ],\n  })\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka and Zookeeper with Docker Compose\nDESCRIPTION: This Docker Compose configuration sets up a Kafka broker and Zookeeper for development purposes. It exposes necessary ports, configures environment variables, and sets up volumes for Docker socket access.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/DockerLocal.md#2025-04-14_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nversion: '2'\nservices:\n  zookeeper:\n    image: wurstmeister/zookeeper:latest\n    ports:\n      - \"2181:2181\"\n  kafka:\n    image: wurstmeister/kafka:2.11-1.1.1\n    ports:\n      - \"9092:9092\"\n    links:\n      - zookeeper\n    environment:\n      KAFKA_ADVERTISED_HOST_NAME: ${HOST_IP}\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'\n      KAFKA_DELETE_TOPIC_ENABLE: 'true'\n      KAFKA_CREATE_TOPICS: \"topic-test:1:1\"\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n```\n\n----------------------------------------\n\nTITLE: Configuring JavaCompatiblePartitioner in KafkaJS Producer\nDESCRIPTION: This snippet demonstrates how to import and use the JavaCompatiblePartitioner when creating a KafkaJS producer. This partitioner is compatible with the default partitioner in the Java Kafka client, which can be important for meeting co-partitioning requirements when joining multiple topics.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.15.0/Producing.md#2025-04-14_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Partitioners } = require('kafkajs')\nkafka.producer({ createPartitioner: Partitioners.JavaCompatiblePartitioner })\n```\n\n----------------------------------------\n\nTITLE: Seeking to Specific Offset in KafkaJS Consumer\nDESCRIPTION: Demonstrates how to use the seek method to move the offset position in a topic/partition. This method should be called after the consumer is initialized and running.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Consuming.md#2025-04-14_snippet_15\n\nLANGUAGE: javascript\nCODE:\n```\nawait consumer.connect()\nawait consumer.subscribe({ topics: ['example'] })\n\n// you don't need to await consumer#run\nconsumer.run({ eachMessage: async ({ topic, message }) => true })\nconsumer.seek({ topic: 'example', partition: 0, offset: 12384 })\n```\n\n----------------------------------------\n\nTITLE: Accessing Loggers After Client Instantiation\nDESCRIPTION: Demonstrates how to access the namespaced logger of a consumer, producer, admin, or root Kafka client after they have been created.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/CustomLogger.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst client = new Kafka( ... )\nclient.logger().info( ... )\n\nconst consumer = kafka.consumer( ... )\nconsumer.logger().info( ... )\n\nconst producer = kafka.producer( ... )\nproducer.logger().info( ... )\n\nconst admin = kafka.admin( ... )\nadmin.logger().info( ... )\n```\n\n----------------------------------------\n\nTITLE: Configuring AutoCommit Interval in KafkaJS Consumer\nDESCRIPTION: Demonstrates how to set up automatic committing of offsets at a specified time interval using the autoCommitInterval option.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Consuming.md#2025-04-14_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconsumer.run({\n  autoCommitInterval: 5000,\n  // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Specifying Custom Kafka Version with COMPOSE_FILE\nDESCRIPTION: Shell command demonstrating how to use a different version of Kafka by specifying a custom docker-compose file through the COMPOSE_FILE environment variable.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/DevelopmentEnvironment.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nCOMPOSE_FILE=\"docker-compose.2_3.yml\" ./scripts/dockerComposeUp.sh\n```\n\n----------------------------------------\n\nTITLE: KafkaJS Consumer with SSL and SASL Authentication\nDESCRIPTION: Example of configuring a Kafka consumer with SSL certificate validation and SASL authentication using scram-sha-256 mechanism. Includes error handling and graceful shutdown.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/ConsumerExample.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst ip = require('ip')\n\nconst { Kafka, logLevel } = require('../index')\n\nconst host = process.env.HOST_IP || ip.address()\n\nconst kafka = new Kafka({\n  logLevel: logLevel.INFO,\n  brokers: [`${host}:9094`],\n  clientId: 'example-consumer',\n  ssl: {\n    rejectUnauthorized: true\n  },\n  sasl: {\n    mechanism: 'scram-sha-256',\n    username: 'test',\n    password: 'testtest',\n  },\n})\n\nconst topic = 'topic-test'\nconst consumer = kafka.consumer({ groupId: 'test-group' })\n\nconst run = async () => {\n  await consumer.connect()\n  await consumer.subscribe({ topic, fromBeginning: true })\n  await consumer.run({\n    // eachBatch: async ({ batch }) => {\n    //   console.log(batch)\n    // },\n    eachMessage: async ({ topic, partition, message }) => {\n      const prefix = `${topic}[${partition} | ${message.offset}] / ${message.timestamp}`\n      console.log(`- ${prefix} ${message.key}#${message.value}`)\n    },\n  })\n}\n\nrun().catch(e => console.error(`[example/consumer] ${e.message}`, e))\n\nconst errorTypes = ['unhandledRejection', 'uncaughtException']\nconst signalTraps = ['SIGTERM', 'SIGINT', 'SIGUSR2']\n\nerrorTypes.forEach(type => {\n  process.on(type, async e => {\n    try {\n      console.log(`process.on ${type}`)\n      console.error(e)\n      await consumer.disconnect()\n      process.exit(0)\n    } catch (_) {\n      process.exit(1)\n    }\n  })\n})\n\nsignalTraps.forEach(type => {\n  process.once(type, async () => {\n    try {\n      await consumer.disconnect()\n    } finally {\n      process.kill(process.pid, type)\n    }\n  })\n})\n```\n\n----------------------------------------\n\nTITLE: Disabling Request Timeout in KafkaJS\nDESCRIPTION: Example of how to disable the request timeout mechanism which is now enabled by default.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/MigrationGuide-2-0-0.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({ enforceRequestTimeout: false })\n```\n\n----------------------------------------\n\nTITLE: Configuring 'fromBeginning' Option in KafkaJS Consumer\nDESCRIPTION: Shows how to set the 'fromBeginning' option when subscribing to topics. This option determines whether the consumer should start reading from the earliest or latest offset when no valid offset is found.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Consuming.md#2025-04-14_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nawait consumer.subscribe({ topics: ['test-topic'], fromBeginning: true })\nawait consumer.subscribe({ topics: ['other-topic'], fromBeginning: false })\n```\n\n----------------------------------------\n\nTITLE: Configuring LZ4 Compression for KafkaJS Producer\nDESCRIPTION: This code demonstrates how to set up LZ4 compression for KafkaJS. It requires installing the kafkajs-lz4 package and then configuring the LZ4 codec in the CompressionCodecs object.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Producing.md#2025-04-14_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst { CompressionTypes, CompressionCodecs } = require('kafkajs')\nconst LZ4 = require('kafkajs-lz4')\n\nCompressionCodecs[CompressionTypes.LZ4] = new LZ4().codec\n```\n\n----------------------------------------\n\nTITLE: Configuring Snappy Compression for KafkaJS\nDESCRIPTION: This snippet shows how to configure Snappy compression in KafkaJS. It imports the necessary modules and assigns the Snappy codec to the CompressionCodecs object.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Producing.md#2025-04-14_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst {  CompressionTypes, CompressionCodecs } = require('kafkajs')\nconst SnappyCodec = require('kafkajs-snappy')\n\nCompressionCodecs[CompressionTypes.Snappy] = SnappyCodec\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Partition Assigner in KafkaJS\nDESCRIPTION: Demonstrates how to create a custom partition assigner for KafkaJS. It includes the basic structure of the assigner function and shows how to implement the required 'assign' and 'protocol' methods.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Consuming.md#2025-04-14_snippet_15\n\nLANGUAGE: javascript\nCODE:\n```\nconst { AssignerProtocol: { MemberAssignment, MemberMetadata } } = require('kafkajs')\n\nconst MyPartitionAssigner = ({ cluster }) => ({\n    name: 'MyPartitionAssigner',\n    version: 1,\n    async assign({ members, topics }) {\n        // perform assignment\n        return myCustomAssignmentArray.map(memberId => ({\n            memberId,\n            memberAssignment: MemberAssignment.encode({\n                version: this.version,\n                assignment: assignment[memberId],\n            })\n        }))\n    },\n    protocol({ topics }) {\n        return {\n            name: this.name,\n            metadata: MemberMetadata.encode({\n            version: this.version,\n            topics,\n            }),\n        }\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Listing Kafka Topics\nDESCRIPTION: Lists all existing topics in the Kafka cluster. Returns an array of topic names.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/Admin.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.listTopics()\n// [ 'topic-1', 'topic-2', 'topic-3', ... ]\n```\n\n----------------------------------------\n\nTITLE: Setting Log Level for KafkaJS Client\nDESCRIPTION: Configures the logging level for the KafkaJS client. Available levels are NOTHING, ERROR, WARN, INFO, and DEBUG. The default is INFO. This example sets the log level to ERROR.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Configuration.md#2025-04-14_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Kafka, logLevel } = require('kafkajs')\n\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  logLevel: logLevel.ERROR\n})\n```\n\n----------------------------------------\n\nTITLE: Using Pause Function in eachMessage Callback in KafkaJS Consumer\nDESCRIPTION: Demonstrates the use of the convenience pause function provided in the eachMessage callback to pause the specific topic-partition of the current message being processed.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Consuming.md#2025-04-14_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nawait consumer.connect()\nawait consumer.subscribe({ topics: ['jobs'] })\n\nawait consumer.run({ eachMessage: async ({ topic, message, pause }) => {\n    try {\n        await sendToDependency(message)\n    } catch (e) {\n        if (e instanceof TooManyRequestsError) {\n            const resumeThisPartition = pause()\n            // Other partitions that are paused will continue to be paused\n            setTimeout(resumeThisPartition, e.retryAfter * 1000)\n        }\n\n        throw e\n    }\n}})\n```\n\n----------------------------------------\n\nTITLE: Initializing Kafka Producer with Options\nDESCRIPTION: Creates a Kafka producer with custom configuration options including auto topic creation and transaction timeout settings.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Producing.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer({\n    allowAutoTopicCreation: false,\n    transactionTimeout: 30000\n})\n```\n\n----------------------------------------\n\nTITLE: Altering Kafka Configs with KafkaJS Admin API\nDESCRIPTION: This snippet shows how to use the alterConfigs method to update the configuration for specified resources. It includes the structure for ResourceConfig and ResourceConfigEntry, and provides an example of updating a topic's cleanup policy.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Admin.md#2025-04-14_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.alterConfigs({\n    validateOnly: false,\n    resources: <ResourceConfig[]>\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst { ConfigResourceTypes } = require('kafkajs')\n\nawait admin.alterConfigs({\n    resources: [{\n        type: ConfigResourceTypes.TOPIC,\n        name: 'topic-name',\n        configEntries: [{ name: 'cleanup.policy', value: 'compact' }]\n    }]\n})\n```\n\n----------------------------------------\n\nTITLE: Initializing Basic Kafka Producer in JavaScript\nDESCRIPTION: Creates a basic Kafka producer instance without any custom options.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Producing.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n```\n\n----------------------------------------\n\nTITLE: KafkaJS Log Format Structure\nDESCRIPTION: Shows the format of the log objects in KafkaJS, which includes level, label, timestamp, logger, message, and any additional custom keys provided to the log function.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/CustomLogger.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    level: 4,\n    label: 'INFO', // NOTHING, ERROR, WARN, INFO, or DEBUG\n    timestamp: '2017-12-29T13:39:54.575Z',\n    logger: 'kafkajs',\n    message: 'Started',\n    // ... any other extra key provided to the log function\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Topics Structure for Sending Offsets in KafkaJS Transactions\nDESCRIPTION: This code defines the structure of the 'topics' object used when sending offsets in a KafkaJS transaction. It specifies the format for topic, partition, and offset information.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/Transactions.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n[{\n  topic: <String>,\n  partitions: [{\n    partition: <Number>,\n    offset: <String>\n  }]\n}]\n```\n\n----------------------------------------\n\nTITLE: Using Java Compatible Partitioner\nDESCRIPTION: Configuration for using the Java-compatible partitioner implementation.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Producing.md#2025-04-14_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Partitioners } = require('kafkajs')\nkafka.producer({ createPartitioner: Partitioners.JavaCompatiblePartitioner })\n```\n\n----------------------------------------\n\nTITLE: Dynamic Broker Discovery Configuration\nDESCRIPTION: Implementation of dynamic broker discovery using an async function to fetch broker information from Confluent REST Proxy. Used when static broker configuration isn't suitable.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.14.0/Configuration.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: async () => {\n    // Example getting brokers from Confluent REST Proxy\n    const clusterResponse = await fetch('https://kafka-rest:8082/v3/clusters', {\n      headers: 'application/vnd.api+json',\n    }).then(response => response.json())\n    const clusterUrl = clusterResponse.data[0].links.self\n\n    const brokersResponse = await fetch(`${clusterUrl}/brokers`, {\n      headers: 'application/vnd.api+json',\n    }).then(response => response.json())\n\n    const brokers = brokersResponse.data.map(broker => {\n      const { host, port } = broker.attributes\n      return `${host}:${port}`\n    })\n\n    return brokers\n  }\n})\n```\n\n----------------------------------------\n\nTITLE: Listing Topics with KafkaJS Admin Client\nDESCRIPTION: Uses the listTopics method to retrieve an array of all existing topic names. Throws exceptions in case of errors.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Admin.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.listTopics()\n// [ 'topic-1', 'topic-2', 'topic-3', ... ]\n```\n\n----------------------------------------\n\nTITLE: Setting KafkaJS Log Level via Environment Variable\nDESCRIPTION: Example of setting the KafkaJS log level using an environment variable, which takes precedence over programmatic configuration.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.14.0/Configuration.md#2025-04-14_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\nKAFKAJS_LOG_LEVEL=info node code.js\n```\n\n----------------------------------------\n\nTITLE: Configuring a KafkaJS Producer for Transactions\nDESCRIPTION: Creates a transactional Kafka client and producer with the required configuration for exactly-once semantics. The producer is configured with maxInFlightRequests set to 1 and idempotent set to true.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Transactions.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst client = new Kafka({\n  clientId: 'transactional-client',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n})\nconst producer = client.producer({ maxInFlightRequests: 1, idempotent: true })\n```\n\n----------------------------------------\n\nTITLE: Fetching Metadata for All Topics in KafkaJS\nDESCRIPTION: Example showing how to fetch metadata for all Kafka topics by omitting the topics argument in fetchTopicMetadata method.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Admin.md#2025-04-14_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.fetchTopicMetadata()\n```\n\n----------------------------------------\n\nTITLE: Creating Kafka ACLs with KafkaJS Admin API\nDESCRIPTION: This snippet demonstrates how to use the createAcls method to set up Access Control Lists (ACLs) for Kafka resources. It includes examples of denying and allowing access to a topic for different users.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Admin.md#2025-04-14_snippet_19\n\nLANGUAGE: javascript\nCODE:\n```\nconst {\n  AclResourceTypes,\n  AclOperationTypes,\n  AclPermissionTypes,\n  ResourcePatternTypes,\n} = require('kafkajs')\n\nconst acl = [\n  {\n    resourceType: AclResourceTypes.TOPIC,\n    resourceName: 'topic-name',\n    resourcePatternType: ResourcePatternTypes.LITERAL,\n    principal: 'User:bob',\n    host: '*',\n    operation: AclOperationTypes.ALL,\n    permissionType: AclPermissionTypes.DENY,\n  },\n  {\n    resourceType: AclResourceTypes.TOPIC,\n    resourceName: 'topic-name',\n    resourcePatternType: ResourcePatternTypes.LITERAL,\n    principal: 'User:alice',\n    host: '*',\n    operation: AclOperationTypes.ALL,\n    permissionType: AclPermissionTypes.ALLOW,\n  },\n]\n\nawait admin.createAcls({ acl })\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Partition Assigner in KafkaJS\nDESCRIPTION: Shows how to configure a consumer with custom partition assigner while maintaining compatibility with default round-robin assigner.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Consuming.md#2025-04-14_snippet_18\n\nLANGUAGE: javascript\nCODE:\n```\nconst { PartitionAssigners: { roundRobin } } = require('kafkajs')\n\nkafka.consumer({\n    groupId: 'my-group',\n    partitionAssigners: [\n        MyPartitionAssigner,\n        roundRobin\n    ]\n})\n```\n\n----------------------------------------\n\nTITLE: Basic KafkaJS Custom Authentication Configuration\nDESCRIPTION: Basic structure for configuring a custom authentication mechanism in KafkaJS. Shows the required configuration object structure.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.4/CustomAuthenticationMechanism.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n{ \n  sasl: { \n      mechanism: <mechanism name>,\n      authenticationProvider: ({ host, port, logger, saslAuthenticate }) => { authenticate: () => Promise<void> }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Duplicate Header Keys in KafkaJS v2.0.0\nDESCRIPTION: Demonstrates how to adapt code to handle potentially duplicate header keys, which are now returned as arrays in KafkaJS v2.0.0.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/MigrationGuide-2-0-0.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n// Before\nconst participants = message.headers[\"participants\"].toString()\n\n// After\nconst participants = Array.isArray(message.headers[\"participants\"])\n    ? message.headers[\"participants\"].map(participant => participant.toString()).join(\", \")\n    : message.headers[\"participants\"].toString()\n```\n\n----------------------------------------\n\nTITLE: Configuring Legacy Partitioner in KafkaJS\nDESCRIPTION: Example showing how to maintain the legacy partitioning behavior by explicitly configuring the LegacyPartitioner.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/MigrationGuide-2-0-0.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Partitioners } = require('kafkajs')\nkafka.producer({ createPartitioner: Partitioners.LegacyPartitioner })\n```\n\n----------------------------------------\n\nTITLE: Seeking to Specific Offset in KafkaJS\nDESCRIPTION: Shows how to move the offset position in a topic/partition using the seek method after consumer initialization.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Consuming.md#2025-04-14_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\nawait consumer.connect()\nawait consumer.subscribe({ topic: 'example' })\n\n// you don't need to await consumer#run\nconsumer.run({ eachMessage: async ({ topic, message }) => true })\nconsumer.seek({ topic: 'example', partition: 0, offset: 12384 })\n```\n\n----------------------------------------\n\nTITLE: Fetching Topic Metadata in KafkaJS\nDESCRIPTION: Method to retrieve metadata for specific Kafka topics or all topics if no topics are specified. The admin client will throw an exception if any of the provided topics don't exist.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Admin.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.fetchTopicMetadata({ topics: <Array<String> })\n```\n\n----------------------------------------\n\nTITLE: Setting Request Timeout in KafkaJS\nDESCRIPTION: This example demonstrates how to configure a custom request timeout for Kafka operations. The timeout is specified in milliseconds and can be disabled using the enforceRequestTimeout option.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Configuration.md#2025-04-14_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  requestTimeout: 25000\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  enforceRequestTimeout: false\n})\n```\n\n----------------------------------------\n\nTITLE: Initializing Transactional Kafka Producer\nDESCRIPTION: Configures a Kafka producer client with necessary settings for transactional operations, including maxInFlightRequests and idempotent mode to guarantee exactly-once-semantics.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Transactions.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst client = new Kafka({\n  clientId: 'transactional-client',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n})\nconst producer = client.producer({ maxInFlightRequests: 1, idempotent: true })\n```\n\n----------------------------------------\n\nTITLE: Configuring Snappy Compression in KafkaJS\nDESCRIPTION: Demonstrates how to set up and use Snappy compression with KafkaJS. Requires the additional package 'kafkajs-snappy'.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Producing.md#2025-04-14_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst {  CompressionTypes, CompressionCodecs } = require('kafkajs')\nconst SnappyCodec = require('kafkajs-snappy')\n\nCompressionCodecs[CompressionTypes.Snappy] = SnappyCodec\n```\n\n----------------------------------------\n\nTITLE: Fetching Offsets for Multiple Topics in KafkaJS v2.0.0\nDESCRIPTION: Shows how to adapt code to fetch offsets for multiple topics using the updated fetchOffsets method in KafkaJS v2.0.0.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/MigrationGuide-2-0-0.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n// Before\nconst partitions = await admin.fetchOffsets({ groupId, topic: 'topic-a' })\nfor (const { partition, offset } of partitions) {\n    admin.logger().info(`${groupId} is at offset ${offset} of partition ${partition}`)\n}\n\n// After\nconst topics = await admin.fetchOffsets({ groupId, topics: ['topic-a', 'topic-b'] })\nfor (const topic of topics) {\n    for (const { partition, offset } of partitions) {\n        admin.logger().info(`${groupId} is at offset ${offset} of ${topic}:${partition}`)\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Partition Assignment in KafkaJS\nDESCRIPTION: Shows how to implement the assign method for custom partition assignment using MemberAssignment encoding.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Consuming.md#2025-04-14_snippet_16\n\nLANGUAGE: javascript\nCODE:\n```\nconst { AssignerProtocol: { MemberAssignment } } = require('kafkajs')\n\nconst MyPartitionAssigner = ({ cluster }) => ({\n    version: 1,\n    async assign({ members, topics }) {\n        // perform assignment\n        return myCustomAssignmentArray.map(memberId => ({\n            memberId,\n            memberAssignment: MemberAssignment.encode({\n                version: this.version,\n                assignment: assignment[memberId],\n            })\n        }))\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Accessing Logger Instances in KafkaJS Components\nDESCRIPTION: This snippet shows how to access the namespaced logger from various KafkaJS components after instantiation, including the client, consumer, producer, and admin interfaces.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/CustomLogger.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst client = new Kafka( ... )\nclient.logger().info( ... )\n\nconst consumer = kafka.consumer( ... )\nconsumer.logger().info( ... )\n\nconst producer = kafka.producer( ... )\nproducer.logger().info( ... )\n\nconst admin = kafka.admin( ... )\nadmin.logger().info( ... )\n```\n\n----------------------------------------\n\nTITLE: Disabling Request Timeout Enforcement in KafkaJS v2.0.0\nDESCRIPTION: Illustrates how to disable the now-default request timeout enforcement mechanism in KafkaJS v2.0.0.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/MigrationGuide-2-0-0.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({ enforceRequestTimeout: false })\n```\n\n----------------------------------------\n\nTITLE: Setting Connection Timeout for KafkaJS Client\nDESCRIPTION: Configures the connection timeout in milliseconds for the KafkaJS client. The default value is 1000ms, but this example sets it to 3000ms.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Configuration.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  connectionTimeout: 3000\n})\n```\n\n----------------------------------------\n\nTITLE: RegExp Topic Subscription\nDESCRIPTION: Demonstrates subscribing to multiple topics using a regular expression pattern.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Consuming.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nawait consumer.connect()\nawait consumer.subscribe({ topic: /topic-(eu|us)-.*/i })\n```\n\n----------------------------------------\n\nTITLE: Fetching Consumer Group Offsets in KafkaJS\nDESCRIPTION: Method to retrieve the current offsets for a consumer group on a specified topic, showing the current position for each partition.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Admin.md#2025-04-14_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.fetchOffsets({ groupId, topic })\n// [\n//   { partition: 0, offset: '31004' },\n//   { partition: 1, offset: '54312' },\n//   { partition: 2, offset: '32103' },\n//   { partition: 3, offset: '28' },\n// ]\n```\n\n----------------------------------------\n\nTITLE: Creating a KafkaJS Producer\nDESCRIPTION: Demonstrates how to create a basic KafkaJS producer instance.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Producing.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n```\n\n----------------------------------------\n\nTITLE: Sending Messages in a KafkaJS Transaction\nDESCRIPTION: Demonstrates how to create a transaction, send messages within it, and either commit or abort the transaction based on the success or failure of operations. If aborted, all messages sent within the transaction will be rolled back.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Transactions.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst transaction = await producer.transaction()\n\ntry {\n  await transaction.send({ topic, messages })\n\n  await transaction.commit()\n} catch (e) {\n  await transaction.abort()\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Compression Codec in KafkaJS Producer\nDESCRIPTION: This code demonstrates how to implement a custom compression codec for use with KafkaJS. It defines a codec object with compress and decompress functions, and shows how to register the custom codec with KafkaJS.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.15.0/Producing.md#2025-04-14_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nconst MyCustomSnappyCodec = {\n    async compress(encoder) {\n        return someCompressFunction(encoder.buffer)\n    },\n\n    async decompress(buffer) {\n        return someDecompressFunction(buffer)\n    }\n}\n\nconst { CompressionTypes, CompressionCodecs } = require('kafkajs')\nCompressionCodecs[CompressionTypes.Snappy] = MyCustomSnappyCodec\n```\n\n----------------------------------------\n\nTITLE: Example Simon Authentication Implementation\nDESCRIPTION: Complete example of a custom authentication mechanism implementation called 'simon' that demonstrates request encoding, response decoding, and authentication flow handling.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.0/CustomAuthenticationMechanism.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst simonAuthenticator = says = ({ host, port, logger, saslAuthenticate }) => {\n    const INT32_SIZE = 4\n  \n    const request = {\n      encode: () => {\n        const byteLength = Buffer.byteLength(says, 'utf8')\n        const buf = Buffer.alloc(INT32_SIZE + byteLength)\n        buf.writeUInt32BE(byteLength, 0)\n        buf.write(says, INT32_SIZE, byteLength, 'utf8')\n        return buf\n      },\n    }\n    const response = {\n      decode: rawData => {\n        const byteLength = rawData.readInt32BE(0)\n        return rawData.slice(INT32_SIZE, INT32_SIZE + byteLength)\n      },\n      parse: data => {\n        return data.toString()\n      },\n    }\n    return {\n      authenticate: async () => {\n        if (says == null) {\n          throw new Error('SASL Simon: Invalid \"says\"')\n        }\n        const broker = `${host}:${port}`\n        try {\n          logger.info('Authenticate with SASL Simon', { broker })\n          const authenticateResponse = await saslAuthenticate({ request, response })\n  \n          const saidSimon = says.startsWith(\"Simon says \")\n          const expectedResponse = saidSimon ? says : \"\"\n          if (authenticateResponse !== expectedResponse) {\n              throw new Error(\"Mismatching response from broker\")\n          }\n          logger.info('SASL Simon authentication successful', { broker })\n        } catch (e) {\n          const error = new Error(\n            `SASL Simon authentication failed: ${e.message}`\n          )\n          logger.error(error.message, { broker })\n          throw error\n        }\n      },\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Paused Topic Partitions in KafkaJS\nDESCRIPTION: Demonstrates how to retrieve the list of paused topic partitions using the consumer's 'paused' method. It iterates through the paused topic partitions and logs the topic and partition information.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Consuming.md#2025-04-14_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nconst pausedTopicPartitions = consumer.paused()\n\nfor (const topicPartitions of pausedTopicPartitions) {\n  const { topic, partitions } = topicPartitions\n  console.log({ topic, partitions })\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Connection Timeout for KafkaJS Client\nDESCRIPTION: Configures the maximum time in milliseconds to wait for a successful connection to the Kafka brokers. The default value is 1000ms (1 second).\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Configuration.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  connectionTimeout: 3000\n})\n```\n\n----------------------------------------\n\nTITLE: Setting Log Level in KafkaJS Configuration\nDESCRIPTION: Demonstrates how to set the log level when initializing a Kafka instance. The example shows setting the log level to ERROR using the logLevel enum.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Configuration.md#2025-04-14_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Kafka, logLevel } = require('kafkajs')\n\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  logLevel: logLevel.ERROR\n})\n```\n\n----------------------------------------\n\nTITLE: Setting Request Timeout for KafkaJS Client\nDESCRIPTION: Configures the request timeout in milliseconds, which is the time to wait for a successful request to the Kafka broker. The default value is 30000ms (30 seconds).\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Configuration.md#2025-04-14_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  requestTimeout: 25000\n})\n```\n\n----------------------------------------\n\nTITLE: Setting up LZ4 Compression in KafkaJS\nDESCRIPTION: Shows how to configure and use LZ4 compression with KafkaJS. Requires the additional package 'kafkajs-lz4'.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Producing.md#2025-04-14_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nconst { CompressionTypes, CompressionCodecs } = require('kafkajs')\nconst LZ4 = require('kafkajs-lz4')\n\nCompressionCodecs[CompressionTypes.LZ4] = new LZ4().codec\n```\n\n----------------------------------------\n\nTITLE: Defining Log Object Structure in JavaScript\nDESCRIPTION: This snippet demonstrates the structure of a log object in KafkaJS, including level, label, timestamp, logger, message, and any additional user-provided data.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/CustomLogger.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    level: 4,\n    label: 'INFO', // NOTHING, ERROR, WARN, INFO, or DEBUG\n    timestamp: '2017-12-29T13:39:54.575Z',\n    logger: 'kafkajs',\n    message: 'Started',\n    // ... any other extra key provided to the log function\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a KafkaJS Producer\nDESCRIPTION: Demonstrates how to create a basic KafkaJS producer instance using the client's producer function.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Producing.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n```\n\n----------------------------------------\n\nTITLE: Sending Offsets in a KafkaJS Transaction in JavaScript\nDESCRIPTION: Shows how to send offsets as part of a transaction using the sendOffsets method. This is crucial for consume-transform-produce patterns where offsets should only be committed if the transaction succeeds.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Transactions.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nawait transaction.sendOffsets({\n  consumerGroupId, topics\n})\n```\n\n----------------------------------------\n\nTITLE: Understanding Log Entry Structure in KafkaJS\nDESCRIPTION: This snippet demonstrates the structure of a log entry object in KafkaJS, showing the standard fields that are available for custom loggers to use including level, label, timestamp, logger, and message.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/CustomLogger.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    level: 4,\n    label: 'INFO', // NOTHING, ERROR, WARN, INFO, or DEBUG\n    timestamp: '2017-12-29T13:39:54.575Z',\n    logger: 'kafkajs',\n    message: 'Started',\n    // ... any other extra key provided to the log function\n}\n```\n\n----------------------------------------\n\nTITLE: Example Authentication Configuration Usage\nDESCRIPTION: Shows how to configure and use the custom Simon Says authentication mechanism in a KafkaJS client configuration.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.4/CustomAuthenticationMechanism.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst config = {\n  sasl: {\n    mechanism: 'simon'\n    authenticationProvider: simonAuthenticator('Simon says authenticate me')\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: KafkaJS Consumer Configuration Options\nDESCRIPTION: Provides a comprehensive list of configuration options available when creating a KafkaJS consumer. These options control various aspects of consumer behavior, including partition assignment, timeouts, and data fetching parameters.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Consuming.md#2025-04-14_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nkafka.consumer({\n  groupId: <String>,\n  partitionAssigners: <Array>,\n  sessionTimeout: <Number>,\n  rebalanceTimeout: <Number>,\n  heartbeatInterval: <Number>,\n  metadataMaxAge: <Number>,\n  allowAutoTopicCreation: <Boolean>,\n  maxBytesPerPartition: <Number>,\n  minBytes: <Number>,\n  maxBytes: <Number>,\n  maxWaitTimeInMs: <Number>,\n  retry: <Object>,\n  maxInFlightRequests: <Number>,\n  rackId: <String>\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Socket Factory for KafkaJS\nDESCRIPTION: Creates a custom socket factory for KafkaJS to allow for specialized socket configurations. This example sets a custom keep-alive timeout on the socket.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Configuration.md#2025-04-14_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Kafka } = require('kafkajs')\n\n// Example socket factory setting a custom TTL\nconst net = require('net')\nconst tls = require('tls')\n\nconst myCustomSocketFactory = ({ host, port, ssl, onConnect }) => {\n  const socket = ssl\n    ? tls.connect(\n        Object.assign({ host, port }, ssl),\n        onConnect\n      )\n    : net.connect(\n        { host, port },\n        onConnect\n      )\n\n  socket.setKeepAlive(true, 30000)\n\n  return socket\n}\n\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  socketFactory: myCustomSocketFactory,\n})\n```\n\n----------------------------------------\n\nTITLE: Dynamically Setting Log Levels in KafkaJS\nDESCRIPTION: Demonstrates how to override log levels after client instantiation using the setLogLevel method. This allows for different log levels for the main client and individual components like producers, consumers, and admins.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Configuration.md#2025-04-14_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Kafka, logLevel } = require('kafkajs')\n\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  logLevel: logLevel.ERROR\n})\nkafka.logger().setLogLevel(logLevel.WARN)\n\nconst producer = kafka.producer(...)\nproducer.logger().setLogLevel(logLevel.INFO)\n\nconst consumer = kafka.consumer(...)\nconsumer.logger().setLogLevel(logLevel.DEBUG)\n\nconst admin = kafka.admin(...)\nadmin.logger().setLogLevel(logLevel.NOTHING)\n```\n\n----------------------------------------\n\nTITLE: Setting Connection Timeout for KafkaJS Client\nDESCRIPTION: Configures the connection timeout in milliseconds, which is the time to wait for a successful connection to the Kafka broker. The default value is 1000ms (1 second).\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Configuration.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  connectionTimeout: 3000\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing ZSTD Compression in KafkaJS\nDESCRIPTION: Demonstrates how to set up and use Zstandard (ZSTD) compression with KafkaJS. Requires the additional package '@kafkajs/zstd'.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Producing.md#2025-04-14_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nconst { CompressionTypes, CompressionCodecs } = require('kafkajs')\nconst ZstdCodec = require('@kafkajs/zstd')\n\nCompressionCodecs[CompressionTypes.ZSTD] = ZstdCodec()\n```\n\n----------------------------------------\n\nTITLE: Instrumentation Event Structure\nDESCRIPTION: Defines the standard structure of instrumentation events in KafkaJS, showing the core properties that all events contain.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.15.0/InstrumentationEvents.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  id: <Number>,\n  type: <String>,\n  timestamp: <Number>,\n  payload: <Object>\n}\n```\n\n----------------------------------------\n\nTITLE: Custom Partition Assigner Interface in KafkaJS\nDESCRIPTION: Defines the interface for implementing a custom partition assignment strategy with name and version properties.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Consuming.md#2025-04-14_snippet_15\n\nLANGUAGE: javascript\nCODE:\n```\nconst MyPartitionAssigner = ({ cluster }) => ({\n    name: 'MyPartitionAssigner',\n    version: 1,\n    async assign({ members, topics }) {},\n    protocol({ topics }) {}\n})\n```\n\n----------------------------------------\n\nTITLE: Describing Configs with KafkaJS Admin Client\nDESCRIPTION: Method to retrieve configuration for specified Kafka resources, with options to include synonyms and filter by specific config names.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Admin.md#2025-04-14_snippet_15\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.describeConfigs({\n  includeSynonyms: <boolean>,\n  resources: <ResourceConfigQuery[]>\n})\n```\n\n----------------------------------------\n\nTITLE: Updated Resource Types Usage in KafkaJS TypeScript\nDESCRIPTION: Shows the migration from ResourceTypes to ConfigResourceTypes for configuration operations.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/MigrationGuide-2-0-0.md#2025-04-14_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst { ConfigResourceTypes } = require('kafkajs')\n\nawait admin.describeConfigs({\n  includeSynonyms: false,\n  resources: [\n    {\n      type: ConfigResourceTypes.TOPIC,\n      name: 'topic-name'\n    }\n  ]\n})\n```\n\n----------------------------------------\n\nTITLE: KafkaJS Producer Send Method Signature in JavaScript\nDESCRIPTION: Provides the full signature of the send method for the KafkaJS producer, including options for topic, messages, acknowledgments, timeout, and compression.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.4/Producing.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nawait producer.send({\n    topic: <String>,\n    messages: <Message[]>,\n    acks: <Number>,\n    timeout: <Number>,\n    compression: <CompressionTypes>,\n})\n```\n\n----------------------------------------\n\nTITLE: Example Simon Says Authentication Implementation\nDESCRIPTION: Complete example of a custom authentication mechanism implementation called 'simon' that demonstrates the authentication flow, request/response handling, and error management.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.4/CustomAuthenticationMechanism.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst simonAuthenticator = says = ({ host, port, logger, saslAuthenticate }) => {\n    const INT32_SIZE = 4\n  \n    const request = {\n      encode: () => {\n        const byteLength = Buffer.byteLength(says, 'utf8')\n        const buf = Buffer.alloc(INT32_SIZE + byteLength)\n        buf.writeUInt32BE(byteLength, 0)\n        buf.write(says, INT32_SIZE, byteLength, 'utf8')\n        return buf\n      },\n    }\n    const response = {\n      decode: rawData => {\n        const byteLength = rawData.readInt32BE(0)\n        return rawData.slice(INT32_SIZE, INT32_SIZE + byteLength)\n      },\n      parse: data => {\n        return data.toString()\n      },\n    }\n    return {\n      authenticate: async () => {\n        if (says == null) {\n          throw new Error('SASL Simon: Invalid \"says\"')\n        }\n        const broker = `${host}:${port}`\n        try {\n          logger.info('Authenticate with SASL Simon', { broker })\n          const authenticateResponse = await saslAuthenticate({ request, response })\n  \n          const saidSimon = says.startsWith(\"Simon says \")\n          const expectedResponse = saidSimon ? says : \"\"\n          if (authenticateResponse !== expectedResponse) {\n              throw new Error(\"Mismatching response from broker\")\n          }\n          logger.info('SASL Simon authentication successful', { broker })\n        } catch (e) {\n          const error = new Error(\n            `SASL Simon authentication failed: ${e.message}`\n          )\n          logger.error(error.message, { broker })\n          throw error\n        }\n      },\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring KafkaJS with a Custom Logger\nDESCRIPTION: This snippet shows how to configure a new Kafka client instance with a custom logger using the logCreator option. It sets the log level and applies the custom WinstonLogCreator.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/CustomLogger.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst kafka = new Kafka({\n    clientId: 'my-app',\n    brokers: ['kafka1:9092', 'kafka2:9092'],\n    logLevel: logLevel.ERROR,\n    logCreator: WinstonLogCreator\n})\n```\n\n----------------------------------------\n\nTITLE: Setting Log Level for KafkaJS Client\nDESCRIPTION: Configures the logging level for the KafkaJS client. The available log levels are NOTHING, ERROR, WARN, INFO, and DEBUG. The default level is INFO.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Configuration.md#2025-04-14_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Kafka, logLevel } = require('kafkajs')\n\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  logLevel: logLevel.ERROR\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring Connection Timeout in KafkaJS\nDESCRIPTION: This code snippet shows how to set a custom connection timeout when instantiating a new Kafka client. The connection timeout is specified in milliseconds.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Configuration.md#2025-04-14_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  connectionTimeout: 3000\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring Snappy Compression in KafkaJS Producer\nDESCRIPTION: This snippet demonstrates how to set up and use Snappy compression in KafkaJS. It requires installing the 'kafkajs-snappy' package and registering the Snappy codec with KafkaJS.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/Producing.md#2025-04-14_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst {  CompressionTypes, CompressionCodecs } = require('kafkajs')\nconst SnappyCodec = require('kafkajs-snappy')\n\nCompressionCodecs[CompressionTypes.Snappy] = SnappyCodec\n```\n\n----------------------------------------\n\nTITLE: Configuring Retry Options for KafkaJS Client\nDESCRIPTION: Sets custom retry configuration for connection and API calls to Kafka. This example modifies the initial retry time and number of retries from the default values.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Configuration.md#2025-04-14_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  retry: {\n    initialRetryTime: 100,\n    retries: 8\n  }\n})\n```\n\n----------------------------------------\n\nTITLE: Adding a Custom Compression Codec to KafkaJS\nDESCRIPTION: This code snippet demonstrates how to add a custom compression codec to KafkaJS. It imports the necessary modules and assigns the custom codec to the CompressionCodecs object.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Producing.md#2025-04-14_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nconst { CompressionTypes, CompressionCodecs } = require('kafkajs')\nCompressionCodecs[CompressionTypes.Snappy] = MyCustomSnappyCodec\n```\n\n----------------------------------------\n\nTITLE: Describing Configs with KafkaJS Admin Client\nDESCRIPTION: Demonstrates how to get the configuration for specified resources using the admin client's describeConfigs method. Includes structures for ResourceConfigQuery and examples of usage.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Admin.md#2025-04-14_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.describeConfigs({\n  includeSynonyms: <boolean>,\n  resources: <ResourceConfigQuery[]>\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    type: <ResourceType>,\n    name: <String>,\n    configNames: <String[]>\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst { ResourceTypes } = require('kafkajs')\n\nawait admin.describeConfigs({\n  includeSynonyms: false,\n  resources: [\n    {\n      type: ResourceTypes.TOPIC,\n      name: 'topic-name'\n    }\n  ]\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst { ResourceTypes } = require('kafkajs')\n\nawait admin.describeConfigs({\n  includeSynonyms: false,\n  resources: [\n    {\n      type: ResourceTypes.TOPIC,\n      name: 'topic-name',\n      configNames: ['cleanup.policy']\n    }\n  ]\n})\n```\n\n----------------------------------------\n\nTITLE: Example Authentication Configuration Usage\nDESCRIPTION: Example configuration showing how to use the custom Simon authentication mechanism with KafkaJS client.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.0/CustomAuthenticationMechanism.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst config = {\n  sasl: {\n    mechanism: 'simon'\n    authenticationProvider: simonAuthenticator('Simon says authenticate me')\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing LZ4 Compression in KafkaJS\nDESCRIPTION: Shows how to configure and use LZ4 compression with KafkaJS producer. Requires installing the 'kafkajs-lz4' package separately.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.4/Producing.md#2025-04-14_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst { CompressionTypes, CompressionCodecs } = require('kafkajs')\nconst LZ4 = require('kafkajs-lz4')\n\nCompressionCodecs[CompressionTypes.LZ4] = new LZ4().codec\n```\n\n----------------------------------------\n\nTITLE: Custom Authentication Mechanism Interface Definition\nDESCRIPTION: TypeScript interface definitions for implementing custom authentication mechanisms, including required types and function signatures for the authentication provider and authenticator.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.0/CustomAuthenticationMechanism.md#2025-04-14_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\ntype AuthenticationProviderArgs = {\n  host: string\n  port: number\n  logger: Logger\n  saslAuthenticate: <ParseResult>(\n    request: SaslAuthenticationRequest,\n    response?: SaslAuthenticationResponse<ParseResult>\n  ) => Promise<ParseResult | void>\n}\n\ntype Mechanism = {\n  mechanism: string\n  authenticationProvider: (args: AuthenticationProviderArgs) => Authenticator\n}\n\ntype Authenticator = {\n  authenticate(): Promise<void>\n}\n\ntype SaslAuthenticationRequest = {\n  encode: () => Buffer | Promise<Buffer>\n}\n\ntype SaslAuthenticationResponse<ParseResult> = {\n  decode: (rawResponse: Buffer) => Buffer | Promise<Buffer>\n  parse: (data: Buffer) => ParseResult\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Compose for Kafka and Zookeeper\nDESCRIPTION: A docker-compose configuration that sets up a Kafka broker and Zookeeper. This creates a basic development environment with a single topic 'topic-test' and necessary port mappings.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/DockerLocal.md#2025-04-14_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nversion: '2'\nservices:\n  zookeeper:\n    image: wurstmeister/zookeeper:latest\n    ports:\n      - \"2181:2181\"\n  kafka:\n    image: wurstmeister/kafka:2.11-1.1.1\n    ports:\n      - \"9092:9092\"\n    links:\n      - zookeeper\n    environment:\n      KAFKA_ADVERTISED_HOST_NAME: ${HOST_IP}\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'\n      KAFKA_DELETE_TOPIC_ENABLE: 'true'\n      KAFKA_CREATE_TOPICS: \"topic-test:1:1\"\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n```\n\n----------------------------------------\n\nTITLE: Setting Request Timeout for KafkaJS Client\nDESCRIPTION: Configures the request timeout in milliseconds for the KafkaJS client. The default value is 30000ms, but this example sets it to 25000ms.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Configuration.md#2025-04-14_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  requestTimeout: 25000\n})\n```\n\n----------------------------------------\n\nTITLE: Structure of Topics for Sending Offsets in KafkaJS\nDESCRIPTION: Defines the data structure required for the topics parameter when sending offsets in a transaction. The structure includes the topic name and an array of partitions with their corresponding offsets.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Transactions.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n[{\n  topic: <String>,\n  partitions: [{\n    partition: <Number>,\n    offset: <String>\n  }]\n}]\n```\n\n----------------------------------------\n\nTITLE: Using Java-Compatible Partitioner in KafkaJS\nDESCRIPTION: Demonstrates how to use the Java-compatible partitioner provided by KafkaJS for consistent partitioning across different client implementations.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Producing.md#2025-04-14_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Partitioners } = require('kafkajs')\nkafka.producer({ createPartitioner: Partitioners.JavaCompatiblePartitioner })\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Socket Factory for KafkaJS\nDESCRIPTION: Creates a custom socket factory function to allow specialized socket configurations. This example sets a custom TTL (Time To Live) for the socket connections.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Configuration.md#2025-04-14_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Kafka } = require('kafkajs')\n\n// Example socket factory setting a custom TTL\nconst net = require('net')\nconst tls = require('tls')\n\nconst myCustomSocketFactory = ({ host, port, ssl, onConnect }) => {\n  const socket = ssl\n    ? tls.connect(\n        Object.assign({ host, port }, ssl),\n        onConnect\n      )\n    : net.connect(\n        { host, port },\n        onConnect\n      )\n\n  socket.setKeepAlive(true, 30000)\n\n  return socket\n}\n\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  socketFactory: myCustomSocketFactory,\n})\n```\n\n----------------------------------------\n\nTITLE: Applying GZIP Compression in KafkaJS\nDESCRIPTION: Shows how to use GZIP compression when sending messages with the KafkaJS producer. GZIP compression is built into KafkaJS core functionality.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Producing.md#2025-04-14_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst { CompressionTypes } = require('kafkajs')\n\nasync () => {\n  await producer.send({\n    topic: 'topic-name',\n    compression: CompressionTypes.GZIP,\n    messages: [\n        { key: 'key1', value: 'hello world' },\n        { key: 'key2', value: 'hey hey!' }\n    ],\n  })\n}\n```\n\n----------------------------------------\n\nTITLE: Fetching Specific Configs for a Topic in KafkaJS\nDESCRIPTION: Example showing how to retrieve specific configuration settings (e.g., cleanup.policy) for a Kafka topic.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Admin.md#2025-04-14_snippet_18\n\nLANGUAGE: javascript\nCODE:\n```\nconst { ResourceTypes } = require('kafkajs')\n\nawait admin.describeConfigs({\n  includeSynonyms: false,\n  resources: [\n    {\n      type: ResourceTypes.TOPIC,\n      name: 'topic-name',\n      configNames: ['cleanup.policy']\n    }\n  ]\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Partition Assigner in KafkaJS Consumer\nDESCRIPTION: Shows how to add a custom partition assigner to the list of assigners when creating a KafkaJS consumer. It's important to keep the default assigner to maintain compatibility with old consumers.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Consuming.md#2025-04-14_snippet_16\n\nLANGUAGE: javascript\nCODE:\n```\nconst { PartitionAssigners: { roundRobin } } = require('kafkajs')\n\nkafka.consumer({\n    groupId: 'my-group',\n    partitionAssigners: [\n        MyPartitionAssigner,\n        roundRobin\n    ]\n})\n```\n\n----------------------------------------\n\nTITLE: Authentication Mechanism Interface Definition in TypeScript\nDESCRIPTION: TypeScript interface that defines the structure required for implementing a custom authentication mechanism in KafkaJS. It shows the expected types for authentication providers, authenticators, and SASL request/response handling.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/CustomAuthenticationMechanism.md#2025-04-14_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\ntype SaslAuthenticateArgs<ParseResult> = {\n  request: SaslAuthenticationRequest\n  response?: SaslAuthenticationResponse<ParseResult>\n}\n\ntype AuthenticationProviderArgs = {\n  host: string\n  port: number\n  logger: Logger\n  saslAuthenticate: <ParseResult>(\n    args: SaslAuthenticateArgs<ParseResult>\n  ) => Promise<ParseResult | void>\n}\n\ntype Mechanism = {\n  mechanism: string\n  authenticationProvider: (args: AuthenticationProviderArgs) => Authenticator\n}\n\ntype Authenticator = {\n  authenticate(): Promise<void>\n}\n\ntype SaslAuthenticationRequest = {\n  encode: () => Buffer | Promise<Buffer>\n}\n\ntype SaslAuthenticationResponse<ParseResult> = {\n  decode: (rawResponse: Buffer) => Buffer | Promise<Buffer>\n  parse: (data: Buffer) => ParseResult\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Logger Instances in KafkaJS\nDESCRIPTION: Demonstrates how to access logger instances from various KafkaJS client components after initialization.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.4/CustomLogger.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst client = new Kafka( ... )\nclient.logger().info( ... )\n\nconst consumer = kafka.consumer( ... )\nconsumer.logger().info( ... )\n\nconst producer = kafka.producer( ... )\nproducer.logger().info( ... )\n\nconst admin = kafka.admin( ... )\nadmin.logger().info( ... )\n```\n\n----------------------------------------\n\nTITLE: Installing Snappy Compression Package for KafkaJS\nDESCRIPTION: This shell command installs the kafkajs-snappy package, which provides Snappy compression support for KafkaJS.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Producing.md#2025-04-14_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\nnpm install kafkajs-snappy\n# yarn add kafkajs-snappy\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Log Creator Function in JavaScript\nDESCRIPTION: This snippet shows the general structure of a custom log creator function in KafkaJS. It takes a log level and returns a function that processes log entries.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/CustomLogger.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst MyLogCreator = logLevel => ({ namespace, level, label, log }) => {\n    // Example:\n    // const { timestamp, logger, message, ...others } = log\n    // console.log(`${label} [${namespace}] ${message} ${JSON.stringify(others)}`)\n}\n```\n\n----------------------------------------\n\nTITLE: ResourceConfig Structure in KafkaJS\nDESCRIPTION: Structure definition for resource configurations used when modifying Kafka configurations, specifying the resource type, name, and config entries to update.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Admin.md#2025-04-14_snippet_21\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    type: <ResourceType>,\n    name: <String>,\n    configEntries: <ResourceConfigEntry[]>\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring LZ4 Compression for KafkaJS\nDESCRIPTION: This snippet demonstrates how to configure LZ4 compression for KafkaJS. It imports the necessary modules and adds the LZ4 codec to the CompressionCodecs object.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Producing.md#2025-04-14_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst { CompressionTypes, CompressionCodecs } = require('kafkajs')\nconst LZ4 = require('kafkajs-lz4')\n\nCompressionCodecs[CompressionTypes.LZ4] = new LZ4().codec\n```\n\n----------------------------------------\n\nTITLE: Enabling Debug Logging with Protocol Buffers in KafkaJS Tests\nDESCRIPTION: Command to run tests in watch mode with debug logging and protocol buffer visibility enabled. Uses environment variables to configure logging behavior.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Testing.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nKAFKAJS_LOG_LEVEL=debug KAFKAJS_DEBUG_PROTOCOL_BUFFERS=1 yarn test:local:watch\n```\n\n----------------------------------------\n\nTITLE: Starting Kafka Cluster with Docker Compose for KafkaJS Development\nDESCRIPTION: Shell commands to start a Kafka cluster using docker-compose and generate SCRAM credentials for authentication. These scripts set up the necessary infrastructure for testing KafkaJS.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/DevelopmentEnvironment.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n# This will run a Kafka cluster configured with your current IP\n./scripts/dockerComposeUp.sh\n./scripts/createScramCredentials.sh\n```\n\n----------------------------------------\n\nTITLE: Example Response from alterConfigs in KafkaJS\nDESCRIPTION: Example of the response object returned by the alterConfigs method, showing the structure with resources, error information, and throttle time.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Admin.md#2025-04-14_snippet_24\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    resources: [{\n        errorCode: 0,\n        errorMessage: null,\n        resourceName: 'topic-name',\n        resourceType: 2,\n    }],\n    throttleTime: 0,\n}\n```\n\n----------------------------------------\n\nTITLE: Sending Messages to Specific Partitions\nDESCRIPTION: Demonstrates how to send messages to specific partitions within a Kafka topic.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.15.0/Producing.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n\nawait producer.connect()\nawait producer.send({\n    topic: 'topic-name',\n    messages: [\n        { key: 'key1', value: 'hello world', partition: 0 },\n        { key: 'key2', value: 'hey hey!', partition: 1 }\n    ],\n})\n```\n\n----------------------------------------\n\nTITLE: Running Kafka Cluster with Docker Compose for KafkaJS Development\nDESCRIPTION: Shell commands to start a Kafka cluster and generate SCRAM credentials for development. The first command runs the cluster using the default docker-compose configuration, while the second creates the necessary authentication credentials.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/DevelopmentEnvironment.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n# This will run a Kafka cluster configured with your current IP\n./scripts/dockerComposeUp.sh\n./scripts/createScramCredentials.sh\n```\n\n----------------------------------------\n\nTITLE: Custom Log Creator Template in KafkaJS\nDESCRIPTION: Template showing the basic structure of a custom log creator function that can be implemented for custom logging.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.4/CustomLogger.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst MyLogCreator = logLevel => ({ namespace, level, label, log }) => {\n    // Example:\n    // const { timestamp, logger, message, ...others } = log\n    // console.log(`${label} [${namespace}] ${message} ${JSON.stringify(others)}`)\n}\n```\n\n----------------------------------------\n\nTITLE: Defining AVRO Schema in AVDL Format\nDESCRIPTION: Example of defining a simple AVRO schema in AVDL format with a namespace and record definition. The schema defines a Simple record type with a single string field 'foo'.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/KafkaIntro.md#2025-04-14_snippet_1\n\nLANGUAGE: avdl\nCODE:\n```\n@namespace(\"com.kafkajs.fixtures\")\nprotocol SimpleProto {\n  record Simple {\n    string foo;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Authentication Configuration in KafkaJS\nDESCRIPTION: Basic configuration structure for implementing a custom authentication mechanism in KafkaJS. Specifies the mechanism name and authentication provider function.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.0/CustomAuthenticationMechanism.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n{ \n  sasl: { \n      mechanism: <mechanism name>,\n      authenticationProvider: ({ host, port, logger, saslAuthenticate }) => { authenticate: () => Promise<void> }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: TopicsMetadata Structure in KafkaJS\nDESCRIPTION: Structure definition for the topics metadata object returned by fetchTopicMetadata, containing an array of topic metadata objects.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Admin.md#2025-04-14_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    topics: <Array<TopicMetadata>>,\n}\n```\n\n----------------------------------------\n\nTITLE: Sending Compressed Messages with Custom Codec in KafkaJS\nDESCRIPTION: This snippet shows how to send messages using a custom compression codec in KafkaJS. It uses the producer's send method with the compression option set to the custom codec.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Producing.md#2025-04-14_snippet_15\n\nLANGUAGE: javascript\nCODE:\n```\nawait producer.send({\n    topic: 'topic-name',\n    compression: CompressionTypes.Snappy,\n    messages: [\n        { key: 'key1', value: 'hello world' },\n        { key: 'key2', value: 'hey hey!' }\n    ],\n})\n```\n\n----------------------------------------\n\nTITLE: Installing KafkaJS Beta Version using Package Managers\nDESCRIPTION: Commands to install the latest pre-release version of KafkaJS using either Yarn or NPM package managers.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/PreReleases.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n# Yarn\nyarn add kafkajs@beta\n\n# Npm\nnpm install --save kafkajs@beta\n```\n\n----------------------------------------\n\nTITLE: Starting Kafka Cluster with Specific Version\nDESCRIPTION: Shell command to start a Kafka cluster using a specific Docker Compose file, allowing developers to test with different Kafka versions.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/DevelopmentEnvironment.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nCOMPOSE_FILE=\"docker-compose.2_3.yml\" ./scripts/dockerComposeUp.sh\n```\n\n----------------------------------------\n\nTITLE: Resetting Consumer Group Offsets by Timestamp with KafkaJS Admin Client\nDESCRIPTION: Shows how to reset consumer group offsets to the earliest offset whose timestamp is greater than or equal to a given timestamp, using a combination of fetchTopicOffsetsByTimestamp and setOffsets methods.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Admin.md#2025-04-14_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.setOffsets({ groupId, topic, partitions: await admin.fetchTopicOffsetsByTimestamp(topic, timestamp) })\n```\n\n----------------------------------------\n\nTITLE: KafkaJS Event Structure Definition\nDESCRIPTION: Defines the structure of instrumentation events in KafkaJS, showing the standard fields that all events contain.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/InstrumentationEvents.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  id: <Number>,\n  type: <String>,\n  timestamp: <Number>,\n  payload: <Object>\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Kafka with Docker Compose\nDESCRIPTION: Shell commands to set the HOST_IP environment variable (required for Kafka connectivity) and launch the Docker containers. This extracts the machine's IP address automatically.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/DockerLocal.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nexport HOST_IP=$(ifconfig | grep -E \"([0-9]{1,3}\\.){3}[0-9]{1,3}\" | grep -v 127.0.0.1 | awk '{ print $2 }' | cut -f2 -d: | head -n1)\ndocker-compose up\n```\n\n----------------------------------------\n\nTITLE: Connection Timeout Configuration\nDESCRIPTION: Setting custom connection timeout value in milliseconds. Controls how long to wait for successful broker connections.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.14.0/Configuration.md#2025-04-14_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  connectionTimeout: 3000\n})\n```\n\n----------------------------------------\n\nTITLE: Starting Kafka Cluster with Docker Scripts\nDESCRIPTION: Shell commands to start a Kafka cluster and create SCRAM credentials using Docker Compose\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/DevelopmentEnvironment.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n# This will run a Kafka cluster configured with your current IP\n./scripts/dockerComposeUp.sh\n./scripts/createScramCredentials.sh\n```\n\n----------------------------------------\n\nTITLE: Applying a Custom Authentication Configuration in KafkaJS\nDESCRIPTION: Shows how to use the custom authentication provider within the KafkaJS client configuration. This example demonstrates how to apply the previously defined 'simon' authenticator with a specific authentication phrase.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/CustomAuthenticationMechanism.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst config = {\n  sasl: {\n    mechanism: 'simon'\n    authenticationProvider: simonAuthenticator('Simon says authenticate me')\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: PartitionMetadata Structure in KafkaJS\nDESCRIPTION: Structure definition for partition metadata, including error code, partition ID, leader, replicas, and in-sync replicas (ISR).\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Admin.md#2025-04-14_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    partitionErrorCode: <Number>, // default: 0\n    partitionId: <Number>,\n    leader: <Number>,\n    replicas: <Array<Number>>,\n    isr: <Array<Number>>,\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring LZ4 Compression for KafkaJS\nDESCRIPTION: This code snippet demonstrates how to configure LZ4 compression for KafkaJS. It imports the necessary modules and adds the LZ4 codec to the CompressionCodecs object.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Producing.md#2025-04-14_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst { CompressionTypes, CompressionCodecs } = require('kafkajs')\nconst LZ4 = require('kafkajs-lz4')\n\nCompressionCodecs[CompressionTypes.LZ4] = new LZ4().codec\n```\n\n----------------------------------------\n\nTITLE: Installing KafkaJS Pre-release Version using Yarn and npm\nDESCRIPTION: Commands to install the latest pre-release version of KafkaJS using Yarn and npm package managers.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/PreReleases.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n# Yarn\nyarn add kafkajs@beta\n\n# Npm\nnpm install --save kafkajs@beta\n```\n\n----------------------------------------\n\nTITLE: Installing KafkaJS with Yarn\nDESCRIPTION: Command to install KafkaJS package using the Yarn package manager.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/GettingStarted.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nyarn add kafkajs\n```\n\n----------------------------------------\n\nTITLE: KafkaJS Instrumentation Event Structure\nDESCRIPTION: Defines the common structure of all instrumentation events in KafkaJS. Each event contains an id, type, timestamp, and payload object with event-specific data.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/InstrumentationEvents.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  id: <Number>,\n  type: <String>,\n  timestamp: <Number>,\n  payload: <Object>\n}\n```\n\n----------------------------------------\n\nTITLE: eachMessage Consumer Handler\nDESCRIPTION: Implementation of message processing using eachMessage handler, which processes one message at a time with automatic offset commits and heartbeats.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Consuming.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nawait consumer.run({\n    eachMessage: async ({ topic, partition, message }) => {\n        console.log({\n            key: message.key.toString(),\n            value: message.value.toString(),\n            headers: message.headers,\n        })\n    },\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic KafkaJS Producer in JavaScript\nDESCRIPTION: A complete JavaScript implementation of a Kafka producer using KafkaJS. Features include compression, random message generation, error handling, and graceful shutdown. The producer connects to Kafka and sends messages at regular intervals.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/ProducerExample.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst ip = require('ip')\n\nconst { Kafka, CompressionTypes, logLevel } = require('kafkajs')\n\nconst host = process.env.HOST_IP || ip.address()\n\nconst kafka = new Kafka({\n  logLevel: logLevel.DEBUG,\n  brokers: [`${host}:9092`],\n  clientId: 'example-producer',\n})\n\nconst topic = 'topic-test'\nconst producer = kafka.producer()\n\nconst getRandomNumber = () => Math.round(Math.random(10) * 1000)\nconst createMessage = num => ({\n  key: `key-${num}`,\n  value: `value-${num}-${new Date().toISOString()}`,\n})\n\nconst sendMessage = () => {\n  return producer\n    .send({\n      topic,\n      compression: CompressionTypes.GZIP,\n      messages: Array(getRandomNumber())\n        .fill()\n        .map(_ => createMessage(getRandomNumber())),\n    })\n    .then(console.log)\n    .catch(e => console.error(`[example/producer] ${e.message}`, e))\n}\n\nconst run = async () => {\n  await producer.connect()\n  setInterval(sendMessage, 3000)\n}\n\nrun().catch(e => console.error(`[example/producer] ${e.message}`, e))\n\nconst errorTypes = ['unhandledRejection', 'uncaughtException']\nconst signalTraps = ['SIGTERM', 'SIGINT', 'SIGUSR2']\n\nerrorTypes.forEach(type => {\n  process.on(type, async () => {\n    try {\n      console.log(`process.on ${type}`)\n      await producer.disconnect()\n      process.exit(0)\n    } catch (_) {\n      process.exit(1)\n    }\n  })\n})\n\nsignalTraps.forEach(type => {\n  process.once(type, async () => {\n    try {\n      await producer.disconnect()\n    } finally {\n      process.kill(process.pid, type)\n    }\n  })\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing a KafkaJS Producer with Error Handling in JavaScript\nDESCRIPTION: This code demonstrates the complete implementation of a Kafka producer using KafkaJS. It includes connection setup, message creation with random values, compression, scheduled message sending, and proper error handling and cleanup for various process termination scenarios.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/ProducerExample.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst fs = require('fs')\nconst ip = require('ip')\n\nconst { Kafka, CompressionTypes, logLevel } = require('../index')\n\nconst host = process.env.HOST_IP || ip.address()\n\nconst kafka = new Kafka({\n  logLevel: logLevel.DEBUG,\n  brokers: [`${host}:9092`],\n  clientId: 'example-producer',\n})\n\nconst topic = 'topic-test'\nconst producer = kafka.producer()\n\nconst getRandomNumber = () => Math.round(Math.random(10) * 1000)\nconst createMessage = num => ({\n  key: `key-${num}`,\n  value: `value-${num}-${new Date().toISOString()}`,\n})\n\nconst sendMessage = () => {\n  return producer\n    .send({\n      topic,\n      compression: CompressionTypes.GZIP,\n      messages: Array(getRandomNumber())\n        .fill()\n        .map(_ => createMessage(getRandomNumber())),\n    })\n    .then(console.log)\n    .catch(e => console.error(`[example/producer] ${e.message}`, e))\n}\n\nconst run = async () => {\n  await producer.connect()\n  setInterval(sendMessage, 3000)\n}\n\nrun().catch(e => console.error(`[example/producer] ${e.message}`, e))\n\nconst errorTypes = ['unhandledRejection', 'uncaughtException']\nconst signalTraps = ['SIGTERM', 'SIGINT', 'SIGUSR2']\n\nerrorTypes.map(type => {\n  process.on(type, async () => {\n    try {\n      console.log(`process.on ${type}`)\n      await producer.disconnect()\n      process.exit(0)\n    } catch (_) {\n      process.exit(1)\n    }\n  })\n})\n\nsignalTraps.map(type => {\n  process.once(type, async () => {\n    try {\n      await producer.disconnect()\n    } finally {\n      process.kill(process.pid, type)\n    }\n  })\n})\n```\n\n----------------------------------------\n\nTITLE: Updated Offset Fetching in KafkaJS Admin\nDESCRIPTION: Shows the migration path for fetching offsets from single topic to multiple topics support.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/MigrationGuide-2-0-0.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n// Before\nconst partitions = await admin.fetchOffsets({ groupId, topic: 'topic-a' })\nfor (const { partition, offset } of partitions) {\n    admin.logger().info(`${groupId} is at offset ${offset} of partition ${partition}`)\n}\n\n// After\nconst topics = await admin.fetchOffsets({ groupId, topics: ['topic-a', 'topic-b'] })\nfor (const topic of topics) {\n    for (const { partition, offset } of partitions) {\n        admin.logger().info(`${groupId} is at offset ${offset} of ${topic}:${partition}`)\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Kafka Docker Environment\nDESCRIPTION: Shell commands to set the host IP address and start the Docker containers. The HOST_IP is extracted from network interfaces excluding localhost.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/DockerLocal.md#2025-04-14_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nexport HOST_IP=$(ifconfig | grep -E \"([0-9]{1,3}\\.){3}[0-9]{1,3}\" | grep -v 127.0.0.1 | awk '{ print $2 }' | cut -f2 -d: | head -n1)\ndocker-compose up\n```\n\n----------------------------------------\n\nTITLE: Defining AVRO Schema in AVDL Format for KafkaJS Messages\nDESCRIPTION: Example of an AVRO schema in AVDL format with a namespace and a simple record definition. This schema defines a 'Simple' record with a single string field named 'foo' within the 'com.kafkajs.fixtures' namespace.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/KafkaIntro.md#2025-04-14_snippet_1\n\nLANGUAGE: avdl\nCODE:\n```\n@namespace(\"com.kafkajs.fixtures\")\nprotocol SimpleProto {\n  record Simple {\n    string foo;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing a Basic Kafka Producer in JavaScript\nDESCRIPTION: Creates a new Kafka producer instance with default settings using the kafka client.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Producing.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = kafka.producer()\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Compression Codec to KafkaJS\nDESCRIPTION: This snippet demonstrates how to add a custom compression codec to KafkaJS. It imports the necessary modules and assigns the custom codec to the CompressionCodecs object.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Producing.md#2025-04-14_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nconst { CompressionTypes, CompressionCodecs } = require('kafkajs')\nCompressionCodecs[CompressionTypes.Snappy] = MyCustomSnappyCodec\n```\n\n----------------------------------------\n\nTITLE: KafkaJS Pre-release Package.json Metadata\nDESCRIPTION: Example of the additional metadata included in package.json for pre-release versions, showing the commit SHA and comparison URL against the stable version.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/PreReleases.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  // package.json\n  \"kafkajs\": {\n    \"sha\": \"43e325e18133b8d6c1c80f8e95ef8610c44ec631\",\n    \"compare\": \"https://github.com/tulios/kafkajs/compare/v1.9.3...43e325e18133b8d6c1c80f8e95ef8610c44ec631\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing KafkaJS with NPM\nDESCRIPTION: Command to install KafkaJS using the NPM package manager.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/GettingStarted.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install kafkajs\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Log Creator in KafkaJS\nDESCRIPTION: Demonstrates the basic structure of a custom log creator function that receives log level and returns a log function. The log function handles namespace, level, label, and log data.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/CustomLogger.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst MyLogCreator = logLevel => ({ namespace, level, label, log }) => {\n    // Example:\n    // const { timestamp, logger, message, ...others } = log\n    // console.log(`${label} [${namespace}] ${message} ${JSON.stringify(others)}`)\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Snappy Compression in KafkaJS Producer\nDESCRIPTION: This snippet illustrates how to set up Snappy compression for use with KafkaJS. It involves installing the kafkajs-snappy package, importing the necessary modules, and registering the Snappy codec with KafkaJS.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.15.0/Producing.md#2025-04-14_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst {  CompressionTypes, CompressionCodecs } = require('kafkajs')\nconst SnappyCodec = require('kafkajs-snappy')\n\nCompressionCodecs[CompressionTypes.Snappy] = SnappyCodec\n```\n\n----------------------------------------\n\nTITLE: Implementing Protocol Method for Custom Assigner in KafkaJS\nDESCRIPTION: Demonstrates implementation of the protocol method for custom partition assigner using MemberMetadata encoding.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Consuming.md#2025-04-14_snippet_17\n\nLANGUAGE: javascript\nCODE:\n```\nconst { AssignerProtocol: { MemberMetadata } } = require('kafkajs')\n\nconst MyPartitionAssigner = ({ cluster }) => ({\n    name: 'MyPartitionAssigner',\n    version: 1,\n    protocol({ topics }) {\n        return {\n            name: this.name,\n            metadata: MemberMetadata.encode({\n            version: this.version,\n            topics,\n            }),\n        }\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Enabling Debug Logging with Buffer Output for KafkaJS Tests\nDESCRIPTION: A shell command to run KafkaJS tests in local watch mode with debug level logging and protocol buffer output enabled. This provides detailed logs including buffer values that are normally filtered out.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Testing.md#2025-04-14_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nKAFKAJS_LOG_LEVEL=debug KAFKAJS_DEBUG_PROTOCOL_BUFFERS=1 yarn test:local:watch\n```\n\n----------------------------------------\n\nTITLE: Installing KafkaJS Beta Version using Package Managers\nDESCRIPTION: Commands to install the latest pre-release version of KafkaJS using either Yarn or NPM package managers.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/PreReleases.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n# Yarn\nyarn add kafkajs@beta\n\n# Npm\nnpm install --save kafkajs@beta\n```\n\n----------------------------------------\n\nTITLE: Defining AVRO Schema in AVDL Format\nDESCRIPTION: This code snippet demonstrates how to define a simple AVRO schema using AVDL (Avro IDL) format. It creates a protocol named 'SimpleProto' with a record 'Simple' containing a single string field 'foo'.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/KafkaIntro.md#2025-04-14_snippet_1\n\nLANGUAGE: avdl\nCODE:\n```\n@namespace(\"com.kafkajs.fixtures\")\nprotocol SimpleProto {\n  record Simple {\n    string foo;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating KafkaJS Consumer\nDESCRIPTION: Initializes a new KafkaJS consumer with a specified group ID.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Consuming.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst consumer = kafka.consumer({ groupId: 'my-group' })\n```\n\n----------------------------------------\n\nTITLE: Configuring PLAIN/SCRAM SASL Authentication in KafkaJS\nDESCRIPTION: Demonstrates how to set up SASL authentication using PLAIN or SCRAM mechanisms in KafkaJS, including SSL configuration for secure transmission of credentials.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Configuration.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  // authenticationTimeout: 10000,\n  // reauthenticationThreshold: 10000,\n  ssl: true,\n  sasl: {\n    mechanism: 'plain', // scram-sha-256 or scram-sha-512\n    username: 'my-username',\n    password: 'my-password'\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Docusaurus Website\nDESCRIPTION: Command to install all the dependencies required for the Docusaurus website using Yarn package manager.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/README.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n# Install dependencies\n$ yarn\n```\n\n----------------------------------------\n\nTITLE: TopicMetadata Structure in KafkaJS\nDESCRIPTION: Structure definition for a single topic's metadata, containing the topic name and an array of partition metadata objects.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Admin.md#2025-04-14_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    topic: <String>,\n    partitions: <Array<PartitionMetadata>> // default: 1\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Winston Logger for KafkaJS in JavaScript\nDESCRIPTION: This snippet demonstrates how to create a custom log creator using Winston for KafkaJS. It includes a function to convert KafkaJS log levels to Winston levels and configures Winston transports.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/CustomLogger.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst { logLevel } = require('kafkajs')\nconst winston = require('winston')\nconst toWinstonLogLevel = level => {\n    switch(level) {\n        case logLevel.ERROR:\n        case logLevel.NOTHING:\n            return 'error'\n        case logLevel.WARN:\n            return 'warn'\n        case logLevel.INFO:\n            return 'info'\n        case logLevel.DEBUG:\n            return 'debug'\n    }\n}\n\nconst WinstonLogCreator = logLevel => {\n    const logger = winston.createLogger({\n        level: toWinstonLogLevel(logLevel),\n        transports: [\n            new winston.transports.Console(),\n            new winston.transports.File({ filename: 'myapp.log' })\n        ]\n    })\n\n    return ({ namespace, level, label, log }) => {\n        const { message, ...extra } = log\n        logger.log({\n            level: toWinstonLogLevel(level),\n            message,\n            extra,\n        })\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Individual Logger Levels in KafkaJS\nDESCRIPTION: Shows how to override log levels for different components (kafka, producer, consumer, admin) after instantiation using setLogLevel method.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.14.0/Configuration.md#2025-04-14_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Kafka, logLevel } = require('kafkajs')\n\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  logLevel: logLevel.ERROR\n})\nkafka.logger().setLogLevel(logLevel.WARN)\n\nconst producer = kafka.producer(...)\nproducer.logger().setLogLevel(logLevel.INFO)\n\nconst consumer = kafka.consumer(...)\nconsumer.logger().setLogLevel(logLevel.DEBUG)\n\nconst admin = kafka.admin(...)\nadmin.logger().setLogLevel(logLevel.NOTHING)\n```\n\n----------------------------------------\n\nTITLE: Checking Docker Compose Kafka Cluster Status\nDESCRIPTION: Shell command to verify the running status of the Kafka cluster containers. This shows the container names and port mappings for the Kafka brokers and Zookeeper.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/DevelopmentEnvironment.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n$ docker-compose -f docker-compose.2_2.yml ps\nWARNING: The HOST_IP variable is not set. Defaulting to a blank string.\n      Name                    Command               State                                   Ports\n----------------------------------------------------------------------------------------------------------------------------------\nkafkajs_kafka1_1   start-kafka.sh                   Up      0.0.0.0:9092->9092/tcp, 0.0.0.0:9093->9093/tcp, 0.0.0.0:9094->9094/tcp\nkafkajs_kafka2_1   start-kafka.sh                   Up      0.0.0.0:9095->9095/tcp, 0.0.0.0:9096->9096/tcp, 0.0.0.0:9097->9097/tcp\nkafkajs_kafka3_1   start-kafka.sh                   Up      0.0.0.0:9098->9098/tcp, 0.0.0.0:9099->9099/tcp, 0.0.0.0:9100->9100/tcp\nkafkajs_zk_1       /bin/sh -c /usr/sbin/sshd  ...   Up      0.0.0.0:2181->2181/tcp, 22/tcp, 2888/tcp, 3888/tcp\n```\n\n----------------------------------------\n\nTITLE: Checking Kafka Cluster Status with Docker Compose\nDESCRIPTION: Docker Compose command to verify the status of the Kafka cluster containers and their exposed ports. The output shows three Kafka brokers and a Zookeeper instance.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/DevelopmentEnvironment.md#2025-04-14_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n$ docker-compose -f docker-compose.2_3.yml ps\nWARNING: The HOST_IP variable is not set. Defaulting to a blank string.\n      Name                    Command               State                                   Ports\n----------------------------------------------------------------------------------------------------------------------------------\nkafkajs_kafka1_1   start-kafka.sh                   Up      0.0.0.0:9092->9092/tcp, 0.0.0.0:9093->9093/tcp, 0.0.0.0:9094->9094/tcp\nkafkajs_kafka2_1   start-kafka.sh                   Up      0.0.0.0:9095->9095/tcp, 0.0.0.0:9096->9096/tcp, 0.0.0.0:9097->9097/tcp\nkafkajs_kafka3_1   start-kafka.sh                   Up      0.0.0.0:9098->9098/tcp, 0.0.0.0:9099->9099/tcp, 0.0.0.0:9100->9100/tcp\nkafkajs_zk_1       /bin/sh -c /usr/sbin/sshd  ...   Up      0.0.0.0:2181->2181/tcp, 22/tcp, 2888/tcp, 3888/tcp\n```\n\n----------------------------------------\n\nTITLE: Running Specific Kafka Version with Docker Compose\nDESCRIPTION: Shell command to start a Kafka cluster with a specific version by specifying a different docker-compose configuration file using the COMPOSE_FILE environment variable.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/DevelopmentEnvironment.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nCOMPOSE_FILE=\"docker-compose.2_2.yml\" ./scripts/dockerComposeUp.sh\n```\n\n----------------------------------------\n\nTITLE: Running Kafka Cluster and Generating Credentials for KafkaJS Development\nDESCRIPTION: These shell commands start a Kafka cluster using Docker Compose and create SCRAM credentials for authentication. The cluster is configured with the current IP address of the host machine.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/DevelopmentEnvironment.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n# This will run a Kafka cluster configured with your current IP\n./scripts/dockerComposeUp.sh\n./scripts/createScramCredentials.sh\n```\n\n----------------------------------------\n\nTITLE: Installing KafkaJS via Package Manager\nDESCRIPTION: Commands to install KafkaJS using npm or yarn package managers.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/README.md#2025-04-14_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install kafkajs\n# yarn add kafkajs\n```\n\n----------------------------------------\n\nTITLE: Configuring Retry Mechanism in KafkaJS\nDESCRIPTION: This snippet shows how to configure the retry mechanism for KafkaJS, which is used for retrying connections and API calls. It demonstrates setting custom values for initialRetryTime and retries.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Configuration.md#2025-04-14_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  retry: {\n    initialRetryTime: 100,\n    retries: 8\n  }\n})\n```\n\n----------------------------------------\n\nTITLE: Creating a Markdown Document in Docusaurus\nDESCRIPTION: Example of a Markdown document with frontmatter for Docusaurus, showing how to set the document ID and title that needs editing.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/README.md#2025-04-14_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n---\nid: page-needs-edit\ntitle: This Doc Needs To Be Edited\n---\n\nEdit me...\n```\n\n----------------------------------------\n\nTITLE: Example Response from describeConfigs in KafkaJS\nDESCRIPTION: Example of the response object returned by the describeConfigs method, showing the structure with resources, config entries, and error information.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Admin.md#2025-04-14_snippet_19\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    resources: [\n        {\n            configEntries: [{\n                configName: 'cleanup.policy',\n                configValue: 'delete',\n                isDefault: true,\n                isSensitive: false,\n                readOnly: false\n            }],\n            errorCode: 0,\n            errorMessage: null,\n            resourceName: 'topic-name',\n            resourceType: 2\n        }\n    ],\n    throttleTime: 0\n}\n```\n\n----------------------------------------\n\nTITLE: TypeScript KafkaJS Consumer Class Implementation\nDESCRIPTION: Object-oriented implementation of a Kafka consumer in TypeScript with support for both single message and batch processing. Includes proper type definitions and error handling.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.16.0/ConsumerExample.md#2025-04-14_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Consumer, ConsumerSubscribeTopic, EachBatchPayload, Kafka, EachMessagePayload } from 'kafkajs'\n\nexport default class ExampleConsumer {\n  private kafkaConsumer: Consumer\n  private messageProcessor: ExampleMessageProcessor\n\n  public constructor(messageProcessor: ExampleMessageProcessor) {\n    this.messageProcessor = messageProcessor\n    this.kafkaConsumer = this.createKafkaConsumer()\n  }\n\n  public async startConsumer(): Promise<void> {\n    const topic: ConsumerSubscribeTopic = {\n      topic: 'example-topic',\n      fromBeginning: false\n    }\n\n    try {\n      await this.kafkaConsumer.connect()\n      await this.kafkaConsumer.subscribe(topic)\n\n      await this.kafkaConsumer.run({\n        eachMessage: async (messagePayload: EachMessagePayload) => {\n          const { topic, partition, message } = messagePayload\n          const prefix = `${topic}[${partition} | ${message.offset}] / ${message.timestamp}`\n          console.log(`- ${prefix} ${message.key}#${message.value}`)\n        }\n      })\n    } catch (error) {\n      console.log('Error: ', error)\n    }\n  }\n\n  public async startBatchConsumer(): Promise<void> {\n    const topic: ConsumerSubscribeTopic = {\n      topic: 'example-topic',\n      fromBeginning: false\n    }\n\n    try {\n      await this.kafkaConsumer.connect()\n      await this.kafkaConsumer.subscribe(topic)\n      await this.kafkaConsumer.run({\n        eachBatch: async (eatchBatchPayload: EachBatchPayload) => {\n          const { topic, partition, batch } = eachBatchPayload\n          for (const message of batch.messages) {\n            const prefix = `${topic}[${partition} | ${message.offset}] / ${message.timestamp}`\n            console.log(`- ${prefix} ${message.key}#${message.value}`) \n          }\n        }\n      })\n    } catch (error) {\n      console.log('Error: ', error)\n    }\n  }\n\n  public async shutdown(): Promise<void> {\n    await this.kafkaConsumer.disconnect()\n  }\n\n  private createKafkaConsumer(): Consumer {\n    const kafka = new Kafka({ \n      clientId: 'client-id',\n      brokers: ['example.kafka.broker:9092']\n    })\n    const consumer = kafka.consumer({ groupId: 'consumer-group' })\n    return consumer\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using JavaCompatiblePartitioner in KafkaJS Producer\nDESCRIPTION: This snippet demonstrates how to use the JavaCompatiblePartitioner with a KafkaJS producer. It imports the Partitioners object and provides the JavaCompatiblePartitioner to the producer constructor.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Producing.md#2025-04-14_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Partitioners } = require('kafkajs')\nkafka.producer({ createPartitioner: Partitioners.JavaCompatiblePartitioner })\n```\n\n----------------------------------------\n\nTITLE: Calculating Retry Time in KafkaJS\nDESCRIPTION: This formula demonstrates how KafkaJS calculates retry times using an exponential backoff algorithm with randomization. It takes into account the previous retry time, a factor for randomization, and a multiplier for exponential growth.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/RetryDetailed.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nRandom(previousRetryTime * (1 - factor), previousRetryTime * (1 + factor)) * multiplier\n```\n\n----------------------------------------\n\nTITLE: Running Specific Kafka Version for KafkaJS Development\nDESCRIPTION: This shell command demonstrates how to start a Kafka cluster using a specific Docker Compose file, allowing developers to test against different Kafka versions.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/DevelopmentEnvironment.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nCOMPOSE_FILE=\"docker-compose.2_2.yml\" ./scripts/dockerComposeUp.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS IAM Authentication in KafkaJS\nDESCRIPTION: Demonstrates how to set up AWS IAM authentication in KafkaJS, including the necessary AWS credentials and optional session token for temporary credentials.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Configuration.md#2025-04-14_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  // authenticationTimeout: 10000,\n  // reauthenticationThreshold: 10000,\n  ssl: true,\n  sasl: {\n    mechanism: 'aws',\n    authorizationIdentity: 'AIDAIOSFODNN7EXAMPLE', // UserId or RoleId\n    accessKeyId: 'AKIAIOSFODNN7EXAMPLE',\n    secretAccessKey: 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',\n    sessionToken: 'WHArYt8i5vfQUrIU5ZbMLCbjcAiv/Eww6eL9tgQMJp6QFNEXAMPLETOKEN' // Optional\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Starting Kafka Development Environment\nDESCRIPTION: This shell command exports the host IP address and starts the Docker Compose configuration. It sets up the Kafka and Zookeeper containers for development use.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/DockerLocal.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nexport HOST_IP=$(ifconfig | grep -E \"([0-9]{1,3}\\.){3}[0-9]{1,3}\" | grep -v 127.0.0.1 | awk '{ print $2 }' | cut -f2 -d: | head -n1)\ndocker-compose up\n```\n\n----------------------------------------\n\nTITLE: Installing KafkaJS with Yarn\nDESCRIPTION: Command to install KafkaJS package using Yarn package manager.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/GettingStarted.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nyarn add kafkajs\n```\n\n----------------------------------------\n\nTITLE: Retry Mechanism Configuration\nDESCRIPTION: Sets up the retry mechanism with custom retry intervals and limits for connection and API call retries.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Configuration.md#2025-04-14_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  retry: {\n    initialRetryTime: 100,\n    retries: 8\n  }\n})\n```\n\n----------------------------------------\n\nTITLE: ResourceConfigEntry Structure in KafkaJS\nDESCRIPTION: Structure definition for resource configuration entries used when updating Kafka configurations, consisting of a configuration name and value pair.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Admin.md#2025-04-14_snippet_22\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    name: <String>,\n    value: <String>\n}\n```\n\n----------------------------------------\n\nTITLE: Installing KafkaJS Beta Version using Package Managers\nDESCRIPTION: Commands for installing the latest pre-release version of KafkaJS using either Yarn or NPM. This allows users to access the newest features before they are included in a stable release.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/PreReleases.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n# Yarn\nyarn add kafkajs@beta\n\n# Npm\nnpm install --save kafkajs@beta\n```\n\n----------------------------------------\n\nTITLE: Custom Authentication Interface Definition\nDESCRIPTION: TypeScript interface definitions for implementing custom authentication mechanisms, including required types and methods.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.4/CustomAuthenticationMechanism.md#2025-04-14_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\ntype SaslAuthenticateArgs<ParseResult> = {\n  request: SaslAuthenticationRequest\n  response?: SaslAuthenticationResponse<ParseResult>\n}\n\ntype AuthenticationProviderArgs = {\n  host: string\n  port: number\n  logger: Logger\n  saslAuthenticate: <ParseResult>(\n    args: SaslAuthenticateArgs<ParseResult>\n  ) => Promise<ParseResult | void>\n}\n\ntype Mechanism = {\n  mechanism: string\n  authenticationProvider: (args: AuthenticationProviderArgs) => Authenticator\n}\n\ntype Authenticator = {\n  authenticate(): Promise<void>\n}\n\ntype SaslAuthenticationRequest = {\n  encode: () => Buffer | Promise<Buffer>\n}\n\ntype SaslAuthenticationResponse<ParseResult> = {\n  decode: (rawResponse: Buffer) => Buffer | Promise<Buffer>\n  parse: (data: Buffer) => ParseResult\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Kafka Cluster and Creating SCRAM Credentials\nDESCRIPTION: Shell commands to start a multi-broker Kafka cluster and generate SCRAM credentials for authentication. These scripts run the cluster and set up the necessary authentication mechanisms.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/DevelopmentEnvironment.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n# This will run a Kafka cluster configured with your current IP\n./scripts/dockerComposeUp.sh\n./scripts/createScramCredentials.sh\n```\n\n----------------------------------------\n\nTITLE: Request Timeout Configuration\nDESCRIPTION: Configures the timeout value for API requests in milliseconds.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Configuration.md#2025-04-14_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  requestTimeout: 25000\n})\n```\n\n----------------------------------------\n\nTITLE: Docusaurus Project Directory Structure\nDESCRIPTION: Example of the expected file structure for a Docusaurus project, showing the organization of documentation files, website configuration, and content.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/README.md#2025-04-14_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nmy-docusaurus/\n  docs/\n    doc-1.md\n    doc-2.md\n    doc-3.md\n  website/\n    blog/\n      2016-3-11-oldest-post.md\n      2017-10-24-newest-post.md\n    core/\n    node_modules/\n    pages/\n    static/\n      css/\n      img/\n    package.json\n    sidebar.json\n    siteConfig.js\n```\n\n----------------------------------------\n\nTITLE: Running Extended Debug Logs with Full Fetch Response in KafkaJS Tests\nDESCRIPTION: This command enables extended debug logging, including full Fetch response buffer values, when running KafkaJS tests in watch mode. It sets the log level to debug and enables both standard and extended protocol buffer debugging.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Testing.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nKAFKAJS_LOG_LEVEL=debug KAFKAJS_DEBUG_PROTOCOL_BUFFERS=1 KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS=1 yarn test:local:watch\n```\n\n----------------------------------------\n\nTITLE: Formatting KafkaJS Changelog Entries\nDESCRIPTION: Changelog entries documenting version history using semantic versioning (MAJOR.MINOR.PATCH) with categorized changes under Fixed, Added, Changed, and Removed sections.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/CHANGELOG.md#2025-04-14_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n### Fixed\n  - Potential offset loss when updating offsets for resolved partitions #124\n  - Refresh metadata on lock timeout #131\n  - Cleans up stale brokers on metadata refresh #131\n  - Force metadata refresh on `ECONNREFUSED` #134\n  - Handle API version not supported #135\n  - Handle v0.10 messages on v0.11 Fetch API #143\n\n### Added\n  - Admin delete topics #117\n  - Update metadata api and allow to disable auto topic creation #118\n  - Use highest available API version #135 #146\n  - Admin describe and alter configs #138\n  - Validate message format in producer #142\n  - Consumers can detect that a topic was updated and force a rebalance #136\n\n### Changed\n  - Improved stack trace for `KafkaJSNumberOfRetriesExceeded` #123\n  - Enable Kafka v0.11 API by default #141\n  - Replace event emitter Lock #154\n  - Add member assignment to `GROUP_JOIN` instrumentation event #136\n```\n\n----------------------------------------\n\nTITLE: Connection Timeout Configuration\nDESCRIPTION: Sets the connection timeout value for broker connections in milliseconds.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Configuration.md#2025-04-14_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  connectionTimeout: 3000\n})\n```\n\n----------------------------------------\n\nTITLE: ResourceConfigQuery Structure in KafkaJS\nDESCRIPTION: Structure definition for resource config queries used when describing configurations, specifying the resource type, name, and optional config names to filter.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Admin.md#2025-04-14_snippet_16\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    type: <ResourceType>,\n    name: <String>,\n    configNames: <String[]>\n}\n```\n\n----------------------------------------\n\nTITLE: Examining Pre-release Package Information in package.json\nDESCRIPTION: Example of the additional metadata included in a pre-release version's package.json file. This includes the git SHA used to generate the version and a GitHub URL to compare changes with the previous stable release.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/PreReleases.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  // package.json\n  \"kafkajs\": {\n    \"sha\": \"43e325e18133b8d6c1c80f8e95ef8610c44ec631\",\n    \"compare\": \"https://github.com/tulios/kafkajs/compare/v1.9.3...43e325e18133b8d6c1c80f8e95ef8610c44ec631\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using ConfigResourceTypes Instead of ResourceTypes in KafkaJS v2.0.0\nDESCRIPTION: Shows how to use the new ConfigResourceTypes enum instead of the deprecated ResourceTypes enum when operating on configs in KafkaJS v2.0.0.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/MigrationGuide-2-0-0.md#2025-04-14_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n// Before\nimport { ResourceTypes } from 'kafkajs'\nawait admin.describeConfigs({\n  includeSynonyms: false,\n  resources: [\n    {\n      type: ResourceTypes.TOPIC,\n      name: 'topic-name'\n    }\n  ]\n})\n\n// After\nconst { ConfigResourceTypes } = require('kafkajs')\n\nawait admin.describeConfigs({\n  includeSynonyms: false,\n  resources: [\n    {\n      type: ConfigResourceTypes.TOPIC,\n      name: 'topic-name'\n    }\n  ]\n})\n```\n\n----------------------------------------\n\nTITLE: Creating Topics with Explicit Partitions in KafkaJS\nDESCRIPTION: Example of creating topics with explicit partition count and replication factor settings.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/MigrationGuide-2-0-0.md#2025-04-14_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nawait admin.createTopics({\n  topics: [{ topic: 'topic-name', numPartitions: 1, replicationFactor: 1 }]\n})\n```\n\n----------------------------------------\n\nTITLE: Running KafkaJS Tests with Docker\nDESCRIPTION: Commands for running tests in the KafkaJS project using Docker and Yarn. Includes options for running tests with docker-compose and creating SCRAM credentials.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/ContributionGuide.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nyarn test\n# or\n./scripts/dockerComposeUp.sh\n./scripts/createScramCredentials.sh\nyarn test:local:watch\n```\n\n----------------------------------------\n\nTITLE: Using Legacy Partitioner in KafkaJS\nDESCRIPTION: Shows how to use the LegacyPartitioner when upgrading from a version older than 2.0.0 to retain previous partitioning behavior.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Producing.md#2025-04-14_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Partitioners } = require('kafkajs')\nkafka.producer({ createPartitioner: Partitioners.LegacyPartitioner })\n```\n\n----------------------------------------\n\nTITLE: Running KafkaJS tests with debug logging\nDESCRIPTION: This command runs KafkaJS tests in local watch mode with debug logging enabled and protocol buffer debugging. It sets environment variables to control log levels and buffer visibility.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Testing.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nKAFKAJS_LOG_LEVEL=debug KAFKAJS_DEBUG_PROTOCOL_BUFFERS=1 yarn test:local:watch\n```\n\n----------------------------------------\n\nTITLE: Connecting to Kafka Cluster with KafkaJS\nDESCRIPTION: JavaScript code example showing how to connect to the local Kafka cluster with SSL and SASL authentication. It demonstrates setting up a Kafka client with the appropriate configuration for the development environment.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/DevelopmentEnvironment.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst fs = require('fs')\nconst ip = require('ip')\n\nconst { Kafka, CompressionTypes, logLevel } = require('./index')\n\nconst host = process.env.HOST_IP || ip.address()\n\nconst kafka = new Kafka({\n  logLevel: logLevel.DEBUG,\n  brokers: [`${host}:9094`, `${host}:9097`, `${host}:9100`],\n  clientId: 'example-producer',\n  ssl: {\n    servername: 'localhost',\n    rejectUnauthorized: false,\n    ca: [fs.readFileSync('./testHelpers/certs/cert-signed', 'utf-8')],\n  },\n  sasl: {\n    mechanism: 'plain',\n    username: 'test',\n    password: 'testtest',\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: SeekEntry Structure for Setting Offsets in KafkaJS\nDESCRIPTION: Structure definition for seek entries used when setting consumer group offsets, specifying the partition and target offset.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/Admin.md#2025-04-14_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    partition: <Number>,\n    offset: <String>,\n}\n```\n\n----------------------------------------\n\nTITLE: Examining KafkaJS Pre-release Package.json Additions\nDESCRIPTION: Example of additional information included in the package.json file for KafkaJS pre-release versions, showing the commit SHA and comparison URL.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/PreReleases.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  // package.json\n  \"kafkajs\": {\n    \"sha\": \"43e325e18133b8d6c1c80f8e95ef8610c44ec631\",\n    \"compare\": \"https://github.com/tulios/kafkajs/compare/v1.9.3...43e325e18133b8d6c1c80f8e95ef8610c44ec631\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running KafkaJS Tests with Docker\nDESCRIPTION: Commands for running the KafkaJS test suite using Docker and Yarn. Includes options for both one-time test execution and watch mode with SCRAM credentials setup.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/ContributionGuide.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nyarn test\n# or\n./scripts/dockerComposeUp.sh\n./scripts/createScramCredentials.sh\nyarn test:local:watch\n```\n\n----------------------------------------\n\nTITLE: Request Timeout Configuration\nDESCRIPTION: Configuration of request timeout duration in milliseconds. Determines maximum wait time for Kafka API requests.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.14.0/Configuration.md#2025-04-14_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nnew Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  requestTimeout: 25000\n})\n```\n\n----------------------------------------\n\nTITLE: Starting Docusaurus Development Server\nDESCRIPTION: Command to run the development server for the Docusaurus website using Yarn.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/README.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n# Start the site\n$ yarn start\n```\n\n----------------------------------------\n\nTITLE: Installing KafkaJS using npm\nDESCRIPTION: Command to install KafkaJS package using npm (Node Package Manager).\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/GettingStarted.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install kafkajs\n```\n\n----------------------------------------\n\nTITLE: Implementing a KafkaJS Producer with Error Handling in JavaScript\nDESCRIPTION: This example creates a Kafka producer that connects to a local Kafka broker, sends random messages at regular intervals, and handles termination gracefully. It includes compression, error handling, and proper cleanup for various process signals and error conditions.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/ProducerExample.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst fs = require('fs')\nconst ip = require('ip')\n\nconst { Kafka, CompressionTypes, logLevel } = require('../index')\n\nconst host = process.env.HOST_IP || ip.address()\n\nconst kafka = new Kafka({\n  logLevel: logLevel.DEBUG,\n  brokers: [`${host}:9092`],\n  clientId: 'example-producer',\n})\n\nconst topic = 'topic-test'\nconst producer = kafka.producer()\n\nconst getRandomNumber = () => Math.round(Math.random(10) * 1000)\nconst createMessage = num => ({\n  key: `key-${num}`,\n  value: `value-${num}-${new Date().toISOString()}`,\n})\n\nconst sendMessage = () => {\n  return producer\n    .send({\n      topic,\n      compression: CompressionTypes.GZIP,\n      messages: Array(getRandomNumber())\n        .fill()\n        .map(_ => createMessage(getRandomNumber())),\n    })\n    .then(console.log)\n    .catch(e => console.error(`[example/producer] ${e.message}`, e))\n}\n\nconst run = async () => {\n  await producer.connect()\n  setInterval(sendMessage, 3000)\n}\n\nrun().catch(e => console.error(`[example/producer] ${e.message}`, e))\n\nconst errorTypes = ['unhandledRejection', 'uncaughtException']\nconst signalTraps = ['SIGTERM', 'SIGINT', 'SIGUSR2']\n\nerrorTypes.map(type => {\n  process.on(type, async () => {\n    try {\n      console.log(`process.on ${type}`)\n      await producer.disconnect()\n      process.exit(0)\n    } catch (_) {\n      process.exit(1)\n    }\n  })\n})\n\nsignalTraps.map(type => {\n  process.once(type, async () => {\n    try {\n      await producer.disconnect()\n    } finally {\n      process.kill(process.pid, type)\n    }\n  })\n})\n```\n\n----------------------------------------\n\nTITLE: Accessing Namespaced Loggers in KafkaJS Components\nDESCRIPTION: This snippet demonstrates how to access and use namespaced loggers for different KafkaJS components such as the client, consumer, producer, and admin after instantiation.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/CustomLogger.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst client = new Kafka( ... )\nclient.logger().info( ... )\n\nconst consumer = kafka.consumer( ... )\nconsumer.logger().info( ... )\n\nconst producer = kafka.producer( ... )\nproducer.logger().info( ... )\n\nconst admin = kafka.admin( ... )\nadmin.logger().info( ... )\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka and Zookeeper Docker Services\nDESCRIPTION: Docker compose configuration that sets up a Kafka broker and Zookeeper instance for local development. Includes port mappings, environment variables, and volume mounts for Docker socket access.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/DockerLocal.md#2025-04-14_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: '2'\nservices:\n  zookeeper:\n    image: wurstmeister/zookeeper:latest\n    ports:\n      - \"2181:2181\"\n  kafka:\n    image: wurstmeister/kafka:2.11-1.1.1\n    ports:\n      - \"9092:9092\"\n    links:\n      - zookeeper\n    environment:\n      KAFKA_ADVERTISED_HOST_NAME: ${HOST_IP}\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'\n      KAFKA_DELETE_TOPIC_ENABLE: 'true'\n      KAFKA_CREATE_TOPICS: \"topic-test:1:1\"\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n```\n\n----------------------------------------\n\nTITLE: Configuring Snappy Compression in KafkaJS\nDESCRIPTION: Shows how to set up and use Snappy compression for message production in KafkaJS using the kafkajs-snappy package.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.0.0/Producing.md#2025-04-14_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst {  CompressionTypes, CompressionCodecs } = require('kafkajs')\nconst SnappyCodec = require('kafkajs-snappy')\n\nCompressionCodecs[CompressionTypes.Snappy] = SnappyCodec\n```\n\n----------------------------------------\n\nTITLE: Installing KafkaJS with Yarn\nDESCRIPTION: Command to install KafkaJS using the Yarn package manager.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/GettingStarted.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nyarn add kafkajs\n```\n\n----------------------------------------\n\nTITLE: Implementing a Basic Kafka Consumer with KafkaJS in JavaScript\nDESCRIPTION: This code demonstrates how to create a basic Kafka consumer that connects to a local Kafka instance. It subscribes to a topic and processes each message individually. The example includes proper error handling and graceful shutdown on various process signals.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/ConsumerExample.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst fs = require('fs')\nconst ip = require('ip')\n\nconst { Kafka, logLevel } = require('../index')\n\nconst host = process.env.HOST_IP || ip.address()\n\nconst kafka = new Kafka({\n  logLevel: logLevel.INFO,\n  brokers: [`${host}:9092`],\n  clientId: 'example-consumer',\n})\n\nconst topic = 'topic-test'\nconst consumer = kafka.consumer({ groupId: 'test-group' })\n\nconst run = async () => {\n  await consumer.connect()\n  await consumer.subscribe({ topic })\n  await consumer.run({\n    // eachBatch: async ({ batch }) => {\n    //   console.log(batch)\n    // },\n    eachMessage: async ({ topic, partition, message }) => {\n      const prefix = `${topic}[${partition} | ${message.offset}] / ${message.timestamp}`\n      console.log(`- ${prefix} ${message.key}#${message.value}`)\n    },\n  })\n}\n\nrun().catch(e => console.error(`[example/consumer] ${e.message}`, e))\n\nconst errorTypes = ['unhandledRejection', 'uncaughtException']\nconst signalTraps = ['SIGTERM', 'SIGINT', 'SIGUSR2']\n\nerrorTypes.map(type => {\n  process.on(type, async e => {\n    try {\n      console.log(`process.on ${type}`)\n      console.error(e)\n      await consumer.disconnect()\n      process.exit(0)\n    } catch (_) {\n      process.exit(1)\n    }\n  })\n})\n\nsignalTraps.map(type => {\n  process.once(type, async () => {\n    try {\n      await consumer.disconnect()\n    } finally {\n      process.kill(process.pid, type)\n    }\n  })\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring KafkaJS Client with Custom Logger\nDESCRIPTION: Shows how to apply a custom log creator when initializing a new Kafka client instance, including setting the desired log level.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/CustomLogger.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst kafka = new Kafka({\n    clientId: 'my-app',\n    brokers: ['kafka1:9092', 'kafka2:9092'],\n    logLevel: logLevel.ERROR,\n    logCreator: WinstonLogCreator\n})\n```\n\n----------------------------------------\n\nTITLE: Basic Log Structure Example in KafkaJS\nDESCRIPTION: Shows the standard structure of a log object in KafkaJS including level, label, timestamp, logger name, and message.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-2.2.4/CustomLogger.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    level: 4,\n    label: 'INFO', // NOTHING, ERROR, WARN, INFO, or DEBUG\n    timestamp: '2017-12-29T13:39:54.575Z',\n    logger: 'kafkajs',\n    message: 'Started',\n    // ... any other extra key provided to the log function\n}\n```\n\n----------------------------------------\n\nTITLE: Overriding Log Levels for Different KafkaJS Components\nDESCRIPTION: Shows how to set different log levels for individual components (kafka instance, producer, consumer, admin) after initialization using the setLogLevel method.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/Configuration.md#2025-04-14_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Kafka, logLevel } = require('kafkajs')\n\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092'],\n  logLevel: logLevel.ERROR\n})\nkafka.logger().setLogLevel(logLevel.WARN)\n\nconst producer = kafka.producer(...)\nproducer.logger().setLogLevel(logLevel.INFO)\n\nconst consumer = kafka.consumer(...)\nconsumer.logger().setLogLevel(logLevel.DEBUG)\n\nconst admin = kafka.admin(...)\nadmin.logger().setLogLevel(logLevel.NOTHING)\n```\n\n----------------------------------------\n\nTITLE: Installing KafkaJS using Yarn\nDESCRIPTION: Command to install KafkaJS package using Yarn package manager.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/GettingStarted.md#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nyarn add kafkajs\n```\n\n----------------------------------------\n\nTITLE: Retry Time Calculation Formula\nDESCRIPTION: Formula used to calculate retry intervals with exponential backoff and randomization factor. The calculation uses initialRetryTime (default 300ms), a randomization factor (default 0.2), and a multiplier (default 2) to determine subsequent retry intervals.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/RetryDetailed.md#2025-04-14_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nRandom(previousRetryTime * (1 - factor), previousRetryTime * (1 + factor)) * multiplier\n```\n\n----------------------------------------\n\nTITLE: VS Code jsconfig.json Configuration for KafkaJS Development\nDESCRIPTION: JSON configuration file for Visual Studio Code to improve JavaScript language service integration with KafkaJS. This helps with type hinting even for modules that don't directly import each other.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/DevelopmentEnvironment.md#2025-04-14_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"compilerOptions\": {\n    \"baseUrl\": \".\",\n    \"module\": \"commonjs\",\n    \"target\": \"es6\",\n    \"paths\": {\n      \"testHelpers\": [\"./testHelpers\"]\n    }\n  },\n  \"include\": [\n    \"src\",\n    \"testHelpers\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Retry Formula Calculation Examples\nDESCRIPTION: Demonstrates the calculation of retry intervals using exponential backoff with randomization. The formula uses initialRetryTime (default 300ms), a factor (0.2), and multiplier (2) to calculate subsequent retry intervals.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/RetryDetailed.md#2025-04-14_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nRandom(previousRetryTime * (1 - factor), previousRetryTime * (1 + factor)) * multiplier\n```\n\n----------------------------------------\n\nTITLE: Creating a New Markdown Document for Docusaurus\nDESCRIPTION: Example of creating a new Markdown document with proper frontmatter including document ID and title.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/README.md#2025-04-14_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n---\nid: newly-created-doc\ntitle: This Doc Needs To Be Edited\n---\n\nMy new content here..\n```\n\n----------------------------------------\n\nTITLE: Running KafkaJS Tests with Docker and Yarn\nDESCRIPTION: Commands for running the KafkaJS test suite using Docker, Docker Compose, and Yarn. Includes options for both one-time test execution and watch mode with SCRAM credentials setup.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/ContributionGuide.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nyarn test\n# or\n./scripts/dockerComposeUp.sh\n./scripts/createScramCredentials.sh\nyarn test:local:watch\n```\n\n----------------------------------------\n\nTITLE: Running KafkaJS Tests with Docker\nDESCRIPTION: Commands for running tests in the KafkaJS project using Docker and Docker Compose. Shows both the simple approach and the more granular approach with individual scripts for setting up the environment and running tests in watch mode.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/ContributionGuide.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nyarn test\n# or\n./scripts/dockerComposeUp.sh\n./scripts/createScramCredentials.sh\nyarn test:local:watch\n```\n\n----------------------------------------\n\nTITLE: Running Specific Kafka Version\nDESCRIPTION: Command to run a specific version of Kafka using an alternate docker-compose file\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.13.0/DevelopmentEnvironment.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nCOMPOSE_FILE=\"docker-compose.2_3.yml\" ./scripts/dockerComposeUp.sh\n```\n\n----------------------------------------\n\nTITLE: Enabling Extended Debug Logging in KafkaJS Tests\nDESCRIPTION: Command to run tests with full debug logging including extended protocol buffers and Fetch response payload visibility. Uses multiple environment variables for maximum debug output.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/Testing.md#2025-04-14_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nKAFKAJS_LOG_LEVEL=debug KAFKAJS_DEBUG_PROTOCOL_BUFFERS=1 KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS=1 yarn test:local:watch\n```\n\n----------------------------------------\n\nTITLE: Connecting to Kafka Cluster in KafkaJS Development Environment\nDESCRIPTION: This JavaScript code snippet shows how to connect to the local Kafka cluster set up for development. It includes SSL and SASL authentication configuration, and uses environment variables for flexibility.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.10.0/DevelopmentEnvironment.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst fs = require('fs')\nconst ip = require('ip')\n\nconst { Kafka, CompressionTypes, logLevel } = require('./index')\n\nconst host = process.env.HOST_IP || ip.address()\n\nconst kafka = new Kafka({\n  logLevel: logLevel.DEBUG,\n  brokers: [`${host}:9094`, `${host}:9097`, `${host}:9100`],\n  clientId: 'example-producer',\n  ssl: {\n    servername: 'localhost',\n    rejectUnauthorized: false,\n    ca: [fs.readFileSync('./testHelpers/certs/cert-signed', 'utf-8')],\n  },\n  sasl: {\n    mechanism: 'plain',\n    username: 'test',\n    password: 'testtest',\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Connecting to a Local Kafka Cluster with KafkaJS\nDESCRIPTION: JavaScript example demonstrating how to create a Kafka client that connects to the local development cluster. The configuration includes SSL settings and SASL authentication with the test credentials.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/docs/DevelopmentEnvironment.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst fs = require('fs')\nconst ip = require('ip')\n\nconst { Kafka, CompressionTypes, logLevel } = require('./index')\n\nconst host = process.env.HOST_IP || ip.address()\n\nconst kafka = new Kafka({\n  logLevel: logLevel.DEBUG,\n  brokers: [`${host}:9094`, `${host}:9097`, `${host}:9100`],\n  clientId: 'example-producer',\n  ssl: {\n    servername: 'localhost',\n    rejectUnauthorized: false,\n    ca: [fs.readFileSync('./testHelpers/certs/cert-signed', 'utf-8')],\n  },\n  sasl: {\n    mechanism: 'plain',\n    username: 'test',\n    password: 'testtest',\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Manually Committing Offsets with KafkaJS Consumer\nDESCRIPTION: Demonstrates how to manually commit offsets using the KafkaJS consumer. This snippet shows how to run the consumer with autoCommit disabled and then commit offsets for specific topics and partitions.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Consuming.md#2025-04-14_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconsumer.run({\n    autoCommit: false,\n    eachMessage: async ({ topic, partition, message }) => {\n        // Process the message somehow\n    },\n})\n\nconsumer.commitOffsets([\n  { topic: 'topic-A', partition: 0, offset: '1' },\n  { topic: 'topic-A', partition: 1, offset: '3' },\n  { topic: 'topic-B', partition: 0, offset: '2' }\n])\n```\n\n----------------------------------------\n\nTITLE: Initializing KafkaJS Consumer with Options\nDESCRIPTION: Demonstrates how to initialize a KafkaJS consumer with various configuration options. This includes settings for group management, metadata refreshing, and fetch behavior.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/Consuming.md#2025-04-14_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nkafka.consumer({\n  groupId: <String>,\n  partitionAssigners: <Array>,\n  sessionTimeout: <Number>,\n  rebalanceTimeout: <Number>,\n  heartbeatInterval: <Number>,\n  metadataMaxAge: <Number>,\n  allowAutoTopicCreation: <Boolean>,\n  maxBytesPerPartition: <Number>,\n  minBytes: <Number>,\n  maxBytes: <Number>,\n  maxWaitTimeInMs: <Number>,\n  retry: <Object>,\n})\n```\n\n----------------------------------------\n\nTITLE: Sending GZIP Compressed Messages with KafkaJS Producer\nDESCRIPTION: This snippet demonstrates how to send GZIP compressed messages using the KafkaJS producer. It imports the CompressionTypes enum and sets the compression option in the send method.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Producing.md#2025-04-14_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst { CompressionTypes } = require('kafkajs')\n\nasync () => {\n  await producer.send({\n    topic: 'topic-name',\n    compression: CompressionTypes.GZIP,\n    messages: [\n        { key: 'key1', value: 'hello world' },\n        { key: 'key2', value: 'hey hey!' }\n    ],\n  })\n}\n```\n\n----------------------------------------\n\nTITLE: Subscribing to KafkaJS Events with Event Listener\nDESCRIPTION: Shows how to subscribe to instrumentation events using the 'on' method of KafkaJS clients. The example demonstrates subscribing to a HEARTBEAT event and later removing the listener.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.11.0/InstrumentationEvents.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst { HEARTBEAT } = consumer.events\nconst removeListener = consumer.on(HEARTBEAT, e => console.log(`heartbeat at ${e.timestamp}`))\n\n// Remove the listener by invoking removeListener()\n```\n\n----------------------------------------\n\nTITLE: Running Debug Logs with Protocol Buffers in KafkaJS Tests\nDESCRIPTION: This command enables debug logging and protocol buffer output when running KafkaJS tests in watch mode. It sets the log level to debug and enables protocol buffer debugging.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/versioned_docs/version-1.12.0/Testing.md#2025-04-14_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nKAFKAJS_LOG_LEVEL=debug KAFKAJS_DEBUG_PROTOCOL_BUFFERS=1 yarn test:local:watch\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Pages to Navigation in Docusaurus\nDESCRIPTION: Example of how to update the siteConfig.js file to add a custom page to the navigation header by defining it in the headerLinks array.\nSOURCE: https://github.com/tulios/kafkajs/blob/master/website/README.md#2025-04-14_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  headerLinks: [\n    ...\n    { page: 'my-new-custom-page', label: 'My New Custom Page' },\n    ...\n  ],\n  ...\n}\n```"
  }
]
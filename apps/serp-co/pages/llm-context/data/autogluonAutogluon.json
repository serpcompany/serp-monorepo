[
  {
    "owner": "autogluon",
    "repo": "autogluon",
    "content": "TITLE: Training and Using TabularPredictor in 3 Lines of Code\nDESCRIPTION: Complete example of AutoGluon's tabular prediction capabilities, demonstrating how to import the TabularPredictor, fit it on training data, and generate predictions on test data in just three lines of code.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/README.md#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom autogluon.tabular import TabularPredictor\npredictor = TabularPredictor(label=\"class\").fit(\"train.csv\")\npredictions = predictor.predict(\"test.csv\")\n```\n\n----------------------------------------\n\nTITLE: Training Document Classifier with MultiModalPredictor\nDESCRIPTION: Creates and trains a MultiModalPredictor for document classification using LayoutLM as the document transformer. The training is limited to 120 seconds for demonstration purposes.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/document_prediction/document_classification.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\n\npredictor = MultiModalPredictor(label=\"label\")\npredictor.fit(\n    train_data=train_data,\n    hyperparameters={\"model.document_transformer.checkpoint_name\":\"microsoft/layoutlm-base-uncased\",\n    \"optim.top_k_average_method\":\"best\",\n    },\n    time_limit=120,\n)\n```\n\n----------------------------------------\n\nTITLE: Tabular Prediction with AutoGluon\nDESCRIPTION: Example of using TabularPredictor to predict a class column in a data table. Loads data from S3, creates a predictor instance, fits it on training data and generates predictions.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/index.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.tabular import TabularDataset, TabularPredictor\n\ndata_root = 'https://autogluon.s3.amazonaws.com/datasets/Inc/'\ntrain_data = TabularDataset(data_root + 'train.csv')\ntest_data = TabularDataset(data_root + 'test.csv')\n\npredictor = TabularPredictor(label='class').fit(train_data=train_data)\npredictions = predictor.predict(test_data)\n```\n\n----------------------------------------\n\nTITLE: Training a MultiModalPredictor with Custom Hyperparameters in Python\nDESCRIPTION: This code demonstrates how to train the MultiModalPredictor with custom hyperparameters. It configures the model to use a Swin Transformer for image classification with specific training transforms, optimization parameters, and loss function.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/kaggle_pawpularity/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npredictor.fit(  \n\ttrain_data=training_df,\n\ttuning_data=valid_df,  \n\tsave_path=save_path,  \n\thyperparameters={\n\t\t\"model.names\": \"['timm_image']\",\n\t\t\"model.timm_image.checkpoint_name\": \"swin_large_patch4_window7_224\",\n\t\t\"model.timm_image.train_transforms\": \"['resize_shorter_side','center_crop','randaug']\",\n\t\t\"data.categorical.convert_to_text\": \"False\",\n\t\t\"env.per_gpu_batch_size\": \"16\",\n\t\t\"env.per_gpu_batch_size_evaluation\": \"32\",\n\t\t\"env.precision\": \"32\",\n\t\t\"optim.lr\": \"2e-5\",\n\t\t\"optim.weight_decay\": \"0\",\n\t\t\"optim.lr_decay\": \"1\",\n\t\t\"optim.max_epochs\": \"5\",\n\t\t\"optim.warmup_steps\": \"0\",\n\t\t\"optim.loss_func\": \"bcewithlogitsloss\",\n\t},\n\tseed=1,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Numerical Label Preprocessing in AutoMM Python\nDESCRIPTION: Specifies the preprocessing method for numerical labels in regression tasks. Options are `\"standardscaler\"` (default, remove mean and scale to unit variance) or `\"minmaxscaler\"` (scale labels to the range [0, 1]). Set this using `data.label.numerical_preprocessing` in the `hyperparameters` argument of `predictor.fit()`.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_63\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"data.label.numerical_preprocessing\": \"standardscaler\"})\n# scale numerical labels to (0, 1)\npredictor.fit(hyperparameters={\"data.label.numerical_preprocessing\": \"minmaxscaler\"})\n```\n\n----------------------------------------\n\nTITLE: Setting Model Compilation Mode\nDESCRIPTION: Configuration for PyTorch compilation mode, affecting optimization and CUDA graph usage.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"env.compile.mode\": \"default\"})\n# reduces the overhead of python with CUDA graphs, useful for small batches.\npredictor.fit(hyperparameters={\"env.compile.mode\": \"reduce-overhead\"})\n```\n\n----------------------------------------\n\nTITLE: Hyperparameter Tuning Custom Models in AutoGluon\nDESCRIPTION: This code snippet shows how to perform hyperparameter tuning for custom models in AutoGluon. It demonstrates specifying a hyperparameter search space and tuning for a specified time limit.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-model.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.common import space\ncustom_hyperparameters_hpo = {CustomRandomForestModel: {\n    'max_depth': space.Int(lower=5, upper=30),\n    'max_features': space.Real(lower=0.1, upper=1.0),\n    'criterion': space.Categorical('gini', 'entropy'),\n}}\n# Hyperparameter tune CustomRandomForestModel for 20 seconds\npredictor = TabularPredictor(label=label).fit(train_data,\n                                              hyperparameters=custom_hyperparameters_hpo,\n                                              hyperparameter_tune_kwargs='auto',  # enables HPO\n                                              time_limit=20)\n\n# Show predictor leaderboard (HPO)\nleaderboard_hpo = predictor.leaderboard()\nleaderboard_hpo\n\n# Get hyperparameters of the best model\nbest_model_name = leaderboard_hpo[leaderboard_hpo['stack_level'] == 1]['model'].iloc[0]\npredictor_info = predictor.info()\nbest_model_info = predictor_info['model_info'][best_model_name]\nprint(best_model_info)\nprint(f'Best Model Hyperparameters ({best_model_name}):')\nprint(best_model_info['hyperparameters'])\n```\n\n----------------------------------------\n\nTITLE: Manually Configuring Models in AutoGluon TimeSeriesPredictor (Python)\nDESCRIPTION: This snippet demonstrates how to manually specify which models the `TimeSeriesPredictor` should train and override their default hyperparameters. The `hyperparameters` argument takes a dictionary where keys are model names and values are either an empty dictionary (for defaults) or a list of dictionaries specifying different hyperparameter sets for that model. This example configures DeepAR with defaults and trains two versions of Theta with different settings.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-indepth.ipynb#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\npredictor = TimeSeriesPredictor(...)\n\npredictor.fit(\n    ...\n    hyperparameters={\n        \"DeepAR\": {},\n        \"Theta\": [\n            {\"decomposition_type\": \"additive\"},\n            {\"seasonal_period\": 1},\n        ],\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Distilling Models in AutoGluon\nDESCRIPTION: Uses model distillation to train a single student model that mimics ensemble predictions, providing a balance between computational efficiency and accuracy.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-indepth.ipynb#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nstudent_models = predictor.distill(time_limit=30)\nprint(student_models)\npreds_student = predictor.predict(test_data_nolabel, model=student_models[0])\nprint(f\"predictions from {student_models[0]}:\", list(preds_student)[:5])\npredictor.leaderboard(test_data)\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Library in Python\nDESCRIPTION: Commands to install the latest version of pip and the AutoGluon library, which is required for multimodal prediction tasks.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal-quick-start.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!python -m pip install --upgrade pip\n!python -m pip install autogluon\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating TimeSeriesPredictor in Python\nDESCRIPTION: This code snippet shows how to train a TimeSeriesPredictor using the train data and evaluate its performance on the test data. It uses the MASE (Mean Absolute Scaled Error) as the evaluation metric.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-indepth.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\npredictor = TimeSeriesPredictor(prediction_length=prediction_length, eval_metric=\"MASE\").fit(train_data)\npredictor.evaluate(test_data)\n```\n\n----------------------------------------\n\nTITLE: Finetuning the Model\nDESCRIPTION: Finetunes the pretrained model on the Pothole dataset with custom hyperparameters for learning rate, batch size, and training epochs.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/advanced/finetune_coco.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npredictor.fit(\n    train_path,\n    tuning_data=val_path,\n    hyperparameters={\n        \"optim.lr\": 1e-4,  # we use two stage and detection head has 100x lr\n        \"env.per_gpu_batch_size\": 16,  # decrease it when model is large or GPU memory is small\n        \"optim.max_epochs\": 30,  # max number of training epochs, note that we may early stop before this based on validation setting\n        \"optim.val_check_interval\": 1.0,  # Do 1 validation each epoch\n        \"optim.check_val_every_n_epoch\": 3,  # Do 1 validation each 3 epochs\n        \"optim.patience\": 3,  # Early stop after 3 consective validations are not the best\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Random Forest Model for AutoGluon\nDESCRIPTION: Complete implementation of a custom Random Forest model class that inherits from AutoGluon's AbstractModel. Includes preprocessing, model fitting, and parameter configuration methods.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-model.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\n\nfrom autogluon.core.models import AbstractModel\nfrom autogluon.features.generators import LabelEncoderFeatureGenerator\n\nclass CustomRandomForestModel(AbstractModel):\n    def __init__(self, **kwargs):\n        # Simply pass along kwargs to parent, and init our internal `_feature_generator` variable to None\n        super().__init__(**kwargs)\n        self._feature_generator = None\n\n    def _preprocess(self, X: pd.DataFrame, is_train=False, **kwargs) -> np.ndarray:\n        print(f'Entering the `_preprocess` method: {len(X)} rows of data (is_train={is_train})')\n        X = super()._preprocess(X, **kwargs)\n\n        if is_train:\n            self._feature_generator = LabelEncoderFeatureGenerator(verbosity=0)\n            self._feature_generator.fit(X=X)\n        if self._feature_generator.features_in:\n            X = X.copy()\n            X[self._feature_generator.features_in] = self._feature_generator.transform(X=X)\n        return X.fillna(0).to_numpy(dtype=np.float32)\n\n    def _fit(self,\n             X: pd.DataFrame,\n             y: pd.Series,\n             **kwargs):\n        print('Entering the `_fit` method')\n\n        from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n\n        if self.problem_type in ['regression', 'softclass']:\n            model_cls = RandomForestRegressor\n        else:\n            model_cls = RandomForestClassifier\n\n        X = self.preprocess(X, is_train=True)\n        params = self._get_model_params()\n        print(f'Hyperparameters: {params}')\n        self.model = model_cls(**params)\n        self.model.fit(X, y)\n        print('Exiting the `_fit` method')\n\n    def _set_default_params(self):\n        default_params = {\n            'n_estimators': 300,\n            'n_jobs': -1,\n            'random_state': 0,\n        }\n        for param, val in default_params.items():\n            self._set_default_param_value(param, val)\n\n    def _get_default_auxiliary_params(self) -> dict:\n        default_auxiliary_params = super()._get_default_auxiliary_params()\n        extra_auxiliary_params = dict(\n            valid_raw_types=['int', 'float', 'category'],\n        )\n        default_auxiliary_params.update(extra_auxiliary_params)\n        return default_auxiliary_params\n```\n\n----------------------------------------\n\nTITLE: Loading Saved Model and Inference\nDESCRIPTION: Loading a saved model, adjusting GPU usage, and performing predictions on the test set.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/quick_start/quick_start_coco.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nnew_predictor = MultiModalPredictor.load(model_path)\nnew_predictor.set_num_gpus(1)\n\nnew_predictor.evaluate(test_path)\n\npred = predictor.predict(test_path)\nprint(len(pred))  # Number of predictions\nprint(pred[:3])   # Sample of first 3 predictions\n```\n\n----------------------------------------\n\nTITLE: Calculating Feature Importance in AutoGluon (Python)\nDESCRIPTION: This snippet demonstrates how to compute feature importance scores for an AutoGluon predictor using permutation-shuffling. These scores help understand which features contribute most to the model's accuracy.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-indepth.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\npredictor.feature_importance(test_data)\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon via pip\nDESCRIPTION: Simple pip installation command for the AutoGluon library. This installs the base package which supports Python 3.9-3.12 on Linux, MacOS, and Windows platforms.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install autogluon\n```\n\n----------------------------------------\n\nTITLE: Maximizing Predictive Performance with AutoGluon Tabular\nDESCRIPTION: Code snippet demonstrating how to initialize and fit a TabularPredictor with best quality presets to maximize predictive accuracy. It specifies a time limit and evaluation metric for the training process.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-essentials.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntime_limit = 60  # for quick demonstration only, you should set this to longest time you are willing to wait (in seconds)\nmetric = 'roc_auc'  # specify your evaluation metric here\npredictor = TabularPredictor(label, eval_metric=metric).fit(train_data, time_limit=time_limit, presets='best_quality')\n```\n\n----------------------------------------\n\nTITLE: Initializing TabularPredictor with Explicit Problem Type in Python\nDESCRIPTION: This code snippet demonstrates how to initialize a TabularPredictor with an explicitly specified problem type, overriding the automatic inference.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-feature-engineering.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npredictor = TabularPredictor(label='class', problem_type='multiclass').fit(train_data)\n```\n\n----------------------------------------\n\nTITLE: Model Fitting with Feature Preservation\nDESCRIPTION: Demonstrates model fitting with both standard and feature-preserving custom models\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-model-advanced.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndummy_model = DummyModel()\ndummy_model.fit(X=X, y=y, feature_metadata=my_custom_feature_generator.feature_metadata)\n\ndummy_model_keep_unique = DummyModelKeepUnique()\ndummy_model_keep_unique.fit(X=X, y=y, feature_metadata=my_custom_feature_generator.feature_metadata)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Models and Generating Leaderboard in Python\nDESCRIPTION: This code demonstrates how to evaluate all trained models on test data, generate a leaderboard with extra information, and display scores for additional metrics.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-indepth.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npredictor.leaderboard(test_data)\npredictor.leaderboard(extra_info=True)\npredictor.leaderboard(test_data, extra_metrics=['accuracy', 'balanced_accuracy', 'log_loss'])\n```\n\n----------------------------------------\n\nTITLE: Creating AutoGluon Wrapper Class for SHAP Compatibility in Python\nDESCRIPTION: This code defines a wrapper class for AutoGluon predictor to make it compatible with SHAP's KernelExplainer.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/interpret/SHAP with AutoGluon-Tabular Diabetes regression.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass AutogluonWrapper:\n    def __init__(self, predictor, feature_names):\n        self.ag_model = predictor\n        self.feature_names = feature_names\n    \n    def predict(self, X):\n        if isinstance(X, pd.Series):\n            X = X.values.reshape(1,-1)\n        if not isinstance(X, pd.DataFrame):\n            X = pd.DataFrame(X, columns=self.feature_names)\n        return self.ag_model.predict(X)\n```\n\n----------------------------------------\n\nTITLE: Enabling GPU Acceleration for TabularPredictor in Python\nDESCRIPTION: This code snippet demonstrates how to enable GPU acceleration for the entire TabularPredictor by specifying the number of GPUs to use during training.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-gpu.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npredictor = TabularPredictor(label=label).fit(\n    train_data,\n    num_gpus=1,  # Grant 1 gpu for the entire Tabular Predictor\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Parameter-Efficient Fine-Tuning in AutoMM\nDESCRIPTION: Examples showing how to configure parameter-efficient fine-tuning (PEFT) in AutoMM, with options including bit_fit, norm_fit, lora, and various combinations.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"optim.peft\": None})\n# finetune only bias parameters\npredictor.fit(hyperparameters={\"optim.peft\": \"bit_fit\"})\n# finetune with IA3 + BitFit\npredictor.fit(hyperparameters={\"optim.peft\": \"ia3_bias\"})\n```\n\n----------------------------------------\n\nTITLE: Training MultiModal Predictor\nDESCRIPTION: Initializes and trains MultiModalPredictor for sentiment analysis with specified parameters.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/beginner_text.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\nimport uuid\nmodel_path = f\"./tmp/{uuid.uuid4().hex}-automm_sst\"\npredictor = MultiModalPredictor(label='label', eval_metric='acc', path=model_path)\npredictor.fit(train_data, time_limit=180)\n```\n\n----------------------------------------\n\nTITLE: Loading Saved TabularPredictor and Making Predictions in Python\nDESCRIPTION: This snippet shows how to load a previously trained TabularPredictor from disk and make predictions on new data, including individual examples and probability predictions.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-indepth.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npredictor = TabularPredictor.load(save_path)\npredictor.features()\n\ndatapoint = test_data_nolabel.iloc[[0]]\nprint(datapoint)\npredictor.predict(datapoint)\n\npredictor.predict_proba(datapoint)\n```\n\n----------------------------------------\n\nTITLE: Predicting Probabilities with AutoGluon Tabular\nDESCRIPTION: Code example showing how to get prediction probabilities from a trained AutoGluon model and access class labels.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-faq.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npredictor.predict_proba(test_data)\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Multimodal Package\nDESCRIPTION: Installs the AutoGluon multimodal package using pip.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/beginner_text.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.multimodal\n```\n\n----------------------------------------\n\nTITLE: Creating TimeSeriesDataFrame with Static Features in Python\nDESCRIPTION: This snippet shows how to create a TimeSeriesDataFrame object from a pandas DataFrame, specifying the id and timestamp columns, and including static features. It also demonstrates how to access the static features of the created object.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-indepth.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntrain_data = TimeSeriesDataFrame.from_data_frame(\n    df,\n    id_column=\"item_id\",\n    timestamp_column=\"timestamp\",\n    static_features_df=static_features_df,\n)\ntrain_data.head()\n\ntrain_data.static_features.head()\n```\n\n----------------------------------------\n\nTITLE: Splitting Time Series Data for Evaluation in Python\nDESCRIPTION: This code demonstrates how to split a TimeSeriesDataFrame into train and test sets for evaluating forecast accuracy. It uses the train_test_split method to create separate datasets for historical data and the forecast horizon.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-indepth.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nprediction_length = 48\ndata = TimeSeriesDataFrame.from_path(\"https://autogluon.s3.amazonaws.com/datasets/timeseries/m4_hourly_subset/train.csv\")\ntrain_data, test_data = data.train_test_split(prediction_length)\n```\n\n----------------------------------------\n\nTITLE: Configuring AutoMM Hyperparameters for FT-Transformer in Python\nDESCRIPTION: Detailed configuration of AutoMM hyperparameters for FT-Transformer experiments. It includes settings for data processing, model architecture, batch size, optimization, and learning rate schedule.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/tabular_dl/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nautomm_hyperparameters = {\n    \"data.categorical.convert_to_text\": False,\n    \"model.names\": [\"ft_transformer\"],\n    \"model.ft_transformer.embedding_arch\": [\"linear\"],\n    \"env.batch_size\": 128,\n    \"env.per_gpu_batch_size\": 128,\n    \"env.inference_batch_size_ratio\": 1,\n    \"env.num_workers\": 12,\n    \"env.num_workers_inference\": 12,\n    \"env.num_gpus\": 1,\n    \"optim.max_epochs\": 2000,\n    \"optim.weight_decay\": 1.0e-5,\n    \"optim.lr_choice\": None,\n    \"optim.lr_schedule\": \"polynomial_decay\",\n    \"optim.warmup_steps\": 0.0,\n    \"optim.patience\": 20,\n    \"optim.top_k\": 3,\n}\n```\n\n----------------------------------------\n\nTITLE: Compiling AutoGluon TabularPredictor Models for Improved Inference Speed in Python\nDESCRIPTION: Demonstrates compiling the optimized predictor to convert sklearn function calls into ONNX equivalents for improved inference speed. This is an experimental feature that requires additional dependencies.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-deployment.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npredictor_clone_opt.compile()\n\ny_pred_compile_opt = predictor_clone_opt.predict(test_data)\ny_pred_compile_opt\n```\n\n----------------------------------------\n\nTITLE: Text Classification with MultiModalPredictor\nDESCRIPTION: Demonstrates text classification using MultiModalPredictor. Loads parquet data, creates a predictor for text labels, fits on training data and makes predictions.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/index.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\nfrom autogluon.core.utils.loaders import load_pd\n\ndata_root = 'https://autogluon-text.s3-accelerate.amazonaws.com/glue/sst/'\ntrain_data = load_pd.load(data_root + 'train.parquet')\ntest_data = load_pd.load(data_root + 'dev.parquet')\n\npredictor = MultiModalPredictor(label='label').fit(train_data=train_data)\npredictions = predictor.predict(test_data)\n```\n\n----------------------------------------\n\nTITLE: Loading AutoGluon Multimodal Predictor in Offline Environment\nDESCRIPTION: Code snippet demonstrating how to load a previously saved AutoGluon Multimodal predictor in an offline environment without internet access. This requires the predictor to have been saved with the standalone=True parameter.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal-faq.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\n\npredictor = MultiModalPredictor.load(SAVE_PATH)\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoGluon Lambda Handler\nDESCRIPTION: Python script implementing the Lambda handler function for serving AutoGluon model predictions. Handles JSON input data and returns prediction probabilities.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/cloud_fit_deploy/cloud-aws-lambda-deployment.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom autogluon.tabular import TabularPredictor\n\nmodel = TabularPredictor.load('/opt/ml/model')\nmodel.persist(models='all')\n\n\n# Lambda handler code\ndef lambda_handler(event, context):\n    df = pd.read_json(event['body'])\n    pred_probs = model.predict_proba(df)\n    return {\n        'statusCode': 200,\n        'body': pred_probs.to_json()\n    }\n```\n\n----------------------------------------\n\nTITLE: Preparing Data and Initializing TimeSeriesPredictor\nDESCRIPTION: Splits the data into train and test sets, and initializes a TimeSeriesPredictor with Chronos-Bolt model for zero-shot forecasting.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-chronos.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprediction_length = 48\ntrain_data, test_data = data.train_test_split(prediction_length)\n\npredictor = TimeSeriesPredictor(prediction_length=prediction_length).fit(\n    train_data, presets=\"bolt_small\",\n)\n```\n\n----------------------------------------\n\nTITLE: Training Object Detection Model with DataFrame in AutoGluon\nDESCRIPTION: This code snippet demonstrates how to train an object detection model using AutoGluon's MultiModalPredictor with data in DataFrame format. It sets up the model configuration, initializes the predictor, and fits the model to the training data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/object_detection_with_dataframe.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\nimport uuid\n\ncheckpoint_name = \"yolov3_mobilenetv2_320_300e_coco\"\nnum_gpus = -1  # use all GPUs\n\nmodel_path = f\"./tmp/{uuid.uuid4().hex}-df_train_temp_save\"\npredictor_df = MultiModalPredictor(\n    hyperparameters={\n        \"model.mmdet_image.checkpoint_name\": checkpoint_name,\n        \"env.num_gpus\": num_gpus,\n    },\n    problem_type=\"object_detection\",\n    sample_data_path=train_df,\n    path=model_path,\n)\n\npredictor_df.fit(\n    train_df,\n    hyperparameters={\n        \"optim.lr\": 2e-4,\n        \"optim.max_epochs\": 30,\n        \"env.per_gpu_batch_size\": 32,\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Importing AutoGluon Tabular Classes\nDESCRIPTION: Imports the TabularDataset and TabularPredictor classes from autogluon.tabular module\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-quick-start.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.tabular import TabularDataset, TabularPredictor\n```\n\n----------------------------------------\n\nTITLE: Configuring Hyperparameter Tuning\nDESCRIPTION: Sets up hyperparameter search spaces for neural networks and gradient boosting models, configures tuning strategy and trains models.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-indepth.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.common import space\n\nnn_options = {\n    'num_epochs': 10,\n    'learning_rate': space.Real(1e-4, 1e-2, default=5e-4, log=True),\n    'activation': space.Categorical('relu', 'softrelu', 'tanh'),\n    'dropout_prob': space.Real(0.0, 0.5, default=0.1),\n}\n\ngbm_options = {\n    'num_boost_round': 100,\n    'num_leaves': space.Int(lower=26, upper=66, default=36),\n}\n\nhyperparameters = {\n                   'GBM': gbm_options,\n                   'NN_TORCH': nn_options,\n                  }\n\ntime_limit = 2*60\nnum_trials = 5\nsearch_strategy = 'auto'\n\nhyperparameter_tune_kwargs = {\n    'num_trials': num_trials,\n    'scheduler' : 'local',\n    'searcher': search_strategy,\n}\n\npredictor = TabularPredictor(label=label, eval_metric=metric).fit(\n    train_data,\n    time_limit=time_limit,\n    hyperparameters=hyperparameters,\n    hyperparameter_tune_kwargs=hyperparameter_tune_kwargs,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Training TimeSeriesPredictor with Covariates\nDESCRIPTION: Sets up TimeSeriesPredictor with two Chronos-Bolt configurations - one zero-shot without covariates and another with CatBoost regressor for handling covariates. Includes target scaling and custom name suffixes for model identification.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-chronos.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npredictor = TimeSeriesPredictor(\n    prediction_length=prediction_length,\n    target=\"unit_sales\",\n    known_covariates_names=[\"scaled_price\", \"promotion_email\", \"promotion_homepage\"],\n).fit(\n    train_data,\n    hyperparameters={\n        \"Chronos\": [\n            # Zero-shot model WITHOUT covariates\n            {\n                \"model_path\": \"bolt_small\",\n                \"ag_args\": {\"name_suffix\": \"ZeroShot\"},\n            },\n            # Chronos-Bolt (Small) combined with CatBoost on covariates\n            {\n                \"model_path\": \"bolt_small\",\n                \"covariate_regressor\": \"CAT\",\n                \"target_scaler\": \"standard\",\n                \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n            },\n        ],\n    },\n    enable_ensemble=False,\n    time_limit=60,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Mean Squared Error Metric\nDESCRIPTION: Example of creating a custom evaluation metric by extending TimeSeriesScorer to implement mean squared error calculation.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-metrics.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport sklearn.metrics\nfrom autogluon.timeseries.metrics import TimeSeriesScorer\n\nclass MeanSquaredError(TimeSeriesScorer):\n   greater_is_better_internal = False\n   optimum = 0.0\n\n   def compute_metric(self, data_future, predictions, target, **kwargs):\n      return sklearn.metrics.mean_squared_error(y_true=data_future[target], y_pred=predictions[\"mean\"])\n```\n\n----------------------------------------\n\nTITLE: Predicting Class Probabilities with AutoGluon MultiModalPredictor\nDESCRIPTION: Generates probability predictions for each class using the trained predictor on the test dataset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal-quick-start.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprobs = predictor.predict_proba(test_data.drop(columns=label_col))\nprobs[:5]\n```\n\n----------------------------------------\n\nTITLE: Enabling GPU Acceleration for Specific Models in AutoGluon\nDESCRIPTION: This snippet shows how to enable GPU acceleration for specific models by passing the 'num_gpus' parameter into the model's hyperparameters.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-gpu.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nhyperparameters = {\n    'GBM': [\n        {'ag_args_fit': {'num_gpus': 0}},  # Train with CPU\n        {'ag_args_fit': {'num_gpus': 1}}   # Train with GPU. This amount needs to be <= total num_gpus granted to TabularPredictor\n    ]\n}\npredictor = TabularPredictor(label=label).fit(\n    train_data, \n    num_gpus=1,\n    hyperparameters=hyperparameters, \n)\n```\n\n----------------------------------------\n\nTITLE: Using Custom Validation Data with TimeSeriesPredictor in Python\nDESCRIPTION: This code demonstrates how to provide a custom validation dataset when fitting the TimeSeriesPredictor. It's important to note that the validation score is computed on the last prediction_length time steps of each time series.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-indepth.ipynb#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\npredictor.fit(train_data=train_data, tuning_data=my_validation_dataset)\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Tabular with All Dependencies\nDESCRIPTION: Command to install the tabular submodule with all its dependencies, equivalent to installing the full AutoGluon package.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/install-modules.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install autogluon.tabular[all]\n```\n\n----------------------------------------\n\nTITLE: Implementing MultilabelPredictor Class in Python\nDESCRIPTION: This code defines the MultilabelPredictor class, which manages multiple TabularPredictor objects for predicting multiple labels in tabular data. It includes methods for initialization, fitting, predicting, evaluating, and saving/loading the predictor.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-multilabel.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.tabular import TabularDataset, TabularPredictor\nfrom autogluon.common.utils.utils import setup_outputdir\nfrom autogluon.core.utils.loaders import load_pkl\nfrom autogluon.core.utils.savers import save_pkl\nimport os.path\n\nclass MultilabelPredictor:\n    \"\"\" Tabular Predictor for predicting multiple columns in table.\n        Creates multiple TabularPredictor objects which you can also use individually.\n        You can access the TabularPredictor for a particular label via: `multilabel_predictor.get_predictor(label_i)`\n\n        Parameters\n        ----------\n        labels : List[str]\n            The ith element of this list is the column (i.e. `label`) predicted by the ith TabularPredictor stored in this object.\n        path : str, default = None\n            Path to directory where models and intermediate outputs should be saved.\n            If unspecified, a time-stamped folder called \"AutogluonModels/ag-[TIMESTAMP]\" will be created in the working directory to store all models.\n            Note: To call `fit()` twice and save all results of each fit, you must specify different `path` locations or don't specify `path` at all.\n            Otherwise files from first `fit()` will be overwritten by second `fit()`.\n            Caution: when predicting many labels, this directory may grow large as it needs to store many TabularPredictors.\n        problem_types : List[str], default = None\n            The ith element is the `problem_type` for the ith TabularPredictor stored in this object.\n        eval_metrics : List[str], default = None\n            The ith element is the `eval_metric` for the ith TabularPredictor stored in this object.\n        consider_labels_correlation : bool, default = True\n            Whether the predictions of multiple labels should account for label correlations or predict each label independently of the others.\n            If True, the ordering of `labels` may affect resulting accuracy as each label is predicted conditional on the previous labels appearing earlier in this list (i.e. in an auto-regressive fashion).\n            Set to False if during inference you may want to individually use just the ith TabularPredictor without predicting all the other labels.\n        kwargs :\n            Arguments passed into the initialization of each TabularPredictor.\n\n    \"\"\"\n\n    multi_predictor_file = 'multilabel_predictor.pkl'\n\n    def __init__(self, labels, path=None, problem_types=None, eval_metrics=None, consider_labels_correlation=True, **kwargs):\n        if len(labels) < 2:\n            raise ValueError(\"MultilabelPredictor is only intended for predicting MULTIPLE labels (columns), use TabularPredictor for predicting one label (column).\")\n        if (problem_types is not None) and (len(problem_types) != len(labels)):\n            raise ValueError(\"If provided, `problem_types` must have same length as `labels`\")\n        if (eval_metrics is not None) and (len(eval_metrics) != len(labels)):\n            raise ValueError(\"If provided, `eval_metrics` must have same length as `labels`\")\n        self.path = setup_outputdir(path, warn_if_exist=False)\n        self.labels = labels\n        self.consider_labels_correlation = consider_labels_correlation\n        self.predictors = {}  # key = label, value = TabularPredictor or str path to the TabularPredictor for this label\n        if eval_metrics is None:\n            self.eval_metrics = {}\n        else:\n            self.eval_metrics = {labels[i] : eval_metrics[i] for i in range(len(labels))}\n        problem_type = None\n        eval_metric = None\n        for i in range(len(labels)):\n            label = labels[i]\n            path_i = os.path.join(self.path, \"Predictor_\" + str(label))\n            if problem_types is not None:\n                problem_type = problem_types[i]\n            if eval_metrics is not None:\n                eval_metric = eval_metrics[i]\n            self.predictors[label] = TabularPredictor(label=label, problem_type=problem_type, eval_metric=eval_metric, path=path_i, **kwargs)\n\n    def fit(self, train_data, tuning_data=None, **kwargs):\n        \"\"\" Fits a separate TabularPredictor to predict each of the labels.\n\n            Parameters\n            ----------\n            train_data, tuning_data : str or pd.DataFrame\n                See documentation for `TabularPredictor.fit()`.\n            kwargs :\n                Arguments passed into the `fit()` call for each TabularPredictor.\n        \"\"\"\n        if isinstance(train_data, str):\n            train_data = TabularDataset(train_data)\n        if tuning_data is not None and isinstance(tuning_data, str):\n            tuning_data = TabularDataset(tuning_data)\n        train_data_og = train_data.copy()\n        if tuning_data is not None:\n            tuning_data_og = tuning_data.copy()\n        else:\n            tuning_data_og = None\n        save_metrics = len(self.eval_metrics) == 0\n        for i in range(len(self.labels)):\n            label = self.labels[i]\n            predictor = self.get_predictor(label)\n            if not self.consider_labels_correlation:\n                labels_to_drop = [l for l in self.labels if l != label]\n            else:\n                labels_to_drop = [self.labels[j] for j in range(i+1, len(self.labels))]\n            train_data = train_data_og.drop(labels_to_drop, axis=1)\n            if tuning_data is not None:\n                tuning_data = tuning_data_og.drop(labels_to_drop, axis=1)\n            print(f\"Fitting TabularPredictor for label: {label} ...\")\n            predictor.fit(train_data=train_data, tuning_data=tuning_data, **kwargs)\n            self.predictors[label] = predictor.path\n            if save_metrics:\n                self.eval_metrics[label] = predictor.eval_metric\n        self.save()\n\n    def predict(self, data, **kwargs):\n        \"\"\" Returns DataFrame with label columns containing predictions for each label.\n\n            Parameters\n            ----------\n            data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n                Data to make predictions for. If label columns are present in this data, they will be ignored. See documentation for `TabularPredictor.predict()`.\n            kwargs :\n                Arguments passed into the predict() call for each TabularPredictor.\n        \"\"\"\n        return self._predict(data, as_proba=False, **kwargs)\n\n    def predict_proba(self, data, **kwargs):\n        \"\"\" Returns dict where each key is a label and the corresponding value is the `predict_proba()` output for just that label.\n\n            Parameters\n            ----------\n            data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n                Data to make predictions for. See documentation for `TabularPredictor.predict()` and `TabularPredictor.predict_proba()`.\n            kwargs :\n                Arguments passed into the `predict_proba()` call for each TabularPredictor (also passed into a `predict()` call).\n        \"\"\"\n        return self._predict(data, as_proba=True, **kwargs)\n\n    def evaluate(self, data, **kwargs):\n        \"\"\" Returns dict where each key is a label and the corresponding value is the `evaluate()` output for just that label.\n\n            Parameters\n            ----------\n            data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n                Data to evalate predictions of all labels for, must contain all labels as columns. See documentation for `TabularPredictor.evaluate()`.\n            kwargs :\n                Arguments passed into the `evaluate()` call for each TabularPredictor (also passed into the `predict()` call).\n        \"\"\"\n        data = self._get_data(data)\n        eval_dict = {}\n        for label in self.labels:\n            print(f\"Evaluating TabularPredictor for label: {label} ...\")\n            predictor = self.get_predictor(label)\n            eval_dict[label] = predictor.evaluate(data, **kwargs)\n            if self.consider_labels_correlation:\n                data[label] = predictor.predict(data, **kwargs)\n        return eval_dict\n\n    def save(self):\n        \"\"\" Save MultilabelPredictor to disk. \"\"\"\n        for label in self.labels:\n            if not isinstance(self.predictors[label], str):\n                self.predictors[label] = self.predictors[label].path\n        save_pkl.save(path=os.path.join(self.path, self.multi_predictor_file), object=self)\n        print(f\"MultilabelPredictor saved to disk. Load with: MultilabelPredictor.load('{self.path}')\")\n\n    @classmethod\n    def load(cls, path):\n        \"\"\" Load MultilabelPredictor from disk `path` previously specified when creating this MultilabelPredictor. \"\"\"\n        path = os.path.expanduser(path)\n        return load_pkl.load(path=os.path.join(path, cls.multi_predictor_file))\n\n    def get_predictor(self, label):\n        \"\"\" Returns TabularPredictor which is used to predict this label. \"\"\"\n        predictor = self.predictors[label]\n        if isinstance(predictor, str):\n            return TabularPredictor.load(path=predictor)\n        return predictor\n\n    def _get_data(self, data):\n        if isinstance(data, str):\n            return TabularDataset(data)\n        return data.copy()\n\n    def _predict(self, data, as_proba=False, **kwargs):\n        data = self._get_data(data)\n        if as_proba:\n            predproba_dict = {}\n        for label in self.labels:\n            print(f\"Predicting with TabularPredictor for label: {label} ...\")\n            predictor = self.get_predictor(label)\n            if as_proba:\n                predproba_dict[label] = predictor.predict_proba(data, as_multiclass=True, **kwargs)\n            data[label] = predictor.predict(data, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Configuring FT-Transformer Hyperparameters in Python\nDESCRIPTION: Example of setting hyperparameters for the FT-Transformer model in MultiModalPredictor. It demonstrates how to specify model name, number of blocks, and dropout rate.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/tabular_dl/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nhyperparameters = {\n   \"model.names\": [\"ft_transformer\"],\n   \"model.ft_transformer.num_blocks\": 5,\n   \"model.ft_transformer.ffn_dropout\": 0.0,\n}\n```\n\n----------------------------------------\n\nTITLE: Training MultiModal Predictor\nDESCRIPTION: Initializes and trains the MultiModal predictor with a time limit of 120 seconds.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/beginner_multimodal.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\npredictor = MultiModalPredictor(label=label_col)\npredictor.fit(\n    train_data=train_data,\n    time_limit=120, # seconds\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Predictions with Custom Model\nDESCRIPTION: Uses the trained custom model to generate predictions by providing past data and known future covariates.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/advanced/forecasting-custom-model.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\npast_data, known_covariates = data.get_model_inputs_for_scoring(\n    prediction_length=prediction_length,\n    known_covariates_names=known_covariates_names,\n)\npredictions = model.predict(past_data, known_covariates)\npredictions.head()\n```\n\n----------------------------------------\n\nTITLE: Making Predictions and Evaluating Results\nDESCRIPTION: Using the trained model to make predictions and compare with true values.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal_text_tabular.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npredictions = predictor.predict(test_data)\nprint('Predictions:')\nprint('------------')\nprint(np.exp(predictions) - 1)\nprint()\nprint('True Value:')\nprint('------------')\nprint(np.exp(test_data['Price']) - 1)\n```\n\n----------------------------------------\n\nTITLE: Training German BERT Model\nDESCRIPTION: Initializes and trains MultiModalPredictor using German BERT model for sentiment classification.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/multilingual_text.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\n\npredictor = MultiModalPredictor(label='label')\npredictor.fit(train_de_df,\n              hyperparameters={\n                  'model.hf_text.checkpoint_name': 'bert-base-german-cased',\n                  'optim.max_epochs': 2\n              })\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Model\nDESCRIPTION: Demonstrates how to save and reload the trained predictor.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/beginner_multimodal.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport uuid\n\nmodel_path = f\"./tmp/{uuid.uuid4().hex}-saved_model\"\npredictor.save(model_path)\nloaded_predictor = MultiModalPredictor.load(model_path)\nscores2 = loaded_predictor.evaluate(test_data, metrics=[\"roc_auc\"])\nscores2\n```\n\n----------------------------------------\n\nTITLE: Training TimeSeriesPredictor with Custom Metric in Python\nDESCRIPTION: Demonstrates how to initialize and train a TimeSeriesPredictor using a custom evaluation metric (MeanQuantileLoss) with multiple forecasting models.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-metrics.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npredictor = TimeSeriesPredictor(eval_metric=MeanQuantileLoss()).fit(train_data, hyperparameters={\"Naive\": {}, \"SeasonalNaive\": {}, \"Theta\": {}})\n```\n\n----------------------------------------\n\nTITLE: NHITS Model Class Definition\nDESCRIPTION: Defines the NHITSModel class inheriting from AbstractTimeSeriesModel with support for various types of covariates.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/advanced/forecasting-custom-model.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass NHITSModel(AbstractTimeSeriesModel):\n    \"\"\"AutoGluon-compatible wrapper for the NHITS model from NeuralForecast.\"\"\"\n\n    # Set these attributes to ensure that AutoGluon passes correct features to the model\n    _supports_known_covariates: bool = True\n    _supports_past_covariates: bool = True\n    _supports_static_features: bool = True\n```\n\n----------------------------------------\n\nTITLE: Calibrating Decision Threshold for Multiple Metrics in Python\nDESCRIPTION: This code demonstrates how to calibrate the decision threshold for multiple metrics ('f1', 'balanced_accuracy', 'mcc') and compare the results.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-indepth.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npredictor.set_decision_threshold(0.5)  # Reset decision threshold\nfor metric_name in ['f1', 'balanced_accuracy', 'mcc']:\n    metric_score = predictor.evaluate(test_data, silent=True)[metric_name]\n    calibrated_decision_threshold = predictor.calibrate_decision_threshold(metric=metric_name, verbose=False)\n    metric_score_calibrated = predictor.evaluate(\n        test_data, decision_threshold=calibrated_decision_threshold, silent=True\n    )[metric_name]\n    print(f'decision_threshold={calibrated_decision_threshold:.3f}\\t| metric=\"{metric_name}\"'\n          f'\\n\\ttest_score uncalibrated: {metric_score:.4f}'\n          f'\\n\\ttest_score   calibrated: {metric_score_calibrated:.4f}'\n          f'\\n\\ttest_score        delta: {metric_score_calibrated-metric_score:.4f}')\n```\n\n----------------------------------------\n\nTITLE: Training MultiModalPredictor with Focal Loss\nDESCRIPTION: Creates and trains a MultiModalPredictor using focal loss with custom alpha weights to address class imbalance. The alpha weights are inversely proportional to the class frequencies.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/focal_loss.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport uuid\nfrom autogluon.multimodal import MultiModalPredictor\n\nmodel_path = f\"./tmp/{uuid.uuid4().hex}-automm_shopee_focal\"\n\npredictor = MultiModalPredictor(label=\"label\", problem_type=\"multiclass\", path=model_path)\n\npredictor.fit(\n    hyperparameters={\n        \"model.mmdet_image.checkpoint_name\": \"swin_tiny_patch4_window7_224\",\n        \"env.num_gpus\": 1,\n        \"optim.loss_func\": \"focal_loss\",\n        \"optim.focal_loss.alpha\": weights,  # shopee dataset has 4 classes.\n        \"optim.focal_loss.gamma\": 1.0,\n        \"optim.focal_loss.reduction\": \"sum\",\n        \"optim.max_epochs\": 10,\n    },\n    train_data=imbalanced_train_data,\n) \n\npredictor.evaluate(test_data, metrics=[\"acc\"])\n```\n\n----------------------------------------\n\nTITLE: Training Multiple Configurations of Custom Model\nDESCRIPTION: Demonstrates how to train multiple versions of the custom model with different hyperparameter settings to find the optimal configuration.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/advanced/forecasting-custom-model.ipynb#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\npredictor = TimeSeriesPredictor(\n    prediction_length=prediction_length,\n    target=target,\n    known_covariates_names=known_covariates_names,\n)\npredictor.fit(\n    train_data,\n    hyperparameters={\n        NHITSModel: [\n            {},  # default hyperparameters\n            {\"input_size\": 20},  # custom input_size\n            {\"scaler_type\": \"robust\"},  # custom scaler_type\n        ]\n    },\n    time_limit=60,\n)\n```\n\n----------------------------------------\n\nTITLE: Training Custom Models with TabularPredictor in AutoGluon\nDESCRIPTION: This snippet demonstrates how to train custom models using TabularPredictor in AutoGluon. It shows how to train multiple CustomRandomForestModels with different hyperparameters.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-model.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.tabular import TabularPredictor\n\n# custom_hyperparameters = {CustomRandomForestModel: {}}  # train 1 CustomRandomForestModel Model with default hyperparameters\ncustom_hyperparameters = {CustomRandomForestModel: [{}, {'max_depth': 10}, {'max_features': 0.9, 'max_depth': 20}]}  # Train 3 CustomRandomForestModel with different hyperparameters\npredictor = TabularPredictor(label=label).fit(train_data, hyperparameters=custom_hyperparameters)\n\n# Show predictor leaderboard\npredictor.leaderboard(test_data)\n\n# Predict with fit predictor\ny_pred = predictor.predict(test_data)\n# y_pred = predictor.predict(test_data, model='CustomRandomForestModel_3')  # If we want a specific model to predict\ny_pred.head(5)\n```\n\n----------------------------------------\n\nTITLE: Generating Forecasts with TimeSeriesPredictor\nDESCRIPTION: Uses the fitted TimeSeriesPredictor to generate probabilistic forecasts for the training data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-quick-start.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npredictions = predictor.predict(train_data)\npredictions.head()\n```\n\n----------------------------------------\n\nTITLE: Training Image Classifier with AutoMM\nDESCRIPTION: Initializes and trains a MultiModalPredictor for image classification using AutoMM.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_prediction/beginner_image_cls.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\nimport uuid\nmodel_path = f\"./tmp/{uuid.uuid4().hex}-automm_shopee\"\npredictor = MultiModalPredictor(label=\"label\", path=model_path)\npredictor.fit(\n    train_data=train_data_path,\n    time_limit=30, # seconds\n)\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating Conda Environment for AutoGluon on Windows\nDESCRIPTION: Commands to create a new conda environment with Python 3.11 and CUDA 11.3 toolkit, then activate it for AutoGluon installation.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/install-windows-gpu.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n myenv python=3.11 cudatoolkit=11.3 -y\nconda activate myenv\n```\n\n----------------------------------------\n\nTITLE: Training Initial Sentiment Analysis Model with MultiModalPredictor\nDESCRIPTION: Creates and trains a MultiModalPredictor for sentiment analysis classification. The model is trained on a subset of data with a time limit of 60 seconds. The predictor uses accuracy as the evaluation metric.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/continuous_training.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\nimport uuid\n\nmodel_path = f\"./tmp/{uuid.uuid4().hex}-automm_sst\"\npredictor = MultiModalPredictor(label=\"label\", eval_metric=\"acc\", path=model_path)\npredictor.fit(train_data_1, time_limit=60)\n```\n\n----------------------------------------\n\nTITLE: Training TabularPredictor\nDESCRIPTION: Creates and trains a TabularPredictor instance on the training data to predict the signature label\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-quick-start.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npredictor = TabularPredictor(label=label).fit(train_data)\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon with conda and mamba in a dedicated environment\nDESCRIPTION: This snippet demonstrates the complete installation process for AutoGluon. It creates a dedicated conda environment with Python 3.10, installs mamba for faster package management, then uses mamba to install AutoGluon and Ray for accelerated training.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/install-conda-full.md#2025-04-22_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nconda create -n ag python=3.10\nconda activate ag\nconda install -c conda-forge mamba\nmamba install -c conda-forge autogluon\nmamba install -c conda-forge \"ray-tune >=2.10.0,<2.32\" \"ray-default >=2.10.0,<2.32\"  # install ray for faster training\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Quantile Loss Metric in Python for AutoGluon\nDESCRIPTION: Defines a custom MeanQuantileLoss class that extends TimeSeriesScorer to compute mean quantile loss across all forecast quantiles. The metric uses sklearn's mean_pinball_loss and is configured for quantile forecasting.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-metrics.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass MeanQuantileLoss(TimeSeriesScorer):\n   needs_quantile = True\n   greater_is_better_internal = False\n   optimum = 0.0\n\n   def compute_metric(self, data_future, predictions, target, **kwargs):\n      quantile_columns = [col for col in predictions if col != \"mean\"]\n      total_quantile_loss = 0.0\n      for q in quantile_columns:\n        total_quantile_loss += sklearn.metrics.mean_pinball_loss(y_true=data_future[target], y_pred=predictions[q], alpha=float(q))\n      return total_quantile_loss / len(quantile_columns)\n```\n\n----------------------------------------\n\nTITLE: Creating and Saving Kaggle Submission File\nDESCRIPTION: This snippet demonstrates how to create a submission file for Kaggle by updating the sample submission with predicted probabilities and saving it as a CSV file.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-kaggle.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsubmission = pd.read_csv(directory+'sample_submission.csv')\nsubmission['isFraud'] = y_predproba\nsubmission.head()\nsubmission.to_csv(directory+'my_submission.csv', index=False)\n```\n\n----------------------------------------\n\nTITLE: Model Evaluation and Analysis\nDESCRIPTION: Shows how to evaluate the model's performance and view the model leaderboard\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-essentials.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npredictor.evaluate(test_data)\n```\n\nLANGUAGE: python\nCODE:\n```\npredictor.leaderboard(test_data)\n```\n\n----------------------------------------\n\nTITLE: Comparing Metric Scores Before and After Calibration in Python\nDESCRIPTION: This snippet shows how to compare metric scores before and after decision threshold calibration for various metrics in binary classification.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-indepth.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfor metric_name in scores:\n    metric_score = scores[metric_name]\n    metric_score_calibrated = scores_calibrated[metric_name]\n    decision_threshold = predictor.decision_threshold\n    print(f'decision_threshold={decision_threshold:.3f}\\t| metric=\"{metric_name}\"'\n          f'\\n\\ttest_score uncalibrated: {metric_score:.4f}'\n          f'\\n\\ttest_score   calibrated: {metric_score_calibrated:.4f}'\n          f'\\n\\ttest_score        delta: {metric_score_calibrated-metric_score:.4f}')\n```\n\n----------------------------------------\n\nTITLE: Advanced Resource Allocation for AutoGluon TabularPredictor\nDESCRIPTION: This example demonstrates advanced resource allocation techniques for AutoGluon's TabularPredictor, including CPU and GPU allocation for ensemble models, individual base models, and hyperparameter tuning.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-gpu.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npredictor.fit(\n    num_cpus=32,\n    num_gpus=4,\n    hyperparameters={\n        'NN_TORCH': {},\n    },\n    num_bag_folds=2,\n    ag_args_ensemble={\n        'ag_args_fit': {\n            'num_cpus': 10,\n            'num_gpus': 2,\n        }\n    },\n    ag_args_fit={\n        'num_cpus': 4,\n        'num_gpus': 0.5,\n    }\n    hyperparameter_tune_kwargs={\n        'searcher': 'random',\n        'scheduler': 'local',\n        'num_trials': 2\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Integrating Custom Model with TimeSeriesPredictor\nDESCRIPTION: Demonstrates how to use the custom model alongside AutoGluon's built-in models within the TimeSeriesPredictor, showing the advantage of AutoGluon's automated pipeline.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/advanced/forecasting-custom-model.ipynb#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.timeseries import TimeSeriesPredictor\n\ntrain_data, test_data = raw_data.train_test_split(prediction_length)\n\npredictor = TimeSeriesPredictor(\n    prediction_length=prediction_length,\n    target=target,\n    known_covariates_names=known_covariates_names,\n)\n\npredictor.fit(\n    train_data,\n    hyperparameters={\n        \"Naive\": {},\n        \"Chronos\": {\"model_path\": \"bolt_small\"},\n        \"ETS\": {},\n        NHITSModel: {},\n    },\n    time_limit=120,\n)\n```\n\n----------------------------------------\n\nTITLE: Resuming Training from Last Checkpoint\nDESCRIPTION: Shows the code pattern for resuming training from the last checkpoint when training was interrupted. This loads the model with the 'resume' option set to True.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/continuous_training.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npredictor_resume = MultiModalPredictor.load(path=model_path, resume=True)\npredictor.fit(train_data, time_limit=60)\n```\n\n----------------------------------------\n\nTITLE: Initializing TimeSeriesPredictor with Custom Quantiles in Python\nDESCRIPTION: This snippet demonstrates how to initialize a TimeSeriesPredictor with custom quantile levels for probabilistic forecasting. It sets the quantile levels to 5%, 50%, and 95%.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-indepth.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npredictor = TimeSeriesPredictor(quantile_levels=[0.05, 0.5, 0.95])\n```\n\n----------------------------------------\n\nTITLE: Configuring FT-Transformer Architecture in AutoMM\nDESCRIPTION: Sets various architectural parameters for the FT-Transformer backbone, including number of blocks, token dimension, hidden size, and FFN hidden size. These are controlled by 'model.ft_transformer.num_blocks', 'model.ft_transformer.token_dim', 'model.ft_transformer.hidden_size', and 'model.ft_transformer.ffn_hidden_size' hyperparameters respectively.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_45\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"model.ft_transformer.num_blocks\": 3})\n# increase the number of blocks to 5 in ft_transformer\npredictor.fit(hyperparameters={\"model.ft_transformer.num_blocks\": 5})\n\n# default used by AutoMM\npredictor.fit(hyperparameters={\"model.ft_transformer.token_dim\": 192})\n# increase the token dimension to 256 in ft_transformer\npredictor.fit(hyperparameters={\"model.ft_transformer.token_dim\": 256})\n\n# default used by AutoMM\npredictor.fit(hyperparameters={\"model.ft_transformer.hidden_size\": 192})\n# increase the model embedding dimension to 256 in ft_transformer\npredictor.fit(hyperparameters={\"model.ft_transformer.hidden_size\": 256})\n\n# default used by AutoMM\npredictor.fit(hyperparameters={\"model.ft_transformer.ffn_hidden_size\": 192})\n# increase the FFN hidden layer dimension to 256 in ft_transformer\npredictor.fit(hyperparameters={\"model.ft_transformer.ffn_hidden_size\": 256})\n```\n\n----------------------------------------\n\nTITLE: Evaluating Ranking Performance\nDESCRIPTION: Evaluates the ranking performance using the predictor's evaluate API. Automatically handles embedding extraction, similarity computation, and ranking metrics calculation.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/text_semantic_search.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npredictor.evaluate(\n        labeled_data,\n        query_data=query_data[[query_id_col]],\n        response_data=doc_data[[doc_id_col]],\n        id_mappings=id_mappings,\n        cutoffs=cutoffs,\n        metrics=[\"ndcg\"],\n    )\n```\n\n----------------------------------------\n\nTITLE: Training AutoMM NER Model\nDESCRIPTION: This snippet demonstrates how to create and train an AutoMM predictor for Named Entity Recognition using the loaded dataset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/ner.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\nimport uuid\n\nlabel_col = \"entity_annotations\"\nmodel_path = f\"./tmp/{uuid.uuid4().hex}-automm_ner\"  # You can rename it to the model path you like\npredictor = MultiModalPredictor(problem_type=\"ner\", label=label_col, path=model_path)\npredictor.fit(\n    train_data=train_data,\n    hyperparameters={'model.ner_text.checkpoint_name':'google/electra-small-discriminator'},\n    time_limit=300, #second\n)\n```\n\n----------------------------------------\n\nTITLE: Training TabularPredictor with fastText for Sentiment Analysis\nDESCRIPTION: Configures and trains an AutoGluon model with FastTextModel for sentiment analysis on the SST dataset, with increased epochs (100) for better performance.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/tabular-fasttext.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncustom_hyperparameters = {'RF': {},\n                         FastTextModel:  {'epoch': 100},\n                         }\n\nfeature_generator = AutoMLPipelineFeatureGenerator(enable_raw_text_features=True)\n\nlabel = 'label'\npredictor = TabularPredictor(label=label).fit(train_data, hyperparameters=custom_hyperparameters, feature_generator=feature_generator)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model Performance\nDESCRIPTION: Evaluates the predictor's performance on the test dataset\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-quick-start.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npredictor.evaluate(test_data, silent=True)\n```\n\n----------------------------------------\n\nTITLE: Predicting and Visualizing NER Results\nDESCRIPTION: This snippet demonstrates how to use the trained model for prediction on a new sentence and visualize the results using the visualize_ner function.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/ner.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal.utils import visualize_ner\n\nsentence = \"Game of Thrones is an American fantasy drama television series created by David Benioff\"\npredictions = predictor.predict({'text_snippet': [sentence]})\nprint('Predicted entities:', predictions[0])\n\n# Visualize\nvisualize_ner(sentence, predictions[0])\n```\n\n----------------------------------------\n\nTITLE: Setting up AutoGluon with CUDA Support in Conda Environment\nDESCRIPTION: Commands to create a Python 3.11 conda environment and install AutoGluon with GPU support using mamba. The installation includes the PyTorch version with CUDA capabilities and Ray for distributed training.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/install-windows-conda-gpu.md#2025-04-22_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nconda create -n ag python=3.11\nconda activate ag\nconda install -c conda-forge mamba\nmamba install -c conda-forge -c pytorch -c nvidia autogluon \"pytorch=*=*cuda*\"\nmamba install -c conda-forge \"ray-tune >=2.10.0,<2.32\" \"ray-default >=2.10.0,<2.32\"  # install ray for faster training\n```\n\n----------------------------------------\n\nTITLE: Reloading and Continuing Training of AutoGluon NER Model\nDESCRIPTION: This code demonstrates how to reload a saved model and continue training it with new data, then evaluate its performance.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal_ner.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nnew_predictor = MultiModalPredictor.load(model_path)\nnew_model_path = f\"./tmp/{uuid.uuid4().hex}-automm_multimodal_ner_continue_train\"\nnew_predictor.fit(train_data, time_limit=60, save_path=new_model_path)\ntest_score = new_predictor.evaluate(test_data, metrics=['overall_f1'])\nprint(test_score)\n```\n\n----------------------------------------\n\nTITLE: Training Multilingual Model with IA3 + BitFit\nDESCRIPTION: Demonstrates parameter-efficient finetuning using IA3 and BitFit techniques with MultiModalPredictor on multilingual data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/efficient_finetuning_basic.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\nimport uuid\n\nmodel_path = f\"./tmp/{uuid.uuid4().hex}-multilingual_ia3\"\npredictor = MultiModalPredictor(label=\"label\",\n                                path=model_path)\npredictor.fit(train_en_df,\n              presets=\"multilingual\",\n              hyperparameters={\n                  \"optim.peft\": \"ia3_bias\",\n                  \"optim.lr_decay\": 0.9,\n                  \"optim.lr\": 3e-03,\n                  \"optim.end_lr\": 3e-03,\n                  \"optim.max_epochs\": 2,\n                  \"optim.warmup_steps\": 0,\n                  \"env.batch_size\": 32,\n              })\n```\n\n----------------------------------------\n\nTITLE: Predicting Image Label Probabilities\nDESCRIPTION: Uses the trained model to predict probabilities for all categories for a new image.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_prediction/beginner_image_cls.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nproba = predictor.predict_proba({'image': [image_path]})\nprint(proba)\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with MultiModalPredictor in Python\nDESCRIPTION: This snippet demonstrates how to use a loaded MultiModalPredictor to generate predictions on a test dataset for Kaggle submission.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/kaggle_pawpularity/README.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntest_pred = pretrained_model.predict(test_df)\n```\n\n----------------------------------------\n\nTITLE: Generating and Plotting Predictions\nDESCRIPTION: Uses the trained predictor to generate forecasts and plot them for the first two time series in the dataset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-chronos.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npredictions = predictor.predict(train_data)\npredictor.plot(\n    data=data,\n    predictions=predictions,\n    item_ids=data.item_ids[:2],\n    max_history_length=200,\n);\n```\n\n----------------------------------------\n\nTITLE: Obtaining Prediction Probabilities for Document Classes\nDESCRIPTION: Retrieves the probability distribution across all document classes for a given document using predict_proba method.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/document_prediction/document_classification.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nproba = predictor.predict_proba({DOC_PATH_COL: [doc_path]})\nprint(proba)\n```\n\n----------------------------------------\n\nTITLE: Performing Zero-Shot Image Classification with CLIP in AutoGluon\nDESCRIPTION: Initializes the MultiModalPredictor with CLIP for zero-shot image classification and classifies a dog image into different dog breeds without training.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_prediction/clip_zeroshot.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\n\npredictor = MultiModalPredictor(problem_type=\"zero_shot_image_classification\")\nprob = predictor.predict_proba({\"image\": [dog_image]}, {\"text\": ['This is a Husky', 'This is a Golden Retriever', 'This is a German Sheperd', 'This is a Samoyed.']})\nprint(\"Label probs:\", prob)\n```\n\n----------------------------------------\n\nTITLE: Making Predictions and Evaluating Model\nDESCRIPTION: Demonstrates how to make predictions on test data and evaluate model performance.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-indepth.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ny_pred = predictor.predict(test_data_nolabel)\nprint(\"Predictions:  \", list(y_pred)[:5])\nperf = predictor.evaluate(test_data, auxiliary_metrics=False)\n```\n\n----------------------------------------\n\nTITLE: Configuring Learning Rate Warmup in AutoMM\nDESCRIPTION: Examples showing how to set the learning rate warmup in AutoMM, which gradually increases the learning rate from 0 to optim.lr during the specified percentage of steps.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"optim.warmup_steps\": 0.1})\n# do learning rate warmup in the first 20% steps.\npredictor.fit(hyperparameters={\"optim.warmup_steps\": 0.2})\n```\n\n----------------------------------------\n\nTITLE: Using Custom Metrics in TabularPredictor\nDESCRIPTION: Demonstrates how to use custom metrics for model evaluation and training in TabularPredictor.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-metric.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npredictor_custom = TabularPredictor(label=label, eval_metric=ag_roc_auc_scorer).fit(train_data, hyperparameters='toy')\n\npredictor_custom.leaderboard(test_data)\n```\n\n----------------------------------------\n\nTITLE: Training TabularPredictor with Custom Metric in Python\nDESCRIPTION: This snippet demonstrates how to initialize and train a TabularPredictor with a custom evaluation metric ('balanced_accuracy') and specific hyperparameters for FASTAI and GBM models.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-indepth.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npredictor = TabularPredictor(label=label, eval_metric='balanced_accuracy', path=save_path).fit(\n    train_data, auto_stack=True,\n    calibrate_decision_threshold=False,  # Disabling for demonstration in next section\n    hyperparameters={'FASTAI': {'num_epochs': 10}, 'GBM': {'num_boost_round': 200}}  # last 2 arguments are for quick demo, omit them in real applications\n)\npredictor.leaderboard(test_data)\n```\n\n----------------------------------------\n\nTITLE: Explaining Single Prediction with SHAP for AutoGluon Regressor in Python\nDESCRIPTION: This code explains a prediction for a single datapoint from the training data using SHAP values and visualizes it with a force plot.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/interpret/SHAP with AutoGluon-Tabular Diabetes regression.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nROW_INDEX = 0  # index of an example datapoint\nsingle_datapoint = X_train.iloc[[ROW_INDEX]]\nsingle_prediction = ag_wrapper.predict(single_datapoint)\n\nshap_values_single = explainer.shap_values(single_datapoint, nsamples=NSHAP_SAMPLES)\nshap.force_plot(explainer.expected_value, shap_values_single, X_train.iloc[ROW_INDEX,:])\n```\n\n----------------------------------------\n\nTITLE: Training Image Matching Model\nDESCRIPTION: Initializes and trains the MultiModalPredictor for image similarity using Swin Transformer\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/image2image_matching.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\npredictor = MultiModalPredictor(\n        problem_type=\"image_similarity\",\n        query=image_col_1,\n        response=image_col_2,\n        label=label_col,\n        match_label=match_label,\n        eval_metric='auc',\n    )\n    \npredictor.fit(\n    train_data=train_data,\n    time_limit=180,\n)\n```\n\n----------------------------------------\n\nTITLE: TabularPredictor Integration Example\nDESCRIPTION: Shows how to integrate custom models and feature generators with TabularPredictor\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-model-advanced.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.tabular import TabularPredictor\n\nfeature_generator = CustomFeatureGeneratorWithUserOverride()\npredictor = TabularPredictor(label=label)\npredictor.fit(\n    train_data=train_data,\n    feature_metadata=feature_metadata,\n    feature_generator=feature_generator,\n    hyperparameters={\n        'GBM': {},\n        DummyModel: {},\n        DummyModelKeepUnique: {},\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model Performance\nDESCRIPTION: Evaluating the trained model on the test dataset and measuring evaluation time.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/quick_start/quick_start_coco.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npredictor.evaluate(test_path)\neval_end = time.time()\n\nprint(\"The evaluation takes %.2f seconds.\" % (eval_end - train_end))\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with AutoGluon NER Model\nDESCRIPTION: This snippet demonstrates how to use the trained model to make predictions on new data and print the results.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal_ner.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprediction_input = test_data.drop(columns=label_col).head(1)\npredictions = predictor.predict(prediction_input)\nprint('Tweet:', prediction_input.text_snippet[0])\nprint('Image path:', prediction_input.image[0])\nprint('Predicted entities:', predictions[0])\n\nfor entity in predictions[0]:\n\tprint(f\"Word '{prediction_input.text_snippet[0][entity['start']:entity['end']]}' belongs to group: {entity['entity_group']}\")\n```\n\n----------------------------------------\n\nTITLE: Defining Hyperparameter Search Spaces for AutoGluon TimeSeriesPredictor (Python)\nDESCRIPTION: This snippet illustrates how to enable hyperparameter optimization (HPO) for specific models. It uses `autogluon.common.space` to define search spaces (e.g., `space.Int`, `space.Categorical`) for hyperparameters within the `hyperparameters` dictionary. Setting `hyperparameter_tune_kwargs=\"auto\"` triggers the HPO process. This example defines search spaces for `DeepAR`'s `hidden_size` and `dropout_rate` and disables the ensemble model.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-indepth.ipynb#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.common import space\n\npredictor = TimeSeriesPredictor()\n\npredictor.fit(\n    train_data,\n    hyperparameters={\n        \"DeepAR\": {\n            \"hidden_size\": space.Int(20, 100),\n            \"dropout_rate\": space.Categorical(0.1, 0.3),\n        },\n    },\n    hyperparameter_tune_kwargs=\"auto\",\n    enable_ensemble=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Fitting Quick Model for Feature Importance\nDESCRIPTION: Uses AutoGluon to fit a quick model on the training data to determine feature importance.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-analyze-interaction.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nstate = auto.quick_fit(train_data=df_train, label='Survived', render_analysis=False, return_state=True)\n```\n\n----------------------------------------\n\nTITLE: Training NER Model with AutoMM\nDESCRIPTION: Sets up and trains the MultiModalPredictor for NER using a Chinese BERT model (chinese-lert-small) as the backbone.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/chinese_ner.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\nimport uuid\n\nlabel_col = \"entity_annotations\"\nmodel_path = f\"./tmp/{uuid.uuid4().hex}-automm_ner\"  # You can rename it to the model path you like\npredictor = MultiModalPredictor(problem_type=\"ner\", label=label_col, path=model_path)\npredictor.fit(\n    train_data=train_data,\n    hyperparameters={'model.ner_text.checkpoint_name':'hfl/chinese-lert-small'},\n    time_limit=300, #second\n)\n```\n\n----------------------------------------\n\nTITLE: Predicting Similarity Scores for Sentence Pairs\nDESCRIPTION: This snippet demonstrates how to use the trained model to predict similarity scores for new pairs of sentences, showcasing the model's practical application.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/beginner_text.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nsentences = ['The child is riding a horse.',\n             'The young boy is riding a horse.',\n             'The young man is riding a horse.',\n             'The young man is riding a bicycle.']\n\nscore1 = predictor_sts.predict({'sentence1': [sentences[0]],\n                                'sentence2': [sentences[1]]}, as_pandas=False)\n\nscore2 = predictor_sts.predict({'sentence1': [sentences[0]],\n                                'sentence2': [sentences[2]]}, as_pandas=False)\n\nscore3 = predictor_sts.predict({'sentence1': [sentences[0]],\n                                'sentence2': [sentences[3]]}, as_pandas=False)\nprint(score1, score2, score3)\n```\n\n----------------------------------------\n\nTITLE: Evaluating AutoMM NER Model\nDESCRIPTION: This code shows how to evaluate the trained NER model using various metrics including overall recall, precision, F1 score, and entity-specific performance.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/ner.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npredictor.evaluate(test_data,  metrics=['overall_recall', \"overall_precision\", \"overall_f1\", \"actor\"])\n```\n\n----------------------------------------\n\nTITLE: Cleaning Labels for Binary Classification in AutoGluon\nDESCRIPTION: This snippet demonstrates how to clean labels for binary classification using AutoGluon's LabelCleaner and infer_problem_type functions. It converts string labels to numeric values (0 and 1) for model input.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-model.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Separate features and labels\nX = train_data.drop(columns=[label])\ny = train_data[label]\nX_test = test_data.drop(columns=[label])\ny_test = test_data[label]\n\nfrom autogluon.core.data import LabelCleaner\nfrom autogluon.core.utils import infer_problem_type\n# Construct a LabelCleaner to neatly convert labels to float/integers during model training/inference, can also use to inverse_transform back to original.\nproblem_type = infer_problem_type(y=y)  # Infer problem type (or else specify directly)\nlabel_cleaner = LabelCleaner.construct(problem_type=problem_type, y=y)\ny_clean = label_cleaner.transform(y)\n\nprint(f'Labels cleaned: {label_cleaner.inv_map}')\nprint(f'inferred problem type as: {problem_type}')\nprint('Cleaned label values:')\ny_clean.head(5)\n```\n\n----------------------------------------\n\nTITLE: Running AutoMM Experiments for House Price Prediction\nDESCRIPTION: This script demonstrates various ways to run experiments using AutoMM for the California House Prices prediction task. It includes single model runs, bagging, and ensemble methods with different configurations.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/kaggle_california_house_price/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Single MultiModalPredictor (MLP)\npython3 example_kaggle_house.py --automm-mode mlp --mode single 2>&1 | tee -a logs/automm_single_mlp.txt\n\n# Single MultiModalPredictor (FT-Transformer For Tabular)\npython3 example_kaggle_house.py --automm-mode ft-transformer --mode single 2>&1 | tee -a logs/automm_single_ft.txt\n\n# MultiModalPredictor + 5-Fold Bagging\npython3 example_kaggle_house.py --automm-mode ft-transformer --mode automm_bag5 2>&1 | tee -a logs/automm_ft_bag5.txt\n\n# MultiModalPredictor + other Tree Models (Weighted Ensemble) \npython3 example_kaggle_house.py --automm-mode ft-transformer --mode weighted 2>&1 | tee -a logs/automm_ft_weighted.txt\n\n# MultiModalPredictor + other Tree Models (5-fold Stack Ensemble) \npython3 example_kaggle_house.py --automm-mode ft-transformer --mode stack5 2>&1 | tee -a logs/automm_ft_stack5.txt\n```\n\n----------------------------------------\n\nTITLE: Training MultiModalPredictor\nDESCRIPTION: Initializing and training the MultiModalPredictor with the prepared dataset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal_text_tabular.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\nimport uuid\n\ntime_limit = 3 * 60  # set to larger value in your applications\nmodel_path = f\"./tmp/{uuid.uuid4().hex}-automm_text_book_price_prediction\"\npredictor = MultiModalPredictor(label='Price', path=model_path)\npredictor.fit(train_data, time_limit=time_limit)\n```\n\n----------------------------------------\n\nTITLE: Configuring TimeSeriesPredictor with Presets in Python\nDESCRIPTION: This code shows how to configure the TimeSeriesPredictor using predefined presets. The 'medium_quality' preset is used, which balances prediction quality and training time.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-indepth.ipynb#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\npredictor = TimeSeriesPredictor(...)\npredictor.fit(train_data, presets=\"medium_quality\")\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with a Trained Custom Model in AutoGluon\nDESCRIPTION: This code snippet shows how to make predictions on new data using the trained custom model. It includes the necessary data and label transformations for the test data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-model.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Prepare test data\nX_test_clean = feature_generator.transform(X_test)\ny_test_clean = label_cleaner.transform(y_test)\n\nX_test.head(5)\n\n# Get raw predictions from the test data\ny_pred = custom_model.predict(X_test_clean)\nprint(y_pred[:5])\n\n# Get more interpretable results\ny_pred_orig = label_cleaner.inverse_transform(y_pred)\ny_pred_orig.head(5)\n```\n\n----------------------------------------\n\nTITLE: Training a Regression Model with Transfer Learning\nDESCRIPTION: Creates a new MultiModalPredictor for a regression task and initializes it with the weights from the previously trained text classification model using hyperparameters to specify the custom model checkpoint.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/continuous_training.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsts_model_path = f\"./tmp/{uuid.uuid4().hex}-automm_sts\"\npredictor_sts = MultiModalPredictor(label=\"score\", path=sts_model_path)\npredictor_sts.fit(\n    sts_train_data, hyperparameters={\"model.hf_text.checkpoint_name\": f\"{dump_model_path}/hf_text\"}, time_limit=30\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for AutoGluon and SHAP Analysis in Python\nDESCRIPTION: This code snippet imports necessary libraries for AutoGluon, data manipulation, and SHAP analysis. It also initializes SHAP JavaScript and suppresses warnings.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/interpret/SHAP with AutoGluon-Tabular Diabetes regression.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.tabular import TabularPredictor\nimport pandas as pd\nimport numpy as np\nimport sklearn\nimport shap\nimport time\n\nshap.initjs()\n\nimport warnings\nwarnings.filterwarnings('ignore')\n```\n\n----------------------------------------\n\nTITLE: Initializing and Training TimeSeriesPredictor\nDESCRIPTION: Creates a TimeSeriesPredictor with specific parameters and fits it to the training data using medium quality presets and a time limit.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-quick-start.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npredictor = TimeSeriesPredictor(\n    prediction_length=48,\n    path=\"autogluon-m4-hourly\",\n    target=\"target\",\n    eval_metric=\"MASE\",\n)\n\npredictor.fit(\n    train_data,\n    presets=\"medium_quality\",\n    time_limit=600,\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating TimeSeriesPredictor with Multiple Custom Metrics in Python\nDESCRIPTION: Shows how to evaluate a trained predictor using multiple custom metrics including MeanAbsoluteScaledError, MeanQuantileLoss, and MeanSquaredError.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-metrics.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npredictor.evaluate(test_data, metrics=[MeanAbsoluteScaledError(), MeanQuantileLoss(), MeanSquaredError()])\n```\n\n----------------------------------------\n\nTITLE: Extracting Embeddings\nDESCRIPTION: Shows how to extract embeddings from the model for each sample.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/beginner_multimodal.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nembeddings = predictor.extract_embedding(test_data.drop(columns=label_col))\nembeddings.shape\n```\n\n----------------------------------------\n\nTITLE: Training Sentence Similarity Model with MultiModalPredictor\nDESCRIPTION: This snippet initializes and trains a MultiModalPredictor for the sentence similarity task, specifying the label column and a time limit for training.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/beginner_text.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nsts_model_path = f\"./tmp/{uuid.uuid4().hex}-automm_sts\"\npredictor_sts = MultiModalPredictor(label='score', path=sts_model_path)\npredictor_sts.fit(sts_train_data, time_limit=60)\n```\n\n----------------------------------------\n\nTITLE: Loading Saved Predictor\nDESCRIPTION: Demonstrates how to load a previously saved predictor from disk\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-essentials.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npredictor.path  # The path on disk where the predictor is saved\n```\n\nLANGUAGE: python\nCODE:\n```\npredictor = TabularPredictor.load(predictor.path)\n```\n\n----------------------------------------\n\nTITLE: Training Multimodal TabularPredictor\nDESCRIPTION: Initializes and trains AutoGluon's TabularPredictor with multimodal configuration for handling tabular, text, and image data simultaneously.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-multimodal.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.tabular import TabularPredictor\npredictor = TabularPredictor(label=label).fit(\n    train_data=train_data,\n    hyperparameters=hyperparameters,\n    feature_metadata=feature_metadata,\n    time_limit=900,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Optimizer Type in AutoMM\nDESCRIPTION: Examples demonstrating how to set the optimizer type in AutoMM, with options including sgd, adam, and adamw (default).\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"optim.optim_type\": \"adamw\"})\n# use optimizer adam\npredictor.fit(hyperparameters={\"optim.optim_type\": \"adam\"})\n```\n\n----------------------------------------\n\nTITLE: Customizing TabularPredictor Configuration\nDESCRIPTION: Code pattern for TabularPredictor usage across regression and classification tasks with multiple presets. Used for regression testing of model performance.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/whats_new/v0.4.1.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nTabularPredictor\n```\n\n----------------------------------------\n\nTITLE: Training TabularPredictor with fastText for Income Prediction\nDESCRIPTION: Configures and trains an AutoGluon TabularPredictor with custom hyperparameters including a Random Forest and FastTextModel, enabling raw text feature processing for the synthesized text column.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/tabular-fasttext.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncustom_hyperparameters = {'RF': {},\n                          FastTextModel:  {'epoch': 50},\n                         }\n\nfeature_generator = AutoMLPipelineFeatureGenerator(enable_raw_text_features=True)\n\npredictor = TabularPredictor(label=label).fit(train_data, hyperparameters=custom_hyperparameters, feature_generator=feature_generator)\n\ny_pred = predictor.predict(test_data)\ndf_res = pd.DataFrame({\n    'pred': y_pred,\n    'label': test_data[label]\n})\nprint('accuracy:', (df_res.pred.str.strip() == df_res.label.str.strip()).mean())\nprint(df_res.sample(5))\n```\n\n----------------------------------------\n\nTITLE: Setting Per-GPU Batch Size in AutoMM\nDESCRIPTION: Examples showing how to set the batch size for each GPU in AutoMM, with a default of 8 or a custom value of 16.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"env.per_gpu_batch_size\": 8})\n# use batch size 16 per GPU\npredictor.fit(hyperparameters={\"env.per_gpu_batch_size\": 16})\n```\n\n----------------------------------------\n\nTITLE: Fitting AutoGluon TabularPredictor in Python\nDESCRIPTION: Creates and fits a TabularPredictor using the prepared training data. The predictor is saved to a specified path for later use.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-deployment.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsave_path = 'agModels-predictClass-deployment'  # specifies folder to store trained models\npredictor = TabularPredictor(label=label, path=save_path).fit(train_data)\n```\n\n----------------------------------------\n\nTITLE: Processing Image Paths\nDESCRIPTION: Expands and processes image paths in the dataset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/beginner_multimodal.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimage_col = 'Images'\ntrain_data[image_col] = train_data[image_col].apply(lambda ele: ele.split(';')[0])\ntest_data[image_col] = test_data[image_col].apply(lambda ele: ele.split(';')[0])\n\ndef path_expander(path, base_folder):\n    path_l = path.split(';')\n    return ';'.join([os.path.abspath(os.path.join(base_folder, path)) for path in path_l])\n\ntrain_data[image_col] = train_data[image_col].apply(lambda ele: path_expander(ele, base_folder=dataset_path))\ntest_data[image_col] = test_data[image_col].apply(lambda ele: path_expander(ele, base_folder=dataset_path))\n\ntrain_data[image_col].iloc[0]\n```\n\n----------------------------------------\n\nTITLE: Comparing Models Using Leaderboard\nDESCRIPTION: Evaluates and compares the performance of all trained models, including the custom model, on the test dataset using AutoGluon's leaderboard functionality.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/advanced/forecasting-custom-model.ipynb#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\npredictor.leaderboard(test_data)\n```\n\n----------------------------------------\n\nTITLE: Making Class Predictions on a Document\nDESCRIPTION: Demonstrates how to use the trained model to predict the class label for a new document.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/document_prediction/document_classification.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npredictions = predictor.predict({DOC_PATH_COL: [doc_path]})\nprint(predictions)\n```\n\n----------------------------------------\n\nTITLE: Adding Time Series Covariates in Python\nDESCRIPTION: Creates log target as past covariate and weekend indicator as known covariate for time series prediction.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-indepth.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\ntrain_data[\"log_target\"] = np.log(train_data[\"target\"])\n\nWEEKEND_INDICES = [5, 6]\ntimestamps = train_data.index.get_level_values(\"timestamp\")\ntrain_data[\"weekend\"] = timestamps.weekday.isin(WEEKEND_INDICES).astype(float)\n\ntrain_data.head()\n```\n\n----------------------------------------\n\nTITLE: Fitting a Custom Random Forest Model in AutoGluon\nDESCRIPTION: This snippet demonstrates how to fit a custom random forest model using the cleaned features and labels. It also shows how to save and load the model.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-model.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncustom_model = CustomRandomForestModel()\n# We could also specify hyperparameters to override defaults\n# custom_model = CustomRandomForestModel(hyperparameters={'max_depth': 10})\ncustom_model.fit(X=X_clean, y=y_clean)  # Fit custom model\n\n# To save to disk and load the model, do the following:\n# load_path = custom_model.path\n# custom_model.save()\n# del custom_model\n# custom_model = CustomRandomForestModel.load(path=load_path)\n```\n\n----------------------------------------\n\nTITLE: Processing Image Paths for AutoGluon MultiModalPredictor\nDESCRIPTION: Processes image paths in the dataset to conform to AutoGluon's requirements. The code extracts the first image path for each record and expands it to an absolute path.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal-quick-start.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimage_col = 'Images'\n\ntrain_data[image_col] = train_data[image_col].apply(lambda ele: ele.split(';')[0])\ntest_data[image_col] = test_data[image_col].apply(lambda ele: ele.split(';')[0])\n\ndef path_expander(path, base_folder):\n    path_l = path.split(';')\n    return ';'.join([os.path.abspath(os.path.join(base_folder, path)) for path in path_l])\n\ntrain_data[image_col] = train_data[image_col].apply(lambda ele: path_expander(ele, base_folder=dataset_path))\ntest_data[image_col] = test_data[image_col].apply(lambda ele: path_expander(ele, base_folder=dataset_path))\n```\n\n----------------------------------------\n\nTITLE: Evaluating HPO Model Performance\nDESCRIPTION: Evaluates the HPO-enhanced model's accuracy on test data\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/hyperparameter_optimization.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nscores_hpo = predictor_hpo.evaluate(test_data, metrics=[\"accuracy\"])\nprint('Top-1 test acc: %.3f' % scores_hpo[\"accuracy\"])\n```\n\n----------------------------------------\n\nTITLE: Calculating Multiple SHAP Values\nDESCRIPTION: Generates SHAP values for multiple validation datapoints and creates force plot visualization.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/interpret/SHAP with AutoGluon-Tabular Census income classification.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nshap_values = explainer.shap_values(X_valid.iloc[0:N_VAL,:], nsamples=NSHAP_SAMPLES)\nshap.force_plot(explainer.expected_value, shap_values, X_valid.iloc[0:N_VAL,:])\n```\n\n----------------------------------------\n\nTITLE: Backtesting with Multiple Windows using ExpandingWindowSplitter in Python\nDESCRIPTION: This code demonstrates how to perform backtesting using multiple windows with an ExpandingWindowSplitter. It evaluates the predictor's performance on multiple forecast horizons generated from the same time series.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-indepth.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.timeseries.splitter import ExpandingWindowSplitter\n\nsplitter = ExpandingWindowSplitter(prediction_length=prediction_length, num_val_windows=3)\nfor window_idx, (train_split, val_split) in enumerate(splitter.split(test_data)):\n    score = predictor.evaluate(val_split)\n    print(f\"Window {window_idx}: score = {score}\")\n```\n\n----------------------------------------\n\nTITLE: Viewing Model Leaderboard in AutoGluon\nDESCRIPTION: Shows how to display a leaderboard of all models trained by AutoGluon, ranked by their performance on test data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-essentials.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\npredictor.leaderboard(test_data)\n```\n\n----------------------------------------\n\nTITLE: Fitting TimeSeriesPredictor with Static Features in Python\nDESCRIPTION: This code demonstrates how to fit a TimeSeriesPredictor using the previously created TimeSeriesDataFrame that includes static features. It sets the prediction length to 14 time steps.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-indepth.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npredictor = TimeSeriesPredictor(prediction_length=14).fit(train_data)\n```\n\n----------------------------------------\n\nTITLE: Extracting Text Embeddings\nDESCRIPTION: Extracts embeddings from query and document data for offline storage and efficient online matching.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/text_semantic_search.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nquery_embeds = predictor.extract_embedding(query_data[[query_id_col]], id_mappings=id_mappings, as_tensor=True)\ndoc_embeds = predictor.extract_embedding(doc_data[[doc_id_col]], id_mappings=id_mappings, as_tensor=True)\n```\n\n----------------------------------------\n\nTITLE: Training AutoGluon MultiModalPredictor for NER\nDESCRIPTION: This snippet initializes and trains an AutoGluon MultiModalPredictor for named entity recognition using text and image data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal_ner.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\nimport uuid\n\nlabel_col = \"entity_annotations\"\nmodel_path = f\"./tmp/{uuid.uuid4().hex}-automm_multimodal_ner\"\npredictor = MultiModalPredictor(problem_type=\"ner\", label=label_col, path=model_path)\npredictor.fit(\n\ttrain_data=train_data,\n\tcolumn_types={\"text_snippet\":\"text_ner\"},\n\ttime_limit=300, #second\n)\n```\n\n----------------------------------------\n\nTITLE: Training a Few Shot Text Classifier with MultiModalPredictor\nDESCRIPTION: Demonstrates how to initialize and train the MultiModalPredictor specifically for few shot text classification. It uses the 'few_shot_classification' problem type which leverages foundation models and SVM for efficient learning with limited data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/few_shot_learning.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\n\npredictor_fs_text = MultiModalPredictor(\n    problem_type=\"few_shot_classification\",\n    label=\"label\",  # column name of the label\n    eval_metric=\"acc\",\n)\npredictor_fs_text.fit(train_df)\nscores = predictor_fs_text.evaluate(test_df, metrics=[\"acc\", \"f1_macro\"])\nprint(scores)\n```\n\n----------------------------------------\n\nTITLE: Training a Few Shot Image Classifier with MultiModalPredictor\nDESCRIPTION: Demonstrates how to initialize and train the MultiModalPredictor for few shot image classification. Uses the 'few_shot_classification' problem type to extract features from images and train an SVM classifier with limited samples.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/few_shot_learning.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\n\npredictor_fs_image = MultiModalPredictor(\n    problem_type=\"few_shot_classification\",\n    label=\"LabelName\",  # column name of the label\n    eval_metric=\"acc\",\n)\npredictor_fs_image.fit(train_df)\nscores = predictor_fs_image.evaluate(test_df, metrics=[\"acc\", \"f1_macro\"])\nprint(scores)\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Checkpoint Averaging in AutoMM\nDESCRIPTION: Examples showing how to set the number of top model checkpoints to use for model averaging in AutoMM, with a default of 3 or a custom value of 5.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"optim.top_k\": 3})\n# use top 5 checkpoints\npredictor.fit(hyperparameters={\"optim.top_k\": 5})\n```\n\n----------------------------------------\n\nTITLE: Time Series Forecasting with AutoGluon\nDESCRIPTION: Example of using TimeSeriesPredictor for forecasting future values. Loads time series data, creates a predictor with specified prediction length, and generates forecasts.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/index.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n\ndata = TimeSeriesDataFrame('https://autogluon.s3.amazonaws.com/datasets/timeseries/m4_hourly/train.csv')\n\npredictor = TimeSeriesPredictor(target='target', prediction_length=48).fit(data)\npredictions = predictor.predict(data)\n```\n\n----------------------------------------\n\nTITLE: Continuing Training with Additional Data\nDESCRIPTION: Demonstrates how to continue training an existing model using additional data. It loads the saved model and fits it on a new subset of training data that wasn't used in the initial training.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/continuous_training.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npredictor_2 = MultiModalPredictor.load(model_path)  # you can also use the `predictor` we assigned above\ntrain_data_2 = train_data.drop(train_data_1.index).sample(n=subsample_size, random_state=0)\npredictor_2.fit(train_data_2, time_limit=60)\n```\n\n----------------------------------------\n\nTITLE: Making Predictions\nDESCRIPTION: Demonstrates how to make predictions on individual sentences and get class probabilities.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/beginner_text.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsentence1 = \"it's a charming and often affecting journey.\"\nsentence2 = \"It's slow, very, very, very slow.\"\npredictions = predictor.predict({'sentence': [sentence1, sentence2]})\nprint('\"Sentence\":', sentence1, '\"Predicted Sentiment\":', predictions[0])\nprint('\"Sentence\":', sentence2, '\"Predicted Sentiment\":', predictions[1])\n```\n\n----------------------------------------\n\nTITLE: Excluding Model Types from Presets in AutoGluon TimeSeriesPredictor (Python)\nDESCRIPTION: This snippet shows how to exclude specific models from being trained when using a predefined preset like `\"high_quality\"`. The `excluded_model_types` argument accepts a list of model names (strings) that should not be considered during the fitting process. In this example, `AutoETS` and `AutoARIMA` models are excluded.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-indepth.ipynb#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\npredictor.fit(\n    ...\n    presets=\"high_quality\",\n    excluded_model_types=[\"AutoETS\", \"AutoARIMA\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Training Custom Models Alongside Default Models in AutoGluon\nDESCRIPTION: This snippet demonstrates how to train custom models alongside AutoGluon's default models. It shows how to incorporate the custom model with tuned hyperparameters into the default model training process.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-model.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.tabular.configs.hyperparameter_configs import get_hyperparameter_config\n\n# Now we can add the custom model with tuned hyperparameters to be trained alongside the default models:\ncustom_hyperparameters = get_hyperparameter_config('default')\n\ncustom_hyperparameters[CustomRandomForestModel] = best_model_info['hyperparameters']\n\nprint(custom_hyperparameters)\n\npredictor = TabularPredictor(label=label).fit(train_data, hyperparameters=custom_hyperparameters)  # Train the default models plus a single tuned CustomRandomForestModel\n# predictor = TabularPredictor(label=label).fit(train_data, hyperparameters=custom_hyperparameters, presets='best_quality')  # We can even use the custom model in a multi-layer stack ensemble\npredictor.leaderboard(test_data)\n```\n\n----------------------------------------\n\nTITLE: Configuring Missing Image Handling in AutoMM Python\nDESCRIPTION: Sets the strategy for handling images that fail to open during data loading. `\"zero\"` (default) replaces the missing image with a zero tensor, while `\"skip\"` discards the entire sample containing the missing image. This configuration is passed via the `hyperparameters` argument to the `predictor.fit()` method.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_57\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"data.image.missing_value_strategy\": \"zero\"})\n# skip the image\npredictor.fit(hyperparameters={\"data.image.missing_value_strategy\": \"skip\"})\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Chronos Model\nDESCRIPTION: Demonstrates how to fine-tune a Chronos-Bolt model on the dataset, comparing zero-shot and fine-tuned versions.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-chronos.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npredictor = TimeSeriesPredictor(prediction_length=prediction_length).fit(\n    train_data=train_data,\n    hyperparameters={\n        \"Chronos\": [\n            {\"model_path\": \"bolt_small\", \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n            {\"model_path\": \"bolt_small\", \"fine_tune\": True, \"ag_args\": {\"name_suffix\": \"FineTuned\"}},\n        ]\n    },\n    time_limit=60,  # time limit in seconds\n    enable_ensemble=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Data Preprocessing Implementation\nDESCRIPTION: Implements preprocessing logic to handle missing values in time series data through forward-fill, backward-fill, and constant value filling.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/advanced/forecasting-custom-model.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef preprocess(\n        self,\n        data: TimeSeriesDataFrame,\n        known_covariates: Optional[TimeSeriesDataFrame] = None,\n        is_train: bool = False,\n        **kwargs,\n    ) -> Tuple[TimeSeriesDataFrame, Optional[TimeSeriesDataFrame]]:\n        data = data.fill_missing_values()\n        data = data.fill_missing_values(method=\"constant\", value=0.0)\n        return data, known_covariates\n```\n\n----------------------------------------\n\nTITLE: Creating Alternative Ensembles in AutoGluon\nDESCRIPTION: Constructs alternative ensemble models in AutoGluon by using different weighting schemes, providing various accuracy-speed tradeoffs. The ensembles are created using the expand_pareto_frontier parameter.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-indepth.ipynb#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nadditional_ensembles = predictor.fit_weighted_ensemble(expand_pareto_frontier=True)\nprint(\"Alternative ensembles you can use for prediction:\", additional_ensembles)\n\npredictor.leaderboard(only_pareto_frontier=True)\n```\n\n----------------------------------------\n\nTITLE: Making and Visualizing NER Predictions\nDESCRIPTION: Demonstrates how to make predictions on new data and visualize the results.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/chinese_ner.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\noutput = predictor.predict(dev_data)\nvisualize_ner(dev_data[\"text_snippet\"].iloc[0], output[0])\n```\n\n----------------------------------------\n\nTITLE: Generating Partial Dependence Plots\nDESCRIPTION: Creates partial dependence plots for all features in the training dataset with respect to the 'Survived' label.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-analyze-interaction.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nstate = auto.partial_dependence_plots(df_train, label='Survived', return_state=True)\n```\n\n----------------------------------------\n\nTITLE: Performing initial covariate shift detection\nDESCRIPTION: Uses AutoGluon's EDA module to detect covariate shift between the training and testing datasets, which identifies features that have different distributions.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-covariate-shift.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport autogluon.eda.auto as auto\n\nauto.covariate_shift_detection(train_data=df_train, test_data=df_test, label=target_col)\n```\n\n----------------------------------------\n\nTITLE: Generating Probability Predictions with MultiModalPredictor in Python\nDESCRIPTION: Uses the pretrained MultiModalPredictor to generate probability predictions for all classes on the preprocessed test data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/kaggle_feedback_prize/README.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntest_pred = pretrained_model.predict_proba(test_df)\n```\n\n----------------------------------------\n\nTITLE: Training Text Similarity Model with AutoMM\nDESCRIPTION: Initializes and trains a MultiModalPredictor for text similarity using the SNLI dataset. It specifies query, response, and label columns, and sets the match label and evaluation metric.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/text2text_matching.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\n\n# Initialize the model\npredictor = MultiModalPredictor(\n        problem_type=\"text_similarity\",\n        query=\"premise\", # the column name of the first sentence\n        response=\"hypothesis\", # the column name of the second sentence\n        label=\"label\", # the label column name\n        match_label=1, # the label indicating that query and response have the same semantic meanings.\n        eval_metric='auc', # the evaluation metric\n    )\n\n# Fit the model\npredictor.fit(\n    train_data=snli_train,\n    time_limit=180,\n)\n```\n\n----------------------------------------\n\nTITLE: Training the Model\nDESCRIPTION: Code to train the object detection model on the provided dataset and measure training time.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/quick_start/quick_start_coco.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nstart = time.time()\npredictor.fit(train_path)  # Fit\ntrain_end = time.time()\n\nprint(\"This finetuning takes %.2f seconds.\" % (train_end - start))\n```\n\n----------------------------------------\n\nTITLE: Processing Custom Label Studio Templates for AutoGluon in Python\nDESCRIPTION: This snippet shows how to use the 'from_customized' function to transform exports from custom Label Studio templates into a DataFrame for AutoGluon input. It requires specifying the path, whether Label Studio host access is needed, and the data and label column names.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/label_studio_export_reader/LabelStudio_export_file_reader.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom_customized(path, ls_host_on, data_columns, label_columns)\n```\n\n----------------------------------------\n\nTITLE: Extracting Embeddings and Visualization\nDESCRIPTION: Extracts embeddings from the model and visualizes them using TSNE dimensionality reduction.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/beginner_text.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nembeddings = predictor.extract_embedding(test_data)\nprint(embeddings.shape)\n\nfrom sklearn.manifold import TSNE\nX_embedded = TSNE(n_components=2, random_state=123).fit_transform(embeddings)\nfor val, color in [(0, 'red'), (1, 'blue')]:\n    idx = (test_data['label'].to_numpy() == val).nonzero()\n    plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], c=color, label=f'label={val}')\nplt.legend(loc='best')\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning SAM Model\nDESCRIPTION: Initializes and fine-tunes SAM using LoRA for efficient training on the leaf disease dataset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_segmentation/beginner_semantic_seg.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\nimport uuid\nsave_path = f\"./tmp/{uuid.uuid4().hex}-automm_semantic_seg\"\npredictor = MultiModalPredictor(\n    problem_type=\"semantic_segmentation\", \n    label=\"label\",\n     hyperparameters={\n            \"model.sam.checkpoint_name\": \"facebook/sam-vit-base\",\n        },\n    path=save_path,\n)\npredictor.fit(\n    train_data=train_data,\n    tuning_data=val_data,\n    time_limit=180, # seconds\n)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Feature Importance with Custom Model\nDESCRIPTION: Uses AutoGluon's feature importance analysis to understand which features are most influential in the custom model's predictions.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/advanced/forecasting-custom-model.ipynb#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\npredictor.feature_importance(test_data, model=\"NHITS\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Epoch to Disable Mixup/Cutmix in AutoMM Python\nDESCRIPTION: Specifies the epoch number after which Mixup and Cutmix augmentations will be disabled during training. The default value for `data.mixup.turn_off_epoch` is `5`. Set this hyperparameter in the `predictor.fit()` method.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_73\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"data.mixup.turn_off_epoch\": 5})\n# turn off mixup after 7 epochs\npredictor.fit(hyperparameters={\"data.mixup.turn_off_epoch\": 7})\n```\n\n----------------------------------------\n\nTITLE: Visualizing Forecasts and Actual Values\nDESCRIPTION: Plots the forecasts, including mean and quantiles, along with actual values for randomly chosen time series.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-quick-start.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n\n# TimeSeriesDataFrame can also be loaded directly from a file\ntest_data = TimeSeriesDataFrame.from_path(\"https://autogluon.s3.amazonaws.com/datasets/timeseries/m4_hourly_subset/test.csv\")\n\n# Plot 4 randomly chosen time series and the respective forecasts\npredictor.plot(test_data, predictions, quantile_levels=[0.1, 0.9], max_history_length=200, max_num_item_ids=4);\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Time Series Model in AutoGluon\nDESCRIPTION: Defines a custom time series model by subclassing AbstractTimeSeriesModel. Implements _fit, _predict, and preprocess methods for the NHITS model from NeuralForecast.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/advanced/forecasting-custom-model.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport pprint\nfrom typing import Optional, Tuple\n\nimport pandas as pd\n\nfrom autogluon.timeseries import TimeSeriesDataFrame\nfrom autogluon.timeseries.models.abstract import AbstractTimeSeriesModel\nfrom autogluon.timeseries.utils.warning_filters import warning_filter\n```\n\n----------------------------------------\n\nTITLE: Extracting Text Embeddings with MultiModalPredictor in Python\nDESCRIPTION: Uses the MultiModalPredictor to extract embeddings from text data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/zero_shot_img_txt_matching.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntext_embeddings = predictor.extract_embedding(texts, as_tensor=True)\nprint(text_embeddings.shape)\n```\n\n----------------------------------------\n\nTITLE: Training Student Model with Knowledge Distillation\nDESCRIPTION: Trains a student model using knowledge distillation from the teacher model with AutoGluon's MultiModalPredictor.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/model_distillation.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nstudent_predictor = MultiModalPredictor(label=\"label\")\nstudent_predictor.fit(\n    train_df,\n    tuning_data=valid_df,\n    teacher_predictor=teacher_predictor,\n    hyperparameters={\n        \"model.hf_text.checkpoint_name\": \"google/bert_uncased_L-6_H-768_A-12\",\n        \"optim.max_epochs\": 2,\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Comparing Model Configurations with Leaderboard\nDESCRIPTION: Evaluates the performance of different hyperparameter configurations of the custom model to identify the best-performing version.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/advanced/forecasting-custom-model.ipynb#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\npredictor.leaderboard(test_data)\n```\n\n----------------------------------------\n\nTITLE: Displaying Model Leaderboard\nDESCRIPTION: Shows the performance leaderboard of all trained models on the test data\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-quick-start.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npredictor.leaderboard(test_data)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Object Detection Model with DataFrame in AutoGluon\nDESCRIPTION: This snippet shows how to evaluate a trained object detection model using AutoGluon's MultiModalPredictor with test data in DataFrame format.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/object_detection_with_dataframe.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntest_df = from_coco(test_path)\npredictor_df.evaluate(test_df)\n```\n\n----------------------------------------\n\nTITLE: Initializing MultiModalPredictor for Text Similarity\nDESCRIPTION: Sets up the MultiModalPredictor with text similarity configuration using a pre-trained sentence transformer model. Specifies query, response, and label columns for the predictor.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/text_semantic_search.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\n\npredictor = MultiModalPredictor(\n        query=query_id_col,\n        response=doc_id_col,\n        label=label_col,\n        problem_type=\"text_similarity\",\n        hyperparameters={\"model.hf_text.checkpoint_name\": \"sentence-transformers/all-MiniLM-L6-v2\"}\n    )\n```\n\n----------------------------------------\n\nTITLE: Visualizing Detection Results\nDESCRIPTION: Visualizes the detection results on a sample image from the test set, including confidence threshold filtering and display.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/advanced/finetune_coco.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal.utils import visualize_detection\nconf_threshold = 0.25  # Specify a confidence threshold to filter out unwanted boxes\nvisualization_result_dir = \"./\"  # Use the pwd as result dir to save the visualized image\nvisualized = visualize_detection(\n    pred=pred[12:13],\n    detection_classes=predictor.classes,\n    conf_threshold=conf_threshold,\n    visualization_result_dir=visualization_result_dir,\n)\nfrom PIL import Image\nfrom IPython.display import display\nimg = Image.fromarray(visualized[0][:, :, ::-1], 'RGB')\ndisplay(img)\n```\n\n----------------------------------------\n\nTITLE: Initializing SHAP Explainer\nDESCRIPTION: Creates KernelExplainer instance and sets parameters for SHAP analysis.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/interpret/SHAP with AutoGluon-Tabular Census income classification.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nag_wrapper = AutogluonWrapper(predictor, feature_names)\nexplainer = shap.KernelExplainer(ag_wrapper.predict_binary_prob, med)\n\nNSHAP_SAMPLES = 100  # how many samples to use to approximate each Shapely value, larger values will be slower\nN_VAL = 30  # how many datapoints from validation data should we interpret predictions for, larger values will be slower\n```\n\n----------------------------------------\n\nTITLE: Training TimeSeriesPredictor with Multiple Models\nDESCRIPTION: Example showing how to train a TimeSeriesPredictor with DeepAR and ETS models using default hyperparameters and weighted ensemble.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-model-zoo.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npredictor = TimeSeriesPredictor().fit(\n   train_data,\n   hyperparameters={\n      \"DeepAR\": {},\n      \"ETS\": {},\n   },\n)\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with MultilabelPredictor in Python\nDESCRIPTION: Loads a test dataset and samples it for demonstration purposes. It uses a trained MultilabelPredictor to predict labels for new, unlabeled data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-multilabel.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ntest_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv')\ntest_data = test_data.sample(n=subsample_size, random_state=0)\ntest_data_nolab = test_data.drop(columns=labels)  # unnecessary, just to demonstrate we're not cheating here\ntest_data_nolab.head()\n```\n\n----------------------------------------\n\nTITLE: Making Predictions\nDESCRIPTION: Demonstrates making class predictions and probability predictions on test data\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-essentials.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ny_pred = predictor.predict(test_data)\ny_pred.head()  # Predictions\n```\n\nLANGUAGE: python\nCODE:\n```\ny_pred_proba = predictor.predict_proba(test_data)\ny_pred_proba.head()  # Prediction Probabilities\n```\n\n----------------------------------------\n\nTITLE: Refitting AutoGluon TabularPredictor Clone in Python\nDESCRIPTION: Demonstrates refitting the cloned predictor with additional models and comparing the results with the original predictor.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-deployment.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npredictor_clone.refit_full()\n\npredictor_clone.leaderboard(test_data)\n\npredictor.leaderboard(test_data)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating CLIP's Vulnerability to Typographic Attacks\nDESCRIPTION: Uses the MultiModalPredictor to classify the image of an apple with 'iPod' text, showing how CLIP's prediction is influenced by the text in the image.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_prediction/clip_zeroshot.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprob = predictor.predict_proba({\"image\": [image_path]}, {\"text\": ['Granny Smith', 'iPod', 'library', 'pizza', 'toaster', 'dough']})\nprint(\"Label probs:\", prob)\n```\n\n----------------------------------------\n\nTITLE: HPO-Enhanced Model Training\nDESCRIPTION: Implements hyperparameter optimization using Ray Tune with Bayesian optimization and ASHA scheduler\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/hyperparameter_optimization.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import tune\n\npredictor_hpo = MultiModalPredictor(label=\"label\")\n\nhyperparameters = {\n            \"optim.lr\": tune.uniform(0.00005, 0.001),\n            \"model.timm_image.checkpoint_name\": tune.choice([\"ghostnet_100\",\n                                                             \"mobilenetv3_large_100\"])\n}\nhyperparameter_tune_kwargs = {\n    \"searcher\": \"bayes\", # random\n    \"scheduler\": \"ASHA\",\n    \"num_trials\": 2,\n    \"num_to_keep\": 3,\n}\nstart_time_hpo = datetime.now()\npredictor_hpo.fit(\n        train_data=train_data,\n        hyperparameters=hyperparameters,\n        hyperparameter_tune_kwargs=hyperparameter_tune_kwargs,\n    )\nend_time_hpo = datetime.now()\nelapsed_seconds_hpo = (end_time_hpo - start_time_hpo).total_seconds()\nelapsed_min_hpo = divmod(elapsed_seconds_hpo, 60)\nprint(\"Total fitting time: \", f\"{int(elapsed_min_hpo[0])}m{int(elapsed_min_hpo[1])}s\")\n```\n\n----------------------------------------\n\nTITLE: Evaluating the Model After Additional Training\nDESCRIPTION: Evaluates the model after additional training with new data to see if performance has improved from the initial model.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/continuous_training.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntest_score_2 = predictor_2.evaluate(test_data)\nprint(test_score_2)\n```\n\n----------------------------------------\n\nTITLE: Mapping Model Keys to Model Classes\nDESCRIPTION: Dictionary mapping model keys used in hyperparameters to their corresponding model classes in AutoGluon. This includes various model types such as random forests, gradient boosting, neural networks, and ensemble models.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/api/autogluon.tabular.models.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nMODEL_TYPES = dict(\n    RF=RFModel,\n    XT=XTModel,\n    KNN=KNNModel,\n    GBM=LGBModel,\n    CAT=CatBoostModel,\n    XGB=XGBoostModel,\n    NN_TORCH=TabularNeuralNetTorchModel,\n    LR=LinearModel,\n    FASTAI=NNFastAiTabularModel,\n    AG_TEXT_NN=TextPredictorModel,\n    AG_IMAGE_NN=ImagePredictorModel,\n    AG_AUTOMM=MultiModalPredictorModel,\n\n    FT_TRANSFORMER=FTTransformerModel,\n    TABPFN=TabPFNModel,\n\n    FASTTEXT=FastTextModel,\n    ENS_WEIGHTED=GreedyWeightedEnsembleModel,\n    SIMPLE_ENS_WEIGHTED=SimpleWeightedEnsembleModel,\n\n    # interpretable models\n    IM_RULEFIT=RuleFitModel,\n    IM_GREEDYTREE=GreedyTreeModel,\n    IM_FIGS=FigsModel,\n    IM_HSTREE=HSTreeModel,\n    IM_BOOSTEDRULES=BoostedRulesModel,\n\n    DUMMY=DummyModel,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Batch Size in AutoMM Training\nDESCRIPTION: Examples of setting the batch size for training. Default is 128, but can be adjusted based on available resources and model requirements.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"env.batch_size\": 128})\n# use batch size 256\npredictor.fit(hyperparameters={\"env.batch_size\": 256})\n```\n\n----------------------------------------\n\nTITLE: Training with Best Quality Preset\nDESCRIPTION: Initializes a MultiModalPredictor with best_quality preset and fits it on the training data with a 180-second time limit.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/presets.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\n\npredictor = MultiModalPredictor(label='label', eval_metric='acc', presets=\"best_quality\")\npredictor.fit(train_data=train_data, time_limit=180)\n```\n\n----------------------------------------\n\nTITLE: Refitting Bagged Ensembles in AutoGluon\nDESCRIPTION: Refits bagged models to the complete dataset to create single models with reduced memory and latency requirements. The leaderboard is updated with these refit models, which lack validation scores.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-indepth.ipynb#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nrefit_model_map = predictor.refit_full()\nprint(\"Name of each refit-full model corresponding to a previous bagged ensemble:\")\nprint(refit_model_map)\npredictor.leaderboard(test_data)\n```\n\n----------------------------------------\n\nTITLE: Extracting Document Embeddings from PDF\nDESCRIPTION: Extracts the learned N-dimensional document feature representation from a PDF document using the trained model.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/document_prediction/pdf_classification.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfeature = predictor.extract_embedding({DOC_PATH_COL: [test_data.iloc[0][DOC_PATH_COL]]})\nprint(feature[0].shape)\n```\n\n----------------------------------------\n\nTITLE: Downloading Twitter Dataset for NER in Python\nDESCRIPTION: This code downloads and unzips a multimodal NER dataset from an S3 bucket using AutoGluon's utility function.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal_ner.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndownload_dir = './ag_automm_tutorial_ner'\nzip_file = 'https://automl-mm-bench.s3.amazonaws.com/ner/multimodal_ner.zip'\nfrom autogluon.core.utils.loaders import load_zip\nload_zip.unzip(zip_file, unzip_dir=download_dir)\n```\n\n----------------------------------------\n\nTITLE: Semantic Search Implementation\nDESCRIPTION: Demonstrates semantic search functionality for finding similar images from text queries\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/image_text_matching.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal.utils import semantic_search\ntext_to_image_hits = semantic_search(\n        matcher=predictor,\n        query_data=test_text_data.iloc[[3]],\n        response_data=test_image_data,\n        top_k=5,\n    )\n```\n\n----------------------------------------\n\nTITLE: Predicting NER for New Chinese Text\nDESCRIPTION: Shows how to make predictions on a new Chinese text example about a rabbit toy.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/chinese_ner.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsentence = \"2023\"\npredictions = predictor.predict({'text_snippet': [sentence]})\nvisualize_ner(sentence, predictions[0])\n```\n\n----------------------------------------\n\nTITLE: Evaluating Custom Model Performance\nDESCRIPTION: Evaluates the performance of the custom model on the provided dataset using the built-in score method.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/advanced/forecasting-custom-model.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nmodel.score(data)\n```\n\n----------------------------------------\n\nTITLE: Training a Default Image Classifier for Comparison\nDESCRIPTION: Initializes and trains a standard MultiModalPredictor for image classification using the default 'classification' problem type. This serves as a baseline for comparing performance with the few shot approach on image data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/few_shot_learning.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\n\npredictor_default_image = MultiModalPredictor(\n    problem_type=\"classification\",\n    label=\"LabelName\",  # column name of the label\n    eval_metric=\"acc\",\n)\npredictor_default_image.fit(train_data=train_df)\nscores = predictor_default_image.evaluate(test_df, metrics=[\"acc\", \"f1_macro\"])\nprint(scores)\n```\n\n----------------------------------------\n\nTITLE: Implementing Hybrid BM25 Search\nDESCRIPTION: Combines BM25 and semantic embedding scores for improved search ranking. Includes functions for score normalization and hybrid ranking computation.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/text_semantic_search.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom autogluon.multimodal.utils import compute_semantic_similarity\n\ndef hybridBM25(query_data, query_embeds, doc_data, doc_embeds, recall_num, top_k, beta):\n    # Recall documents with BM25 scores\n    tokenized_corpus = tokenize_corpus(doc_data[text_col].tolist())\n    bm25_model = BM25Okapi(tokenized_corpus, k1=1.2, b=0.75)\n    bm25_scores = rank_documents_bm25(query_data[text_col].tolist(), query_data[query_id_col].tolist(), doc_data[doc_id_col].tolist(), recall_num, bm25_model)\n    \n    all_bm25_scores = [score for scores in bm25_scores.values() for score in scores.values()]\n    max_bm25_score = max(all_bm25_scores)\n    min_bm25_score = min(all_bm25_scores)\n\n    q_embeddings = {qid: embed for qid, embed in zip(query_data[query_id_col].tolist(), query_embeds)}\n    d_embeddings = {did: embed for did, embed in zip(doc_data[doc_id_col].tolist(), doc_embeds)}\n    \n    query_ids = query_data[query_id_col].tolist()\n    results = {qid: {} for qid in query_ids}\n    for idx, qid in enumerate(query_ids):\n        rec_docs = bm25_scores[qid]\n        rec_doc_emb = [d_embeddings[doc_id] for doc_id in rec_docs.keys()]\n        rec_doc_id = [doc_id for doc_id in rec_docs.keys()]\n        rec_doc_emb = torch.stack(rec_doc_emb)\n        scores = compute_semantic_similarity(q_embeddings[qid], rec_doc_emb)\n        scores[torch.isnan(scores)] = -1\n        top_k_values, top_k_idxs = torch.topk(\n            scores,\n            min(top_k + 1, len(scores[0])),\n            dim=1,\n            largest=True,\n            sorted=False,\n        )\n\n        for doc_idx, score in zip(top_k_idxs[0], top_k_values[0]):\n            doc_id = rec_doc_id[int(doc_idx)]\n            # Hybrid scores from BM25 and cosine similarity of embeddings\n            results[qid][doc_id] = \\\n                (1 - beta) * float(score.numpy()) \\\n                + beta * (bm25_scores[qid][doc_id] - min_bm25_score) / (max_bm25_score - min_bm25_score)\n    \n    return results\n```\n\n----------------------------------------\n\nTITLE: Configuring Predictor with Custom Quantile Levels\nDESCRIPTION: Demonstrates how to initialize a TimeSeriesPredictor with WQL metric and custom quantile levels for probabilistic forecasting.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-metrics.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npredictor = TimeSeriesPredictor(eval_metric=\"WQL\", quantile_levels=[0.1, 0.5, 0.75, 0.9])\n```\n\n----------------------------------------\n\nTITLE: Calculating Feature Importance using AutoGluon's Built-in Method in Python\nDESCRIPTION: This code demonstrates how to use AutoGluon's built-in feature importance method based on permutation-shuffling to assess overall feature importance.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/interpret/SHAP with AutoGluon-Tabular Diabetes regression.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nval_data[label] = y_valid  # add labels to validation DataFrame\npredictor.feature_importance(val_data)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Text Similarity Model on Test Dataset\nDESCRIPTION: Evaluates the trained model on the test dataset using the ROC AUC score.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/text2text_matching.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nscore = predictor.evaluate(snli_test)\nprint(\"evaluation score: \", score)\n```\n\n----------------------------------------\n\nTITLE: Loading SNLI Dataset with AutoGluon\nDESCRIPTION: Downloads and loads the SNLI corpus into dataframes using AutoGluon's load_pd function.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/text2text_matching.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.core.utils.loaders import load_pd\nimport pandas as pd\n\nsnli_train = load_pd.load('https://automl-mm-bench.s3.amazonaws.com/snli/snli_train.csv', delimiter=\"|\")\nsnli_test = load_pd.load('https://automl-mm-bench.s3.amazonaws.com/snli/snli_test.csv', delimiter=\"|\")\nsnli_train.head()\n```\n\n----------------------------------------\n\nTITLE: Fitting the MultilabelPredictor in Python\nDESCRIPTION: This code snippet demonstrates training a MultilabelPredictor with specified labels, problem types, and evaluation metrics. The fit method is called on the training dataset with a specified time limit.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-multilabel.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nmulti_predictor = MultilabelPredictor(labels=labels, problem_types=problem_types, eval_metrics=eval_metrics, path=save_path)\nmulti_predictor.fit(train_data, time_limit=time_limit)\n```\n\n----------------------------------------\n\nTITLE: Selecting Text Tokenizer\nDESCRIPTION: Choice of text tokenizer implementation from various options including auto, BERT, ELECTRA, and CLIP tokenizers.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_36\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"model.hf_text.tokenizer_name\": \"hf_auto\"})\n# using the tokenizer of the ELECTRA model\npredictor.fit(hyperparameters={\"model.hf_text.tokenizer_name\": \"electra\"})\n```\n\n----------------------------------------\n\nTITLE: Evaluating Document Classifier Performance\nDESCRIPTION: Evaluates the trained document classifier on the test dataset using accuracy as the evaluation metric.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/document_prediction/document_classification.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nscores = predictor.evaluate(test_data, metrics=[\"accuracy\"])\nprint('The test acc: %.3f' % scores[\"accuracy\"])\n```\n\n----------------------------------------\n\nTITLE: Setting Compilation Backend\nDESCRIPTION: Configuration for the backend used in PyTorch model compilation.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_32\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"env.compile.backend\": \"inductor\"})\n```\n\n----------------------------------------\n\nTITLE: Training a Default Text Classifier for Comparison\nDESCRIPTION: Initializes and trains a standard MultiModalPredictor for text classification using the default 'classification' problem type. This serves as a baseline for comparing performance with the few shot approach.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/few_shot_learning.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\n\npredictor_default_text = MultiModalPredictor(\n    label=\"label\",\n    problem_type=\"classification\",\n    eval_metric=\"acc\",\n)\npredictor_default_text.fit(train_data=train_df)\nscores = predictor_default_text.evaluate(test_df, metrics=[\"acc\", \"f1_macro\"])\nprint(scores)\n```\n\n----------------------------------------\n\nTITLE: Downloading and Displaying an Image for Classification\nDESCRIPTION: Downloads a dog image from a URL and displays it in the notebook for subsequent zero-shot classification.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_prediction/clip_zeroshot.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import Image, display\nfrom autogluon.multimodal import download\n\nurl = \"https://farm4.staticflickr.com/3445/3262471985_ed886bf61a_z.jpg\"\ndog_image = download(url)\n\npil_img = Image(filename=dog_image)\ndisplay(pil_img)\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon with PyTorch CPU Dependencies\nDESCRIPTION: This console command sequence installs AutoGluon with its dependencies. It first updates pip and setuptools to their latest versions, then installs AutoGluon while specifying PyTorch's CPU-only package repository as an extra index URL.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/install-mac-cpu.md#2025-04-22_snippet_0\n\nLANGUAGE: console\nCODE:\n```\npip install -U pip\npip install -U setuptools wheel\n\npip install autogluon --extra-index-url https://download.pytorch.org/whl/cpu\n```\n\n----------------------------------------\n\nTITLE: Extracting Document Embeddings\nDESCRIPTION: Extracts feature embeddings from a document using the trained model, which can be useful for downstream tasks or similarity-based document retrieval.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/document_prediction/document_classification.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfeature = predictor.extract_embedding({DOC_PATH_COL: [doc_path]})\nprint(feature[0].shape)\n```\n\n----------------------------------------\n\nTITLE: Setting Hugging Face Text Model Checkpoint\nDESCRIPTION: Specification of the text backbone model from Hugging Face's AutoModel collection.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_34\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"model.hf_text.checkpoint_name\": \"google/electra-base-discriminator\"})\n# choose roberta base\npredictor.fit(hyperparameters={\"model.hf_text.checkpoint_name\": \"roberta-base\"})\n```\n\n----------------------------------------\n\nTITLE: Maximizing Forecast Accuracy with AutoGluon TimeSeriesPredictor in Python\nDESCRIPTION: This code snippet demonstrates how to configure AutoGluon's TimeSeriesPredictor for maximum forecast accuracy. It sets the presets to 'best_quality', specifies a high time limit, and increases the number of validation windows.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-faq.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npredictor.fit(presets=\"best_quality\", time_limit=high_value, num_val_windows=3)\n```\n\n----------------------------------------\n\nTITLE: Configuring Multilabel Prediction in AutoGluon\nDESCRIPTION: Defines configuration for multilabel prediction, including labels, problem types, evaluation metrics, and save paths. These settings help define how the MultilabelPredictor should train models for each label.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-multilabel.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nlabels = ['education-num','education','class']  # which columns to predict based on the others\nproblem_types = ['regression','multiclass','binary']  # type of each prediction problem (optional)\neval_metrics = ['mean_absolute_error','accuracy','accuracy']  # metrics used to evaluate predictions for each label (optional)\nsave_path = 'agModels-predictEducationClass'  # specifies folder to store trained models (optional)\n\ntime_limit = 5  # how many seconds to train the TabularPredictor for each label, set much larger in your applications!\n```\n\n----------------------------------------\n\nTITLE: Running GLUE Benchmark with AutoGluon Text\nDESCRIPTION: Prepares data and runs AutoGluon TextPredictor on the GLUE benchmark tasks. Includes examples for single model and stacking modes, as well as generating submission files.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/text_prediction/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython3 prepare_glue.py --benchmark glue\n\n# Run single model\nbash run_glue.sh single\n\n# Run 5-fold stacking\nbash run_glue.sh stacking\n\n# Generate submission file\npython3 generate_submission.py --prefix autogluon_text --save_dir submission\n```\n\n----------------------------------------\n\nTITLE: Cleaning Features Using AutoGluon's Feature Generator\nDESCRIPTION: This code snippet shows how to clean and transform features using AutoGluon's AutoMLPipelineFeatureGenerator. It converts object dtypes to categorical and minimizes memory usage.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-model.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.common.utils.log_utils import set_logger_verbosity\nfrom autogluon.features.generators import AutoMLPipelineFeatureGenerator\nset_logger_verbosity(2)  # Set logger so more detailed logging is shown for tutorial\n\nfeature_generator = AutoMLPipelineFeatureGenerator()\nX_clean = feature_generator.fit_transform(X)\n\nX_clean.head(5)\n```\n\n----------------------------------------\n\nTITLE: Calibrating Decision Threshold for Binary Classification in Python\nDESCRIPTION: This code demonstrates how to calibrate the decision threshold for binary classification tasks to improve metric scores like 'balanced_accuracy' or 'f1'.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-indepth.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprint(f'Prior to calibration (predictor.decision_threshold={predictor.decision_threshold}):'))\nscores = predictor.evaluate(test_data)\n\ncalibrated_decision_threshold = predictor.calibrate_decision_threshold()\npredictor.set_decision_threshold(calibrated_decision_threshold)\n\nprint(f'After calibration (predictor.decision_threshold={predictor.decision_threshold}):'))\nscores_calibrated = predictor.evaluate(test_data)\n```\n\n----------------------------------------\n\nTITLE: Training AutoGluon Regressor for Diabetes Prediction in Python\nDESCRIPTION: This code trains an AutoGluon regressor on the prepared diabetes dataset with a time limit of 20 seconds.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/interpret/SHAP with AutoGluon-Tabular Diabetes regression.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npredictor = TabularPredictor(label=label, problem_type='regression').fit(train_data, time_limit=20)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Image Classifier on Test Dataset\nDESCRIPTION: Evaluates the trained classifier on the test dataset and prints the accuracy.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_prediction/beginner_image_cls.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nscores = predictor.evaluate(test_data_path, metrics=[\"accuracy\"])\nprint('Top-1 test acc: %.3f' % scores[\"accuracy\"])\n```\n\n----------------------------------------\n\nTITLE: Evaluating AutoGluon NER Model Performance\nDESCRIPTION: This code evaluates the trained NER model on the test dataset using recall, precision, and F1 score metrics.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal_ner.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npredictor.evaluate(test_data,  metrics=['overall_recall', \"overall_precision\", \"overall_f1\"])\n```\n\n----------------------------------------\n\nTITLE: Creating AutoGluon Wrapper Class\nDESCRIPTION: Implements a wrapper class to make AutoGluon predictor compatible with SHAP's interface requirements.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/interpret/SHAP with AutoGluon-Tabular Census income classification.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass AutogluonWrapper:\n    def __init__(self, predictor, feature_names):\n        self.ag_model = predictor\n        self.feature_names = feature_names\n    \n    def predict_binary_prob(self, X):\n        if isinstance(X, pd.Series):\n            X = X.values.reshape(1,-1)\n        if not isinstance(X, pd.DataFrame):\n            X = pd.DataFrame(X, columns=self.feature_names)\n        return self.ag_model.predict_proba(X, as_multiclass=False)\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon and Dependencies via Console Commands\nDESCRIPTION: This snippet demonstrates the process of installing AutoGluon and its required dependencies. It includes steps for installing uv, PyTorch with CPU support, cloning the AutoGluon repository, and running the full installation script.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/install-cpu-source.md#2025-04-22_snippet_0\n\nLANGUAGE: console\nCODE:\n```\npip install uv\npython -m uv pip install -U torch torchvision --extra-index-url https://download.pytorch.org/whl/cpu\ngit clone https://github.com/autogluon/autogluon\ncd autogluon && ./full_install.sh\n```\n\n----------------------------------------\n\nTITLE: Zero-Shot SAM Initialization and Prediction\nDESCRIPTION: Initializes and performs zero-shot prediction using the base SAM model.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_segmentation/beginner_semantic_seg.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\npredictor_zero_shot = MultiModalPredictor(\n    problem_type=\"semantic_segmentation\", \n    label=label_col,\n     hyperparameters={\n            \"model.sam.checkpoint_name\": \"facebook/sam-vit-base\",\n        },\n    num_classes=1, # forground-background segmentation\n)\n\npred_zero_shot = predictor_zero_shot.predict({'image': [test_data.iloc[0]['image']]})\n```\n\n----------------------------------------\n\nTITLE: Configuring TimeSeriesPredictor with Multiple Validation Windows in Python\nDESCRIPTION: This code shows how to configure the TimeSeriesPredictor to use multiple validation windows during training. It increases the number of validation windows to 3, which can reduce overfitting but increases training time.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-indepth.ipynb#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\npredictor = TimeSeriesPredictor(...)\npredictor.fit(train_data, num_val_windows=3)\n```\n\n----------------------------------------\n\nTITLE: Checking Positive Class and Getting Predictions for Binary Classification\nDESCRIPTION: This code shows how to check which class AutoGluon's predicted probabilities correspond to and how to get predictions for the positive class in binary classification.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-kaggle.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npredictor.positive_class\n\ny_predproba = predictor.predict_proba(test_data, as_multiclass=False)\n```\n\n----------------------------------------\n\nTITLE: Customizing Hyperparameter Tuning Settings in AutoGluon TimeSeriesPredictor (Python)\nDESCRIPTION: This snippet demonstrates how to customize the hyperparameter optimization (HPO) process by providing a dictionary to the `hyperparameter_tune_kwargs` argument. This allows specifying the `num_trials` (number of configurations to try per model), the `searcher` (e.g., `\"random\"`), and the `scheduler` (e.g., `\"local\"`). This provides more control over the HPO execution compared to using `\"auto\"`.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-indepth.ipynb#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\npredictor.fit(\n    ...\n    hyperparameter_tune_kwargs={\n        \"num_trials\": 20,\n        \"scheduler\": \"local\",\n        \"searcher\": \"random\",\n    },\n    ...\n)\n```\n\n----------------------------------------\n\nTITLE: Covariate Shift Analysis\nDESCRIPTION: Detecting and visualizing covariate shift between train and test datasets.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-anomaly-detection.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nauto.covariate_shift_detection(train_data=x, test_data=x_test, label=target_col)\n\nax = sns.lineplot(data=df_train[['PassengerId']].reset_index(), x='index', y='PassengerId', label='Train')\nsns.lineplot(ax=ax, data=df_test[['PassengerId']].reset_index(), x='index', y='PassengerId', label='Test');\n```\n\n----------------------------------------\n\nTITLE: Implementing Bagging for Custom Models in AutoGluon\nDESCRIPTION: This code snippet shows how to implement bagging for custom models in AutoGluon using the BaggedEnsembleModel class. It demonstrates a quick way to improve model quality.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-model.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.core.models import BaggedEnsembleModel\nbagged_custom_model = BaggedEnsembleModel(CustomRandomForestModel())\n# Parallel folding currently doesn't work with a class not defined in a separate module because of underlying pickle serialization issue\n# You don't need this following line if you put your custom model in a separate file and import it.\nbagged_custom_model.params['fold_fitting_strategy'] = 'sequential_local' \nbagged_custom_model.fit(X=X_clean, y=y_clean, k_fold=10)  # Perform 10-fold bagging\nbagged_score = bagged_custom_model.score(X_test_clean, y_test_clean)\nprint(f'Test score ({bagged_custom_model.eval_metric.name}) = {bagged_score} (bagged)')\nprint(f'Bagging increased model accuracy by {round(bagged_score - score, 4) * 100}%!')\n```\n\n----------------------------------------\n\nTITLE: Saving a Standalone MultiModalPredictor Model in Python\nDESCRIPTION: This snippet shows how to save a MultiModalPredictor model as a standalone model, which can be loaded without internet access. This is crucial for Kaggle competitions where network access is restricted.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/kaggle_pawpularity/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npredictor.save(path=save_standalone_path, standalone=True)\n```\n\n----------------------------------------\n\nTITLE: Training TabularPredictor\nDESCRIPTION: Initializes and fits the TabularPredictor on the training data\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-essentials.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npredictor = TabularPredictor(label=label).fit(train_data)\n```\n\n----------------------------------------\n\nTITLE: Model Performance Evaluation\nDESCRIPTION: Evaluating the model's performance on test data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal_text_tabular.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nperformance = predictor.evaluate(test_data)\nprint(performance)\n```\n\n----------------------------------------\n\nTITLE: Persisting AutoGluon Models in Memory for Faster Inference (Python)\nDESCRIPTION: This code shows how to keep AutoGluon models in memory for faster repeated predictions. It demonstrates persisting models, making predictions on individual datapoints, and then unpersisting to free memory.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-indepth.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\npredictor.persist()\n\nnum_test = 20\npreds = np.array(['']*num_test, dtype='object')\nfor i in range(num_test):\n    datapoint = test_data_nolabel.iloc[[i]]\n    pred_numpy = predictor.predict(datapoint, as_pandas=False)\n    preds[i] = pred_numpy[0]\n\nperf = predictor.evaluate_predictions(y_test[:num_test], preds, auxiliary_metrics=True)\nprint(\"Predictions: \", preds)\n\npredictor.unpersist()  # free memory by clearing models, future predict() calls will load models from disk\n```\n\n----------------------------------------\n\nTITLE: Creating Deployment-Optimized AutoGluon TabularPredictor Clone in Python\nDESCRIPTION: Creates a clone of the predictor optimized for deployment with minimal artifacts needed for prediction.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-deployment.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nsave_path_clone_opt = save_path + '-clone-opt'\n# will return the path to the cloned predictor, identical to save_path_clone_opt\npath_clone_opt = predictor.clone_for_deployment(path=save_path_clone_opt)\n\npredictor_clone_opt = TabularPredictor.load(path=path_clone_opt)\n\npredictor_clone_opt.persist()\n\ny_pred_clone_opt = predictor_clone_opt.predict(test_data)\ny_pred_clone_opt\n\ny_pred.equals(y_pred_clone_opt)\n\npredictor_clone_opt.leaderboard(test_data)\n```\n\n----------------------------------------\n\nTITLE: Predicting with Selected Ensembles in AutoGluon\nDESCRIPTION: This snippet selects a model for prediction from a list of alternative ensembles, using AutoGluon's predict and delete_models methods to generate predictions and manage model persistence.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-indepth.ipynb#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nmodel_for_prediction = additional_ensembles[0]\npredictions = predictor.predict(test_data, model=model_for_prediction)\npredictor.delete_models(models_to_delete=additional_ensembles, dry_run=False)\n```\n\n----------------------------------------\n\nTITLE: Excluding Unwieldy Model Types in AutoGluon\nDESCRIPTION: Excludes specific model types known to be slower during AutoGluon's `fit()` to ensure more efficient models are prioritized, tailored to user needs.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-indepth.ipynb#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nexcluded_model_types = ['KNN', 'NN_TORCH']\npredictor_light = TabularPredictor(label=label, eval_metric=metric).fit(train_data, excluded_model_types=excluded_model_types, time_limit=30)\n```\n\n----------------------------------------\n\nTITLE: Generating Single Prediction SHAP Values\nDESCRIPTION: Explains prediction for a single datapoint using SHAP values and creates visualization.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/interpret/SHAP with AutoGluon-Tabular Census income classification.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nROW_INDEX = 0  # index of an example datapoint\nsingle_datapoint = X_train.iloc[[ROW_INDEX]]\nsingle_prediction = ag_wrapper.predict_binary_prob(single_datapoint)\n\nshap_values_single = explainer.shap_values(single_datapoint, nsamples=NSHAP_SAMPLES)\nshap.force_plot(explainer.expected_value, shap_values_single, X_display.iloc[ROW_INDEX,:])\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model Performance\nDESCRIPTION: Generates a leaderboard to compare the performance of different model configurations on the test dataset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-chronos.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npredictor.leaderboard(test_data)\n```\n\n----------------------------------------\n\nTITLE: Initial Anomaly Detection Analysis\nDESCRIPTION: Running initial anomaly detection with visualization parameters.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-anomaly-detection.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nauto.detect_anomalies(\n    train_data=x,\n    test_data=x_test,\n    label=target_col,\n    threshold_stds=threshold_stds,\n    show_top_n_anomalies=None,\n    fig_args={\n        'figsize': (6, 4)\n    },\n    chart_args={\n        'normal.color': 'lightgrey',\n        'anomaly.color': 'orange',\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Computing Matching Probabilities for Sentence Pairs\nDESCRIPTION: Calculates the matching probabilities for sentence pairs using the trained model.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/text2text_matching.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprobabilities = predictor.predict_proba(pred_data)\nprint(probabilities)\n```\n\n----------------------------------------\n\nTITLE: Training PDF Document Classifier with MultiModalPredictor\nDESCRIPTION: Creates and trains a PDF document classifier using MultiModalPredictor with LayoutLM as the document transformer. The training is limited to 120 seconds for demonstration purposes.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/document_prediction/pdf_classification.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\n\npredictor = MultiModalPredictor(label=\"label\")\npredictor.fit(\n    train_data=train_data,\n    hyperparameters={\"model.document_transformer.checkpoint_name\":\"microsoft/layoutlm-base-uncased\",\n    \"optim.top_k_average_method\":\"best\",\n    },\n    time_limit=120,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Error Handlers for Text Encoding in Normalization Process\nDESCRIPTION: Functions that define error handlers for codecs to properly handle encoding issues during text normalization. These handlers replace encoding errors with UTF-8 and decoding errors with CP1252 encoding.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/kaggle_feedback_prize/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n\ndef replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\nreturn error.object[error.start : error.end].decode(\"cp1252\"), error.end\n```\n\n----------------------------------------\n\nTITLE: Implementing Model Ensembling\nDESCRIPTION: Demonstrates how to use bagging and stacking ensemble methods to improve model performance.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-indepth.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nlabel = 'class'\ntest_data_nolabel = test_data.drop(columns=[label])\ny_test = test_data[label]\nsave_path = 'agModels-predictClass'\n\npredictor = TabularPredictor(label=label, eval_metric=metric).fit(train_data,\n    num_bag_folds=5, num_bag_sets=1, num_stack_levels=1,\n    hyperparameters = {'NN_TORCH': {'num_epochs': 2}, 'GBM': {'num_boost_round': 20}},\n)\n```\n\n----------------------------------------\n\nTITLE: Running AutoMM Distillation Example for GLUE/PAWS-X Tasks in Python\nDESCRIPTION: Command to run the distillation example script for either GLUE or PAWS-X tasks. It demonstrates how to set various parameters such as task name, teacher/student models, and distillation settings.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/distillation/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython automm_distillation_<task_name>.py --<flag> <value>\n```\n\n----------------------------------------\n\nTITLE: Creating SHAP Summary Plot\nDESCRIPTION: Generates summary plot showing feature importance based on SHAP values across multiple samples.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/interpret/SHAP with AutoGluon-Tabular Census income classification.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nshap.summary_plot(shap_values, X_valid.iloc[0:N_VAL,:])\n```\n\n----------------------------------------\n\nTITLE: Extracting Image Embeddings with MultiModalPredictor in Python\nDESCRIPTION: Uses the MultiModalPredictor to extract embeddings from image paths.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/zero_shot_img_txt_matching.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimage_embeddings = predictor.extract_embedding(image_paths, as_tensor=True)\nprint(image_embeddings.shape)\n```\n\n----------------------------------------\n\nTITLE: Predicting Image-Text Pair Matching Probabilities in Python\nDESCRIPTION: Uses the MultiModalPredictor to predict matching probabilities for an image-text pair.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/zero_shot_img_txt_matching.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nproba = predictor.predict_proba({\"abc\": [image_paths[4]], \"xyz\": [texts[3]]})\nprint(proba)\n```\n\n----------------------------------------\n\nTITLE: Loading and Splitting Document Dataset\nDESCRIPTION: Loads the dataset from CSV file and splits it into training and testing sets with an 80-20 ratio.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/document_prediction/document_classification.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndataset_path = os.path.join(download_dir, \"rvl_cdip_sample\")\nrvl_cdip_data = pd.read_csv(f\"{dataset_path}/rvl_cdip_train_data.csv\")\ntrain_data = rvl_cdip_data.sample(frac=0.8, random_state=200)\ntest_data = rvl_cdip_data.drop(train_data.index)\n```\n\n----------------------------------------\n\nTITLE: Visualizing NER Annotations\nDESCRIPTION: This snippet shows how to use the visualize_ner utility to display NER annotations for a given sentence.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/ner.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal.utils import visualize_ner\n\nsentence = \"Albert Einstein was born in Germany and is widely acknowledged to be one of the greatest physicists.\"\nannotation = [{\"entity_group\": \"PERSON\", \"start\": 0, \"end\": 15},\n              {\"entity_group\": \"LOCATION\", \"start\": 28, \"end\": 35}]\n\nvisualize_ner(sentence, annotation)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Predictions with MultilabelPredictor in Python\nDESCRIPTION: Evaluates the performance of predictions using the original test dataset with true labels. Evaluation metrics are printed to the console.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-multilabel.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nevaluations = multi_predictor.evaluate(test_data)\nprint(evaluations)\nprint(\"Evaluated using metrics:\", multi_predictor.eval_metrics)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Model Classes\nDESCRIPTION: Defines custom model classes that demonstrate feature preservation during preprocessing\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-model-advanced.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.core.models import AbstractModel\n\nclass DummyModel(AbstractModel):\n    def _fit(self, X, **kwargs):\n        print(f'Before {self.__class__.__name__} Preprocessing ({len(X.columns)} features):\\n\\t{list(X.columns)}')\n        X = self.preprocess(X)\n        print(f'After  {self.__class__.__name__} Preprocessing ({len(X.columns)} features):\\n\\t{list(X.columns)}')\n        print(X.head(5))\n\nclass DummyModelKeepUnique(DummyModel):\n    def _get_default_auxiliary_params(self) -> dict:\n        default_auxiliary_params = super()._get_default_auxiliary_params()\n        extra_auxiliary_params = dict(\n            drop_unique=False,\n        )\n        default_auxiliary_params.update(extra_auxiliary_params)\n        return default_auxiliary_params\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting PetFinder Dataset\nDESCRIPTION: Downloads a zip file containing the PetFinder dataset for the tutorial and extracts it to the specified directory using AutoGluon's utility function.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal-quick-start.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.core.utils.loaders import load_zip\n\ndownload_dir = './ag_multimodal_tutorial'\nzip_file = 'https://automl-mm-bench.s3.amazonaws.com/petfinder_for_tutorial.zip'\n\nload_zip.unzip(zip_file, unzip_dir=download_dir)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Interaction between Fare and SibSp\nDESCRIPTION: Visualizes the two-way interaction between 'Fare' and 'SibSp' features.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-analyze-interaction.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nauto.partial_dependence_plots(df_train, label='Survived', features=['Fare', 'SibSp'], two_way=True, show_help_text=False)\n```\n\n----------------------------------------\n\nTITLE: Performing Semantic Search for Image Retrieval in Python\nDESCRIPTION: Demonstrates image retrieval using semantic search with a text query.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/zero_shot_img_txt_matching.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal.utils import semantic_search\nhits = semantic_search(\n        matcher=predictor,\n        query_embeddings=text_embeddings[6][None,],\n        response_embeddings=image_embeddings,\n        top_k=5,\n    )\nprint(hits)\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing Census Data\nDESCRIPTION: Imports required libraries, loads census data from S3, and prepares training/test datasets for occupation prediction task.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-indepth.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.tabular import TabularDataset, TabularPredictor\n\nimport numpy as np\n\ntrain_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv')\nsubsample_size = 1000  # subsample subset of data for faster demo, try setting this to much larger values\ntrain_data = train_data.sample(n=subsample_size, random_state=0)\nprint(train_data.head())\n\nlabel = 'occupation'\nprint(\"Summary of occupation column: \\n\", train_data['occupation'].describe())\n\ntest_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv')\ny_test = test_data[label]\ntest_data_nolabel = test_data.drop(columns=[label])  # delete label column\n\nmetric = 'accuracy' # we specify eval-metric just for demo (unnecessary as it's the default)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model Performance\nDESCRIPTION: Generates a leaderboard to compare the performance of zero-shot and fine-tuned Chronos models on the test data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-chronos.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npredictor.leaderboard(test_data)\n```\n\n----------------------------------------\n\nTITLE: Converting VOC Format to COCO Format using AutoGluon CLI\nDESCRIPTION: Python commands to convert a VOC format dataset to COCO format using AutoGluon's CLI utility, with options for custom train/val/test splits or using the dataset-provided splits.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/convert_data_to_coco_format.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# If you'd like to customize train/val/test ratio. Note test_ratio = 1 - train_ratio - val_ratio.\npython3 -m autogluon.multimodal.cli.voc2coco --root_dir <root_dir> --train_ratio <train_ratio> --val_ratio <val_ratio>  \n# If you'd like to use the dataset provided train/val/test splits:\npython3 -m autogluon.multimodal.cli.voc2coco --root_dir <root_dir>\n```\n\n----------------------------------------\n\nTITLE: Performing Semantic Search\nDESCRIPTION: Implements semantic search functionality using the predictor to find similar documents based on text embeddings and cosine similarity.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/text_semantic_search.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal.utils import semantic_search\nhits = semantic_search(\n        matcher=predictor,\n        query_data=query_data[text_col].tolist(),\n        response_data=doc_data[text_col].tolist(),\n        query_chunk_size=len(query_data),\n        top_k=max(cutoffs),\n    )\n```\n\n----------------------------------------\n\nTITLE: Initializing TimeSeriesPredictor with Covariates\nDESCRIPTION: Configures TimeSeriesPredictor with target and known covariates for 14-day predictions.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-indepth.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npredictor = TimeSeriesPredictor(\n    prediction_length=14,\n    target=\"target\",\n    known_covariates_names=[\"weekend\"],\n).fit(train_data)\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Tabular with Specific ML Libraries\nDESCRIPTION: Example of installing the tabular submodule with only specific machine learning libraries (LightGBM and CatBoost) as optional dependencies.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/install-modules.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install autogluon.tabular[lightgbm,catboost]\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model Performance with Leaderboard\nDESCRIPTION: Displays the leaderboard of trained models, showing their performance on both validation and test datasets.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-quick-start.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# The test score is computed using the last\n# prediction_length=48 timesteps of each time series in test_data\npredictor.leaderboard(test_data)\n```\n\n----------------------------------------\n\nTITLE: Creating MultiModalPredictor Instance\nDESCRIPTION: Initializes a MultiModalPredictor with specified hyperparameters for object detection task using the Pothole dataset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/advanced/finetune_coco.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npredictor = MultiModalPredictor(\n    hyperparameters={\n        \"model.mmdet_image.checkpoint_name\": checkpoint_name,\n        \"env.num_gpus\": num_gpus,\n    },\n    problem_type=\"object_detection\",\n    sample_data_path=train_path,\n)\n```\n\n----------------------------------------\n\nTITLE: Disabling Preprocessing in AutoGluons TabularPredictor\nDESCRIPTION: By setting feature_generator to None, preprocessing in AutoGluons TabularPredictor is disabled. It's ideal when external preprocessing is preferred, requiring understanding of data handling and feature generation.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-indepth.ipynb#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\npredictor.fit(..., feature_generator=None, feature_metadata=YOUR_CUSTOM_FEATURE_METADATA)\n```\n\n----------------------------------------\n\nTITLE: Training a Regression Model with AutoGluon\nDESCRIPTION: Creates a TabularPredictor for predicting a numeric 'age' variable, specifying a path for model storage and a 60-second time limit for training.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-essentials.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\npredictor_age = TabularPredictor(label=age_column, path=\"agModels-predictAge\").fit(train_data, time_limit=60)\n```\n\n----------------------------------------\n\nTITLE: Importing AutoGluon in Kaggle Environment in Python\nDESCRIPTION: This code shows how to import AutoGluon from a Kaggle dataset for use in a competition where network access is restricted. It adds the AutoGluon package to the Python path.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/kaggle_pawpularity/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport sys\nsys.path.append(\"../input/autogluon/\")\n```\n\n----------------------------------------\n\nTITLE: Evaluating AutoGluon MultiModalPredictor Performance\nDESCRIPTION: Evaluates the predictor's performance on the test dataset using the ROC AUC metric, which measures the model's ability to discriminate between classes.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal-quick-start.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nscores = predictor.evaluate(test_data, metrics=[\"roc_auc\"])\nscores\n```\n\n----------------------------------------\n\nTITLE: Displaying Sample Image and Description from Dataset\nDESCRIPTION: Retrieves and displays an example image and its corresponding description from the dataset to showcase the multimodal nature of the data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal-quick-start.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nexample_row = train_data.iloc[0]\nexample_image = example_row[image_col]\n\nfrom IPython.display import Image, display\npil_img = Image(filename=example_image)\ndisplay(pil_img)\n\nexample_row['Description']\n```\n\n----------------------------------------\n\nTITLE: Configuring Learning Rate Decay in AutoMM\nDESCRIPTION: Examples showing how to set learning rate decay in AutoMM, allowing different layers to have varying learning rates, with a default value of 0.9.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"optim.lr_decay\": 0.9})\n# turn off learning rate decay\npredictor.fit(hyperparameters={\"optim.lr_decay\": 1})\n```\n\n----------------------------------------\n\nTITLE: Implementing Knowledge Distillation in AutoMM\nDESCRIPTION: Example showing how to use knowledge distillation in AutoMMPredictor by training a student model using a pre-trained teacher model. The process reuses the .fit() function with additional parameters for distillation.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/whats_new/v0.4.1.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.text.automm import AutoMMPredictor\nteacher_predictor = AutoMMPredictor(label=\"label_column\").fit(train_data)\nstudent_predictor = AutoMMPredictor(label=\"label_column\").fit(\n    train_data,\n    hyperparameters=student_and_distiller_hparams,\n    teacher_predictor=teacher_predictor,\n)\n```\n\n----------------------------------------\n\nTITLE: Handling Irregular Time Series Data\nDESCRIPTION: Example of creating and handling irregular time series data with missing values.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-indepth.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndf_irregular = TimeSeriesDataFrame(\n    pd.DataFrame(\n        {\n            \"item_id\": [0, 0, 0, 1, 1],\n            \"timestamp\": [\"2022-01-01\", \"2022-01-02\", \"2022-01-04\", \"2022-01-01\", \"2022-01-04\"],\n            \"target\": [1, 2, 3, 4, 5],\n        }\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Regular Model Training with MultiModal Predictor\nDESCRIPTION: Demonstrates basic model training using AutoGluon's MultiModal Predictor with a specific image backbone model\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/hyperparameter_optimization.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\npredictor_regular = MultiModalPredictor(label=\"label\")\nstart_time = datetime.now()\npredictor_regular.fit(\n    train_data=train_data,\n    hyperparameters = {\"model.timm_image.checkpoint_name\": \"ghostnet_100\"}\n)\nend_time = datetime.now()\nelapsed_seconds = (end_time - start_time).total_seconds()\nelapsed_min = divmod(elapsed_seconds, 60)\nprint(\"Total fitting time: \", f\"{int(elapsed_min[0])}m{int(elapsed_min[1])}s\")\n```\n\n----------------------------------------\n\nTITLE: Loading Saved Model\nDESCRIPTION: Demonstrates how to load a previously saved model and evaluate it.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_segmentation/beginner_semantic_seg.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nloaded_predictor = MultiModalPredictor.load(save_path)\nscores = loaded_predictor.evaluate(test_data, metrics=[\"iou\"])\nprint(scores)\n```\n\n----------------------------------------\n\nTITLE: Making Predictions\nDESCRIPTION: Loads test data and generates predictions using the trained predictor\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-quick-start.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntest_data = TabularDataset(f'{data_url}test.csv')\n\ny_pred = predictor.predict(test_data.drop(columns=[label]))\ny_pred.head()\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch with CUDA Support for AutoGluon\nDESCRIPTION: Command to install PyTorch with CUDA 12.1 support using pip and the PyTorch package index.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/install-windows-gpu.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install torchvision==0.19.1 --force-reinstall --extra-index-url https://download.pytorch.org/whl/cu121\n```\n\n----------------------------------------\n\nTITLE: Creating Different Types of Search Spaces in AutoGluon\nDESCRIPTION: This snippet demonstrates how to create different types of search spaces in AutoGluon for hyperparameter optimization. It shows how to define categorical, real (continuous), integer, and boolean search spaces with their respective value ranges.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/api/autogluon.common.space.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.common import space\n\ncategorical_space = space.Categorical('a', 'b', 'c', 'd')  # Nested search space for hyperparameters which are categorical.\nreal_space = space.Real(0.01, 0.1)  # Search space for numeric hyperparameter that takes continuous values\nint_space = space.Int(0, 100)  # Search space for numeric hyperparameter that takes integer values\nbool_space = space.Bool()  # Search space for hyperparameter that is either True or False.\n```\n\n----------------------------------------\n\nTITLE: Path Expansion Function\nDESCRIPTION: Expands relative paths to absolute paths for image and label files.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_segmentation/beginner_semantic_seg.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef path_expander(path, base_folder):\n    path_l = path.split(';')\n    return ';'.join([os.path.abspath(os.path.join(base_folder, path)) for path in path_l])\n\nfor per_col in [image_col, label_col]:\n    train_data[per_col] = train_data[per_col].apply(lambda ele: path_expander(ele, base_folder=dataset_path))\n    val_data[per_col] = val_data[per_col].apply(lambda ele: path_expander(ele, base_folder=dataset_path))\n    test_data[per_col] = test_data[per_col].apply(lambda ele: path_expander(ele, base_folder=dataset_path))\n    \nprint(train_data[image_col].iloc[0])\nprint(train_data[label_col].iloc[0])\n```\n\n----------------------------------------\n\nTITLE: Creating SHAP KernelExplainer for AutoGluon Predictions in Python\nDESCRIPTION: This code creates a SHAP KernelExplainer for the AutoGluon predictor and sets up parameters for SHAP analysis.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/interpret/SHAP with AutoGluon-Tabular Diabetes regression.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nag_wrapper = AutogluonWrapper(predictor, feature_names)\nprint_accuracy(ag_wrapper.predict)\n\nexplainer = shap.KernelExplainer(ag_wrapper.predict, X_train_summary)\n\nNSHAP_SAMPLES = 100  # how many samples to use to approximate each Shapely value, larger values will be slower\nN_VAL = 30  # how many datapoints from validation data should we interpret predictions for, larger values will be slower\n```\n\n----------------------------------------\n\nTITLE: Predicting Similarity for New Sentence Pair\nDESCRIPTION: Creates a new sentence pair and uses the trained model to predict their similarity.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/text2text_matching.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npred_data = pd.DataFrame.from_dict({\"premise\":[\"The teacher gave his speech to an empty room.\"], \n                                    \"hypothesis\":[\"There was almost nobody when the professor was talking.\"]})\n\npredictions = predictor.predict(pred_data)\nprint('Predicted entities:', predictions[0])\n```\n\n----------------------------------------\n\nTITLE: Configuring AutoGluon Model Presets for Lambda Deployment\nDESCRIPTION: Defines AutoGluon presets optimized for deployment to minimize model size and improve Lambda cold start performance.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/cloud_fit_deploy/cloud-aws-lambda-deployment.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npresets = ['good_quality_faster_inference_only_refit', 'optimize_for_deployment']\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon via conda-forge\nDESCRIPTION: New installation method available through conda-forge repository for Python versions 3.8, 3.9, and 3.10\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/whats_new/v0.7.0.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c conda-forge autogluon\n```\n\n----------------------------------------\n\nTITLE: Cloning AutoGluon TabularPredictor in Python\nDESCRIPTION: Creates a snapshot of the given predictor by cloning its artifacts to a new location. This allows for reverting to a prior state if needed.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-deployment.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsave_path_clone = save_path + '-clone'\n# will return the path to the cloned predictor, identical to save_path_clone\npath_clone = predictor.clone(path=save_path_clone)\n```\n\n----------------------------------------\n\nTITLE: Predicting with MultilabelPredictor in Python\nDESCRIPTION: Demonstrates how to load a saved MultilabelPredictor and use it to predict on new data. The predictions are printed to the console.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-multilabel.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nmulti_predictor = MultilabelPredictor.load(save_path)  # unnecessary, just demonstrates how to load previously-trained multilabel predictor from file\n\npredictions = multi_predictor.predict(test_data_nolab)\nprint(\"Predictions:  \\n\", predictions)\n```\n\n----------------------------------------\n\nTITLE: Loading Time Series Data and Static Features in Python\nDESCRIPTION: This code loads a subset of the M4 Daily dataset and its corresponding static features. It demonstrates how to read CSV files into pandas DataFrames for both time series data and static features.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-indepth.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv(\"https://autogluon.s3.amazonaws.com/datasets/timeseries/m4_daily_subset/train.csv\")\ndf.head()\n\nstatic_features_df = pd.read_csv(\"https://autogluon.s3.amazonaws.com/datasets/timeseries/m4_daily_subset/metadata.csv\")\nstatic_features_df.head()\n```\n\n----------------------------------------\n\nTITLE: Creating JSON-formatted NER Annotations\nDESCRIPTION: This code demonstrates how to create JSON-formatted annotations for Named Entity Recognition using the required format for AutoMM.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/ner.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\njson.dumps([\n    {\"entity_group\": \"PERSON\", \"start\": 0, \"end\": 15},\n    {\"entity_group\": \"LOCATION\", \"start\": 28, \"end\": 35}\n])\n```\n\n----------------------------------------\n\nTITLE: Visualizing Test Document\nDESCRIPTION: Displays a document from the test set to visually examine before making predictions on it.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/document_prediction/document_classification.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndoc_path = test_data.iloc[1][DOC_PATH_COL]\nfrom IPython.display import Image, display\npil_img = Image(filename=doc_path, width=500)\ndisplay(pil_img)\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Feature Pipeline with CategoryFeatureGenerator in AutoGluon\nDESCRIPTION: This code snippet demonstrates how to create a custom feature generation pipeline in AutoGluon. It configures a PipelineFeatureGenerator with a CategoryFeatureGenerator that limits categorical values to 10 and an IdentityFeatureGenerator that handles integer and float features.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-feature-engineering.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.features.generators import PipelineFeatureGenerator, CategoryFeatureGenerator, IdentityFeatureGenerator\nfrom autogluon.common.features.types import R_INT, R_FLOAT\nmypipeline = PipelineFeatureGenerator(\n    generators = [[        \n        CategoryFeatureGenerator(maximum_num_cat=10),  # Overridden from default.\n        IdentityFeatureGenerator(infer_features_in_args=dict(valid_raw_types=[R_INT, R_FLOAT])),\n    ]]\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom MSE Function\nDESCRIPTION: Demonstrates how to implement a custom mean squared error function from scratch using numpy arrays.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-metric.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef mse_func(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    return ((y_true - y_pred) ** 2).mean()\n\nmse_func(y_true, y_pred)\n```\n\n----------------------------------------\n\nTITLE: Training Multilingual Model\nDESCRIPTION: Trains a multilingual model using English data with cross-lingual transfer capabilities.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/multilingual_text.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\n\npredictor = MultiModalPredictor(label='label')\npredictor.fit(train_en_df,\n              presets='multilingual',\n              hyperparameters={\n                  'optim.max_epochs': 2\n              })\n```\n\n----------------------------------------\n\nTITLE: Evaluating PDF Classifier on Test Dataset\nDESCRIPTION: Evaluates the trained classifier's performance on the test dataset using accuracy as the metric.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/document_prediction/pdf_classification.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nscores = predictor.evaluate(test_data, metrics=[\"accuracy\"])\nprint('The test acc: %.3f' % scores[\"accuracy\"])\n```\n\n----------------------------------------\n\nTITLE: Evaluating Student Model Performance\nDESCRIPTION: Evaluates the performance of the trained student model on the test dataset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/model_distillation.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprint(student_predictor.evaluate(data=test_df))\n```\n\n----------------------------------------\n\nTITLE: Optimizing for TensorRT Inference\nDESCRIPTION: Loads and optimizes the model for TensorRT inference using optimize_for_inference().\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/tensorrt.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmodel_path = predictor.path\ntrt_predictor = MultiModalPredictor.load(path=model_path)\ntrt_predictor.optimize_for_inference()\n\n# Again, use first prediction for initialization (e.g., allocating memory)\ny_pred_trt = trt_predictor.predict_proba(sample)\n\nclear_output()\n```\n\n----------------------------------------\n\nTITLE: Model Finetuning\nDESCRIPTION: Finetunes the predictor on the training data with validation data for 180 seconds\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/image_text_matching.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npredictor.fit(\n            train_data=train_data,\n            tuning_data=val_data,\n            time_limit=180,\n        )\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Tabular Dependencies\nDESCRIPTION: Installs the AutoGluon tabular package with all optional dependencies\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-essentials.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.tabular[all]\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with Specific Models in Python\nDESCRIPTION: This snippet shows how to make predictions using a specific model from the trained ensemble, access model information, and evaluate predictions.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-indepth.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ni = 0  # index of model to use\nmodel_to_use = predictor.model_names()[i]\nmodel_pred = predictor.predict(datapoint, model=model_to_use)\nprint(\"Prediction from %s model: %s\" % (model_to_use, model_pred.iloc[0]))\n\nall_models = predictor.model_names()\nmodel_to_use = all_models[i]\nspecific_model = predictor._trainer.load_model(model_to_use)\n\nmodel_info = specific_model.get_info()\npredictor_information = predictor.info()\n\ny_pred_proba = predictor.predict_proba(test_data_nolabel)\nperf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred_proba)\n\nperf = predictor.evaluate(test_data)\n```\n\n----------------------------------------\n\nTITLE: Classifying the Segway Image with CLIP Zero-Shot Classification\nDESCRIPTION: Uses the MultiModalPredictor to classify the Segway image among different transportation-related options, demonstrating CLIP's effectiveness on uncommon object classes.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_prediction/clip_zeroshot.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprob = predictor.predict_proba({\"image\": [segway_image]}, {\"text\": ['segway', 'bicycle', 'wheel', 'car']})\nprint(\"Label probs:\", prob)\n```\n\n----------------------------------------\n\nTITLE: Preparing Data for AutoGluon Training in Python\nDESCRIPTION: This code prepares the training data for AutoGluon by creating a pandas DataFrame with features and labels.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/interpret/SHAP with AutoGluon-Tabular Diabetes regression.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nlabel = 'label'\nfeature_names = X_train.columns\ntrain_data = X_train.copy()\ntrain_data[label] = y_train\nval_data = X_valid.copy()\n\ndisplay(train_data.head())\n```\n\n----------------------------------------\n\nTITLE: Making Predictions on New PDF Documents\nDESCRIPTION: Uses the trained model to predict the label of a new PDF document from the test dataset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/document_prediction/pdf_classification.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npredictions = predictor.predict({DOC_PATH_COL: [test_data.iloc[0][DOC_PATH_COL]]})\nprint(f\"Ground-truth label: {test_data.iloc[0]['label']}, Prediction: {predictions}\")\n```\n\n----------------------------------------\n\nTITLE: Setting Time Limit for TimeSeriesPredictor Training in Python\nDESCRIPTION: This code demonstrates how to set a time limit for training the TimeSeriesPredictor. The time_limit argument is set to 1 hour (3600 seconds), after which training will stop even if not all models have been fit.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-indepth.ipynb#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\npredictor.fit(\n    train_data,\n    time_limit=60 * 60,  # total training time in seconds\n)\n```\n\n----------------------------------------\n\nTITLE: Obtaining Prediction Probabilities for PDF Documents\nDESCRIPTION: Gets the probability distribution across all possible labels for a PDF document from the test dataset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/document_prediction/pdf_classification.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nproba = predictor.predict_proba({DOC_PATH_COL: [test_data.iloc[0][DOC_PATH_COL]]})\nprint(proba)\n```\n\n----------------------------------------\n\nTITLE: Loading a Pretrained MultiModalPredictor in Python\nDESCRIPTION: This code shows how to load a previously saved standalone MultiModalPredictor model in Kaggle for making predictions, avoiding the need to download models during submission.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/kaggle_pawpularity/README.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npretrained_model = predictor.load(path=save_standalone_path)\n```\n\n----------------------------------------\n\nTITLE: Configuring Faster Presets in AutoGluon\nDESCRIPTION: Adjusts presets for training a lighter model in AutoGluon, specified by the user to avoid producing models with cumbersome inference latency or memory usage.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-indepth.ipynb#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\npresets = ['good_quality', 'optimize_for_deployment']\npredictor_light = TabularPredictor(label=label, eval_metric=metric).fit(train_data, presets=presets, time_limit=30)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Hybrid BM25 Performance\nDESCRIPTION: Executes the hybrid BM25 search and evaluates its performance using specified metrics and cutoff values.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/text_semantic_search.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nrecall_num = 1000\nbeta = 0.3\nquery_embeds = predictor.extract_embedding(query_data[[query_id_col]], id_mappings=id_mappings, as_tensor=True)\ndoc_embeds = predictor.extract_embedding(doc_data[[doc_id_col]], id_mappings=id_mappings, as_tensor=True)\nevaluate_hybridBM25(query_data, query_embeds, doc_data, doc_embeds, recall_num, beta, cutoffs)\n```\n\n----------------------------------------\n\nTITLE: Scoring a Trained Custom Model in AutoGluon\nDESCRIPTION: This snippet demonstrates how to score the trained custom model using the default evaluation metric (accuracy for binary classification).\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-model.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nscore = custom_model.score(X_test_clean, y_test_clean)\nprint(f'Test score ({custom_model.eval_metric.name}) = {score}')\n```\n\n----------------------------------------\n\nTITLE: Removing problematic features and re-analyzing covariate shift\nDESCRIPTION: Drops the PassengerId column which was identified as having a different distribution between train and test sets, then reruns the covariate shift detection to see if other issues remain.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-covariate-shift.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf_train = df_train.drop(columns='PassengerId')\ndf_test = df_test.drop(columns='PassengerId')\nauto.covariate_shift_detection(train_data=df_train, test_data=df_test, label=target_col)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Missing Values\nDESCRIPTION: Handling missing values in Age and Fare columns by filling them with mean values.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-anomaly-detection.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nx = df_train\nx_test = df_test\nx.Age.fillna(x.Age.mean(), inplace=True)\nx_test.Age.fillna(x.Age.mean(), inplace=True)\nx_test.Fare.fillna(x.Fare.mean(), inplace=True)\n```\n\n----------------------------------------\n\nTITLE: Defining COCO Format Data Structure in Python\nDESCRIPTION: This snippet demonstrates the structure of a COCO format dataset in Python. It includes definitions for categories, images, and annotations, which are essential components of the COCO format.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/object_detection_with_dataframe.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndata = {\n    \"categories\": [\n        {\"supercategory\": \"none\", \"id\": 1, \"name\": \"person\"},\n        {\"supercategory\": \"none\", \"id\": 2, \"name\": \"bicycle\"},\n        {\"supercategory\": \"none\", \"id\": 3, \"name\": \"car\"},\n        {\"supercategory\": \"none\", \"id\": 4, \"name\": \"motorcycle\"},\n        # ...\n    ],\n\n    \"images\": [\n        {\n            \"file_name\": \"<imagename0>.<ext>\",\n            \"height\": 427,\n            \"width\": 640,\n            \"id\": 1\n        },\n        {\n            \"file_name\": \"<imagename2>.<ext>\",\n            \"height\": 427,\n            \"width\": 640,\n            \"id\": 2\n        },\n        # ...\n    ],\n    \"annotations\": [\n        {\n            'area': 33453,\n            'iscrowd': 0,\n            'bbox': [181, 133, 177, 189],\n            'category_id': 8,\n            'ignore': 0,\n            'segmentation': [],\n            'image_id': 1617,\n            'id': 1\n        },\n        {\n            'area': 25740, \n            'iscrowd': 0,\n            'bbox': [192, 100, 156, 165],\n            'category_id': 9,\n            'ignore': 0,\n            'segmentation': [],\n            'image_id': 1617,\n            'id': 2\n        },\n        # ...\n    ],\n    \n    \"type\": \"instances\"\n}\n```\n\n----------------------------------------\n\nTITLE: Using Experimental Zeroshot HPO Config\nDESCRIPTION: Example of using the experimental Zeroshot HPO configuration in AutoGluon Tabular, which performs well on small datasets with less than 10,000 rows when given at least an hour of training time.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/whats_new/v0.8.0.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npredictor.fit(presets=\"experimental_zeroshot_hpo_hybrid\")\n```\n\n----------------------------------------\n\nTITLE: Specifying Positive Label for Binary Classification in AutoMM Python\nDESCRIPTION: Defines the positive class label for binary classification tasks. Setting `data.pos_label` is necessary for correctly calculating metrics like ROC AUC, average precision, and F1-score. The default is `None`. Provide the actual positive label string (e.g., `\"changed\"`) in the `hyperparameters` for `predictor.fit()`.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_64\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"data.pos_label\": None})\n# assume the labels are [\"changed\", \"not changed\"] and \"changed\" is the positive label\npredictor.fit(hyperparameters={\"data.pos_label\": \"changed\"})\n```\n\n----------------------------------------\n\nTITLE: Verifying GPU Detection in PyTorch for AutoGluon\nDESCRIPTION: Python code to verify that PyTorch can detect and access the GPU. It checks if CUDA is available and counts the number of available GPU devices.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/install-windows-gpu.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nprint(torch.cuda.is_available())  # Should be True\nprint(torch.cuda.device_count())  # Should be > 0\n```\n\n----------------------------------------\n\nTITLE: Evaluating AutoGluon TabularPredictor Models in Python\nDESCRIPTION: Uses the leaderboard function to evaluate the performance of each individual trained model on labeled test data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-deployment.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npredictor.leaderboard(test_data)\n```\n\n----------------------------------------\n\nTITLE: Extracting Image Embeddings\nDESCRIPTION: Extracts feature embeddings from an image using the trained model.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_prediction/beginner_image_cls.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfeature = predictor.extract_embedding({'image': [image_path]})\nprint(feature[0].shape)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Best Quality Model\nDESCRIPTION: Evaluates the best quality model on the test data using ROC AUC metric.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/presets.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nscores = predictor.evaluate(test_data, metrics=[\"roc_auc\"])\nscores\n```\n\n----------------------------------------\n\nTITLE: Training AutoGluon Classifier\nDESCRIPTION: Initializes and trains an AutoGluon binary classifier with a 20-second time limit.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/interpret/SHAP with AutoGluon-Tabular Census income classification.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npredictor = TabularPredictor(label=label, problem_type='binary').fit(train_data, time_limit=20)\n```\n\n----------------------------------------\n\nTITLE: Reloading and Continuing Training of NER Model\nDESCRIPTION: This snippet demonstrates how to reload a saved NER model and continue training it with new data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/ner.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nnew_predictor = MultiModalPredictor.load(model_path)\nnew_model_path = f\"./tmp/{uuid.uuid4().hex}-automm_ner_continue_train\"\nnew_predictor.fit(train_data, time_limit=60, save_path=new_model_path)\ntest_score = new_predictor.evaluate(test_data, metrics=['overall_f1', 'ACTOR'])\nprint(test_score)\n```\n\n----------------------------------------\n\nTITLE: Selecting SAM Model in AutoMM\nDESCRIPTION: Chooses a Segment Anything Model (SAM) backbone from Hugging Face using the 'model.sam.checkpoint_name' hyperparameter.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_52\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"model.sam.checkpoint_name\": \"facebook/sam-vit-huge\"})\n# choose SAM-Large\npredictor.fit(hyperparameters={\"model.sam.checkpoint_name\": \"facebook/sam-vit-large\"})\n# choose SAM-Base\npredictor.fit(hyperparameters={\"model.sam.checkpoint_name\": \"facebook/sam-vit-base\"})\n```\n\n----------------------------------------\n\nTITLE: Modifying Static Features in TimeSeriesDataFrame in Python\nDESCRIPTION: This snippet shows how to add a new static feature to an existing TimeSeriesDataFrame and change its data type to categorical. It demonstrates handling both continuous and categorical static features.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-indepth.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntrain_data.static_features[\"store_id\"] = list(range(len(train_data.item_ids)))\ntrain_data.static_features[\"store_id\"] = train_data.static_features[\"store_id\"].astype(\"category\")\n```\n\n----------------------------------------\n\nTITLE: Saving AutoGluon Multimodal Predictor for Offline Deployment\nDESCRIPTION: Code snippet showing how to save an AutoGluon Multimodal predictor in a standalone format that can be loaded without internet access. This is useful for deployment in environments without internet connectivity.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal-faq.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npredictor.save(SAVE_PATH, standalone=True)\n```\n\n----------------------------------------\n\nTITLE: Creating ROC AUC Scorer\nDESCRIPTION: Shows how to create a custom ROC AUC scorer for thresholding metrics that depend on prediction probabilities.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-metric.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nag_roc_auc_scorer = make_scorer(name='roc_auc',\n                                score_func=sklearn.metrics.roc_auc_score,\n                                optimum=1,\n                                greater_is_better=True,\n                                needs_threshold=True)\nag_roc_auc_scorer(y_true, y_pred_proba)\n```\n\n----------------------------------------\n\nTITLE: Comparing Disk Usage of Original and Optimized AutoGluon TabularPredictor in Python\nDESCRIPTION: Compares the disk usage of the original predictor and the optimized clone, showing the reduction in size achieved through optimization.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-deployment.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nsize_original = predictor.disk_usage()\nsize_opt = predictor_clone_opt.disk_usage()\nprint(f'Size Original:  {size_original} bytes')\nprint(f'Size Optimized: {size_opt} bytes')\nprint(f'Optimized predictor achieved a {round((1 - (size_opt/size_original)) * 100, 1)}% reduction in disk usage.')\n\npredictor.disk_usage_per_file()\n\npredictor_clone_opt.disk_usage_per_file()\n```\n\n----------------------------------------\n\nTITLE: Loading Time Series Data in AutoGluon\nDESCRIPTION: Loads the Grocery Sales dataset from S3 including both time series data and static features for forecasting tasks.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/advanced/forecasting-custom-model.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.timeseries import TimeSeriesDataFrame\n\nraw_data = TimeSeriesDataFrame.from_path(\n    \"https://autogluon.s3.amazonaws.com/datasets/timeseries/grocery_sales/test.csv\",\n    static_features_path=\"https://autogluon.s3.amazonaws.com/datasets/timeseries/grocery_sales/static.csv\",\n)\nraw_data.head()\n```\n\n----------------------------------------\n\nTITLE: Loading Test Data for AutoGluon TabularPredictor in Python\nDESCRIPTION: Loads separate test data to demonstrate making predictions on new examples at inference time.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-deployment.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntest_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv')\ny_test = test_data[label]  # values to predict\ntest_data.head()\n```\n\n----------------------------------------\n\nTITLE: Viewing Training Summary\nDESCRIPTION: Shows how to view a summary of the training process including hyperparameter tuning details.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-indepth.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nresults = predictor.fit_summary()\n```\n\n----------------------------------------\n\nTITLE: Setting Learning Rate Multiplier in AutoMM\nDESCRIPTION: Examples demonstrating how to use learning rate multiplier for two-stage lr choice, where the head layer has a learning rate of optim.lr * optim.lr_mult.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"optim.lr_mult\": 1})\n# turn on two-stage lr for 10 times learning rate in head layer\npredictor.fit(hyperparameters={\"optim.lr_mult\": 10})\n```\n\n----------------------------------------\n\nTITLE: Generating SHAP Dependence Plot for BMI Feature in Python\nDESCRIPTION: This code creates a dependence plot to visualize how the BMI feature influences predicted outcomes in the AutoGluon model.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/interpret/SHAP with AutoGluon-Tabular Diabetes regression.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nshap.dependence_plot(\"bmi\", shap_values, X_valid.iloc[0:N_VAL,:])\n```\n\n----------------------------------------\n\nTITLE: Saving a Standalone MultiModalPredictor Model\nDESCRIPTION: Code for saving an AutoGluon MultiModalPredictor as a standalone model for offline deployment. This is particularly useful for Kaggle competitions where internet access is restricted during inference.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/kaggle_feedback_prize/README.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npredictor.save(path=save_standalone_path, standalone=True)\n```\n\n----------------------------------------\n\nTITLE: Loading Sentence Similarity Dataset with AutoGluon\nDESCRIPTION: This snippet loads the training and test data for the Semantic Textual Similarity Benchmark dataset using AutoGluon's load_pd function.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/beginner_text.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsts_train_data = load_pd.load('https://autogluon-text.s3-accelerate.amazonaws.com/glue/sts/train.parquet')[['sentence1', 'sentence2', 'score']]\nsts_test_data = load_pd.load('https://autogluon-text.s3-accelerate.amazonaws.com/glue/sts/dev.parquet')[['sentence1', 'sentence2', 'score']]\nsts_train_data.head(10)\n```\n\n----------------------------------------\n\nTITLE: Displaying Tweet Image using IPython in Python\nDESCRIPTION: This code displays the image associated with an example tweet using IPython's display functionality.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal_ner.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nexample_image = example_row[image_col]\n\nfrom IPython.display import Image, display\npil_img = Image(filename=example_image, width =300)\ndisplay(pil_img)\n```\n\n----------------------------------------\n\nTITLE: Explaining Model Predictions with SHAP\nDESCRIPTION: Analyzes and visualizes SHAP values for predictions with highest error using force layout.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-quick-fit.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nauto.explain_rows(\n    train_data=df_train,\n    model=state.model,\n    display_rows=True,\n    rows=state.model_evaluation.highest_error[:1]\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating NER Model Performance\nDESCRIPTION: Evaluates the trained model's performance on the development dataset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/chinese_ner.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npredictor.evaluate(dev_data)\n```\n\n----------------------------------------\n\nTITLE: Training MultiModalPredictor without Focal Loss\nDESCRIPTION: Creates and trains a MultiModalPredictor using default cross-entropy loss for comparison with the focal loss model.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/focal_loss.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport uuid\nfrom autogluon.multimodal import MultiModalPredictor\n\nmodel_path = f\"./tmp/{uuid.uuid4().hex}-automm_shopee_non_focal\"\n\npredictor2 = MultiModalPredictor(label=\"label\", problem_type=\"multiclass\", path=model_path)\n\npredictor2.fit(\n    hyperparameters={\n        \"model.mmdet_image.checkpoint_name\": \"swin_tiny_patch4_window7_224\",\n        \"env.num_gpus\": 1,\n        \"optim.max_epochs\": 10,\n    },\n    train_data=imbalanced_train_data,\n)\n\npredictor2.evaluate(test_data, metrics=[\"acc\"])\n```\n\n----------------------------------------\n\nTITLE: Training FLAN-T5-XL with Gradient Checkpointing\nDESCRIPTION: Shows how to train the large FLAN-T5-XL model on a single GPU using gradient checkpointing and parameter-efficient finetuning.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/efficient_finetuning_basic.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\n\ntrain_en_df_downsample = train_en_df.sample(200, random_state=123)\n\nnew_model_path = f\"./tmp/{uuid.uuid4().hex}-multilingual_ia3_gradient_checkpoint\"\npredictor = MultiModalPredictor(label=\"label\",\n                                path=new_model_path)\npredictor.fit(train_en_df_downsample,\n              presets=\"multilingual\",\n              hyperparameters={\n                  \"model.hf_text.checkpoint_name\": \"google/flan-t5-xl\",\n                  \"model.hf_text.gradient_checkpointing\": True,\n                  \"model.hf_text.low_cpu_mem_usage\": True,\n                  \"optim.peft\": \"ia3_bias\",\n                  \"optim.lr_decay\": 0.9,\n                  \"optim.lr\": 3e-03,\n                  \"optim.end_lr\": 3e-03,\n                  \"optim.max_epochs\": 1,\n                  \"optim.warmup_steps\": 0,\n                  \"env.batch_size\": 1,\n                  \"env.inference_batch_size_ratio\": 1\n              })\n```\n\n----------------------------------------\n\nTITLE: Setting Hardware Accelerator\nDESCRIPTION: Configuration for choosing between CPU, GPU, or automatic hardware acceleration selection.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"env.accelerator\": \"auto\"})\n# use cpu for training\npredictor.fit(hyperparameters={\"env.accelerator\": \"cpu\"})\n```\n\n----------------------------------------\n\nTITLE: Setting Early Stopping Patience in AutoMM\nDESCRIPTION: Examples showing how to configure early stopping patience in AutoMM, which determines how many validation checks without improvement before stopping training.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"optim.patience\": 10})\n# set patience to 5 checks\npredictor.fit(hyperparameters={\"optim.patience\": 5})\n```\n\n----------------------------------------\n\nTITLE: Limiting CPU Cores in AutoGluon TabularPredictor\nDESCRIPTION: Sets the number of CPU cores to use when fitting a TabularPredictor model. This allows controlling computational resource usage at the global level.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-faq.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npredictor = TabularPredictor(...).fit(..., num_cpus = NUM_CORES_YOU_WANT)\n```\n\n----------------------------------------\n\nTITLE: Initializing MultiModalPredictor for Image-Text Similarity in Python\nDESCRIPTION: Creates a MultiModalPredictor instance for image-text similarity tasks.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/zero_shot_img_txt_matching.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\npredictor = MultiModalPredictor(problem_type=\"image_text_similarity\")\n```\n\n----------------------------------------\n\nTITLE: Loading Cloned AutoGluon TabularPredictor in Python\nDESCRIPTION: Loads the cloned predictor from the specified path and demonstrates that it has the same functionality as the original.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-deployment.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npredictor_clone = TabularPredictor.load(path=path_clone)\n# You can alternatively load the cloned TabularPredictor at the time of cloning:\n# predictor_clone = predictor.clone(path=save_path_clone, return_clone=True)\n\ny_pred_clone = predictor.predict(test_data)\ny_pred_clone\n\ny_pred.equals(y_pred_clone)\n\npredictor_clone.leaderboard(test_data)\n```\n\n----------------------------------------\n\nTITLE: Model Evaluation\nDESCRIPTION: Evaluates the trained model using accuracy and F1 score metrics.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/beginner_text.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntest_score = predictor.evaluate(test_data, metrics=['acc', 'f1'])\nprint(test_score)\n```\n\n----------------------------------------\n\nTITLE: Printing Similarity Matching Problem Types Information\nDESCRIPTION: This snippet prints information about text-to-text, image-to-image, and image-to-text similarity matching problem types.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/problem_types_and_metrics.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsimilarity_types = [\n    (TEXT_SIMILARITY, \"Text Similarity\"),\n    (IMAGE_SIMILARITY, \"Image Similarity\"),\n    (IMAGE_TEXT_SIMILARITY, \"Image-Text Similarity\")\n]\n\nprint(\"\\n=== Similarity Matching ===\")\nfor type_key, type_name in similarity_types:\n    props = PROBLEM_TYPES_REG.get(type_key)\n    print(f\"\\n{type_name}:\")\n    print(\"Input Requirements:\")\n    for modality in props.supported_modality_type:\n        print(f\"- {modality}\")\n    print(f\"Zero-shot prediction: {'Supported' if props.support_zero_shot else 'Not supported'}\")\n```\n\n----------------------------------------\n\nTITLE: Constructing Analysis Graph in AutoGluon EDA\nDESCRIPTION: Demonstrates how to construct a nested graph of analyses using BaseAnalysis class. The example shows configuration of various analysis components like data sampling, summary statistics, and feature analysis with their hierarchical relationships.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/references/autogluon.eda.base-apis.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nanalysis = BaseAnalysis(\n    # State\n    state=state,\n    # Arguments\n    train_data=train_data, test_data=test_data, val_data=val_data, model=model, label=label,\n    # Nested analyses\n    children=[\n        Sampler(sample=sample, children=[\n            DatasetSummary(),\n            MissingValuesAnalysis(),\n            RawTypesAnalysis(),\n            SpecialTypesAnalysis(),\n            ApplyFeatureGenerator(category_to_numbers=True, children=[\n                FeatureDistanceAnalysis()\n            ]),\n        ]),\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Downloading COCO2017 Dataset with Python CLI\nDESCRIPTION: Commands to download and prepare the COCO2017 dataset using AutoGluon's CLI tool, which works on all major platforms. The script downloads the dataset to the current directory or a specified output path.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/prepare_coco17.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m autogluon.multimodal.cli.prepare_detection_dataset --dataset_name coco2017\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m autogluon.multimodal.cli.prepare_detection_dataset --dataset_name coco2017 --output_path ~/data\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m autogluon.multimodal.cli.prepare_detection_dataset -d coco17 -o ~/data\n```\n\n----------------------------------------\n\nTITLE: Using Custom Metric with TimeSeriesPredictor\nDESCRIPTION: Complete example demonstrating how to use a custom metric to evaluate forecasting accuracy with dummy dataset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-metrics.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n\n# Create dummy dataset\ndata = TimeSeriesDataFrame.from_iterable_dataset(\n   [\n       {\"start\": pd.Period(\"2023-01-01\", freq=\"D\"), \"target\": list(range(15))},\n       {\"start\": pd.Period(\"2023-01-01\", freq=\"D\"), \"target\": list(range(30, 45))},\n    ]\n)\nprediction_length = 3\ntrain_data, test_data = data.train_test_split(prediction_length=prediction_length)\npredictor = TimeSeriesPredictor(prediction_length=prediction_length, verbosity=0).fit(train_data, hyperparameters={\"Naive\": {}})\npredictions = predictor.predict(train_data)\n\nmse = MeanSquaredError()\nmse_score = mse(\n  data=test_data,\n  predictions=predictions,\n  prediction_length=predictor.prediction_length,\n  target=predictor.target,\n)\nprint(f\"{mse.name_with_sign} = {mse_score}\")\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Multimodal Package\nDESCRIPTION: Installs the AutoGluon multimodal package using pip.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/beginner_multimodal.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.multimodal\n```\n\n----------------------------------------\n\nTITLE: Minimal Training Example\nDESCRIPTION: Shows the minimal code required to train a model with AutoGluon\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-essentials.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.tabular import TabularPredictor\npredictor = TabularPredictor(label=<variable-name>).fit(train_data=<file-name>)\n```\n\n----------------------------------------\n\nTITLE: Predicting Image-Text Pair Matching with MultiModalPredictor in Python\nDESCRIPTION: Uses the MultiModalPredictor to predict whether an image-text pair matches.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/zero_shot_img_txt_matching.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npred = predictor.predict({\"abc\": [image_paths[4]], \"xyz\": [texts[3]]})\nprint(pred)\n```\n\n----------------------------------------\n\nTITLE: Predicting Match Probabilities\nDESCRIPTION: Calculates matching probabilities for image pairs without thresholding\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/image2image_matching.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nproba = predictor.predict_proba(test_data.head(3))\nprint(proba)\n```\n\n----------------------------------------\n\nTITLE: Installing UV Package Installer and AutoGluon with GPU Support\nDESCRIPTION: This snippet shows how to install the UV package installer, which is faster than pip, and then use it to install AutoGluon with GPU support. The UV installer is first updated or installed, followed by the installation of AutoGluon using UV.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/install-gpu-uv.md#2025-04-22_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n# Install UV package installer (faster than pip)\npip install -U uv\n\n# Install AutoGluon with GPU support\npython -m uv pip install autogluon\n```\n\n----------------------------------------\n\nTITLE: Configuring Text Normalization in MultiModalPredictor Hyperparameters\nDESCRIPTION: A code snippet showing how to enable text normalization in AutoGluon's MultiModalPredictor by setting the appropriate hyperparameter. This configuration helps standardize text data for better model performance.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/kaggle_feedback_prize/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nhyperparameters={\n    \"data.text.normalize_text\": True,\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Training Steps in AutoMM\nDESCRIPTION: Examples showing how to set the maximum number of training steps in AutoMM, with -1 as default (disabled) or a custom value like 100 steps.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"optim.max_steps\": -1})\n# train 100 steps\npredictor.fit(hyperparameters={\"optim.max_steps\": 100})\n```\n\n----------------------------------------\n\nTITLE: Installing Complete AutoGluon with All Optional Dependencies\nDESCRIPTION: Command to install the full AutoGluon package along with all optional dependencies including imodels, skex, and skl2onnx.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/install-modules.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install autogluon && pip install autogluon.tabular[imodels,skex,skl2onnx]\n```\n\n----------------------------------------\n\nTITLE: Loading Saved MultiModalPredictor\nDESCRIPTION: Loads a previously saved MultiModalPredictor and uses it for prediction.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_prediction/beginner_image_cls.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nloaded_predictor = MultiModalPredictor.load(model_path)\nload_proba = loaded_predictor.predict_proba({'image': [image_path]})\nprint(load_proba)\n```\n\n----------------------------------------\n\nTITLE: Loading English Dataset\nDESCRIPTION: Loads and preprocesses the English training and test datasets from TSV files, sampling 1000 training and 200 test examples.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/multilingual_text.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrain_en_df = pd.read_csv('amazon_review_sentiment_cross_lingual/en_train.tsv',\n                          sep='\\t',\n                          header=None,\n                          names=['label', 'text']) \\\n                .sample(1000, random_state=123)\ntrain_en_df.reset_index(inplace=True, drop=True)\n\ntest_en_df = pd.read_csv('amazon_review_sentiment_cross_lingual/en_test.tsv',\n                          sep='\\t',\n                          header=None,\n                          names=['label', 'text']) \\\n               .sample(200, random_state=123)\ntest_en_df.reset_index(inplace=True, drop=True)\nprint(train_en_df)\n```\n\n----------------------------------------\n\nTITLE: Loading CSV Data for Multimodal NER in Python\nDESCRIPTION: This snippet loads training and test data from CSV files and defines the label column for named entity recognition.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal_ner.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndataset_path = download_dir + '/multimodal_ner'\ntrain_data = pd.read_csv(f'{dataset_path}/twitter17_train.csv')\ntest_data = pd.read_csv(f'{dataset_path}/twitter17_test.csv')\nlabel_col = 'entity_annotations'\n```\n\n----------------------------------------\n\nTITLE: Loading Training and Test Data\nDESCRIPTION: Loads SST dataset from S3 storage and creates training/test splits with subsampling.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/beginner_text.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.core.utils.loaders import load_pd\ntrain_data = load_pd.load('https://autogluon-text.s3-accelerate.amazonaws.com/glue/sst/train.parquet')\ntest_data = load_pd.load('https://autogluon-text.s3-accelerate.amazonaws.com/glue/sst/dev.parquet')\nsubsample_size = 1000  # subsample data for faster demo, try setting this to larger values\ntrain_data = train_data.sample(n=subsample_size, random_state=0)\ntrain_data.head(10)\n```\n\n----------------------------------------\n\nTITLE: Adding Holiday Features Function\nDESCRIPTION: Implements a function to add holiday indicator columns to TimeSeriesDataFrame with options for individual holidays or combined indicator.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-indepth.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef add_holiday_features(\n    ts_df: TimeSeriesDataFrame,\n    country_holidays: dict,\n    include_individual_holidays: bool = True,\n    include_holiday_indicator: bool = True,\n) -> TimeSeriesDataFrame:\n    \"\"\"Add holiday indicator columns to a TimeSeriesDataFrame.\"\"\"\n    ts_df = ts_df.copy()\n    if not isinstance(ts_df, TimeSeriesDataFrame):\n        ts_df = TimeSeriesDataFrame(ts_df)\n    timestamps = ts_df.index.get_level_values(\"timestamp\")\n    country_holidays_df = pd.get_dummies(pd.Series(country_holidays)).astype(float)\n    holidays_df = country_holidays_df.reindex(timestamps.date).fillna(0)\n    if include_individual_holidays:\n        ts_df[holidays_df.columns] = holidays_df.values\n    if include_holiday_indicator:\n        ts_df[\"Holiday\"] = holidays_df.max(axis=1).values\n    return ts_df\n```\n\n----------------------------------------\n\nTITLE: Extracting Image Embeddings\nDESCRIPTION: Extracts feature embeddings for individual images using the trained model\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/image2image_matching.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nembeddings_1 = predictor.extract_embedding({image_col_1: test_data[image_col_1][:5].tolist()})\nprint(embeddings_1.shape)\nembeddings_2 = predictor.extract_embedding({image_col_2: test_data[image_col_2][:5].tolist()})\nprint(embeddings_2.shape)\n```\n\n----------------------------------------\n\nTITLE: Classifying the Apple Image with CLIP Zero-Shot Classification\nDESCRIPTION: Uses the MultiModalPredictor to classify the apple image among different objects, demonstrating CLIP's accurate classification on clean images.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_prediction/clip_zeroshot.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprob = predictor.predict_proba({\"image\": [image_path]}, {\"text\": ['Granny Smith', 'iPod', 'library', 'pizza', 'toaster', 'dough']})\nprint(\"Label probs:\", prob)\n```\n\n----------------------------------------\n\nTITLE: Training MultiModal Predictor\nDESCRIPTION: Configures and trains the AutoGluon MultiModal predictor with specified hyperparameters and time limit.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/tensorrt.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\nhyperparameters = {\n    \"optim.max_epochs\": 2,\n    \"model.names\": [\"numerical_mlp\", \"categorical_mlp\", \"timm_image\", \"hf_text\", \"fusion_mlp\"],\n    \"model.timm_image.checkpoint_name\": \"mobilenetv3_small_100\",\n    \"model.hf_text.checkpoint_name\": \"google/electra-small-discriminator\",\n    \n}\npredictor = MultiModalPredictor(label=label_col).fit(\n    train_data=train_data,\n    hyperparameters=hyperparameters,\n    time_limit=120,\n)\n\nclear_output()\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model Performance\nDESCRIPTION: Evaluates the trained model on the test dataset using ROC AUC score\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/image2image_matching.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nscore = predictor.evaluate(test_data)\nprint(\"evaluation score: \", score)\n```\n\n----------------------------------------\n\nTITLE: Importing AutoGluon MultiModalPredictor\nDESCRIPTION: Imports the MultiModalPredictor class from AutoGluon's multimodal module.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/advanced/finetune_coco.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\n```\n\n----------------------------------------\n\nTITLE: Visualizing Train-Test Split for Time Series in Python\nDESCRIPTION: This code creates a visualization of the train-test split for a single time series. It plots the historical data and the forecast horizon, highlighting the difference between train and test datasets.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-indepth.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nitem_id = \"H1\"\nfig, (ax1, ax2) = plt.subplots(nrows=2, figsize=[10, 4], sharex=True)\ntrain_ts = train_data.loc[item_id]\ntest_ts = test_data.loc[item_id]\nax1.set_title(\"Train data (past time series values)\")\nax1.plot(train_ts)\nax2.set_title(\"Test data (past + future time series values)\")\nax2.plot(test_ts)\nfor ax in (ax1, ax2):\n    ax.fill_between(np.array([train_ts.index[-1], test_ts.index[-1]]), test_ts.min(), test_ts.max(), color=\"C1\", alpha=0.3, label=\"Forecast horizon\")\nplt.legend()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Downloading Pre-trained Teacher Model\nDESCRIPTION: Downloads a pre-trained teacher model for distillation using wget and unzips it.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/model_distillation.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n!wget --quiet https://automl-mm-bench.s3.amazonaws.com/unit-tests/distillation_sample_teacher.zip -O distillation_sample_teacher.zip\n!unzip -q -o distillation_sample_teacher.zip -d .\n```\n\n----------------------------------------\n\nTITLE: Default Hyperparameters Configuration\nDESCRIPTION: Defines default hyperparameters for the NHITS model including loss function, input size, scaling, and device configuration with CUDA support.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/advanced/forecasting-custom-model.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef _get_default_hyperparameters(self) -> dict:\n    import torch\n    from neuralforecast.losses.pytorch import MQLoss\n\n    default_hyperparameters = dict(\n        loss=MQLoss(quantiles=self.quantile_levels),\n        input_size=2 * self.prediction_length,\n        scaler_type=\"standard\",\n        enable_progress_bar=False,\n        enable_model_summary=False,\n        logger=False,\n        accelerator=\"cpu\",\n        start_padding_enabled=True,\n        futr_exog_list=self.covariate_metadata.known_covariates_real,\n        hist_exog_list=self.covariate_metadata.past_covariates_real,\n        stat_exog_list=self.covariate_metadata.static_features_real,\n    )\n\n    if torch.cuda.is_available():\n        default_hyperparameters[\"accelerator\"] = \"gpu\"\n        default_hyperparameters[\"devices\"] = 1\n\n    return default_hyperparameters\n```\n\n----------------------------------------\n\nTITLE: Evaluating the Model\nDESCRIPTION: Evaluates the finetuned model on the test set of the Pothole dataset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/advanced/finetune_coco.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npredictor.evaluate(test_path)\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing Image Dataset\nDESCRIPTION: Sets up the environment and loads the Shopee-IET dataset for image classification using AutoGluon's utility functions\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/hyperparameter_optimization.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom datetime import datetime\n\nfrom autogluon.multimodal.utils.misc import shopee_dataset\ndownload_dir = './ag_automm_tutorial_hpo'\ntrain_data, test_data = shopee_dataset(download_dir)\ntrain_data = train_data.sample(frac=0.5)\nprint(train_data)\n```\n\n----------------------------------------\n\nTITLE: Configuring Learning Rate Choice Strategy in AutoMM\nDESCRIPTION: Examples showing how to select the learning rate choice strategy in AutoMM, with options for layerwise decay (default) or two-stage approach.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"optim.lr_choice\": \"layerwise_decay\"})\n# turn on two-stage lr choice\npredictor.fit(hyperparameters={\"optim.lr_choice\": \"two_stages\"})\n```\n\n----------------------------------------\n\nTITLE: Setting Weight Decay in AutoMM\nDESCRIPTION: Examples showing how to configure weight decay in AutoMM, with a default value of 1.0e-3 or a custom value of 1.0e-4.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"optim.weight_decay\": 1.0e-3})\n# set weight decay to 1.0e-4\npredictor.fit(hyperparameters={\"optim.weight_decay\": 1.0e-4})\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Training Epochs in AutoMM\nDESCRIPTION: Examples showing how to set the maximum number of training epochs in AutoMM, with a default of 10 epochs or a custom value of 20 epochs.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"optim.max_epochs\": 10})\n# train 20 epochs\npredictor.fit(hyperparameters={\"optim.max_epochs\": 20})\n```\n\n----------------------------------------\n\nTITLE: Configuring Precision Settings\nDESCRIPTION: Setting numerical precision for training, supporting double, float, bfloat16, or float16 precision options.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"env.precision\": \"16-mixed\"})\n# use bfloat16 mixed precision\npredictor.fit(hyperparameters={\"env.precision\": \"bf16-mixed\"})\n```\n\n----------------------------------------\n\nTITLE: Applying Custom Feature Pipeline in AutoGluon\nDESCRIPTION: This code snippet shows how to apply the custom feature generation pipeline to transform input data. The fit_transform method processes the dataframe, converting categorical columns to numeric and replacing rare categorical values with NaN.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-feature-engineering.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmypipeline.fit_transform(X=dfx)\n```\n\n----------------------------------------\n\nTITLE: Testing Model Inference Speed in AutoGluon\nDESCRIPTION: The snippet tests the inference speed of a model in AutoGluon, measuring prediction time and checking if it meets specified speed constraints. It calculates the inference time per row and checks if the model's speed ratio satisfies the user-defined limit.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-indepth.ipynb#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ntest_data_batch = test_data.sample(infer_limit_batch_size, replace=True, ignore_index=True)\n\nimport time\ntime_start = time.time()\npredictor_infer_limit.predict(test_data_batch)\ntime_end = time.time()\n\ninfer_time_per_row = (time_end - time_start) / len(test_data_batch)\nrows_per_second = 1 / infer_time_per_row\ninfer_time_per_row_ratio = infer_time_per_row / infer_limit\nis_constraint_satisfied = infer_time_per_row_ratio <= 1\n\nprint(f'Model is able to predict {round(rows_per_second, 1)} rows per second. (User-specified Throughput = {1 / infer_limit})')\nprint(f'Model uses {round(infer_time_per_row_ratio * 100, 1)}% of infer_limit time per row.')\nprint(f'Model satisfies inference constraint: {is_constraint_satisfied}')\n```\n\n----------------------------------------\n\nTITLE: Loading German Dataset\nDESCRIPTION: Loads and preprocesses the German training and test datasets from TSV files, sampling 1000 training and 200 test examples.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/multilingual_text.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntrain_de_df = pd.read_csv('amazon_review_sentiment_cross_lingual/de_train.tsv',\n                          sep='\\t', header=None, names=['label', 'text']) \\\n                .sample(1000, random_state=123)\ntrain_de_df.reset_index(inplace=True, drop=True)\n\ntest_de_df = pd.read_csv('amazon_review_sentiment_cross_lingual/de_test.tsv',\n                          sep='\\t', header=None, names=['label', 'text']) \\\n               .sample(200, random_state=123)\ntest_de_df.reset_index(inplace=True, drop=True)\nprint(train_de_df)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Model Predictions\nDESCRIPTION: Makes predictions on test data and analyzes the distribution of predicted values using various statistical distributions.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-quick-fit.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel = state.model\ny_pred = model.predict(df_test)\nauto.analyze_interaction(\n    train_data=pd.DataFrame({'SalePrice_Pred': y_pred}), \n    x='SalePrice_Pred', \n    fit_distributions=['johnsonsu', 'norm', 'exponnorm']\n)\n```\n\n----------------------------------------\n\nTITLE: Converting DataFrame to COCO Format in Python\nDESCRIPTION: This code demonstrates how to convert a pandas DataFrame back to COCO format using AutoGluon's utility function. It also shows how to save the converted data to a JSON file.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/object_detection_with_dataframe.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal.utils.object_detection import object_detection_df_to_coco\n\ntrain_coco = object_detection_df_to_coco(train_df)\nprint(train_coco)\n\n# Saving to a JSON file\ntrain_coco = object_detection_df_to_coco(train_df, save_path=\"./df_converted_to_coco.json\")\n\n# Loading from the saved JSON file\ntrain_df_from_saved_coco = from_coco(\"./df_converted_to_coco.json\", root=\"./\")\n```\n\n----------------------------------------\n\nTITLE: Loading Training Data for AutoGluon Model\nDESCRIPTION: Code to load and prepare the adult income dataset for training the custom model. Includes downloading data and basic sampling.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-model.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.tabular import TabularDataset\n\ntrain_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv')\ntest_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv')\nlabel = 'class'\ntrain_data = train_data.sample(n=1000, random_state=0)\n\ntrain_data.head(5)\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Submodule from Source\nDESCRIPTION: Example of installing an AutoGluon submodule from source with specific optional dependencies.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/install-modules.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncd autogluon && pip install -e tabular/[lightgbm,catboost]\n```\n\n----------------------------------------\n\nTITLE: Using Cached Preprocessed Data for Fast Predictions in AutoGluon\nDESCRIPTION: Caches preprocessed data to speed up prediction using AutoGluons predictor when predicting on the same dataset multiple times, thus skipping the preprocessing steps.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-indepth.ipynb#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ntest_data_preprocessed = predictor.transform_features(test_data)\n\n# The following call will be faster than a normal predict call because we are skipping the preprocessing stage.\npredictions = predictor.predict(test_data_preprocessed, transform_features=False)\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Multimodal Package\nDESCRIPTION: Installs the AutoGluon multimodal package required for running the examples.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/efficient_finetuning_basic.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.multimodal\n```\n\n----------------------------------------\n\nTITLE: Chunk-based Inference with AutoGluon\nDESCRIPTION: Example showing how to perform inference on large datasets by processing them in chunks using pandas DataFrame chunking functionality.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-faq.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.tabular import TabularDataset, TabularPredictor\nimport pandas as pd\nimport requests\n\ntrain_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv')\npredictor = TabularPredictor(label='class').fit(train_data.sample(n=100, random_state=0), hyperparameters={'GBM': {}})\n\n# Get the test dataset, if you are working with local data then omit the next two lines\nr = requests.get('https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv', allow_redirects=True)\nopen('test.csv', 'wb').write(r.content)\nreader = pd.read_csv('test.csv', chunksize=1024)\ny_pred = []\ny_true = []\nfor df_chunk in reader:\n    y_pred.append(predictor.predict(df_chunk))\n    y_true.append(df_chunk['class'])\ny_pred = pd.concat(y_pred, axis=0, ignore_index=True)\ny_true = pd.concat(y_true, axis=0, ignore_index=True)\npredictor.evaluate_predictions(y_true=y_true, y_pred=y_pred)\n```\n\n----------------------------------------\n\nTITLE: Installing TabPFN Extension for AutoGluon Tabular\nDESCRIPTION: Command to install AutoGluon Tabular with TabPFN support, which is a pre-trained tabular transformer model optimized for datasets with less than 10,000 rows.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/whats_new/v0.8.0.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install autogluon.tabular[all,tabpfn]\n```\n\n----------------------------------------\n\nTITLE: Exporting Trained Model for Transfer Learning\nDESCRIPTION: Demonstrates how to export a trained model for use as a foundation model in other tasks. This dumps the model weights to a specified path.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/continuous_training.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndump_model_path = f\"./tmp/{uuid.uuid4().hex}-automm_sst\"\npredictor.dump_model(save_path=dump_model_path)\n```\n\n----------------------------------------\n\nTITLE: Examining Model Properties\nDESCRIPTION: Shows how to inspect the model's inferred properties and feature importance\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-essentials.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nprint(\"AutoGluon infers problem type is: \", predictor.problem_type)\nprint(\"AutoGluon identified the following types of features:\")\nprint(predictor.feature_metadata)\n```\n\nLANGUAGE: python\nCODE:\n```\ntest_data_transform = predictor.transform_features(test_data)\ntest_data_transform.head()\n```\n\nLANGUAGE: python\nCODE:\n```\npredictor.feature_importance(test_data)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Test Data with Text Normalization in Python\nDESCRIPTION: Preprocesses test data using a custom function from the kaggle_feedback_prize_preprocess module. It applies text normalization to the test set.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/kaggle_feedback_prize/README.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntest_df = kaggle_feedback_prize_preprocess.read_and_process_data_with_norm(data_path, \"test.csv\", is_train=False)\n```\n\n----------------------------------------\n\nTITLE: Running AutoMM Distillation for GLUE Task with Specific Parameters\nDESCRIPTION: Bash script to run the AutoMM distillation example for a GLUE task (QNLI) with specified teacher and student models, seed, epochs, and loss weights.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/distillation/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nglue_task=qnli\nteacher_model=google/bert_uncased_L-12_H-768_A-12\nstudent_model=google/bert_uncased_L-6_H-768_A-12\nseed=123\nmax_epoch=12\nmetric=\"accuracy\"\ntemperature=5\nhard_label_weight=0.1\nsoft_label_weight=1\n\npython3 automm_distillation_glue.py --teacher_model ${teacher_model} \\\n                                    --student_model ${student_model} \\\n                                    --seed ${seed} \\\n                                    --max_epoch ${max_epoch} \\\n                                    --hard_label_weight ${hard_label_weight} \\\n                                    --soft_label_weight ${soft_label_weight} \\\n                                    --glue_task ${glue_task}\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Sample Dataset for Object Detection\nDESCRIPTION: This code snippet demonstrates how to download a sample object detection dataset in COCO format and extract it to a local directory. It uses AutoGluon's utility functions for handling zip files.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/object_detection_with_dataframe.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom autogluon.core.utils.loaders import load_zip\n\nzip_file = \"https://automl-mm-bench.s3.amazonaws.com/object_detection_dataset/tiny_motorbike_coco.zip\"\ndownload_dir = \"./tiny_motorbike_coco\"\n\nload_zip.unzip(zip_file, unzip_dir=download_dir)\ndata_dir = os.path.join(download_dir, \"tiny_motorbike\")\ntrain_path = os.path.join(data_dir, \"Annotations\", \"trainval_cocoformat.json\")\ntest_path = os.path.join(data_dir, \"Annotations\", \"test_cocoformat.json\")\n```\n\n----------------------------------------\n\nTITLE: Splitting Time Series Data for Training and Testing\nDESCRIPTION: Creates train-test split for time series data with an 8-week prediction window.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-chronos.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprediction_length = 8\ntrain_data, test_data = data.train_test_split(prediction_length=prediction_length)\n```\n\n----------------------------------------\n\nTITLE: Extracting Anomaly Scores\nDESCRIPTION: Retrieving anomaly scores from the analysis state for both training and test datasets.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-anomaly-detection.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntrain_anomaly_scores = state.anomaly_detection.scores.train_data\ntest_anomaly_scores = state.anomaly_detection.scores.test_data\n```\n\n----------------------------------------\n\nTITLE: Loading MLDoc Text Dataset for Few Shot Learning\nDESCRIPTION: Prepares the MLDoc text classification dataset by downloading and loading it into pandas DataFrames. The dataset is downsampled to 10 samples per class (10 shots) to demonstrate few shot learning capabilities.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/few_shot_learning.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport os\nfrom autogluon.core.utils.loaders import load_zip\n\ndownload_dir = \"./ag_automm_tutorial_fs_cls\"\nzip_file = \"https://automl-mm-bench.s3.amazonaws.com/nlp_datasets/MLDoc-10shot-en.zip\"\nload_zip.unzip(zip_file, unzip_dir=download_dir)\ndataset_path = os.path.join(download_dir)\ntrain_df = pd.read_csv(f\"{dataset_path}/train.csv\", names=[\"label\", \"text\"])\ntest_df = pd.read_csv(f\"{dataset_path}/test.csv\", names=[\"label\", \"text\"])\nprint(train_df)\nprint(test_df)\n```\n\n----------------------------------------\n\nTITLE: Loading NF Corpus Dataset\nDESCRIPTION: Loads the NF Corpus dataset and prepares dataframes for queries, documents, and relevance scores.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/text_semantic_search.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n%%capture\ndataset = ir_datasets.load(\"beir/nfcorpus/test\")\n\n# prepare dataset\ndoc_data = pd.DataFrame(dataset.docs_iter())\nquery_data = pd.DataFrame(dataset.queries_iter())\nlabeled_data = pd.DataFrame(dataset.qrels_iter())\nlabel_col = \"relevance\"\nquery_id_col = \"query_id\"\ndoc_id_col = \"doc_id\"\ntext_col = \"text\"\nid_mappings={query_id_col: query_data.set_index(query_id_col)[text_col], doc_id_col: doc_data.set_index(doc_id_col)[text_col]}\n```\n\n----------------------------------------\n\nTITLE: Applying AutoMLPipelineFeatureGenerator to DataFrame in Python\nDESCRIPTION: This code demonstrates how to apply the AutoMLPipelineFeatureGenerator to a DataFrame, showing the resulting feature transformations.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-feature-engineering.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.features.generators import AutoMLPipelineFeatureGenerator\nauto_ml_pipeline_feature_generator = AutoMLPipelineFeatureGenerator()\nauto_ml_pipeline_feature_generator.fit_transform(X=dfx)\n```\n\n----------------------------------------\n\nTITLE: Loading Shopee-IET Dataset for Image Classification\nDESCRIPTION: Downloads and loads the Shopee-IET dataset for image classification using AutoGluon's utility function.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_prediction/beginner_image_cls.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\n\nfrom autogluon.multimodal.utils.misc import shopee_dataset\ndownload_dir = './ag_automm_tutorial_imgcls'\ntrain_data_path, test_data_path = shopee_dataset(download_dir)\nprint(train_data_path)\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing Dataset\nDESCRIPTION: Loads the Stanford Sentiment Treebank dataset, subsamples it, and displays the first 10 rows.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/presets.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.core.utils.loaders import load_pd\n\ntrain_data = load_pd.load('https://autogluon-text.s3-accelerate.amazonaws.com/glue/sst/train.parquet')\ntest_data = load_pd.load('https://autogluon-text.s3-accelerate.amazonaws.com/glue/sst/dev.parquet')\nsubsample_size = 1000  # subsample data for faster demo, try setting this to larger values\ntrain_data = train_data.sample(n=subsample_size, random_state=0)\ntrain_data.head(10)\n```\n\n----------------------------------------\n\nTITLE: Creating TimeSeriesDataFrame from Pandas DataFrame\nDESCRIPTION: Converts the loaded pandas DataFrame into a TimeSeriesDataFrame, specifying the item_id and timestamp columns.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-quick-start.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrain_data = TimeSeriesDataFrame.from_data_frame(\n    df,\n    id_column=\"item_id\",\n    timestamp_column=\"timestamp\"\n)\ntrain_data.head()\n```\n\n----------------------------------------\n\nTITLE: Setting Download Paths\nDESCRIPTION: Defines paths for downloading and storing the PetFinder dataset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-multimodal.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndownload_dir = './ag_petfinder_tutorial'\nzip_file = 'https://automl-mm-bench.s3.amazonaws.com/petfinder_kaggle.zip'\n```\n\n----------------------------------------\n\nTITLE: Defining Forecasting Task Parameters\nDESCRIPTION: Sets up the key parameters for the forecasting task, including prediction horizon, target variable, and known covariates.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/advanced/forecasting-custom-model.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nprediction_length = 7  # number of future steps to predict\ntarget = \"unit_sales\"  # target column\nknown_covariates_names = [\"promotion_email\", \"promotion_homepage\"]  # covariates known in the future\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing Dataset\nDESCRIPTION: Loads and preprocesses the multilingual Amazon review dataset, sampling training and test sets for English, German, and Japanese languages.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/efficient_finetuning_basic.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ntrain_en_df = pd.read_csv(\"amazon_review_sentiment_cross_lingual/en_train.tsv\",\n                          sep=\"\\t\",\n                          header=None,\n                          names=[\"label\", \"text\"]) \\\n                .sample(1000, random_state=123).reset_index(drop=True)\n\ntest_en_df = pd.read_csv(\"amazon_review_sentiment_cross_lingual/en_test.tsv\",\n                          sep=\"\\t\",\n                          header=None,\n                          names=[\"label\", \"text\"]) \\\n               .sample(200, random_state=123).reset_index(drop=True)\ntest_de_df = pd.read_csv(\"amazon_review_sentiment_cross_lingual/de_test.tsv\",\n                          sep=\"\\t\", header=None, names=[\"label\", \"text\"]) \\\n               .sample(200, random_state=123).reset_index(drop=True)\n\ntest_jp_df = pd.read_csv('amazon_review_sentiment_cross_lingual/jp_test.tsv',\n                          sep='\\t', header=None, names=['label', 'text']) \\\n               .sample(200, random_state=123).reset_index(drop=True)\ntrain_en_df.head(5)\n```\n\n----------------------------------------\n\nTITLE: Loading CSV Data\nDESCRIPTION: Loads training, validation, and test data from CSV files into pandas DataFrames.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_segmentation/beginner_semantic_seg.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport os\ndataset_path = os.path.join(download_dir, 'leaf_disease_segmentation')\ntrain_data = pd.read_csv(f'{dataset_path}/train.csv', index_col=0)\nval_data = pd.read_csv(f'{dataset_path}/val.csv', index_col=0)\ntest_data = pd.read_csv(f'{dataset_path}/test.csv', index_col=0)\nimage_col = 'image'\nlabel_col = 'label'\n```\n\n----------------------------------------\n\nTITLE: Configuring Mixup/Cutmix Application Mode in AutoMM Python\nDESCRIPTION: Specifies how Mixup or Cutmix parameters are applied. Options are `\"batch\"` (default, applied across the batch), `\"pair\"` (applied to pairs of elements), or `\"elem\"` (applied per element). Set `data.mixup.mode` within the `hyperparameters` passed to `predictor.fit()`. Refer to the linked PyTorch Image Models (timm) code for details.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_71\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"data.mixup.mode\": \"batch\"})\n# use \"pair\"\npredictor.fit(hyperparameters={\"data.mixup.mode\": \"pair\"})\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Document Classification Dataset\nDESCRIPTION: Sets up the environment by downloading a sample of the RVL-CDIP dataset and extracting it to a local directory.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/document_prediction/document_classification.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport pandas as pd\nfrom autogluon.core.utils.loaders import load_zip\n\ndownload_dir = './ag_automm_tutorial_doc_classifier'\nzip_file = \"https://automl-mm-bench.s3.amazonaws.com/doc_classification/rvl_cdip_sample.zip\"\nload_zip.unzip(zip_file, unzip_dir=download_dir)\n```\n\n----------------------------------------\n\nTITLE: Setting Learning Rate Schedule in AutoMM\nDESCRIPTION: Examples showing how to configure the learning rate schedule in AutoMM, with options including cosine_decay (default), polynomial_decay, and linear_decay.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"optim.lr_schedule\": \"cosine_decay\"})\n# use polynomial decay\npredictor.fit(hyperparameters={\"optim.lr_schedule\": \"polynomial_decay\"})\n```\n\n----------------------------------------\n\nTITLE: Loading Japanese Test Dataset\nDESCRIPTION: Loads and preprocesses the Japanese test dataset for evaluating cross-lingual performance.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/multilingual_text.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntest_jp_df = pd.read_csv('amazon_review_sentiment_cross_lingual/jp_test.tsv',\n                          sep='\\t', header=None, names=['label', 'text']) \\\n               .sample(200, random_state=123)\ntest_jp_df.reset_index(inplace=True, drop=True)\nprint(test_jp_df)\n```\n\n----------------------------------------\n\nTITLE: Loading Time Series Data with AutoGluon\nDESCRIPTION: Loads grocery sales dataset from an S3 bucket using TimeSeriesDataFrame.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-chronos.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndata = TimeSeriesDataFrame.from_path(\n    \"https://autogluon.s3.amazonaws.com/datasets/timeseries/grocery_sales/test.csv\",\n)\ndata.head()\n```\n\n----------------------------------------\n\nTITLE: Classification Model Quick Fit\nDESCRIPTION: Loads Titanic dataset and performs automated model fitting using AutoGluon's quick_fit function with feature importance visualization.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-quick-fit.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport autogluon.eda.auto as auto\n\ndf_train = pd.read_csv('https://autogluon.s3.amazonaws.com/datasets/titanic/train.csv')\ndf_test = pd.read_csv('https://autogluon.s3.amazonaws.com/datasets/titanic/test.csv')\ntarget_col = 'Survived'\n\nstate = auto.quick_fit(\n    df_train, \n    target_col, \n    return_state=True,\n    show_feature_importance_barplots=True\n)\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Dataset\nDESCRIPTION: Downloads and unzips the Stanford Online Products dataset for image matching\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/image2image_matching.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndownload_dir = './ag_automm_tutorial_img2img'\nzip_file = 'https://automl-mm-bench.s3.amazonaws.com/Stanford_Online_Products.zip'\nfrom autogluon.core.utils.loaders import load_zip\nload_zip.unzip(zip_file, unzip_dir=download_dir)\n```\n\n----------------------------------------\n\nTITLE: Selecting MMDetection Model in AutoMM\nDESCRIPTION: Specifies an MMDetection model for object detection tasks using the 'model.mmdet_image.checkpoint_name' hyperparameter. Includes options for YOLOX and other MMDetection models.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_49\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor = MultiModalPredictor(hyperparameters={\"model.mmdet_image.checkpoint_name\": \"yolov3_mobilenetv2_8xb24-320-300e_coco\"})\n# choose YOLOX-L\npredictor = MultiModalPredictor(hyperparameters={\"model.mmdet_image.checkpoint_name\": \"yolox_l\"})\n# choose DINO-SwinL\npredictor = MultiModalPredictor(hyperparameters={\"model.mmdet_image.checkpoint_name\": \"dino-5scale_swin-l_8xb2-36e_coco\"})\n```\n\n----------------------------------------\n\nTITLE: Preparing Demo Data for Image-Text Matching in Python\nDESCRIPTION: Downloads sample images and defines sample texts for image-text matching demonstration.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/zero_shot_img_txt_matching.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import download\n\ntexts = [\n    \"A cheetah chases prey on across a field.\",\n    \"A man is eating a piece of bread.\",\n    \"The girl is carrying a baby.\",\n    \"There is an airplane over a car.\",\n    \"A man is riding a horse.\",\n    \"Two men pushed carts through the woods.\",\n    \"There is a carriage in the image.\",\n    \"A man is riding a white horse on an enclosed ground.\",\n    \"A monkey is playing drums.\",\n]\n\nurls = ['http://farm4.staticflickr.com/3179/2872917634_f41e6987a8_z.jpg',\n        'http://farm4.staticflickr.com/3629/3608371042_75f9618851_z.jpg',\n        'https://farm4.staticflickr.com/3795/9591251800_9c9727e178_z.jpg',\n        'http://farm8.staticflickr.com/7188/6848765123_252bfca33d_z.jpg',\n        'https://farm6.staticflickr.com/5251/5548123650_1a69ce1e34_z.jpg']\n\nimage_paths = [download(url) for url in urls]\n```\n\n----------------------------------------\n\nTITLE: Creating New Features and Analyzing Interactions\nDESCRIPTION: Creates 'GroupSize' and 'FarePerPerson' features, then analyzes their interactions with 'Age', 'Survived', and 'Pclass'.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-analyze-interaction.ipynb#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nticket_to_count = df_train.groupby(by='Ticket')['Embarked'].count().to_dict()\ndata = df_train.copy()\ndata['GroupSize'] = data.Ticket.map(ticket_to_count)\ndata['FarePerPerson'] = data.Fare / data.GroupSize\n\nauto.analyze_interaction(x='FarePerPerson', y='Age', hue='Survived', train_data=data)\nauto.analyze_interaction(x='FarePerPerson', y='Age', hue='Pclass', train_data=data)\n```\n\n----------------------------------------\n\nTITLE: Implementing BM25 Search Functions\nDESCRIPTION: Defines helper functions for tokenizing corpus, ranking documents using BM25, and evaluating search results with NDCG metric.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/text_semantic_search.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n!pip3 install rank_bm25\nfrom collections import defaultdict\nimport string\nimport nltk\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom rank_bm25 import BM25Okapi\n\nnltk.download('stopwords')\nnltk.download('punkt')\n\ndef tokenize_corpus(corpus):\n    stop_words = set(stopwords.words(\"english\") + list(string.punctuation))\n    \n    tokenized_docs = []\n    for doc in corpus:\n        tokens = nltk.word_tokenize(doc.lower())\n        tokenized_doc = [w for w in tokens if w not in stop_words and len(w) > 2]\n        tokenized_docs.append(tokenized_doc)\n    return tokenized_docs\n\ndef rank_documents_bm25(queries_text, queries_id, docs_id, top_k, bm25):\n    tokenized_queries = tokenize_corpus(queries_text)\n    \n    results = {qid: {} for qid in queries_id}\n    for query_idx, query in enumerate(tokenized_queries):\n        scores = bm25.get_scores(query)\n        scores_top_k_idx = np.argsort(scores)[::-1][:top_k]\n        for doc_idx in scores_top_k_idx:\n            results[queries_id[query_idx]][docs_id[doc_idx]] = float(scores[doc_idx])\n    return results\n\ndef get_qrels(dataset):\n    \"\"\"\n    Get the ground truth of relevance score for all queries\n    \"\"\"\n    qrel_dict = defaultdict(dict)\n    for qrel in dataset.qrels_iter():\n        qrel_dict[qrel.query_id][qrel.doc_id] = qrel.relevance\n    return qrel_dict\n\ndef evaluate_bm25(doc_data, query_data, qrel_dict, cutoffs):\n    \n    tokenized_corpus = tokenize_corpus(doc_data[text_col].tolist())\n    bm25_model = BM25Okapi(tokenized_corpus, k1=1.2, b=0.75)\n    \n    results = rank_documents_bm25(query_data[text_col].tolist(), query_data[query_id_col].tolist(), doc_data[doc_id_col].tolist(), max(cutoffs), bm25_model)\n    ndcg = compute_ranking_score(results=results, qrel_dict=qrel_dict, metrics=[\"ndcg\"], cutoffs=cutoffs)\n    \n    return ndcg\n```\n\n----------------------------------------\n\nTITLE: Visualizing Feature Interactions\nDESCRIPTION: Creating visualization of feature interactions with anomaly scores for both datasets.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-anomaly-detection.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nauto.analyze_interaction(train_data=df_train.join(train_anomaly_scores), x=\"Fare\", y=\"Age\", hue=\"score\", chart_args=dict(palette='viridis'))\n\nauto.analyze_interaction(train_data=df_test.join(test_anomaly_scores), x=\"Fare\", y=\"Age\", hue=\"score\", chart_args=dict(palette='viridis'))\n```\n\n----------------------------------------\n\nTITLE: Using Pseudolabeling in TabularPredictor\nDESCRIPTION: Example of using automated semi-supervised learning with pseudolabeling in TabularPredictor.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/whats_new/v0.4.0.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nTabularPredictor.fit_pseudolabel(...)\n```\n\n----------------------------------------\n\nTITLE: Processing Different Label-Studio Task Formats in AutoGluon\nDESCRIPTION: Demonstrates how to transform different types of Label-Studio export files (image classification, named entity recognition, or custom formats) into DataFrames for AutoGluon. Both CSV and JSON formats are supported.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/label_studio_export_reader/LabelStudio_export_file_reader.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# transforming the export files from Label-Studio image classification template (image)\ndf, labels = ls.from_image_classification(\"ic.json\", ls_host_on=False) \n\n# transforming the export files from Label-Studio named entity recognition template (text)\ndf, labels = ls.from_named_entity_recognition('neg.json')\n\n# transforming the export files from user's customized labeling template\ndf, labels=ls.from_customized('custom.csv',\n                              ls_host_on=True, \n                              data_columns=['image1','image2','image3'],\n                              label_columns=['label'])\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Package\nDESCRIPTION: Installs the AutoGluon library using pip package manager.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-multimodal.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon\n```\n\n----------------------------------------\n\nTITLE: Analyzing Undecided Classifications\nDESCRIPTION: Examines predictions near decision boundaries using SHAP waterfall plots.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-quick-fit.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nauto.explain_rows(\n    train_data=df_train,\n    model=state.model,\n    display_rows=True,\n    plot=\"waterfall\",\n    rows=state.model_evaluation.undecided[:1],\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Text Features from Categorical Variables\nDESCRIPTION: Generates synthetic text features by concatenating multiple categorical columns into a single text field for both training and test datasets.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/tabular-fasttext.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntrain_data['text'] = (\n    train_data[['education', 'marital-status', 'occupation', 'relationship', \n                'workclass', 'native-country',  'sex', 'race']]\n    .apply(lambda r: ', '.join(r.values) + '.', axis=1)\n)\n\n\ntest_data['text'] = (\n    test_data[['education', 'marital-status', 'occupation', 'relationship',\n               'workclass', 'native-country',  'sex', 'race']]\n    .apply(lambda r: ', '.join(r.values) + '.', axis=1)\n)\nprint('sample text column values')\nprint(train_data['text'].sample(5).to_list())\n```\n\n----------------------------------------\n\nTITLE: Viewing Regression Model Leaderboard in AutoGluon\nDESCRIPTION: Displays a performance leaderboard of all models trained for the regression task on the test data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-essentials.ipynb#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\npredictor_age.leaderboard(test_data)\n```\n\n----------------------------------------\n\nTITLE: Importing AutoGluon Core Classes\nDESCRIPTION: Imports the essential TabularDataset and TabularPredictor classes from AutoGluon\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-essentials.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.tabular import TabularDataset, TabularPredictor\n```\n\n----------------------------------------\n\nTITLE: Loading Census Income Dataset in AutoGluon\nDESCRIPTION: Loads training and test data from AWS S3 into TabularDataset objects and performs basic cleaning by stripping whitespace from class labels.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/tabular-fasttext.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrain_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv')\ntrain_data['class'] = train_data['class'].str.strip()\ntest_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv')  # another Pandas DataFrame\nprint(train_data.head())\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset with Pandas in Python\nDESCRIPTION: Loads the training and test datasets from CSV files using pandas and identifies the target column 'AdoptionSpeed' for prediction.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal-quick-start.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndataset_path = f'{download_dir}/petfinder_for_tutorial'\n\ntrain_data = pd.read_csv(f'{dataset_path}/train.csv', index_col=0)\ntest_data = pd.read_csv(f'{dataset_path}/test.csv', index_col=0)\n\nlabel_col = 'AdoptionSpeed'\n```\n\n----------------------------------------\n\nTITLE: Training with Medium Quality Preset\nDESCRIPTION: Initializes a MultiModalPredictor with medium_quality preset and fits it on the training data with a 20-second time limit.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/presets.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\n\npredictor = MultiModalPredictor(label='label', eval_metric='acc', presets=\"medium_quality\")\npredictor.fit(\n    train_data=train_data,\n    time_limit=20, # seconds\n)\n```\n\n----------------------------------------\n\nTITLE: Model Evaluation\nDESCRIPTION: Evaluates the trained model on test data using ROC AUC metric.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/beginner_multimodal.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nscores = predictor.evaluate(test_data, metrics=[\"roc_auc\"])\nscores\n```\n\n----------------------------------------\n\nTITLE: Downloading Stanford Cars Dataset for Few Shot Image Classification\nDESCRIPTION: Prepares the Stanford Cars image classification dataset by downloading the dataset archive and CSV files. The training set is downsampled to have 8 samples per class to demonstrate few shot learning for image classification.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/few_shot_learning.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom autogluon.core.utils.loaders import load_zip, load_s3\n\ndownload_dir = \"./ag_automm_tutorial_fs_cls/stanfordcars/\"\nzip_file = \"https://automl-mm-bench.s3.amazonaws.com/vision_datasets/stanfordcars/stanfordcars.zip\"\ntrain_csv = \"https://automl-mm-bench.s3.amazonaws.com/vision_datasets/stanfordcars/train_8shot.csv\"\ntest_csv = \"https://automl-mm-bench.s3.amazonaws.com/vision_datasets/stanfordcars/test.csv\"\n\nload_zip.unzip(zip_file, unzip_dir=download_dir)\ndataset_path = os.path.join(download_dir)\n```\n\n----------------------------------------\n\nTITLE: Enabling Gradient Checkpointing in AutoMM HuggingFace Text Models\nDESCRIPTION: Toggles gradient checkpointing to reduce memory consumption during gradient calculations. This is configured using the 'model.hf_text.gradient_checkpointing' hyperparameter.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_43\n\nLANGUAGE: python\nCODE:\n```\n# by default, AutoMM doesn't turn on gradient checkpointing\npredictor.fit(hyperparameters={\"model.hf_text.gradient_checkpointing\": False})\n# Turn on gradient checkpointing\npredictor.fit(hyperparameters={\"model.hf_text.gradient_checkpointing\": True})\n```\n\n----------------------------------------\n\nTITLE: Displaying High Quality Preset Details\nDESCRIPTION: Retrieves and displays the hyperparameters and hyperparameter tuning settings for the high_quality preset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/presets.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom autogluon.multimodal.presets import get_automm_presets\n\nhyperparameters, hyperparameter_tune_kwargs = get_automm_presets(problem_type=\"default\", presets=\"high_quality\")\nprint(f\"hyperparameters: {json.dumps(hyperparameters, sort_keys=True, indent=4)}\")\nprint(f\"hyperparameter_tune_kwargs: {json.dumps(hyperparameter_tune_kwargs, sort_keys=True, indent=4)}\")\n```\n\n----------------------------------------\n\nTITLE: Importing Required Modules\nDESCRIPTION: Import statements for MultiModalPredictor and other necessary packages.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/quick_start/quick_start_coco.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\nimport os\nimport time\nfrom autogluon.core.utils.loaders import load_zip\n```\n\n----------------------------------------\n\nTITLE: Preparing and Running Product Sentiment Classification with AutoGluon\nDESCRIPTION: Downloads the dataset for MachineHack Product Sentiment Classification competition and runs AutoGluon TextPredictor using stacking mode. Generates a submission file for the competition.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/text_prediction/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p machine_hack_product_sentiment\nwget https://automl-mm-bench.s3.amazonaws.com/machine_hack_product_sentiment/all_train.csv -O machine_hack_product_sentiment/all_train.csv\nwget https://automl-mm-bench.s3.amazonaws.com/machine_hack_product_sentiment/test.csv -O machine_hack_product_sentiment/test.csv\n\nmkdir -p ag_product_sentiment\npython3 run_competition.py --train_file machine_hack_product_sentiment/all_train.csv \\\n                           --test_file machine_hack_product_sentiment/test.csv \\\n                           --task product_sentiment \\\n                           --eval_metric log_loss \\\n                           --exp_dir ag_product_sentiment \\\n                           --mode stacking  2>&1  | tee -a ag_product_sentiment/log.txt\n```\n\n----------------------------------------\n\nTITLE: Expanding Image Paths\nDESCRIPTION: Expands relative image paths to absolute paths for both training and test datasets\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/image2image_matching.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef path_expander(path, base_folder):\n    path_l = path.split(';')\n    return ';'.join([os.path.abspath(os.path.join(base_folder, path)) for path in path_l])\n\nfor image_col in [image_col_1, image_col_2]:\n    train_data[image_col] = train_data[image_col].apply(lambda ele: path_expander(ele, base_folder=dataset_path))\n    test_data[image_col] = test_data[image_col].apply(lambda ele: path_expander(ele, base_folder=dataset_path))\n```\n\n----------------------------------------\n\nTITLE: Loading NER Dataset\nDESCRIPTION: This code loads the MIT movies corpus dataset for NER training and testing using AutoGluon's load_pd utility.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/ner.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.core.utils.loaders import load_pd\ntrain_data = load_pd.load('https://automl-mm-bench.s3.amazonaws.com/ner/mit-movies/train_v2.csv')\ntest_data = load_pd.load('https://automl-mm-bench.s3.amazonaws.com/ner/mit-movies/test_v2.csv')\ntrain_data.head(5)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking PyTorch Prediction\nDESCRIPTION: Performs and measures prediction speed using default PyTorch implementation.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/tensorrt.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nbatch_size = 2\nn_trails = 10\nsample = test_data.head(batch_size)\n\n# Use first prediction for initialization (e.g., allocating memory)\ny_pred = predictor.predict_proba(sample)\n\npred_time = []\nfor _ in range(n_trails):\n    tic = time.time()\n    y_pred = predictor.predict_proba(sample)\n    elapsed = time.time()-tic\n    pred_time.append(elapsed)\n    print(f\"elapsed (pytorch): {elapsed*1000:.1f} ms (batch_size={batch_size})\")\n```\n\n----------------------------------------\n\nTITLE: Analyzing Interaction between Embarked, Fare, and Pclass\nDESCRIPTION: Visualizes the interaction between 'Embarked', 'Fare', and 'Pclass' features to help fill missing values.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-analyze-interaction.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nauto.analyze_interaction(train_data=df_train, x='Embarked', y='Fare', hue='Pclass')\n```\n\n----------------------------------------\n\nTITLE: Setting Learning Rate in AutoMM\nDESCRIPTION: Examples showing how to set the learning rate in AutoMM, with a default value of 1.0e-4 or a custom value of 5.0e-4.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"optim.lr\": 1.0e-4})\n# set learning rate to 5.0e-4\npredictor.fit(hyperparameters={\"optim.lr\": 5.0e-4})\n```\n\n----------------------------------------\n\nTITLE: Custom Feature Generator Implementation\nDESCRIPTION: Implements a custom feature generator that handles user-specified feature overrides separately from standard preprocessing\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-model-advanced.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.features import BulkFeatureGenerator, AutoMLPipelineFeatureGenerator, IdentityFeatureGenerator\n\nclass CustomFeatureGeneratorWithUserOverride(BulkFeatureGenerator):\n    def __init__(self, automl_generator_kwargs: dict = None, **kwargs):\n        generators = self._get_default_generators(automl_generator_kwargs=automl_generator_kwargs)\n        super().__init__(generators=generators, **kwargs)\n\n    def _get_default_generators(self, automl_generator_kwargs: dict = None):\n        if automl_generator_kwargs is None:\n            automl_generator_kwargs = dict()\n\n        generators = [\n            [\n                AutoMLPipelineFeatureGenerator(banned_feature_special_types=['user_override'], **automl_generator_kwargs),\n                IdentityFeatureGenerator(infer_features_in_args=dict(required_special_types=['user_override'])),\n            ],\n        ]\n        return generators\n```\n\n----------------------------------------\n\nTITLE: Analyzing Interaction for Age and Survived\nDESCRIPTION: Visualizes the interaction between 'Age' and 'Survived' features for both train and test data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-analyze-interaction.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nauto.analyze_interaction(x='Age', hue='Survived', train_data=df_train, test_data=df_test)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Multimodal Hyperparameter Configuration in AutoGluon\nDESCRIPTION: This code snippet shows how to retrieve the default hyperparameter configuration for multimodal data processing in AutoGluon.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-gpu.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.tabular.configs.hyperparameter_configs import get_hyperparameter_config\nhyperparameters = get_hyperparameter_config('multimodal')\nhyperparameters\n```\n\n----------------------------------------\n\nTITLE: Data Preprocessing Implementation\nDESCRIPTION: Implements the complete preprocessing pipeline including label cleaning and feature generation\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-model-advanced.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nX = train_data.drop(columns=[label])\ny = train_data[label]\nX_test = test_data.drop(columns=[label])\ny_test = test_data[label]\n\nfrom autogluon.core.data import LabelCleaner\nfrom autogluon.core.utils import infer_problem_type\nproblem_type = infer_problem_type(y=y)\nlabel_cleaner = LabelCleaner.construct(problem_type=problem_type, y=y)\ny_preprocessed = label_cleaner.transform(y)\ny_test_preprocessed = label_cleaner.transform(y_test)\n\nmy_custom_feature_generator = CustomFeatureGeneratorWithUserOverride(feature_metadata_in=feature_metadata)\n\nX_preprocessed = my_custom_feature_generator.fit_transform(X)\nX_test_preprocessed = my_custom_feature_generator.transform(X_test)\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Dataset\nDESCRIPTION: Commands to create directory, download and extract the book price prediction dataset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal_text_tabular.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n!mkdir -p price_of_books\n!wget https://automl-mm-bench.s3.amazonaws.com/machine_hack_competitions/predict_the_price_of_books/Data.zip -O price_of_books/Data.zip\n!cd price_of_books && unzip -o Data.zip\n!ls price_of_books/Participants_Data\n```\n\n----------------------------------------\n\nTITLE: Loading and Merging Training Data with Pandas in Python\nDESCRIPTION: This snippet demonstrates how to load and merge multiple CSV files containing training data for the ieee-fraud-detection competition using Pandas.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-kaggle.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\nfrom autogluon.tabular import TabularPredictor\n\ndirectory = '~/IEEEfraud/'\nlabel = 'isFraud'\neval_metric = 'roc_auc'\nsave_path = directory + 'AutoGluonModels/'\n\ntrain_identity = pd.read_csv(directory+'train_identity.csv')\ntrain_transaction = pd.read_csv(directory+'train_transaction.csv')\n\ntrain_data = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with AutoGluon TabularPredictor in Python\nDESCRIPTION: Loads a previously trained predictor and uses it to make predictions on new test data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-deployment.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npredictor = TabularPredictor.load(save_path)  # unnecessary, just demonstrates how to load previously-trained predictor from file\n\ny_pred = predictor.predict(test_data)\ny_pred\n```\n\n----------------------------------------\n\nTITLE: Preparing Test Data and Making Predictions with AutoGluon\nDESCRIPTION: This snippet demonstrates how to load and merge test data, then use the trained AutoGluon predictor to make predictions on the test set.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-kaggle.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntest_identity = pd.read_csv(directory+'test_identity.csv')\ntest_transaction = pd.read_csv(directory+'test_transaction.csv')\ntest_data = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')\n\ny_predproba = predictor.predict_proba(test_data)\ny_predproba.head(5)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Medium Quality Model\nDESCRIPTION: Evaluates the medium quality model on the test data using ROC AUC metric.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/presets.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nscores = predictor.evaluate(test_data, metrics=[\"roc_auc\"])\nscores\n```\n\n----------------------------------------\n\nTITLE: Loading M4 Hourly Dataset\nDESCRIPTION: Loads a subset of the M4 hourly dataset as a pandas DataFrame from a CSV file.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-quick-start.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv(\"https://autogluon.s3.amazonaws.com/datasets/timeseries/m4_hourly_subset/train.csv\")\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Creating a Sample DataFrame for Feature Engineering Demonstration in Python\nDESCRIPTION: This code creates a sample DataFrame with various column types (float, int, datetime, categorical, text) to demonstrate AutoGluon's feature engineering process.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-feature-engineering.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.tabular import TabularDataset, TabularPredictor\nimport pandas as pd\nimport numpy as np\nimport random\nfrom sklearn.datasets import make_regression\nfrom datetime import datetime\n\nx, y = make_regression(n_samples = 100,n_features = 5,n_targets = 1, random_state = 1)\ndfx = pd.DataFrame(x, columns=['A','B','C','D','E'])\ndfy = pd.DataFrame(y, columns=['label'])\n\n# Create an integer column, a datetime column, a categorical column and a string column to demonstrate how they are processed.\ndfx['B'] = (dfx['B']).astype(int)\ndfx['C'] = datetime(2000,1,1) + pd.to_timedelta(dfx['C'].astype(int), unit='D')\ndfx['D'] = pd.cut(dfx['D'] * 10, [-np.inf,-5,0,5,np.inf],labels=['v','w','x','y'])\ndfx['E'] = pd.Series(list(' '.join(random.choice([\"abc\", \"d\", \"ef\", \"ghi\", \"jkl\"]) for i in range(4)) for j in range(100)))\ndataset=TabularDataset(dfx)\nprint(dfx)\n```\n\n----------------------------------------\n\nTITLE: Predicting Book Prices with AutoGluon\nDESCRIPTION: Prepares data and runs AutoGluon TextPredictor for the MachineHack Book Price Prediction Hackathon. Uses stacking mode and generates a submission file in Excel format.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/text_prediction/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbash prepare_price_of_books.sh\npython3 -m pip install openpyxl\nmkdir -p ag_price_of_books\npython3 run_competition.py --train_file price_of_books/Participants_Data/Data_Train.xlsx \\\n                           --test_file price_of_books/Participants_Data/Data_Test.xlsx \\\n                           --sample_submission price_of_books/Participants_Data/Sample_Submission.xlsx \\\n                           --task price_of_books \\\n                           --eval_metric r2 \\\n                           --exp_dir ag_price_of_books \\\n                           --mode stacking 2>&1  | tee -a ag_price_of_books/log.txt\n```\n\n----------------------------------------\n\nTITLE: Preparing Training and Test Datasets\nDESCRIPTION: Preprocessing and subsampling the data for training and testing purposes.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal_text_tabular.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntrain_subsample_size = 1500  # subsample for faster demo, you can try setting to larger values\ntest_subsample_size = 5\ntrain_df = preprocess(train_df)\ntrain_data = train_df.iloc[100:].sample(train_subsample_size, random_state=123)\ntest_data = train_df.iloc[:100].sample(test_subsample_size, random_state=245)\ntrain_data.head()\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Kaggle California House Prices Dataset\nDESCRIPTION: These commands download the Kaggle California House Prices competition dataset and extract it to a local directory. Requires the Kaggle CLI to be installed and configured.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/kaggle_california_house_price/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkaggle competitions download -c california-house-prices\nunzip california-house-prices.zip -d california-house-prices\n```\n\n----------------------------------------\n\nTITLE: Loading Training Data\nDESCRIPTION: Loading the training data from Excel file using pandas.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal_text_tabular.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntrain_df = pd.read_excel(os.path.join('price_of_books', 'Participants_Data', 'Data_Train.xlsx'), engine='openpyxl')\ntrain_df.head()\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Dataset\nDESCRIPTION: Code to download and extract a sample COCO format dataset for object detection.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/quick_start/quick_start_coco.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nzip_file = \"https://automl-mm-bench.s3.amazonaws.com/object_detection_dataset/tiny_motorbike_coco.zip\"\ndownload_dir = \"./tiny_motorbike_coco\"\n\nload_zip.unzip(zip_file, unzip_dir=download_dir)\ndata_dir = os.path.join(download_dir, \"tiny_motorbike\")\ntrain_path = os.path.join(data_dir, \"Annotations\", \"trainval_cocoformat.json\")\ntest_path = os.path.join(data_dir, \"Annotations\", \"test_cocoformat.json\")\n```\n\n----------------------------------------\n\nTITLE: Setting Checkpoint Averaging Method in AutoMM\nDESCRIPTION: Examples showing how to configure the model checkpoint averaging method in AutoMM, with options including greedy_soup (default), uniform_soup, and best.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"optim.top_k_average_method\": \"greedy_soup\"})\n# average all the top k checkpoints\npredictor.fit(hyperparameters={\"optim.top_k_average_method\": \"uniform_soup\"})\n```\n\n----------------------------------------\n\nTITLE: Evaluating Regular Model Performance\nDESCRIPTION: Evaluates the trained model's accuracy on test data\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/hyperparameter_optimization.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nscores = predictor_regular.evaluate(test_data, metrics=[\"accuracy\"])\nprint('Top-1 test acc: %.3f' % scores[\"accuracy\"])\n```\n\n----------------------------------------\n\nTITLE: Importing AutoGluon EDA Visualization Model Components\nDESCRIPTION: This code snippet shows how to import the model visualization components from AutoGluon's EDA module. It includes classes for confusion matrices, feature importance, regression evaluation, and model leaderboards.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.model.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom autogluon.eda.visualization.model import ConfusionMatrix, FeatureImportance, RegressionEvaluation, ModelLeaderboard\n```\n\n----------------------------------------\n\nTITLE: Creating Custom MSE Scorer\nDESCRIPTION: Creates a custom mean squared error scorer using AutoGluon's make_scorer function with specific optimization parameters.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-metric.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nag_mean_squared_error_scorer = make_scorer(name='mean_squared_error',\n                                           score_func=sklearn.metrics.mean_squared_error,\n                                           optimum=0,\n                                           greater_is_better=False)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for AutoGluon TimeSeries in Python\nDESCRIPTION: This snippet imports the necessary Python libraries for working with AutoGluon TimeSeries, including pandas for data manipulation and TimeSeriesDataFrame and TimeSeriesPredictor from autogluon.timeseries.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-indepth.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon with pip\nDESCRIPTION: A sequence of commands to properly install AutoGluon. The commands update pip, setuptools, and wheel to their latest versions before installing the AutoGluon package to ensure proper dependency handling.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/install-gpu-pip.md#2025-04-22_snippet_0\n\nLANGUAGE: console\nCODE:\n```\npip install -U pip\npip install -U setuptools wheel\npip install autogluon\n```\n\n----------------------------------------\n\nTITLE: Limiting CPU Cores for Specific Models in AutoGluon\nDESCRIPTION: Demonstrates how to set different CPU core limits for individual models within AutoGluon. This example shows setting 1 core for CatBoost, 4 cores for XGBoost, and unlimited cores for LightGBM.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-faq.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# We use 1 core for CatBoost model, 4 cores for XGBoost model, and all cores for lightGBM model here.\npredictor = TabularPredictor(...).fit(..., hyperparameters= {'CAT': {'ag_args_fit': {'num_cpus': 1}}, 'XGB': {'ag_args_fit': {'num_cpus': 4}}, 'GBM': {}},)\n```\n\n----------------------------------------\n\nTITLE: Cloning and Installing AutoGluon from GitHub\nDESCRIPTION: These commands clone the AutoGluon repository from GitHub and run the full installation script. This process will set up AutoGluon and its dependencies on your local machine.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/install-gpu-source.md#2025-04-22_snippet_0\n\nLANGUAGE: console\nCODE:\n```\ngit clone https://github.com/autogluon/autogluon\ncd autogluon && ./full_install.sh\n```\n\n----------------------------------------\n\nTITLE: Downloading Dataset\nDESCRIPTION: Downloads and unzips the PetFinder dataset from S3.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/beginner_multimodal.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndownload_dir = './ag_automm_tutorial'\nzip_file = 'https://automl-mm-bench.s3.amazonaws.com/petfinder_for_tutorial.zip'\nfrom autogluon.core.utils.loaders import load_zip\nload_zip.unzip(zip_file, unzip_dir=download_dir)\n```\n\n----------------------------------------\n\nTITLE: Loading Titanic Dataset with Pandas\nDESCRIPTION: Loads the Titanic train and test datasets from S3 using pandas. It also defines the target column for the analysis.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-dataset-overview.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndf_train = pd.read_csv('https://autogluon.s3.amazonaws.com/datasets/titanic/train.csv')\ndf_test = pd.read_csv('https://autogluon.s3.amazonaws.com/datasets/titanic/test.csv')\ntarget_col = 'Survived'\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon MultiModal Package\nDESCRIPTION: This snippet shows how to install the AutoGluon MultiModal package using pip. This package is required for using the MultiModalPredictor for object detection tasks.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/object_detection_with_dataframe.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.multimodal\n```\n\n----------------------------------------\n\nTITLE: Predicting Data Scientist Salaries with AutoGluon\nDESCRIPTION: Prepares data and runs AutoGluon TextPredictor for the MachineHack Data Scientist Salary Prediction Hackathon. Uses stacking mode and generates a submission file in Excel format.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/text_prediction/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbash prepare_data_scientist_salary.sh\npython3 -m pip install openpyxl\nmkdir -p ag_data_scientist_salary\npython3 run_competition.py --train_file data_scientist_salary/Data/Final_Train_Dataset.csv \\\n                           --test_file data_scientist_salary/Data/Final_Test_Dataset.csv \\\n                           --sample_submission data_scientist_salary/Data/sample_submission.xlsx \\\n                           --task data_scientist_salary \\\n                           --eval_metric acc \\\n                           --exp_dir ag_data_scientist_salary \\\n                           --mode stacking 2>&1  | tee -a ag_data_scientist_salary/log.txt\n```\n\n----------------------------------------\n\nTITLE: Creating an Imbalanced Dataset\nDESCRIPTION: Artificially downsamples the Shopee training data to create an imbalanced class distribution, reducing each class by a factor of 1/3 compared to the previous class.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/focal_loss.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\n\nds = 1\n\nimbalanced_train_data = []\nfor lb in range(4):\n    class_data = train_data[train_data.label == lb]\n    sample_index = np.random.choice(np.arange(len(class_data)), size=int(len(class_data) * ds), replace=False)\n    ds /= 3  # downsample 1/3 each time for each class\n    imbalanced_train_data.append(class_data.iloc[sample_index])\nimbalanced_train_data = pd.concat(imbalanced_train_data)\nprint(imbalanced_train_data)\n\nweights = []\nfor lb in range(4):\n    class_data = imbalanced_train_data[imbalanced_train_data.label == lb]\n    weights.append(1 / (class_data.shape[0] / imbalanced_train_data.shape[0]))\n    print(f\"class {lb}: num samples {len(class_data)}\")\nweights = list(np.array(weights) / np.sum(weights))\nprint(weights)\n```\n\n----------------------------------------\n\nTITLE: Configuring Feature Metadata\nDESCRIPTION: Sets up feature metadata to specify image path column type for AutoGluon training.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-multimodal.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.tabular import FeatureMetadata\nfeature_metadata = FeatureMetadata.from_df(train_data)\nfeature_metadata = feature_metadata.add_special_types({image_col: ['image_path']})\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing NER Dataset\nDESCRIPTION: Imports necessary libraries and loads the pre-processed Chinese NER dataset from S3 storage.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/chinese_ner.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport autogluon.multimodal\nfrom autogluon.core.utils.loaders import load_pd\nfrom autogluon.multimodal.utils import visualize_ner\ntrain_data = load_pd.load('https://automl-mm-bench.s3.amazonaws.com/ner/taobao-ner/chinese_ner_train.csv')\ndev_data = load_pd.load('https://automl-mm-bench.s3.amazonaws.com/ner/taobao-ner/chinese_ner_dev.csv')\ntrain_data.head(5)\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker for AutoGluon Lambda Deployment\nDESCRIPTION: Dockerfile configuration for creating a Lambda-compatible container with AutoGluon dependencies. Includes optimizations for reducing container size by installing only necessary components.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/cloud_fit_deploy/cloud-aws-lambda-deployment.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM public.ecr.aws/lambda/python:3.8\n\nRUN yum install libgomp git -y \\\n && yum clean all -y && rm -rf /var/cache/yum\n\nARG TORCH_VER=1.9.1+cpu\nARG TORCH_VISION_VER=0.10.1+cpu\nARG NUMPY_VER=1.19.5\nRUN python3.8 -m pip --no-cache-dir install --upgrade --trusted-host pypi.org --trusted-host files.pythonhosted.org pip \\\n && python3.8 -m pip --no-cache-dir install --upgrade wheel setuptools \\\n && python3.8 -m pip uninstall -y dataclasses \\\n && python3.8 -m pip --no-cache-dir install --upgrade torch==\"${TORCH_VER}\" torchvision==\"${TORCH_VISION_VER}\" -f https://download.pytorch.org/whl/torch_stable.html \\\n && python3.8 -m pip --no-cache-dir install --upgrade numpy==${NUMPY_VER} \\\n && python3.8 -m pip --no-cache-dir install --upgrade autogluon.tabular[all]\"\n\nCOPY app.py ./\nCOPY ag_models /opt/ml/model/\n\nCMD [\"app.lambda_handler\"]\n```\n\n----------------------------------------\n\nTITLE: Creating Two-Way Partial Dependence Plot\nDESCRIPTION: Generates a two-way partial dependence plot for 'Fare' and 'Age' features to visualize their interaction.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-analyze-interaction.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nauto.partial_dependence_plots(df_train, label='Survived', features=['Fare', 'Age'], two_way=True)\n```\n\n----------------------------------------\n\nTITLE: Training a Custom NHITS Model in Standalone Mode\nDESCRIPTION: Initializes and trains a custom N-HiTS model with specified parameters on the preprocessed data, setting up quantile levels for probabilistic forecasting.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/advanced/forecasting-custom-model.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nmodel = NHITSModel(\n    prediction_length=prediction_length,\n    target=target,\n    covariate_metadata=feature_generator.covariate_metadata,\n    freq=data.freq,\n    quantile_levels=[0.1, 0.5, 0.9],\n)\nmodel.fit(train_data=data, time_limit=20)\n```\n\n----------------------------------------\n\nTITLE: Using Custom AutoGluon Scorer\nDESCRIPTION: Examples of using the custom AutoGluon scorer to calculate scores and errors\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-metric.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# score\nag_accuracy_scorer(y_true, y_pred)\n```\n\nLANGUAGE: python\nCODE:\n```\nag_accuracy_scorer.score(y_true, y_pred)\n```\n\nLANGUAGE: python\nCODE:\n```\n# error, error=sign*optimum-score -> error=1*1-score -> error=1-score\nag_accuracy_scorer.error(y_true, y_pred)\n```\n\n----------------------------------------\n\nTITLE: Extracting Embeddings\nDESCRIPTION: Extracting embeddings from the trained model for the test data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal_text_tabular.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nembeddings = predictor.extract_embedding(test_data)\nembeddings.shape\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon TimeSeries Dependencies with UV in Python\nDESCRIPTION: This code block installs the necessary dependencies for AutoGluon TimeSeries using the UV package manager. It also uninstalls some potentially conflicting packages to ensure compatibility.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-indepth.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# We use uv for faster installation\n!pip install uv\n!uv pip install -q autogluon.timeseries --system\n!uv pip uninstall -q torchaudio torchvision torchtext --system # fix incompatible package versions on Colab\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing Image Data\nDESCRIPTION: Preprocesses the image column to handle single images and updates file paths to absolute paths.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-multimodal.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntrain_data[image_col] = train_data[image_col].apply(lambda ele: ele.split(';')[0])\ntest_data[image_col] = test_data[image_col].apply(lambda ele: ele.split(';')[0])\n\ndef path_expander(path, base_folder):\n    path_l = path.split(';')\n    return ';'.join([os.path.abspath(os.path.join(base_folder, path)) for path in path_l])\n\ntrain_data[image_col] = train_data[image_col].apply(lambda ele: path_expander(ele, base_folder=dataset_path))\ntest_data[image_col] = test_data[image_col].apply(lambda ele: path_expander(ele, base_folder=dataset_path))\n```\n\n----------------------------------------\n\nTITLE: Obtaining Prediction Probabilities\nDESCRIPTION: This code shows how to get prediction probabilities for the identified entities in a given sentence.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/ner.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npredictions = predictor.predict_proba({'text_snippet': [sentence]})\nprint(predictions[0][0]['probability'])\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon from Source using Console Commands\nDESCRIPTION: This snippet shows the complete process for installing AutoGluon from source. It updates pip and setuptools, clones the GitHub repository, and runs the full installation script to set up all dependencies and components.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/install-mac-cpu-source.md#2025-04-22_snippet_0\n\nLANGUAGE: console\nCODE:\n```\npip install -U pip\npip install -U setuptools wheel\n\ngit clone https://github.com/autogluon/autogluon\ncd autogluon && ./full_install.sh\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset for TabularPredictor\nDESCRIPTION: Loads and prepares the Adult Income dataset for demonstration with TabularPredictor.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-metric.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.tabular import TabularDataset\n\ntrain_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv')\ntest_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv')\nlabel = 'class'\ntrain_data = train_data.sample(n=1000, random_state=0)\n\ntrain_data.head(5)\n```\n\n----------------------------------------\n\nTITLE: Cache Management Functions\nDESCRIPTION: Sets up transformer cache location and provides a function to clear the cache directory.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/efficient_finetuning_basic.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport shutil\nos.environ[\"TRANSFORMERS_CACHE\"] = \"cache\"\n\ndef clear_cache():\n    if os.path.exists(\"cache\"):\n        shutil.rmtree(\"cache\")\n\nclear_cache()\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon and Dependencies\nDESCRIPTION: Commands to install AutoGluon, MMCV, MMDet, and other required packages for object detection tasks.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/quick_start/quick_start_coco.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install autogluon.multimodal\n!pip install -U pip setuptools wheel\n!sudo apt-get install -y ninja-build gcc g++\n!python3 -m mim install \"mmcv==2.1.0\"\n!python3 -m pip install \"mmdet==3.2.0\"\n!python3 -m pip install \"mmengine>=0.10.6\"\n```\n\n----------------------------------------\n\nTITLE: Mercari Price Suggestion with AutoGluon\nDESCRIPTION: Prepares data and runs AutoGluon TextPredictor for the Kaggle Mercari Price Suggestion competition. Includes examples for single model, weighted ensemble, and stacking modes.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/text_prediction/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install -y p7zip-full\nbash prepare_mercari_kaggle.sh\n\nmkdir -p ag_mercari_price_single\npython3 run_competition.py --train_file mercari_price/train.tsv \\\n                           --test_file mercari_price/test_stg2.tsv \\\n                           --sample_submission mercari_price/sample_submission_stg2.csv \\\n                           --task mercari_price \\\n                           --eval_metric r2 \\\n                           --exp_dir ag_mercari_price_single \\\n                           --mode single 2>&1  | tee -a ag_mercari_price_single/log.txt\n\nmkdir -p ag_mercari_price_weighted\npython3 run_competition.py --train_file mercari_price/train.tsv \\\n                           --test_file mercari_price/test_stg2.tsv \\\n                           --sample_submission mercari_price/sample_submission_stg2.csv \\\n                           --task mercari_price \\\n                           --eval_metric r2 \\\n                           --exp_dir ag_mercari_price_weighted \\\n                           --mode weighted 2>&1  | tee -a ag_mercari_price_weighted/log.txt\n\nmkdir -p ag_mercari_price_stacking\npython3 run_competition.py --train_file mercari_price/train.tsv \\\n                           --test_file mercari_price/test_stg2.tsv \\\n                           --sample_submission mercari_price/sample_submission_stg2.csv \\\n                           --task mercari_price \\\n                           --eval_metric r2 \\\n                           --exp_dir ag_mercari_price_stacking \\\n                           --mode stacking 2>&1  | tee -a ag_mercari_price_stacking/log.txt\n```\n\n----------------------------------------\n\nTITLE: Training with High Quality Preset\nDESCRIPTION: Initializes a MultiModalPredictor with high_quality preset and fits it on the training data with a 20-second time limit.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/presets.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\n\npredictor = MultiModalPredictor(label='label', eval_metric='acc', presets=\"high_quality\")\npredictor.fit(\n    train_data=train_data,\n    time_limit=20, # seconds\n)\n```\n\n----------------------------------------\n\nTITLE: Regression Model Training\nDESCRIPTION: Loads Ames Housing dataset, selects relevant features, and performs regression model fitting with bagging.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-quick-fit.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf_train = pd.read_csv('https://autogluon.s3.amazonaws.com/datasets/AmesHousingPriceRegression/train_data.csv')\ndf_test = pd.read_csv('https://autogluon.s3.amazonaws.com/datasets/AmesHousingPriceRegression/test_data.csv')\ntarget_col = 'SalePrice'\n\nkeep_cols = [\n  'Overall.Qual', 'Gr.Liv.Area', 'Neighborhood', 'Total.Bsmt.SF', 'BsmtFin.SF.1',\n  'X1st.Flr.SF', 'Bsmt.Qual', 'Garage.Cars', 'Half.Bath', 'Year.Remod.Add', target_col\n]\n\ndf_train = df_train[[c for c in df_train.columns if c in keep_cols]][:500]\ndf_test = df_test[[c for c in df_test.columns if c in keep_cols]][:500]\n\n\nstate = auto.quick_fit(df_train, target_col, fit_bagging_folds=3, return_state=True)\n```\n\n----------------------------------------\n\nTITLE: Evaluating High Quality Model\nDESCRIPTION: Evaluates the high quality model on the test data using ROC AUC metric.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/presets.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nscores = predictor.evaluate(test_data, metrics=[\"roc_auc\"])\nscores\n```\n\n----------------------------------------\n\nTITLE: Loading and Analyzing Regression Dataset (Ames Housing)\nDESCRIPTION: Loads the Ames Housing dataset and performs target analysis with specific distribution fitting parameters for regression analysis\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-target-analysis.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf_train = pd.read_csv('https://autogluon.s3.amazonaws.com/datasets/AmesHousingPriceRegression/train_data.csv')\ndf_test = pd.read_csv('https://autogluon.s3.amazonaws.com/datasets/AmesHousingPriceRegression/test_data.csv')\ntarget_col = 'SalePrice'\n\nauto.target_analysis(\n    train_data=df_train, label=target_col, \n    # Optional; default will try to fit all available distributions\n    fit_distributions=['laplace_asymmetric', 'johnsonsu', 'exponnorm']  \n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Numerical Data Scaling to Unit Variance in AutoMM Python\nDESCRIPTION: Determines if numerical features (excluding labels) should be scaled to have unit variance. Set `data.numerical.scaler_with_std` to `True` (default) to enable scaling or `False` to disable it. Pass this setting within the `hyperparameters` dictionary to `predictor.fit()`.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_62\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"data.numerical.scaler_with_std\": True})\n# turn off scaling\npredictor.fit(hyperparameters={\"data.numerical.scaler_with_std\": False})\n```\n\n----------------------------------------\n\nTITLE: Downloading Cross-Lingual Amazon Review Dataset\nDESCRIPTION: Downloads and extracts the cross-lingual Amazon product review sentiment dataset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/efficient_finetuning_basic.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!wget --quiet https://automl-mm-bench.s3.amazonaws.com/multilingual-datasets/amazon_review_sentiment_cross_lingual.zip -O amazon_review_sentiment_cross_lingual.zip\n!unzip -q -o amazon_review_sentiment_cross_lingual.zip -d .\n```\n\n----------------------------------------\n\nTITLE: Evaluating Sentence Similarity Model Performance\nDESCRIPTION: This code evaluates the trained model's performance on the test dataset, computing RMSE, Pearson Correlation, and Spearman Correlation metrics.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/beginner_text.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntest_score = predictor_sts.evaluate(sts_test_data, metrics=['rmse', 'pearsonr', 'spearmanr'])\nprint('RMSE = {:.2f}'.format(test_score['rmse']))\nprint('PEARSONR = {:.4f}'.format(test_score['pearsonr']))\nprint('SPEARMANR = {:.4f}'.format(test_score['spearmanr']))\n```\n\n----------------------------------------\n\nTITLE: Default Model Names Mapping\nDESCRIPTION: Dictionary mapping model classes to their default names when trained. This provides the standard naming conventions for various model types in AutoGluon.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/api/autogluon.tabular.models.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nDEFAULT_MODEL_NAMES = {\n    RFModel: 'RandomForest',\n    XTModel: 'ExtraTrees',\n    KNNModel: 'KNeighbors',\n    LGBModel: 'LightGBM',\n    CatBoostModel: 'CatBoost',\n    XGBoostModel: 'XGBoost',\n    TabularNeuralNetTorchModel: 'NeuralNetTorch',\n    LinearModel: 'LinearModel',\n    NNFastAiTabularModel: 'NeuralNetFastAI',\n    TextPredictorModel: 'TextPredictor',\n    ImagePredictorModel: 'ImagePredictor',\n    MultiModalPredictorModel: 'MultiModalPredictor',\n\n    FTTransformerModel: 'FTTransformer',\n    TabPFNModel: 'TabPFN',\n\n    FastTextModel: 'FastText',\n    GreedyWeightedEnsembleModel: 'WeightedEnsemble',\n    SimpleWeightedEnsembleModel: 'WeightedEnsemble',\n\n    # Interpretable models\n    RuleFitModel: 'RuleFit',\n    GreedyTreeModel: 'GreedyTree',\n    FigsModel: 'Figs',\n    HSTreeModel: 'HierarchicalShrinkageTree',\n    BoostedRulesModel: 'BoostedRules',\n}\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Time Series Dependencies\nDESCRIPTION: Installs the required AutoGluon time series package and uninstalls incompatible torch-related packages using uv for faster installation.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-quick-start.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# We use uv for faster installation\n!pip install uv\n!uv pip install -q autogluon.timeseries --system\n!uv pip uninstall -q torchaudio torchvision torchtext --system # fix incompatible package versions on Colab\n```\n\n----------------------------------------\n\nTITLE: Feature Pruning Configuration\nDESCRIPTION: Example of enabling automated feature pruning in TabularPredictor using feature_prune_kwargs.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/whats_new/v0.4.0.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nTabularPredictor.fit(..., feature_prune_kwargs={})\n```\n\n----------------------------------------\n\nTITLE: Feature Metadata Configuration\nDESCRIPTION: Sets up feature metadata with user overrides for specific columns to prevent preprocessing\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-model-advanced.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntrain_data['dummy_feature'] = 'dummy value'\ntest_data['dummy_feature'] = 'dummy value'\n\nfrom autogluon.tabular import FeatureMetadata\nfeature_metadata = FeatureMetadata.from_df(train_data)\n\nprint('Before inserting overrides:')\nprint(feature_metadata)\n\nfeature_metadata = feature_metadata.add_special_types(\n    {\n        'age': ['user_override'],\n        'native-country': ['user_override'],\n        'dummy_feature': ['user_override'],\n    }\n)\n\nprint('After inserting overrides:')\nprint(feature_metadata)\n```\n\n----------------------------------------\n\nTITLE: Explicitly Marking a Column as Categorical in Python\nDESCRIPTION: This code demonstrates how to explicitly mark a column as categorical in the DataFrame before applying feature engineering.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-feature-engineering.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndfx[\"B\"] = dfx[\"B\"].astype(\"category\")\nauto_ml_pipeline_feature_generator = AutoMLPipelineFeatureGenerator()\nauto_ml_pipeline_feature_generator.fit_transform(X=dfx)\n```\n\n----------------------------------------\n\nTITLE: Initializing FT-Transformer with Pre-trained Weights in AutoMM\nDESCRIPTION: Specifies the checkpoint for initializing the FT-Transformer backbone. This can be a local file path or a URL to pre-trained weights, set using the 'model.ft_transformer.checkpoint_name' hyperparameter.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_44\n\nLANGUAGE: python\nCODE:\n```\n# by default, AutoMM doesn't use pre-trained weights\npredictor.fit(hyperparameters={\"model.ft_transformer.checkpoint_name\": None})\n# initialize the ft_transformer backbone from local checkpoint\npredictor.fit(hyperparameters={\"model.ft_transformer.checkpoint_name\": 'my_checkpoint.ckpt'})\n# initialize the ft_transformer backbone from url of checkpoint\npredictor.fit(hyperparameters={\"model.ft_transformer.checkpoint_name\": 'https://automl-mm-bench.s3.amazonaws.com/ft_transformer_pretrained_ckpt/iter_2k.ckpt'})\n```\n\n----------------------------------------\n\nTITLE: Running AutoMM Distillation for PAWS-X Task with Best Performance Settings\nDESCRIPTION: Bash command to reproduce the best model for PAWS-X task distillation, specifying precision and various loss weights.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/distillation/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 automm_distillation_pawsx.py --precision 16 \\\n                                     --output_feature_loss_weight 0.01 \\\n                                     --softmax_regression_weight 0.1 \\\n                                     --rkd_distance_loss_weight 1 \\\n                                     --rkd_angle_loss_weight 2\n```\n\n----------------------------------------\n\nTITLE: Expanding Document File Paths\nDESCRIPTION: Converts relative document paths to absolute paths using the path_expander utility to ensure documents can be properly loaded during training.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/document_prediction/document_classification.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal.utils.misc import path_expander\n\nDOC_PATH_COL = \"doc_path\"\n\ntrain_data[DOC_PATH_COL] = train_data[DOC_PATH_COL].apply(lambda ele: path_expander(ele, base_folder=download_dir))\ntest_data[DOC_PATH_COL] = test_data[DOC_PATH_COL].apply(lambda ele: path_expander(ele, base_folder=download_dir))\nprint(test_data.head())\n```\n\n----------------------------------------\n\nTITLE: Configuring Numerical to Text Conversion in AutoMM Python\nDESCRIPTION: Controls whether numerical features should be converted and treated as text data. Setting this to `True` disables the use of numerical-specific models like `\"numerical_mlp\"` or `\"numerical_transformer\"`. The default value is `False`. Configure this via the `hyperparameters` dictionary in `predictor.fit()`.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_60\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"data.numerical.convert_to_text\": False})\n# turn on the conversion\npredictor.fit(hyperparameters={\"data.numerical.convert_to_text\": True})\n```\n\n----------------------------------------\n\nTITLE: Generating Automated Dataset Overview with AutoGluon EDA\nDESCRIPTION: Uses AutoGluon's EDA module to generate a comprehensive dataset overview. This includes statistical information, feature types detection, missing value counts, and feature distance analysis.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-dataset-overview.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport autogluon.eda.auto as auto\n\nauto.dataset_overview(train_data=df_train, test_data=df_test, label=target_col)\n```\n\n----------------------------------------\n\nTITLE: Initializing AutoMM Predictor\nDESCRIPTION: Sets up the MultiModal Predictor for image-text similarity matching\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/image_text_matching.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\npredictor = MultiModalPredictor(\n            query=text_col,\n            response=image_col,\n            problem_type=\"image_text_similarity\",\n            eval_metric=\"recall\",\n        )\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Multimodal in Python\nDESCRIPTION: This command installs the AutoGluon Multimodal package using pip.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal_ner.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.multimodal\n```\n\n----------------------------------------\n\nTITLE: Deploying Docker Images to ECR for AutoGluon CI\nDESCRIPTION: Shell commands for deploying updated Docker images to ECR. The script requires setting an environment variable for the ECR repository address and then running the docker_deploy.sh script with either 'cpu' or 'gpu' parameter to build, tag and push the appropriate image.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/CI/batch/docker/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# First export your ecr repo address as a environment variable\nexport AWS_ECR_REPO=${your_repo}\n\n# Following script will build, tag, and push the image\n# For cpu\n./docker_deploy.sh cpu\n# For gpu\n./docker_deploy.sh gpu\n```\n\n----------------------------------------\n\nTITLE: Loading Semantic Textual Similarity Dataset\nDESCRIPTION: Loads the Semantic Textual Similarity Benchmark dataset for a regression task to demonstrate transfer learning from the previously trained classification model.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/continuous_training.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsts_train_data = load_pd.load(\"https://autogluon-text.s3-accelerate.amazonaws.com/glue/sts/train.parquet\")[\n    [\"sentence1\", \"sentence2\", \"score\"]\n]\nsts_test_data = load_pd.load(\"https://autogluon-text.s3-accelerate.amazonaws.com/glue/sts/dev.parquet\")[\n    [\"sentence1\", \"sentence2\", \"score\"]\n]\nsts_train_data.head(10)\n```\n\n----------------------------------------\n\nTITLE: Configuring Soft Label Loss Weight for Distillation in AutoMM Python\nDESCRIPTION: Specifies the weight applied to the student model's distillation loss calculated using the teacher model's soft labels (logits). The final soft label loss is multiplied by this weight. The default `distiller.soft_label_weight` for classification is `50`. Configure this via the `hyperparameters` argument in `predictor.fit()`.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_77\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM for classification\npredictor.fit(hyperparameters={\"distiller.soft_label_weight\": 50})\n# set not to scale the soft label loss\npredictor.fit(hyperparameters={\"distiller.soft_label_weight\": 1})\n```\n\n----------------------------------------\n\nTITLE: Building a MultiModalPredictor for Image Regression in Python\nDESCRIPTION: This code snippet shows how to instantiate a MultiModalPredictor for a regression problem. It specifies the target label, problem type, evaluation metric, save path, and verbosity level.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/kaggle_pawpularity/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npredictor = MultiModalPredictor(\n\tlabel=\"Pawpularity\", \n\tproblem_type=\"regression\", \n\teval_metric=\"rmse\", \n\tpath=save_path,  \n\tverbosity=4, \n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Python Environment with Required Libraries\nDESCRIPTION: Sets up the Python environment by importing necessary libraries, suppressing warnings, and setting a random seed for reproducibility.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal-quick-start.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport warnings\n\nimport numpy as np\n\nwarnings.filterwarnings('ignore')\nnp.random.seed(123)\n```\n\n----------------------------------------\n\nTITLE: Creating SHAP Dependence Plot\nDESCRIPTION: Generates dependence plot to visualize relationship between education years and prediction outcome.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/interpret/SHAP with AutoGluon-Tabular Census income classification.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nshap.dependence_plot(\"Education-Num\", shap_values, X_valid.iloc[0:N_VAL,:])\n```\n\n----------------------------------------\n\nTITLE: Evaluating the Initial Sentiment Analysis Model\nDESCRIPTION: Evaluates the trained MultiModalPredictor on a test dataset to assess its performance. The test data has the same format as the training data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/continuous_training.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntest_score = predictor.evaluate(test_data)\nprint(test_score)\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing Dataset\nDESCRIPTION: Loads CSV files and preprocesses image paths for the PetFinder dataset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/tensorrt.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\ndataset_path = download_dir + '/petfinder_for_tutorial'\ntrain_data = pd.read_csv(f'{dataset_path}/train.csv', index_col=0)\ntest_data = pd.read_csv(f'{dataset_path}/test.csv', index_col=0)\nlabel_col = 'AdoptionSpeed'\n```\n\nLANGUAGE: python\nCODE:\n```\nimage_col = 'Images'\ntrain_data[image_col] = train_data[image_col].apply(lambda ele: ele.split(';')[0])\ntest_data[image_col] = test_data[image_col].apply(lambda ele: ele.split(';')[0])\n\ndef path_expander(path, base_folder):\n    path_l = path.split(';')\n    return ';'.join([os.path.abspath(os.path.join(base_folder, path)) for path in path_l])\n\ntrain_data[image_col] = train_data[image_col].apply(lambda ele: path_expander(ele, base_folder=dataset_path))\ntest_data[image_col] = test_data[image_col].apply(lambda ele: path_expander(ele, base_folder=dataset_path))\n```\n\n----------------------------------------\n\nTITLE: Analyzing Three-Way Interaction\nDESCRIPTION: Visualizes the three-way interaction between 'Fare', 'Age', and 'Survived' features.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-analyze-interaction.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nauto.analyze_interaction(x='Fare', y='Age', hue='Survived', train_data=df_train, test_data=df_test)\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon PR Branch from Source\nDESCRIPTION: Instructions for installing a specific pull request branch of AutoGluon from source. Useful for testing PRs or reviewing code changes.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/install.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Edit these two variables to change which PR / branch is being installed\nGITHUB_USER=innixma\nBRANCH=accel_preprocess_bool\n\npip install -U pip\ngit clone --depth 1 --single-branch --branch ${BRANCH} --recurse-submodules https://github.com/${GITHUB_USER}/autogluon.git\ncd autogluon && ./full_install.sh\n```\n\n----------------------------------------\n\nTITLE: Defining Helper Function for Problem Type Information\nDESCRIPTION: This function prints formatted information about a problem type, including supported modalities, evaluation metrics, and special capabilities.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/problem_types_and_metrics.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal.constants import *\nfrom autogluon.multimodal.problem_types import PROBLEM_TYPES_REG\n\ndef print_problem_type_info(name: str, props):\n    \"\"\"Helper function to print problem type information\"\"\"\n    print(f\"\\n=== {name} ===\")\n    \n    print(\"\\nSupported Input Modalities:\")\n    # Convert set to sorted list for complete display\n    for modality in sorted(list(props.supported_modality_type)):\n        print(f\"- {modality}\")\n        \n    if hasattr(props, 'supported_evaluation_metrics') and props.supported_evaluation_metrics:\n        print(\"\\nEvaluation Metrics:\")\n        # Convert to sorted list to ensure complete and consistent display\n        for metric in sorted(list(props.supported_evaluation_metrics)):\n            if metric == props.fallback_evaluation_metric:\n                print(f\"- {metric} (default)\")\n            else:\n                print(f\"- {metric}\")\n                \n    if hasattr(props, 'support_zero_shot'):\n        print(\"\\nSpecial Capabilities:\")\n        print(f\"- Zero-shot prediction: {'Supported' if props.support_zero_shot else 'Not supported'}\")\n        print(f\"- Training support: {'Supported' if props.support_fit else 'Not supported'}\")\n```\n\n----------------------------------------\n\nTITLE: Setting Gradient Clipping Algorithm in AutoMM\nDESCRIPTION: Examples showing how to set the gradient clipping algorithm in AutoMM, with options to clip gradients by norm (default) or value.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"optim.gradient_clip_algorithm\": \"norm\"})\n# clip gradients by value\npredictor.fit(hyperparameters={\"optim.gradient_clip_algorithm\": \"value\"})\n```\n\n----------------------------------------\n\nTITLE: Running AutoMM Distillation for GLUE Task with Best Performance Settings\nDESCRIPTION: Bash command to reproduce the best performance for GLUE task distillation using specific model and parameter settings.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/distillation/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 automm_distillation.py --teacher_model google/bert_uncased_L-12_H-768_A-12 \\\n                                --student_model google/bert_uncased_L-6_H-768_A-12 \\\n                                --seed 123 \\\n                                --max_epoch 8 \\\n                                --hard_label_weight 0.5 \\\n                                --soft_label_weight 5\n```\n\n----------------------------------------\n\nTITLE: Expanding Document Paths for MultiModalPredictor\nDESCRIPTION: Prepares the document paths by expanding them to full paths so that MultiModalPredictor can correctly locate the PDF documents.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/document_prediction/pdf_classification.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal.utils.misc import path_expander\n\nDOC_PATH_COL = \"doc_path\"\n\ntrain_data[DOC_PATH_COL] = train_data[DOC_PATH_COL].apply(lambda ele: path_expander(ele, base_folder=download_dir))\ntest_data[DOC_PATH_COL] = test_data[DOC_PATH_COL].apply(lambda ele: path_expander(ele, base_folder=download_dir))\nprint(test_data.head())\n```\n\n----------------------------------------\n\nTITLE: Configuring Cutmix Alpha Parameter in AutoMM Python\nDESCRIPTION: Sets the alpha parameter for the Cutmix data augmentation technique. Cutmix becomes active if `data.mixup.cutmix_alpha` is greater than 0. The default value is `1.0`, which means Cutmix is off by default. Set a value between 0 and 1 (e.g., `0.8`) to enable it via the `hyperparameters` argument in `predictor.fit()`.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_68\n\nLANGUAGE: python\nCODE:\n```\n# by default, Cutmix is turned off by using alpha 1.0\npredictor.fit(hyperparameters={\"data.mixup.cutmix_alpha\": 1.0})\n# turn it on by choosing a number in range (0, 1)\npredictor.fit(hyperparameters={\"data.mixup.cutmix_alpha\": 0.8})\n```\n\n----------------------------------------\n\nTITLE: Downloading Flickr30K Dataset\nDESCRIPTION: Downloads and extracts the Flickr30K dataset for image-text matching\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/image_text_matching.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.core.utils.loaders import load_pd\nimport pandas as pd\ndownload_dir = './ag_automm_tutorial_imgtxt'\nzip_file = 'https://automl-mm-bench.s3.amazonaws.com/flickr30k.zip'\nfrom autogluon.core.utils.loaders import load_zip\nload_zip.unzip(zip_file, unzip_dir=download_dir)\n```\n\n----------------------------------------\n\nTITLE: Initializing MultiModalPredictor for Image-Text Pair Matching in Python\nDESCRIPTION: Creates a MultiModalPredictor instance for predicting whether image-text pairs match.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/zero_shot_img_txt_matching.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npredictor = MultiModalPredictor(\n            query=\"abc\",\n            response=\"xyz\",\n            problem_type=\"image_text_similarity\",\n        )\n```\n\n----------------------------------------\n\nTITLE: Making Predictions\nDESCRIPTION: Demonstrates making predictions and getting class probabilities.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/beginner_multimodal.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npredictions = predictor.predict(test_data.drop(columns=label_col))\npredictions[:5]\n\nprobas = predictor.predict_proba(test_data.drop(columns=label_col))\nprobas[:5]\n```\n\n----------------------------------------\n\nTITLE: Running All Unit Tests with pytest\nDESCRIPTION: Command to execute all unit tests in the 'unittests/' directory using pytest, ensuring comprehensive testing of the project's components.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/multimodal/tests/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npytest unittests/\n```\n\n----------------------------------------\n\nTITLE: Loading Classification Dataset (Titanic)\nDESCRIPTION: Loads the Titanic dataset from S3 using pandas and defines the target column for classification analysis\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-target-analysis.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndf_train = pd.read_csv('https://autogluon.s3.amazonaws.com/datasets/titanic/train.csv')\ndf_test = pd.read_csv('https://autogluon.s3.amazonaws.com/datasets/titanic/test.csv')\ntarget_col = 'Survived'\n```\n\n----------------------------------------\n\nTITLE: Generating Future Known Covariates\nDESCRIPTION: Creates future data frame with weekend indicators for prediction horizon.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-indepth.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npredictor = TimeSeriesPredictor(prediction_length=14, freq=train_data.freq)\n\nknown_covariates = predictor.make_future_data_frame(train_data)\nknown_covariates[\"weekend\"] = known_covariates[\"timestamp\"].dt.weekday.isin(WEEKEND_INDICES).astype(float)\n\nknown_covariates.head()\n```\n\n----------------------------------------\n\nTITLE: Generating Object Documentation in Sphinx RST Format\nDESCRIPTION: Template for generating documentation using Sphinx restructured text format. Uses template variables for object name, module name and object type that get populated during documentation build.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/_templates/autosummary/base.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n{{ objname | escape | underline}}\n\n.. currentmodule:: {{ module }}\n\n.. auto{{ objtype }}:: {{ objname }}\n```\n\n----------------------------------------\n\nTITLE: Loading Shopee-IET Dataset with Image Bytearrays\nDESCRIPTION: Loads the Shopee-IET dataset with image bytearrays instead of file paths.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_prediction/beginner_image_cls.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndownload_dir = './ag_automm_tutorial_imgcls'\ntrain_data_byte, test_data_byte = shopee_dataset(download_dir, is_bytearray=True)\n```\n\n----------------------------------------\n\nTITLE: Loading Census Dataset\nDESCRIPTION: Loads the adult census dataset using SHAP's built-in dataset loader and splits it into training and validation sets.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/interpret/SHAP with AutoGluon-Tabular Census income classification.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nX,y = shap.datasets.adult()\nX_display,y_display = shap.datasets.adult(display=True)\nX_train, X_valid, y_train, y_valid = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=7)\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon with Conda and Mamba\nDESCRIPTION: This snippet shows the process of creating a Conda environment named 'ag' with Python 3.11, activating it, installing Mamba, and then using Mamba to install AutoGluon from the conda-forge channel. This approach ensures a clean environment for AutoGluon and uses Mamba for faster package resolution and installation.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/install-mac-conda.md#2025-04-22_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nconda create -n ag python=3.11\nconda activate ag\nconda install -c conda-forge mamba\nmamba install -c conda-forge autogluon\n```\n\n----------------------------------------\n\nTITLE: Installing TensorRT Dependencies\nDESCRIPTION: Checks for and installs required TensorRT, ONNX, and ONNX Runtime packages if not already present.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/tensorrt.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    import tensorrt, onnx, onnxruntime\n    print(f\"tensorrt=={tensorrt.__version__}, onnx=={onnx.__version__}, onnxruntime=={onnxruntime.__version__}\")\nexcept ImportError:\n    !pip install autogluon.multimodal[tests]\n    !pip install -U \"tensorrt>=10.0.0b0,<11.0\"\n    clear_output()\n```\n\n----------------------------------------\n\nTITLE: Setting GPU Count in AutoMM\nDESCRIPTION: Examples showing how to specify the number of GPUs to use in AutoMM, with -1 to use all available GPUs or a specific number like 1.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# by default, all available gpus are used by AutoMM\npredictor.fit(hyperparameters={\"env.num_gpus\": -1})\n# use 1 gpu only\npredictor.fit(hyperparameters={\"env.num_gpus\": 1})\n```\n\n----------------------------------------\n\nTITLE: Setting Gradient Clipping Value in AutoMM\nDESCRIPTION: Examples showing how to set the gradient clipping value in AutoMM, which caps gradients at the specified absolute value or norm depending on the clipping algorithm.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"optim.gradient_clip_val\": 1})\n# cap the gradients to 5\npredictor.fit(hyperparameters={\"optim.gradient_clip_val\": 5})\n```\n\n----------------------------------------\n\nTITLE: Specifying Lightweight Hyperparameters in AutoGluon\nDESCRIPTION: Configures the TabularPredictor to train models with lightweight hyperparameters in order to produce smaller, faster models with reduced accuracy in AutoGluon.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-indepth.ipynb#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\npredictor_light = TabularPredictor(label=label, eval_metric=metric).fit(train_data, hyperparameters='very_light', time_limit=30)\n```\n\n----------------------------------------\n\nTITLE: Freezing Layers in AutoMM MMDetection Models\nDESCRIPTION: Specifies which layers to freeze in the MMDetection model using the 'model.mmdet_image.frozen_layers' hyperparameter. Layers containing the specified substrings will be frozen during training.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_51\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM, freeze nothing and update all parameters\npredictor = MultiModalPredictor(hyperparameters={\"model.mmdet_image.frozen_layers\": []})\n# freeze the model's backbone\npredictor = MultiModalPredictor(hyperparameters={\"model.mmdet_image.frozen_layers\": [\"backbone\"]})\n# freeze the model's backbone and neck\npredictor = MultiModalPredictor(hyperparameters={\"model.mmdet_image.frozen_layers\": [\"backbone\", \"neck\"]})\n```\n\n----------------------------------------\n\nTITLE: Visualizing NER Example\nDESCRIPTION: Visualizes the first example from the training dataset to show entity annotations.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/chinese_ner.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nvisualize_ner(train_data[\"text_snippet\"].iloc[0], train_data[\"entity_annotations\"].iloc[0])\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Pothole Dataset\nDESCRIPTION: Downloads the Pothole dataset ZIP file and extracts it, setting up paths for train, validation, and test splits.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/advanced/finetune_coco.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nzip_file = \"https://automl-mm-bench.s3.amazonaws.com/object_detection/dataset/pothole.zip\"\ndownload_dir = \"./pothole\"\n\nload_zip.unzip(zip_file, unzip_dir=download_dir)\ndata_dir = os.path.join(download_dir, \"pothole\")\ntrain_path = os.path.join(data_dir, \"Annotations\", \"usersplit_train_cocoformat.json\")\nval_path = os.path.join(data_dir, \"Annotations\", \"usersplit_val_cocoformat.json\")\ntest_path = os.path.join(data_dir, \"Annotations\", \"usersplit_test_cocoformat.json\")\n```\n\n----------------------------------------\n\nTITLE: Loading Training Data\nDESCRIPTION: Loads the training dataset from a URL using TabularDataset and displays the first few rows\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-quick-start.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndata_url = 'https://raw.githubusercontent.com/mli/ag-docs/main/knot_theory/'\ntrain_data = TabularDataset(f'{data_url}train.csv')\ntrain_data.head()\n```\n\n----------------------------------------\n\nTITLE: Displaying Rows with Missing Embarked Values\nDESCRIPTION: Shows the rows in the training dataset where the 'Embarked' feature has null values.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-analyze-interaction.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf_train[df_train.Embarked.isna()]\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon MultiModal Package\nDESCRIPTION: Installation command for the AutoGluon multimodal package using pip.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/multilingual_text.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.multimodal\n```\n\n----------------------------------------\n\nTITLE: Downloading Multilingual Dataset\nDESCRIPTION: Downloads and extracts the Amazon review sentiment cross-lingual dataset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/multilingual_text.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!wget --quiet https://automl-mm-bench.s3.amazonaws.com/multilingual-datasets/amazon_review_sentiment_cross_lingual.zip -O amazon_review_sentiment_cross_lingual.zip\n!unzip -q -o amazon_review_sentiment_cross_lingual.zip -d .\n```\n\n----------------------------------------\n\nTITLE: Displaying Example Tweet Data in Python\nDESCRIPTION: This snippet selects and displays an example row from the training data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal_ner.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nexample_row = train_data.iloc[0]\n\nexample_row\n```\n\n----------------------------------------\n\nTITLE: Configuring Dynamic Shape Tracing\nDESCRIPTION: Setting for dynamic shape tracing in PyTorch model compilation.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"env.compile.dynamic\": True})\n# assumes a static input shape across mini-batches.\npredictor.fit(hyperparameters={\"env.compile.dynamic\": False})\n```\n\n----------------------------------------\n\nTITLE: Configuring Text Pooling Mode\nDESCRIPTION: Setting the feature pooling mode for transformer architectures, choosing between CLS token or mean pooling.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_35\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"model.hf_text.pooling_mode\": \"cls\"})\n# using the mean pooling\npredictor.fit(hyperparameters={\"model.hf_text.pooling_mode\": \"mean\"})\n```\n\n----------------------------------------\n\nTITLE: VOC Dataset Folder Structure Example\nDESCRIPTION: Shows the expected folder structure for a Pascal VOC dataset before conversion to COCO format, with Annotations, ImageSets, and JPEGImages folders.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/convert_data_to_coco_format.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n<path_to_VOCdevkit>/\n    VOC2007/\n        Annotations/\n        ImageSets/\n        JPEGImages/\n        labels.txt\n    VOC2012/\n        Annotations/\n        ImageSets/\n        JPEGImages/\n        labels.txt\n    ...\n```\n\n----------------------------------------\n\nTITLE: Accessing Class Labels in AutoGluon\nDESCRIPTION: Demonstrates how to access class labels and positive class information for classification tasks.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-faq.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npredictor.class_labels\npredictor.positive_class\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Time Series Data with TimeSeriesFeatureGenerator\nDESCRIPTION: Applies AutoGluon's preprocessing steps to normalize data types and handle missing values in the covariates using TimeSeriesFeatureGenerator.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/advanced/forecasting-custom-model.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.timeseries.utils.features import TimeSeriesFeatureGenerator\n\nfeature_generator = TimeSeriesFeatureGenerator(target=target, known_covariates_names=known_covariates_names)\ndata = feature_generator.fit_transform(raw_data)\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing Data for AutoGluon TabularPredictor in Python\nDESCRIPTION: Imports necessary modules, loads the AdultIncome dataset, and prepares a subsample for training. This snippet sets up the data for predicting income class.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-deployment.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.tabular import TabularDataset, TabularPredictor\ntrain_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv')\nlabel = 'class'\nsubsample_size = 500  # subsample subset of data for faster demo, try setting this to much larger values\ntrain_data = train_data.sample(n=subsample_size, random_state=0)\ntrain_data.head()\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon via pip\nDESCRIPTION: Command to install AutoGluon package using pip package manager.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/index.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install autogluon\n```\n\n----------------------------------------\n\nTITLE: Selecting TIMM Image Backbone in AutoMM\nDESCRIPTION: Chooses an image backbone from the TIMM (PyTorch Image Models) library using the 'model.timm_image.checkpoint_name' hyperparameter.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_46\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"model.timm_image.checkpoint_name\": \"swin_base_patch4_window7_224\"})\n# choose a vit base\npredictor.fit(hyperparameters={\"model.timm_image.checkpoint_name\": \"vit_base_patch32_224\"})\n```\n\n----------------------------------------\n\nTITLE: Loading Stanford Sentiment Treebank (SST) Dataset\nDESCRIPTION: Loads the SST dataset from parquet files stored in S3 for sentiment analysis, displaying a sample of test records.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/tabular-fasttext.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntrain_data = pd.read_parquet('https://autogluon-text.s3.amazonaws.com/glue/sst/train.parquet')\ntest_data = pd.read_parquet('https://autogluon-text.s3.amazonaws.com/glue/sst/dev.parquet')\ntest_data.sample(5).to_dict(orient='records')\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon and Dependencies via pip\nDESCRIPTION: This snippet shows the process of installing AutoGluon and its prerequisites. It first upgrades pip and setuptools, then installs AutoGluon with a specific PyTorch dependency source.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/install-cpu-pip.md#2025-04-22_snippet_0\n\nLANGUAGE: console\nCODE:\n```\npip install -U pip\npip install -U setuptools wheel\npip install autogluon --extra-index-url https://download.pytorch.org/whl/cpu\n```\n\n----------------------------------------\n\nTITLE: Preparing Train, Validation, and Test Datasets\nDESCRIPTION: Prepares train, validation, and test datasets by sampling and splitting the QNLI data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/model_distillation.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import train_test_split\n\ntrain_valid_df = dataset[\"train\"].to_pandas()[[\"question\", \"sentence\", \"label\"]].sample(1000, random_state=123)\ntrain_df, valid_df = train_test_split(train_valid_df, test_size=0.2, random_state=123)\ntest_df = dataset[\"validation\"].to_pandas()[[\"question\", \"sentence\", \"label\"]].sample(1000, random_state=123)\n```\n\n----------------------------------------\n\nTITLE: Training Models with TabularDataset in Python\nDESCRIPTION: This snippet shows how to load and preprocess a tabular dataset for training using AutoGluon. The dataset is sampled for quicker demonstration, and the head of the dataset is displayed for verification.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-multilabel.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ntrain_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv')\nsubsample_size = 500  # subsample subset of data for faster demo, try setting this to much larger values\ntrain_data = train_data.sample(n=subsample_size, random_state=0)\ntrain_data.head()\n```\n\n----------------------------------------\n\nTITLE: Initializing Label-Studio Reader for AutoGluon\nDESCRIPTION: Creates a LabelStudioReader object and optionally sets the Label-Studio host address. The default host is set to http://localhost:8080, which is Label-Studio's default local host and port.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/label_studio_export_reader/LabelStudio_export_file_reader.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal.utils import LabelStudioReader\n\n# initialize LabelStudioReader with default localhost host\nls = LabelStudioReader() \n\n# set \nls.set_labelstudio_host(\"http://localhost:8080\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Column Feature Pooling Mode in AutoMM Python\nDESCRIPTION: Determines how features from multiple columns in a dataframe are aggregated into a single feature vector, currently applicable only for `few_shot_classification`. `\"concat\"` (default) concatenates features, while `\"mean\"` computes the average. Set `data.column_features_pooling_mode` within the `hyperparameters` passed to `predictor.fit()`.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_65\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"data.column_features_pooling_mode\": \"concat\"})\n# use the mean pooling\npredictor.fit(hyperparameters={\"data.column_features_pooling_mode\": \"mean\"})\n```\n\n----------------------------------------\n\nTITLE: Configuring SAM Training Transforms in AutoMM\nDESCRIPTION: Sets up image augmentation transforms for SAM training using the 'model.sam.train_transforms' hyperparameter. Currently supports 'random_horizontal_flip'.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_53\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"model.sam.train_transforms\": [\"random_horizontal_flip\"]})\n```\n\n----------------------------------------\n\nTITLE: Evaluating Sentiment Analysis Model Performance\nDESCRIPTION: Makes predictions on the SST test dataset and calculates accuracy using sklearn's accuracy_score function.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/tabular-fasttext.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ny_pred = predictor.predict(test_data)\nprint('accuracy:', accuracy_score(y_pred, test_data[label]))\n```\n\n----------------------------------------\n\nTITLE: Making Predictions\nDESCRIPTION: Generates predictions on the test set using the finetuned model.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/advanced/finetune_coco.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npred = predictor.predict(test_path)\n```\n\n----------------------------------------\n\nTITLE: Printing Semantic Segmentation Problem Type Information\nDESCRIPTION: This code prints information about the semantic segmentation problem type using the helper function.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/problem_types_and_metrics.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Semantic Segmentation\nsegmentation_props = PROBLEM_TYPES_REG.get(SEMANTIC_SEGMENTATION)\nprint_problem_type_info(\"Semantic Segmentation\", segmentation_props)\n```\n\n----------------------------------------\n\nTITLE: Enabling Model Calibration in TabularPredictor\nDESCRIPTION: Configuration example for enabling model calibration in TabularPredictor for classification problems.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/whats_new/v0.4.0.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nTabularPredictor.fit(..., calibrate=True)\n```\n\n----------------------------------------\n\nTITLE: Registering Error Handlers with Codecs for Text Normalization\nDESCRIPTION: Code for registering custom error handlers with the codecs module to manage encoding and decoding errors during text normalization. This enables proper handling of special characters and encoding issues.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/kaggle_feedback_prize/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncodecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\ncodecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Text Length\nDESCRIPTION: Configuration for maximum text length, with option to use model's default maximum length.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_37\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"model.hf_text.max_text_len\": 512})\n# set to use the length allowed by the tokenizer.\npredictor.fit(hyperparameters={\"model.hf_text.max_text_len\": -1})\n```\n\n----------------------------------------\n\nTITLE: Displaying High Fare Outliers\nDESCRIPTION: Shows the rows in the training dataset where the 'Fare' is over $400.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-analyze-interaction.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndf_train[df_train.Fare > 400]\n```\n\n----------------------------------------\n\nTITLE: Configuring Categorical to Text Conversion in AutoMM Python\nDESCRIPTION: Determines if categorical features should be treated as text data. If set to `True`, categorical-specific models like `\"categorical_mlp\"` or `\"categorical_transformer\"` will not be used. The default is `False`. This is set using the `hyperparameters` argument in `predictor.fit()`.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_59\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"data.categorical.convert_to_text\": False})\n# turn on the conversion\npredictor.fit(hyperparameters={\"data.categorical.convert_to_text\": True})\n```\n\n----------------------------------------\n\nTITLE: Configuring Bounding Box Output Format in AutoMM MMDetection\nDESCRIPTION: Sets the output format for bounding boxes in object detection tasks using the 'model.mmdet_image.output_bbox_format' hyperparameter. Supports 'xyxy' and 'xywh' formats.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_50\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor = MultiModalPredictor(hyperparameters={\"model.mmdet_image.output_bbox_format\": \"xyxy\"})\n# choose xywh output format\npredictor = MultiModalPredictor(hyperparameters={\"model.mmdet_image.output_bbox_format\": \"xywh\"})\n```\n\n----------------------------------------\n\nTITLE: Selecting Model Types\nDESCRIPTION: Configuration for choosing which types of models to use, including text, image, CLIP, and various MLPs.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_33\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"model.names\": [\"hf_text\", \"timm_image\", \"clip\", \"categorical_mlp\", \"numerical_mlp\", \"fusion_mlp\"]})\n# use only text models\npredictor.fit(hyperparameters={\"model.names\": [\"hf_text\"]})\n# use only image models\npredictor.fit(hyperparameters={\"model.names\": [\"timm_image\"]})\n# use only clip models\npredictor.fit(hyperparameters={\"model.names\": [\"clip\"]})\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating Conda Environment for AutoGluon on Windows\nDESCRIPTION: These commands create a new Conda environment named 'myenv' with Python 3.11 and activate it for AutoGluon installation. This step ensures a clean, isolated environment for AutoGluon dependencies.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/install-windows-cpu.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n myenv python=3.11 -y\nconda activate myenv\n```\n\n----------------------------------------\n\nTITLE: Configuring Text Normalization in AutoMM Python\nDESCRIPTION: Controls whether to apply text normalization to fix potential encoding issues. Setting `data.text.normalize_text` to `True` enables a series of encoding/decoding steps for normalization. The default is `False`. This hyperparameter is configured within the `predictor.fit()` method.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_58\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"data.text.normalize_text\": False})\n# turn on text normalization\npredictor.fit(hyperparameters={\"data.text.normalize_text\": True})\n```\n\n----------------------------------------\n\nTITLE: Analyzing Data Structure in TimeSeriesDataFrame\nDESCRIPTION: Examines the column types and missing values in both the time series data and static features to understand the dataset structure.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/advanced/forecasting-custom-model.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Types of the columns in raw data:\")\nprint(raw_data.dtypes)\nprint(\"\\nTypes of the columns in raw static features:\")\nprint(raw_data.static_features.dtypes)\n\nprint(\"\\nNumber of missing values per column:\")\nprint(raw_data.isna().sum())\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch-Lightning Loggers\nDESCRIPTION: Disables specific PyTorch-Lightning loggers by setting their log level to ERROR to reduce noise in the output.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/advanced/forecasting-custom-model.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfor logger_name in [\n    \"lightning.pytorch.utilities.rank_zero\",\n    \"pytorch_lightning.accelerators.cuda\",\n    \"lightning_fabric.utilities.seed\",\n]:\n    logging.getLogger(logger_name).setLevel(logging.ERROR)\n```\n\n----------------------------------------\n\nTITLE: Markdown Table - Competition Results 2024\nDESCRIPTION: A formatted markdown table documenting competition placements, solution links, authors, dates, and implementation details for AutoGluon usage in Kaggle competitions during 2024.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/AWESOME.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Placement                                | Competition Solution                                                                                                                                    | Author                                                                                                                                     | Date       | AutoGluon Details | Notes                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n|:-----------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------|:-----------|:------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| :2nd_place_medal: Rank 2/2392 (Top 0.1%) | [Regression with an Insurance Dataset](https://www.kaggle.com/competitions/playground-series-s4e12/discussion/554505)                                   | [SCRIPTCHEF](https://www.kaggle.com/noodl35)                                                                                               | 2024/12/31 | v1.2, Tabular     | Kaggle Playground Series S4E12. Also used in [9th](https://www.kaggle.com/competitions/playground-series-s4e12/discussion/554377) and [10th](https://www.kaggle.com/competitions/playground-series-s4e12/discussion/554332) place solutions!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n| :1st_place_medal: Rank 1/2687            | [Exploring Mental Health Data](https://www.kaggle.com/competitions/playground-series-s4e11/discussion/549160)                                           | [Mahdi Ravaghi](https://www.kaggle.com/ravaghi)                                                                                            | 2024/11/30 | v1.1, Tabular     | Kaggle Playground Series S4E11. Also used in [4th](https://www.kaggle.com/competitions/playground-series-s4e11/discussion/549197) and [13th](https://www.kaggle.com/competitions/playground-series-s4e11/discussion/549155) place solutions!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n| Rank 8/3859 (Top 0.3%)                   | [Loan Approval Prediction](https://www.kaggle.com/competitions/playground-series-s4e10/discussion/543772)                                               | [Mahdi Ravaghi](https://www.kaggle.com/ravaghi)                                                                                            | 2024/10/31 | v1.1, Tabular     | Kaggle Playground Series S4E10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n| :1st_place_medal: Rank 1/3066            | [Regression of Used Car Prices](https://www.kaggle.com/competitions/playground-series-s4e9/discussion/537052)                                           | [Mart Preusse](https://www.kaggle.com/martinapreusse)                                                                                      | 2024/09/30 | v1.1, Tabular     | Kaggle Playground Series S4E9. Also used in :2nd_place_medal: [2nd](https://www.kaggle.com/competitions/playground-series-s4e9/discussion/537349), :3rd_place_medal: [3rd](https://www.kaggle.com/competitions/playground-series-s4e9/discussion/537029), [4th](https://www.kaggle.com/competitions/playground-series-s4e9/discussion/536973), and [5th](https://www.kaggle.com/competitions/playground-series-s4e9/discussion/537173) place solutions!                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| :1st_place_medal: Rank 1/1116            | [Kaggle AutoML Grand Prix (Overall)](https://www.kaggle.com/automl-grand-prix)                                                                          | [Alexander R.](https://www.kaggle.com/alexryzhkov), [Dmitry S.](https://www.kaggle.com/simakov), [Rinchin](https://www.kaggle.com/rinchin) | 2024/09/01 | v1.1, Tabular     | Teams using AutoGluon in the Grand Prix: :1st_place_medal: [1st](https://www.kaggle.com/competitions/playground-series-s4e8/discussion/523732), :2nd_place_medal: [2nd](https://www.kaggle.com/competitions/playground-series-s4e8/discussion/523656), :3rd_place_medal: [3rd](https://www.kaggle.com/competitions/playground-series-s4e9/discussion/532028), [4th](https://www.kaggle.com/competitions/playground-series-s4e8/discussion/524709), [6th](https://www.kaggle.com/competitions/playground-series-s4e9/discussion/532758), [7th](https://www.kaggle.com/competitions/playground-series-s4e9/discussion/532419), [8th](https://www.kaggle.com/competitions/playground-series-s4e9/discussion/532668), [9th](https://www.kaggle.com/competitions/playground-series-s4e6/discussion/509937), and [10th](https://www.kaggle.com/competitions/playground-series-s4e8/discussion/524752) place teams! |\n```\n\n----------------------------------------\n\nTITLE: Checking Positive Class\nDESCRIPTION: Prints which class value AutoGluon considers as the positive class for binary classification.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/interpret/SHAP with AutoGluon-Tabular Census income classification.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint(\"positive class:\", predictor.positive_class)\n```\n\n----------------------------------------\n\nTITLE: Loading Test Data\nDESCRIPTION: Loads test data for model evaluation and prediction\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-essentials.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntest_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv')\ntest_data.head()\n```\n\n----------------------------------------\n\nTITLE: TimeSeriesPredictor Missing Value Imputation\nDESCRIPTION: Example of filling missing values in TimeSeriesDataFrame with customizable imputation logic for irregular time series\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/whats_new/v0.7.0.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npredictor.fill_missing_values(custom_imputation_logic)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Detection Results\nDESCRIPTION: Code to visualize object detection results on a sample image from the test set.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/quick_start/quick_start_coco.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal.utils import ObjectDetectionVisualizer\n\nconf_threshold = 0.4  # Specify a confidence threshold to filter out unwanted boxes\nimage_result = pred.iloc[30]\n\nimg_path = image_result.image  # Select an image to visualize\n\nvisualizer = ObjectDetectionVisualizer(img_path)  # Initialize the Visualizer\nout = visualizer.draw_instance_predictions(image_result, conf_threshold=conf_threshold)  # Draw detections\nvisualized = out.get_image()  # Get the visualized image\n\nfrom PIL import Image\nfrom IPython.display import display\nimg = Image.fromarray(visualized, 'RGB')\ndisplay(img)\n```\n\n----------------------------------------\n\nTITLE: Configuring Text Augmentation Detection Length in AutoMM\nDESCRIPTION: Sets the minimum text token length for applying text augmentation. This example shows how to configure the 'model.hf_text.text_aug_detect_length' hyperparameter.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_41\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"model.hf_text.text_aug_detect_length\": 10})\n# Allow text augmentation for texts whose token number is no less than 5\npredictor.fit(hyperparameters={\"model.hf_text.text_aug_detect_length\": 5})\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon with All Dependencies in Python\nDESCRIPTION: This command installs AutoGluon with all its dependencies, including those required for tabular data processing.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-gpu.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.tabular[all]\n```\n\n----------------------------------------\n\nTITLE: Examining Target Variable\nDESCRIPTION: Displays descriptive statistics for the target 'signature' column in the training data\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-quick-start.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlabel = 'signature'\ntrain_data[label].describe()\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset CSV Files\nDESCRIPTION: Loads training, validation and test data from CSV files and defines column names\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/image_text_matching.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndataset_path = os.path.join(download_dir, 'flickr30k_processed')\ntrain_data = pd.read_csv(f'{dataset_path}/train.csv', index_col=0)\nval_data = pd.read_csv(f'{dataset_path}/val.csv', index_col=0)\ntest_data = pd.read_csv(f'{dataset_path}/test.csv', index_col=0)\nimage_col = \"image\"\ntext_col = \"caption\"\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Dependencies for Development\nDESCRIPTION: This snippet shows the commands to install the necessary dependencies for AutoGluon development. It updates pip and wheel, then runs a full installation script.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -U pip wheel\n./full_install.sh\n```\n\n----------------------------------------\n\nTITLE: Setting Inference Speed Constraints in AutoGluon Fit (Python)\nDESCRIPTION: This snippet demonstrates how to set inference speed constraints when fitting an AutoGluon predictor. It shows how to specify the maximum allowed time per prediction and the batch size for inference speed calculations.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-indepth.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# At most 0.05 ms per row (20000 rows per second throughput)\ninfer_limit = 0.00005\n# adhere to infer_limit with batches of size 10000 (batch-inference, easier to satisfy infer_limit)\ninfer_limit_batch_size = 10000\n# adhere to infer_limit with batches of size 1 (online-inference, much harder to satisfy infer_limit)\n# infer_limit_batch_size = 1  # Note that infer_limit<0.02 when infer_limit_batch_size=1 can be difficult to satisfy.\npredictor_infer_limit = TabularPredictor(label=label, eval_metric=metric).fit(\n    train_data=train_data,\n    time_limit=30,\n    infer_limit=infer_limit,\n    infer_limit_batch_size=infer_limit_batch_size,\n)\n\n# NOTE: If bagging was enabled, it is important to call refit_full at this stage.\n#  infer_limit assumes that the user will call refit_full after fit.\n# predictor_infer_limit.refit_full()\n\n# NOTE: To align with inference speed calculated during fit, models must be persisted.\npredictor_infer_limit.persist()\n# Below is an optimized version that only persists the minimum required models for prediction.\n```\n\n----------------------------------------\n\nTITLE: Configuring SAM Image and Ground Truth Transforms in AutoMM\nDESCRIPTION: Sets up image and ground truth mask transforms for SAM using 'model.sam.img_transforms' and 'model.sam.gt_transforms' hyperparameters. Currently supports 'resize_to_square' and 'resize_gt_to_square' respectively.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_54\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"model.sam.img_transforms\": [\"resize_to_square\"]})\n\n# default used by AutoMM\npredictor.fit(hyperparameters={\"model.sam.gt_transforms\": [\"resize_gt_to_square\"]})\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for MultiModalPredictor Object Detection\nDESCRIPTION: Commands to install additional dependencies required for object detection functionality in the MultiModalPredictor.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/install-modules.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmim install \"mmcv==2.1.0\"\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install \"mmdet==3.2.0\"\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install pycocotools\n```\n\n----------------------------------------\n\nTITLE: Submitting Predictions to Kaggle Competition using Bash\nDESCRIPTION: This command submits the predictions file to the Kaggle competition using the Kaggle API.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-kaggle.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkaggle competitions submit -c ieee-fraud-detection -f sample_submission.csv -m \"my first submission\"\n```\n\n----------------------------------------\n\nTITLE: Loading Time Series Data\nDESCRIPTION: Loads the Australian Electricity Demand dataset as a TimeSeriesDataFrame from a CSV file.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-chronos.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndata = TimeSeriesDataFrame.from_path(\n    \"https://autogluon.s3.amazonaws.com/datasets/timeseries/australian_electricity_subset/test.csv\"\n)\ndata.head()\n```\n\n----------------------------------------\n\nTITLE: Encoding Images to Base85 Strings for MultiModalPredictor in Python\nDESCRIPTION: Functions to read image files, convert them to Base85 encoded strings, and update a dataframe column with these encoded values for processing with MultiModalPredictor. This enables image data to be embedded in the test data for inference.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/cloud_fit_deploy/cloud-aws-sagemaker-train-deploy.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef read_image_bytes_and_encode(image_path):\n    image_obj = open(image_path, 'rb')\n    image_bytes = image_obj.read()\n    image_obj.close()\n    b85_image = base64.b85encode(image_bytes).decode(\"utf-8\")\n\n    return b85_image\n\n\ndef convert_image_path_to_encoded_bytes_in_dataframe(dataframe, image_column):\n    assert image_column in dataframe, 'Please specify a valid image column name'\n    dataframe[image_column] = [read_image_bytes_and_encode(path) for path in dataframe[image_column]]\n\n    return dataframe\n\ntest_data_image_column = \"YOUR_COLUMN_CONTAINING_IMAGE_PATH\"\ntest_data = convert_image_path_to_encoded_bytes_in_dataframe(test_data, test_data_image_column)\n```\n\n----------------------------------------\n\nTITLE: Example COCO Format JSON Structure\nDESCRIPTION: Shows a partial example of a COCO format annotation file with info, licenses, categories, and images sections populated, demonstrating how actual data should be formatted.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/convert_data_to_coco_format.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"info\": {...},\n    \"licenses\": [\n        {\n            \"url\": \"http://creativecommons.org/licenses/by-nc-sa/2.0/\", \n            \"id\": 1, \n            \"name\": \"Attribution-NonCommercial-ShareAlike License\"\n        },\n        ...\n    ],\n    \"categories\": [\n        {\"supercategory\": \"person\", \"id\": 1, \"name\": \"person\"},\n        {\"supercategory\": \"vehicle\", \"id\": 2, \"name\": \"bicycle\"},\n        {\"supercategory\": \"vehicle\", \"id\": 3, \"name\": \"car\"},\n        {\"supercategory\": \"vehicle\", \"id\": 4, \"name\": \"motorcycle\"},\n        ...\n    ],\n        \n    \"images\": [\n        {\n            \"license\": 4, \n            \"file_name\": \"<imagename0>.<ext>\", \n            \"height\": 427, \n            \"width\": 640, \n            \"date_captured\": null, \n            \"id\": 397133\n        },\n        ...\n    ],\n    \"annotations\": [\n        \n        ...\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Using TabularPredictor with Custom Feature Generator in Python\nDESCRIPTION: This code shows how to use TabularPredictor with a custom feature generator, combining feature and label DataFrames for training.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-feature-engineering.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.concat([dfx, dfy], axis=1)\npredictor = TabularPredictor(label='label')\npredictor.fit(df, hyperparameters={'GBM' : {}}, feature_generator=auto_ml_pipeline_feature_generator)\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon in Kaggle Notebooks\nDESCRIPTION: Command to install AutoGluon in Kaggle notebooks that have internet access.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/install.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n!pip install -U autogluon > /dev/null\n```\n\n----------------------------------------\n\nTITLE: Enabling PyTorch Model Compilation\nDESCRIPTION: Toggle for PyTorch model compilation using torch.compile, recommended for large models and long training sessions.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"env.compile.turn_on\": False})\n# turn on torch.compile\npredictor.fit(hyperparameters={\"env.compile.turn_on\": True})\n```\n\n----------------------------------------\n\nTITLE: Downloading VOC Dataset with Bash Script Using Custom Path\nDESCRIPTION: Command to download and extract both VOC 2007 and 2012 datasets to a specified directory using the bash script with a path argument.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/prepare_voc.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbash download_voc0712.sh ~/data\n```\n\n----------------------------------------\n\nTITLE: Accessing Train Dataset\nDESCRIPTION: Accesses the train split of the loaded QNLI dataset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/model_distillation.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndataset['train']\n```\n\n----------------------------------------\n\nTITLE: Generating Random Data for Regression Metrics\nDESCRIPTION: Generates random ground truth labels and predictions as floating point values for testing regression metrics.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-metric.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ny_true = rng.random(10)\ny_pred = rng.random(10)\n\nprint(f'y_true: {y_true}')\nprint(f'y_pred: {y_pred}')\n```\n\n----------------------------------------\n\nTITLE: Training SAM with Conv-LoRA for Semantic Segmentation\nDESCRIPTION: Command to run the semantic segmentation training with customizable parameters including dataset selection, seed, rank, expert number, and batch size.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/Conv-LoRA/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython run_semantic_segmentation.py --<flag> <value>\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing Training Data\nDESCRIPTION: Loads the adult income dataset from S3 and prepares it for model training by sampling 1000 rows\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-model-advanced.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.tabular import TabularDataset\n\ntrain_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv')  \ntest_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv')  \nlabel = 'class'  \ntrain_data = train_data.sample(n=1000, random_state=0)  \n\ntrain_data.head(5)\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon TimesSeries and Dependencies with UV\nDESCRIPTION: Installs AutoGluon TimeSeries and its dependencies using UV package manager, and removes incompatible package versions.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/advanced/forecasting-custom-model.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# We use uv for faster installation\n!pip install uv\n!uv pip install -q autogluon.timeseries --system\n!uv pip uninstall -q torchaudio torchvision torchtext --system # fix incompatible package versions on Colab\n```\n\n----------------------------------------\n\nTITLE: Evaluating a Regression Model in AutoGluon\nDESCRIPTION: Evaluates the performance of the age prediction model on test data, which automatically uses an appropriate regression metric like RMSE.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-essentials.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\npredictor_age.evaluate(test_data)\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Text Augmentation Scale in AutoMM\nDESCRIPTION: Configures the maximum percentage of text tokens for data augmentation using the 'model.hf_text.text_trivial_aug_maxscale' hyperparameter. This controls the intensity of trivial augmentations like synonym replacement and word swapping.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_42\n\nLANGUAGE: python\nCODE:\n```\n# by default, AutoMM doesn't do text augmentation\npredictor.fit(hyperparameters={\"model.hf_text.text_trivial_aug_maxscale\": 0})\n# Enable trivial augmentation by setting the max scale to 0.1\npredictor.fit(hyperparameters={\"model.hf_text.text_trivial_aug_maxscale\": 0.1})\n```\n\n----------------------------------------\n\nTITLE: Loading the Shopee Dataset\nDESCRIPTION: Downloads and loads the Shopee dataset which contains 4 classes with 200 samples each in the training set.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/focal_loss.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal.utils.misc import shopee_dataset\n\ndownload_dir = \"./ag_automm_tutorial_imgcls_focalloss\"\ntrain_data, test_data = shopee_dataset(download_dir)\n```\n\n----------------------------------------\n\nTITLE: Loading and Extracting Dataset\nDESCRIPTION: Downloads and extracts the leaf disease segmentation dataset from S3.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_segmentation/beginner_semantic_seg.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndownload_dir = './ag_automm_tutorial'\nzip_file = 'https://automl-mm-bench.s3.amazonaws.com/semantic_segmentation/leaf_disease_segmentation.zip'\nfrom autogluon.core.utils.loaders import load_zip\nload_zip.unzip(zip_file, unzip_dir=download_dir)\n```\n\n----------------------------------------\n\nTITLE: Configuring Mixup Alpha Parameter in AutoMM Python\nDESCRIPTION: Sets the alpha parameter for the Mixup data augmentation technique. Mixup is active if `data.mixup.mixup_alpha` is greater than 0. The default value is `0.8`. Setting it to `1.0` effectively disables Mixup. Configure this within the `hyperparameters` for `predictor.fit()`.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_67\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"data.mixup.mixup_alpha\": 0.8})\n# set it to 1.0 to turn off Mixup\npredictor.fit(hyperparameters={\"data.mixup.mixup_alpha\": 1.0})\n```\n\n----------------------------------------\n\nTITLE: Displaying High Quality HPO Preset Details\nDESCRIPTION: Retrieves and displays the hyperparameters and hyperparameter tuning settings for the high_quality_hpo preset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/presets.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport yaml\nfrom autogluon.multimodal.presets import get_automm_presets\n\nhyperparameters, hyperparameter_tune_kwargs = get_automm_presets(problem_type=\"default\", presets=\"high_quality_hpo\")\nprint(f\"hyperparameters: {yaml.dump(hyperparameters, allow_unicode=True, default_flow_style=False)}\")\nprint(f\"hyperparameter_tune_kwargs: {json.dumps(hyperparameter_tune_kwargs, sort_keys=True, indent=4)}\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Hard Label Loss Weight for Distillation in AutoMM Python\nDESCRIPTION: Specifies the weight applied to the student model's loss calculated using the ground truth (hard) labels during knowledge distillation. The final hard label loss is multiplied by this weight. The default `distiller.hard_label_weight` for classification is `0.2`. Set this in the `hyperparameters` for `predictor.fit()`.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_76\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM for classification\npredictor.fit(hyperparameters={\"distiller.hard_label_weight\": 0.2})\n# set not to scale the hard label loss\npredictor.fit(hyperparameters={\"distiller.hard_label_weight\": 1})\n```\n\n----------------------------------------\n\nTITLE: Executing Analysis State Generation\nDESCRIPTION: Shows how to execute the analysis graph to produce a state dictionary that stores analysis results. The state serves as a shared namespace for all components.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/references/autogluon.eda.base-apis.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nstate = analysis.fit()\n```\n\n----------------------------------------\n\nTITLE: Building AutoGluon Documentation\nDESCRIPTION: This snippet shows the command to build the AutoGluon documentation. It notes that GPU and CUDA are required for building tutorials, and provides an option to skip running tutorials by editing the configuration file.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# Note: GPU & CUDA is required to build tutorials\n# To skip running tutorials, manually edit `docs/conf.py` and set `nb_execution_mode=off`\nbash build_doc.sh\n```\n\n----------------------------------------\n\nTITLE: Setting Image Transforms for Validation in AutoMM\nDESCRIPTION: Configures image transforms for validation, testing, and deployment using the 'model.timm_image.val_transforms' hyperparameter. Similar to training transforms, it supports both string-based and custom callable objects.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_48\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"model.timm_image.val_transforms\": [\"resize_shorter_side\", \"center_crop\"]})\n# resize image to square\npredictor.fit(hyperparameters={\"model.timm_image.val_transforms\": [\"resize_to_square\"]})\n# or use a list of callable and pickle-able objects, e.g., torchvision transforms\npredictor.fit(hyperparameters={\"model.timm_image.val_transforms\": [torchvision.transforms.Resize((224, 224)]}\n```\n\n----------------------------------------\n\nTITLE: Loading and Splitting Diabetes Dataset for AutoGluon Regression in Python\nDESCRIPTION: This code loads the diabetes dataset, splits it into training and validation sets, and defines a function to print the root mean squared error of predictions.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/interpret/SHAP with AutoGluon-Tabular Diabetes regression.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nX,y = shap.datasets.diabetes()\nX_train,X_valid,y_train,y_valid = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=0)\n\ndef print_accuracy(f):\n    print(\"Root mean squared test error = {0}\".format(np.sqrt(np.mean((f(X_valid) - y_valid)**2))))\n    time.sleep(0.5) # to let the print get out before any progress bars\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon MultiModal Package\nDESCRIPTION: Installing the AutoGluon MultiModal library using pip for document classification capabilities.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/document_prediction/pdf_classification.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.multimodal\n```\n\n----------------------------------------\n\nTITLE: Configuring Gradient Norm Tracking in AutoMM\nDESCRIPTION: Examples showing how to configure gradient norm tracking during training in AutoMM, with options to disable tracking (-1) or track specific p-norms.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM (no tracking)\npredictor.fit(hyperparameters={\"optim.track_grad_norm\": -1})\n# track the 2-norm\npredictor.fit(hyperparameters={\"optim.track_grad_norm\": 2})\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon without Internet Access in Kaggle\nDESCRIPTION: Code to set up AutoGluon from local files in a Kaggle environment without internet access. This imports the ANTLR runtime and installs AutoGluon from pre-downloaded wheel files, essential for code competitions with restricted connectivity.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/kaggle_feedback_prize/README.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport sys\nsys.path.append(\"../input/autogluon-standalone/antlr4-python3-runtime-4.8/antlr4-python3-runtime-4.8/src/\")\n!pip install --no-deps --no-index --quiet ../input/autogluon-standalone/autogluon_standalone/*.whl\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Nightly Builds\nDESCRIPTION: Commands to install the latest nightly builds of AutoGluon using UV package manager with the --pre flag.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/install.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -U uv\npython -m uv pip install --pre autogluon\n```\n\n----------------------------------------\n\nTITLE: Importing AutoGluon TimeSeries Classes\nDESCRIPTION: Imports the TimeSeriesDataFrame and TimeSeriesPredictor classes from autogluon.timeseries module.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-chronos.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n```\n\n----------------------------------------\n\nTITLE: Initializing Python Environment and Dependencies\nDESCRIPTION: Sets up the Python environment by importing required packages and suppressing warnings. Also includes seed setting for reproducibility.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/tensorrt.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport numpy as np\nimport time\nimport warnings\nfrom IPython.display import clear_output\nwarnings.filterwarnings('ignore')\nnp.random.seed(123)\n```\n\n----------------------------------------\n\nTITLE: Setting Baseline Reference Values\nDESCRIPTION: Calculates median values for features to serve as baseline reference values for SHAP explanations.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/interpret/SHAP with AutoGluon-Tabular Census income classification.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmed = X_train.median()  # X_train.mode() would be a more appropriate baseline for ordinally-encoded categorical features\nprint(\"Baseline feature-values: \\n\", med)\n```\n\n----------------------------------------\n\nTITLE: Configuring Label Smoothing for Mixup/Cutmix in AutoMM Python\nDESCRIPTION: Sets the label smoothing value applied to the target labels when Mixup or Cutmix augmentations are active. The default value for `data.mixup.label_smoothing` is `0.1`. Configure this hyperparameter within the `predictor.fit()` method.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_72\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"data.mixup.label_smoothing\": 0.1})\n# set it to 0.2\npredictor.fit(hyperparameters={\"data.mixup.label_smoothing\": 0.2})\n```\n\n----------------------------------------\n\nTITLE: Configuring Validation Check Interval in AutoMM\nDESCRIPTION: Examples showing how to set the validation check frequency in AutoMM, with options to check after a fraction of the training epoch or a fixed number of training batches.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"optim.val_check_interval\": 0.5})\n# check validation set 4 times during a training epoch\npredictor.fit(hyperparameters={\"optim.val_check_interval\": 0.25})\n```\n\n----------------------------------------\n\nTITLE: Loading QNLI Dataset with Hugging Face Datasets\nDESCRIPTION: Loads the QNLI dataset using the Hugging Face datasets library and disables the progress bar.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/model_distillation.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport datasets\nfrom datasets import load_dataset\n\ndatasets.logging.disable_progress_bar()\n\ndataset = load_dataset(\"glue\", \"qnli\")\n```\n\n----------------------------------------\n\nTITLE: Converting Existing VOC Splits to COCO Format\nDESCRIPTION: Command to convert predefined VOC dataset splits to COCO format using AutoGluon's voc2coco utility, specifying the root directory of the VOC dataset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/voc_to_coco.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m autogluon.multimodal.cli.voc2coco --root_dir ./VOCdevkit/VOC2007\n```\n\n----------------------------------------\n\nTITLE: Performing Classification Target Analysis\nDESCRIPTION: Performs automated target analysis on the Titanic classification dataset using AutoGluon's EDA functionality\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-target-analysis.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport autogluon.eda.auto as auto\n\nauto.target_analysis(train_data=df_train, label=target_col)\n```\n\n----------------------------------------\n\nTITLE: Downloading Stanford Cars CSV Files\nDESCRIPTION: Uses wget to download the train and test CSV files for the Stanford Cars dataset, containing references to the image files and their labels.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/few_shot_learning.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n!wget https://automl-mm-bench.s3.amazonaws.com/vision_datasets/stanfordcars/train_8shot.csv -O ./ag_automm_tutorial_fs_cls/stanfordcars/train.csv\n!wget https://automl-mm-bench.s3.amazonaws.com/vision_datasets/stanfordcars/test.csv -O ./ag_automm_tutorial_fs_cls/stanfordcars/test.csv\n```\n\n----------------------------------------\n\nTITLE: Viewing Static Features in TimeSeriesDataFrame\nDESCRIPTION: Displays the first few rows of static features associated with the time series data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/advanced/forecasting-custom-model.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nraw_data.static_features.head()\n```\n\n----------------------------------------\n\nTITLE: Expanding Image Paths\nDESCRIPTION: Converts relative image paths to absolute paths in the dataset\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/image_text_matching.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef path_expander(path, base_folder):\n    path_l = path.split(';')\n    return ';'.join([os.path.abspath(os.path.join(base_folder, path)) for path in path_l])\n\ntrain_data[image_col] = train_data[image_col].apply(lambda ele: path_expander(ele, base_folder=dataset_path))\nval_data[image_col] = val_data[image_col].apply(lambda ele: path_expander(ele, base_folder=dataset_path))\ntest_data[image_col] = test_data[image_col].apply(lambda ele: path_expander(ele, base_folder=dataset_path))\n```\n\n----------------------------------------\n\nTITLE: Configuring Distributed Training Strategy\nDESCRIPTION: Setting the distributed training mode between data parallel (dp), distributed data parallel (ddp), or spawn-based ddp.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"env.strategy\": \"ddp_spawn\"})\n# use ddp during training\npredictor.fit(hyperparameters={\"env.strategy\": \"ddp\"})\n```\n\n----------------------------------------\n\nTITLE: Making Class Predictions with AutoGluon MultiModalPredictor\nDESCRIPTION: Uses the trained predictor to make class predictions on the test dataset by dropping the label column from the input data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal-quick-start.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npredictions = predictor.predict(test_data.drop(columns=label_col))\npredictions[:5]\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Multimodal Package\nDESCRIPTION: Installs the AutoGluon multimodal package using pip.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/text2text_matching.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.multimodal\n```\n\n----------------------------------------\n\nTITLE: Downloading VOC Dataset with Bash Script Using Default Path\nDESCRIPTION: Command to download and extract both VOC 2007 and 2012 datasets to the current directory using the provided bash script.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/prepare_voc.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbash download_voc0712.sh\n```\n\n----------------------------------------\n\nTITLE: Downloading and Displaying an Apple Image for Typographic Attack Demonstration\nDESCRIPTION: Downloads an image of a Granny Smith apple to demonstrate CLIP's vulnerability to typographic attacks.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_prediction/clip_zeroshot.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nurl = \"https://cdn.openai.com/multimodal-neurons/assets/apple/apple-blank.jpg\"\nimage_path = download(url)\n\npil_img = Image(filename=image_path)\ndisplay(pil_img)\n```\n\n----------------------------------------\n\nTITLE: Predicting Image Label\nDESCRIPTION: Uses the trained model to predict the label of a new image.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_prediction/beginner_image_cls.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npredictions = predictor.predict({'image': [image_path]})\nprint(predictions)\n```\n\n----------------------------------------\n\nTITLE: Running Tabular Example with FT-Transformer in Python\nDESCRIPTION: Command to execute the example script for tabular data processing using FT-Transformer. It allows specifying dataset, directories, and optional parameters like GPU and learning rate.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/tabular_dl/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython example_tabular.py --dataset_name ad --dataset_dir ./dataset --exp_dir ./result\n```\n\n----------------------------------------\n\nTITLE: Expanding Image Paths for AutoGluon NER Dataset\nDESCRIPTION: This code expands image paths in the dataset to full absolute paths for training and testing data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal_ner.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimage_col = 'image'\ntrain_data[image_col] = train_data[image_col].apply(lambda ele: ele.split(';')[0]) # Use the first image for a quick tutorial\ntest_data[image_col] = test_data[image_col].apply(lambda ele: ele.split(';')[0])\n\ndef path_expander(path, base_folder):\n\tpath_l = path.split(';')\n\tp = ';'.join([os.path.abspath(base_folder+path) for path in path_l])\n\treturn p\n\ntrain_data[image_col] = train_data[image_col].apply(lambda ele: path_expander(ele, base_folder=dataset_path))\ntest_data[image_col] = test_data[image_col].apply(lambda ele: path_expander(ele, base_folder=dataset_path))\n\ntrain_data[image_col].iloc[0]\n```\n\n----------------------------------------\n\nTITLE: Setting Inference Worker Processes\nDESCRIPTION: Configuration for the number of worker processes used by PyTorch dataloader during prediction or evaluation.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"env.num_workers_inference\": 2})\n# use 4 workers in the prediction/evaluation dataloader\npredictor.fit(hyperparameters={\"env.num_workers_inference\": 4})\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Multimodal Package\nDESCRIPTION: Installs the AutoGluon multimodal package required for image-text matching\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/image_text_matching.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.multimodal\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset\nDESCRIPTION: Loads training and test data from CSV files.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/beginner_multimodal.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\ndataset_path = download_dir + '/petfinder_for_tutorial'\ntrain_data = pd.read_csv(f'{dataset_path}/train.csv', index_col=0)\ntest_data = pd.read_csv(f'{dataset_path}/test.csv', index_col=0)\nlabel_col = 'AdoptionSpeed'\n```\n\n----------------------------------------\n\nTITLE: Loading Pretrained MultiModalPredictor in Python\nDESCRIPTION: Loads a pretrained MultiModalPredictor model from a specified path. This is typically used in Kaggle competitions after uploading the model as a dataset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/kaggle_feedback_prize/README.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npretrained_model = MultiModalPredictor.load(path=save_standalone_path)\n```\n\n----------------------------------------\n\nTITLE: Final Anomaly Detection Analysis\nDESCRIPTION: Running anomaly detection on cleaned data with detailed visualization and explanation settings.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-anomaly-detection.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nstate = auto.detect_anomalies(\n    train_data=x,\n    test_data=x_test,\n    label=target_col,\n    threshold_stds=3,\n    show_top_n_anomalies=5,\n    explain_top_n_anomalies=1,\n    return_state=True,\n    show_help_text=False,\n    fig_args={\n        'figsize': (6, 4)\n    },\n    chart_args={\n        'normal.color': 'lightgrey',\n        'anomaly.color': 'orange',\n    }    \n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Switch Probability between Mixup and Cutmix in AutoMM Python\nDESCRIPTION: Sets the probability of choosing Cutmix over Mixup when both augmentation techniques are active (both alphas > 0 and `data.mixup.turn_on` is True). The default value for `data.mixup.switch_prob` is `0.5`. This is configured via the `hyperparameters` argument in `predictor.fit()`.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_70\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"data.mixup.switch_prob\": 0.5})\n# set probability to 0.7\npredictor.fit(hyperparameters={\"data.mixup.switch_prob\": 0.7})\n```\n\n----------------------------------------\n\nTITLE: Analyzing Interaction for Parch and Survived\nDESCRIPTION: Visualizes the interaction between 'Parch' and 'Survived' features.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-analyze-interaction.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nauto.analyze_interaction(x='Parch', hue='Survived', train_data=df_train)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Imports necessary Python packages including AutoGluon, pandas, scikit-learn and SHAP. Initializes SHAP JavaScript visualization and suppresses warnings.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/interpret/SHAP with AutoGluon-Tabular Census income classification.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.tabular import TabularPredictor\nimport pandas as pd\nimport sklearn\nimport shap\n\nshap.initjs()\n\nimport warnings\nwarnings.filterwarnings('ignore')\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Submodules via pip\nDESCRIPTION: Basic command for installing specific AutoGluon submodules. This allows users to reduce dependencies by only installing the components they need.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/install-modules.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install <submodule>\n```\n\n----------------------------------------\n\nTITLE: Displaying Document Example\nDESCRIPTION: Displays an example document from the training set using IPython's display capabilities to visualize the document structure.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/document_prediction/document_classification.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import Image, display\n\nexample_image = train_data.iloc[0][DOC_PATH_COL]\npil_img = Image(filename=example_image, width=500)\ndisplay(pil_img)\n```\n\n----------------------------------------\n\nTITLE: Initializing MultiModalPredictor\nDESCRIPTION: Setting up the MultiModalPredictor with specific parameters for object detection task.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/quick_start/quick_start_coco.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport uuid\n\nmodel_path = f\"./tmp/{uuid.uuid4().hex}-quick_start_tutorial_temp_save\"\n\npredictor = MultiModalPredictor(\n    problem_type=\"object_detection\",\n    sample_data_path=train_path,\n    presets=\"medium_quality\",\n    path=model_path,\n)\n```\n\n----------------------------------------\n\nTITLE: Tabular Benchmarking Command for AutoGluon\nDESCRIPTION: Command to trigger automated benchmarking for the tabular module with specified preset and time limit settings.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/CONTRIBUTING.md#2025-04-22_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n/benchmark module=tabular preset=tabular_best benchmark=tabular_full time_limit=1h\n```\n\n----------------------------------------\n\nTITLE: Analyzing Interaction for Fare and Survived\nDESCRIPTION: Visualizes the interaction between 'Fare' and 'Survived' features with filled chart.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-analyze-interaction.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nauto.analyze_interaction(x='Fare', hue='Survived', train_data=df_train, test_data=df_test, chart_args=dict(fill=True))\n```\n\n----------------------------------------\n\nTITLE: Processing Stanford Cars Dataset for Training\nDESCRIPTION: Loads and processes the Stanford Cars dataset CSV files by removing unnecessary columns and updating image paths. This prepares the data for training and evaluation with MultiModalPredictor.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/few_shot_learning.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport os\n\ntrain_df_raw = pd.read_csv(os.path.join(download_dir, \"train.csv\"))\ntrain_df = train_df_raw.drop(\n        columns=[\n            \"Source\",\n            \"Confidence\",\n            \"XMin\",\n            \"XMax\",\n            \"YMin\",\n            \"YMax\",\n            \"IsOccluded\",\n            \"IsTruncated\",\n            \"IsGroupOf\",\n            \"IsDepiction\",\n            \"IsInside\",\n        ]\n    )\ntrain_df[\"ImageID\"] = download_dir + train_df[\"ImageID\"].astype(str)\n\n\ntest_df_raw = pd.read_csv(os.path.join(download_dir, \"test.csv\"))\ntest_df = test_df_raw.drop(\n        columns=[\n            \"Source\",\n            \"Confidence\",\n            \"XMin\",\n            \"XMax\",\n            \"YMin\",\n            \"YMax\",\n            \"IsOccluded\",\n            \"IsTruncated\",\n            \"IsGroupOf\",\n            \"IsDepiction\",\n            \"IsInside\",\n        ]\n    )\ntest_df[\"ImageID\"] = download_dir + test_df[\"ImageID\"].astype(str)\n\nprint(os.path.exists(train_df.iloc[0][\"ImageID\"]))\nprint(train_df)\nprint(os.path.exists(test_df.iloc[0][\"ImageID\"]))\nprint(test_df)\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon MultiModal Package\nDESCRIPTION: Installs the AutoGluon MultiModal package using pip\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/hyperparameter_optimization.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.multimodal\n```\n\n----------------------------------------\n\nTITLE: AutoGluon API Module References in RST\nDESCRIPTION: Sphinx documentation configuration defining the API structure for AutoGluon's main components. Uses autosummary directives to generate API documentation for key classes across different modules.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/api.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. currentmodule:: autogluon.tabular\n\n.. autosummary::\n    :toctree: api\n    :template: custom_class.rst\n\n    TabularPredictor\n\n.. currentmodule:: autogluon.core\n\n.. autosummary::\n    :toctree: api\n    :template: custom_class.rst\n\n    TabularDataset\n\n.. currentmodule:: autogluon.multimodal\n\n.. autosummary::\n    :toctree: api\n    :template: custom_class.rst\n\n    MultiModalPredictor\n\n\n.. currentmodule:: autogluon.timeseries\n\n.. autosummary::\n    :toctree: api\n    :template: custom_class.rst\n\n    TimeSeriesDataFrame\n    TimeSeriesPredictor\n\n\n.. currentmodule:: autogluon.common.features.feature_metadata\n\n.. autosummary::\n    :toctree: api\n    :template: custom_class.rst\n\n    FeatureMetadata\n```\n\n----------------------------------------\n\nTITLE: Save and Convert Pretrained Models\nDESCRIPTION: Methods for saving pretrained models and converting checkpoint names, with improved implementations and documentation.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/whats_new/v0.4.1.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsave_pretrained_models\nconvert_checkpoint_name\n```\n\n----------------------------------------\n\nTITLE: Displaying Feature Importance\nDESCRIPTION: Shows the feature importance determined by the quick fit model.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-analyze-interaction.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nstate.model_evaluation.importance\n```\n\n----------------------------------------\n\nTITLE: Displaying VOC Dataset Directory Structure\nDESCRIPTION: Shows the typical folder structure of a VOC format dataset root directory with Annotations, ImageSets, and JPEGImages folders.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/voc_to_coco.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nAnnotations  ImageSets  JPEGImages\n```\n\n----------------------------------------\n\nTITLE: Loading Titanic Dataset\nDESCRIPTION: Loading training and test data from Titanic dataset stored in AutoGluon's S3 bucket.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-anomaly-detection.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf_train = pd.read_csv('https://autogluon.s3.amazonaws.com/datasets/titanic/train.csv')\ndf_test = pd.read_csv('https://autogluon.s3.amazonaws.com/datasets/titanic/test.csv')\ntarget_col = 'Survived'\n```\n\n----------------------------------------\n\nTITLE: Preparing Training Data\nDESCRIPTION: Formats the data for AutoGluon training by creating pandas DataFrames with appropriate labels.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/interpret/SHAP with AutoGluon-Tabular Census income classification.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nlabel = 'label'\nfeature_names = X_train.columns\ntrain_data = X_train.copy()\ntrain_data[label] = y_train\nval_data = X_valid.copy()\n\ndisplay(train_data.head())\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Dataset\nDESCRIPTION: Downloads and unzips the PetFinder dataset for model training and evaluation.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/tensorrt.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndownload_dir = './ag_automm_tutorial'\nzip_file = 'https://automl-mm-bench.s3.amazonaws.com/petfinder_for_tutorial.zip'\nfrom autogluon.core.utils.loaders import load_zip\nload_zip.unzip(zip_file, unzip_dir=download_dir)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for AutoGluon Text Classification\nDESCRIPTION: Imports necessary modules including pandas for data manipulation, sklearn for metrics, and AutoGluon components for tabular data processing and the fastText model implementation.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/tabular-fasttext.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\n\nfrom autogluon.tabular import TabularDataset, TabularPredictor\nfrom autogluon.features.generators import AutoMLPipelineFeatureGenerator\nfrom autogluon.tabular.models.fasttext.fasttext_model import FastTextModel\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Importing necessary Python libraries including pandas, seaborn, and AutoGluon EDA components.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-anomaly-detection.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport seaborn as sns\n\nimport autogluon.eda.auto as auto\n```\n\n----------------------------------------\n\nTITLE: Configuring Soft Label Loss Type for Distillation in AutoMM Python\nDESCRIPTION: Determines the loss function used for computing the distillation loss based on the teacher model's soft labels (logits). The default for classification is `\"cross_entropy\"`, and for regression, it's `\"mse\"`. Set `distiller.soft_label_loss_type` in the `hyperparameters` argument of `predictor.fit()` when using knowledge distillation.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_74\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM for classification\npredictor.fit(hyperparameters={\"distiller.soft_label_loss_type\": \"cross_entropy\"})\n# default used by AutoMM for regression\npredictor.fit(hyperparameters={\"distiller.soft_label_loss_type\": \"mse\"})\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset Annotations\nDESCRIPTION: Loads training and test data annotations from CSV files and defines column names\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/image2image_matching.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndataset_path = os.path.join(download_dir, 'Stanford_Online_Products')\ntrain_data = pd.read_csv(f'{dataset_path}/train.csv', index_col=0)\ntest_data = pd.read_csv(f'{dataset_path}/test.csv', index_col=0)\nimage_col_1 = \"Image1\"\nimage_col_2 = \"Image2\"\nlabel_col = \"Label\"\nmatch_label = 1\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for AutoGluon Modules in Python\nDESCRIPTION: Commands to run unit tests for different AutoGluon modules using pytest. These tests check the functionality of common, core, features, tabular, multimodal, and timeseries components.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/CONTRIBUTING.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npytest common/tests\npytest core/tests\npytest features/tests\npytest tabular/tests\npytest multimodal/tests\npytest timeseries/tests\n```\n\n----------------------------------------\n\nTITLE: Installing Documentation Requirements for AutoGluon\nDESCRIPTION: This code installs the specific requirements for building AutoGluon documentation. It changes to the docs directory and uses pip to install the requirements listed in a text file.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd docs/\npython3 -m pip install -r requirements_doc.txt\n```\n\n----------------------------------------\n\nTITLE: Building a MultiModalPredictor for Text Classification\nDESCRIPTION: Code for initializing an AutoGluon MultiModalPredictor with appropriate configuration for a multiclass classification problem. This setup specifies the target label, problem type, evaluation metric, and other parameters.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/kaggle_feedback_prize/README.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npredictor = MultiModalPredictor(\n    label=\"discourse_effectiveness\", \n    problem_type=\"multiclass\", \n    eval_metric=\"log_loss\", \n    path=save_path,  \n    verbosity=3, \n)\n```\n\n----------------------------------------\n\nTITLE: Enabling/Disabling Mixup Augmentation in AutoMM Python\nDESCRIPTION: Controls whether Mixup data augmentation is used during training. Set `data.mixup.turn_on` to `True` to enable Mixup or `False` (default) to disable it. This setting is passed via the `hyperparameters` argument to `predictor.fit()`.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_66\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"data.mixup.turn_on\": False})\n# turn on Mixup\npredictor.fit(hyperparameters={\"data.mixup.turn_on\": True})\n```\n\n----------------------------------------\n\nTITLE: Displaying Benchmark Comparison Table in Markdown\nDESCRIPTION: A markdown table comparing AutoGluon 1.0 performance against other AutoML systems and traditional models on the OpenML AutoML Benchmark across 1040 tasks. The table shows win rates, loss improvements, rescaled loss, ranking, and champion percentages.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/whats_new/v1.0.0.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Method                       | AG Winrate | AG Loss Improvement | Rescaled Loss |     Rank | Champion |\n|:-----------------------------|:-----------|:--------------------|--------------:|---------:|:---------|\n| AutoGluon 1.0 (Best, 4h8c)   | **-**      | **-**               |      **0.04** | **1.95** | **63%**  |\n| lightautoml (2023, 4h8c)     | 84%        | 12.0%               |           0.2 |     4.78 | 12%      |\n| H2OAutoML (2023, 4h8c)       | 94%        | 10.8%               |          0.17 |     4.98 | 1%       |\n| FLAML (2023, 4h8c)           | 86%        | 16.7%               |          0.23 |     5.29 | 5%       |\n| MLJAR (2023, 4h8c)           | 82%        | 23.0%               |          0.33 |     5.53 | 6%       |\n| autosklearn (2023, 4h8c)     | 91%        | 12.5%               |          0.22 |     6.07 | 4%       |\n| GAMA (2023, 4h8c)            | 86%        | 15.4%               |          0.28 |     6.13 | 5%       |\n| CatBoost (2023, 4h8c)        | 95%        | 18.2%               |          0.28 |     6.89 | 3%       |\n| TPOT (2023, 4h8c)            | 91%        | 23.1%               |           0.4 |     8.15 | 1%       |\n| LightGBM (2023, 4h8c)        | 99%        | 23.6%               |           0.4 |     8.95 | 0%       |\n| XGBoost (2023, 4h8c)         | 100%       | 24.1%               |          0.43 |      9.5 | 0%       |\n| RandomForest (2023, 4h8c)    | 97%        | 25.1%               |          0.53 |     9.78 | 1%       |\n```\n\n----------------------------------------\n\nTITLE: Downloading VOC Dataset with Python Script Using Custom Path\nDESCRIPTION: Command to download and extract the Pascal VOC 2007 and 2012 datasets to a specified directory path using AutoGluon's CLI tool with the output_path parameter.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/prepare_voc.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m autogluon.multimodal.cli.prepare_detection_dataset --dataset_name voc0712 --output_path ~/data\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Multimodal\nDESCRIPTION: Installs the AutoGluon multimodal package using pip.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/presets.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.multimodal\n```\n\n----------------------------------------\n\nTITLE: Freezing SAM Layers and Configuring Mask Tokens in AutoMM\nDESCRIPTION: Specifies which SAM modules to freeze during training and sets the number of mask proposals using 'model.sam.frozen_layers' and 'model.sam.num_mask_tokens' hyperparameters.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_55\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"model.sam.frozen_layers\": [\"mask_decoder.iou_prediction_head\", \"prompt_encoder\"]})\n\n# default used by AutoMM\npredictor.fit(hyperparameters={\"model.sam.num_mask_tokens\": 1})\n```\n\n----------------------------------------\n\nTITLE: Generating Aggregate SHAP Explanations for AutoGluon Predictions in Python\nDESCRIPTION: This code generates and visualizes SHAP explanations for multiple predictions from the validation dataset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/interpret/SHAP with AutoGluon-Tabular Diabetes regression.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nshap_values = explainer.shap_values(X_valid.iloc[0:N_VAL,:], nsamples=NSHAP_SAMPLES)\nshap.force_plot(explainer.expected_value, shap_values, X_valid.iloc[0:N_VAL,:])\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon EDA Package\nDESCRIPTION: Installs the AutoGluon EDA package using pip.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-analyze-interaction.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.eda\n```\n\n----------------------------------------\n\nTITLE: Documenting ApplyFeatureGenerator Class\nDESCRIPTION: This RST directive generates documentation for the ApplyFeatureGenerator class, specifically including its 'init' method.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.transform.md#2025-04-22_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: ApplyFeatureGenerator\n   :members: init\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Tabular Package\nDESCRIPTION: Installs the AutoGluon tabular package with all dependencies using pip.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-indepth.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.tabular[all]\n```\n\n----------------------------------------\n\nTITLE: Running Memory Bank Model Example Command in Python\nDESCRIPTION: Example command to run the memory bank model with CLIP on the food101 dataset with 16 shots per class. The command specifies the model type, dataset, and number of training examples per class.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/memory_bank/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython memory_bank.py --type clip --dataset food101 --shots 16\n```\n\n----------------------------------------\n\nTITLE: Configuring Numerical Data Centering in AutoMM Python\nDESCRIPTION: Specifies whether to center numerical features (excluding labels) by subtracting the mean before scaling. Set `data.numerical.scaler_with_mean` to `True` (default) to enable centering or `False` to disable it. This is configured in the `hyperparameters` argument of `predictor.fit()`.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_61\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"data.numerical.scaler_with_mean\": True})\n# turn off centering\npredictor.fit(hyperparameters={\"data.numerical.scaler_with_mean\": False})\n```\n\n----------------------------------------\n\nTITLE: Generating Test Data for Metric Evaluation\nDESCRIPTION: Creates random test data for demonstrating metric calculations using numpy's random number generator\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-metric.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nrng = np.random.default_rng(seed=42)\ny_true = rng.integers(low=0, high=2, size=10)\ny_pred = rng.integers(low=0, high=2, size=10)\n\nprint(f'y_true: {y_true}')\nprint(f'y_pred: {y_pred}')\n```\n\n----------------------------------------\n\nTITLE: Loading Sentiment Analysis Dataset with AutoGluon\nDESCRIPTION: Loads the Stanford Sentiment Treebank (SST) dataset for sentiment analysis using AutoGluon's data loading utilities. It loads training and test data and subsamples the training data for a faster demonstration.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/continuous_training.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.core.utils.loaders import load_pd\n\ntrain_data = load_pd.load(\"https://autogluon-text.s3-accelerate.amazonaws.com/glue/sst/train.parquet\")\ntest_data = load_pd.load(\"https://autogluon-text.s3-accelerate.amazonaws.com/glue/sst/dev.parquet\")\nsubsample_size = 1000  # subsample data for faster demo, try setting this to larger values\ntrain_data_1 = train_data.sample(n=subsample_size, random_state=0)\ntrain_data_1.head(10)\n```\n\n----------------------------------------\n\nTITLE: Configuring Stochastic Text Chunking\nDESCRIPTION: Toggle for random text chunk selection when text length exceeds maximum allowed length.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_40\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"model.hf_text.stochastic_chunk\": False})\n# select a stochastic text chunk if a text sequence is over-long\npredictor.fit(hyperparameters={\"model.hf_text.stochastic_chunk\": True})\n```\n\n----------------------------------------\n\nTITLE: Installing MMCV and MMDetection for AutoGluon\nDESCRIPTION: This snippet shows how to install MMCV and MMDetection, which are required dependencies for using certain object detection models in AutoGluon.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/object_detection_with_dataframe.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n!mim install mmcv\n!pip install \"mmdet==3.1.0\"\n```\n\n----------------------------------------\n\nTITLE: Initializing TimeSeriesPredictor with Evaluation Metric\nDESCRIPTION: Example showing how to initialize a TimeSeriesPredictor with a specific evaluation metric (MASE in this case).\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-metrics.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.timeseries import TimeSeriesPredictor\n\npredictor = TimeSeriesPredictor(eval_metric=\"MASE\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Mixup/Cutmix Application Probability in AutoMM Python\nDESCRIPTION: Defines the probability of applying either Mixup or Cutmix augmentation per batch/element when at least one of them is enabled (respective alpha > 0). The default probability is `1.0`. Set `data.mixup.prob` in the `hyperparameters` dictionary for `predictor.fit()`.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_69\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"data.mixup.prob\": 1.0})\n# set probability to 0.5\npredictor.fit(hyperparameters={\"data.mixup.prob\": 0.5})\n```\n\n----------------------------------------\n\nTITLE: Importing AutoGluon Anomaly Analysis Module\nDESCRIPTION: Imports the anomaly analysis module from AutoGluon's EDA package. This module provides classes for performing anomaly detection analysis.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.anomaly.md#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom autogluon.eda.analysis.anomaly import AnomalyDetectorAnalysis, AnomalyDetector\n```\n\n----------------------------------------\n\nTITLE: Running Specific Unit Tests in Python\nDESCRIPTION: Commands to run specific unit tests or all tests in a file using pytest. This is useful for testing individual components or functions.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/CONTRIBUTING.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pytest path_to_file::test_mytest\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pytest path_to_file\n```\n\n----------------------------------------\n\nTITLE: COCO Format Annotation Schema Definition\nDESCRIPTION: Defines the JSON schema for COCO format annotation files, including the structure of info, licenses, images, annotations, and categories objects.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/convert_data_to_coco_format.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    \"info\": info,\n    \"licenses\": [license], \n    \"images\": [image],  // list of all images in the dataset\n    \"annotations\": [annotation],  // list of all annotations in the dataset\n    \"categories\": [category]  // list of all categories\n}\n\nwhere:\n\ninfo = {\n    \"year\": int, \n    \"version\": str, \n    \"description\": str, \n    \"contributor\": str, \n    \"url\": str, \n    \"date_created\": datetime,\n}\n\nlicense = {\n    \"id\": int, \n    \"name\": str, \n    \"url\": str,\n}\n\nimage = {\n    \"id\": int, \n    \"width\": int, \n    \"height\": int, \n    \"file_name\": str, \n    \"license\": int,  // the id of the license\n    \"date_captured\": datetime,\n}\n\ncategory = {\n    \"id\": int, \n    \"name\": str, \n    \"supercategory\": str,\n}\n\nannotation = {\n    \"id\": int, \n    \"image_id\": int,  // the id of the image that the annotation belongs to\n    \"category_id\": int,  // the id of the category that the annotation belongs to\n    \"segmentation\": RLE or [polygon], \n    \"area\": float, \n    \"bbox\": [x,y,width,height], \n    \"iscrowd\": int,  // 0 or 1,\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment and Dependencies\nDESCRIPTION: Importing required libraries and setting up the environment with necessary configurations.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal_text_tabular.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nimport warnings\nimport os\n\nwarnings.filterwarnings('ignore')\nnp.random.seed(123)\n```\n\n----------------------------------------\n\nTITLE: Setting Training Worker Processes\nDESCRIPTION: Configuration for the number of worker processes used by PyTorch dataloader during training.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"env.num_workers\": 2})\n# use 4 workers in the training dataloader\npredictor.fit(hyperparameters={\"env.num_workers\": 4})\n```\n\n----------------------------------------\n\nTITLE: Implementing Mean Absolute Scaled Error (MASE) Metric in Python for AutoGluon\nDESCRIPTION: Implements the MASE metric class that considers both past and future values for error calculation. Includes methods for storing past metrics and computing the final scaled error score.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-metrics.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass MeanAbsoluteScaledError(TimeSeriesScorer):\n  greater_is_better_internal = False\n  optimum = 0.0\n  optimized_by_median = True\n  equivalent_tabular_regression_metric = \"mean_absolute_error\"\n\n  def save_past_metrics(\n      self, data_past: TimeSeriesDataFrame, target: str = \"target\", seasonal_period: int = 1, **kwargs\n  ) -> None:\n      seasonal_diffs = data_past[target].groupby(level=\"item_id\").diff(seasonal_period).abs()\n      self._abs_seasonal_error_per_item = seasonal_diffs.groupby(level=\"item_id\").mean().fillna(1.0)\n\n  def clear_past_metrics(self):\n      self._abs_seasonal_error_per_item = None\n  \n  def compute_metric(\n      self, data_future: TimeSeriesDataFrame, predictions: TimeSeriesDataFrame, target: str = \"target\", **kwargs\n  ) -> float:\n      mae_per_item = (data_future[target] - predictions[\"mean\"]).abs().groupby(level=\"item_id\").mean()\n      return (mae_per_item / self._abs_seasonal_error_per_item).mean()\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon MultiModal Package\nDESCRIPTION: Installation command for the AutoGluon multimodal package using pip.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_segmentation/beginner_semantic_seg.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.multimodal\n```\n\n----------------------------------------\n\nTITLE: Creating Class Summary for XShiftDetector\nDESCRIPTION: RST directive to generate an automatic summary for the XShiftDetector class without including method signatures.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.shift.md#2025-04-22_snippet_6\n\nLANGUAGE: rst\nCODE:\n```\n.. autosummary::\n    :nosignatures:\n\n    XShiftDetector\n```\n\n----------------------------------------\n\nTITLE: Downloading VOC Dataset with Python Script Using Default Path\nDESCRIPTION: Command to download and extract the Pascal VOC 2007 and 2012 datasets (combined as voc0712) to the current directory using AutoGluon's built-in CLI tool.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/prepare_voc.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m autogluon.multimodal.cli.prepare_detection_dataset --dataset_name voc0712\n```\n\n----------------------------------------\n\nTITLE: Importing AutoGluon EDA Analysis Model Components\nDESCRIPTION: This code snippet demonstrates how to import the model analysis components from AutoGluon's EDA module. It includes classes for model evaluation and quick fitting.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.model.md#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom autogluon.eda.analysis.model import AutoGluonModelEvaluator, AutoGluonModelQuickFit\n```\n\n----------------------------------------\n\nTITLE: Setting Text Segment Number\nDESCRIPTION: Configuration for the number of text segments in a token sequence, each with unique token type ID.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_39\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"model.hf_text.text_segment_num\": 2})\n# use 1 text segment\npredictor.fit(hyperparameters={\"model.hf_text.text_segment_num\": 1})\n```\n\n----------------------------------------\n\nTITLE: Downloading Pothole Dataset with AutoGluon CLI (Python)\nDESCRIPTION: This code snippet demonstrates how to use the AutoGluon CLI tool to download and prepare the Pothole dataset. It allows specifying the dataset name and output path.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/prepare_pothole.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython3 -m autogluon.multimodal.cli.prepare_detection_dataset --dataset_name pothole\n```\n\nLANGUAGE: python\nCODE:\n```\npython3 -m autogluon.multimodal.cli.prepare_detection_dataset --dataset_name pothole --output_path ~/data\n```\n\nLANGUAGE: python\nCODE:\n```\npython3 -m autogluon.multimodal.cli.prepare_detection_dataset -d pothole -o ~/data\n```\n\n----------------------------------------\n\nTITLE: Configuring TOC Tree for Object Detection Documentation\nDESCRIPTION: Sphinx toctree configuration for organizing object detection documentation pages. Sets up the table of contents structure with specific parameters for visibility and depth.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/quick_start/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{toctree}\n---\ncaption: Object Detection Quick Start\nmaxdepth: 1\nhidden: true\n---\n\nquick_start_coco\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Dependencies\nDESCRIPTION: Installs the AutoGluon tabular package with all optional dependencies\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-model-advanced.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.tabular[all]\n```\n\n----------------------------------------\n\nTITLE: Calculating Feature Importance\nDESCRIPTION: Uses AutoGluon's built-in permutation-based feature importance calculation on validation data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/interpret/SHAP with AutoGluon-Tabular Census income classification.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nval_data[label] = y_valid  # add labels to validation DataFrame\npredictor.feature_importance(val_data)\n```\n\n----------------------------------------\n\nTITLE: Setting Inference Batch Size Ratio\nDESCRIPTION: Configuration for adjusting the batch size during prediction or evaluation phases relative to training batch size.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"env.inference_batch_size_ratio\": 4})\n# use 2x per gpu batch size during prediction or evaluation\npredictor.fit(hyperparameters={\"env.inference_batch_size_ratio\": 2})\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon EDA Package\nDESCRIPTION: Installs the AutoGluon EDA package using pip\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-target-analysis.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.eda\n```\n\n----------------------------------------\n\nTITLE: Printing Classification Problem Types Information\nDESCRIPTION: This snippet prints information about binary and multiclass classification problem types using the helper function.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/problem_types_and_metrics.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Classification\nbinary_props = PROBLEM_TYPES_REG.get(BINARY)\nmulticlass_props = PROBLEM_TYPES_REG.get(MULTICLASS)\nprint_problem_type_info(\"Binary Classification\", binary_props)\nprint_problem_type_info(\"Multiclass Classification\", multiclass_props)\n```\n\n----------------------------------------\n\nTITLE: Configuring Temperature for Distillation in AutoMM Python\nDESCRIPTION: Sets the temperature value used to scale both the teacher's and student's logits before calculating the soft label loss during knowledge distillation (logits are divided by temperature). The default value for `distiller.temperature` in classification tasks is `5`. This is configured via `hyperparameters` in `predictor.fit()`.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_75\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM for classification\npredictor.fit(hyperparameters={\"distiller.temperature\": 5})\n# set temperature to 1\npredictor.fit(hyperparameters={\"distiller.temperature\": 1})\n```\n\n----------------------------------------\n\nTITLE: Installing OpenPyXL Dependency\nDESCRIPTION: Installing the OpenPyXL package required for Excel file handling.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal_text_tabular.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n!python3 -m pip install openpyxl\n```\n\n----------------------------------------\n\nTITLE: Saving Decoded Images to Disk for TabularPredictor Inference\nDESCRIPTION: Function to decode Base85 encoded image data, save the images to disk, and update the dataframe with file paths. This is necessary for TabularPredictor which requires image file paths rather than binary data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/cloud_fit_deploy/cloud-aws-sagemaker-train-deploy.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimage_index = 0\n\n\ndef _save_image_and_update_dataframe_column(bytes):\n    global image_index\n    im = Image.open(BytesIO(base64.b85decode(bytes)))\n    im_name = f'Image_{image_index}.png'\n    im.save(im_name)\n    image_index += 1\n\n    return im_name\n\n\ntest_data[image_column] = [_save_image_and_update_dataframe_column(bytes) for bytes in test_data[image_column]]\n```\n\n----------------------------------------\n\nTITLE: Installing scikit-learn-intelex for LinearModel Acceleration\nDESCRIPTION: Command to install scikit-learn-intelex package for accelerating LinearModel training and inference by 20x. This is required to achieve the improved performance for LinearModel.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/whats_new/v0.4.0.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install \"scikit-learn-intelex>=2021.5,<2021.6\"\n```\n\n----------------------------------------\n\nTITLE: Using Sklearn's Mean Squared Error\nDESCRIPTION: Example of using sklearn's built-in mean squared error metric function.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-metric.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsklearn.metrics.mean_squared_error(y_true, y_pred)\n```\n\n----------------------------------------\n\nTITLE: TabularPredictor Multi-Model Prediction\nDESCRIPTION: New methods predict_multi and predict_proba_multi for efficient predictions from multiple models\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/whats_new/v0.7.0.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npredictor.predict_multi(data)\npredictor.predict_proba_multi(data)\n```\n\n----------------------------------------\n\nTITLE: Working with TabularPredictor in Python\nDESCRIPTION: Illustrates how to access and utilize a TabularPredictor for one label. It also suggests model configurations for optimizing resource usage.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-multilabel.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\npredictor_class = multi_predictor.get_predictor('class')\npredictor_class.leaderboard()\n```\n\n----------------------------------------\n\nTITLE: Setting up Python Environment\nDESCRIPTION: Imports required libraries and sets up matplotlib, numpy with seed initialization and warning suppression.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/beginner_text.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n%matplotlib inline\n\nimport numpy as np\nimport warnings\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings('ignore')\nnp.random.seed(123)\n```\n\n----------------------------------------\n\nTITLE: Handling Missing Values in Feature Engineering with Python\nDESCRIPTION: This code shows how AutoGluon handles missing values during feature engineering by setting the first row to NaN and reprocessing the DataFrame.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-feature-engineering.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndfx.iloc[0] = np.nan\ndfx.head()\n\nauto_ml_pipeline_feature_generator = AutoMLPipelineFeatureGenerator()\nauto_ml_pipeline_feature_generator.fit_transform(X=dfx)\n```\n\n----------------------------------------\n\nTITLE: RST Role Definition for Hidden Sections\nDESCRIPTION: Defines a custom RST role for hidden sections in the documentation.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.interaction.md#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. role:: hidden\n    :class: hidden-section\n```\n\n----------------------------------------\n\nTITLE: Defining Hidden Section Role in reStructuredText\nDESCRIPTION: Defines a custom role 'hidden' in reStructuredText for creating hidden sections in the documentation.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.missing.md#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. role:: hidden\n    :class: hidden-section\n```\n\n----------------------------------------\n\nTITLE: Checking Unique Classes\nDESCRIPTION: Displays the unique classes in the target variable\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-essentials.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlabel = 'class'\nprint(f\"Unique classes: {list(train_data[label].unique())}\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Final Validation in AutoMM\nDESCRIPTION: Examples showing how to control whether to perform a final validation after training is complete in AutoMM.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"optim.skip_final_val\": False})\n# skip the final validation\npredictor.fit(hyperparameters={\"optim.skip_final_val\": True})\n```\n\n----------------------------------------\n\nTITLE: Printing Score Range for Sentence Similarity Dataset\nDESCRIPTION: This code prints the minimum and maximum scores in the training dataset to understand the range of similarity scores.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/beginner_text.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprint('Min score=', min(sts_train_data['score']), ', Max score=', max(sts_train_data['score']))\n```\n\n----------------------------------------\n\nTITLE: Printing Named Entity Recognition Problem Type Information\nDESCRIPTION: This code prints information about the named entity recognition (NER) problem type using the helper function.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/problem_types_and_metrics.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Named Entity Recognition\nner_props = PROBLEM_TYPES_REG.get(NER)\nprint_problem_type_info(\"Named Entity Recognition\", ner_props)\n```\n\n----------------------------------------\n\nTITLE: Setting Anomaly Detection Parameters\nDESCRIPTION: Defining threshold parameter for anomaly detection visualization.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-anomaly-detection.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nthreshold_stds = 3\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon EDA Package\nDESCRIPTION: Installs the AutoGluon EDA package using pip. This package is required for the automated dataset overview functionality.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-dataset-overview.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.eda\n```\n\n----------------------------------------\n\nTITLE: Listing VOC Dataset Split Files\nDESCRIPTION: Shows the common split files found in the ImageSets/Main directory of a VOC dataset that define train, validation, and test sets.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/voc_to_coco.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntrain.txt\nval.txt\ntest.txt\n...\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Dependencies in Kaggle Environment in Python\nDESCRIPTION: This snippet demonstrates how to install AutoGluon and its dependencies from a pre-downloaded package in Kaggle's environment without internet access, using ANTLR runtime and wheel files.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/kaggle_pawpularity/README.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport sys\nsys.path.append('../input/autogluon-standalone-install/autogluon_standalone/antlr4-python3-runtime-4.8/antlr4-python3-runtime-4.8/src/')\n!pip install --no-deps --no-index --quiet ../input/autogluon-standalone-install/autogluon_standalone/*.whl --find-links autogluon_standalone\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon MultiModal Package in Python\nDESCRIPTION: Installs the AutoGluon MultiModal package using pip.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/zero_shot_img_txt_matching.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.multimodal\n```\n\n----------------------------------------\n\nTITLE: Evaluating Conv-LoRA Fine-tuned SAM Models\nDESCRIPTION: Command to evaluate a trained checkpoint on a specific dataset. This allows testing the performance of the fine-tuned model on the test set.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/Conv-LoRA/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npython3 run_semantic_segmentation.py --task {dataset_name} --output_dir {output_dir} --ckpt_path {ckpt_path} --eval\n```\n\n----------------------------------------\n\nTITLE: Installing Tesseract OCR on Ubuntu\nDESCRIPTION: Command to install the Tesseract OCR package on Ubuntu systems, which is required for document data processing.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/document_prediction/index.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install tesseract-ocr\n```\n\n----------------------------------------\n\nTITLE: Printing Regression Problem Type Information\nDESCRIPTION: This code prints information about the regression problem type using the helper function.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/problem_types_and_metrics.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Regression\nregression_props = PROBLEM_TYPES_REG.get(REGRESSION)\nprint_problem_type_info(\"Regression\", regression_props)\n```\n\n----------------------------------------\n\nTITLE: Table of Contents Configuration for Documentation\nDESCRIPTION: Toctree configuration for organizing documentation structure, specifying maxdepth and visibility settings for custom model documentation.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/advanced/index.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n---\nmaxdepth: 1\nhidden: true\n---\n\nCustom Models <forecasting-custom-model>\n```\n```\n\n----------------------------------------\n\nTITLE: AutoSummary for Visualization Explain Classes\nDESCRIPTION: Directive that generates a summary table listing the ExplainForcePlot and ExplainWaterfallPlot classes without their signatures.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.explain.md#2025-04-22_snippet_2\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autosummary::\n    :nosignatures:\n\n    ExplainForcePlot\n    ExplainWaterfallPlot\n```\n\n----------------------------------------\n\nTITLE: Data Cleaning and Feature Removal\nDESCRIPTION: Removing PassengerId column from both training and test datasets.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-anomaly-detection.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nx = x.drop(columns=['PassengerId'], errors='ignore')\nx_test = x_test.drop(columns=['PassengerId'], errors='ignore')\n```\n\n----------------------------------------\n\nTITLE: Setting Logging Frequency in AutoMM\nDESCRIPTION: Examples showing how to set the logging frequency in AutoMM, determining how often to log metrics during training steps.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"optim.log_every_n_steps\": 10})\n# log once every 50 steps\npredictor.fit(hyperparameters={\"optim.log_every_n_steps\": 50})\n```\n\n----------------------------------------\n\nTITLE: Calling Predictor Evaluate with Weight Evaluation\nDESCRIPTION: Example of predictor evaluation calls that were previously causing crashes when weight_evaluation was set to True. This issue has been fixed in the update.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/whats_new/v0.4.1.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npredictor.evaluate(...)\npredictor.evaluate_predictions(...)\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Dependencies\nDESCRIPTION: Installation of required AutoGluon packages including EDA and tabular modules with LightGBM support.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-quick-fit.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.eda\n!pip install autogluon.tabular[lightgbm]\n```\n\n----------------------------------------\n\nTITLE: Running AutoGluon Benchmarks for Cancer Survival Prediction\nDESCRIPTION: Commands to run the cancer survival prediction example using either all models or just the FT_Transformer model. The script accepts parameters for specifying the dataset, execution mode, save path, and computing resources.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/TCGA_cancer_survival/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Benchmark on multiple AG models\npython3 example_cancer_survival.py --task TCGA_HNSC --mode all_models\n# Just on FT_Transformer \npython3 example_cancer_survival.py --task TCGA_HNSC --mode FT_Transformer\n```\n\n----------------------------------------\n\nTITLE: Adding Duplicated Column for Near-Duplicates Detection\nDESCRIPTION: Adds a duplicated 'Fare' column to both train and test datasets to showcase the near-duplicates detection functionality in the automated overview.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-dataset-overview.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf_train['Fare_duplicate'] = df_train['Fare']\ndf_test['Fare_duplicate'] = df_test['Fare']\n```\n\n----------------------------------------\n\nTITLE: Downloading and Displaying a Second Test Image\nDESCRIPTION: Downloads an image of Segways and displays it in the notebook for demonstrating CLIP's ability to identify less common object classes.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_prediction/clip_zeroshot.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nurl = \"https://live.staticflickr.com/7236/7114602897_9cf00b2820_b.jpg\"\nsegway_image = download(url)\n\npil_img = Image(filename=segway_image)\ndisplay(pil_img)\n```\n\n----------------------------------------\n\nTITLE: Creating Custom VOC to COCO Splits with Specific Ratios\nDESCRIPTION: Command to convert VOC dataset to COCO format while creating custom train/validation/test splits with specified ratios (0.6/0.2/0.2).\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/voc_to_coco.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m autogluon.multimodal.cli.voc2coco --root_dir ./VOCdevkit/VOC2007 --train_ratio 0.6 --val_ratio 0.2\n```\n\n----------------------------------------\n\nTITLE: Setting up AutoGluon Environment with CUDA and Ray\nDESCRIPTION: Creates a new conda environment 'ag' with Python 3.11, installs AutoGluon with CUDA-enabled PyTorch, and Ray for distributed training optimization. Uses mamba for faster package installation.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/install-linux-conda-gpu.md#2025-04-22_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nconda create -n ag python=3.11\nconda activate ag\nconda install -c conda-forge mamba\nmamba install -c conda-forge autogluon \"pytorch=*=cuda*\"\nmamba install -c conda-forge \"ray-tune >=2.10.0,<2.32\" \"ray-default >=2.10.0,<2.32\"  # install ray for faster training\n```\n\n----------------------------------------\n\nTITLE: Defining Hidden Table of Contents in Markdown\nDESCRIPTION: This code snippet defines a hidden table of contents using Markdown syntax. It includes links to base APIs and auto components documentation pages.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/references/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n:maxdepth: 1\n:hidden:\n\nBase APIs <autogluon.eda.base-apis.md>\nAuto Components <autogluon.eda.auto.md>\n```\n```\n\n----------------------------------------\n\nTITLE: Custom Split COCO Format Files\nDESCRIPTION: Lists the COCO format JSON files generated in the Annotations folder after creating custom train/validation/test splits.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/voc_to_coco.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nusersplit_train_cocoformat.json\nusersplit_val_cocoformat.json\nusersplit_test_cocoformat.json\n```\n\n----------------------------------------\n\nTITLE: Running Specific Unit Test File with pytest\nDESCRIPTION: Command to run unit tests from a specific file (test_utils.py) using pytest, allowing focused testing on particular components.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/multimodal/tests/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npytest unittests/test_utils.py\n```\n\n----------------------------------------\n\nTITLE: Installing NeuralForecast Library\nDESCRIPTION: Installs the NeuralForecast library, which contains the implementation of the custom NHITS model used in this tutorial.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/advanced/forecasting-custom-model.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npip install -q neuralforecast==2.0\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing PDF Dataset for Classification\nDESCRIPTION: Downloads a PDF document dataset containing resumes and historical documents, then splits it into training and test sets with an 80-20 ratio.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/document_prediction/pdf_classification.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport pandas as pd\nfrom autogluon.core.utils.loaders import load_zip\n\ndownload_dir = './ag_automm_tutorial_pdf_classifier'\nzip_file = \"https://automl-mm-bench.s3.amazonaws.com/doc_classification/pdf_docs_small.zip\"\nload_zip.unzip(zip_file, unzip_dir=download_dir)\n\ndataset_path = os.path.join(download_dir, \"pdf_docs_small\")\npdf_docs = pd.read_csv(f\"{dataset_path}/data.csv\")\ntrain_data = pdf_docs.sample(frac=0.8, random_state=200)\ntest_data = pdf_docs.drop(train_data.index)\n```\n\n----------------------------------------\n\nTITLE: Configuring SEP Token Insertion\nDESCRIPTION: Toggle for inserting SEP tokens between texts from different dataframe columns.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_38\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"model.hf_text.insert_sep\": True})\n# use no SEP token.\npredictor.fit(hyperparameters={\"model.hf_text.insert_sep\": False})\n```\n\n----------------------------------------\n\nTITLE: Markdown Grid Layout for Advanced Forecasting Documentation\nDESCRIPTION: Grid layout structure defining documentation sections for advanced forecasting features, specifically linking to custom model implementation tutorials.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/advanced/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n::::{grid} 2\n  :gutter: 3\n\n:::{grid-item-card} Custom Models\n  :link: advanced/forecasting-custom-model.html\n\n  How to add a custom time series forecasting model to AutoGluon.\n:::\n\n::::\n```\n\n----------------------------------------\n\nTITLE: AutoModule Import for Analysis Interaction\nDESCRIPTION: RST directive to import and document the analysis.interaction module.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.interaction.md#2025-04-22_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: autogluon.eda.analysis.interaction\n```\n\n----------------------------------------\n\nTITLE: Running Experiments for All Datasets using Bash\nDESCRIPTION: Bash command to execute experiments on all datasets using a predefined script. This is used to reproduce the AutoMM FT-Transformer results.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/tabular_dl/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbash run_all.sh\n```\n\n----------------------------------------\n\nTITLE: Installing VowpalWabbit with AutoGluon\nDESCRIPTION: Commands to install AutoGluon with VowpalWabbit support. VowpalWabbit is an optional dependency that must be installed separately.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/whats_new/v0.4.0.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install autogluon.tabular[all, vowpalwabbit]\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install \"vowpalwabbit>=8.10,<8.11\"\n```\n\n----------------------------------------\n\nTITLE: Examining Target Variable Distribution\nDESCRIPTION: Displays a summary of the target variable ('class') to understand the distribution of income levels in the dataset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/tabular-fasttext.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nlabel = 'class'\nprint(\"Summary of class variable: \\n\", train_data[label].describe())\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon MultiModal Package\nDESCRIPTION: This snippet shows how to install the AutoGluon MultiModal package using pip.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/ner.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.multimodal\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Tabular Package\nDESCRIPTION: This snippet installs the AutoGluon Tabular package with all optional dependencies using pip.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-multilabel.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.tabular[all]\n```\n\n----------------------------------------\n\nTITLE: Importing AutoGluon Time Series Modules\nDESCRIPTION: Imports the necessary modules from pandas and AutoGluon for time series forecasting.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-quick-start.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon EDA and Tabular packages\nDESCRIPTION: Installation commands for the AutoGluon EDA module and the tabular module with LightGBM support, which are required for the covariate shift analysis.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-covariate-shift.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.eda\n!pip install autogluon.tabular[lightgbm]\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon using UV Package Installer\nDESCRIPTION: This snippet demonstrates how to install AutoGluon using UV, a faster alternative to pip. The installation process includes two steps: first installing the UV package installer, then using it to install AutoGluon with PyTorch CPU dependencies.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/install-cpu-uv.md#2025-04-22_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n# Install UV package installer (faster than pip)\npip install -U uv\n\n# Install AutoGluon\npython -m uv pip install autogluon --extra-index-url https://download.pytorch.org/whl/cpu\n```\n\n----------------------------------------\n\nTITLE: Creating Hidden Table of Contents for Image Segmentation in Markdown\nDESCRIPTION: A toctree directive that creates a hidden table of contents with a single entry pointing to the semantic segmentation beginner guide. The maxdepth parameter limits the depth of the table to 1 level.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_segmentation/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n---\nmaxdepth: 1\nhidden: true\n---\n\nbeginner_semantic_seg\n```\n```\n\n----------------------------------------\n\nTITLE: Importing Additional Required Packages\nDESCRIPTION: Imports os module and load_zip function from AutoGluon's core utilities.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/advanced/finetune_coco.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom autogluon.core.utils.loaders import load_zip\n```\n\n----------------------------------------\n\nTITLE: Visualization Components Summary\nDESCRIPTION: RST directive listing the main visualization classes for feature interactions.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.interaction.md#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. autosummary::\n    :nosignatures:\n\n    CorrelationVisualization\n    CorrelationSignificanceVisualization\n    FeatureInteractionVisualization\n    FeatureDistanceAnalysisVisualization\n    PDPInteractions\n```\n\n----------------------------------------\n\nTITLE: Downloading Watercolor Dataset with Python CLI\nDESCRIPTION: Uses the AutoGluon CLI to download the Watercolor dataset. The script can extract the dataset in the current directory or a specified output path.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/prepare_watercolor.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython3 -m autogluon.multimodal.cli.prepare_detection_dataset --dataset_name watercolor\n```\n\nLANGUAGE: python\nCODE:\n```\npython3 -m autogluon.multimodal.cli.prepare_detection_dataset --dataset_name watercolor --output_path ~/data\n```\n\nLANGUAGE: python\nCODE:\n```\npython3 -m autogluon.multimodal.cli.prepare_detection_dataset -d watercolor -o ~/data\n```\n\n----------------------------------------\n\nTITLE: Setting Model Configuration\nDESCRIPTION: Sets the checkpoint name to YOLOX-small and specifies the number of GPUs to use.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/advanced/finetune_coco.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncheckpoint_name = \"yolox_s\"\nnum_gpus = 1  # only use one GPU\n```\n\n----------------------------------------\n\nTITLE: Listing Sphinx Documentation Dependencies\nDESCRIPTION: This snippet lists the Sphinx-related Python packages required for building the AutoGluon documentation. It includes the core Sphinx package, the furo theme, and various extensions for enhanced documentation features like copybutton, design components, tabs, togglebuttons, OpenGraph metadata, and Google Analytics integration.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/requirements_doc.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nfuro\nmyst_nb\nsphinx\nsphinx-copybutton\nsphinx-design\nsphinx-inline-tabs\nsphinx-togglebutton\nsphingext-opengraph\nsphixcontrib-googleanalytics\n```\n\n----------------------------------------\n\nTITLE: Importing AutoGluon EDA Visualization Shift Module\nDESCRIPTION: RST directive to import the AutoGluon EDA visualization shift module, which contains components for visualizing distribution shifts.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.shift.md#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: autogluon.eda.visualization.shift\n```\n\n----------------------------------------\n\nTITLE: Downloading VOC Dataset with Python Script Using Short Arguments\nDESCRIPTION: Shortened command version to download and extract the Pascal VOC dataset using abbreviated command-line arguments for dataset name and output path.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/prepare_voc.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m autogluon.multimodal.cli.prepare_detection_dataset -d voc -o ~/data\n```\n\n----------------------------------------\n\nTITLE: Using sklearn Accuracy Metric\nDESCRIPTION: Demonstrates usage of scikit-learn's built-in accuracy score function\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-metric.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport sklearn.metrics\n\nsklearn.metrics.accuracy_score(y_true, y_pred)\n```\n\n----------------------------------------\n\nTITLE: Decoding Base85 Image Strings in MultiModalPredictor Inference\nDESCRIPTION: Simple one-line function to decode Base85 encoded image bytes back to binary form during inference. This processes the encoded image data for use with MultiModalPredictor.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/cloud_fit_deploy/cloud-aws-sagemaker-train-deploy.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntest_data[image_column] = [base64.b85decode(bytes) for bytes in test_data[image_column]]\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon MultiModal Package\nDESCRIPTION: Installs the AutoGluon MultiModal package which is required to run the examples in this tutorial.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/focal_loss.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.multimodal\n```\n\n----------------------------------------\n\nTITLE: Dependency Version Updates for AutoGluon\nDESCRIPTION: List of updated package dependencies with their version requirements for AutoGluon v1.2, including updates to core packages like numpy, torch, ray, and scikit-learn.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/whats_new/v1.2.0.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nnumpy >= 1.25.0,<2.1.4\nscipython >= 1.5.4,<1.16\ntorch >= 2.2,<2.6\nray >= 2.10.0,<2.40\nscikit-learn >= 1.4.0,<1.5.3\nmatplotlib >= 3.7.0,<3.11\npyarrow >= 15.0.0\npsutil >= 5.7.3,<7.0.0\nPillow >= 10.0.1,<12\nxgboost >= 1.6,<2.2\ntorchvision >= 0.16.0,<0.21.0\nnltk >= 3.4.5,<3.9\ntimm >= 0.9.5,<1.0.7\nlightning >= 2.2,<2.6\nasync_timeout >= 4.0,<6\ntransformers > 4.38.0,<5\naccelerate >= 0.34.0,<1.0\nlightgbm >= 4.0,<4.6\nscikit-learn-intelex >= 2024.0,<2025.1\n```\n\n----------------------------------------\n\nTITLE: Loading Titanic Dataset\nDESCRIPTION: Loads the Titanic train and test datasets from S3 and defines the target column.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-analyze-interaction.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndf_train = pd.read_csv('https://autogluon.s3.amazonaws.com/datasets/titanic/train.csv')\ndf_test = pd.read_csv('https://autogluon.s3.amazonaws.com/datasets/titanic/test.csv')\ntarget_col = 'Survived'\n```\n\n----------------------------------------\n\nTITLE: Printing Few-shot Classification Problem Type Information\nDESCRIPTION: This code prints information about the few-shot classification problem type using the helper function.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/problem_types_and_metrics.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Few-shot Classification\nfew_shot_props = PROBLEM_TYPES_REG.get(FEW_SHOT_CLASSIFICATION)\nprint_problem_type_info(\"Few-shot Classification\", few_shot_props)\n```\n\n----------------------------------------\n\nTITLE: Printing Object Detection Problem Type Information\nDESCRIPTION: This snippet prints information about the object detection problem type using the helper function.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/problem_types_and_metrics.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Object Detection\nobject_detection_props = PROBLEM_TYPES_REG.get(OBJECT_DETECTION)\nprint_problem_type_info(\"Object Detection\", object_detection_props)\n```\n\n----------------------------------------\n\nTITLE: Importing AutoGluon EDA Analysis Module for Missing Values\nDESCRIPTION: Imports and documents the autogluon.eda.analysis.missing module, which contains tools for analyzing missing values in datasets.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.missing.md#2025-04-22_snippet_5\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: autogluon.eda.analysis.missing\n```\n\n----------------------------------------\n\nTITLE: Creating Conda Environment\nDESCRIPTION: Command to create a new conda environment for AutoGluon\nSOURCE: https://github.com/autogluon/autogluon/blob/master/release_instructions/update-conda-recipes.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmamba create -n ag python=3.9\n```\n\n----------------------------------------\n\nTITLE: Importing AutoGluon EDA Visualization Module for Missing Values\nDESCRIPTION: Imports and documents the autogluon.eda.visualization.missing module, which contains tools for visualizing missing values in datasets.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.missing.md#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: autogluon.eda.visualization.missing\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Multimodal Package\nDESCRIPTION: Installation command for the AutoGluon multimodal package using pip.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal_text_tabular.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.multimodal\n```\n\n----------------------------------------\n\nTITLE: Loading and Sampling Training Data\nDESCRIPTION: Loads training data from a CSV file into a TabularDataset and creates a subsample for demonstration\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-essentials.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntrain_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv')\nsubsample_size = 500  # subsample subset of data for faster demo, try setting this to much larger values\ntrain_data = train_data.sample(n=subsample_size, random_state=0)\ntrain_data.head()\n```\n\n----------------------------------------\n\nTITLE: Verifying Preprocessing Results\nDESCRIPTION: Checks the structure of the preprocessed data to confirm that data types are normalized and missing values are handled properly.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/advanced/forecasting-custom-model.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Types of the columns in preprocessed data:\")\nprint(data.dtypes)\nprint(\"\\nTypes of the columns in preprocessed static features:\")\nprint(data.static_features.dtypes)\n\nprint(\"\\nNumber of missing values per column:\")\nprint(data.isna().sum())\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents in Markdown\nDESCRIPTION: This code snippet defines a hidden table of contents for the time series forecasting documentation using Markdown and Sphinx toctree directive. It includes links to various sections such as quick start, in-depth tutorial, Chronos forecasting, model zoo, metrics, and advanced topics.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n---\nmaxdepth: 2\nhidden: true\n---\n\nQuick Start <forecasting-quick-start>\nIn Depth <forecasting-indepth>\nForecasting with Chronos <forecasting-chronos>\nModel Zoo <forecasting-model-zoo>\nMetrics <forecasting-metrics>\nAdvanced <advanced/index>\n```\n```\n\n----------------------------------------\n\nTITLE: Creating SHAP Summary Plot for AutoGluon Predictions in Python\nDESCRIPTION: This code creates a summary plot of SHAP values to visualize the impact of all features across multiple predictions.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/interpret/SHAP with AutoGluon-Tabular Diabetes regression.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nshap.summary_plot(shap_values, X_valid.iloc[0:N_VAL,:])\n```\n\n----------------------------------------\n\nTITLE: Defining Baseline Reference Values for SHAP Analysis in Python\nDESCRIPTION: This code creates a summary of the training data using k-means clustering to serve as baseline reference values for SHAP analysis.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/interpret/SHAP with AutoGluon-Tabular Diabetes regression.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nX_train_summary = shap.kmeans(X_train, 10)\nprint(\"Baseline feature-values: \\n\", X_train_summary)\n```\n\n----------------------------------------\n\nTITLE: Installing Compatible LibOMP Version on MacOS\nDESCRIPTION: Commands to uninstall existing libomp, download a specific compatible version, and install it using Homebrew. This process ensures LightGBM will work correctly without segmentation faults on MacOS systems.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/install-mac-libomp.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Uninstall libomp if it was previous installed\nbrew uninstall -f libomp\nwget https://raw.githubusercontent.com/Homebrew/homebrew-core/fb8323f2b170bd4ae97e1bac9bf3e2983af3fdb0/Formula/libomp.rb\nbrew install libomp.rb\nrm libomp.rb\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Dependencies\nDESCRIPTION: Installation command for AutoGluon with all optional dependencies\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-metric.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.tabular[all]\n```\n\n----------------------------------------\n\nTITLE: Loading the Titanic dataset for covariate shift analysis\nDESCRIPTION: Loads the training and testing portions of the Titanic dataset from Amazon S3 and defines the target column for analysis.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-covariate-shift.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndf_train = pd.read_csv('https://autogluon.s3.amazonaws.com/datasets/titanic/train.csv')\ndf_test = pd.read_csv('https://autogluon.s3.amazonaws.com/datasets/titanic/test.csv')\ntarget_col = 'Survived'\n```\n\n----------------------------------------\n\nTITLE: Printing Feature Extraction Problem Type Information\nDESCRIPTION: This snippet prints information about the feature extraction problem type using the helper function.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/problem_types_and_metrics.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Feature Extraction\nfeature_extraction_props = PROBLEM_TYPES_REG.get(FEATURE_EXTRACTION)\nprint_problem_type_info(\"Feature Extraction\", feature_extraction_props)\n```\n\n----------------------------------------\n\nTITLE: Downloading Semantic Segmentation Datasets\nDESCRIPTION: Python command to download the segmentation datasets required for Conv-LoRA fine-tuning experiments.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/Conv-LoRA/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython prepare_semantic_segmentation_datasets.py\n```\n\n----------------------------------------\n\nTITLE: Summarizing MissingValues Class in AutoGluon EDA\nDESCRIPTION: Creates a summary of the MissingValues class from the autogluon.eda.visualization.missing module, hiding method signatures.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.missing.md#2025-04-22_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n.. autosummary::\n    :nosignatures:\n\n    MissingValues\n```\n\n----------------------------------------\n\nTITLE: Implementing Text Normalization Function with Encoding Conversion\nDESCRIPTION: A function that resolves text encoding issues and normalizes text by applying a series of encoding and decoding operations. It handles special characters and converts them to standard ASCII equivalents using the unidecode function.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/kaggle_feedback_prize/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef resolve_encodings_and_normalize(text: str) -> str:\n    text = (\n        text.encode(\"raw_unicode_escape\")\n        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n    )\n    text = unidecode(text)\n    return text\n```\n\n----------------------------------------\n\nTITLE: ShapAnalysis Class Documentation\nDESCRIPTION: Directive to include detailed documentation for the ShapAnalysis class, specifically documenting its init method.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.explain.md#2025-04-22_snippet_8\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autoclass:: ShapAnalysis\n   :members: init\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Dependencies\nDESCRIPTION: Pip commands to install and upgrade AutoGluon package prerequisites\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-quick-start.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!python -m pip install --upgrade pip\n!python -m pip install autogluon\n```\n\n----------------------------------------\n\nTITLE: Analyzing Interaction for Pclass and Survived\nDESCRIPTION: Visualizes the interaction between 'Pclass' and 'Survived' features for both train and test data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-analyze-interaction.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nauto.analyze_interaction(x='Pclass', y='Survived', train_data=df_train, test_data=df_test)\n```\n\n----------------------------------------\n\nTITLE: Citing AutoGluon Tabular Distillation in BibTeX\nDESCRIPTION: BibTeX entry for citing AutoGluon Tabular's model distillation functionality, which provides fast, accurate, and simple models for tabular data through augmented distillation techniques.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/CITING.md#2025-04-22_snippet_5\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{agtabulardistill,\n  title={Fast, Accurate, and Simple Models for Tabular Data via Augmented Distillation},\n  author={Fakoor, Rasool and Mueller, Jonas W and Erickson, Nick and Chaudhari, Pratik and Smola, Alexander J},\n  journal={Advances in Neural Information Processing Systems},\n  volume={33},\n  year={2020}\n}\n```\n\n----------------------------------------\n\nTITLE: Citation for Watercolor Dataset\nDESCRIPTION: Provides the BibTeX citation for the Watercolor dataset paper.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/prepare_watercolor.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: bibtex\nCODE:\n```\n@inproceedings{inoue_2018_cvpr,\n    author = {Inoue, Naoto and Furuta, Ryosuke and Yamasaki, Toshihiko and Aizawa, Kiyoharu},\n    title = {Cross-Domain Weakly-Supervised Object Detection Through Progressive Domain Adaptation},\n    booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n    month = {June},\n    year = {2018}\n}\n```\n\n----------------------------------------\n\nTITLE: Processing Named Entity Recognition Exports for AutoGluon in Python\nDESCRIPTION: This snippet demonstrates how to use the 'from_named_entity_recognition' function to transform Label Studio named entity recognition exports into a DataFrame suitable for AutoGluon input. It includes optional parameters for specifying data and label columns.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/label_studio_export_reader/LabelStudio_export_file_reader.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom_named_entity_recognition(path, data_columns=None, label_columns=None)\n```\n\n----------------------------------------\n\nTITLE: Multimodal Benchmarking Command for AutoGluon\nDESCRIPTION: Command to trigger automated benchmarking for the multimodal module with specified preset and GPU resource allocation.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/CONTRIBUTING.md#2025-04-22_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n/benchmark module=multimodal preset=multimodal_best benchmark=automm-image time_limit=g4_12x\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Dependencies\nDESCRIPTION: Installation command for AutoGluon with all optional dependencies\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-model.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.tabular[all]\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents in Markdown\nDESCRIPTION: This code snippet defines a hidden table of contents for the documentation using the toctree directive in Markdown. It includes links to various sections of the tabular prediction documentation.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n---\nmaxdepth: 2\nhidden: true\n---\n\nEssentials <tabular-essentials>\nIn Depth <tabular-indepth>\nHow It Works <how-it-works>\nFeature Engineering <tabular-feature-engineering>\nTabular + Text + Images <tabular-multimodal>\nAdvanced <advanced/index>\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging for AutoGluon\nDESCRIPTION: Sets up basic logging configuration to display timestamps and function names for debugging purposes.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/tabular-fasttext.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nlogging.basicConfig(format='%(asctime)s: [%(funcName)s] %(message)s',\n                   level=logging.INFO)\nlogger = logging.getLogger(__name__)\n```\n\n----------------------------------------\n\nTITLE: Generated COCO Format Files\nDESCRIPTION: Lists the COCO format JSON files generated in the Annotations folder after converting existing VOC splits.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/voc_to_coco.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ntrain_cocoformat.json\nval_cocoformat.json\ntest_cocoformat.json\n...\n```\n\n----------------------------------------\n\nTITLE: Excluding Models in AutoGluon\nDESCRIPTION: Shows how to exclude specific model types from training in AutoGluon using the excluded_model_types parameter.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-faq.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntask.fit(..., excluded_model_types=['KNN','RF','XT'])\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Tabular with SKEX Optimization\nDESCRIPTION: Command to install the tabular submodule with SKEX dependency which speeds up KNN models by 25x in training and inference on CPU.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/install-modules.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install autogluon.tabular[all,skex]\n```\n\n----------------------------------------\n\nTITLE: Citing AutoGluon-Tabular in BibTeX\nDESCRIPTION: BibTeX entry for citing the core AutoGluon paper on tabular data AutoML. This citation should be used for general usage of AutoGluon, regardless of the specific module being used.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/CITING.md#2025-04-22_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{agtabular,\n  title={AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data},\n  author={Erickson, Nick and Mueller, Jonas and Shirkov, Alexander and Zhang, Hang and Larroy, Pedro and Li, Mu and Smola, Alexander},\n  journal={arXiv preprint arXiv:2003.06505},\n  year={2020}\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Ignore Label for SAM in AutoMM\nDESCRIPTION: Specifies a target value to be ignored in loss and metric calculations for SAM using the 'model.sam.ignore_label' hyperparameter.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_56\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"model.sam.ignore_label\": 255})\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Multimodal Package\nDESCRIPTION: Installs the required AutoGluon multimodal package using pip.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/chinese_ner.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.multimodal\n```\n\n----------------------------------------\n\nTITLE: Creating Custom AutoGluon Scorer\nDESCRIPTION: Converts sklearn's accuracy metric into an AutoGluon scorer with additional metadata about the metric's behavior and requirements\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-custom-metric.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.core.metrics import make_scorer\n\nag_accuracy_scorer = make_scorer(name='accuracy',\n                                 score_func=sklearn.metrics.accuracy_score,\n                                 optimum=1,\n                                 greater_is_better=True,\n                                 needs_class=True)\n```\n\n----------------------------------------\n\nTITLE: Citing Chronos Pretrained Model in BibTeX\nDESCRIPTION: BibTeX entry for citing the Chronos pretrained model, which is used in AutoGluon's time series forecasting module for learning the language of time series.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/CITING.md#2025-04-22_snippet_4\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{ansari2024chronos,\n  author  = {Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\n  title   = {Chronos: Learning the Language of Time Series},\n  journal = {arXiv preprint arXiv:2403.07815},\n  year    = {2024}\n}\n```\n\n----------------------------------------\n\nTITLE: Platform Test Command for AutoGluon\nDESCRIPTION: Command to trigger platform tests for Linux, MacOS, and Windows across supported Python versions. Must be run from the official autogluon/autogluon repository.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/CONTRIBUTING.md#2025-04-22_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n/platform_tests\n```\n\n----------------------------------------\n\nTITLE: Analyzing Missing Values\nDESCRIPTION: Uses AutoGluon EDA to analyze missing values in the training dataset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-analyze-interaction.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport autogluon.eda.auto as auto\n\nauto.missing_values_analysis(train_data=df_train)\n```\n\n----------------------------------------\n\nTITLE: Displaying Time Series Cheat Sheet Image in Markdown\nDESCRIPTION: This code snippet displays the image of the AutoGluon Time Series cheat sheet using Markdown syntax. It specifies the image source and sets the width to 900 pixels.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/cheatsheet.md#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n{image} https://raw.githubusercontent.com/Innixma/autogluon-doc-utils/main/docs/cheatsheets/stable/timeseries/autogluon-cheat-sheet-ts.jpeg\n:width: 900\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Tabular Package in Python\nDESCRIPTION: Installs the AutoGluon tabular package with all dependencies using pip.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-deployment.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.tabular[all]\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents in Markdown\nDESCRIPTION: This code snippet defines a hidden table of contents for the text prediction documentation, listing the available guides and tutorials.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/text_prediction/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n---\nmaxdepth: 1\nhidden: true\n---\n\nbeginner_text\nner\nchinese_ner\nmultilingual_text\n```\n```\n\n----------------------------------------\n\nTITLE: Citation for Pothole Dataset (BibTeX)\nDESCRIPTION: This code snippet provides the BibTeX citation for the Pothole dataset, referencing the original paper that introduced the dataset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/prepare_pothole.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bibtex\nCODE:\n```\n@inproceedings{inoue_2018_cvpr,\n    author = {Inoue, Naoto and Furuta, Ryosuke and Yamasaki, Toshihiko and Aizawa, Kiyoharu},\n    title = {Cross-Domain Weakly-Supervised Object Detection Through Progressive Domain Adaptation},\n    booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n    month = {June},\n    year = {2018}\n}\n```\n\n----------------------------------------\n\nTITLE: Installing MMCV and MMDet Dependencies\nDESCRIPTION: Installs necessary dependencies including MMCV, MMDet, and MMEngine with specific versions compatible with CUDA 12.4 and PyTorch 2.5.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/advanced/finetune_coco.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n!pip install -U pip setuptools wheel\n!sudo apt-get install -y ninja-build gcc g++\n!python3 -m mim install \"mmcv==2.1.0\"\n!python3 -m pip install \"mmdet==3.2.0\"\n!python3 -m pip install \"mmengine>=0.10.6\"\n```\n\n----------------------------------------\n\nTITLE: Running ruff for Python Code Formatting\nDESCRIPTION: Command to run ruff on a source file or directory, formatting the code to meet the project's style guidelines with a specified line length of 119 characters.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/multimodal/tests/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nruff format source_file_or_directory --line-length 119\n```\n\n----------------------------------------\n\nTITLE: Downloading COCO2017 Dataset with Bash Script\nDESCRIPTION: Commands to download and prepare the COCO2017 dataset using a bash script. The script shows a progress bar during download and can extract the dataset to the current directory or a specified output path.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/prepare_coco17.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbash download_coco17.sh\n```\n\nLANGUAGE: bash\nCODE:\n```\nbash download_coco17.sh ~/data\n```\n\n----------------------------------------\n\nTITLE: Installing Tesseract OCR on macOS with MacPorts\nDESCRIPTION: Command to install the Tesseract OCR package on macOS using MacPorts, which is required for document data processing.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/document_prediction/index.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo port install tesseract\n```\n\n----------------------------------------\n\nTITLE: AutoSummary for Analysis Explain Classes\nDESCRIPTION: Directive that generates a summary table listing the ShapAnalysis class without its signature.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.explain.md#2025-04-22_snippet_7\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autosummary::\n    :nosignatures:\n\n    ShapAnalysis\n```\n\n----------------------------------------\n\nTITLE: Generating SHA256 Hash\nDESCRIPTION: Command to generate SHA256 hash for the source package using openssl\nSOURCE: https://github.com/autogluon/autogluon/blob/master/release_instructions/update-conda-recipes.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nopenssl sha256 autogluon-0.7.0.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Inference on Custom Data\nDESCRIPTION: Examples of performing inference on custom images and COCO format data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/quick_start/quick_start_coco.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import download\nimage_url = \"https://raw.githubusercontent.com/dmlc/web-data/master/gluoncv/detection/street_small.jpg\"\ntest_image = download(image_url)\n\nimport json\n\n# create a input file for demo\ndata = {\"images\": [{\"id\": 0, \"width\": -1, \"height\": -1, \"file_name\": test_image}], \"categories\": []}\nos.mkdir(\"input_data_for_demo\")\ninput_file = \"input_data_for_demo/demo_annotation.json\"\nwith open(input_file, \"w+\") as f:\n    json.dump(data, f)\n\npred_test_image = predictor.predict(input_file)\nprint(pred_test_image)\n\npred_test_image = predictor.predict([test_image])\nprint(pred_test_image)\n```\n\n----------------------------------------\n\nTITLE: Downloading VOC Datasets Separately with Python Script\nDESCRIPTION: Commands to download VOC 2007 and VOC 2012 datasets separately using AutoGluon's CLI tool, specifying the output path for each dataset.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/prepare_voc.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m autogluon.multimodal.cli.prepare_detection_dataset -d voc07 -o ~/data\npython3 -m autogluon.multimodal.cli.prepare_detection_dataset -d voc12 -o ~/data\n```\n\n----------------------------------------\n\nTITLE: Summarizing MissingValuesAnalysis Class in AutoGluon EDA\nDESCRIPTION: Creates a summary of the MissingValuesAnalysis class from the autogluon.eda.analysis.missing module, hiding method signatures.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.missing.md#2025-04-22_snippet_7\n\nLANGUAGE: rst\nCODE:\n```\n.. autosummary::\n    :nosignatures:\n\n    MissingValuesAnalysis\n```\n\n----------------------------------------\n\nTITLE: Installing ruff for Python Code Formatting\nDESCRIPTION: Command to install ruff, a PEP 8 compliant code formatter used in the AutoGluon project for maintaining consistent code style.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/multimodal/tests/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install ruff\n```\n\n----------------------------------------\n\nTITLE: Suppressing Warnings in Python\nDESCRIPTION: This code snippet suppresses all warnings in the Python environment.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/problem_types_and_metrics.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\n\nwarnings.filterwarnings('ignore')\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Multimodal\nDESCRIPTION: This snippet installs the AutoGluon Multimodal package using pip.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/problem_types_and_metrics.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.multimodal\n```\n\n----------------------------------------\n\nTITLE: Documenting XShiftSummary Class\nDESCRIPTION: RST directive to generate detailed documentation for the XShiftSummary class, including its init method, which is used for visualizing distribution shifts.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.shift.md#2025-04-22_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: XShiftSummary\n   :members: init\n\n```\n\n----------------------------------------\n\nTITLE: Documentation Version Update Command in Bash\nDESCRIPTION: Command to update documentation links from master to stable branch in tutorial files using find and sed.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/release_instructions/ReleaseInstructions.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nLC_ALL=C find docs/tutorials/ -type f -exec sed -i '' 's#blob/master/docs#blob/stable/docs#' {} +\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon TimeSeries with UV\nDESCRIPTION: Installs AutoGluon TimeSeries package using UV for faster installation and uninstalls incompatible package versions.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/timeseries/forecasting-chronos.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install uv\n!uv pip install -q autogluon.timeseries --system\n!uv pip uninstall -q torchaudio torchvision torchtext --system\n```\n\n----------------------------------------\n\nTITLE: Setting Current Module for Missing Values Visualization\nDESCRIPTION: Sets the current module context to autogluon.eda.visualization.missing for subsequent documentation references.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.missing.md#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. currentmodule:: autogluon.eda.visualization.missing\n```\n\n----------------------------------------\n\nTITLE: Installing Conv-LoRA Dependencies with Conda and Pip\nDESCRIPTION: Shell commands to create a conda environment, install dependencies, and set up AutoGluon for Conv-LoRA fine-tuning of SAM models.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/Conv-LoRA/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nconda create -n conv-lora python=3.10\nconda activate conv-lora\npip install -U pip\npip install -U setuptools wheel\ngit clone https://github.com/autogluon/autogluon\ncd autogluon && pip install -e multimodal/[tests]\n```\n\n----------------------------------------\n\nTITLE: Displaying Tabular Cheat Sheet Image in Markdown\nDESCRIPTION: This code snippet displays the image of the AutoGluon Tabular cheat sheet using Markdown syntax. It specifies the image source and sets the width to 900 pixels.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/cheatsheet.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{image} https://raw.githubusercontent.com/Innixma/autogluon-doc-utils/main/docs/cheatsheets/stable/autogluon-cheat-sheet.jpeg\n:width: 900\n```\n\n----------------------------------------\n\nTITLE: Configuring Documentation Table of Contents in Markdown\nDESCRIPTION: Toctree configuration for semantic matching documentation pages, specifying maxdepth and hiding the table of contents while listing all relevant documentation pages.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{toctree}\n---\nmaxdepth: 1\nhidden: true\n---\n\nimage2image_matching\nimage_text_matching\ntext2text_matching\ntext_semantic_search\nzero_shot_img_txt_matching\n```\n\n----------------------------------------\n\nTITLE: ExplainWaterfallPlot Class Documentation\nDESCRIPTION: Directive to include detailed documentation for the ExplainWaterfallPlot class, specifically documenting its init method.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.explain.md#2025-04-22_snippet_4\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autoclass:: ExplainWaterfallPlot\n   :members: init\n```\n\n----------------------------------------\n\nTITLE: Defining Toctree for Multimodal Prediction Documentation in Markdown\nDESCRIPTION: This code snippet defines a toctree (table of contents tree) in Markdown format, listing the pages related to multimodal prediction tutorials. It sets the maximum depth to 1 and hides the toctree from direct view.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n---\nmaxdepth: 1\nhidden: true\n---\n\nbeginner_multimodal\nmultimodal_ner\nmultimodal_text_tabular\n```\n```\n\n----------------------------------------\n\nTITLE: Importing Experimental Tabular Classifiers and Regressors in Python\nDESCRIPTION: This snippet demonstrates how to import the experimental scikit-learn API compatible wrappers for TabularPredictor in AutoGluon. These wrappers provide TabularClassifier and TabularRegressor classes.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/whats_new/v1.0.0.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.tabular.experimental import TabularClassifier, TabularRegressor\n```\n\n----------------------------------------\n\nTITLE: Inspecting a Regression Target Variable in AutoGluon\nDESCRIPTION: Examines the first few values of the numeric 'age' column that will be used as the target variable for a regression task.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/tabular-essentials.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nage_column = 'age'\ntrain_data[age_column].head()\n```\n\n----------------------------------------\n\nTITLE: Specifying Models in TabularPredictor Hyperparameters\nDESCRIPTION: Example of how to specify models and their hyperparameters when using TabularPredictor.fit(). This demonstrates the structure of the hyperparameters dictionary, including multiple configurations for some models.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/api/autogluon.tabular.models.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nhyperparameters = {\n    'NN_TORCH': {},\n    'GBM': [\n        {'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}},\n        {},\n        {\n            \"learning_rate\": 0.03,\n            \"num_leaves\": 128,\n            \"feature_fraction\": 0.9,\n            \"min_data_in_leaf\": 3,\n            \"ag_args\": {\"name_suffix\": \"Large\", \"priority\": 0, \"hyperparameter_tune_kwargs\": None},\n        },\n    ],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\n```\n\n----------------------------------------\n\nTITLE: Hiding TOC in Markdown\nDESCRIPTION: A code snippet that creates a hidden table of contents in Markdown format. It includes links to various tutorial pages related to AutoGluon's tabular prediction capabilities.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n---\nmaxdepth: 1\nhidden: true\n---\n\nMultilabel <tabular-multilabel>\nKaggle <tabular-kaggle>\nGPU <tabular-gpu>\nCustom Metrics <tabular-custom-metric>\nCustom Models <tabular-custom-model>\nCustom Models Advanced <tabular-custom-model-advanced>\nDeployment <tabular-deployment>\n```\n```\n\n----------------------------------------\n\nTITLE: Citing AutoGluon-TimeSeries in BibTeX\nDESCRIPTION: BibTeX entry for citing AutoGluon's time series forecasting functionality, which provides AutoML capabilities for probabilistic time series forecasting.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/CITING.md#2025-04-22_snippet_3\n\nLANGUAGE: bibtex\nCODE:\n```\n@inproceedings{agtimeseries,\n  title={{AutoGluon-TimeSeries}: {AutoML} for Probabilistic Time Series Forecasting},\n  author={Shchur, Oleksandr and Turkmen, Caner and Erickson, Nick and Shen, Huibin and Shirkov, Alexander and Hu, Tony and Wang, Yuyang},\n  booktitle={International Conference on Automated Machine Learning},\n  year={2023}\n}\n```\n\n----------------------------------------\n\nTITLE: ExplainForcePlot Class Documentation\nDESCRIPTION: Directive to include detailed documentation for the ExplainForcePlot class, specifically documenting its init method.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.explain.md#2025-04-22_snippet_3\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autoclass:: ExplainForcePlot\n   :members: init\n```\n\n----------------------------------------\n\nTITLE: Style Checking and Import Sorting for AutoGluon Code in Python\nDESCRIPTION: Commands to check and apply code style formatting and import sorting for AutoGluon modules using ruff. This ensures code adheres to the project's style guidelines.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/CONTRIBUTING.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Check formatting and the order of imports\nfor dir in \"timeseries\" \"common\" \"features\"; do\n  ruff format --diff $dir\n  ruff check --select I $dir\ndone\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Apply style checks\nfor dir in \"timeseries\" \"common\" \"features\"; do\n  ruff format $dir\n  ruff check --fix --select I $dir\ndone\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon MultiModal Package\nDESCRIPTION: Installs the AutoGluon MultiModal package using pip.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_prediction/beginner_image_cls.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install autogluon.multimodal\n```\n\n----------------------------------------\n\nTITLE: Setting Current Module for Visualization Explain Documentation\nDESCRIPTION: Directive to set the current module context to the visualization.explain module for subsequent documentation sections.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.explain.md#2025-04-22_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. currentmodule:: autogluon.eda.visualization.explain\n```\n\n----------------------------------------\n\nTITLE: Setting Current Module for Missing Values Analysis\nDESCRIPTION: Sets the current module context to autogluon.eda.analysis.missing for subsequent documentation references.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.missing.md#2025-04-22_snippet_6\n\nLANGUAGE: rst\nCODE:\n```\n.. currentmodule:: autogluon.eda.analysis.missing\n```\n\n----------------------------------------\n\nTITLE: Documenting XShiftDetector Class\nDESCRIPTION: RST directive to generate detailed documentation for the XShiftDetector class, including its init method, which is used for detecting distribution shifts in data.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.shift.md#2025-04-22_snippet_7\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: XShiftDetector\n   :members: init\n```\n\n----------------------------------------\n\nTITLE: Installing Mamba Package Manager\nDESCRIPTION: Command to install Mamba in the base conda environment\nSOURCE: https://github.com/autogluon/autogluon/blob/master/release_instructions/update-conda-recipes.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nconda install -n base mamba -c conda-forge\n```\n\n----------------------------------------\n\nTITLE: Converting COCO Format to DataFrame in Python\nDESCRIPTION: This snippet shows how to convert data from COCO format to a pandas DataFrame using AutoGluon's utility function. This conversion is useful for working with the data in a tabular format.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/object_detection_with_dataframe.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal.utils.object_detection import from_coco\ntrain_df = from_coco(train_path)\nprint(train_df)\n```\n\n----------------------------------------\n\nTITLE: Sphinx AutoModule Documentation Template with Jinja2\nDESCRIPTION: A comprehensive Jinja2 template for Sphinx documentation that structures module documentation into organized sections. The template creates separate sections for module attributes, functions, classes, exceptions, and submodules, using autosummary directives to generate summaries and links to detailed documentation.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/_templates/autosummary/module.rst#2025-04-22_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ fullname | escape | underline}}\n\n.. automodule:: {{ fullname }}\n\n   {% block attributes %}\n   {% if attributes %}\n   .. rubric:: {{ _('Module Attributes') }}\n\n   .. autosummary::\n   {% for item in attributes %}\n      {{ item }}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n\n   {% block functions %}\n   {% if functions %}\n   .. rubric:: {{ _('Functions') }}\n\n   .. autosummary::\n       :toctree: \n   {% for item in functions %}\n      {{ item }}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n\n   {% block classes %}\n   {% if classes %}\n   .. rubric:: {{ _('Classes') }}\n\n   .. autosummary::\n      :toctree: \n   {% for item in classes %}\n      {{ item }}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n\n   {% block exceptions %}\n   {% if exceptions %}\n   .. rubric:: {{ _('Exceptions') }}\n\n   .. autosummary::\n   {% for item in exceptions %}\n      {{ item }}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n\n{% block modules %}\n{% if modules %}\n.. rubric:: Modules\n\n.. autosummary::\n   :toctree:\n   :recursive:\n{% for item in modules %}\n   {{ item }}\n{%- endfor %}\n{% endif %}\n{% endblock %}\n```\n\n----------------------------------------\n\nTITLE: Running Individual Unit Test Function with pytest\nDESCRIPTION: Command to execute a single unit test function (test_inferring_pos_label) within a specific test file using pytest, enabling precise testing of individual features.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/multimodal/tests/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npytest unittests/test_utils.py -k test_inferring_pos_label\n```\n\n----------------------------------------\n\nTITLE: Including Release Notes for AutoGluon v1.2.0 in Markdown\nDESCRIPTION: This code snippet demonstrates how to include the content of the v1.2.0 release notes using the include directive within a dropdown structure.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/whats_new/index.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n```{include} v1.2.0.md\n```\n```\n\n----------------------------------------\n\nTITLE: Creating Markdown Table of AutoGluon Talks and Tutorials\nDESCRIPTION: A formatted markdown table listing various AutoGluon presentations, tutorials, and talks with details including title, format, location, and date.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/AWESOME.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Title                                                                                                                                                              | Format    | Location                                                                                                                           | Date       |\n|--------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------|------------------------------------------------------------------------------------------------------------------------------------|------------|\n| :tv: [AutoGluon: Towards No-Code Automated Machine Learning](https://www.youtube.com/watch?v=SwPq9qjaN2Q)                                                          | Tutorial  | [AutoML 2024](https://2024.automl.cc/)                                                                                             | 2024/09/09 |\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Dependencies\nDESCRIPTION: Installation of required AutoGluon packages for EDA and tabular data processing with LightGBM support.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/eda-auto-anomaly-detection.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.eda\n!pip install autogluon.tabular[lightgbm]\n```\n\n----------------------------------------\n\nTITLE: Specifying AutoGluon Version in Markdown\nDESCRIPTION: This snippet defines the version number of the AutoGluon release. It uses a Markdown header to highlight the version information.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/whats_new/v0.5.2.md#2025-04-22_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n# Version 0.5.2\n```\n\n----------------------------------------\n\nTITLE: Citing YOLOX Paper in BibTeX Format\nDESCRIPTION: This BibTeX entry provides the citation details for the YOLOX paper titled 'YOLOX: Exceeding YOLO Series in 2021'. It includes author information, publication details, and links to the paper.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/quick_start/quick_start_coco.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{DBLP:journals/corr/abs-2107-08430,\n  author    = {Zheng Ge and\n               Songtao Liu and\n               Feng Wang and\n               Zeming Li and\n               Jian Sun},\n  title     = {{YOLOX:} Exceeding {YOLO} Series in 2021},\n  journal   = {CoRR},\n  volume    = {abs/2107.08430},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2107.08430},\n  eprinttype = {arXiv},\n  eprint    = {2107.08430},\n  timestamp = {Tue, 05 Apr 2022 14:09:44 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-08430.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Jupyter Notebook Extensions\nDESCRIPTION: Configures Jupyter notebook to automatically reload modules for development purposes.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/tabular/tabular-fasttext.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon MultiModal\nDESCRIPTION: Installs the AutoGluon MultiModal package using pip.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/model_distillation.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.multimodal\n```\n\n----------------------------------------\n\nTITLE: Configuring Image Augmentations for Training in AutoMM\nDESCRIPTION: Sets up image augmentation transforms for training using the 'model.timm_image.train_transforms' hyperparameter. Supports both string-based predefined transforms and custom callable objects.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/customization.ipynb#2025-04-22_snippet_47\n\nLANGUAGE: python\nCODE:\n```\n# default used by AutoMM\npredictor.fit(hyperparameters={\"model.timm_image.train_transforms\": [\"resize_shorter_side\", \"center_crop\", \"trivial_augment\"]})\n# use random resize crop and random horizontal flip\npredictor.fit(hyperparameters={\"model.timm_image.train_transforms\": [\"random_resize_crop\", \"random_horizontal_flip\"]})\n# or use a list of callable and pickle-able objects, e.g., torchvision transforms\npredictor.fit(hyperparameters={\"model.timm_image.train_transforms\": [torchvision.transforms.RandomResizedCrop(224), torchvision.transforms.RandomHorizontalFlip()]})\n```\n\n----------------------------------------\n\nTITLE: Installing Built Package\nDESCRIPTION: Command to install locally built conda package\nSOURCE: https://github.com/autogluon/autogluon/blob/master/release_instructions/update-conda-recipes.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmamba install -n ag -c \"file://${PWD}/build_artifacts\" -c conda-forge  autogluon.multimodal\n```\n\n----------------------------------------\n\nTITLE: Templating Class Documentation Structure with Jinja2 for Sphinx\nDESCRIPTION: This Jinja2 template defines the structure for documenting Python classes in Sphinx. It generates documentation that includes the class initialization method, other methods (excluding inherited ones), and class attributes. The template uses autosummary and automethod directives to generate links to detailed documentation for each component.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/_templates/custom_class.rst#2025-04-22_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ fullname | escape | underline}}\n\n.. currentmodule:: {{ module }}\n\n.. autoclass:: {{ objname }}\n\n   {% block methods %}\n   .. automethod:: __init__\n\n   {% if methods %}\n   .. rubric:: {{ _('Methods') }}\n\n   .. autosummary::\n      :toctree: .\n      :nosignatures:\n\n   {% for item in methods if item != '__init__' %}\n   {%- if item not in inherited_members %}\n      ~{{ name }}.{{ item }}\n   {%- endif %}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n\n   {% block attributes %}\n   {% if attributes %}\n   .. rubric:: {{ _('Attributes') }}\n\n   .. autosummary::\n   {% for item in attributes %}\n   {%- if item not in inherited_members %}\n      ~{{ name }}.{{ item }}\n   {%- endif %}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n```\n\n----------------------------------------\n\nTITLE: Citation for Conv-LoRA Research Paper\nDESCRIPTION: BibTeX citation for the Conv-LoRA paper that introduces the convolution-meets-LoRA approach for parameter efficient fine-tuning of SAM models.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/Conv-LoRA/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{zhong2024convolution,\n  title={Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model},\n  author={Zhong, Zihan and Tang, Zhiqiang and He, Tong and Fang, Haoyang and Yuan, Chun},\n  journal={arXiv preprint arXiv:2401.17868},\n  year={2024}\n}\n```\n\n----------------------------------------\n\nTITLE: Building Package Locally\nDESCRIPTION: Commands to prepare and build conda-forge package locally\nSOURCE: https://github.com/autogluon/autogluon/blob/master/release_instructions/update-conda-recipes.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nchmod 777 -R autogluon.multimodal-feedstock\ncd autogluon.multimodal-feedstock\npython build-locally.py\n```\n\n----------------------------------------\n\nTITLE: CI Build Trigger Command\nDESCRIPTION: Command to trigger CI build in conda-forge pull requests\nSOURCE: https://github.com/autogluon/autogluon/blob/master/release_instructions/update-conda-recipes.md#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n@conda-forge-admin, please rerender\n```\n\n----------------------------------------\n\nTITLE: Importing AutoGluon EDA Visualization Explain Module\nDESCRIPTION: Directive to include the visualization.explain module documentation, which contains classes for visualizing model explanations.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.explain.md#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. automodule:: autogluon.eda.visualization.explain\n```\n\n----------------------------------------\n\nTITLE: Displaying Multimodal Cheat Sheet Image in Markdown\nDESCRIPTION: This code snippet displays the image of the AutoGluon Multimodal cheat sheet using Markdown syntax. It specifies the image source and sets the width to 900 pixels.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/cheatsheet.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n{image} https://automl-mm-bench.s3-accelerate.amazonaws.com/cheatsheet/stable/automm.jpeg\n:width: 900\n```\n\n----------------------------------------\n\nTITLE: Configuring TOC Tree for Cloud Documentation\nDESCRIPTION: RestructuredText configuration block that defines the table of contents structure for the cloud deployment documentation, setting up navigation for AutoGluon cloud-related pages with maxdepth of 1 and hidden attribute.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/cloud_fit_deploy/index.md#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n{toctree}\n---\nmaxdepth: 1\nhidden: true\n---\n\nAutoGluon Cloud <autogluon-cloud>\nAutoGluon Tabular on SageMaker AutoPilot <autopilot-autogluon>\nDeploy AutoGluon Models on Serverless Templates <cloud-aws-lambda-deployment>\nCloud Training and Deployment with Amazon SageMaker <cloud-aws-sagemaker-train-deploy>\n```\n\n----------------------------------------\n\nTITLE: Documenting MissingValuesAnalysis Class in AutoGluon EDA\nDESCRIPTION: Generates detailed documentation for the MissingValuesAnalysis class, including its 'init' method, used for analyzing missing values in datasets.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.missing.md#2025-04-22_snippet_8\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: MissingValuesAnalysis\n   :members: init\n```\n\n----------------------------------------\n\nTITLE: Getting AutoGluon Source URL\nDESCRIPTION: Example source code URL format for AutoGluon releases\nSOURCE: https://github.com/autogluon/autogluon/blob/master/release_instructions/update-conda-recipes.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nhttps://github.com/autogluon/autogluon/archive/refs/tags/v0.7.0.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Analysis Components Summary\nDESCRIPTION: RST directive listing the main analysis classes for feature interactions.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.interaction.md#2025-04-22_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\n.. autosummary::\n    :nosignatures:\n\n    Correlation\n    CorrelationSignificance\n    FeatureInteraction\n    DistributionFit\n```\n\n----------------------------------------\n\nTITLE: Version Range Definition Example in Python\nDESCRIPTION: Examples of proper dependency version range specifications in Python setup files, demonstrating best practices for version constraints.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/release_instructions/ReleaseInstructions.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"scikit-learn>=1.0,<1.2\"\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Multimodal Package\nDESCRIPTION: Installs the AutoGluon multimodal package which is required for document classification tasks.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/document_prediction/document_classification.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.multimodal\n```\n\n----------------------------------------\n\nTITLE: AutoModule Import for Visualization Interaction\nDESCRIPTION: RST directive to import and document the visualization.interaction module.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.interaction.md#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: autogluon.eda.visualization.interaction\n```\n\n----------------------------------------\n\nTITLE: Importing AutoGluon EDA Analysis Explain Module\nDESCRIPTION: Directive to include the analysis.explain module documentation, which contains classes for analyzing model explanations.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.explain.md#2025-04-22_snippet_5\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. automodule:: autogluon.eda.analysis.explain\n```\n\n----------------------------------------\n\nTITLE: Suppressing Warnings\nDESCRIPTION: Imports the warnings module and filters out all warning messages.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/presets.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\n\nwarnings.filterwarnings('ignore')\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents with toctree in Sphinx Documentation\nDESCRIPTION: A hidden toctree directive that defines the navigation structure for the EDA module documentation. It lists eight markdown files corresponding to the API categories with maxdepth set to 1.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/index.md#2025-04-22_snippet_0\n\nLANGUAGE: toctree\nCODE:\n```\n{toctree}\n:maxdepth: 1\n:hidden:\n\ndataset <autogluon.eda.dataset.md>\ninteraction <autogluon.eda.interaction.md>\nmissing <autogluon.eda.missing.md>\nmodel <autogluon.eda.model.md>\nshift <autogluon.eda.shift.md>\ntransform <autogluon.eda.transform.md>\nexplain <autogluon.eda.explain.md>\nanomaly <autogluon.eda.anomaly.md>\n```\n\n----------------------------------------\n\nTITLE: Generating AutoSummary for ApplyFeatureGenerator\nDESCRIPTION: This RST directive creates an automatic summary for the ApplyFeatureGenerator class, excluding function signatures.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.transform.md#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. autosummary::\n    :nosignatures:\n\n    ApplyFeatureGenerator\n```\n\n----------------------------------------\n\nTITLE: Command Line Output for VOC to COCO Conversion\nDESCRIPTION: Shows the progress output displayed when running the voc2coco conversion tool, indicating the percentage of completion and conversion speed.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/voc_to_coco.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nStart converting !\n 17%|                                                                                  | 841/4952 [00:00<00:00, 15571.88it/s\n```\n\n----------------------------------------\n\nTITLE: Setting Current Module to Analysis Shift\nDESCRIPTION: RST directive to set the current module context to autogluon.eda.analysis.shift for subsequent documentation generation.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.shift.md#2025-04-22_snippet_5\n\nLANGUAGE: rst\nCODE:\n```\n.. currentmodule:: autogluon.eda.analysis.shift\n```\n\n----------------------------------------\n\nTITLE: Downloading Watercolor Dataset with Bash Script\nDESCRIPTION: Uses a Bash script to download the Watercolor dataset. The script can extract the dataset in the current directory or a specified output path.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/prepare_watercolor.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbash download_watercolor.sh\n```\n\nLANGUAGE: bash\nCODE:\n```\nbash download_watercolor.sh ~/data\n```\n\n----------------------------------------\n\nTITLE: Importing AutoGluon EDA Analysis Shift Module\nDESCRIPTION: RST directive to import the AutoGluon EDA analysis shift module, which contains components for detecting distribution shifts.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.shift.md#2025-04-22_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: autogluon.eda.analysis.shift\n```\n\n----------------------------------------\n\nTITLE: Adding Maintainer Command\nDESCRIPTION: Command format for adding new maintainers to conda-forge feedstock\nSOURCE: https://github.com/autogluon/autogluon/blob/master/release_instructions/update-conda-recipes.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n@conda-forge-admin, please add user @username\n```\n\n----------------------------------------\n\nTITLE: Setting Current Module to Visualization Shift\nDESCRIPTION: RST directive to set the current module context to autogluon.eda.visualization.shift for subsequent documentation generation.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.shift.md#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. currentmodule:: autogluon.eda.visualization.shift\n```\n\n----------------------------------------\n\nTITLE: Downloading Kaggle Competition Data using Bash\nDESCRIPTION: This command downloads the data for a specific Kaggle competition using the Kaggle API.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-kaggle.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkaggle competitions download -c [COMPETITION]\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for AutoGluon Release Notes in Markdown\nDESCRIPTION: This code snippet defines a hidden table of contents for AutoGluon release notes, listing versions from v1.2.0 to v0.4.0. It uses the toctree directive to structure the content.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/whats_new/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n:hidden: true\n:maxdepth: 1\n\nv1.2.0\nv1.1.1\nv1.1.0\nv1.0.0\nv0.8.3\nv0.8.2\nv0.8.1\nv0.8.0\nv0.7.0\nv0.6.2\nv0.6.1\nv0.6.0\nv0.5.2\nv0.5.1\nv0.4.3\nv0.4.2\nv0.4.1\nv0.4.0\n```\n```\n\n----------------------------------------\n\nTITLE: Hidden Table of Contents Configuration in Markdown\nDESCRIPTION: A toctree directive for Sphinx documentation that configures the table of contents. It's set to be hidden with a maximum depth of 1, referencing index files in the quick_start, advanced, and data_preparation directories.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n---\nmaxdepth: 1\nhidden: true\n---\n\nquick_start/index\nadvanced/index\ndata_preparation/index\n```\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment and Dependencies\nDESCRIPTION: Imports required packages and sets random seed for reproducibility.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/beginner_multimodal.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nnp.random.seed(123)\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon Multimodal for CLIP Zero-Shot Classification\nDESCRIPTION: Installs the AutoGluon multimodal package required for running the CLIP model for zero-shot image classification.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_prediction/clip_zeroshot.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.multimodal\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon MultiModal Package\nDESCRIPTION: Installs the AutoGluon multimodal package required for semantic search implementation.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/text_semantic_search.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.multimodal\n```\n\n----------------------------------------\n\nTITLE: Installing and Importing Dependencies\nDESCRIPTION: Sets up required packages including ir_datasets for dataset handling and configures pandas display options.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/text_semantic_search.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n!pip3 install ir_datasets\nimport ir_datasets\nimport pandas as pd\npd.set_option('display.max_colwidth', None)\n```\n\n----------------------------------------\n\nTITLE: Evaluating the Transfer Learning Model\nDESCRIPTION: Evaluates the transferred model on the regression task using multiple metrics: RMSE, Pearson correlation, and Spearman correlation. This shows how well knowledge from the classification task transferred to the regression task.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/continuous_training.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntest_score = predictor_sts.evaluate(sts_test_data, metrics=[\"rmse\", \"pearsonr\", \"spearmanr\"])\nprint(\"RMSE = {:.2f}\".format(test_score[\"rmse\"]))\nprint(\"PEARSONR = {:.4f}\".format(test_score[\"pearsonr\"]))\nprint(\"SPEARMANR = {:.4f}\".format(test_score[\"spearmanr\"]))\n```\n\n----------------------------------------\n\nTITLE: Creating Hidden Table of Contents for Object Detection Advanced in Markdown\nDESCRIPTION: This code snippet creates a hidden table of contents for the 'Object Detection Advanced' section. It uses the toctree directive with specific options to control the display and depth of the table of contents.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/advanced/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n---\ncaption: Object Detection Advanced\nmaxdepth: 1\nhidden: true\n---\n\nfinetune_coco\n```\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Imports necessary Python packages including pandas for data manipulation and IPython display utilities\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/image2image_matching.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport pandas as pd\nimport warnings\nfrom IPython.display import Image, display\nwarnings.filterwarnings('ignore')\n```\n\n----------------------------------------\n\nTITLE: Configuring Documentation Table of Contents in Markdown\nDESCRIPTION: Markdown configuration for documentation structure using toctree directive to organize pages related to image classification tutorials.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_prediction/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{toctree}\n---\nmaxdepth: 1\nhidden: true\n---\n\nbeginner_image_cls\nclip_zeroshot\n```\n\n----------------------------------------\n\nTITLE: Training MultiModalPredictor with Text Normalization\nDESCRIPTION: Code for training the MultiModalPredictor with text normalization enabled. This configuration uses a DeBERTa model and specifies hyperparameters for learning rate and training epochs while enabling text normalization for improved performance.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/examples/automm/kaggle_feedback_prize/README.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npredictor.fit(\n    train_data=train_df,\n    tuning_data=val_df,\n    presets=\"best_quality\",\n    hyperparameters={\n        \"model.hf_text.checkpoint_name\": \"microsoft/deberta-v3-large\",\n        \"data.text.normalize_text\": True,\n        \"optim.lr\": 5e-5,\n        \"optim.max_epochs\": 7,\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCV for Visualization\nDESCRIPTION: Installs the OpenCV Python package for image visualization.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/advanced/finetune_coco.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n!pip install opencv-python\n```\n\n----------------------------------------\n\nTITLE: Importing AutoGluon Dataset Analysis Components\nDESCRIPTION: This snippet demonstrates how to import various dataset analysis components from AutoGluon's EDA module. These components include classes for sampling, train-validation splitting, problem type control, and different types of dataset analysis.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.dataset.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.eda.analysis.dataset import Sampler, TrainValidationSplit, ProblemTypeControl, RawTypesAnalysis, VariableTypeAnalysis, SpecialTypesAnalysis, DatasetSummary, LabelInsightsAnalysis\n```\n\n----------------------------------------\n\nTITLE: Generating Sphinx Class Documentation Template with Jinja2\nDESCRIPTION: A Jinja2 template that creates structured documentation for Python classes using Sphinx autodoc. It includes sections for class methods and attributes, using autosummary for concise listings. The template handles object name escaping and proper formatting.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/_templates/autosummary/class.rst#2025-04-22_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ objname | escape | underline}}\n\n.. currentmodule:: {{ module }}\n\n.. autoclass:: {{ objname }}\n   :members: \n\n   {% block methods %}\n\n   {% if methods %}\n   .. rubric:: {{ _('Methods') }}\n\n   .. autosummary::\n      :nosignatures:\n\n   {% for item in methods if item != '__init__' %}\n      ~{{ name }}.{{ item }}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n\n   {% block attributes %}\n   {% if attributes %}\n   .. rubric:: {{ _('Attributes') }}\n\n   .. autosummary::\n   {% for item in attributes %}\n      ~{{ name }}.{{ item }}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n```\n\n----------------------------------------\n\nTITLE: Organizing Folder Structure for COCO Format Dataset\nDESCRIPTION: Shows the required directory structure for a dataset following the COCO format, with images in an 'images' folder and annotation JSON files in an 'annotations' folder.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/convert_data_to_coco_format.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n<dataset_dir>/\n    images/\n        <imagename0>.<ext>\n        <imagename1>.<ext>\n        <imagename2>.<ext>\n        ...\n    annotations/\n        train_labels.json\n        val_labels.json\n        test_labels.json\n        ...\n```\n\n----------------------------------------\n\nTITLE: Importing Required Python Libraries\nDESCRIPTION: This snippet imports necessary Python libraries including os, pandas, and sets up warning filters.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal_ner.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n```\n\n----------------------------------------\n\nTITLE: Data Preprocessing Function\nDESCRIPTION: Function to preprocess the dataset by converting Reviews and Ratings to numeric values and transforming prices to log-scale.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal_text_tabular.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef preprocess(df):\n    df = df.copy(deep=True)\n    df.loc[:, 'Reviews'] = pd.to_numeric(df['Reviews'].apply(lambda ele: ele[:-len(' out of 5 stars')]))\n    df.loc[:, 'Ratings'] = pd.to_numeric(df['Ratings'].apply(lambda ele: ele.replace(',', '')[:-len(' customer reviews')]))\n    df.loc[:, 'Price'] = np.log(df['Price'] + 1)\n    return df\n```\n\n----------------------------------------\n\nTITLE: Training AutoGluon MultiModalPredictor on PetFinder Dataset\nDESCRIPTION: Initializes and trains the MultiModalPredictor on the PetFinder dataset with a time limit of 120 seconds. The predictor automatically handles the multimodal data and selects appropriate models.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/multimodal_prediction/multimodal-quick-start.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\n\npredictor = MultiModalPredictor(label=label_col).fit(\n    train_data=train_data,\n    time_limit=120\n)\n```\n\n----------------------------------------\n\nTITLE: Downloading and Displaying an Apple Image with Text for Typographic Attack\nDESCRIPTION: Downloads an image of an apple with 'iPod' text written on it to demonstrate CLIP's vulnerability to typographic attacks.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_prediction/clip_zeroshot.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nurl = \"https://cdn.openai.com/multimodal-neurons/assets/apple/apple-ipod.jpg\"\nimage_path = download(url)\n\npil_img = Image(filename=image_path)\ndisplay(pil_img)\n```\n\n----------------------------------------\n\nTITLE: Rendering Analysis Visualization Components\nDESCRIPTION: Demonstrates the construction and rendering of visualization components using the generated state. The example shows how to create a vertical layout with multiple visualization facets.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/references/autogluon.eda.base-apis.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nviz = SimpleVerticalLinearLayout(\n    facets=[\n        DatasetStatistics(headers=True),\n        DatasetTypeMismatch(headers=True),\n        MarkdownSectionComponent(\"### Feature Distance\"),\n        FeatureDistanceAnalysisVisualization(),\n    ],\n)\nviz.render(state)\n```\n\n----------------------------------------\n\nTITLE: Importing AutoGluon Dataset Visualization Components\nDESCRIPTION: This snippet shows how to import the dataset visualization components from AutoGluon's EDA module. It includes classes for dataset statistics, type mismatch detection, and label insights visualization.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.dataset.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.eda.visualization.dataset import DatasetStatistics, DatasetTypeMismatch, LabelInsightsVisualization\n```\n\n----------------------------------------\n\nTITLE: Training AutoGluon TabularPredictor on Merged Data\nDESCRIPTION: This code snippet shows how to train an AutoGluon TabularPredictor on the merged training data, using the 'best_quality' preset for maximum accuracy.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/tabular/advanced/tabular-kaggle.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npredictor = TabularPredictor(label=label, eval_metric=eval_metric, path=save_path, verbosity=3).fit(\n    train_data, presets='best_quality', time_limit=3600\n)\n\nresults = predictor.fit_summary()\n```\n\n----------------------------------------\n\nTITLE: Toctree Configuration in Sphinx Documentation\nDESCRIPTION: Sphinx documentation configuration that defines the hidden table of contents structure for the EDA tools documentation pages.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/index.md#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n{toctree}\n:hidden: true\n:maxdepth: 1\n\neda-auto-dataset-overview\neda-auto-target-analysis\neda-auto-quick-fit\neda-auto-covariate-shift\neda-auto-analyze-interaction\neda-auto-anomaly-detection\nReferences <references/index>\nComponents <components/index>\n```\n\n----------------------------------------\n\nTITLE: Extracting Embeddings from Sentences\nDESCRIPTION: Demonstrates how to extract embeddings separately for two sentence groups using the trained model.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/text2text_matching.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nembeddings_1 = predictor.extract_embedding({\"premise\":[\"The teacher gave his speech to an empty room.\"]})\nprint(embeddings_1.shape)\nembeddings_2 = predictor.extract_embedding({\"hypothesis\":[\"There was almost nobody when the professor was talking.\"]})\nprint(embeddings_2.shape)\n```\n\n----------------------------------------\n\nTITLE: Configuring TOC Tree in Sphinx Documentation\nDESCRIPTION: Sphinx documentation configuration for table of contents tree structure, listing advanced topics pages with maxdepth of 1 and hidden attribute.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n---\nmaxdepth: 1\nhidden: true\n---\n\nproblem_types_and_metrics\nhyperparameter_optimization\ncontinuous_training\ncustomization\nmodel_distillation\nefficient_finetuning_basic\nfew_shot_learning\nfocal_loss\npresets\ntensorrt\n```\n```\n\n----------------------------------------\n\nTITLE: Visualizing a PDF Document with IFrame\nDESCRIPTION: Displays one of the historical PDF documents from the dataset using an IFrame that loads the document from an S3 URL.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/document_prediction/pdf_classification.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import IFrame\nIFrame(\"https://automl-mm-bench.s3.amazonaws.com/doc_classification/historical_1.pdf\", width=400, height=500)\n```\n\n----------------------------------------\n\nTITLE: Defining Hidden Section Role in RST\nDESCRIPTION: This RST directive defines a 'hidden' role for use in the documentation. It allows certain sections to be marked as hidden.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.transform.md#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. role:: hidden\n    :class: hidden-section\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Imports necessary Python libraries for data handling, visualization and numerical operations\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/image_text_matching.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport warnings\nfrom IPython.display import Image, display\nimport numpy as np\nwarnings.filterwarnings('ignore')\nnp.random.seed(123)\n```\n\n----------------------------------------\n\nTITLE: Setting Current Module for Analysis Explain Documentation\nDESCRIPTION: Directive to set the current module context to the analysis.explain module for subsequent documentation sections.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.explain.md#2025-04-22_snippet_6\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. currentmodule:: autogluon.eda.analysis.explain\n```\n\n----------------------------------------\n\nTITLE: Installing Tesseract OCR on macOS with Homebrew\nDESCRIPTION: Command to install the Tesseract OCR package on macOS using Homebrew, which is required for document data processing.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/document_prediction/index.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbrew install tesseract\n```\n\n----------------------------------------\n\nTITLE: Performing Semantic Search for Text Retrieval in Python\nDESCRIPTION: Demonstrates text retrieval using semantic search with an image query.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/zero_shot_img_txt_matching.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nhits = semantic_search(\n        matcher=predictor,\n        query_embeddings=image_embeddings[4][None,],\n        response_embeddings=text_embeddings,\n        top_k=5,\n    )\nprint(hits)\n```\n\n----------------------------------------\n\nTITLE: Making Predictions on Image Pairs\nDESCRIPTION: Predicts matching status for new image pairs using a 0.5 probability threshold\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/image2image_matching.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npred = predictor.predict(test_data.head(3))\nprint(pred)\n```\n\n----------------------------------------\n\nTITLE: Creating Class Summary for XShiftSummary\nDESCRIPTION: RST directive to generate an automatic summary for the XShiftSummary class without including method signatures.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.shift.md#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. autosummary::\n    :nosignatures:\n\n    XShiftSummary\n```\n\n----------------------------------------\n\nTITLE: Installing AutoGluon MultiModal Package\nDESCRIPTION: Installs the AutoGluon MultiModal package using pip.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/advanced/finetune_coco.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install autogluon.multimodal\n```\n\n----------------------------------------\n\nTITLE: Loading Teacher Model with MultiModalPredictor\nDESCRIPTION: Loads the pre-trained teacher model using AutoGluon's MultiModalPredictor.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/model_distillation.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.multimodal import MultiModalPredictor\n\nteacher_predictor = MultiModalPredictor.load(\"ag_distillation_sample_teacher/\")\n```\n\n----------------------------------------\n\nTITLE: Installing AutoMM Dependencies\nDESCRIPTION: Installs the required AutoGluon multimodal package for image matching\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/semantic_matching/image2image_matching.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install autogluon.multimodal\n```\n\n----------------------------------------\n\nTITLE: Documenting MissingValues Class in AutoGluon EDA\nDESCRIPTION: Generates detailed documentation for the MissingValues class, including its 'init' method, used for visualizing missing values in datasets.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.missing.md#2025-04-22_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: MissingValues\n   :members: init\n```\n\n----------------------------------------\n\nTITLE: Importing AutoGluon EDA Transform Module\nDESCRIPTION: This snippet shows how to import the transform module from AutoGluon's EDA package. It sets up the current module for documentation purposes.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.transform.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom autogluon.eda.analysis.transform import *\n```\n\n----------------------------------------\n\nTITLE: Importing AutoGluon Anomaly Visualization Module\nDESCRIPTION: Imports the anomaly visualization module from AutoGluon's EDA package. This module contains tools for visualizing anomaly detection results.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/eda/components/autogluon.eda.anomaly.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom autogluon.eda.visualization.anomaly import AnomalyScoresVisualization\n```\n\n----------------------------------------\n\nTITLE: Citing AutoGluon TextPredictor in BibTeX\nDESCRIPTION: BibTeX entry for citing AutoGluon's TextPredictor functionality, which is part of the multimodal module and focuses on handling tabular data with text fields.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/CITING.md#2025-04-22_snippet_2\n\nLANGUAGE: bibtex\nCODE:\n```\n@inproceedings{agmultimodaltext,\n  title={Benchmarking Multimodal AutoML for Tabular Data with Text Fields},\n  author={Shi, Xingjian and Mueller, Jonas and Erickson, Nick and Li, Mu and Smola, Alexander J},\n  journal={Advances in Neural Information Processing Systems Datasets and Benchmarks Track},\n  volume={35},\n  year={2021}\n}\n```\n\n----------------------------------------\n\nTITLE: Citing AutoGluon-Multimodal in BibTeX\nDESCRIPTION: BibTeX entry for citing AutoGluon's multimodal functionality, specifically the AutoMM paper which discusses the integration of foundation models in multimodal AutoML.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/CITING.md#2025-04-22_snippet_1\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{tang2024autogluon,\n  title={AutoGluon-Multimodal (AutoMM): Supercharging Multimodal AutoML with Foundation Models},\n  author={Tang, Zhiqiang and Fang, Haoyang and Zhou, Su and Yang, Taojiannan and Zhong, Zihan and Hu, Tony and Kirchhoff, Katrin and Karypis, George},\n  journal={arXiv preprint arXiv:2404.16233},\n  year={2024}\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Toctree for Object Detection Data Preparation in Markdown\nDESCRIPTION: This code snippet defines a toctree (table of contents) for various object detection data preparation topics. It includes links to guides for converting data formats, preparing specific datasets, and using DataFrame for object detection.\nSOURCE: https://github.com/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/data_preparation/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n---\ncaption: Object Detection Data Preparation\nmaxdepth: 1\nhidden: true\n---\n\nconvert_data_to_coco_format\nprepare_pothole\nprepare_watercolor\nprepare_coco17\nprepare_voc\nvoc_to_coco\n```\n```\n\n----------------------------------------\n\nTITLE: SHA256 Hash Output Example\nDESCRIPTION: Example output of the SHA256 hash generation command\nSOURCE: https://github.com/autogluon/autogluon/blob/master/release_instructions/update-conda-recipes.md#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n455831de3c9de8fbe11b100054b8f150661d0651212fcfa4ec2e42417fdac355\n```"
  }
]
[
  {
    "owner": "piskvorky",
    "repo": "gensim",
    "content": "TITLE: Training a new Word2Vec model in Python using Gensim\nDESCRIPTION: Demonstrates how to train a new Word2Vec model using a custom corpus iterator.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_word2vec.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport gensim.models\n\nsentences = MyCorpus()\nmodel = gensim.models.Word2Vec(sentences=sentences)\n```\n\n----------------------------------------\n\nTITLE: Loading Word2Vec Format Files in Python using Gensim\nDESCRIPTION: Shows how to load Word2Vec models in text and binary formats, including support for compressed files.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_word2vec.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nmodel = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.txt', binary=False)\n# using gzipped/bz2 input works too, no need to unzip\nmodel = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.bin.gz', binary=True)\n```\n\n----------------------------------------\n\nTITLE: Creating Simple Poincaré Model with Direct Relations\nDESCRIPTION: Demonstrates basic model initialization with explicit node relations provided as tuples.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Tutorial.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel = PoincareModel(train_data=[('node.1', 'node.2'), ('node.2', 'node.3')])\n```\n\n----------------------------------------\n\nTITLE: Word Mover's Distance Calculation\nDESCRIPTION: Demonstrates how to calculate Word Mover's Distance between two sentences using FastText embeddings.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsentence_obama = 'Obama speaks to the media in Illinois'.lower().split()\nsentence_president = 'The president greets the press in Chicago'.lower().split()\n\nfrom nltk.corpus import stopwords\nstopwords = stopwords.words('english')\nsentence_obama = [w for w in sentence_obama if w not in stopwords]\nsentence_president = [w for w in sentence_president if w not in stopwords]\n\ndistance = model_wrapper.wmdistance(sentence_obama, sentence_president)\ndistance\n```\n\n----------------------------------------\n\nTITLE: Training Word2Vec Models with Parameter Experimentation in Python\nDESCRIPTION: Trains Word2Vec models with different combinations of parameters (sg, hs, compute_loss) and measures training times. Uses nested loops to experiment with different configurations and collects timing statistics.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_word2vec.ipynb#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n# Temporarily reduce logging verbosity\nlogging.root.level = logging.ERROR\n\nimport time\nimport numpy as np\nimport pandas as pd\n\ntrain_time_values = []\nseed_val = 42\nsg_values = [0, 1]\nhs_values = [0, 1]\n\nfast = True\nif fast:\n    input_data_subset = input_data[:3]\nelse:\n    input_data_subset = input_data\n\n\nfor data in input_data_subset:\n    for sg_val in sg_values:\n        for hs_val in hs_values:\n            for loss_flag in [True, False]:\n                time_taken_list = []\n                for i in range(3):\n                    start_time = time.time()\n                    w2v_model = gensim.models.Word2Vec(\n                        data,\n                        compute_loss=loss_flag,\n                        sg=sg_val,\n                        hs=hs_val,\n                        seed=seed_val,\n                    )\n                    time_taken_list.append(time.time() - start_time)\n\n                time_taken_list = np.array(time_taken_list)\n                time_mean = np.mean(time_taken_list)\n                time_std = np.std(time_taken_list)\n\n                model_result = {\n                    'train_data': data.name,\n                    'compute_loss': loss_flag,\n                    'sg': sg_val,\n                    'hs': hs_val,\n                    'train_time_mean': time_mean,\n                    'train_time_std': time_std,\n                }\n                print(\"Word2vec model #%i: %s\" % (len(train_time_values), model_result))\n                train_time_values.append(model_result)\n\ntrain_times_table = pd.DataFrame(train_time_values)\ntrain_times_table = train_times_table.sort_values(\n    by=['train_data', 'sg', 'hs', 'compute_loss'],\n    ascending=[False, False, True, False],\n)\nprint(train_times_table)\n```\n\n----------------------------------------\n\nTITLE: Downloading and Preparing Text Corpora for Word Embedding Training\nDESCRIPTION: Downloads the Brown corpus from NLTK and creates a text file. Also downloads text8 (100MB sample of cleaned Wikipedia text) and text9 corpus, preprocessing them for training word embeddings.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport nltk\nfrom smart_open import smart_open\nnltk.download('brown') \n# Only the brown corpus is needed in case you don't have it.\n\n# Generate brown corpus text file\nwith smart_open('brown_corp.txt', 'w+') as f:\n    for word in nltk.corpus.brown.words():\n        f.write('{word} '.format(word=word))\n\n# Make sure you set FT_HOME to your fastText directory root\nFT_HOME = 'fastText/'\n# download the text8 corpus (a 100 MB sample of cleaned wikipedia text)\nimport os.path\nif not os.path.isfile('text8'):\n    !wget -c https://mattmahoney.net/dc/text8.zip\n    !unzip text8.zip\n# download and preprocess the text9 corpus\nif not os.path.isfile('text9'):\n  !wget -c https://mattmahoney.net/dc/enwik9.zip\n  !unzip enwik9.zip\n  !perl {FT_HOME}wikifil.pl enwik9 > text9\n```\n\n----------------------------------------\n\nTITLE: Visualizing Animal Word Translations\nDESCRIPTION: Processes and visualizes word vector translations for animal terms between English and Italian, using PCA for dimensionality reduction.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/translation_matrix.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nwords = [(\"dog\", \"cane\"), (\"pig\", \"maiale\"), (\"cat\", \"gatto\"), (\"horse\", \"cavallo\"), (\"birds\", \"uccelli\")]\nen_words_vec = [source_word_vec[item[0]] for item in words]\nit_words_vec = [target_word_vec[item[1]] for item in words]\n\nen_words, it_words = zip(*words)\n```\n\n----------------------------------------\n\nTITLE: Using Word2Vec with Scikit-Learn Pipeline in Python\nDESCRIPTION: This snippet demonstrates how to use a Word2Vec model with Scikit-Learn's Pipeline for text classification. It includes data preparation, model training, and evaluation using logistic regression.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/sklearn_api.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nw2v_texts = [\n    ['calculus', 'is', 'the', 'mathematical', 'study', 'of', 'continuous', 'change'],\n    ['geometry', 'is', 'the', 'study', 'of', 'shape'],\n    ['algebra', 'is', 'the', 'study', 'of', 'generalizations', 'of', 'arithmetic', 'operations'],\n    ['differential', 'calculus', 'is', 'related', 'to', 'rates', 'of', 'change', 'and', 'slopes', 'of', 'curves'],\n    ['integral', 'calculus', 'is', 'realted', 'to', 'accumulation', 'of', 'quantities', 'and', 'the', 'areas', 'under', 'and', 'between', 'curves'],\n    ['physics', 'is', 'the', 'natural', 'science', 'that', 'involves', 'the', 'study', 'of', 'matter', 'and', 'its', 'motion', 'and', 'behavior', 'through', 'space', 'and', 'time'],\n    ['the', 'main', 'goal', 'of', 'physics', 'is', 'to', 'understand', 'how', 'the', 'universe', 'behaves'],\n    ['physics', 'also', 'makes', 'significant', 'contributions', 'through', 'advances', 'in', 'new', 'technologies', 'that', 'arise', 'from', 'theoretical', 'breakthroughs'],\n    ['advances', 'in', 'the', 'understanding', 'of', 'electromagnetism', 'or', 'nuclear', 'physics', 'led', 'directly', 'to', 'the', 'development', 'of', 'new', 'products', 'that', 'have', 'dramatically', 'transformed', 'modern', 'day', 'society']\n]\n\nmodel = W2VTransformer(size=10, min_count=1)\nmodel.fit(w2v_texts)\n\nclass_dict = {'mathematics': 1, 'physics': 0}\ntrain_data = [\n    ('calculus', 'mathematics'), ('mathematical', 'mathematics'), ('geometry', 'mathematics'), ('operations', 'mathematics'), ('curves', 'mathematics'),\n    ('natural', 'physics'), ('nuclear', 'physics'), ('science', 'physics'), ('electromagnetism', 'physics'), ('natural', 'physics')\n]\n\ntrain_input = list(map(lambda x: x[0], train_data))\ntrain_target = list(map(lambda x: class_dict[x[1]], train_data))\n\nclf = linear_model.LogisticRegression(penalty='l2', C=0.1)\nclf.fit(model.transform(train_input), train_target)\ntext_w2v = Pipeline([('features', model,), ('classifier', clf)])\nscore = text_w2v.score(train_input, train_target)\n\nprint(score)\n```\n\n----------------------------------------\n\nTITLE: Loading a Pre-trained Word Embedding Model\nDESCRIPTION: Demonstrates loading a pre-trained GloVe word embedding model and using it to find words similar to 'glass' based on vector similarity.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/downloader_api_tutorial.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmodel = api.load(\"glove-wiki-gigaword-50\")\nmodel.most_similar(\"glass\")\n```\n\n----------------------------------------\n\nTITLE: Listing Available Models\nDESCRIPTION: Shows how to iterate through and display information about available models, including name, number of records, and description.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_downloader_api.rst#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfor model_name, model_data in sorted(info['models'].items()):\n    print(\n        '%s (%d records): %s' % (\n            model_name,\n            model_data.get('num_records', -1),\n            model_data['description'][:40] + '...',\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Handling KeyError for unknown words in Word2Vec model with Python\nDESCRIPTION: Demonstrates error handling when trying to access a vector for a word not in the model's vocabulary.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_word2vec.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    vec_cameroon = wv['cameroon']\nexcept KeyError:\n    print(\"The word 'cameroon' does not appear in this model\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Corpus and Dictionary in Gensim\nDESCRIPTION: Demonstrates how to create a corpus and dictionary from a list of documents, including text preprocessing steps like removing common words and tokens that appear only once.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_topics_and_transformations.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom collections import defaultdict\nfrom gensim import corpora\n\ndocuments = [\n    \"Human machine interface for lab abc computer applications\",\n    \"A survey of user opinion of computer system response time\",\n    \"The EPS user interface management system\",\n    \"System and human system engineering testing of EPS\",\n    \"Relation of user perceived response time to error measurement\",\n    \"The generation of random binary unordered trees\",\n    \"The intersection graph of paths in trees\",\n    \"Graph minors IV Widths of trees and well quasi ordering\",\n    \"Graph minors A survey\",\n]\n\n# remove common words and tokenize\nstoplist = set('for a of the and to in'.split())\ntexts = [\n    [word for word in document.lower().split() if word not in stoplist]\n    for document in documents\n]\n\n# remove words that appear only once\nfrequency = defaultdict(int)\nfor text in texts:\n    for token in text:\n        frequency[token] += 1\n\ntexts = [\n    [token for token in text if frequency[token] > 1]\n    for text in texts\n]\n\ndictionary = corpora.Dictionary(texts)\ncorpus = [dictionary.doc2bow(text) for text in texts]\n```\n\n----------------------------------------\n\nTITLE: Saving and loading Word2Vec models in Python using Gensim\nDESCRIPTION: Shows how to save a trained Word2Vec model to a temporary file using Gensim's save method.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_word2vec.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport tempfile\n\nwith tempfile.NamedTemporaryFile(prefix='gensim-model-', delete=False) as tmp:\n    temporary_filepath = tmp.name\n    model.save(temporary_filepath)\n    #\n```\n\n----------------------------------------\n\nTITLE: Preparing 20 Newsgroups Dataset for LDA Topic Modeling\nDESCRIPTION: Fetches the 20 Newsgroups dataset, cleans and tokenizes the text, removes stopwords, applies stemming, and creates a bag-of-words representation suitable for LDA modeling.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_compare_lda.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom string import punctuation\nfrom nltk import RegexpTokenizer\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom sklearn.datasets import fetch_20newsgroups\n\n\nnewsgroups = fetch_20newsgroups()\neng_stopwords = set(stopwords.words('english'))\n\ntokenizer = RegexpTokenizer(r'\\s+', gaps=True)\nstemmer = PorterStemmer()\ntranslate_tab = {ord(p): u\" \" for p in punctuation}\n\ndef text2tokens(raw_text):\n    \"\"\"Split the raw_text string into a list of stemmed tokens.\"\"\"\n    clean_text = raw_text.lower().translate(translate_tab)\n    tokens = [token.strip() for token in tokenizer.tokenize(clean_text)]\n    tokens = [token for token in tokens if token not in eng_stopwords]\n    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n    return [token for token in stemmed_tokens if len(token) > 2]  # skip short tokens\n\ndataset = [text2tokens(txt) for txt in newsgroups['data']]  # convert a documents to list of tokens\n\nfrom gensim.corpora import Dictionary\ndictionary = Dictionary(documents=dataset, prune_at=None)\ndictionary.filter_extremes(no_below=5, no_above=0.3, keep_n=None)  # use Dictionary to remove un-relevant tokens\ndictionary.compactify()\n\nd2b_dataset = [dictionary.doc2bow(doc) for doc in dataset]  # convert list of tokens to bag of word representation\n```\n\n----------------------------------------\n\nTITLE: Finding most similar words using Word2Vec in Python\nDESCRIPTION: Uses the Word2Vec model to find and print the top 5 most similar words to 'car' and 'minivan'.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_word2vec.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprint(wv.most_similar(positive=['car', 'minivan'], topn=5))\n```\n\n----------------------------------------\n\nTITLE: Inferring Document Vectors with Doc2Vec in Python\nDESCRIPTION: This snippet demonstrates how to infer a document vector for a randomly selected document using different Doc2Vec models. It then finds the most similar documents to the inferred vector.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_45\n\nLANGUAGE: python\nCODE:\n```\ndoc_id = np.random.randint(len(simple_models[0].dv))  # Pick random doc; re-run cell for more examples\nprint(f'for doc {doc_id}...')\nfor model in simple_models:\n    inferred_docvec = model.infer_vector(alldocs[doc_id].words)\n    print(f'{model}:\\n {model.dv.most_similar([inferred_docvec], topn=3)}')\n```\n\n----------------------------------------\n\nTITLE: Calculating Coherence Measures - Python\nDESCRIPTION: Computes different coherence measures (u_mass, c_v, c_uci, c_npmi) using gensim's CoherenceModel\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence-movies.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncm = CoherenceModel(topics=usable_topics, corpus=corpus, dictionary=dictionary, coherence='u_mass')\nu_mass = cm.get_coherence_per_topic()\nprint(\"Calculated u_mass coherence for %d topics\" % len(u_mass))\n\ncm = CoherenceModel(topics=usable_topics, texts=texts, dictionary=dictionary, coherence='c_v')\nc_v = cm.get_coherence_per_topic()\nprint(\"Calculated c_v coherence for %d topics\" % len(c_v))\n\ncm.coherence = 'c_uci'\nc_uci = cm.get_coherence_per_topic()\nprint(\"Calculated c_uci coherence for %d topics\" % len(c_uci))\n\ncm.coherence = 'c_npmi'\nc_npmi = cm.get_coherence_per_topic()\nprint(\"Calculated c_npmi coherence for %d topics\" % len(c_npmi))\n```\n\n----------------------------------------\n\nTITLE: Calculating word similarities using Word2Vec in Python\nDESCRIPTION: Computes and prints similarity scores between pairs of words using the Word2Vec model.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_word2vec.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npairs = [\n    ('car', 'minivan'),   # a minivan is a kind of car\n    ('car', 'bicycle'),   # still a wheeled vehicle\n    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n    ('car', 'cereal'),    # ... and so on\n    ('car', 'communism'),\n]\nfor w1, w2 in pairs:\n    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))\n```\n\n----------------------------------------\n\nTITLE: Training Models on Text8 Corpus\nDESCRIPTION: Calls the previously defined training function to train FastText and Word2Vec models on the text8 corpus, which is a 100MB sample of cleaned Wikipedia text.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntrain_models(corpus_file='text8', output_name='text8')\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating Doc2Vec Models in Python\nDESCRIPTION: This code trains multiple Doc2Vec models on a shuffled dataset and evaluates their performance using the previously defined error rate function. It iterates through simple models and combined models, printing the error rates for each.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom collections import defaultdict\nerror_rates = defaultdict(lambda: 1.0)  # To selectively print only best errors achieved\n\nfrom random import shuffle\nshuffled_alldocs = alldocs[:]\nshuffle(shuffled_alldocs)\n\nfor model in simple_models:\n    print(f\"Training {model}\")\n    model.train(shuffled_alldocs, total_examples=len(shuffled_alldocs), epochs=model.epochs)\n\n    print(f\"\\nEvaluating {model}\")\n    err_rate, err_count, test_count, predictor = error_rate_for_model(model, train_docs, test_docs)\n    error_rates[str(model)] = err_rate\n    print(\"\\n%f %s\\n\" % (err_rate, model))\n\nfor model in [models_by_name['dbow+dmm'], models_by_name['dbow+dmc']]:\n    print(f\"\\nEvaluating {model}\")\n    err_rate, err_count, test_count, predictor = error_rate_for_model(model, train_docs, test_docs)\n    error_rates[str(model)] = err_rate\n    print(f\"\\n{err_rate} {model}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Training Word2Vec Model on Text8 Corpus in Python\nDESCRIPTION: Creates and trains a Word2Vec model using the downloaded text8 corpus.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_downloader_api.rst#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models.word2vec import Word2Vec\nmodel = Word2Vec(corpus)\n```\n\n----------------------------------------\n\nTITLE: Word Vector Similarity Analysis with City Analogies\nDESCRIPTION: Demonstrates finding words similar to the relationship between Baghdad/England and London using word vector arithmetic.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_fasttext.rst#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(wv.most_similar(positive=['baghdad', 'england'], negative=['london']))\n```\n\n----------------------------------------\n\nTITLE: Document Topic Analysis with LSI in Python\nDESCRIPTION: Transforms documents through bow->tfidf->lsi pipeline and prints the topic distributions alongside original text. Shows how documents map to the latent topic space.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_topics_and_transformations.rst#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\nfor doc, as_text in zip(corpus_lsi, documents):\n    print(doc, as_text)\n```\n\n----------------------------------------\n\nTITLE: Training FastText Model using Gensim Native Implementation\nDESCRIPTION: Demonstrates how to initialize, build vocabulary and train a FastText model using Gensim's native implementation. Uses the Lee Corpus for training with basic parameter configuration.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models.fasttext import FastText as FT_gensim\nfrom gensim.test.utils import datapath\n\n# Set file names for train and test data\ncorpus_file = datapath('lee_background.cor')\n\nmodel_gensim = FT_gensim(size=100)\n\n# build the vocabulary\nmodel_gensim.build_vocab(corpus_file=corpus_file)\n\n# train the model\nmodel_gensim.train(\n    corpus_file=corpus_file, epochs=model_gensim.epochs,\n    total_examples=model_gensim.corpus_count, total_words=model_gensim.corpus_total_words\n)\n\nprint(model_gensim)\n```\n\n----------------------------------------\n\nTITLE: Initializing Doc2Vec Models (DBOW and DM) for Wikipedia Training in Python\nDESCRIPTION: This code initializes two Doc2Vec models: DBOW (Distributed Bag of Words) and DM (Distributed Memory). It sets various parameters like vector size, window, and maximum vocabulary size.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nworkers = 20  # multiprocessing.cpu_count() - 1  # leave one core for the OS & other stuff\n\n# PV-DBOW: paragraph vector in distributed bag of words mode\nmodel_dbow = Doc2Vec(\n    dm=0, dbow_words=1,  # dbow_words=1 to train word vectors at the same time too, not only DBOW\n    vector_size=200, window=8, epochs=10, workers=workers, max_final_vocab=1000000,\n)\n\n# PV-DM: paragraph vector in distributed memory mode\nmodel_dm = Doc2Vec(\n    dm=1, dm_mean=1,  # use average of context word vectors to train DM\n    vector_size=200, window=8, epochs=10, workers=workers, max_final_vocab=1000000,\n)\n```\n\n----------------------------------------\n\nTITLE: Similarity Operations with FastText Word Vectors in Python\nDESCRIPTION: This snippet shows how to perform similarity operations with FastText word vectors, including for out-of-vocabulary words. It demonstrates similarity calculation and finding most similar words.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_fasttext.rst#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint(\"nights\" in wv.key_to_index)\nprint(\"night\" in wv.key_to_index)\nprint(wv.similarity(\"night\", \"nights\"))\nprint(wv.most_similar(\"nights\"))\nprint(wv.n_similarity(['sushi', 'shop'], ['japanese', 'restaurant']))\n```\n\n----------------------------------------\n\nTITLE: IMDB Dataset Processing for Doc2Vec\nDESCRIPTION: Implements document processing for IMDB dataset to prepare for Doc2Vec training, including sentiment analysis and data splitting.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/translation_matrix.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef read_sentimentDocs():\n    SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n\n    alldocs = []  # will hold all docs in original order\n    with smart_open.open('aclImdb/alldata-id.txt', encoding='utf-8') as alldata:\n        for line_no, line in enumerate(alldata):\n            tokens = gensim.utils.to_unicode(line).split()\n            words = tokens[1:]\n            tags = [line_no] \n            split = ['train','test','extra','extra'][line_no // 25000]  \n            sentiment = [1.0, 0.0, 1.0, 0.0, None, None, None, None][line_no // 12500]\n            alldocs.append(SentimentDocument(words, tags, split, sentiment))\n\n    train_docs = [doc for doc in alldocs if doc.split == 'train']\n    test_docs = [doc for doc in alldocs if doc.split == 'test']\n    doc_list = alldocs[:]\n\n    return train_docs, test_docs, doc_list\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Word2Vec Training\nDESCRIPTION: Imports necessary Python libraries including Gensim's WikiCorpus and Word2Vec, along with utility modules for processing.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/online_w2v_tutorial.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.corpora.wikicorpus import WikiCorpus\nfrom gensim.models.word2vec import Word2Vec, LineSentence\nfrom pprint import pprint\nfrom copy import deepcopy\nfrom multiprocessing import cpu_count\nfrom smart_open import smart_open\n```\n\n----------------------------------------\n\nTITLE: Computing WMD Distance Between Similar Sentences\nDESCRIPTION: Calculates the Word Mover's Distance (WMD) between two semantically similar sentences about Obama and the president using the wmdistance method from the loaded Word2Vec model.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/WMD_tutorial.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndistance = model.wmdistance(sentence_obama, sentence_president)\nprint('distance = %.4f' % distance)\n```\n\n----------------------------------------\n\nTITLE: Creating AnnoyIndex for Fast Similarity Queries in Python\nDESCRIPTION: Demonstrates how to create an AnnoyIndex for the trained Word2Vec model and perform a similarity query using both Annoy and traditional methods.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_annoy.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.similarities.annoy import AnnoyIndexer\n\n# 100 trees are being used in this example\nannoy_index = AnnoyIndexer(model, 100)\n# Derive the vector for the word \"science\" in our model\nvector = wv[\"science\"]\n# The instance of AnnoyIndexer we just created is passed\napproximate_neighbors = wv.most_similar([vector], topn=11, indexer=annoy_index)\n# Neatly print the approximate_neighbors and their corresponding cosine similarity values\nprint(\"Approximate Neighbors\")\nfor neighbor in approximate_neighbors:\n    print(neighbor)\n\nnormal_neighbors = wv.most_similar([vector], topn=11)\nprint(\"\\nExact Neighbors\")\nfor neighbor in normal_neighbors:\n    print(neighbor)\n```\n\n----------------------------------------\n\nTITLE: Word Embedding Accuracy Calculator in Python\nDESCRIPTION: Function to calculate and print semantic and syntactic accuracy metrics for word embedding models using analogy questions\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef print_accuracy(model, questions_file):\n    print('Evaluating...\\n')\n    acc = model.accuracy(questions_file)\n\n    sem_correct = sum((len(acc[i]['correct']) for i in range(5)))\n    sem_total = sum((len(acc[i]['correct']) + len(acc[i]['incorrect'])) for i in range(5))\n    sem_acc = 100*float(sem_correct)/sem_total\n    print('\\nSemantic: {:d}/{:d}, Accuracy: {:.2f}%'.format(sem_correct, sem_total, sem_acc))\n    \n    syn_correct = sum((len(acc[i]['correct']) for i in range(5, len(acc)-1)))\n    syn_total = sum((len(acc[i]['correct']) + len(acc[i]['incorrect'])) for i in range(5,len(acc)-1))\n    syn_acc = 100*float(syn_correct)/syn_total\n    print('Syntactic: {:d}/{:d}, Accuracy: {:.2f}%\\n'.format(syn_correct, syn_total, syn_acc))\n    return (sem_acc, syn_acc)\n```\n\n----------------------------------------\n\nTITLE: Vector Operations with Poincaré Embeddings\nDESCRIPTION: Shows various vector operations including distance calculations between random vectors in Poincaré space.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Tutorial.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nvector_1 = np.random.uniform(size=(100,))\nvector_2 = np.random.uniform(size=(100,))\nvectors_multiple = np.random.uniform(size=(5, 100))\n\n# Distance between vector_1 and vector_2\nprint(PoincareKeyedVectors.vector_distance(vector_1, vector_2))\n# Distance between vector_1 and each vector in vectors_multiple\nprint(PoincareKeyedVectors.vector_distance_batch(vector_1, vectors_multiple))\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Poincaré Models\nDESCRIPTION: Demonstrates two methods for persisting models: saving the full model state and saving only the vectors in word2vec format.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Tutorial.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Saves the entire PoincareModel instance, the loaded model can be trained further\nmodel.save('/tmp/test_model')\nPoincareModel.load('/tmp/test_model')\n\n# Saves only the vectors from the PoincareModel instance, in the commonly used word2vec format\nmodel.kv.save_word2vec_format('/tmp/test_vectors')\nPoincareKeyedVectors.load_word2vec_format('/tmp/test_vectors')\n```\n\n----------------------------------------\n\nTITLE: Combining Doc2Vec Models in Python\nDESCRIPTION: This snippet demonstrates how to combine two Doc2Vec models (DBOW and DM) using a ConcatenatedDoc2Vec wrapper class. This approach is based on Le and Mikolov's observation that combining paragraph vectors from different models can improve performance.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.test.test_doc2vec import ConcatenatedDoc2Vec\nmodels_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[0], simple_models[1]])\nmodels_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[0], simple_models[2]])\n```\n\n----------------------------------------\n\nTITLE: Creating a Dictionary from Processed Texts\nDESCRIPTION: Creates a Gensim Dictionary object from the processed texts, which maps words to their integer IDs. The dictionary is saved to disk for future reference.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_corpora_and_vector_spaces.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim import corpora\ndictionary = corpora.Dictionary(texts)\ndictionary.save('/tmp/deerwester.dict')  # store the dictionary, for future reference\nprint(dictionary)\n```\n\n----------------------------------------\n\nTITLE: Finding Similar Words Using Word2Vec\nDESCRIPTION: Demonstrates how to use the trained Word2Vec model to find words most similar to 'tree' based on vector similarity.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/downloader_api_tutorial.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel.most_similar('tree')\n```\n\n----------------------------------------\n\nTITLE: Using Doc2Vec Model with Scikit-Learn Pipeline in Python\nDESCRIPTION: This example shows how to use the Doc2Vec model with Scikit-Learn's Pipeline for text classification. It includes data preparation, model training, and evaluation using logistic regression.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/sklearn_api.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models import doc2vec\nd2v_sentences = [doc2vec.TaggedDocument(words, [i]) for i, words in enumerate(w2v_texts)]\n\nmodel = D2VTransformer(min_count=1)\nmodel.fit(d2v_sentences)\n\nclass_dict = {'mathematics': 1, 'physics': 0}\ntrain_data = [\n    (['calculus', 'mathematical'], 'mathematics'), (['geometry', 'operations', 'curves'], 'mathematics'),\n    (['natural', 'nuclear'], 'physics'), (['science', 'electromagnetism', 'natural'], 'physics')\n]\ntrain_input = list(map(lambda x: x[0], train_data))\ntrain_target = list(map(lambda x: class_dict[x[1]], train_data))\n\nclf = linear_model.LogisticRegression(penalty='l2', C=0.1)\nclf.fit(model.transform(train_input), train_target)\ntext_d2v = Pipeline([('features', model,), ('classifier', clf)])\nscore = text_d2v.score(train_input, train_target)\n\nprint(score)\n```\n\n----------------------------------------\n\nTITLE: Loading Word Embeddings and Creating Similarity Matrix\nDESCRIPTION: Loading pre-trained word2vec embeddings and building term similarity matrix\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_scm.rst#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport gensim.downloader as api\nmodel = api.load('word2vec-google-news-300')\n\nfrom gensim.similarities import SparseTermSimilarityMatrix, WordEmbeddingSimilarityIndex\ntermsim_index = WordEmbeddingSimilarityIndex(model)\ntermsim_matrix = SparseTermSimilarityMatrix(termsim_index, dictionary, tfidf)\n```\n\n----------------------------------------\n\nTITLE: Implementing Author Prediction with Gensim Topic Models\nDESCRIPTION: Function to predict document authors using topic modeling and Hellinger distance similarity. Takes a new document and returns top N most similar authors based on topic distributions.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_prediction_tutorial.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef predict_author(new_doc, atmodel, top_n=10, smallest_author=1):\n    from gensim import matutils\n    import pandas as pd\n\n    def similarity(vec1, vec2):\n        '''Get similarity between two vectors'''\n        dist = matutils.hellinger(matutils.sparse2full(vec1, atmodel.num_topics), \\\n                                  matutils.sparse2full(vec2, atmodel.num_topics))\n        sim = 1.0 / (1.0 + dist)\n        return sim\n\n    def get_sims(vec):\n        '''Get similarity of vector to all authors.'''\n        sims = [similarity(vec, vec2) for vec2 in author_vecs]\n        return sims\n\n    author_vecs = [atmodel.get_author_topics(author) for author in atmodel.id2author.values()]\n    new_doc_topics = atmodel.get_new_author_topics(new_doc)\n    sims = get_sims(new_doc_topics)\n\n    table = []\n    for elem in enumerate(sims):\n        author_name = atmodel.id2author[elem[0]]\n        sim = elem[1]\n        author_size = len(atmodel.author2doc[author_name])\n        if author_size >= smallest_author:\n            table.append((author_name, sim, author_size))\n\n    df = pd.DataFrame(table, columns=['Author', 'Score', 'Size'])\n    df = df.sort_values('Score', ascending=False)[:top_n]\n\n    return df\n```\n\n----------------------------------------\n\nTITLE: Comparing Coherence Models\nDESCRIPTION: Evaluates and compares coherence estimates across different trained models using the c_w2v method.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence_model_selection.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\ncoherence_estimates = cm.compare_models(trained_models.values())\ncoherences = dict(zip(trained_models.keys(), coherence_estimates))\nprint_coherence_rankings(coherences)\n```\n\n----------------------------------------\n\nTITLE: Training LDA Model with Gensim in Python\nDESCRIPTION: Configures and trains an LDA model with specified parameters including number of topics, chunk size, passes, and iterations. Uses auto-tuned alpha and eta parameters for optimization.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_lda.rst#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Train LDA model.\nfrom gensim.models import LdaModel\n\n# Set training parameters.\nnum_topics = 10\nchunksize = 2000\npasses = 20\niterations = 400\neval_every = None  # Don't evaluate model perplexity, takes too much time.\n\n# Make an index to word dictionary.\ntemp = dictionary[0]  # This is only to \"load\" the dictionary.\nid2word = dictionary.id2token\n\nmodel = LdaModel(\n    corpus=corpus,\n    id2word=id2word,\n    chunksize=chunksize,\n    alpha='auto',\n    eta='auto',\n    iterations=iterations,\n    num_topics=num_topics,\n    passes=passes,\n    eval_every=eval_every\n)\n```\n\n----------------------------------------\n\nTITLE: Downloading Word Embedding Evaluation Dataset\nDESCRIPTION: Downloads the questions-words.txt file containing word analogies for evaluating word embeddings. This dataset is used to compare the semantic and syntactic capabilities of the trained models.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# download the file questions-words.txt to be used for comparing word embeddings\n!wget https://raw.githubusercontent.com/tmikolov/word2vec/master/questions-words.txt\n```\n\n----------------------------------------\n\nTITLE: Printing Sentiment Prediction Error Rates in Python\nDESCRIPTION: Code snippet that prints the error rates for different Doc2Vec model configurations, sorted from best to worst. The snippet iterates through error rates stored in a dictionary and presents them in a formatted output.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Err_rate Model\")\nfor rate, name in sorted((rate, name) for name, rate in error_rates.items()):\n    print(f\"{rate} {name}\")\n```\n\n----------------------------------------\n\nTITLE: Training an Author-Topic Model in Python using Gensim\nDESCRIPTION: This code snippet demonstrates how to train an author-topic model using Gensim. It sets various parameters like number of topics, passes, and random state for reproducibility.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models import AuthorTopicModel\n%time model = AuthorTopicModel(corpus=corpus, num_topics=10, id2word=dictionary.id2token, \\\n                author2doc=author2doc, chunksize=2000, passes=1, eval_every=0, \\\n                iterations=1, random_state=1)\n```\n\n----------------------------------------\n\nTITLE: Initializing Annoy and Nmslib indexers with Word2Vec model\nDESCRIPTION: Sets up the model and initializes both Annoy and Nmslib indexers for comparison. The model's word vectors are normalized and the indexers are configured with specific parameters.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Set up the model and vector that we are using in the comparison\nfrom gensim.similarities.index import AnnoyIndexer\nfrom gensim.similarities.nmslib import NmslibIndexer\n\nmodel.init_sims()\nannoy_index = AnnoyIndexer(model, 300)\nnmslib_index = NmslibIndexer(model, {'M': 100, 'indexThreadQty': 1, 'efConstruction': 100}, {'efSearch': 10})\n```\n\n----------------------------------------\n\nTITLE: Training Word Embedding Models with Consistent Parameters\nDESCRIPTION: Creates a function to train three models: FastText with character n-grams, FastText without character n-grams, and Gensim's Word2Vec. All models use the same hyperparameters for fair comparison including learning rate, vector dimension, window size, and negative sampling.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nMODELS_DIR = 'models/'\n!mkdir -p {MODELS_DIR}\n\nlr = 0.05\ndim = 100\nws = 5\nepoch = 5\nminCount = 5\nneg = 5\nloss = 'ns'\nt = 1e-4\n\nfrom gensim.models import Word2Vec, KeyedVectors\nfrom gensim.models.word2vec import Text8Corpus\n\n# Same values as used for fastText training above\nparams = {\n    'alpha': lr,\n    'size': dim,\n    'window': ws,\n    'iter': epoch,\n    'min_count': minCount,\n    'sample': t,\n    'sg': 1,\n    'hs': 0,\n    'negative': neg\n}\n\ndef train_models(corpus_file, output_name):\n    output_file = '{:s}_ft'.format(output_name)\n    if not os.path.isfile(os.path.join(MODELS_DIR, '{:s}.vec'.format(output_file))):\n        print('Training fasttext on {:s} corpus..'.format(corpus_file))\n        %time !{FT_HOME}fasttext skipgram -input {corpus_file} -output {MODELS_DIR+output_file}  -lr {lr} -dim {dim} -ws {ws} -epoch {epoch} -minCount {minCount} -neg {neg} -loss {loss} -t {t}\n    else:\n        print('\\nUsing existing model file {:s}.vec'.format(output_file))\n        \n    output_file = '{:s}_ft_no_ng'.format(output_name)\n    if not os.path.isfile(os.path.join(MODELS_DIR, '{:s}.vec'.format(output_file))):\n        print('\\nTraining fasttext on {:s} corpus (without char n-grams)..'.format(corpus_file))\n        %time !{FT_HOME}fasttext skipgram -input {corpus_file} -output {MODELS_DIR+output_file}  -lr {lr} -dim {dim} -ws {ws} -epoch {epoch} -minCount {minCount} -neg {neg} -loss {loss} -t {t} -maxn 0\n    else:\n        print('\\nUsing existing model file {:s}.vec'.format(output_file))\n        \n    output_file = '{:s}_gs'.format(output_name)\n    if not os.path.isfile(os.path.join(MODELS_DIR, '{:s}.vec'.format(output_file))):\n        print('\\nTraining word2vec on {:s} corpus..'.format(corpus_file))\n        \n        # Text8Corpus class for reading space-separated words file\n        %time gs_model = Word2Vec(Text8Corpus(corpus_file), **params); gs_model\n        # Direct local variable lookup doesn't work properly with magic statements (%time)\n        locals()['gs_model'].wv.save_word2vec_format(os.path.join(MODELS_DIR, '{:s}.vec'.format(output_file)))\n        print('\\nSaved gensim model as {:s}.vec'.format(output_file))\n    else:\n        print('\\nUsing existing model file {:s}.vec'.format(output_file))\n\nevaluation_data = {}\ntrain_models('brown_corp.txt', 'brown')\n```\n\n----------------------------------------\n\nTITLE: Error Handling for Unknown Words\nDESCRIPTION: Demonstrates error handling when looking up words with no component character n-grams in the training data.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    model_wrapper['axe']\nexcept KeyError:\n    pass\nelse:\n    assert False, 'the above code should have raised a KeyError'\n```\n\n----------------------------------------\n\nTITLE: Initializing LDA Model in Python\nDESCRIPTION: Creates a Latent Dirichlet Allocation model for topic modeling with 100 topics. Uses dictionary for word mappings and operates on a document corpus. Implements online LDA parameter estimation for distributed processing.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_topics_and_transformations.rst#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nmodel = models.LdaModel(corpus, id2word=dictionary, num_topics=100)\n```\n\n----------------------------------------\n\nTITLE: Loading and Testing Saved Word Vectors and NMSLIB Index in Python\nDESCRIPTION: This code loads saved word vectors and NMSLIB index, then performs similarity queries using both the indexed and non-indexed methods.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nwv = KeyedVectors.load_word2vec_format('/tmp/vectors.bin', binary=True)\nnmslib_index = NmslibIndexer.load('/tmp/mymodel.index')\nnmslib_index.model = wv\n\nvector = wv[\"cat\"]\napproximate_neighbors = wv.most_similar([vector], topn=11, indexer=nmslib_index)\nprint(\"Approximate Neighbors\")\nfor neighbor in approximate_neighbors:\n    print(neighbor)\n\nnormal_neighbors = wv.most_similar([vector], topn=11)\nprint(\"\\nNormal (not Nmslib-indexed) Neighbors\")\nfor neighbor in normal_neighbors:\n    print(neighbor)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Word Embedding Models on Larger Corpus in Python\nDESCRIPTION: This snippet evaluates the performance of word embedding models trained on a larger corpus (text8) using the same analogy and similarity tasks as before.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Wordrank_comparisons.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint('Loading Gensim embeddings')\ntext8_gs = KeyedVectors.load_word2vec_format(MODELS_DIR + 'text8_gs.vec')\nprint('Accuracy for word2vec:')\nprint_analogy_accuracy(text8_gs, word_analogies_file)\nprint('SimLex-999 similarity')\nprint_similarity_accuracy(text8_gs, simlex_file)\nprint('\\nWordSim-353 similarity')\nprint_similarity_accuracy(text8_gs, wordsim_file)\n\nprint('Loading FastText embeddings')\ntext8_ft = KeyedVectors.load_word2vec_format(MODELS_DIR + 'text8_ft.vec')\nprint('Accuracy for FastText (with n-grams):')\nprint_analogy_accuracy(text8_ft, word_analogies_file)\nprint('SimLex-999 similarity')\nprint_similarity_accuracy(text8_ft, simlex_file)\nprint('\\nWordSim-353 similarity')\nprint_similarity_accuracy(text8_ft, wordsim_file)\n\nprint('\\nLoading Wordrank embeddings')\ntext8_wr = KeyedVectors.load_word2vec_format(MODELS_DIR + 'text8_wr.vec')\nprint('Accuracy for Wordrank:')\nprint_analogy_accuracy(text8_wr, word_analogies_file)\nprint('SimLex-999 similarity')\nprint_similarity_accuracy(text8_wr, simlex_file)\nprint('\\nWordSim-353 similarity')\nprint_similarity_accuracy(text8_wr, wordsim_file)\n\nprint('\\nLoading Wordrank ensemble embeddings')\ntext8_wr_ensemble = KeyedVectors.load_word2vec_format(MODELS_DIR + 'text8_wr_ensemble.vec')\nprint('Accuracy for Wordrank:')\nprint_analogy_accuracy(text8_wr_ensemble, word_analogies_file)\nprint('SimLex-999 similarity')\nprint_similarity_accuracy(text8_wr_ensemble, simlex_file)\nprint('\\nWordSim-353 similarity')\nprint_similarity_accuracy(text8_wr_ensemble, wordsim_file)\n```\n\n----------------------------------------\n\nTITLE: Training FastText Model using C++ Wrapper\nDESCRIPTION: Shows how to train a FastText model using Gensim's wrapper for FastText's C++ implementation. Requires local FastText installation.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models.wrappers.fasttext import FastText as FT_wrapper\n\n# Set FastText home to the path to the FastText executable\nft_home = '/home/misha/src/fastText-0.1.0/fasttext'\n\n# train the model\nmodel_wrapper = FT_wrapper.train(ft_home, corpus_file)\n\nprint(model_wrapper)\n```\n\n----------------------------------------\n\nTITLE: Downloading Text8 Corpus for Word2Vec Training in Python\nDESCRIPTION: Uses Gensim's downloader API to fetch the Text8 corpus for training the Word2Vec model.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_annoy.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport gensim.downloader as api\ntext8_path = api.load('text8', return_path=True)\nprint(\"Using corpus from\", text8_path)\n```\n\n----------------------------------------\n\nTITLE: Transforming Vectors using TF-IDF Model in Gensim\nDESCRIPTION: Demonstrates how to use the TF-IDF model to transform a bag-of-words vector into TF-IDF weights.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_topics_and_transformations.rst#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndoc_bow = [(0, 1), (1, 1)]\nprint(tfidf[doc_bow])  # step 2 -- use the model to transform vectors\n```\n\n----------------------------------------\n\nTITLE: Loading and Using a Model\nDESCRIPTION: Demonstrates how to load a model into memory and use it (example shows finding similar words using GloVe embeddings).\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_downloader_api.rst#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nmodel = api.load(\"glove-wiki-gigaword-50\")\nmodel.most_similar(\"glass\")\n```\n\n----------------------------------------\n\nTITLE: Analyzing Word Vector Similarities with Doc2Vec in Python\nDESCRIPTION: This snippet explores word vector similarities by selecting a random word and finding its most similar words across different Doc2Vec models. It demonstrates how the models capture semantic relationships between words.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_47\n\nLANGUAGE: python\nCODE:\n```\nimport random\n\nword_models = simple_models[:]\n\ndef pick_random_word(model, threshold=10):\n    # pick a random word with a suitable number of occurences\n    while True:\n        word = random.choice(model.wv.index_to_key)\n        if model.wv.get_vecattr(word, \"count\") > threshold:\n            return word\n\ntarget_word = pick_random_word(word_models[0])\n# or uncomment below line, to just pick a word from the relevant domain:\n# target_word = 'comedy/drama'\n\nfor model in word_models:\n    print(f'target_word: {repr(target_word)} model: {model} similar words:')\n    for i, (word, sim) in enumerate(model.wv.most_similar(target_word, topn=10), 1):\n        print(f'    {i}. {sim:.2f} {repr(word)}')\n    print()\n```\n\n----------------------------------------\n\nTITLE: Creating Word Embedding Similarity Matrix in Python\nDESCRIPTION: Loads pre-trained word embeddings and creates a sparse term similarity matrix for use in Soft Cosine Measure calculations.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n%%time\nimport gensim.downloader as api\n\nfrom gensim.similarities import SparseTermSimilarityMatrix\nfrom gensim.similarities import WordEmbeddingSimilarityIndex\n\nw2v_model = api.load(\"glove-wiki-gigaword-50\")\nsimilarity_index = WordEmbeddingSimilarityIndex(w2v_model)\nsimilarity_matrix = SparseTermSimilarityMatrix(similarity_index, dictionary)\n```\n\n----------------------------------------\n\nTITLE: Training Models on Text9 Corpus\nDESCRIPTION: Calls the previously defined training function to train FastText and Word2Vec models on the text9 corpus, which is a larger preprocessed Wikipedia text corpus.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrain_models(corpus_file='text9', output_name='text9')\n```\n\n----------------------------------------\n\nTITLE: Training an EnsembleLda Model on Opinosis Corpus in Python\nDESCRIPTION: Initializes and trains an EnsembleLda model with specific parameters tailored for the small Opinosis corpus. Parameters include 128 individual models, 20 topics, multiple passes, and parallel processing configurations to optimize performance and topic quality.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/ensemble_lda_with_opinosis.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nelda = EnsembleLda(\n    corpus=opinosis.corpus, id2word=opinosis.id2word, num_models=128, num_topics=20,\n    passes=20, iterations=100, ensemble_workers=3, distance_workers=4,\n    topic_model_class='ldamulticore', masking_method=rank_masking,\n)\npretty_print_topics()\n```\n\n----------------------------------------\n\nTITLE: Advanced Similarity Operations\nDESCRIPTION: Shows advanced similarity operations including n_similarity, doesn't_match, and analogy operations.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmodel_wrapper.n_similarity(['sushi', 'shop'], ['japanese', 'restaurant'])\n\nmodel_wrapper.doesnt_match(\"breakfast cereal dinner lunch\".split())\n\nmodel_wrapper.most_similar(positive=['baghdad', 'england'], negative=['london'])\n\nmodel_wrapper.accuracy(questions=datapath('questions-words.txt'))\n```\n\n----------------------------------------\n\nTITLE: Evaluating Word2Vec Word Pairs in Python\nDESCRIPTION: Demonstrates semantic similarity evaluation using word pairs dataset.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_word2vec.ipynb#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nmodel.wv.evaluate_word_pairs(datapath('wordsim353.tsv'))\n```\n\n----------------------------------------\n\nTITLE: Implementing a Memory-Efficient Corpus Iterator\nDESCRIPTION: Creates a custom corpus class that streams documents from a file one at a time, converting each to bag-of-words format without loading the entire corpus into memory.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_corpora_and_vector_spaces.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom smart_open import open  # for transparently opening remote files\n\n\nclass MyCorpus:\n    def __iter__(self):\n        for line in open('https://radimrehurek.com/mycorpus.txt'):\n            # assume there's one document per line, tokens separated by whitespace\n            yield dictionary.doc2bow(line.lower().split())\n```\n\n----------------------------------------\n\nTITLE: Implementing Corpus Reader Function\nDESCRIPTION: Defines a function to read and preprocess text documents from files, tokenizing text and optionally adding document tags for training data\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_doc2vec_lee.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport smart_open\n\ndef read_corpus(fname, tokens_only=False):\n    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n        for i, line in enumerate(f):\n            tokens = gensim.utils.simple_preprocess(line)\n            if tokens_only:\n                yield tokens\n            else:\n                # For training data, add tags\n                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n\ntrain_corpus = list(read_corpus(lee_train_file))\ntest_corpus = list(read_corpus(lee_test_file, tokens_only=True))\n```\n\n----------------------------------------\n\nTITLE: Performing WMD Similarity Query in Python with Gensim\nDESCRIPTION: This snippet demonstrates how to perform a similarity query using the WmdSimilarity instance. It preprocesses the query, runs the similarity search, and prints the results with their similarity scores.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/WMD_tutorial.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nstart = time()\n\nsent = 'Very good, you should seat outdoor.'\nquery = preprocess(sent)\n\nsims = instance[query]  # A query is simply a \"look-up\" in the similarity class.\n\nprint 'Cell took %.2f seconds to run.' %(time() - start)\n```\n\n----------------------------------------\n\nTITLE: Word Vector Lookup in FastText Model using Python\nDESCRIPTION: This code demonstrates how to access the word vectors from a FastText model, including for out-of-vocabulary words. It shows that FastText can generate vectors for words not seen during training.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_fasttext.rst#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwv = model.wv\nprint(wv)\n\nprint('night' in wv.key_to_index)\nprint('nights' in wv.key_to_index)\nprint(wv['night'])\nprint(wv['nights'])\n```\n\n----------------------------------------\n\nTITLE: Implementing Similarity Measures and Evaluation in Python\nDESCRIPTION: This extensive code block defines functions for data preprocessing, implementing various similarity measures (cosine, soft cosine, Word Mover's Distance), and performing evaluation using cross-validation.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom math import isnan\nfrom time import time\n\nfrom gensim.similarities import MatrixSimilarity, WmdSimilarity, SoftCosineSimilarity\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom wmd import WMD\n\ndef produce_test_data(dataset):\n    for orgquestion in datasets[dataset]:\n        query = preprocess(orgquestion[\"OrgQSubject\"]) + preprocess(orgquestion[\"OrgQBody\"])\n        documents = [\n            preprocess(thread[\"RelQuestion\"][\"RelQSubject\"]) + preprocess(thread[\"RelQuestion\"][\"RelQBody\"])\n            for thread in orgquestion[\"Threads\"]]\n        relevance = [\n            thread[\"RelQuestion\"][\"RELQ_RELEVANCE2ORGQ\"] in (\"PerfectMatch\", \"Relevant\")\n            for thread in orgquestion[\"Threads\"]]\n        yield query, documents, relevance\n\ndef cossim(query, documents):\n    # Compute cosine similarity between the query and the documents.\n    query = tfidf[dictionary.doc2bow(query)]\n    index = MatrixSimilarity(\n        tfidf[[dictionary.doc2bow(document) for document in documents]],\n        num_features=len(dictionary))\n    similarities = index[query]\n    return similarities\n\ndef softcossim(query, documents):\n    # Compute Soft Cosine Measure between the query and the documents.\n    query = tfidf[dictionary.doc2bow(query)]\n    index = SoftCosineSimilarity(\n        tfidf[[dictionary.doc2bow(document) for document in documents]],\n        similarity_matrix)\n    similarities = index[query]\n    return similarities\n\ndef wmd_gensim(query, documents):\n    # Compute Word Mover's Distance as implemented in POT\n    # between the query and the documents.\n    index = WmdSimilarity(documents, w2v_model)\n    similarities = index[query]\n    return similarities\n\ndef wmd_relax(query, documents):\n    # Compute Word Mover's Distance as implemented in WMD by Source{d}\n    # between the query and the documents.\n    words = [word for word in set(chain(query, *documents)) if word in w2v_model.wv]\n    indices, words = zip(*sorted((\n        (index, word) for (index, _), word in zip(dictionary.doc2bow(words), words))))\n    query = dict(tfidf[dictionary.doc2bow(query)])\n    query = [\n        (new_index, query[dict_index])\n        for new_index, dict_index in enumerate(indices)\n        if dict_index in query]\n    documents = [dict(tfidf[dictionary.doc2bow(document)]) for document in documents]\n    documents = [[\n        (new_index, document[dict_index])\n        for new_index, dict_index in enumerate(indices)\n        if dict_index in document] for document in documents]\n    embeddings = np.array([w2v_model.wv[word] for word in words], dtype=np.float32)\n    nbow = dict(((index, list(chain([None], zip(*document)))) for index, document in enumerate(documents)))\n    nbow[\"query\"] = tuple([None] + list(zip(*query)))\n    distances = WMD(embeddings, nbow, vocabulary_min=1).nearest_neighbors(\"query\")\n    similarities = [-distance for _, distance in sorted(distances)]\n    return similarities\n\nstrategies = {\n    \"cossim\" : cossim,\n    \"softcossim\": softcossim,\n    \"wmd-gensim\": wmd_gensim,\n    \"wmd-relax\": wmd_relax}\n\ndef evaluate(split, strategy):\n    # Perform a single round of evaluation.\n    results = []\n    start_time = time()\n    for query, documents, relevance in split:\n        similarities = strategies[strategy](query, documents)\n        assert len(similarities) == len(documents)\n        precision = [\n            (num_correct + 1) / (num_total + 1) for num_correct, num_total in enumerate(\n                num_total for num_total, (_, relevant) in enumerate(\n                    sorted(zip(similarities, relevance), reverse=True)) if relevant)]\n        average_precision = np.mean(precision) if precision else 0.0\n        results.append(average_precision)\n    return (np.mean(results) * 100, time() - start_time)\n\ndef crossvalidate(args):\n    # Perform a cross-validation.\n    dataset, strategy = args\n    test_data = np.array(list(produce_test_data(dataset)))\n    kf = KFold(n_splits=10)\n    samples = []\n    for _, test_index in kf.split(test_data):\n        samples.append(evaluate(test_data[test_index], strategy))\n    return (np.mean(samples, axis=0), np.std(samples, axis=0))\n```\n\n----------------------------------------\n\nTITLE: Computing Soft Cosine Measure Between Sentences in Python\nDESCRIPTION: Calculates the Soft Cosine Measure similarity between two sentences using the inner_product method.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsimilarity = similarity_matrix.inner_product(sentence_obama, sentence_president, normalized=True)\nprint('similarity = %.4f' % similarity)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running Distributed Online LDA in Python\nDESCRIPTION: This snippet demonstrates how to create and run a distributed online LDA model using Gensim. It sets up 100 topics and uses a corpus and dictionary to train the model.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/dist_lda.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> # extract 100 LDA topics, using default parameters\n>>> lda = LdaModel(corpus=mm, id2word=id2word, num_topics=100, distributed=True)\nusing distributed version with 4 workers\nrunning online LDA training, 100 topics, 1 passes over the supplied corpus of 3199665 documets, updating model once every 40000 documents\n..\n```\n\n----------------------------------------\n\nTITLE: Evaluating Models for WordNet Reconstruction in Python\nDESCRIPTION: Loop that evaluates different model implementations on the WordNet reconstruction task. For each model, it loads the embedding, creates a ReconstructionEvaluation instance, and calculates metrics including mean rank and MAP across different model dimensions.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Evaluation.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfor implementation, models in sorted(model_files.items()):\n    for model_name, files in models.items():\n        if model_name in reconstruction_results:\n            continue\n        reconstruction_results[model_name] = OrderedDict()\n        for metric in metrics:\n            reconstruction_results[model_name][metric] = {}\n        for model_size, model_file in files.items():\n            print('Evaluating model %s of size %d' % (model_name, model_size))\n            embedding = load_model(implementation, model_file)\n            eval_instance = ReconstructionEvaluation(wordnet_file, embedding)\n            eval_result = eval_instance.evaluate(max_n=1000)\n            for metric in metrics:\n                reconstruction_results[model_name][metric][model_size] = eval_result[metric]\n```\n\n----------------------------------------\n\nTITLE: Creating a Coherence Model for Multiple LDA Models in Python\nDESCRIPTION: Initializes a CoherenceModel for evaluating the coherence of all trained LDA models at once. Uses the 'c_v' coherence measure which is highly correlated with human judgments of topic quality.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence_model_selection.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\n# Now estimate the probabilities for the CoherenceModel.\n# This performs a single pass over the reference corpus, accumulating\n# the necessary statistics for all of the models at once.\ncm = models.CoherenceModel.for_models(\n    trained_models.values(), dictionary, texts=corpus.get_texts(), coherence='c_v')\n```\n\n----------------------------------------\n\nTITLE: Text8 Corpus Model Evaluation in Python\nDESCRIPTION: Evaluates Word2Vec and FastText models (with and without n-grams) trained on Text8 corpus\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\naccuracies = []\nprint('Loading Gensim embeddings')\ntext8_gs = KeyedVectors.load_word2vec_format(MODELS_DIR + 'text8_gs.vec')\nprint('Accuracy for word2vec:')\naccuracies.append(print_accuracy(text8_gs, word_analogies_file))\n\nprint('Loading FastText embeddings (with n-grams)')\ntext8_ft = KeyedVectors.load_word2vec_format(MODELS_DIR + 'text8_ft.vec')\nprint('Accuracy for FastText (with n-grams):')\naccuracies.append(print_accuracy(text8_ft, word_analogies_file))\n\nprint('Loading FastText embeddings')\ntext8_ft_no_ng = KeyedVectors.load_word2vec_format(MODELS_DIR + 'text8_ft_no_ng.vec')\nprint('Accuracy for FastText (without n-grams):')\naccuracies.append(print_accuracy(text8_ft_no_ng, word_analogies_file))\n```\n\n----------------------------------------\n\nTITLE: Implementing Hellinger Distance for Author Similarity in Python\nDESCRIPTION: This snippet defines functions to calculate author similarities using the Hellinger distance. It includes methods for computing similarities, generating a table of similar authors, and formatting the results using pandas.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim import matutils\nimport pandas as pd\n\n# Make a list of all the author-topic distributions.\nauthor_vecs = [model.get_author_topics(author) for author in model.id2author.values()]\n\ndef similarity(vec1, vec2):\n    '''Get similarity between two vectors'''\n    dist = matutils.hellinger(matutils.sparse2full(vec1, model.num_topics), \\\n                              matutils.sparse2full(vec2, model.num_topics))\n    sim = 1.0 / (1.0 + dist)\n    return sim\n\ndef get_sims(vec):\n    '''Get similarity of vector to all authors.'''\n    sims = [similarity(vec, vec2) for vec2 in author_vecs]\n    return sims\n\ndef get_table(name, top_n=10, smallest_author=1):\n    '''\n    Get table with similarities, author names, and author sizes.\n    Return `top_n` authors as a dataframe.\n    \n    '''\n    \n    # Get similarities.\n    sims = get_sims(model.get_author_topics(name))\n\n    # Arrange author names, similarities, and author sizes in a list of tuples.\n    table = []\n    for elem in enumerate(sims):\n        author_name = model.id2author[elem[0]]\n        sim = elem[1]\n        author_size = len(model.author2doc[author_name])\n        if author_size >= smallest_author:\n            table.append((author_name, sim, author_size))\n            \n    # Make dataframe and retrieve top authors.\n    df = pd.DataFrame(table, columns=['Author', 'Score', 'Size'])\n    df = df.sort_values('Score', ascending=False)[:top_n]\n    \n    return df\n```\n\n----------------------------------------\n\nTITLE: Using Text2Bow Model with Scikit-Learn Pipeline in Python\nDESCRIPTION: This example demonstrates how to use the Text2Bow model with Scikit-Learn's Pipeline for text classification. It combines Text2Bow, LDA, and logistic regression in a pipeline.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/sklearn_api.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntext2bow_model = Text2BowTransformer()\nlda_model = LdaTransformer(num_topics=2, passes=10, minimum_probability=0, random_state=np.random.seed(0))\nclf = linear_model.LogisticRegression(penalty='l2', C=0.1)\ntext_t2b = Pipeline([('bow_model', text2bow_model), ('ldamodel', lda_model), ('classifier', clf)])\ntext_t2b.fit(data.data, data.target)\nscore = text_t2b.score(data.data, data.target)\n\nprint(score)\n```\n\n----------------------------------------\n\nTITLE: Loading a Pre-trained WordRank Model in Python\nDESCRIPTION: This code shows how to load pre-trained WordRank embeddings into Gensim. It loads the word embedding file and vocabulary file, with the option to sort the vocabulary according to word frequency in the corpus.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/WordRank_wrapper_quickstart.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwr_word_embedding = 'wordrank.words'\nvocab_file = 'vocab.txt'\n\nmodel = Wordrank.load_wordrank_model(wr_word_embedding, vocab_file, sorted_vocab=1)\n```\n\n----------------------------------------\n\nTITLE: Online Training Word2Vec Model in Python\nDESCRIPTION: Example of resuming Word2Vec training with additional sentences and vocabulary updating.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_word2vec.ipynb#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nmodel = gensim.models.Word2Vec.load(temporary_filepath)\nmore_sentences = [\n    ['Advanced', 'users', 'can', 'load', 'a', 'model',\n     'and', 'continue', 'training', 'it', 'with', 'more', 'sentences'],\n]\nmodel.build_vocab(more_sentences, update=True)\nmodel.train(more_sentences, total_examples=model.corpus_count, epochs=model.epochs)\n\n# cleaning up temporary file\nimport os\nos.remove(temporary_filepath)\n```\n\n----------------------------------------\n\nTITLE: Finding Similar Words with Word2Vec in Python\nDESCRIPTION: Uses the trained Word2Vec model to find words similar to 'tree'.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_downloader_api.rst#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(model.wv.most_similar('tree'))\n```\n\n----------------------------------------\n\nTITLE: Performing Vector Arithmetic with Doc2Vec Models on Wikipedia Data in Python\nDESCRIPTION: This snippet demonstrates vector arithmetic capabilities of Doc2Vec models. It finds entries similar to 'Lady Gaga' - 'American' + 'Japanese', showcasing the semantic relationships captured by the model.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfor model in [model_dbow, model_dm]:\n    print(model)\n    vec = [model.dv[\"Lady Gaga\"] - model.wv[\"american\"] + model.wv[\"japanese\"]]\n    pprint([m for m in model.dv.most_similar(vec, topn=11) if m[0] != \"Lady Gaga\"])\n```\n\n----------------------------------------\n\nTITLE: Computing WMD Distance Between Unrelated Sentences\nDESCRIPTION: Creates a new sentence about oranges that is semantically unrelated to the previous sentences, removes stopwords, and calculates its WMD distance from the Obama sentence, demonstrating how unrelated sentences have larger distances.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/WMD_tutorial.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsentence_orange = 'Oranges are my favorite fruit'\nsentence_orange = sentence_orange.lower().split()\nsentence_orange = [w for w in sentence_orange if w not in stop_words]\n\ndistance = model.wmdistance(sentence_obama, sentence_orange)\nprint('distance = %.4f' % distance)\n```\n\n----------------------------------------\n\nTITLE: Accessing word vectors in Word2Vec model using Python\nDESCRIPTION: Shows how to retrieve the vector representation of a word from the Word2Vec model.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_word2vec.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nvec_king = wv['king']\n```\n\n----------------------------------------\n\nTITLE: Demonstrating efficient multiprocessing with memory-mapped Nmslib index\nDESCRIPTION: Shows an efficient approach where two processes load the Word2Vec model separately but share a memory-mapped Nmslib index from disk, reducing total memory usage.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\nmodel.save('/tmp/mymodel.pkl')\n\ndef f(process_id):\n    print('Process Id: {}'.format(os.getpid()))\n    process = psutil.Process(os.getpid())\n    new_model = Word2Vec.load('/tmp/mymodel.pkl')\n    vector = new_model[\"science\"]\n    nmslib_index = NmslibIndexer.load('/tmp/mymodel.index')\n    nmslib_index.model = new_model\n    approximate_neighbors = new_model.most_similar([vector], topn=5, indexer=nmslib_index)\n    print('\\nMemory used by process {}: {}\\n---'.format(os.getpid(), process.memory_info()))\n```\n\n----------------------------------------\n\nTITLE: Training Doc2Vec Models on Wikipedia Corpus in Python\nDESCRIPTION: These code blocks train the DBOW and DM Doc2Vec models on the entire Wikipedia corpus. Training is time-consuming, taking several hours on a typical machine.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Train DBOW doc2vec incl. word vectors.\n# Report progress every ½ hour.\nmodel_dbow.train(documents, total_examples=model_dbow.corpus_count, epochs=model_dbow.epochs, report_delay=30*60)\n```\n\nLANGUAGE: python\nCODE:\n```\n# Train DM doc2vec.\nmodel_dm.train(documents, total_examples=model_dm.corpus_count, epochs=model_dm.epochs, report_delay=30*60)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Text Data with Tokenization and Filtering\nDESCRIPTION: Processes the document corpus by tokenizing, removing stopwords, and filtering out words that appear only once in the corpus. Uses a defaultdict to track word frequencies.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_corpora_and_vector_spaces.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pprint import pprint  # pretty-printer\nfrom collections import defaultdict\n\n# remove common words and tokenize\nstoplist = set('for a of the and to in'.split())\ntexts = [\n    [word for word in document.lower().split() if word not in stoplist]\n    for document in documents\n]\n\n# remove words that appear only once\nfrequency = defaultdict(int)\nfor text in texts:\n    for token in text:\n        frequency[token] += 1\n\ntexts = [\n    [token for token in text if frequency[token] > 1]\n    for text in texts\n]\n\npprint(texts)\n```\n\n----------------------------------------\n\nTITLE: Training Word2Vec Model on Text8 Corpus in Python\nDESCRIPTION: Trains a Word2Vec model on the Text8 corpus using specified parameters. The trained model is then used for similarity queries.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_annoy.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models import Word2Vec, KeyedVectors\nfrom gensim.models.word2vec import Text8Corpus\n\n# Using params from Word2Vec_FastText_Comparison\nparams = {\n    'alpha': 0.05,\n    'vector_size': 100,\n    'window': 5,\n    'epochs': 5,\n    'min_count': 5,\n    'sample': 1e-4,\n    'sg': 1,\n    'hs': 0,\n    'negative': 5,\n}\nmodel = Word2Vec(Text8Corpus(text8_path), **params)\nwv = model.wv\nprint(\"Using trained model\", wv)\n```\n\n----------------------------------------\n\nTITLE: Converting a Document to Bag-of-Words Vector\nDESCRIPTION: Demonstrates how to convert a new document into a bag-of-words vector representation using the previously created dictionary.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_corpora_and_vector_spaces.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nnew_doc = \"Human computer interaction\"\nnew_vec = dictionary.doc2bow(new_doc.lower().split())\nprint(new_vec)  # the word \"interaction\" does not appear in the dictionary and is ignored\n```\n\n----------------------------------------\n\nTITLE: Using File-based Training for *2Vec Models in Gensim\nDESCRIPTION: Example demonstrating the file-based training approach for Word2Vec, Doc2Vec, and FastText models. This approach allows models to scale linearly with the number of CPU cores by eliminating GIL limitations, resulting in significant performance improvements.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/CHANGELOG.md#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport gensim.downloader as api\nfrom multiprocessing import cpu_count\nfrom gensim.utils import save_as_line_sentence\nfrom gensim.test.utils import get_tmpfile\nfrom gensim.models import Word2Vec, Doc2Vec, FastText\n\n\n# Convert any corpus to the needed format: 1 document per line, words delimited by \" \"\ncorpus = api.load(\"text8\")\ncorpus_fname = get_tmpfile(\"text8-file-sentence.txt\")\nsave_as_line_sentence(corpus, corpus_fname)\n\n# Choose num of cores that you want to use (let's use all, models scale linearly now!)\nnum_cores = cpu_count()\n\n# Train models using all cores\nw2v_model = Word2Vec(corpus_file=corpus_fname, workers=num_cores)\nd2v_model = Doc2Vec(corpus_file=corpus_fname, workers=num_cores)\nft_model = FastText(corpus_file=corpus_fname, workers=num_cores)\n```\n\n----------------------------------------\n\nTITLE: Training Doc2Vec with File-based Mode in Python\nDESCRIPTION: Example demonstrating how to use the file-based training mode for Doc2Vec in Gensim. The code loads a corpus, saves it in line-sentence format, and trains a Doc2Vec model using the corpus_file parameter.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Any2Vec_Filebased.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport gensim.downloader as api\nfrom gensim.utils import save_as_line_sentence\nfrom gensim.models.doc2vec import Doc2Vec\n\ncorpus = api.load(\"text8\")\nsave_as_line_sentence(corpus, \"my_corpus.txt\")\n\nmodel = Doc2Vec(corpus_file=\"my_corpus.txt\", epochs=5, vector_size=300, workers=14)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running Distributed Batch LDA in Python\nDESCRIPTION: This code snippet shows how to set up and run a distributed batch LDA model in Gensim. It configures 100 topics, 20 passes over the corpus, and updates the model after processing all documents.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/dist_lda.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> lda = LdaModel(corpus=mm, id2word=id2token, num_topics=100, update_every=0, passes=20, distributed=True)\nusing distributed version with 4 workers\nrunning batch LDA training, 100 topics, 20 passes over the supplied corpus of 3199665 documets, updating model once every 3199665 documents\ninitializing workers\niteration 0, dispatching documents up to #10000/3199665\niteration 0, dispatching documents up to #20000/3199665\n...\n```\n\n----------------------------------------\n\nTITLE: Loading Relations from File and Training Poincaré Model\nDESCRIPTION: Shows how to load relations from a TSV file and initialize/train the model with specific parameters.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Tutorial.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nrelations = PoincareRelations(file_path=wordnet_mammal_file, delimiter='\\t')\nmodel = PoincareModel(train_data=relations)\n\nmodel = PoincareModel(train_data=relations, size=2, burn_in=0)\nmodel.train(epochs=1, print_every=500)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Yelp Review Data for Word2Vec and WMD in Python\nDESCRIPTION: This snippet loads Yelp review data, preprocesses the text, and creates corpora for Word2Vec training and WMD similarity queries. It filters reviews for specific restaurants and handles JSON parsing.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/WMD_tutorial.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Business IDs of the restaurants.\nids = ['4bEjOyTaDG24SY5TxsaUNQ', '2e2e7WgqU1BnpxmQL5jbfw', 'zt1TpTuJ6y9n551sw9TaEg',\n      'Xhg93cMdemu5pAMkDoEdtQ', 'sIyHTizqAiGu12XMLX3N3g', 'YNQgak-ZLtYJQxlDwN-qIg']\n\nw2v_corpus = []  # Documents to train word2vec on (all 6 restaurants).\nwmd_corpus = []  # Documents to run queries against (only one restaurant).\ndocuments = []  # wmd_corpus, with no pre-processing (so we can see the original documents).\nwith smart_open('/data/yelp_academic_dataset_review.json', 'rb') as data_file:\n    for line in data_file:\n        json_line = json.loads(line)\n        \n        if json_line['business_id'] not in ids:\n            # Not one of the 6 restaurants.\n            continue\n        \n        # Pre-process document.\n        text = json_line['text']  # Extract text from JSON object.\n        text = preprocess(text)\n        \n        # Add to corpus for training Word2Vec.\n        w2v_corpus.append(text)\n        \n        if json_line['business_id'] == ids[0]:\n            # Add to corpus for similarity queries.\n            wmd_corpus.append(text)\n            documents.append(json_line['text'])\n\nprint 'Cell took %.2f seconds to run.' %(time() - start)\n```\n\n----------------------------------------\n\nTITLE: Training Word2Vec with File-based Mode in Python\nDESCRIPTION: Example showing how to use the file-based training mode for Word2Vec in Gensim. It loads a corpus, saves it in line-sentence format, and trains a Word2Vec model using the corpus_file parameter.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Any2Vec_Filebased.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport gensim\nimport gensim.downloader as api\nfrom gensim.utils import save_as_line_sentence\nfrom gensim.models.word2vec import Word2Vec\n\nprint(gensim.models.word2vec.CORPUSFILE_VERSION)  # must be >= 0, i.e. optimized compiled version\n\ncorpus = api.load(\"text8\")\nsave_as_line_sentence(corpus, \"my_corpus.txt\")\n\nmodel = Word2Vec(corpus_file=\"my_corpus.txt\", iter=5, size=300, workers=14)\n```\n\n----------------------------------------\n\nTITLE: Word Dissimilarity Analysis using Gensim\nDESCRIPTION: Uses the doesnt_match() function to find which word is semantically dissimilar from a given set of words related to meals.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_fasttext.rst#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(wv.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n```\n\n----------------------------------------\n\nTITLE: Loading Pre-trained Word2Vec Model with Gensim\nDESCRIPTION: Uses Gensim's downloader API to load the pre-trained 'word2vec-google-news-300' model, which contains word embeddings trained on Google News data with 300 dimensions.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/WMD_tutorial.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport gensim.downloader as api\napi.load('word2vec-google-news-300')\n```\n\n----------------------------------------\n\nTITLE: Evaluating Word Analogy Accuracy in Gensim Models\nDESCRIPTION: Tests and compares word analogy accuracy between models trained using 'sentences' and 'corpus_file' methods. Uses Gensim's built-in evaluation function with the questions-words.txt dataset, displaying results as percentages.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Any2Vec_Filebased.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.test.utils import datapath\n\nmodel_sent_accuracy = model_sent.wv.evaluate_word_analogies(datapath('questions-words.txt'))[0]\nprint(\"Word analogy accuracy with `sentences`: {:.1f}%\".format(100.0 * model_sent_accuracy))\n\nmodel_corp_file_accuracy = model_corp_file.wv.evaluate_word_analogies(datapath('questions-words.txt'))[0]\nprint(\"Word analogy accuracy with `corpus_file`: {:.1f}%\".format(100.0 * model_corp_file_accuracy))\n```\n\n----------------------------------------\n\nTITLE: Word2Vec Model Initialization and Training\nDESCRIPTION: Setup and training of Word2Vec models for each star rating category using Gensim\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models import Word2Vec\nimport multiprocessing\n\n## create a w2v learner \nbasemodel = Word2Vec(\n    workers=multiprocessing.cpu_count(), # use your cores\n    iter=3, # iter = sweeps of SGD through the data; more is better\n    hs=1, negative=0 # we only have scoring for the hierarchical softmax setup\n    )\nprint(basemodel)\n\nbasemodel.build_vocab(StarSentences(revtrain))\n\nfrom copy import deepcopy\nstarmodels = [deepcopy(basemodel) for i in range(5)]\nfor i in range(5):\n    slist = list(StarSentences(revtrain, [i+1]))\n    print(i+1, \"stars (\", len(slist), \")\")\n    starmodels[i].train(  slist, total_examples=len(slist) )\n```\n\n----------------------------------------\n\nTITLE: Word Vector Lookup Operations\nDESCRIPTION: Shows how to perform word vector lookups, including handling of out-of-vocabulary words through character n-grams.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprint('night' in model_wrapper.wv.vocab)\nprint('nights' in model_wrapper.wv.vocab)\nprint(model_wrapper['night'])\nprint(model_wrapper['nights'])\n```\n\n----------------------------------------\n\nTITLE: Computing Word Similarity with WordRank in Python\nDESCRIPTION: This snippet demonstrates how to calculate the semantic similarity between two words ('President' and 'military') using the similarity method of the trained WordRank model.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/WordRank_wrapper_quickstart.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel.similarity('President', 'military')\n```\n\n----------------------------------------\n\nTITLE: Brown Corpus Model Evaluation in Python\nDESCRIPTION: Loads and evaluates Word2Vec and FastText models trained on Brown corpus\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nword_analogies_file = 'questions-words.txt'\naccuracies = []\nprint('\\nLoading Gensim embeddings')\nbrown_gs = KeyedVectors.load_word2vec_format(MODELS_DIR + 'brown_gs.vec')\nprint('Accuracy for Word2Vec:')\naccuracies.append(print_accuracy(brown_gs, word_analogies_file))\n\nprint('\\nLoading FastText embeddings')\nbrown_ft = KeyedVectors.load_word2vec_format(MODELS_DIR + 'brown_ft.vec')\nprint('Accuracy for FastText (with n-grams):')\naccuracies.append(print_accuracy(brown_ft, word_analogies_file))\n```\n\n----------------------------------------\n\nTITLE: Identifying outlier words using Word2Vec in Python\nDESCRIPTION: Demonstrates the use of Word2Vec's 'doesnt_match' function to find the word that doesn't belong in a list.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_word2vec.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprint(wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))\n```\n\n----------------------------------------\n\nTITLE: Creating Serialized Author-Topic Model with Gensim in Python\nDESCRIPTION: This snippet demonstrates how to create an AuthorTopicModel using serialized corpora. It specifies the serialization path and sets the serialized flag to True, which is useful for handling large datasets that don't fit in memory.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n%time model_ser = AuthorTopicModel(corpus=corpus, num_topics=10, id2word=dictionary.id2token, \\\n                               author2doc=author2doc, random_state=1, serialized=True, \\\n                               serialization_path='/tmp/model_serialization.mm')\n```\n\n----------------------------------------\n\nTITLE: Building Vocabulary for Doc2Vec Models on Wikipedia Corpus in Python\nDESCRIPTION: This snippet builds the vocabulary for the DBOW model and then copies it to the DM model to save time. It processes the entire Wikipedia corpus to collect the top 1M vocabulary items.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel_dbow.build_vocab(documents, progress_per=500000)\nprint(model_dbow)\n\n# Save some time by copying the vocabulary structures from the DBOW model to the DM model.\n# Both models are built on top of exactly the same data, so there's no need to repeat the vocab-building step.\nmodel_dm.reset_from(model_dbow)\nprint(model_dm)\n```\n\n----------------------------------------\n\nTITLE: Loading and Timing Pre-trained Word2Vec Model\nDESCRIPTION: Loads the Google News pre-trained Word2Vec model using Gensim's API and measures the time taken for loading. This model provides 300-dimensional word embeddings needed for WMD calculations.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/WMD_tutorial.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nstart = time()\nimport os\n\n# from gensim.models import KeyedVectors\n# if not os.path.exists('/data/w2v_googlenews/GoogleNews-vectors-negative300.bin.gz'):\n#     raise ValueError(\"SKIP: You need to download the google news model\")\n#    \n# model = KeyedVectors.load_word2vec_format('/data/w2v_googlenews/GoogleNews-vectors-negative300.bin.gz', binary=True)\nmodel = api.load('word2vec-google-news-300')\n\nprint('Cell took %.2f seconds to run.' % (time() - start))\n```\n\n----------------------------------------\n\nTITLE: Converting Documents to Bag-of-Words Vectors\nDESCRIPTION: Converts a processed text corpus into bag-of-words vector representations using Gensim's dictionary.doc2bow() method.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_core_concepts.rst#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nbow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\npprint.pprint(bow_corpus)\n```\n\n----------------------------------------\n\nTITLE: Transforming Query to LSI Space\nDESCRIPTION: Demonstrates how to transform a user query into the LSI space for similarity comparison.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_similarity_queries.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndoc = \"Human computer interaction\"\nvec_bow = dictionary.doc2bow(doc.lower().split())\nvec_lsi = lsi[vec_bow]  # convert the query to LSI space\nprint(vec_lsi)\n```\n\n----------------------------------------\n\nTITLE: Using TfIdf Model with Scikit-Learn Pipeline in Python\nDESCRIPTION: This example shows how to use the TfIdf model with Scikit-Learn's Pipeline for text classification. It combines TfIdf, LDA, and logistic regression in a pipeline.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/sklearn_api.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ntfidf_model = TfIdfTransformer()\ntfidf_model.fit(corpus)\nlda_model = LdaTransformer(num_topics=2, passes=10, minimum_probability=0, random_state=np.random.seed(0))\nclf = linear_model.LogisticRegression(penalty='l2', C=0.1)\ntext_tfidf = Pipeline((('tfidf_model', tfidf_model), ('ldamodel', lda_model), ('classifier', clf)))\ntext_tfidf.fit(corpus, data.target)\nscore = text_tfidf.score(corpus, data.target)\n\nprint(score)\n```\n\n----------------------------------------\n\nTITLE: Training FastText Model on Lee Corpus using Gensim\nDESCRIPTION: Demonstrates how to initialize a FastText model, build vocabulary, and train the model using the Lee Corpus. It sets up the model with 100-dimensional vectors and trains for 5 epochs.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_fasttext.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pprint import pprint as print\nfrom gensim.models.fasttext import FastText\nfrom gensim.test.utils import datapath\n\n# Set file names for train and test data\ncorpus_file = datapath('lee_background.cor')\n\nmodel = FastText(vector_size=100)\n\n# build the vocabulary\nmodel.build_vocab(corpus_file=corpus_file)\n\n# train the model\nmodel.train(\n    corpus_file=corpus_file, epochs=model.epochs,\n    total_examples=model.corpus_count, total_words=model.corpus_total_words,\n)\n\nprint(model)\n```\n\n----------------------------------------\n\nTITLE: Comparing FastText Training Methods in Python\nDESCRIPTION: Code comparing the performance of training FastText using both the traditional sentences parameter and the new corpus_file parameter. It measures and compares both training time and model accuracy.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Any2Vec_Filebased.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models.word2vec import LineSentence\nfrom gensim.models.fasttext import FastText\nimport time\n\nstart_time = time.time()\nmodel_corp_file = FastText(corpus_file=CORPUS_FILE, iter=5, size=300, workers=32)\nfile_time = time.time() - start_time\n\nstart_time = time.time()\nmodel_sent = FastText(sentences=LineSentence(CORPUS_FILE), iter=5, size=300, workers=32)\nsent_time = time.time() - start_time\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Training model with `sentences` took {:.3f} seconds\".format(sent_time))\nprint(\"Training model with `corpus_file` took {:.3f} seconds\".format(file_time))\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.test.utils import datapath\n\nmodel_sent_accuracy = model_sent.wv.evaluate_word_analogies(datapath('questions-words.txt'))[0]\nprint(\"Word analogy accuracy with `sentences`: {:.1f}%\".format(100.0 * model_sent_accuracy))\n\nmodel_corp_file_accuracy = model_corp_file.wv.evaluate_word_analogies(datapath('questions-words.txt'))[0]\nprint(\"Word analogy accuracy with `corpus_file`: {:.1f}%\".format(100.0 * model_corp_file_accuracy))\n```\n\n----------------------------------------\n\nTITLE: Working with Word2Vec File Formats\nDESCRIPTION: Demonstrates various operations with word2vec files including saving and loading models in both text and binary formats, creating and saving Annoy indices, and comparing approximate vs exact neighbors.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_annoy.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# To export our model as text\nwv.save_word2vec_format('/tmp/vectors.txt', binary=False)\n\nfrom smart_open import open\n# View the first 3 lines of the exported file\nwith open('/tmp/vectors.txt', encoding='utf8') as myfile:\n    for i in range(3):\n        print(myfile.readline().strip())\n\n# To import a word2vec text model\nwv = KeyedVectors.load_word2vec_format('/tmp/vectors.txt', binary=False)\n\n# To export a model as binary\nwv.save_word2vec_format('/tmp/vectors.bin', binary=True)\n\n# To import a word2vec binary model\nwv = KeyedVectors.load_word2vec_format('/tmp/vectors.bin', binary=True)\n\n# To create and save Annoy Index from a loaded `KeyedVectors` object (with 100 trees)\nannoy_index = AnnoyIndexer(wv, 100)\nannoy_index.save('/tmp/mymodel.index')\n\n# Load and test the saved word vectors and saved Annoy index\nwv = KeyedVectors.load_word2vec_format('/tmp/vectors.bin', binary=True)\nannoy_index = AnnoyIndexer()\nannoy_index.load('/tmp/mymodel.index')\nannoy_index.model = wv\n\nvector = wv[\"cat\"]\napproximate_neighbors = wv.most_similar([vector], topn=11, indexer=annoy_index)\n# Neatly print the approximate_neighbors and their corresponding cosine similarity values\nprint(\"Approximate Neighbors\")\nfor neighbor in approximate_neighbors:\n    print(neighbor)\n\nnormal_neighbors = wv.most_similar([vector], topn=11)\nprint(\"\\nExact Neighbors\")\nfor neighbor in normal_neighbors:\n    print(neighbor)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Dynamic Topic Models with pyLDAvis in Python\nDESCRIPTION: This snippet demonstrates how to use pyLDAvis to visualize a Dynamic Topic Model for a specific time slice using the dtm_vis method.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/ldaseqmodel.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndoc_topic, topic_term, doc_lengths, term_frequency, vocab = ldaseq.dtm_vis(time=0, corpus=corpus)\nvis_dtm = pyLDAvis.prepare(topic_term_dists=topic_term, doc_topic_dists=doc_topic, doc_lengths=doc_lengths, vocab=vocab, term_frequency=term_frequency)\npyLDAvis.display(vis_dtm)\n```\n\n----------------------------------------\n\nTITLE: Creating and Saving a Document Corpus\nDESCRIPTION: Converts all documents in the preprocessed texts to their bag-of-words vector representations and saves the corpus to disk in Matrix Market format.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_corpora_and_vector_spaces.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncorpus = [dictionary.doc2bow(text) for text in texts]\ncorpora.MmCorpus.serialize('/tmp/deerwester.mm', corpus)  # store to disk, for later use\nprint(corpus)\n```\n\n----------------------------------------\n\nTITLE: Converting Documents to Bag-of-Words Representation in Python\nDESCRIPTION: Transforms documents into a vectorized bag-of-words representation using a pre-built dictionary. Creates corpus by computing frequency of each word including bigrams.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_lda.rst#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Bag-of-words representation of the documents.\ncorpus = [dictionary.doc2bow(doc) for doc in docs]\n```\n\n----------------------------------------\n\nTITLE: Evaluating Topic Coherence for Dynamic Topic Models in Python\nDESCRIPTION: This code demonstrates how to calculate topic coherence for a specific time slice of a Dynamic Topic Model using both U_mass and C_v coherence measures.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/ldaseqmodel.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models.coherencemodel import CoherenceModel\nimport pickle\n\ntopics_wrapper = dtm_model.dtm_coherence(time=0)\ntopics_dtm = ldaseq.dtm_coherence(time=2)\n\ncm_wrapper = CoherenceModel(topics=topics_wrapper, corpus=corpus, dictionary=dictionary, coherence='u_mass')\ncm_DTM = CoherenceModel(topics=topics_dtm, corpus=corpus, dictionary=dictionary, coherence='u_mass')\n\nprint (\"U_mass topic coherence\")\nprint (\"Wrapper coherence is \", cm_wrapper.get_coherence())\nprint (\"DTM Python coherence is\", cm_DTM.get_coherence())\n\ntexts = pickle.load(open('Corpus/texts', 'rb'))\ncm_wrapper = CoherenceModel(topics=topics_wrapper, texts=texts, dictionary=dictionary, coherence='c_v')\ncm_DTM = CoherenceModel(topics=topics_dtm, texts=texts, dictionary=dictionary, coherence='c_v')\n\nprint (\"C_v topic coherence\")\nprint (\"Wrapper coherence is \", cm_wrapper.get_coherence())\nprint (\"DTM Python coherence is\", cm_DTM.get_coherence())\n```\n\n----------------------------------------\n\nTITLE: Creating LSI Model for Similarity Analysis\nDESCRIPTION: Creates a 2-dimensional LSI model from the corpus for similarity analysis.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_similarity_queries.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim import models\nlsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)\n```\n\n----------------------------------------\n\nTITLE: Loading and Evaluating Word Embedding Models in Python\nDESCRIPTION: This snippet loads pre-trained word embedding models (Word2Vec, FastText, WordRank) and evaluates their performance on word analogy and similarity tasks using the previously defined functions.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Wordrank_comparisons.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nMODELS_DIR = 'models/'\nword_analogies_file = './datasets/questions-words.txt'\nsimlex_file = '../../gensim/test/test_data/simlex999.txt'\nwordsim_file = '../../gensim/test/test_data/wordsim353.tsv'\n\nprint('\\nLoading Gensim embeddings')\nbrown_gs = KeyedVectors.load_word2vec_format(MODELS_DIR + 'brown_gs.vec')\nprint('Accuracy for Word2Vec:')\nprint_analogy_accuracy(brown_gs, word_analogies_file)\nprint('SimLex-999 similarity')\nprint_similarity_accuracy(brown_gs, simlex_file)\nprint('\\nWordSim-353 similarity')\nprint_similarity_accuracy(brown_gs, wordsim_file)\n\nprint('\\nLoading FastText embeddings')\nbrown_ft = KeyedVectors.load_word2vec_format(MODELS_DIR + 'brown_ft.vec')\nprint('Accuracy for FastText:')\nprint_analogy_accuracy(brown_ft, word_analogies_file)\nprint('SimLex-999 similarity')\nprint_similarity_accuracy(brown_ft, simlex_file)\nprint('\\nWordSim-353 similarity')\nprint_similarity_accuracy(brown_ft, wordsim_file)\n\nprint('\\nLoading Wordrank embeddings')\nbrown_wr = KeyedVectors.load_word2vec_format(MODELS_DIR + 'brown_wr.vec')\nprint('Accuracy for Wordrank:')\nprint_analogy_accuracy(brown_wr, word_analogies_file)\nprint('SimLex-999 similarity')\nprint_similarity_accuracy(brown_wr, simlex_file)\nprint('\\nWordSim-353 similarity')\nprint_similarity_accuracy(brown_wr, wordsim_file)\n\nprint('\\nLoading Wordrank ensemble embeddings')\nbrown_wr_ensemble = KeyedVectors.load_word2vec_format(MODELS_DIR + 'brown_wr_ensemble.vec')\nprint('Accuracy for Wordrank:')\nprint_analogy_accuracy(brown_wr_ensemble, word_analogies_file)\nprint('SimLex-999 similarity')\nprint_similarity_accuracy(brown_wr_ensemble, simlex_file)\nprint('\\nWordSim-353 similarity')\nprint_similarity_accuracy(brown_wr_ensemble, wordsim_file)\n```\n\n----------------------------------------\n\nTITLE: Logistic Regression for Doc2Vec Evaluation in Python\nDESCRIPTION: This code defines functions to train a logistic regression model on Doc2Vec vectors and evaluate its performance. It uses the statsmodels library for logistic regression and calculates the error rate on a test set.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport statsmodels.api as sm\nfrom random import sample\n\ndef logistic_predictor_from_data(train_targets, train_regressors):\n    \"\"\"Fit a statsmodel logistic predictor on supplied data\"\"\"\n    logit = sm.Logit(train_targets, train_regressors)\n    predictor = logit.fit(disp=0)\n    # print(predictor.summary())\n    return predictor\n\ndef error_rate_for_model(test_model, train_set, test_set):\n    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n\n    train_targets = [doc.sentiment for doc in train_set]\n    train_regressors = [test_model.dv[doc.tags[0]] for doc in train_set]\n    train_regressors = sm.add_constant(train_regressors)\n    predictor = logistic_predictor_from_data(train_targets, train_regressors)\n\n    test_regressors = [test_model.dv[doc.tags[0]] for doc in test_set]\n    test_regressors = sm.add_constant(test_regressors)\n\n    # Predict & evaluate\n    test_predictions = predictor.predict(test_regressors)\n    corrects = sum(np.rint(test_predictions) == [doc.sentiment for doc in test_set])\n    errors = len(test_predictions) - corrects\n    error_rate = float(errors) / len(test_predictions)\n    return (error_rate, errors, len(test_predictions), predictor)\n```\n\n----------------------------------------\n\nTITLE: Training the Author-Topic Model\nDESCRIPTION: Defines a function to train the Gensim Author-Topic Model with configurable parameters including topic count, evaluation frequency, iterations, and passes. It also calculates and prints the average topic coherence for model evaluation.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_prediction_tutorial.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef train_model(corpus, author2doc, dictionary, num_topics=20, eval_every=0, iterations=50, passes=20):\n    from gensim.models import AuthorTopicModel\n    \n    model = AuthorTopicModel(corpus=corpus, num_topics=num_topics, id2word=dictionary.id2token, \\\n                    author2doc=author2doc, chunksize=2500, passes=passes, \\\n                    eval_every=eval_every, iterations=iterations, random_state=1)\n    top_topics = model.top_topics(corpus)\n    tc = sum([t[1] for t in top_topics]) \n    print(tc / num_topics)\n    return model\n```\n\n----------------------------------------\n\nTITLE: Exploring Topics in an Author-Topic Model using Python and Gensim\nDESCRIPTION: This code snippet shows how to print the most important words for each topic in the trained author-topic model.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfor topic in model.show_topics(num_topics=10):\n    print('Label: ' + topic_labels[topic[0]])\n    words = ''\n    for word, prob in model.show_topic(topic[0]):\n        words += word + ' '\n    print('Words: ' + words)\n    print()\n```\n\n----------------------------------------\n\nTITLE: Implementing IMDB Dataset Download and Processing\nDESCRIPTION: Functions to download, extract and process IMDB movie review dataset. Includes utilities for downloading the tar.gz file and creating sentiment documents from the reviews.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport io\nimport re\nimport tarfile\nimport os.path\n\nimport smart_open\nimport gensim.utils\n\ndef download_dataset(url='http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'):\n    fname = url.split('/')[-1]\n\n    if os.path.isfile(fname):\n       return fname\n\n    # Download the file to local storage first.\n    try:\n        kwargs = { 'compression': smart_open.compression.NO_COMPRESSION }\n        fin = smart_open.open(url, \"rb\", **kwargs)\n    except (AttributeError, TypeError):\n        kwargs = { 'ignore_ext': True }\n        fin = smart_open.open(url, \"rb\", **kwargs)\n    if fin:\n        with smart_open.open(fname, 'wb', **kwargs) as fout:\n            while True:\n                buf = fin.read(io.DEFAULT_BUFFER_SIZE)\n                if not buf:\n                    break\n                fout.write(buf)\n        fin.close()\n\n    return fname\n\ndef create_sentiment_document(name, text, index):\n    _, split, sentiment_str, _ = name.split('/')\n    sentiment = {'pos': 1.0, 'neg': 0.0, 'unsup': None}[sentiment_str]\n\n    if sentiment is None:\n        split = 'extra'\n\n    tokens = gensim.utils.to_unicode(text).split()\n    return SentimentDocument(tokens, [index], split, sentiment)\n\ndef extract_documents():\n    fname = download_dataset()\n\n    index = 0\n\n    with tarfile.open(fname, mode='r:gz') as tar:\n        for member in tar.getmembers():\n            if re.match(r'aclImdb/(train|test)/(pos|neg|unsup)/\\d+_\\d+.txt$', member.name):\n                member_bytes = tar.extractfile(member).read()\n                member_text = member_bytes.decode('utf-8', errors='replace')\n                assert member_text.count('\\n') == 0\n                yield create_sentiment_document(member.name, member_text, index)\n                index += 1\n\nalldocs = list(extract_documents())\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading an Author-Topic Model in Python using Gensim\nDESCRIPTION: These code snippets demonstrate how to save a trained author-topic model to disk and load it back into memory.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Save model.\nmodel.save('/tmp/model.atmodel')\n```\n\nLANGUAGE: python\nCODE:\n```\n# Load model.\nmodel = AuthorTopicModel.load('/tmp/model.atmodel')\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Inner Product Between Two Documents in Python\nDESCRIPTION: This function benchmarks the speed of the inner_product method for computing term similarities between single documents. It measures execution time, corpus properties, and matrix characteristics for various configurations.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_benchmark.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef benchmark(configuration):\n    (matrix, dictionary, nonzero_limit), corpus, normalized, repetition = configuration\n    corpus_size = len(corpus)\n    corpus = [dictionary.doc2bow(doc) for doc in corpus]\n    corpus = [vec for vec in corpus if len(vec) > 0]\n    \n    start_time = time()\n    for vec1 in corpus:\n        for vec2 in corpus:\n            matrix.inner_product(vec1, vec2, normalized=normalized)\n    end_time = time()\n    duration = end_time - start_time\n    \n    return {\n        \"dictionary_size\": matrix.matrix.shape[0],\n        \"matrix_nonzero\": matrix.matrix.nnz,\n        \"nonzero_limit\": nonzero_limit,\n        \"normalized\": normalized,\n        \"corpus_size\": corpus_size,\n        \"corpus_actual_size\": len(corpus),\n        \"corpus_nonzero\": sum(len(vec) for vec in corpus),\n        \"mean_document_length\": np.mean([len(doc) for doc in corpus]),\n        \"repetition\": repetition,\n        \"duration\": duration, }\n```\n\n----------------------------------------\n\nTITLE: Comparing Word2Vec Training Methods: Queue-based vs File-based in Python\nDESCRIPTION: Code comparing the performance of training Word2Vec using both the traditional sentences parameter and the new corpus_file parameter. It measures training time and model accuracy on word analogies.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Any2Vec_Filebased.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models.word2vec import LineSentence\nimport time\n\nstart_time = time.time()\nmodel_sent = Word2Vec(sentences=LineSentence(CORPUS_FILE), iter=5, size=300, workers=32)\nsent_time = time.time() - start_time\n\nstart_time = time.time()\nmodel_corp_file = Word2Vec(corpus_file=CORPUS_FILE, iter=5, size=300, workers=32)\nfile_time = time.time() - start_time\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Training model with `sentences` took {:.3f} seconds\".format(sent_time))\nprint(\"Training model with `corpus_file` took {:.3f} seconds\".format(file_time))\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.test.utils import datapath\n```\n\nLANGUAGE: python\nCODE:\n```\nmodel_sent_accuracy = model_sent.wv.evaluate_word_analogies(datapath('questions-words.txt'))[0]\nprint(\"Word analogy accuracy with `sentences`: {:.1f}%\".format(100.0 * model_sent_accuracy))\n\nmodel_corp_file_accuracy = model_corp_file.wv.evaluate_word_analogies(datapath('questions-words.txt'))[0]\nprint(\"Word analogy accuracy with `corpus_file`: {:.1f}%\".format(100.0 * model_corp_file_accuracy))\n```\n\n----------------------------------------\n\nTITLE: Importing Word2Vec Binary Model in Python\nDESCRIPTION: This code imports a word2vec binary model into a KeyedVectors object.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nwv = KeyedVectors.load_word2vec_format('/tmp/vectors.bin', binary=True)\n```\n\n----------------------------------------\n\nTITLE: Building a Word2Vec model with Text8 corpus\nDESCRIPTION: Creates a Word2Vec model using the Text8 corpus with specific parameters for training. The model will be used later to demonstrate similarity queries with different indexers.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models import Word2Vec, KeyedVectors\nfrom gensim.models.word2vec import Text8Corpus\n\n# Using params from Word2Vec_FastText_Comparison\n\nparams = {\n    'alpha': 0.05,\n    'size': 100,\n    'window': 5,\n    'iter': 5,\n    'min_count': 5,\n    'sample': 1e-4,\n    'sg': 1,\n    'hs': 0,\n    'negative': 5\n}\n\nmodel = Word2Vec(Text8Corpus('text8'), **params)\nprint(model)\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading FastText Model in Python\nDESCRIPTION: This snippet demonstrates how to save a trained FastText model to a temporary file and then load it back. It uses Gensim's save and load methods for FastText models.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_fasttext.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport tempfile\nimport os\nwith tempfile.NamedTemporaryFile(prefix='saved_model_gensim-', delete=False) as tmp:\n    model.save(tmp.name, separately=[])\n\n# Load back the same model.\nloaded_model = FastText.load(tmp.name)\nprint(loaded_model)\n\nos.unlink(tmp.name)  # demonstration complete, don't need the temp file anymore\n```\n\n----------------------------------------\n\nTITLE: Word2Vec Model Training and Frequency Analysis - Brown Corpus\nDESCRIPTION: Initializes a Word2Vec model using the Brown corpus, builds vocabulary and computes word frequencies. Then plots comparison graphs showing accuracy of Word2Vec, WordRank and FastText models for semantic and syntactic analogies.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Wordrank_comparisons.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# a sample model using gensim's Word2Vec for getting vocab counts\ncorpus = Text8Corpus('proc_brown_corp.txt')\nmodel = Word2Vec(min_count=5)\nmodel.build_vocab(corpus)\nfreq = {}\nfor word in model.wv.index2word:\n    freq[word] = model.wv.vocab[word].count\n\n# plot results\nword2vec = compute_accuracies('brown_gs.vec', freq)\nwordrank = compute_accuracies('brown_wr_ensemble.vec', freq)\nfasttext = compute_accuracies('brown_ft.vec', freq)\n\nfig = plt.figure(figsize=(7,15))\n\nfor i, subplot, title in zip([0, 1, 2], ['311', '312', '313'], ['Semantic Analogies', 'Syntactic Analogies', 'Total Analogy']):\n    ax = fig.add_subplot(subplot)\n    ax.plot(word2vec[i][0], word2vec[i][1], 'r-', label='Word2Vec')\n    ax.plot(wordrank[i][0], wordrank[i][1], 'g--', label='WordRank')\n    ax.plot(fasttext[i][0], fasttext[i][1], 'b:', label='FastText')\n    ax.set_ylabel('Average accuracy')\n    ax.set_xlabel('Log mean frequency')\n    ax.set_title(title)\n    ax.legend(loc='upper right', prop={'size':10})\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Classifier Testing Function Definition\nDESCRIPTION: Defines a function to evaluate document vector classification performance using logistic regression. Takes training and test data with labels as input and returns classification accuracy.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/translation_matrix.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\n\ndef test_classifier_error(train, train_label, test, test_label):\n    classifier = LogisticRegression()\n    classifier.fit(train, train_label)\n    score = classifier.score(test, test_label)\n    print(\"the classifier score :\", score)\n    return score\n```\n\n----------------------------------------\n\nTITLE: Coloring Words in Dictionary Based on LDA Topic Model in Python\nDESCRIPTION: This function takes an LDA model and a dictionary, then colors each word based on its most likely topic. It uses matplotlib to create a visualization where words are colored according to their topic association. The function handles cases where a word is associated with one or multiple topics.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_methods.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef color_words_dict(model, dictionary):\n    import matplotlib.pyplot as plt\n    import matplotlib.patches as patches\n\n    word_topics = []\n    for word_id in dictionary:\n        word = str(dictionary[word_id])\n        # get_term_topics returns static topics, as mentioned before\n        probs = model.get_term_topics(word)\n        # we are creating word_topics which is similar to the one created by get_document_topics\n        try:\n            if probs[0][1] >= probs[1][1]:\n                word_topics.append((word_id, [0, 1]))\n            else:\n                word_topics.append((word_id, [1, 0]))\n        # this in the case only one topic is returned\n        except IndexError:\n            word_topics.append((word_id, [probs[0][0]]))\n            \n    # color-topic matching\n    topic_colors = { 1:'red', 0:'blue'}\n    \n    # set up fig to plot\n    fig = plt.figure()\n    ax = fig.add_axes([0,0,1,1])\n\n    # a sort of hack to make sure the words are well spaced out.\n    word_pos = 1/len(doc)\n         \n    # use matplotlib to plot words\n    for word, topics in word_topics:\n        ax.text(word_pos, 0.8, model.id2word[word],\n                horizontalalignment='center',\n                verticalalignment='center',\n                fontsize=20, color=topic_colors[topics[0]],  # choose just the most likely topic\n                transform=ax.transAxes)\n        word_pos += 0.2 # to move the word for the next iter\n\n    ax.set_axis_off()\n    plt.show()\n```\n\n----------------------------------------\n\nTITLE: Visualizing Word Embeddings using t-SNE in Python\nDESCRIPTION: Implements dimensionality reduction of word embeddings using t-SNE and provides visualization functions using both Plotly and Matplotlib. Includes utilities for extracting vectors and labels from the Word2Vec model.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_word2vec.ipynb#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.decomposition import IncrementalPCA    # inital reduction\nfrom sklearn.manifold import TSNE                   # final reduction\nimport numpy as np                                  # array handling\n\n\ndef reduce_dimensions(model):\n    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n\n    # extract the words & their vectors, as numpy arrays\n    vectors = np.asarray(model.wv.vectors)\n    labels = np.asarray(model.wv.index_to_key)  # fixed-width numpy strings\n\n    # reduce using t-SNE\n    tsne = TSNE(n_components=num_dimensions, random_state=0)\n    vectors = tsne.fit_transform(vectors)\n\n    x_vals = [v[0] for v in vectors]\n    y_vals = [v[1] for v in vectors]\n    return x_vals, y_vals, labels\n\n\nx_vals, y_vals, labels = reduce_dimensions(model)\n\ndef plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True):\n    from plotly.offline import init_notebook_mode, iplot, plot\n    import plotly.graph_objs as go\n\n    trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)\n    data = [trace]\n\n    if plot_in_notebook:\n        init_notebook_mode(connected=True)\n        iplot(data, filename='word-embedding-plot')\n    else:\n        plot(data, filename='word-embedding-plot.html')\n\n\ndef plot_with_matplotlib(x_vals, y_vals, labels):\n    import matplotlib.pyplot as plt\n    import random\n\n    random.seed(0)\n\n    plt.figure(figsize=(12, 12))\n    plt.scatter(x_vals, y_vals)\n\n    #\n    # Label randomly subsampled 25 data points\n    #\n    indices = list(range(len(labels)))\n    selected_indices = random.sample(indices, 25)\n    for i in selected_indices:\n        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n\ntry:\n    get_ipython()\nexcept Exception:\n    plot_function = plot_with_matplotlib\nelse:\n    plot_function = plot_with_plotly\n\nplot_function(x_vals, y_vals, labels)\n```\n\n----------------------------------------\n\nTITLE: Transforming New Documents with Trained LDA Model in Python\nDESCRIPTION: This snippet demonstrates how to use a trained LDA model to transform new, unseen documents into LDA topic distributions. It assumes a pre-trained LDA model and a new document represented as a bag-of-words count vector.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/wiki.rst#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndoc_lda = lda[doc_bow]\n```\n\n----------------------------------------\n\nTITLE: Filtering Extreme Words from NIPS Documents using Gensim in Python\nDESCRIPTION: Creates a dictionary representation of the NIPS documents and filters out rare and common words. Words appearing in less than 20 documents or more than 50% of documents are removed to improve the quality of the LDA model.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_lda.rst#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.corpora import Dictionary\n\n# Create a dictionary representation of the documents.\ndictionary = Dictionary(docs)\n\n# Filter out words that occur less than 20 documents, or more than 50% of the documents.\ndictionary.filter_extremes(no_below=20, no_above=0.5)\n```\n\n----------------------------------------\n\nTITLE: Lemmatizing NIPS Documents using NLTK in Python\nDESCRIPTION: Applies lemmatization to the tokenized NIPS documents using NLTK's WordNetLemmatizer. Lemmatization is preferred over stemming for more readable output in topic modeling.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_lda.rst#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\ndocs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Sentences and Removing Stopwords in Python\nDESCRIPTION: This snippet downloads stopwords, defines a preprocessing function to remove stopwords, and applies it to the sample sentences.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_wmd.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Import and download stopwords from NLTK.\nfrom nltk.corpus import stopwords\nfrom nltk import download\ndownload('stopwords')  # Download stopwords list.\nstop_words = stopwords.words('english')\n\ndef preprocess(sentence):\n    return [w for w in sentence.lower().split() if w not in stop_words]\n\nsentence_obama = preprocess(sentence_obama)\nsentence_president = preprocess(sentence_president)\n```\n\n----------------------------------------\n\nTITLE: Training LDA Model\nDESCRIPTION: Creates and trains the LDA model with 35 topics\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_network.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nlda_fake = LdaModel(corpus=corpus_fake, id2word=dictionary, num_topics=35, chunksize=1500, iterations=200, alpha='auto')\nlda_fake.save('lda_35')\n```\n\n----------------------------------------\n\nTITLE: Testing similarity search with different indexers\nDESCRIPTION: Performs a dry run of similarity searches using Annoy, Nmslib, and the traditional brute force method to ensure all indices are properly loaded into RAM.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Dry run to make sure both indices are fully in RAM\nvector = model.wv.syn0norm[0]\nprint(model.most_similar([vector], topn=5, indexer=annoy_index))\nprint(model.most_similar([vector], topn=5, indexer=nmslib_index))\nprint(model.most_similar([vector], topn=5))\n```\n\n----------------------------------------\n\nTITLE: Data Preprocessing and LDA Input Preparation\nDESCRIPTION: Loads and preprocesses the text data, removing stopwords and punctuation, and creates the dictionary and corpus for LDA\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_network.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf_fake = pd.read_csv('fake.csv')\ndf_fake[['title', 'text', 'language']].head()\ndf_fake = df_fake.loc[(pd.notnull(df_fake.text)) & (df_fake.language=='english')]\n\n# remove stopwords and punctuations\ndef preprocess(row):\n    return strip_punctuation(remove_stopwords(row.lower()))\n    \ndf_fake['text'] = df_fake['text'].apply(preprocess)\n\n# Convert data to required input format by LDA\ntexts = []\nfor line in df_fake.text:\n    lowered = line.lower()\n    words = re.findall(r'\\w+', lowered, flags=re.UNICODE|re.LOCALE)\n    texts.append(words)\n# Create a dictionary representation of the documents.\ndictionary = Dictionary(texts)\n\n# Filter out words that occur less than 2 documents, or more than 30% of the documents.\ndictionary.filter_extremes(no_below=2, no_above=0.4)\n# Bag-of-words representation of the documents.\ncorpus_fake = [dictionary.doc2bow(text) for text in texts]\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Text Data\nDESCRIPTION: Preprocesses the corpus text data by filtering specific topics and applying string preprocessing\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_methods.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport collections\nfrom gensim.parsing.preprocessing import preprocess_string\n\ntexts = [\n    preprocess_string(text['data'])\n    for text in corpus\n    if text['topic'] in ('soc.religion.christian', 'talk.politics.guns')\n]\n```\n\n----------------------------------------\n\nTITLE: Benchmarking query times of Gensim, Annoy, and Nmslib\nDESCRIPTION: Compares the query performance of traditional brute force method, Annoy indexer, and Nmslib indexer over 10,000 queries, calculating speed improvements.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nqueries = 10000\n\ngensim_time = avg_query_time(queries=queries)\nannoy_time = avg_query_time(annoy_index, queries=queries)\nnmslib_time = avg_query_time(nmslib_index, queries=queries)\nprint(\"Gensim (s/query):\\t{0:.5f}\".format(gensim_time))\nprint(\"Annoy (s/query):\\t{0:.5f}\".format(annoy_time))\nprint(\"Nmslib (s/query):\\t{0:.5f}\".format(nmslib_time))\nspeed_improvement_gensim = gensim_time / nmslib_time\nspeed_improvement_annoy = annoy_time / nmslib_time\nprint (\"\\nNmslib is {0:.2f} times faster on average on this particular run\".format(speed_improvement_gensim))\nprint (\"\\nNmslib is {0:.2f} times faster on average than annoy on this particular run\".format(speed_improvement_annoy))\n```\n\n----------------------------------------\n\nTITLE: Retrieving Author Topic Distributions in Python using Gensim\nDESCRIPTION: This code demonstrates how to retrieve and print the topic distribution for a specific author using the trained author-topic model.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nmodel['YannLeCun']\n```\n\n----------------------------------------\n\nTITLE: Initializing Word2Vec and WmdSimilarity in Python with Gensim\nDESCRIPTION: This snippet trains a Word2Vec model on the preprocessed corpus and initializes a WmdSimilarity instance. It sets up the framework for performing similarity queries using Word Mover's Distance.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/WMD_tutorial.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Train Word2Vec on all the restaurants.\nmodel = Word2Vec(w2v_corpus, workers=3, size=100)\n\n# Initialize WmdSimilarity.\nfrom gensim.similarities import WmdSimilarity\nnum_best = 10\ninstance = WmdSimilarity(wmd_corpus, model, num_best=10)\n```\n\n----------------------------------------\n\nTITLE: Word Analogy Evaluation using Gensim\nDESCRIPTION: Evaluates word analogies using a predefined test set (questions-words.txt) to assess the quality of word embeddings across different categories.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_fasttext.rst#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprint(wv.evaluate_word_analogies(datapath('questions-words.txt')))\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Documents for Author-Topic Modeling\nDESCRIPTION: Defines a comprehensive preprocessing function that organizes document IDs by author, cleans text, performs NLP processing with SpaCy, lemmatizes words, removes stopwords, and generates bigrams for better topic modeling.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_prediction_tutorial.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os, re, io\ndef preprocess_docs(data_dir):\n    doc_ids = []\n    author2doc = {}\n    docs = []\n    \n    folders = os.listdir(data_dir)  # List of filenames.\n    for authorname in folders:\n        files = file = os.listdir(data_dir + '/' + authorname)\n        for filen in files:\n            (idx1, idx2) = re.search('[0-9]+', filen).span()  # Matches the indexes of the start end end of the ID.\n            if not author2doc.get(authorname):\n                # This is a new author.\n                author2doc[authorname] = []\n            doc_id = str(int(filen[idx1:idx2]))\n            doc_ids.append(doc_id)\n            author2doc[authorname].extend([doc_id])\n\n            # Read document text.\n            # Note: ignoring characters that cause encoding errors.\n            with io.open(data_dir + '/' + authorname + '/' + filen, errors='ignore', encoding='utf-8') as fid:\n                txt = fid.read()\n\n            # Replace any whitespace (newline, tabs, etc.) by a single space.\n            txt = re.sub('\\s', ' ', txt)\n            docs.append(txt)\n            \n    doc_id_dict = dict(zip(doc_ids, range(len(doc_ids))))\n    # Replace dataset IDs by integer IDs.\n    for a, a_doc_ids in author2doc.items():\n        for i, doc_id in enumerate(a_doc_ids):\n            author2doc[a][i] = doc_id_dict[doc_id]\n    import spacy\n    nlp = spacy.load('en')\n    \n    %%time\n    processed_docs = []\n    for doc in nlp.pipe(docs, n_threads=4, batch_size=100):\n        # Process document using Spacy NLP pipeline.\n\n        ents = doc.ents  # Named entities.\n\n        # Keep only words (no numbers, no punctuation).\n        # Lemmatize tokens, remove punctuation and remove stopwords.\n        doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n\n        # Remove common words from a stopword list.\n        #doc = [token for token in doc if token not in STOPWORDS]\n\n        # Add named entities, but only if they are a compound of more than word.\n        doc.extend([str(entity) for entity in ents if len(entity) > 1])\n        processed_docs.append(doc)\n    docs = processed_docs\n    del processed_docs\n    \n    # Compute bigrams.\n\n    from gensim.models import Phrases\n\n    # Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n    bigram = Phrases(docs, min_count=20)\n    for idx in range(len(docs)):\n        for token in bigram[docs[idx]]:\n            if '_' in token:\n                # Token is a bigram, add to document.\n                docs[idx].append(token)\n    return docs, author2doc\n```\n\n----------------------------------------\n\nTITLE: Training Doc2Vec Models on Small and Large Corpora\nDESCRIPTION: Trains two Doc2Vec models on different sized document collections with vocabulary building and iterative training over 50 epochs. Saves the trained models to disk.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/translation_matrix.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nmodel1.build_vocab(small_train_docs)\nfor epoch in range(50):\n    shuffle(small_train_docs)\n    model1.train(small_train_docs, total_examples=len(small_train_docs), epochs=1)\nmodel.save(\"small_doc_15000_iter50.bin\")\n\nlarge_train_docs = train_docs + test_docs\nmodel2.build_vocab(large_train_docs)\nfor epoch in range(50):\n    shuffle(large_train_docs)\n    model2.train(large_train_docs, total_examples=len(train_docs), epochs=1)\nmodel2.save(\"large_doc_50000_iter50.bin\")\n```\n\n----------------------------------------\n\nTITLE: Converting GloVe Models to Word2Vec Format in Python\nDESCRIPTION: Command for converting Stanford's GloVe pre-trained models to Word2Vec format using Gensim's conversion script. This preprocessing step is necessary to make GloVe models compatible with Gensim's Word2Vec implementation.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_downloader_api.rst#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npython -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-50.txt\n```\n\n----------------------------------------\n\nTITLE: Training Doc2Vec Model\nDESCRIPTION: Trains the Doc2Vec model on the training corpus using specified parameters\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_doc2vec_lee.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmodel.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Topic Models\nDESCRIPTION: Implements pyLDAvis visualization for both LDA models if the visualization library is available.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence_tutorial.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nif CAN_VISUALIZE:\n    prepared = pyLDAvis.gensim.prepare(goodLdaModel, corpus, dictionary)\n    display(pyLDAvis.display(prepared))\n```\n\n----------------------------------------\n\nTITLE: Training FastText with File-based Mode in Python\nDESCRIPTION: Example demonstrating how to use the file-based training mode for FastText in Gensim. The code loads a corpus, saves it in line-sentence format, and trains a FastText model using the corpus_file parameter.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Any2Vec_Filebased.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport gensim.downloader as api\nfrom gensim.utils import save_as_line_sentence\nfrom gensim.models.fasttext import FastText\n\ncorpus = api.load(\"text8\")\nsave_as_line_sentence(corpus, \"my_corpus.txt\")\n\nmodel = FastText(corpus_file=\"my_corpus.txt\", iter=5, size=300, workers=14)\n```\n\n----------------------------------------\n\nTITLE: Visualizing topic evolution over time in Dynamic Topic Model\nDESCRIPTION: Shows how to print the evolution of a specific topic (in this case, topic 0) across all time slices.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/ldaseqmodel.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nldaseq.print_topic_times(topic=0) # evolution of 1st topic\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Qatar Living Dataset in Python\nDESCRIPTION: Loads and preprocesses the Qatar Living dataset, removing HTML tags, replacing URLs, and tokenizing the text while removing stopwords.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n%%time\nfrom itertools import chain\nimport json\nfrom re import sub\nfrom os.path import isfile\n\nimport gensim.downloader as api\nfrom gensim.utils import simple_preprocess\nfrom nltk.corpus import stopwords\nfrom nltk import download\n\ndownload(\"stopwords\")  # Download stopwords list.\nstopwords = set(stopwords.words(\"english\"))\n\ndef preprocess(doc):\n    doc = sub(r'<img[^<>]+(>|$)', \" image_token \", doc)\n    doc = sub(r'<[^<>]+(>|$)', \" \", doc)\n    doc = sub(r'\\[img_assist[^]]*?\\]', \" \", doc)\n    doc = sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \" url_token \", doc)\n    return [token for token in simple_preprocess(doc, min_len=0, max_len=float(\"inf\")) if token not in stopwords]\n\ncorpus = list(chain(*[\n    chain(\n        [preprocess(thread[\"RelQuestion\"][\"RelQSubject\"]), preprocess(thread[\"RelQuestion\"][\"RelQBody\"])],\n        [preprocess(relcomment[\"RelCText\"]) for relcomment in thread[\"RelComments\"]])\n    for thread in api.load(\"semeval-2016-2017-task3-subtaskA-unannotated\")]))\n\nprint(\"Number of documents: %d\" % len(corpus))\n```\n\n----------------------------------------\n\nTITLE: Calculating Coherence Scores for Different Topic Counts in Python\nDESCRIPTION: Compares the coherence scores across all trained models and creates a dictionary mapping the number of topics to its corresponding coherence score.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence_model_selection.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\ncoherence_estimates = cm.compare_models(trained_models.values())\ncoherences = dict(zip(trained_models.keys(), coherence_estimates))\n```\n\n----------------------------------------\n\nTITLE: Comparing Doc2Vec Training Methods in Python\nDESCRIPTION: Code comparing the performance of training Doc2Vec using both the traditional documents parameter and the new corpus_file parameter. It measures and compares training time between the two approaches.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Any2Vec_Filebased.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models.doc2vec import Doc2Vec, TaggedLineDocument\nimport time\n\nstart_time = time.time()\nmodel_corp_file = Doc2Vec(corpus_file=CORPUS_FILE, epochs=5, vector_size=300, workers=32)\nfile_time = time.time() - start_time\n\nstart_time = time.time()\nmodel_sent = Doc2Vec(documents=TaggedLineDocument(CORPUS_FILE), epochs=5, vector_size=300, workers=32)\nsent_time = time.time() - start_time\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Training model with `sentences` took {:.3f} seconds\".format(sent_time))\nprint(\"Training model with `corpus_file` took {:.3f} seconds\".format(file_time))\n```\n\n----------------------------------------\n\nTITLE: Defining a Topic Visualization Function in Python\nDESCRIPTION: Creates a helper function to pretty-print topic keywords from the trained EnsembleLda model. The function processes each topic to display its top words in a readable format, removing special characters and reformatting the output.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/ensemble_lda_with_opinosis.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef pretty_print_topics():\n    # note that the words are stemmed so they appear chopped off\n    for t in elda.print_topics(num_words=7):\n        print('-', t[1].replace('*',' ').replace('\"','').replace(' +',','), '\\n')\n```\n\n----------------------------------------\n\nTITLE: Removing Stopwords Using Gensim\nDESCRIPTION: Filters out stopwords from tokenized sentences using gensim's STOPWORDS list. Creates clean versions of the sentences by removing common words like 'to', 'the', 'in'.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_fasttext.rst#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.parsing.preprocessing import STOPWORDS\nsentence_obama = [w for w in sentence_obama if w not in STOPWORDS]\nsentence_president = [w for w in sentence_president if w not in STOPWORDS]\n```\n\n----------------------------------------\n\nTITLE: Comparing Annoy and Traditional Indexer Performance in Python\nDESCRIPTION: Compares the query time of Annoy indexer with traditional indexing method in Gensim, demonstrating the speed improvement offered by Annoy.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_annoy.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Set up the model and vector that we are using in the comparison\nannoy_index = AnnoyIndexer(model, 100)\n\n# Dry run to make sure both indexes are fully in RAM\nnormed_vectors = wv.get_normed_vectors()\nvector = normed_vectors[0]\nwv.most_similar([vector], topn=5, indexer=annoy_index)\nwv.most_similar([vector], topn=5)\n\nimport time\nimport numpy as np\n\ndef avg_query_time(annoy_index=None, queries=1000):\n    \"\"\"Average query time of a most_similar method over 1000 random queries.\"\"\"\n    total_time = 0\n    for _ in range(queries):\n        rand_vec = normed_vectors[np.random.randint(0, len(wv))]\n        start_time = time.process_time()\n        wv.most_similar([rand_vec], topn=5, indexer=annoy_index)\n        total_time += time.process_time() - start_time\n    return total_time / queries\n\nqueries = 1000\n\ngensim_time = avg_query_time(queries=queries)\nannoy_time = avg_query_time(annoy_index, queries=queries)\nprint(\"Gensim (s/query):\\t{0:.5f}\".format(gensim_time))\nprint(\"Annoy (s/query):\\t{0:.5f}\".format(annoy_time))\nspeed_improvement = gensim_time / annoy_time\nprint (\"\\nAnnoy is {0:.2f} times faster on average on this particular run\".format(speed_improvement))\n```\n\n----------------------------------------\n\nTITLE: Chaining LSI Transformation with TF-IDF in Gensim\nDESCRIPTION: Demonstrates how to create an LSI (Latent Semantic Indexing) model on top of the TF-IDF transformed corpus, effectively chaining two transformations.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_topics_and_transformations.rst#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nlsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)  # initialize an LSI transformation\ncorpus_lsi = lsi_model[corpus_tfidf]  # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n```\n\n----------------------------------------\n\nTITLE: Streaming LSI Model Training\nDESCRIPTION: Example of training LSI model with streaming corpus for large datasets that don't fit in RAM. Uses single precision for reduced memory usage.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/CHANGELOG.md#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nmodel = LsiModel(corpus=streaming_corpus, num_topics=500, dtype=np.float32)\n```\n\n----------------------------------------\n\nTITLE: Implementing Model Save and Load Utilities in Python\nDESCRIPTION: Defines utility functions for saving and loading trained LDA models. The save_models function serializes each model with its topic count in the filename, while load_models deserializes them back into an OrderedDict.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence_model_selection.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Some useful utility functions in case you want to save your models.\n\nhome = os.path.expanduser('~/')\nmodels_dir = os.path.join(home, 'workshop', 'nlp', 'models')  # use whatever directory you prefer\n\ndef save_models(named_models):\n    for num_topics, model in named_models.items():\n        model_path = os.path.join(models_dir, 'lda-newsgroups-k%d.lda' % num_topics)\n        model.save(model_path, separately=False)\n\n        \ndef load_models():\n    trained_models = OrderedDict()\n    for num_topics in range(20, 101, 10):\n        model_path = os.path.join(models_dir, 'lda-newsgroups-k%d.lda' % num_topics)\n        print(\"Loading LDA(k=%d) from %s\" % (num_topics, model_path))\n        trained_models[num_topics] = models.LdaMulticore.load(model_path)\n\n    return trained_models\n```\n\n----------------------------------------\n\nTITLE: Building Models for Soft Cosine Similarity in Python\nDESCRIPTION: Creates a dictionary, TF-IDF model, Word2Vec model, and term similarity matrix for the Qatar Living corpus to be used in similarity computations.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n%%time\nfrom multiprocessing import cpu_count\n\nfrom gensim.corpora import Dictionary\nfrom gensim.models import TfidfModel\nfrom gensim.models import Word2Vec\nfrom gensim.similarities import WordEmbeddingSimilarityIndex\nfrom gensim.similarities import SparseTermSimilarityMatrix\n\ndictionary = Dictionary(corpus)\ntfidf = TfidfModel(dictionary=dictionary)\nw2v_model = Word2Vec(corpus, workers=cpu_count(), min_count=5, size=300, seed=12345)\nsimilarity_index = WordEmbeddingSimilarityIndex(w2v_model.wv)\nsimilarity_matrix = SparseTermSimilarityMatrix(similarity_index, dictionary, tfidf, nonzero_limit=100)\n```\n\n----------------------------------------\n\nTITLE: Comparing Document Similarities with Doc2Vec in Python\nDESCRIPTION: This code selects a random document and model, then compares it with the most similar, median similar, and least similar documents. It demonstrates how Doc2Vec captures document relationships.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_46\n\nLANGUAGE: python\nCODE:\n```\nimport random\n\ndoc_id = np.random.randint(len(simple_models[0].dv))  # pick random doc, re-run cell for more examples\nmodel = random.choice(simple_models)  # and a random model\nsims = model.dv.most_similar(doc_id, topn=len(model.dv))  # get *all* similar documents\nprint(f'TARGET ({doc_id}): «{\" \".join(alldocs[doc_id].words)}»\\n')\nprint(f'SIMILAR/DISSIMILAR DOCS PER MODEL {model}%s:\\n')\nfor label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n    s = sims[index]\n    i = sims[index][0]\n    words = ' '.join(alldocs[i].words)\n    print(f'{label} {s}: «{words}»\\n')\n```\n\n----------------------------------------\n\nTITLE: Saving and loading Nmslib index to/from disk\nDESCRIPTION: Demonstrates how to persist a Nmslib index to disk and load it back, which can save time by avoiding index reconstruction.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfname = '/tmp/mymodel.index'\n\n# Persist index to disk\nnmslib_index.save(fname)\n\n# Load index back\nif os.path.exists(fname):\n    nmslib_index2 = NmslibIndexer.load(fname)\n    nmslib_index2.model = model\n```\n\n----------------------------------------\n\nTITLE: Optimized FastText Model Implementation\nDESCRIPTION: Shows the new native Python/Cython implementation of FastText, which replaces the previous Facebook C++ wrapper\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/CHANGELOG.md#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport gensim.downloader as api\nfrom gensim.models import FastText\n\nmodel = FastText(api.load(\"text8\"))\nmodel.most_similar(\"cat\")\n```\n\n----------------------------------------\n\nTITLE: Doc2Vec Vector Classification Experiment\nDESCRIPTION: Loads trained Doc2Vec model and evaluates classification performance on IMDB dataset by splitting 50k documents into training and test sets with balanced positive/negative labels.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/translation_matrix.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nbasedir = \"/home/robotcator/doc2vec\"\n\nmodel2 = Doc2Vec.load(os.path.join(basedir, \"large_doc_50000_iter50.bin\"))\nm2 = []\nfor i in range(len(large_corpus)):\n    m2.append(model2.docvecs[large_corpus[i].tags])\n\ntrain_array = np.zeros((25000, 100))\ntrain_label = np.zeros((25000, 1))\ntest_array = np.zeros((25000, 100))\ntest_label = np.zeros((25000, 1))\n\nfor i in range(12500):\n    train_array[i] = m2[i]\n    train_label[i] = 1\n    train_array[i + 12500] = m2[i + 12500]\n    train_label[i + 12500] = 0\n    test_array[i] = m2[i + 25000]\n    test_label[i] = 1\n    test_array[i + 12500] = m2[i + 37500]\n    test_label[i + 12500] = 0\n\nprint(\"The vectors are learned by doc2vec method\")\ntest_classifier_error(train_array, train_label, test_array, test_label)\n```\n\n----------------------------------------\n\nTITLE: Setting up Coherence Models\nDESCRIPTION: Initializes coherence models using u_mass metric for both good and bad LDA models to compare their performance.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence_tutorial.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ngoodcm = CoherenceModel(model=goodLdaModel, corpus=corpus, dictionary=dictionary, coherence='u_mass')\nbadcm = CoherenceModel(model=badLdaModel, corpus=corpus, dictionary=dictionary, coherence='u_mass')\n```\n\n----------------------------------------\n\nTITLE: Training a Dynamic Topic Model using LdaSeqModel in Gensim\nDESCRIPTION: Creates and trains an LdaSeqModel using the loaded corpus, dictionary, and time slices. Sets the number of topics to 5.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/ldaseqmodel.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nldaseq = ldaseqmodel.LdaSeqModel(corpus=corpus, id2word=dictionary, time_slice=time_slice, num_topics=5)\n```\n\n----------------------------------------\n\nTITLE: Creating Nmslib indexer and comparing similarity search results\nDESCRIPTION: Builds a Nmslib indexer with specific parameters and compares the results of similarity searches for the word 'science' with and without the indexer.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Building nmslib indexer\nnmslib_index = NmslibIndexer(model, {'M': 100, 'indexThreadQty': 1, 'efConstruction': 100}, {'efSearch': 10})\n# Derive the vector for the word \"science\" in our model\nvector = model[\"science\"]\n# The instance of AnnoyIndexer we just created is passed \napproximate_neighbors = model.most_similar([vector], topn=11, indexer=nmslib_index)\n\n# Neatly print the approximate_neighbors and their corresponding cosine similarity values\nprint(\"Approximate Neighbors\")\nfor neighbor in approximate_neighbors:\n    print(neighbor)\n\nnormal_neighbors = model.most_similar([vector], topn=11)\nprint(\"\\nNormal (not nmslib-indexed) Neighbors\")\nfor neighbor in normal_neighbors:\n    print(neighbor)\n```\n\n----------------------------------------\n\nTITLE: Loading Poincare Embeddings from Different Implementations\nDESCRIPTION: This set of functions loads Poincare embeddings trained using different implementations (C++, NumPy, and Gensim). It includes methods to transform embeddings to KeyedVector format and a convenience function to load models based on the implementation.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Evaluation.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef transform_cpp_embedding_to_kv(input_file, output_file, encoding='utf8'):\n    \"\"\"Given a C++ embedding tsv filepath, converts it to a KeyedVector-supported file\"\"\"\n    with smart_open(input_file, 'rb') as f:\n        lines = [line.decode(encoding) for line in f]\n    if not len(lines):\n         raise ValueError(\"file is empty\")\n    first_line = lines[0]\n    parts = first_line.rstrip().split(\"\\t\")\n    model_size = len(parts) - 1\n    vocab_size = len(lines)\n    with smart_open(output_file, 'w') as f:\n        f.write('%d %d\\n' % (vocab_size, model_size))\n        for line in lines:\n            f.write(line.replace('\\t', ' '))\n\ndef transform_numpy_embedding_to_kv(input_file, output_file, encoding='utf8'):\n    \"\"\"Given a numpy poincare embedding pkl filepath, converts it to a KeyedVector-supported file\"\"\"\n    np_embeddings = pickle.load(open(input_file, 'rb'))\n    random_embedding = np_embeddings[list(np_embeddings.keys())[0]]\n    \n    model_size = random_embedding.shape[0]\n    vocab_size = len(np_embeddings)\n    with smart_open(output_file, 'w') as f:\n        f.write('%d %d\\n' % (vocab_size, model_size))\n        for key, vector in np_embeddings.items():\n            vector_string = ' '.join('%.6f' % value for value in vector)\n            f.write('%s %s\\n' % (key, vector_string))\n\ndef load_poincare_cpp(input_filename):\n    \"\"\"Load embedding trained via C++ Poincare model.\n\n    Parameters\n    ----------\n    filepath : str\n        Path to tsv file containing embedding.\n\n    Returns\n    -------\n    PoincareKeyedVectors instance.\n\n    \"\"\"\n    keyed_vectors_filename = input_filename + '.kv'\n    transform_cpp_embedding_to_kv(input_filename, keyed_vectors_filename)\n    embedding = PoincareKeyedVectors.load_word2vec_format(keyed_vectors_filename)\n    os.unlink(keyed_vectors_filename)\n    return embedding\n\ndef load_poincare_numpy(input_filename):\n    \"\"\"Load embedding trained via Python numpy Poincare model.\n\n    Parameters\n    ----------\n    filepath : str\n        Path to pkl file containing embedding.\n\n    Returns:\n        PoincareKeyedVectors instance.\n\n    \"\"\"\n    keyed_vectors_filename = input_filename + '.kv'\n    transform_numpy_embedding_to_kv(input_filename, keyed_vectors_filename)\n    embedding = PoincareKeyedVectors.load_word2vec_format(keyed_vectors_filename)\n    os.unlink(keyed_vectors_filename)\n    return embedding\n\ndef load_poincare_gensim(input_filename):\n    \"\"\"Load embedding trained via Gensim PoincareModel.\n\n    Parameters\n    ----------\n    filepath : str\n        Path to model file.\n\n    Returns:\n        PoincareKeyedVectors instance.\n\n    \"\"\"\n    model = PoincareModel.load(input_filename)\n    return model.kv\n\ndef load_model(implementation, model_file):\n    \"\"\"Convenience function over functions to load models from different implementations.\n    \n    Parameters\n    ----------\n    implementation : str\n        Implementation used to create model file ('c++'/'numpy'/'gensim').\n    model_file : str\n        Path to model file.\n    \n    Returns\n    -------\n    PoincareKeyedVectors instance\n    \n    Notes\n    -----\n    Raises ValueError in case of invalid value for `implementation`\n\n    \"\"\"\n    if implementation == 'c++':\n        return load_poincare_cpp(model_file)\n    elif implementation == 'numpy':\n        return load_poincare_numpy(model_file)\n    elif implementation == 'gensim':\n        return load_poincare_gensim(model_file)\n    else:\n        raise ValueError('Invalid implementation %s' % implementation)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Text Corpus in Python\nDESCRIPTION: Preprocesses the text corpus by removing stopwords, lowercasing, tokenizing, and filtering out infrequent words.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_core_concepts.rst#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Create a set of frequent words\nstoplist = set('for a of the and to in'.split(' '))\n# Lowercase each document, split it by white space and filter out stopwords\ntexts = [[word for word in document.lower().split() if word not in stoplist]\n         for document in text_corpus]\n\n# Count word frequencies\nfrom collections import defaultdict\nfrequency = defaultdict(int)\nfor text in texts:\n    for token in text:\n        frequency[token] += 1\n\n# Only keep words that appear more than once\nprocessed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]\npprint.pprint(processed_corpus)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Word Analogies with Doc2Vec Models in Python\nDESCRIPTION: This code snippet evaluates the quality of word vectors from Doc2Vec models trained on IMDB data using word analogy tasks. It loads a pre-defined questions file and tests each model's performance on various analogy categories.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_48\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.test.utils import datapath\nquestions_filename = datapath('questions-words.txt')\n\n# Note: this analysis takes many minutes\nfor model in word_models:\n    score, sections = model.wv.evaluate_word_analogies(questions_filename)\n    correct, incorrect = len(sections[-1]['correct']), len(sections[-1]['incorrect'])\n    print(f'{model}: {float(correct*100)/(correct+incorrect):0.2f}%% correct ({correct} of {correct+incorrect}')\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Wikipedia Articles for Doc2Vec Training in Python\nDESCRIPTION: This code preprocesses Wikipedia articles from an XML dump, tokenizing and normalizing the text. It outputs the processed articles to a compressed text file for efficient storage.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwiki = WikiCorpus(\n    \"enwiki-latest-pages-articles.xml.bz2\",  # path to the file you downloaded above\n    tokenizer_func=tokenize,  # simple regexp; plug in your own tokenizer here\n    metadata=True,  # also return the article titles and ids when parsing\n    dictionary={},  # don't start processing the data yet\n)\n\nwith smart_open.open(\"wiki.txt.gz\", \"w\", encoding='utf8') as fout:\n    for article_no, (content, (page_id, title)) in enumerate(wiki.get_texts()):\n        title = ' '.join(title.split())\n        if article_no % 500000 == 0:\n            logging.info(\"processing article #%i: %r (%i tokens)\", article_no, title, len(content))\n        fout.write(f\"{title}\\t{' '.join(content)}\\n\")  # title_of_article [TAB] words of the article\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing Corpus Data in Python\nDESCRIPTION: This code loads the Google News word embeddings and preprocesses a sample of Wikipedia articles. It creates a dictionary from the word embeddings and processes the corpus, saving the results for future use. The code includes error handling for loading existing data or creating new data as needed.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_benchmark.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfull_model = api.load(\"word2vec-google-news-300\")\n\ntry:\n    with open(\"matrix_speed.corpus\", \"rb\") as file:\n        full_corpus = pickle.load(file)        \nexcept IOError:\n    original_corpus = list(tqdm(api.load(\"wiki-english-20171001\"), desc=\"original_corpus\", total=4924894))\n    seed(RANDOM_SEED)\n    full_corpus = [\n        simple_preprocess(u'\\n'.join(article[\"section_texts\"]))\n        for article in tqdm(sample(original_corpus, 10**5), desc=\"full_corpus\", total=10**5)]\n    del original_corpus\n    with open(\"matrix_speed.corpus\", \"wb\") as file:\n        pickle.dump(full_corpus, file)\n\ntry:\n    full_dictionary = Dictionary.load(\"matrix_speed.dictionary\")\nexcept IOError:\n    full_dictionary = Dictionary([[term] for term in full_model.vocab.keys()])\n    full_dictionary.save(\"matrix_speed.dictionary\")\n```\n\n----------------------------------------\n\nTITLE: Data Preprocessing and Corpus Download\nDESCRIPTION: Downloads and preprocesses the Brown corpus and Text8 dataset for word embedding training. Includes text cleaning steps like punctuation removal and whitespace normalization.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Wordrank_comparisons.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport nltk\nfrom smart_open import smart_open\nfrom gensim.parsing.preprocessing import strip_punctuation, strip_multiple_whitespaces\n\n# Only the brown corpus is needed in case you don't have it.\nnltk.download('brown') \n\n# Generate brown corpus text file\nwith smart_open('brown_corp.txt', 'w+') as f:\n    for word in nltk.corpus.brown.words():\n        f.write('{word} '.format(word=word))\n    f.seek(0)\n    brown = f.read()\n\n# Preprocess brown corpus\nwith smart_open('proc_brown_corp.txt', 'w') as f:\n    proc_brown = strip_punctuation(brown)\n    proc_brown = strip_multiple_whitespaces(proc_brown).lower()\n    f.write(proc_brown)\n\n# Set WR_HOME and FT_HOME to respective directory root\nWR_HOME = 'wordrank/'\nFT_HOME = 'fastText/'\n\n# download the text8 corpus (a 100 MB sample of preprocessed wikipedia text)\nimport os.path\nif not os.path.isfile('text8'):\n    !wget -c https://mattmahoney.net/dc/text8.zip\n    !unzip text8.zip\n```\n\n----------------------------------------\n\nTITLE: Text Preprocessing with SpaCy NLP\nDESCRIPTION: Processes documents using SpaCy's NLP pipeline, performing tokenization, lemmatization, stopword removal, and named entity recognition. Multi-word named entities are retained for enriched document representation.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n%%time\nprocessed_docs = []    \nfor doc in nlp.pipe(docs, n_threads=4, batch_size=100):\n    # Process document using Spacy NLP pipeline.\n    \n    ents = doc.ents  # Named entities.\n\n    # Keep only words (no numbers, no punctuation).\n    # Lemmatize tokens, remove punctuation and remove stopwords.\n    doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n\n    # Remove common words from a stopword list.\n    #doc = [token for token in doc if token not in STOPWORDS]\n\n    # Add named entities, but only if they are a compound of more than word.\n    doc.extend([str(entity) for entity in ents if len(entity) > 1])\n    \n    processed_docs.append(doc)\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Query\nDESCRIPTION: Executes a similarity query against the indexed corpus and prints the results.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_similarity_queries.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsims = index[vec_lsi]  # perform a similarity query against the corpus\nprint(list(enumerate(sims)))  # print (document_number, document_similarity) 2-tuples\n```\n\n----------------------------------------\n\nTITLE: Performing Latent Dirichlet Allocation on Wikipedia\nDESCRIPTION: Code for creating an LDA model using the online algorithm. This snippet initializes a Latent Dirichlet Allocation model with 100 topics, using a single pass over the corpus and updating the model after every 10,000 documents. It then prints 20 randomly selected topics with their most contributing words.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/wiki.rst#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> # extract 100 LDA topics, using 1 pass and updating once every 1 chunk (10,000 documents)\n>>> lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=100, update_every=1, passes=1)\nusing serial LDA version on this node\nrunning online LDA training, 100 topics, 1 passes over the supplied corpus of 3931787 documents, updating model once every 10000 documents\n...\n\n>>> # print the most contributing words for 20 randomly selected topics\n>>> lda.print_topics(20)\ntopic #0: 0.009*river + 0.008*lake + 0.006*island + 0.005*mountain + 0.004*area + 0.004*park + 0.004*antarctic + 0.004*south + 0.004*mountains + 0.004*dam\ntopic #1: 0.026*relay + 0.026*athletics + 0.025*metres + 0.023*freestyle + 0.022*hurdles + 0.020*ret + 0.017*divisão + 0.017*athletes + 0.016*bundesliga + 0.014*medals\ntopic #2: 0.002*were + 0.002*he + 0.002*court + 0.002*his + 0.002*had + 0.002*law + 0.002*government + 0.002*police + 0.002*patrolling + 0.002*their\ntopic #3: 0.040*courcelles + 0.035*centimeters + 0.023*mattythewhite + 0.021*wine + 0.019*stamps + 0.018*oko + 0.017*perennial + 0.014*stubs + 0.012*ovate + 0.011*greyish\ntopic #4: 0.039*al + 0.029*sysop + 0.019*iran + 0.015*pakistan + 0.014*ali + 0.013*arab + 0.010*islamic + 0.010*arabic + 0.010*saudi + 0.010*muhammad\ntopic #5: 0.020*copyrighted + 0.020*northamerica + 0.014*uncopyrighted + 0.007*rihanna + 0.005*cloudz + 0.005*knowles + 0.004*gaga + 0.004*zombie + 0.004*wigan + 0.003*maccabi\ntopic #6: 0.061*israel + 0.056*israeli + 0.030*sockpuppet + 0.025*jerusalem + 0.025*tel + 0.023*aviv + 0.022*palestinian + 0.019*ifk + 0.016*palestine + 0.014*hebrew\ntopic #7: 0.015*melbourne + 0.014*rovers + 0.013*vfl + 0.012*australian + 0.012*wanderers + 0.011*afl + 0.008*dinamo + 0.008*queensland + 0.008*tracklist + 0.008*brisbane\ntopic #8: 0.011*film + 0.007*her + 0.007*she + 0.004*he + 0.004*series + 0.004*his + 0.004*episode + 0.003*films + 0.003*television + 0.003*best\ntopic #9: 0.019*wrestling + 0.013*château + 0.013*ligue + 0.012*discus + 0.012*estonian + 0.009*uci + 0.008*hockeyarchives + 0.008*wwe + 0.008*estonia + 0.007*reign\ntopic #10: 0.078*edits + 0.059*notability + 0.035*archived + 0.025*clearer + 0.022*speedy + 0.021*deleted + 0.016*hook + 0.015*checkuser + 0.014*ron + 0.011*nominator\ntopic #11: 0.013*admins + 0.009*acid + 0.009*molniya + 0.009*chemical + 0.007*ch + 0.007*chemistry + 0.007*compound + 0.007*anemone + 0.006*mg + 0.006*reaction\n```\n\n----------------------------------------\n\nTITLE: Finding Optimal Slope for Pivoted Document Length Normalization\nDESCRIPTION: Tests different slope values (0 to 1 in 0.1 increments) for pivoted document length normalization to find the optimal value that maximizes model accuracy.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/pivoted_document_length_normalisation.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nbest_model_accuracy = 0\noptimum_slope = 0\nfor slope in np.arange(0, 1.1, 0.1):\n    params = {\"pivot\": 10, \"slope\": slope}\n\n    model_accuracy, doc_scores = get_tfidf_scores(params)\n\n    if model_accuracy > best_model_accuracy:\n        best_model_accuracy = model_accuracy\n        optimum_slope = slope\n\n    print(\"Score for slope {} is {}\".format(slope, model_accuracy))\n\nprint(\"We get best score of {} at slope {}\".format(best_model_accuracy, optimum_slope))\n```\n\n----------------------------------------\n\nTITLE: Evaluating Author-Topic Model Performance in Python using Gensim\nDESCRIPTION: This code computes the per-word bound, a measure of the model's predictive performance, for the trained author-topic model.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models import atmodel\ndoc2author = atmodel.construct_doc2author(model.corpus, model.author2doc)\n\n# Compute the per-word bound.\n# Number of words in corpus.\ncorpus_words = sum(cnt for document in model.corpus for _, cnt in document)\n\n# Compute bound and divide by number of words.\nperwordbound = model.bound(model.corpus, author2doc=model.author2doc, \\\n                           doc2author=model.doc2author) / corpus_words\nprint(perwordbound)\n```\n\n----------------------------------------\n\nTITLE: Word2Vec Model Training and Frequency Analysis - Text8 Corpus\nDESCRIPTION: Similar analysis performed on the larger text8 corpus (17M tokens). Compares performance of Word2Vec, WordRank and FastText models across different word frequencies and analogy types.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Wordrank_comparisons.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# a sample model using gensim's Word2Vec for getting vocab counts\ncorpus = Text8Corpus('text8')\nmodel = Word2Vec(min_count=5)\nmodel.build_vocab(corpus)\nfreq = {}\nfor word in model.wv.index2word:\n    freq[word] = model.wv.vocab[word].count\n        \nword2vec = compute_accuracies('text8_gs.vec', freq)\nwordrank = compute_accuracies('text8_wr.vec', freq)\nfasttext = compute_accuracies('text8_ft.vec', freq)\n\nfig = plt.figure(figsize=(7,15))\n\nfor i, subplot, title in zip([0, 1, 2], ['311', '312', '313'], ['Semantic Analogies', 'Syntactic Analogies', 'Total Analogy']):\n    ax = fig.add_subplot(subplot)\n    ax.plot(word2vec[i][0], word2vec[i][1], 'r-', label='Word2Vec')\n    ax.plot(wordrank[i][0], wordrank[i][1], 'g--', label='WordRank')\n    ax.plot(fasttext[i][0], fasttext[i][1], 'b:', label='FastText')\n    ax.set_ylabel('Average accuracy')\n    ax.set_xlabel('Log mean frequency')\n    ax.set_title(title)\n    ax.legend(loc='upper right', prop={'size':10})\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Evaluating Lexical Entailment Models\nDESCRIPTION: This snippet evaluates models trained for lexical entailment using the HyperLex dataset, calculating the Spearman correlation score between predicted and actual scores. The results are stored separately for each model evaluated.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Evaluation.ipynb#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nentailment_results = OrderedDict()\neval_instance = LexicalEntailmentEvaluation(hyperlex_file)\nfor implementation, models in sorted(model_files.items()):\n    for model_name, files in models.items():\n        if model_name in entailment_results:\n            continue\n        entailment_results[model_name] = OrderedDict()\n        entailment_results[model_name]['spearman'] = {}\n        for model_size, model_file in files.items():\n            print('Evaluating model %s of size %d' % (model_name, model_size))\n            embedding = load_model(implementation, model_file)\n            entailment_results[model_name]['spearman'][model_size] = eval_instance.evaluate_spearman(embedding)\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading FastText Models\nDESCRIPTION: Demonstrates how to save and load FastText models for both implementations. Shows the process for both Gensim native and wrapper implementations.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# saving a model trained via Gensim's fastText implementation\nmodel_gensim.save('saved_model_gensim')\nloaded_model = FT_gensim.load('saved_model_gensim')\nprint(loaded_model)\n\n# saving a model trained via fastText wrapper\nmodel_wrapper.save('saved_model_wrapper')\nloaded_model = FT_wrapper.load('saved_model_wrapper')\nprint(loaded_model)\n```\n\n----------------------------------------\n\nTITLE: Training Translation Matrix\nDESCRIPTION: Creates and trains the translation matrix using source and target word vectors and word pairs.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/translation_matrix.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntransmat = translation_matrix.TranslationMatrix(source_word_vec, target_word_vec, word_pair)\ntransmat.train(word_pair)\nprint(\"the shape of translation matrix is: \", transmat.translation_matrix.shape)\n```\n\n----------------------------------------\n\nTITLE: Creating Dictionary from Processed Corpus in Gensim\nDESCRIPTION: Uses Gensim's Dictionary class to create a vocabulary from the processed corpus, associating each word with a unique integer ID.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_core_concepts.rst#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim import corpora\n\ndictionary = corpora.Dictionary(processed_corpus)\nprint(dictionary)\n```\n\n----------------------------------------\n\nTITLE: Computing Topic Coherence with Gensim in Python\nDESCRIPTION: This Python snippet computes the average topic coherence for topics generated by Gensim's LDA model using the 'Umass' measure. It requires a trained LDA model and a corpus. The key inputs are the corpus and the number of topics, while the output is the average topic coherence value. Limitations include dependency on the quality and coherence of the model's topics.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_lda.rst#2025-04-21_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\ntop_topics = model.top_topics(corpus)\n\n    # Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n    avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n    print('Average topic coherence: %.4f.' % avg_topic_coherence)\n\n    from pprint import pprint\n    pprint(top_topics)\n\n```\n\n----------------------------------------\n\nTITLE: Training Multiple Author-Topic Models for Comparison in Python\nDESCRIPTION: This code trains multiple author-topic models with different random initializations to find the model with the highest topic coherence.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n%%time\nmodel_list = []\nfor i in range(5):\n    model = AuthorTopicModel(corpus=corpus, num_topics=10, id2word=dictionary.id2token, \\\n                    author2doc=author2doc, chunksize=2000, passes=100, gamma_threshold=1e-10, \\\n                    eval_every=0, iterations=1, random_state=i)\n    top_topics = model.top_topics(corpus)\n    tc = sum([t[1] for t in top_topics])\n    model_list.append((model, tc))\n```\n\n----------------------------------------\n\nTITLE: Persisting and Loading Annoy Indices in Python\nDESCRIPTION: Shows how to save Annoy indices to disk and load them back, allowing for reuse of pre-built indices across sessions.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_annoy.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfname = '/tmp/mymodel.index'\n\n# Persist index to disk\nannoy_index.save(fname)\n\n# Load index back\nimport os.path\nif os.path.exists(fname):\n    annoy_index2 = AnnoyIndexer()\n    annoy_index2.load(fname)\n    annoy_index2.model = model\n\n# Results should be identical to above\nvector = wv[\"science\"]\napproximate_neighbors2 = wv.most_similar([vector], topn=11, indexer=annoy_index2)\nfor neighbor in approximate_neighbors2:\n    print(neighbor)\n\nassert approximate_neighbors == approximate_neighbors2\n```\n\n----------------------------------------\n\nTITLE: Loading VarEmbed Model with Morphological Information\nDESCRIPTION: Code to load a pre-trained VarEmbed model into Gensim with morphological information by providing both word vectors and a trained Morfessor model binary file.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Varembed.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmorfessor_file = '../../gensim/test/test_data/varembed_leecorpus_morfessor.bin'\nmodel_with_morphemes = varembed.VarEmbed.load_varembed_format(vectors=vector_file, morfessor_model=morfessor_file)\n```\n\n----------------------------------------\n\nTITLE: Text Preprocessing with NLTK\nDESCRIPTION: Removing stopwords from sentences using NLTK's stopwords corpus\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_scm.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Import and download stopwords from NLTK.\nfrom nltk.corpus import stopwords\nfrom nltk import download\ndownload('stopwords')  # Download stopwords list.\nstop_words = stopwords.words('english')\n\ndef preprocess(sentence):\n    return [w for w in sentence.lower().split() if w not in stop_words]\n\nsentence_obama = preprocess(sentence_obama)\nsentence_president = preprocess(sentence_president)\nsentence_orange = preprocess(sentence_orange)\n```\n\n----------------------------------------\n\nTITLE: Training LDA Models with Varying Topic Counts in Python\nDESCRIPTION: Trains LDA models on the 20 Newsgroups corpus with different numbers of topics (from 20 to 100 in steps of 10) using Gensim's LdaMulticore. Each model is configured with asymmetric alpha, multiple passes, and parallel processing.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence_model_selection.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\ntrained_models = OrderedDict()\nfor num_topics in range(20, 101, 10):\n    print(\"Training LDA(k=%d)\" % num_topics)\n    lda = models.LdaMulticore(\n        mm_corpus, id2word=dictionary, num_topics=num_topics, workers=4,\n        passes=10, iterations=100, random_state=42, eval_every=None,\n        alpha='asymmetric',  # shown to be better than symmetric in most cases\n        decay=0.5, offset=64  # best params from Hoffman paper\n    )\n    trained_models[num_topics] = lda\n```\n\n----------------------------------------\n\nTITLE: Evaluating TF-IDF with Optimal Pivoted Normalization\nDESCRIPTION: Applies TF-IDF with the optimal slope value found earlier and prints the model accuracy to demonstrate the improvement.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/pivoted_document_length_normalisation.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nparams = {\"pivot\": 10, \"slope\": optimum_slope}\nmodel_accuracy, doc_scores = get_tfidf_scores(params)\nprint(model_accuracy)\n```\n\n----------------------------------------\n\nTITLE: Using HDP Model with Scikit-Learn Pipeline in Python\nDESCRIPTION: This example demonstrates how to use the HDP model with Scikit-Learn's Pipeline for text classification. It combines HDP and logistic regression in a pipeline.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/sklearn_api.ipynb#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nmodel = HdpTransformer(id2word=id2word)\nclf = linear_model.LogisticRegression(penalty='l2', C=0.1)\ntext_hdp = Pipeline([('features', model,), ('classifier', clf)])\ntext_hdp.fit(corpus, data.target)\nscore = text_hdp.score(corpus, data.target)\n\nprint(score)\n```\n\n----------------------------------------\n\nTITLE: Initializing Similarity Index\nDESCRIPTION: Creates a similarity index for the corpus in LSI space to enable efficient similarity queries.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_similarity_queries.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim import similarities\nindex = similarities.MatrixSimilarity(lsi[corpus])  # transform corpus to LSI space and index it\n```\n\n----------------------------------------\n\nTITLE: Evaluating Models for Link Prediction\nDESCRIPTION: This snippet evaluates the trained models based on various metrics such as mean rank and MAP. It iterates through the available models, loads each model, and performs evaluation using a defined instance of LinkPredictionEvaluation. The results of the evaluation are captured in an ordered dictionary.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Evaluation.ipynb#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nlp_results = OrderedDict()\nmetrics = ['mean_rank', 'MAP']\nfor implementation, models in sorted(lp_model_files.items()):\n    for model_name, files in models.items():\n        lp_results[model_name] = OrderedDict()\n        for metric in metrics:\n            lp_results[model_name][metric] = {}\n        for model_size, model_file in files.items():\n            print('Evaluating model %s of size %d' % (model_name, model_size))\n            embedding = load_model(implementation, model_file)\n            eval_instance = LinkPredictionEvaluation(wordnet_train_file, wordnet_test_file, embedding)\n            eval_result = eval_instance.evaluate(max_n=1000)\n            for metric in metrics:\n                lp_results[model_name][metric][model_size] = eval_result[metric]\n```\n\n----------------------------------------\n\nTITLE: Creating a custom corpus iterator for Word2Vec in Python\nDESCRIPTION: Defines a custom iterator class 'MyCorpus' that yields preprocessed sentences from a text file for training Word2Vec.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_word2vec.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.test.utils import datapath\nfrom gensim import utils\n\nclass MyCorpus:\n    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n\n    def __iter__(self):\n        corpus_path = datapath('lee_background.cor')\n        for line in open(corpus_path):\n            # assume there's one document per line, tokens separated by whitespace\n            yield utils.simple_preprocess(line)\n```\n\n----------------------------------------\n\nTITLE: TF-IDF Model Training and Document Transformation\nDESCRIPTION: Initializes and trains a TF-IDF model on the corpus, then transforms a sample document using the trained model.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_core_concepts.rst#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim import models\n\n# train the model\ntfidf = models.TfidfModel(bow_corpus)\n\n# transform the \"system minors\" string\nwords = \"system minors\".lower().split()\nprint(tfidf[dictionary.doc2bow(words)])\n```\n\n----------------------------------------\n\nTITLE: Network Visualization\nDESCRIPTION: Creates and displays the interactive network visualization using Plotly\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_network.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfig = Figure(data=Data([edge_trace, node_trace]),\n             layout=Layout(showlegend=False,\n                hovermode='closest',\n                xaxis=XAxis(showgrid=True, zeroline=False, showticklabels=True),\n                yaxis=YAxis(showgrid=True, zeroline=False, showticklabels=True)))\n\npy.iplot(fig)\n```\n\n----------------------------------------\n\nTITLE: Extracting NIPS Documents from URL in Python\nDESCRIPTION: Defines a function to extract NIPS documents from a tarball URL. It uses smart_open and tarfile libraries to download and extract text files from the NIPS corpus.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_lda.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport io\nimport os.path\nimport re\nimport tarfile\n\nimport smart_open\n\ndef extract_documents(url='https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'):\n    with smart_open.open(url, \"rb\") as file:\n        with tarfile.open(fileobj=file) as tar:\n            for member in tar.getmembers():\n                if member.isfile() and re.search(r'nipstxt/nips\\d+/\\d+\\.txt', member.name):\n                    member_bytes = tar.extractfile(member).read()\n                    yield member_bytes.decode('utf-8', errors='replace')\n\ndocs = list(extract_documents())\n```\n\n----------------------------------------\n\nTITLE: Training Batch LDA Model with Gensim in Python\nDESCRIPTION: This code snippet shows how to train a batch Latent Dirichlet Allocation (LDA) model using Gensim. It sets up the model with 100 topics, 20 passes, and no online updates. This approach is suitable for scenarios where the entire training corpus is known beforehand or does not exhibit topic drift.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/wiki.rst#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# extract 100 LDA topics, using 20 full passes, no online updates\nlda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=100, update_every=0, passes=20)\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Queries with Gensim in Python\nDESCRIPTION: This snippet demonstrates how to use Gensim's similarity framework to find similar authors based on their topic distributions. It uses the MatrixSimilarity class to generate a similarity index.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.similarities import MatrixSimilarity\n\n# Generate a similarity object for the transformed corpus.\nindex = MatrixSimilarity(model[list(model.id2author.values())])\n\n# Get similarities to some author.\nauthor_name = 'YannLeCun'\nsims = index[model[author_name]]\n```\n\n----------------------------------------\n\nTITLE: Word2Vec Training Progress Log Output\nDESCRIPTION: Training progress log for a Gensim Word2Vec model showing periodic status updates including completion percentage, words processed per second, and queue sizes across multiple training epochs. Each epoch processes approximately 23 million raw words (22.9 million effective words) taking around 42 seconds per epoch.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_18\n\nLANGUAGE: log\nCODE:\n```\n2023-08-23 12:51:45,524 : INFO : EPOCH 2 - PROGRESS: at 65.14% examples, 547959 words/s, in_qsize 4, out_qsize 0\n2023-08-23 12:51:46,536 : INFO : EPOCH 2 - PROGRESS: at 67.50% examples, 547529 words/s, in_qsize 3, out_qsize 0\n[...]\n2023-08-23 12:53:09,208 : INFO : EPOCH 4 - PROGRESS: at 62.50% examples, 544368 words/s, in_qsize 3, out_qsize 0\n```\n\n----------------------------------------\n\nTITLE: Visualizing Document Length Distribution with Matplotlib in Python\nDESCRIPTION: This snippet creates a histogram of document lengths using Matplotlib. It calculates the average document length and overlays it on the plot. This visualization helps understand the distribution of document sizes in the corpus.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/WMD_tutorial.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n# Document lengths.\nlens = [len(doc) for doc in wmd_corpus]\n\n# Plot.\nplt.rc('figure', figsize=(8,6))\nplt.rc('font', size=14)\nplt.rc('lines', linewidth=2)\nplt.rc('axes', color_cycle=('#377eb8','#e41a1c','#4daf4a',\n                            '#984ea3','#ff7f00','#ffff33'))\n# Histogram.\nplt.hist(lens, bins=20)\nplt.hold(True)\n# Average length.\navg_len = sum(lens) / float(len(lens))\nplt.axvline(avg_len, color='#e41a1c')\nplt.hold(False)\nplt.title('Histogram of document lengths.')\nplt.xlabel('Length')\nplt.text(100, 800, 'mean = %.2f' % avg_len)\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Verifying loaded Nmslib index produces identical results\nDESCRIPTION: Tests that the loaded Nmslib index produces the same similarity search results as the original index, confirming proper persistence and loading.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Results should be identical to above\nvector = model[\"science\"]\napproximate_neighbors2 = model.most_similar([vector], topn=11, indexer=nmslib_index2)\nfor neighbor in approximate_neighbors2:\n    print(neighbor)\n    \nassert approximate_neighbors == approximate_neighbors2\n```\n\n----------------------------------------\n\nTITLE: Initializing Multiple Doc2Vec Models in Python\nDESCRIPTION: Sets up three different Doc2Vec models: PV-DBOW, PV-DM with averaging, and PV-DM with concatenation. Uses common parameters including vector size of 100, 20 epochs, and minimum word count of 2. Initializes vocabulary for each model using a corpus.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport multiprocessing\nfrom collections import OrderedDict\n\nimport gensim.models.doc2vec\nassert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n\nfrom gensim.models.doc2vec import Doc2Vec\n\ncommon_kwargs = dict(\n    vector_size=100, epochs=20, min_count=2,\n    sample=0, workers=multiprocessing.cpu_count(), negative=5, hs=0,\n)\n\nsimple_models = [\n    # PV-DBOW plain\n    Doc2Vec(dm=0, **common_kwargs),\n    # PV-DM w/ default averaging; a higher starting alpha may improve CBOW/PV-DM modes\n    Doc2Vec(dm=1, window=10, alpha=0.05, comment='alpha=0.05', **common_kwargs),\n    # PV-DM w/ concatenation - big, slow, experimental mode\n    # window=5 (both sides) approximates paper's apparent 10-word total window size\n    Doc2Vec(dm=1, dm_concat=1, window=5, **common_kwargs),\n]\n\nfor model in simple_models:\n    model.build_vocab(alldocs)\n    print(f\"{model} vocabulary scanned & state initialized\")\n\nmodels_by_name = OrderedDict((str(model), model) for model in simple_models)\n```\n\n----------------------------------------\n\nTITLE: Training a WordRank Model in Python using Gensim\nDESCRIPTION: This snippet demonstrates how to train a WordRank model using Gensim's wrapper. It specifies the path to WordRank, output directory, and sample corpus. The training parameters iter and dump_period need to be synchronized as WordRank dumps the embedding file at the start of each iteration.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/WordRank_wrapper_quickstart.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models.wrappers import Wordrank\n\nwr_path = 'wordrank' # path to Wordrank directory\nout_dir = 'model' # name of output directory to save data to\ndata = '../../gensim/test/test_data/lee.cor' # sample corpus\n\nmodel = Wordrank.train(wr_path, data, out_dir, iter=11, dump_period=5)\n```\n\n----------------------------------------\n\nTITLE: Creating Word Vector Scatter Plot with Plotly\nDESCRIPTION: Creates an interactive scatter plot showing word vectors for English and Italian words using Plotly. Includes text labels and positioning for word pairs.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/translation_matrix.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntrace1 = Scatter(\n    x = new_en_words_vec[:, 0],\n    y = new_en_words_vec[:, 1],\n    mode = 'markers+text',\n    text = en_words,\n    textposition = 'top'\n)\ntrace2 = Scatter(\n    x = new_it_words_vec[:, 0],\n    y = new_it_words_vec[:, 1],\n    mode = 'markers+text',\n    text = it_words,\n    textposition = 'top'\n)\nlayout = Layout(\n    showlegend = False,\n    annotations = [dict(\n        x = new_it_words_vec[5][0],\n        y = new_it_words_vec[5][1],\n        text = translated_word[en_words[4]][0],\n        arrowcolor = \"black\",\n        arrowsize = 1.5,\n        arrowwidth = 1,\n        arrowhead = 0.5\n      ), dict(\n        x = new_it_words_vec[6][0],\n        y = new_it_words_vec[6][1],\n        text = translated_word[en_words[4]][1],\n        arrowcolor = \"black\",\n        arrowsize = 1.5,\n        arrowwidth = 1,\n        arrowhead = 0.5\n      ), dict(\n        x = new_it_words_vec[7][0],\n        y = new_it_words_vec[7][1],\n        text = translated_word[en_words[4]][2],\n        arrowcolor = \"black\",\n        arrowsize = 1.5,\n        arrowwidth = 1,\n        arrowhead = 0.5\n      )]\n)\ndata = [trace1, trace2]\n\nfig = Figure(data=data, layout=layout)\nplot_url = plotly.offline.iplot(fig, filename='relatie_position_for_numbers.html')\n```\n\n----------------------------------------\n\nTITLE: Retrieving vocabulary from Word2Vec model in Python\nDESCRIPTION: Iterates through the vocabulary of a Word2Vec model, printing the first 10 words as an example.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_word2vec.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfor index, word in enumerate(wv.index_to_key):\n    if index == 10:\n        break\n    print(f\"word #{index}/{len(wv.index_to_key)} is {word}\")\n```\n\n----------------------------------------\n\nTITLE: Calculating Prediction Accuracy with Precision@k Metric\nDESCRIPTION: Function to measure prediction accuracy using the precision at k principle. Evaluates how often the correct author appears in the top k predictions.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_prediction_tutorial.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef prediction_accuracy(test_author2doc, test_corpus, model, k=5):\n\n    print(\"Precision@k: top_n={}\".format(k))\n    matches=0\n    tries = 0\n    for author in test_author2doc:\n        author_id = model.author2id[author]\n        for doc_id in test_author2doc[author]:\n            predicted_authors = predict_author(test_corpus[doc_id:doc_id+1], atmodel=model, top_n=k)\n            tries = tries+1\n            if author_id in predicted_authors[\"Author\"]:\n                matches=matches+1\n\n    accuracy = matches/tries\n    print(\"Prediction accuracy: {}\".format(accuracy))\n    return accuracy, k\n```\n\n----------------------------------------\n\nTITLE: Analyzing Document-Topic Proportions in Python using Gensim\nDESCRIPTION: This snippet demonstrates how to check the topic distribution for a specific document in the corpus using the LdaSeq model's doc_topics method.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/ldaseqmodel.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndoc = ldaseq.doc_topics(558) # check the 558th document in the corpuses topic distribution\nprint (doc)\n```\n\n----------------------------------------\n\nTITLE: Displaying Evaluation Results for Poincaré Embeddings in Python\nDESCRIPTION: Functions for displaying evaluation results in a tabular format and extracting model descriptions from model names. The display_results function creates an HTML table showing metrics across different model dimensions, while model_description_from_name parses model names to extract implementation type and parameters.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Evaluation.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef display_results(task_name, results):\n    \"\"\"Display evaluation results of multiple embeddings on a single task in a tabular format\n    \n    Args:\n        task_name (str): name the task being evaluated\n        results (dict): mapping between embeddings and corresponding results\n    \n    \"\"\"\n    result_table = PrettyTable()\n    result_table.field_names = [\"Model Description\", \"Metric\"] + [str(dim) for dim in sorted(model_sizes)]\n    for model_name, model_results in results.items():\n        metrics = [metric for metric in model_results.keys()]\n        dims = sorted([dim for dim in model_results[metrics[0]].keys()])\n        description = model_description_from_name(model_name)\n        row = [description, '\\n'.join(metrics) + '\\n']\n        for dim in dims:\n            scores = ['%.2f' % model_results[metric][dim] for metric in metrics]\n            row.append('\\n'.join(scores))\n        result_table.add_row(row)\n    result_table.align = 'r'\n    result_html = result_table.get_html_string()\n    search = \"<table>\"\n    insert_at = result_html.index(search) + len(search)\n    new_row = \"\"\"\n        <tr>\n            <th colspan=\"1\" style=\"text-align:left\">%s</th>\n            <th colspan=\"1\"></th>\n            <th colspan=\"%d\" style=\"text-align:center\"> Dimensions</th>\n        </tr>\"\"\" % (task_name, len(model_sizes))\n    result_html = result_html[:insert_at] + new_row + result_html[insert_at:]\n    display(HTML(result_html))\n    \ndef model_description_from_name(model_name):\n    if model_name.startswith('gensim'):\n        implementation = 'Gensim'\n    elif model_name.startswith('cpp'):\n        implementation = 'C++'\n    elif model_name.startswith('np'):\n        implementation = 'Numpy'\n    else:\n        raise ValueError('Unsupported implementation for model: %s' % model_name)\n    description = []\n    for param_key in sorted(default_params.keys()):\n        pattern = '%s_([^_]*)_?' % param_key\n        match = re.search(pattern, model_name)\n        if match:\n            description.append(\"%s=%s\" % (param_key, match.groups()[0]))\n    return \"%s: %s\" % (implementation, \", \".join(description))\n```\n\n----------------------------------------\n\nTITLE: Loading Text8 Corpus with Gensim Downloader API in Python\nDESCRIPTION: Downloads and loads the text8 corpus as a Python object supporting streamed access.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_downloader_api.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncorpus = api.load('text8')\n```\n\n----------------------------------------\n\nTITLE: Creating and Saving NMSLIB Index from KeyedVectors in Python\nDESCRIPTION: This code creates an NMSLIB index from a loaded KeyedVectors object and saves it to a file.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nnmslib_index = NmslibIndexer(wv, \n                            {'M': 100, 'indexThreadQty': 1, 'efConstruction': 100}, {'efSearch': 100})\nnmslib_index.save('/tmp/mymodel.index')\n```\n\n----------------------------------------\n\nTITLE: Creating a Tagged Wikipedia Corpus for Doc2Vec Training in Python\nDESCRIPTION: This snippet defines a TaggedWikiCorpus class that streams preprocessed Wikipedia articles from a file, converting them into the format required by Doc2Vec for training.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass TaggedWikiCorpus:\n    def __init__(self, wiki_text_path):\n        self.wiki_text_path = wiki_text_path\n        \n    def __iter__(self):\n        for line in smart_open.open(self.wiki_text_path, encoding='utf8'):\n            title, words = line.split('\\t')\n            yield TaggedDocument(words=words.split(), tags=[title])\n\ndocuments = TaggedWikiCorpus('wiki.txt.gz')  # A streamed iterable; nothing in RAM yet.\n```\n\n----------------------------------------\n\nTITLE: Memory-Mapped Annoy Indices for Multi-Process Usage in Python\nDESCRIPTION: Demonstrates how to use memory-mapping with Annoy indices to efficiently share the index across multiple processes, reducing overall memory usage.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_annoy.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Remove verbosity from code below (if logging active)\nif LOGS:\n    logging.disable(logging.CRITICAL)\n\nfrom multiprocessing import Process\nimport os\nimport psutil\n\nmodel.save('/tmp/mymodel.pkl')\n\ndef f(process_id):\n    print('Process Id: {}'.format(os.getpid()))\n    process = psutil.Process(os.getpid())\n    new_model = Word2Vec.load('/tmp/mymodel.pkl')\n    vector = new_model.wv[\"science\"]\n    annoy_index = AnnoyIndexer()\n    annoy_index.load('/tmp/mymodel.index')\n    annoy_index.model = new_model\n    approximate_neighbors = new_model.wv.most_similar([vector], topn=5, indexer=annoy_index)\n    print('\\nMemory used by process {}: {}\\n---'.format(os.getpid(), process.memory_info()))\n\n\n\n```\n\n----------------------------------------\n\nTITLE: Loading Pre-trained Google News Word Vectors\nDESCRIPTION: Loads pre-trained word vectors from Google News dataset and initializes a coherence model using these vectors.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence_model_selection.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\nmodels_dir = os.path.join(home, 'workshop', 'nlp', 'models')\nvectors_path = os.path.join(models_dir, 'GoogleNews-vectors-negative300.bin.gz')\nkeyed_vectors = models.KeyedVectors.load_word2vec_format(vectors_path, binary=True)\n\n# still need to estimate_probabilities, but corpus is not scanned\ncm = models.CoherenceModel.for_models(\n    trained_models.values(), dictionary, texts=corpus.get_texts(),\n    coherence='c_w2v', keyed_vectors=keyed_vectors)\n```\n\n----------------------------------------\n\nTITLE: Serializing the Corpus in Matrix Market Format in Python\nDESCRIPTION: Serializes the preprocessed corpus to disk in Matrix Market format and loads it back for LDA training. This allows reusing the same corpus without repeating preprocessing steps.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence_model_selection.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\nmm_path = '20_newsgroups.mm'\nMmCorpus.serialize(mm_path, corpus, id2word=dictionary)\nmm_corpus = MmCorpus(mm_path)  # load back in to use for LDA training\n```\n\n----------------------------------------\n\nTITLE: Training C++ Poincare Embeddings\nDESCRIPTION: Defines functions to train Poincare embeddings using the C++ implementation. It sets up parameters for training models of different sizes and with various hyperparameters.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Evaluation.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef train_cpp_model(\n    binary_path, data_file, output_file, dim, epochs, neg,\n    num_threads, epsilon, burn_in, seed=0):\n    \"\"\"Train a poincare embedding using the c++ implementation\n    \n    Args:\n        binary_path (str): Path to the compiled c++ implementation binary\n        data_file (str): Path to tsv file containing relation pairs\n        output_file (str): Path to output file containing model\n        dim (int): Number of dimensions of the trained model\n        epochs (int): Number of epochs to use\n        neg (int): Number of negative samples to use\n        num_threads (int): Number of threads to use for training the model\n        epsilon (float): Constant used for clipping below a norm of one\n        burn_in (int): Number of epochs to use for burn-in init (0 means no burn-in)\n    \n    Notes: \n        If `output_file` already exists, skips training\n    \"\"\"\n    if os.path.exists(output_file):\n        print('File %s exists, skipping' % output_file)\n        return\n    args = {\n        'dim': dim,\n        'max_epoch': epochs,\n        'neg_size': neg,\n        'num_thread': num_threads,\n        'epsilon': epsilon,\n        'burn_in': burn_in,\n        'learning_rate_init': 0.1,\n        'learning_rate_final': 0.0001,\n    }\n    cmd = [binary_path, data_file, output_file]\n    for option, value in args.items():\n        cmd.append(\"--%s\" % option)\n        cmd.append(str(value))\n    \n    return check_output(args=cmd)\n\nmodel_sizes = [5, 10, 20, 50, 100, 200]\ndefault_params = {\n    'neg': 20,\n    'epochs': 50,\n    'threads': 8,\n    'eps': 1e-6,\n    'burn_in': 0,\n    'batch_size': 10,\n    'reg': 0.0\n}\n\nnon_default_params = {\n    'neg': [10],\n    'epochs': [200],\n    'burn_in': [10]\n}\n\ndef cpp_model_name_from_params(params, prefix):\n    param_keys = ['burn_in', 'epochs', 'neg', 'eps', 'threads']\n    name = ['%s_%s' % (key, params[key]) for key in sorted(param_keys)]\n    return '%s_%s' % (prefix, '_'.join(name))\n\ndef train_model_with_params(params, train_file, model_sizes, prefix, implementation):\n    \"\"\"Trains models with given params for multiple model sizes using the given implementation\n    \n    Args:\n        params (dict): parameters to train the model with\n        train_file (str): Path to tsv file containing relation pairs\n        model_sizes (list): list of dimension sizes (integer) to train the model with\n        prefix (str): prefix to use for the saved model filenames\n        implementation (str): whether to use the numpy or c++ implementation,\n                              allowed values: 'numpy', 'c++'\n   \n   Returns:\n        tuple (model_name, model_files)\n        model_files is a dict of (size, filename) pairs\n        Example: ('cpp_model_epochs_50', {5: 'models/cpp_model_epochs_50_dim_5'})\n    \"\"\"\n    files = {}\n    if implementation == 'c++':\n        model_name = cpp_model_name_from_params(params, prefix)\n    elif implementation == 'numpy':\n        model_name = np_model_name_from_params(params, prefix)\n    elif implementation == 'gensim':\n        model_name = gensim_model_name_from_params(params, prefix)\n    else:\n        raise ValueError('Given implementation %s not found' % implementation)\n    for model_size in model_sizes:\n        output_file_name = '%s_dim_%d' % (model_name, model_size)\n        output_file = os.path.join(models_directory, output_file_name)\n        print('Training model %s of size %d' % (model_name, model_size))\n        if implementation == 'c++':\n            out = train_cpp_model(\n                cpp_binary_path, train_file, output_file, model_size,\n                params['epochs'], params['neg'], params['threads'],\n                params['eps'], params['burn_in'], seed=0)\n        elif implementation == 'numpy':\n            train_external_numpy_model(\n                python_script_path, train_file, output_file, model_size,\n                params['epochs'], params['neg'], seed=0)\n        elif implementation == 'gensim':\n            train_gensim_model(\n                train_file, output_file, model_size, params['epochs'],\n                params['neg'], params['burn_in'], params['batch_size'], params['reg'], seed=0)\n        else:\n            raise ValueError('Given implementation %s not found' % implementation)\n        files[model_size] = output_file\n    return (model_name, files)\n\nmodel_files = {}\n\nmodel_files['c++'] = {}\n# Train c++ models with default params\nmodel_name, files = train_model_with_params(default_params, wordnet_file, model_sizes, 'cpp_model', 'c++')\nmodel_files['c++'][model_name] = {}\nfor dim, filepath in files.items():\n    model_files['c++'][model_name][dim] = filepath\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Inner Product Between Two Corpora in Python\nDESCRIPTION: This function benchmarks the speed of the inner_product method for computing term similarities between entire corpora. It measures execution time and various corpus and matrix properties for different configurations.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_benchmark.ipynb#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef benchmark(configuration):\n    (matrix, dictionary, nonzero_limit), corpus, normalized, repetition = configuration\n    corpus_size = len(corpus)\n    corpus = [dictionary.doc2bow(doc) for doc in corpus]\n    corpus = [vec for vec in corpus if len(vec) > 0]\n    \n    start_time = time()\n    matrix.inner_product(corpus, corpus, normalized=normalized)\n    end_time = time()\n    duration = end_time - start_time\n    \n    return {\n        \"dictionary_size\": matrix.matrix.shape[0],\n        \"matrix_nonzero\": matrix.matrix.nnz,\n        \"nonzero_limit\": nonzero_limit,\n        \"normalized\": normalized,\n        \"corpus_size\": corpus_size,\n        \"corpus_actual_size\": len(corpus),\n        \"corpus_nonzero\": sum(len(vec) for vec in corpus),\n        \"mean_document_length\": np.mean([len(doc) for doc in corpus]),\n        \"repetition\": repetition,\n        \"duration\": duration, }\n```\n\n----------------------------------------\n\nTITLE: Loading Wikipedia Corpus and Dictionary for LSA\nDESCRIPTION: Code to load the preprocessed Wikipedia corpus and dictionary. This is the initial setup required before performing Latent Semantic Analysis on the Wikipedia data. The snippet configures logging, loads word mappings from a text file, and initializes the corpus from a Matrix Market file.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/wiki.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> import logging\n>>> import gensim\n>>>\n>>> logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n>>>\n>>> # load id->word mapping (the dictionary), one of the results of step 2 above\n>>> id2word = gensim.corpora.Dictionary.load_from_text('wiki_en_wordids.txt')\n>>> # load corpus iterator\n>>> mm = gensim.corpora.MmCorpus('wiki_en_tfidf.mm')\n>>> # mm = gensim.corpora.MmCorpus('wiki_en_tfidf.mm.bz2') # use this if you compressed the TFIDF output (recommended)\n>>>\n>>> print(mm)\nMmCorpus(3931787 documents, 100000 features, 756379027 non-zero entries)\n```\n\n----------------------------------------\n\nTITLE: Comparing Topics Between Two Different LDA Models\nDESCRIPTION: Visualizes the correlation between topics from two different LDA models using Jaccard distance, with annotations showing the intersection and difference in terms of words for each topic pair.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_compare_lda.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmdiff, annotation = lda_fst.diff(lda_snd, distance='jaccard', num_words=50)\nplot_difference(mdiff, title=\"Topic difference (two models)[jaccard distance]\", annotation=annotation)\n```\n\n----------------------------------------\n\nTITLE: Initializing LDA Models\nDESCRIPTION: Creates two LDA models with different training iterations to demonstrate coherence differences between well-trained and poorly-trained models.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence_tutorial.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ngoodLdaModel = LdaModel(corpus=corpus, id2word=dictionary, iterations=50, num_topics=2)\nbadLdaModel = LdaModel(corpus=corpus, id2word=dictionary, iterations=1, num_topics=2)\n```\n\n----------------------------------------\n\nTITLE: Creating Evaluation Graphs for NMSLIB Performance in Python\nDESCRIPTION: This function creates multiple subplots to visualize the relationship between various parameters and performance metrics for NMSLIB indexer.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef create_evaluation_graph(x_values, y_values_init, y_values_accuracy, y_values_query, param_name):\n    plt.figure(1, figsize=(12, 6))\n    plt.subplot(231)\n    plt.plot(x_values, y_values_init)\n    plt.title(\"{} vs initalization time\".format(param_name))\n    plt.ylabel(\"Initialization time (s)\")\n    plt.xlabel(param_name)\n    plt.subplot(232)\n    plt.plot(x_values, y_values_accuracy)\n    plt.title(\"{} vs accuracy\".format(param_name))\n    plt.ylabel(\"% accuracy\")\n    plt.xlabel(param_name)\n    plt.tight_layout()\n    plt.subplot(233)\n    plt.plot(y_values_init, y_values_accuracy)\n    plt.title(\"Initialization time vs accuracy\")\n    plt.ylabel(\"% accuracy\")\n    plt.xlabel(\"Initialization time (s)\")\n    plt.tight_layout()\n    plt.subplot(234)\n    plt.plot(x_values, y_values_query)\n    plt.title(\"{} vs query time\".format(param_name))\n    plt.ylabel(\"query time\")\n    plt.xlabel(param_name)\n    plt.tight_layout()\n    plt.subplot(235)\n    plt.plot(y_values_query, y_values_accuracy)\n    plt.title(\"query time vs accuracy\")\n    plt.ylabel(\"% accuracy\")\n    plt.xlabel(\"query time (s)\")\n    plt.tight_layout()\n    plt.show()\n```\n\n----------------------------------------\n\nTITLE: Initializing LDA Model\nDESCRIPTION: Sets up the LDA model with 2 topics and asymmetric alpha parameter\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_methods.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nnumpy.random.seed(1) # setting random seed to get the same results each time.\nmodel = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=2, alpha='asymmetric', minimum_probability=1e-8)\n```\n\n----------------------------------------\n\nTITLE: Word Embedding Models Training Configuration and Implementation\nDESCRIPTION: Implements training function for Word2Vec, FastText, and WordRank models with configurable parameters. Includes model saving and ensemble embedding generation for WordRank.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Wordrank_comparisons.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nMODELS_DIR = 'models/'\n!mkdir -p {MODELS_DIR}\n\nfrom gensim.models import Word2Vec\nfrom gensim.models.wrappers import Wordrank\nfrom gensim.models.word2vec import Text8Corpus\n\n# fasttext params\nlr = 0.05\ndim = 100\nws = 5\nepoch = 5\nminCount = 5\nneg = 5\nloss = 'ns'\nt = 1e-4\n\nw2v_params = {\n    'alpha': 0.025,\n    'size': 100,\n    'window': 15,\n    'iter': 5,\n    'min_count': 5,\n    'sample': t,\n    'sg': 1,\n    'hs': 0,\n    'negative': 5\n}\n\nwr_params = {\n    'size': 100,\n    'window': 15,\n    'iter': 91,\n    'min_count': 5\n}\n\ndef train_models(corpus_file, output_name):\n    # Train using word2vec\n    output_file = '{:s}_gs'.format(output_name)\n    if not os.path.isfile(os.path.join(MODELS_DIR, '{:s}.vec'.format(output_file))):\n        print('\\nTraining word2vec on {:s} corpus..'.format(corpus_file))\n        # Text8Corpus class for reading space-separated words file\n        %time gs_model = Word2Vec(Text8Corpus(corpus_file), **w2v_params); gs_model\n        locals()['gs_model'].save_word2vec_format(os.path.join(MODELS_DIR, '{:s}.vec'.format(output_file)))\n        print('\\nSaved gensim  model as {:s}.vec'.format(output_file))\n    else:\n        print('\\nUsing existing model file {:s}.vec'.format(output_file))\n\n    # Train using fasttext\n    output_file = '{:s}_ft'.format(output_name)\n    if not os.path.isfile(os.path.join(MODELS_DIR, '{:s}.vec'.format(output_file))):\n        print('Training fasttext on {:s} corpus..'.format(corpus_file))\n        %time !{FT_HOME}fasttext skipgram -input {corpus_file} -output {MODELS_DIR+output_file}  -lr {lr} -dim {dim} -ws {ws} -epoch {epoch} -minCount {minCount} -neg {neg} -loss {loss} -t {t}\n    else:\n        print('\\nUsing existing model file {:s}.vec'.format(output_file))\n        \n    # Train using wordrank\n    output_file = '{:s}_wr'.format(output_name)\n    output_dir = 'model' # directory to save embeddings and metadata to\n    if not os.path.isfile(os.path.join(MODELS_DIR, '{:s}.vec'.format(output_file))):\n        print('\\nTraining wordrank on {:s} corpus..'.format(corpus_file))\n        %time wr_model = Wordrank.train(WR_HOME, corpus_file, output_dir, **wr_params); wr_model\n        locals()['wr_model'].save_word2vec_format(os.path.join(MODELS_DIR, '{:s}.vec'.format(output_file)))\n        print('\\nSaved wordrank model as {:s}.vec'.format(output_file))\n    else:\n        print('\\nUsing existing model file {:s}.vec'.format(output_file))\n     \n    # Loading ensemble embeddings\n    output_file = '{:s}_wr_ensemble'.format(output_name)\n    if not os.path.isfile(os.path.join(MODELS_DIR, '{:s}.vec'.format(output_file))):\n        print('\\nLoading ensemble embeddings (vector combination of word and context embeddings)..')\n        %time wr_model = Wordrank.load_wordrank_model(os.path.join(WR_HOME, 'model/wordrank.words'), os.path.join(WR_HOME, 'model/meta/vocab.txt'), os.path.join(WR_HOME, 'model/wordrank.contexts'), sorted_vocab=1, ensemble=1); wr_model\n        locals()['wr_model'].wv.save_word2vec_format(os.path.join(MODELS_DIR, '{:s}.vec'.format(output_file)))\n        print('\\nSaved wordrank (ensemble) model as {:s}.vec'.format(output_file))\n    else:\n        print('\\nUsing existing model file {:s}.vec'.format(output_file))\n```\n\n----------------------------------------\n\nTITLE: Preparing WordNet and HyperLex Data for Poincare Embeddings\nDESCRIPTION: Creates directories for storing datasets and models, prepares WordNet noun hypernyms data, and downloads and extracts HyperLex dataset for evaluation.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Evaluation.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# These directories are auto created in the current directory for storing poincare datasets and models\ndata_directory = os.path.join(parent_directory, 'data')\nmodels_directory = os.path.join(parent_directory, 'models')\n\n# Create directories\n! mkdir -p {data_directory}\n! mkdir -p {models_directory}\n\n# Prepare the WordNet data\n# Can also be downloaded directly from -\n# https://github.com/jayantj/gensim/raw/wordnet_data/docs/notebooks/poincare/data/wordnet_noun_hypernyms.tsv\n\nwordnet_file = os.path.join(data_directory, 'wordnet_noun_hypernyms.tsv')\nif not os.path.exists(wordnet_file):\n    ! python {parent_directory}/{cpp_repo_name}/scripts/create_wordnet_noun_hierarchy.py {wordnet_file}\n\n# Prepare the HyperLex data\nhyperlex_url = \"http://people.ds.cam.ac.uk/iv250/paper/hyperlex/hyperlex-data.zip\"\n! wget {hyperlex_url} -O {data_directory}/hyperlex-data.zip\nif os.path.exists(os.path.join(data_directory, 'hyperlex')):\n    ! rm -r {data_directory}/hyperlex\n! unzip {data_directory}/hyperlex-data.zip -d {data_directory}/hyperlex/\nhyperlex_file = os.path.join(data_directory, 'hyperlex', 'nouns-verbs', 'hyperlex-nouns.txt')\n```\n\n----------------------------------------\n\nTITLE: Creating Dictionary and Corpus\nDESCRIPTION: Creates a Dictionary object and filters extreme words, then converts texts to bag-of-words format\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_methods.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndictionary = Dictionary(texts)\ndictionary.filter_extremes(no_above=0.1, no_below=10)\ncorpus = [dictionary.doc2bow(text) for text in texts]\n```\n\n----------------------------------------\n\nTITLE: Displaying WMD Similarity Query Results in Python\nDESCRIPTION: This snippet prints the query and the most similar documents along with their similarity scores. It helps in analyzing the effectiveness of the WMD similarity search by showing the actual text of similar documents.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/WMD_tutorial.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Print the query and the retrieved documents, together with their similarities.\nprint 'Query:'\nprint sent\nfor i in range(num_best):\n    print\n    print 'sim = %.4f' % sims[i][1]\n    print documents[sims[i][0]]\n```\n\n----------------------------------------\n\nTITLE: LSI Model Persistence in Python with Gensim\nDESCRIPTION: Demonstrates saving and loading an LSI model to/from disk using temporary files. Shows how to persist trained models for later use.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_topics_and_transformations.rst#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport tempfile\n\nwith tempfile.NamedTemporaryFile(prefix='model-', suffix='.lsi', delete=False) as tmp:\n    lsi_model.save(tmp.name)  # same for tfidf, lda, ...\n\nloaded_lsi_model = models.LsiModel.load(tmp.name)\n\nos.unlink(tmp.name)\n```\n\n----------------------------------------\n\nTITLE: Loading Pre-trained Word Embeddings with Gensim Downloader\nDESCRIPTION: Shows how to load pre-trained word embeddings and perform similarity queries using the new download API\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/CHANGELOG.md#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport gensim.downloader as api\n\nmodel = api.load(\"glove-twitter-25\")\nmodel.most_similar(\"engineer\")\n```\n\n----------------------------------------\n\nTITLE: Plotting Accuracy Results\nDESCRIPTION: Function to visualize prediction accuracy results using matplotlib. Supports comparing multiple model configurations with customizable labels and formatting.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_prediction_tutorial.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef plot_accuracy(scores1, label1, scores2=None, label2=None):\n    \n    import matplotlib.pyplot as plt\n    s = [score*100 for score in scores1.values()]\n    t = list(scores1.keys())\n\n    plt.plot(t, s, \"b-\", label=label1)\n    plt.plot(t, s, \"r^\", label=label1+\" data points\")\n    \n    if scores2 is not None:\n        s2 = [score*100 for score in scores2.values()]\n        plt.plot(t, s2, label=label2)\n        plt.plot(t, s2, \"o\", label=label2+\" data points\")\n        \n    plt.legend(loc=\"lower right\")\n\n    plt.xlabel('parameter k')\n    plt.ylabel('prediction accuracy')\n    plt.title('Precision at k')\n    plt.xticks(t)\n    plt.grid(True)\n    plt.yticks([30,40,50,60,70,80,90,100])\n    plt.axis([0, 11, 30, 100])\n    plt.show()\n```\n\n----------------------------------------\n\nTITLE: Creating Author-to-Document Mapping\nDESCRIPTION: Constructs a dictionary mapping author names to their document IDs by parsing the index files from the NIPS dataset. The document IDs are converted to integer indices for easier processing.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom smart_open import smart_open\nfilenames = [data_dir + 'idx/a' + yr + '.txt' for yr in yrs]  # Using the years defined in previous cell.\n\n# Get all author names and their corresponding document IDs.\nauthor2doc = dict()\ni = 0\nfor yr in yrs:\n    # The files \"a00.txt\" and so on contain the author-document mappings.\n    filename = data_dir + 'idx/a' + yr + '.txt'\n    for line in smart_open(filename, errors='ignore', encoding='utf-8', 'rb'):\n        # Each line corresponds to one author.\n        contents = re.split(',', line)\n        author_name = (contents[1] + contents[0]).strip()\n        # Remove any whitespace to reduce redundant author names.\n        author_name = re.sub('\\s', '', author_name)\n        # Get document IDs for author.\n        ids = [c.strip() for c in contents[2:]]\n        if not author2doc.get(author_name):\n            # This is a new author.\n            author2doc[author_name] = []\n            i += 1\n        \n        # Add document IDs to author.\n        author2doc[author_name].extend([yr + '_' + id for id in ids])\n\n# Use an integer ID in author2doc, instead of the IDs provided in the NIPS dataset.\n# Mapping from ID of document in NIPS datast, to an integer ID.\ndoc_id_dict = dict(zip(doc_ids, range(len(doc_ids))))\n# Replace NIPS IDs by integer IDs.\nfor a, a_doc_ids in author2doc.items():\n    for i, doc_id in enumerate(a_doc_ids):\n        author2doc[a][i] = doc_id_dict[doc_id]\n```\n\n----------------------------------------\n\nTITLE: Extracting Topics from DTM Model in Python\nDESCRIPTION: Retrieves and prints the top words for a specific topic and time slice from the trained DTM model.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/dtm_example.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntopics = model.show_topic(topicid=1, time=1, num_words=10)\n\ntopics\n```\n\n----------------------------------------\n\nTITLE: Computing Bigrams with Gensim Phrases\nDESCRIPTION: Uses Gensim's Phrases model to identify and add frequently occurring bigrams (word pairs) to the documents. Only bigrams appearing 20 or more times are included.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Compute bigrams.\nfrom gensim.models import Phrases\n# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\nbigram = Phrases(docs, min_count=20)\nfor idx in range(len(docs)):\n    for token in bigram[docs[idx]]:\n        if '_' in token:\n            # Token is a bigram, add to document.\n            docs[idx].append(token)\n```\n\n----------------------------------------\n\nTITLE: Creating a Word2Vec Model from a Corpus\nDESCRIPTION: Imports the Word2Vec class and creates a model from the previously loaded corpus. This trains a new word vector model on the text8 dataset.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/downloader_api_tutorial.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models.word2vec import Word2Vec\n\nmodel = Word2Vec(corpus)\n```\n\n----------------------------------------\n\nTITLE: Building a Memory-Efficient Dictionary\nDESCRIPTION: Creates a dictionary from a stream of documents without loading all texts into memory, removes stopwords and infrequent words, and compacts the dictionary.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_corpora_and_vector_spaces.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# collect statistics about all tokens\ndictionary = corpora.Dictionary(line.lower().split() for line in open('https://radimrehurek.com/mycorpus.txt'))\n# remove stop words and words that appear only once\nstop_ids = [\n    dictionary.token2id[stopword]\n    for stopword in stoplist\n    if stopword in dictionary.token2id\n]\nonce_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1]\ndictionary.filter_tokens(stop_ids + once_ids)  # remove stop words and words that appear only once\ndictionary.compactify()  # remove gaps in id sequence after words that were removed\nprint(dictionary)\n```\n\n----------------------------------------\n\nTITLE: Saving Trained Doc2Vec Models in Python\nDESCRIPTION: This code shows how to save the trained Doc2Vec models to disk, allowing for later use without retraining. It saves both the DBOW and DM models.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmodel_dbow.save('doc2vec_dbow.model')\nmodel_dm.save('doc2vec_dm.model')\n```\n\n----------------------------------------\n\nTITLE: Loading Ensemble WordRank Embeddings in Python\nDESCRIPTION: This snippet demonstrates how to load both word and context embeddings from WordRank to create an ensemble embedding. It requires both the word embedding file and context embedding file, with the ensemble parameter set to 1.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/WordRank_wrapper_quickstart.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwr_context_file = 'wordrank.contexts'\nmodel = Wordrank.load_wordrank_model(wr_word_embedding, vocab_file, wr_context_file, sorted_vocab=1, ensemble=1)\n```\n\n----------------------------------------\n\nTITLE: Processing and Saving Wikipedia Corpus for 2Vec Training in Python\nDESCRIPTION: Code to prepare a Wikipedia dataset for training by preprocessing the text and saving it in line-sentence format. It loads the Wikipedia dump, preprocesses each article, and streams the result to disk.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Any2Vec_Filebased.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nCORPUS_FILE = 'wiki-en-20171001.txt'\n```\n\nLANGUAGE: python\nCODE:\n```\nimport itertools\nfrom gensim.parsing.preprocessing import preprocess_string\n\ndef processed_corpus():\n    raw_corpus = api.load('wiki-english-20171001')\n    for article in raw_corpus:\n        # concatenate all section titles and texts of each Wikipedia article into a single \"sentence\"\n        doc = '\\n'.join(itertools.chain.from_iterable(zip(article['section_titles'], article['section_texts'])))\n        yield preprocess_string(doc)\n\n# serialize the preprocessed corpus into a single file on disk, using memory-efficient streaming\nsave_as_line_sentence(processed_corpus(), CORPUS_FILE)\n```\n\n----------------------------------------\n\nTITLE: Word Similarity Operations\nDESCRIPTION: Demonstrates various similarity operations including most similar words, word pairs similarity, and doesn't match functionality.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(\"nights\" in model_wrapper.wv.vocab)\nprint(\"night\" in model_wrapper.wv.vocab)\nmodel_wrapper.similarity(\"night\", \"nights\")\n```\n\n----------------------------------------\n\nTITLE: Processing Training Data for Author-Topic Model\nDESCRIPTION: Preprocesses the training dataset, creates a corpus and dictionary with specified frequency parameters (max_freq=0.5, min_wordcount=20) for topic modeling.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_prediction_tutorial.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntraindata_dir = \"/tmp/C50train\"\ntrain_docs, train_author2doc = preprocess_docs(traindata_dir)\ntrain_corpus_50_20, train_dictionary_50_20 = create_corpus_dictionary(train_docs, 0.5, 20)\n```\n\n----------------------------------------\n\nTITLE: Printing topics for a specific time slice in Dynamic Topic Model\nDESCRIPTION: Demonstrates how to print the topics for the first time slice (index 0) of the trained DTM model.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/ldaseqmodel.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nldaseq.print_topics(time=0)\n```\n\n----------------------------------------\n\nTITLE: Exporting Word2Vec Model to Binary Format in Python\nDESCRIPTION: This code exports the Word2Vec model to a binary file in word2vec format.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nmodel.wv.save_word2vec_format('/tmp/vectors.bin', binary=True)\n```\n\n----------------------------------------\n\nTITLE: Comparing Document Similarities using Hellinger Distance in Python\nDESCRIPTION: This snippet demonstrates how to compare the topic distributions of two documents using the Hellinger distance metric in Gensim.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/ldaseqmodel.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndoc_football_2 = ['arsenal', 'fourth', 'wenger', 'oil', 'middle', 'east', 'sanction', 'fluctuation']\ndoc_football_2 = dictionary.doc2bow(doc_football_2)\ndoc_football_2 = ldaseq[doc_football_2]\n\nhellinger(doc_football_1, doc_football_2)\n```\n\n----------------------------------------\n\nTITLE: Evaluating NMSLIB Performance with Different Parameters in Python\nDESCRIPTION: These snippets evaluate NMSLIB indexer performance by varying the parameters M, efConstruction, and efSearch.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nevaluate_nmslib_performance(\"M\", False, 50, 401, 50)\n```\n\nLANGUAGE: python\nCODE:\n```\nevaluate_nmslib_performance(\"efConstruction\", False, 50, 1001, 100)\n```\n\nLANGUAGE: python\nCODE:\n```\nevaluate_nmslib_performance(\"efSearch\", True, 50, 401, 100)\n```\n\n----------------------------------------\n\nTITLE: Loading corpus and dictionary for Dynamic Topic Modeling\nDESCRIPTION: Loads a pre-processed corpus and dictionary from disk, and defines the time slices for the DTM model.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/ldaseqmodel.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    dictionary = Dictionary.load('datasets/news_dictionary')\nexcept FileNotFoundError as e:\n    raise ValueError(\"SKIP: Please download the Corpus/news_dictionary dataset.\")\ncorpus = bleicorpus.BleiCorpus('datasets/news_corpus')\n\ntime_slice = [438, 430, 456]\n```\n\n----------------------------------------\n\nTITLE: Finding Similar Documents Using Trained Doc2Vec Models in Python\nDESCRIPTION: This code demonstrates how to use the trained Doc2Vec models to find similar Wikipedia articles. It shows examples for 'Machine learning' and 'Lady Gaga'.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfor model in [model_dbow, model_dm]:\n    print(model)\n    pprint(model.dv.most_similar(positive=[\"Machine learning\"], topn=20))\n```\n\nLANGUAGE: python\nCODE:\n```\nfor model in [model_dbow, model_dm]:\n    print(model)\n    pprint(model.dv.most_similar(positive=[\"Lady Gaga\"], topn=10))\n```\n\n----------------------------------------\n\nTITLE: Creating Train-Test Split for WordNet Link Prediction in Python\nDESCRIPTION: Function that splits a data file into training and testing sets for link prediction evaluation. It ensures that root and leaf nodes are not included in the test set and that all nodes in the dataset appear in the training set to allow for proper evaluation of generalization ability.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Evaluation.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef train_test_split(data_file, test_ratio=0.1):\n    \"\"\"Creates train and test files from given data file, returns train/test file names\n    \n    Args:\n        data_file (str): path to data file for which train/test split is to be created\n        test_ratio (float): fraction of lines to be used for test data\n    \n    Returns\n        (train_file, test_file): tuple of strings with train file and test file paths\n    \"\"\"\n    train_filename = data_file + '.train'\n    test_filename = data_file + '.test'\n    if os.path.exists(train_filename) and os.path.exists(test_filename):\n        print('Train and test files already exist, skipping')\n        return (train_filename, test_filename)\n    root_nodes, leaf_nodes = get_root_and_leaf_nodes(data_file)\n    test_line_candidates = []\n    line_count = 0\n    all_nodes = set()\n    with smart_open(data_file, 'rb') as f:\n        for i, line in enumerate(f):\n            node_1, node_2 = line.split()\n            all_nodes.update([node_1, node_2])\n            if (\n                    node_1 not in leaf_nodes\n                    and node_2 not in leaf_nodes\n                    and node_1 not in root_nodes\n                    and node_2 not in root_nodes\n                    and node_1 != node_2\n                ):\n                test_line_candidates.append(i)\n            line_count += 1\n\n    num_test_lines = int(test_ratio * line_count)\n    if num_test_lines > len(test_line_candidates):\n        raise ValueError('Not enough candidate relations for test set')\n    print('Choosing %d test lines from %d candidates' % (num_test_lines, len(test_line_candidates)))\n    test_line_indices = set(random.sample(test_line_candidates, num_test_lines))\n    train_line_indices = set(l for l in range(line_count) if l not in test_line_indices)\n    \n    train_set_nodes = set()\n    with smart_open(data_file, 'rb') as f:\n        train_file = smart_open(train_filename, 'wb')\n        test_file = smart_open(test_filename, 'wb')\n        for i, line in enumerate(f):\n            if i in train_line_indices:\n                train_set_nodes.update(line.split())\n                train_file.write(line)\n            elif i in test_line_indices:\n                test_file.write(line)\n            else:\n                raise AssertionError('Line %d not present in either train or test line indices' % i)\n        train_file.close()\n        test_file.close()\n    assert len(train_set_nodes) == len(all_nodes), 'Not all nodes from dataset present in train set relations'\n    return (train_filename, test_filename)\n```\n\n----------------------------------------\n\nTITLE: Training TfidfModel with SMART Notation in Python\nDESCRIPTION: Demonstrates how to use the new SMART notation support in TfidfModel to specify different weighting and normalization schemes. The example loads data, creates a dictionary and corpus, then trains a TfidfModel using the 'ntc' SMART scheme.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/CHANGELOG.md#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.corpora import Dictionary\nfrom gensim.models import TfidfModel\nimport gensim.downloader as api\n\ndata = api.load(\"text8\")\ndct = Dictionary(data)\ncorpus = [dct.doc2bow(line) for line in data]\n\n# Train Tfidf model using the SMART notation, smartirs=\"ntc\" where\n# 'n' - natural term frequency\n# 't' - idf document frequency\n# 'c' - cosine normalization\n#\n# More information about possible values available in documentation or https://nlp.stanford.edu/IR-book/html/htmledition/document-and-query-weighting-schemes-1.html\n\nmodel = TfidfModel(corpus, id2word=dct, smartirs=\"ntc\")\nvectorized_corpus = list(model[corpus])\n```\n\n----------------------------------------\n\nTITLE: Displaying Mean Benchmark Results in Python\nDESCRIPTION: This code snippet displays the mean values of the benchmark results for specific configurations. It filters the results for certain dictionary sizes, nonzero limits, and ANNOY tree configurations, focusing on key metrics like construction duration, production duration, and speeds.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_benchmark.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndisplay(df.mean()).loc[\n    [1000000, len(full_dictionary)], [1, 100], [0, 1, 100]].loc[\n    :, [\"constructor_duration\", \"production_duration\", \"production_speed\", \"processing_speed\"]]\n```\n\n----------------------------------------\n\nTITLE: Loading Saved Word2Vec Model in Python using Gensim\nDESCRIPTION: Demonstrates how to load a previously saved Word2Vec model from a file path using Gensim's load method.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_word2vec.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nnew_model = gensim.models.Word2Vec.load(temporary_filepath)\n```\n\n----------------------------------------\n\nTITLE: Creating DTM Corpus Wrapper in Python\nDESCRIPTION: Defines a corpus wrapper class DTMcorpus that extends TextCorpus to load a premade corpus for use with Dynamic Topic Modeling.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/dtm_example.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass DTMcorpus(corpora.textcorpus.TextCorpus):\n\n    def get_texts(self):\n        return self.input\n\n    def __len__(self):\n        return len(self.input)\n\ncorpus = DTMcorpus(documents)\n```\n\n----------------------------------------\n\nTITLE: Processing Test Data for Author-Topic Model\nDESCRIPTION: Preprocesses the test dataset and creates a test corpus using the dictionary from the training data to ensure word-to-id mapping consistency between train and test data.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_prediction_tutorial.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntestdata_dir = \"/tmp/C50test\"\ntest_docs, test_author2doc = preprocess_docs(testdata_dir)\ntest_corpus_50_20 = create_test_corpus(train_dictionary_50_20, test_docs)\n```\n\n----------------------------------------\n\nTITLE: Displaying Evaluation Results in Markdown Table using Python\nDESCRIPTION: This code generates and displays a markdown table showing the evaluation results, including MAP scores and elapsed times for different similarity measures and baselines.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import display, Markdown\n\noutput = []\nbaselines = [\n    ((\"2016-test\", \"**Winner (UH-PRHLT-primary)**\"), ((76.70, 0), (0, 0))),\n    ((\"2016-test\", \"**Baseline 1 (IR)**\"), ((74.75, 0), (0, 0))),\n    ((\"2016-test\", \"**Baseline 2 (random)**\"), ((46.98, 0), (0, 0))),\n    ((\"2017-test\", \"**Winner (SimBow-primary)**\"), ((47.22, 0), (0, 0))),\n    ((\"2017-test\", \"**Baseline 1 (IR)**\"), ((41.85, 0), (0, 0))),\n    ((\"2017-test\", \"**Baseline 2 (random)**\"), ((29.81, 0), (0, 0)))]\ntable_header = [\"Dataset | Strategy | MAP score | Elapsed time (sec)\", \":---|:---|:---|---:\"]\nfor row, ((dataset, technique), ((mean_map_score, mean_duration), (std_map_score, std_duration))) \\\n        in enumerate(sorted(chain(zip(args_list, results), baselines), key=lambda x: (x[0][0], -x[1][0][0]))):\n    if row % (len(strategies) + 3) == 0:\n        output.extend(chain([\"\n\"], table_header))\n    map_score = \"%.02f ±%.02f\" % (mean_map_score, std_map_score)\n    duration = \"%.02f ±%.02f\" % (mean_duration, std_duration) if mean_duration else \"\"\n    output.append(\"%s|%s|%s|%s\" % (dataset, technique, map_score, duration))\n\ndisplay(Markdown('\n'.join(output)))\n```\n\n----------------------------------------\n\nTITLE: Implementing OOV Terms Reporter\nDESCRIPTION: Creates a function to analyze and report out-of-vocabulary terms in the coherence model, helping identify vocabulary mismatches.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence_model_selection.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim import utils\n\n\ndef report_on_oov_terms(cm, topic_models):\n    \"\"\"OOV = out-of-vocabulary\"\"\"\n    topics_as_topn_terms = [\n        models.CoherenceModel.top_topics_as_word_lists(model, dictionary)\n        for model in topic_models\n    ]\n\n    oov_words = cm._accumulator.not_in_vocab(topics_as_topn_terms)\n    print('number of oov words: %d' % len(oov_words))\n    \n    for num_topics, words in zip(trained_models.keys(), topics_as_topn_terms):\n        oov_words = cm._accumulator.not_in_vocab(words)\n        print('number of oov words for num_topics=%d: %d' % (num_topics, len(oov_words)))\n\nreport_on_oov_terms(cm, trained_models.values())\n```\n\n----------------------------------------\n\nTITLE: Importing Word2Vec Text Model in Python\nDESCRIPTION: This code imports a word2vec text model into a KeyedVectors object.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nwv = KeyedVectors.load_word2vec_format('/tmp/vectors.txt', binary=False)\n```\n\n----------------------------------------\n\nTITLE: Identifying Root and Leaf Nodes in WordNet Graph in Python\nDESCRIPTION: Helper function that identifies root and leaf nodes in a directed graph represented by a file of transitive closure relations. Root nodes have no incoming edges while leaf nodes have no outgoing edges, which is important for creating proper train/test splits.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Evaluation.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef get_root_and_leaf_nodes(data_file):\n    \"\"\"Return keys of root and leaf nodes from a file with transitive closure relations\n    \n    Args:\n        data_file(str): file path containing transitive closure relations\n    \n    Returns:\n        (root_nodes, leaf_nodes) - tuple containing keys of root and leaf nodes\n    \"\"\"\n    root_candidates = set()\n    leaf_candidates = set()\n    with smart_open(data_file, 'rb') as f:\n        for line in f:\n            nodes = line.split()\n            root_candidates.update(nodes)\n            leaf_candidates.update(nodes)\n    \n    with smart_open(data_file, 'rb') as f:\n        for line in f:\n            node_1, node_2 = line.split()\n            if node_1 == node_2:\n                continue\n            leaf_candidates.discard(node_1)\n            root_candidates.discard(node_2)\n    \n    return (leaf_candidates, root_candidates)\n```\n\n----------------------------------------\n\nTITLE: Wikipedia Article Processing\nDESCRIPTION: Example showing how to process Wikipedia dump files and extract articles with their sections into JSON format.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/CHANGELOG.md#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nfor line in smart_open('enwiki-latest-pages-articles.json.gz'):  # read the file we just created\n    article = json.loads(line)\n    print(\"Article title: %s\" % article['title'])\n    for section_title, section_text in zip(article['section_titles'], article['section_texts']):\n        print(\"Section title: %s\" % section_title)\n        print(\"Section text: %s\" % section_text)\n```\n\n----------------------------------------\n\nTITLE: Plotting Annoy Index Performance Metrics\nDESCRIPTION: Creates visualization plots to show the relationship between num_trees parameter and both initialization time and accuracy.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_annoy.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nplt.figure(1, figsize=(12, 6))\nplt.subplot(121)\nplt.plot(x_values, y_values_init)\nplt.title(\"num_trees vs initalization time\")\nplt.ylabel(\"Initialization time (s)\")\nplt.xlabel(\"num_trees\")\nplt.subplot(122)\nplt.plot(x_values, y_values_accuracy)\nplt.title(\"num_trees vs accuracy\")\nplt.ylabel(\"%% accuracy\")\nplt.xlabel(\"num_trees\")\nplt.tight_layout()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Initializing TF-IDF Model in Gensim\nDESCRIPTION: Creates a TF-IDF (Term Frequency-Inverse Document Frequency) model using the previously created corpus.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_topics_and_transformations.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim import models\n\ntfidf = models.TfidfModel(corpus)  # step 1 -- initialize a model\n```\n\n----------------------------------------\n\nTITLE: Comparing WMD between Unrelated Sentences in Python\nDESCRIPTION: This snippet preprocesses a new unrelated sentence and computes its WMD from the original sentence for comparison.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_wmd.rst#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsentence_orange = preprocess('Oranges are my favorite fruit')\ndistance = model.wmdistance(sentence_obama, sentence_orange)\nprint('distance = %.4f' % distance)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Inner Product Between a Document and a Corpus in Python\nDESCRIPTION: This function benchmarks the speed of the inner_product method for computing term similarities between a document and a corpus. It measures execution time and various corpus and matrix properties for different configurations.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_benchmark.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef benchmark(configuration):\n    (matrix, dictionary, nonzero_limit), corpus, normalized, repetition = configuration\n    corpus_size = len(corpus)\n    corpus = [dictionary.doc2bow(doc) for doc in corpus if doc]\n    \n    start_time = time()\n    for vec in corpus:\n        matrix.inner_product(vec, corpus, normalized=normalized)\n    end_time = time()\n    duration = end_time - start_time\n    \n    return {\n        \"dictionary_size\": matrix.matrix.shape[0],\n        \"matrix_nonzero\": matrix.matrix.nnz,\n        \"nonzero_limit\": nonzero_limit,\n        \"normalized\": normalized,\n        \"corpus_size\": corpus_size,\n        \"corpus_actual_size\": len(corpus),\n        \"corpus_nonzero\": sum(len(vec) for vec in corpus),\n        \"mean_document_length\": np.mean([len(doc) for doc in corpus]),\n        \"repetition\": repetition,\n        \"duration\": duration, }\n```\n\n----------------------------------------\n\nTITLE: Fitting LDA Model\nDESCRIPTION: Fits the LDA model to the prepared corpus\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/sklearn_api.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nobj = LdaTransformer(id2word=id2word, num_topics=5, iterations=20)\nlda = obj.fit(corpus)\n```\n\n----------------------------------------\n\nTITLE: Dendrogram Distance Calculation Setup\nDESCRIPTION: Sets up the distance calculation for creating topic dendrograms\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_network.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.matutils import jensen_shannon\nimport scipy as scp\nfrom scipy.cluster import hierarchy as sch\nfrom scipy import spatial as scs\n\n# get topic distributions\ntopic_dist = lda_fake.state.get_lambda()\n\n# get topic terms\nnum_words = 300\ntopic_terms = [{w for (w, _) in lda_fake.show_topic(topic, topn=num_words)} for topic in range(topic_dist.shape[0])]\n\n# no. of terms to display in annotation\nn_ann_terms = 10\n\n# use Jenson-Shannon distance metric in dendrogram\ndef js_dist(X):\n    return pdist(X, lambda u, v: jensen_shannon(u, v))\n\n# define method for distance calculation in clusters\nlinkagefun=lambda x: sch.linkage(x, 'single')\n```\n\n----------------------------------------\n\nTITLE: Listing Available Models and Corpora\nDESCRIPTION: Shows how to retrieve and display a list of all available models and corpora from Gensim's downloader API, formatting the output as indented JSON.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/downloader_api_tutorial.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport json\ndata_list = api.info()\nprint(json.dumps(data_list, indent=4))\n```\n\n----------------------------------------\n\nTITLE: Evaluating NMSLIB Indexer with Multiple Parameters in Python\nDESCRIPTION: This code evaluates NMSLIB indexer performance by varying multiple parameters (M, efConstruction, efSearch) simultaneously.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nnmslib_y_values_init = []\nnmslib_y_values_accuracy = []\nnmslib_y_values_query = []\n\nfor M in [100, 200]:\n    for efConstruction in [100, 200]:\n        for efSearch in [100, 200]:\n            start_time = time.time()\n            nmslib_index = NmslibIndexer(model, \n                                    {'M': M, 'indexThreadQty': 10, 'efConstruction': efConstruction, 'post': 0},\n                                    {'efSearch': efSearch})\n            nmslib_y_values_init.append(time.time() - start_time)\n            approximate_results = model.most_similar([model.wv.syn0norm[0]], topn=100, indexer=nmslib_index)\n            top_words = [result[0] for result in approximate_results]\n            nmslib_y_values_accuracy.append(len(set(top_words).intersection(exact_results)))\n            nmslib_y_values_query.append(avg_query_time(nmslib_index, queries=queries))\n```\n\n----------------------------------------\n\nTITLE: Displaying Topic Coherence Rankings in Python\nDESCRIPTION: Calls the print_coherence_rankings function to display the ranking of different LDA models based on their coherence scores, showing which model has the best overall coherence.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence_model_selection.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nprint_coherence_rankings(coherences)\n```\n\n----------------------------------------\n\nTITLE: Testing Model with Test Corpus\nDESCRIPTION: Tests the model by comparing a random test document with the training corpus\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_doc2vec_lee.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndoc_id = random.randint(0, len(test_corpus) - 1)\ninferred_vector = model.infer_vector(test_corpus[doc_id])\nsims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n\nprint('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\nprint(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\nfor label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))\n```\n\n----------------------------------------\n\nTITLE: Comparing NMSLIB and Annoy Indexers Performance in Python\nDESCRIPTION: This code creates scatter plots to compare the performance of NMSLIB and Annoy indexers in terms of initialization time, query time, and accuracy.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nplt.figure(1, figsize=(12, 6))\nplt.subplot(121)\nplt.scatter(nmslib_y_values_init, nmslib_y_values_accuracy, label=\"nmslib\", color='r', marker='o')\nplt.scatter(annoy_y_values_init, annoy_y_values_accuracy, label=\"annoy\", color='b', marker='x')\nplt.legend()\nplt.title(\"Initialization time vs accuracy. Upper left is better.\")\nplt.ylabel(\"% accuracy\")\nplt.xlabel(\"Initialization time (s)\")\nplt.subplot(122)\nplt.scatter(nmslib_y_values_query, nmslib_y_values_accuracy, label=\"nmslib\", color='r', marker='o')\nplt.scatter(annoy_y_values_query, annoy_y_values_accuracy, label=\"annoy\", color='b', marker='x')\nplt.legend()\nplt.title(\"Query time vs accuracy. Upper left is better.\")\nplt.ylabel(\"% accuracy\")\nplt.xlabel(\"Query time (s)\")\nplt.xlim(min(nmslib_y_values_query+annoy_y_values_query), max(nmslib_y_values_query+annoy_y_values_query))\nplt.tight_layout()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Comparing Topics Within a Model Using Hellinger Distance\nDESCRIPTION: Visualizes topic correlation within a single LDA model using Hellinger distance, which may reveal different patterns of similarity than Jaccard distance.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_compare_lda.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmdiff, annotation = lda_fst.diff(lda_fst, distance='hellinger', num_words=50)\nplot_difference(mdiff, title=\"Topic difference (one model)[hellinger distance]\", annotation=annotation)\n```\n\n----------------------------------------\n\nTITLE: Building Doc2Vec Vocabulary\nDESCRIPTION: Builds vocabulary from the training corpus for the Doc2Vec model\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_doc2vec_lee.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmodel.build_vocab(train_corpus)\n```\n\n----------------------------------------\n\nTITLE: Network Trace Creation\nDESCRIPTION: Creates the node and edge traces for the network visualization with annotations\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_network.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# initialize traces for drawing nodes and edges \nnode_trace = Scatter(\n    x=[],\n    y=[],\n    text=[],\n    mode='markers',\n    hoverinfo='text',\n    marker=Marker(\n        showscale=True,\n        colorscale='YIGnBu',\n        reversescale=True,\n        color=[],\n        size=10,\n        colorbar=dict(\n            thickness=15,\n            xanchor='left'\n        ),\n        line=dict(width=2)))\n\nedge_trace = Scatter(\n    x=[],\n    y=[],\n    text=[],\n    line=Line(width=0.5, color='#888'),\n    hoverinfo='text',\n    mode='lines')\n\n\n# no. of terms to display in annotation\nn_ann_terms = 10\n\n# add edge trace with annotations\nfor edge in G.edges():\n    x0, y0 = graph_pos[edge[0]]\n    x1, y1 = graph_pos[edge[1]]\n    \n    pos_tokens = topic_terms[edge[0]] & topic_terms[edge[1]]\n    neg_tokens = topic_terms[edge[0]].symmetric_difference(topic_terms[edge[1]])\n    pos_tokens = list(pos_tokens)[:min(len(pos_tokens), n_ann_terms)]\n    neg_tokens = list(neg_tokens)[:min(len(neg_tokens), n_ann_terms)]\n    annotation = \"<br>\".join((\":\".join((\"+++\", str(pos_tokens))), \":\".join((\"---\", str(neg_tokens)))))\n    \n    x_trace = list(np.linspace(x0, x1, 10))\n    y_trace = list(np.linspace(y0, y1, 10))\n    text_annotation = [annotation] * 10\n    x_trace.append(None)\n    y_trace.append(None)\n    text_annotation.append(None)\n    \n    edge_trace['x'] += x_trace\n    edge_trace['y'] += y_trace\n    edge_trace['text'] += text_annotation\n\n# add node trace with annotations\nfor node in G.nodes():\n    x, y = graph_pos[node]\n    node_trace['x'].append(x)\n    node_trace['y'].append(y)\n    node_info = ''.join((str(node+1), ': ', str(list(topic_terms[node])[:n_ann_terms])))\n    node_trace['text'].append(node_info)\n    \n# color node according to no. of connections\nfor node, adjacencies in enumerate(G.adjacency()):\n    node_trace['marker']['color'].append(len(adjacencies))\n```\n\n----------------------------------------\n\nTITLE: Training Models with Default and Non-Default Parameters using Gensim\nDESCRIPTION: This snippet trains models using both default and non-default parameters. It initializes model names and file paths and stores them in a dictionary for further evaluation. The models are trained on specified files with specified sizes while updating dictionary entries for each model created.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Evaluation.ipynb#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nmodel_name, files = train_model_with_params(default_params, wordnet_train_file, model_sizes, 'gensim_lp_model', 'gensim')\nlp_model_files['gensim'][model_name] = {}\nfor dim, filepath in files.items():\n    lp_model_files['gensim'][model_name][dim] = filepath\n# Train models with non-default params\nfor new_params in non_default_params_gensim:\n    params = default_params.copy()\n    params.update(new_params)\n    model_name, files = train_model_with_params(params, wordnet_file, model_sizes, 'gensim_lp_model', 'gensim')\n    lp_model_files['gensim'][model_name] = {}\n    for dim, filepath in files.items():\n        lp_model_files['gensim'][model_name][dim] = filepath\n```\n\n----------------------------------------\n\nTITLE: Online Vocabulary Update Training\nDESCRIPTION: Updates the Word2Vec model's vocabulary using the newer Wikipedia dump with online learning enabled.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/online_w2v_tutorial.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n%%time\nmodel.build_vocab(newwiki, update=True)\nmodel.train(newwiki, total_examples=model.corpus_count, epochs=model.iter)\nmodel.save('newmodel')\n# model = Word2Vec.load('newmodel')\n```\n\n----------------------------------------\n\nTITLE: Analyzing Document Length Bias in Standard TF-IDF\nDESCRIPTION: Defines a function to sort documents by score and calculate their lengths, then demonstrates how standard cosine normalization favors shorter documents by comparing mean lengths.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/pivoted_document_length_normalisation.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\n# Sort the document scores by their scores and return a sorted list\n# of document score and corresponding document lengths.\ndef sort_length_by_score(doc_scores, X_test):\n    doc_scores = sorted(enumerate(doc_scores), key=lambda x: x[1])\n    doc_leng = np.empty(len(doc_scores))\n\n    ds = np.empty(len(doc_scores))\n\n    for i, _ in enumerate(doc_scores):\n        doc_leng[i] = len(X_test[_[0]])\n        ds[i] = _[1]\n\n    return ds, doc_leng\n\n\nprint(\n   \"Normal cosine normalisation favors short documents as our top {} \"\n   \"docs have a smaller mean doc length of {:.3f} compared to the corpus mean doc length of {:.3f}\"\n   .format(\n       k, sort_length_by_score(doc_scores, X_test)[1][:k].mean(), \n       sort_length_by_score(doc_scores, X_test)[1].mean()\n   )\n)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Word2Vec Training in Python\nDESCRIPTION: Code for preparing benchmark data to test Word2Vec training performance with different corpus sizes.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_word2vec.ipynb#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport io\nimport os\n\nimport gensim.models.word2vec\nimport gensim.downloader as api\nimport smart_open\n\n\ndef head(path, size):\n    with smart_open.open(path) as fin:\n        return io.StringIO(fin.read(size))\n\n\ndef generate_input_data():\n    lee_path = datapath('lee_background.cor')\n    ls = gensim.models.word2vec.LineSentence(lee_path)\n    ls.name = '25kB'\n    yield ls\n\n    text8_path = api.load('text8').fn\n    labels = ('1MB', '10MB', '50MB', '100MB')\n    sizes = (1024 ** 2, 10 * 1024 ** 2, 50 * 1024 ** 2, 100 * 1024 ** 2)\n    for l, s in zip(labels, sizes):\n        ls = gensim.models.word2vec.LineSentence(head(text8_path, s))\n        ls.name = l\n        yield ls\n\n\ninput_data = list(generate_input_data())\n```\n\n----------------------------------------\n\nTITLE: Text Preprocessing - Python\nDESCRIPTION: Processes text files by lowercasing words and removing punctuation, while filtering out disambiguation, file, and category pages\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence-movies.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\ntexts = []\nfile_num = 0\npreprocessed = 0\nlisting = os.listdir(data_dir)\n\nfor fname in listing:\n    file_num += 1\n    if 'disambiguation' in fname:\n        continue  # discard disambiguation and redirect pages\n    elif fname.startswith('File_'):\n        continue  # discard images, gifs, etc.\n    elif fname.startswith('Category_'):\n        continue  # discard category articles\n        \n    # Not sure how to identify portal and redirect pages,\n    # as well as pages about a single year.\n    # As a result, this preprocessing differs from the paper.\n    \n    with smart_open(os.path.join(data_dir, fname), 'rb') as f:\n        for line in f:\n            # lower case all words\n            lowered = line.lower()\n            #remove punctuation and split into seperate words\n            words = re.findall(r'\\w+', lowered, flags = re.UNICODE | re.LOCALE)\n            texts.append(words)\n            \n    preprocessed += 1\n    if file_num % 10000 == 0:\n        print('PROGRESS: %d/%d, preprocessed %d, discarded %d' % (\n            file_num, len(listing), preprocessed, (file_num - preprocessed)))\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Sentences and Creating a Dictionary in Python\nDESCRIPTION: Removes stopwords from sentences, creates a dictionary, and converts sentences to bag-of-words vectors for similarity computation.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom nltk.corpus import stopwords\nfrom nltk import download\n\ndownload('stopwords')  # Download stopwords list.\n\n# Remove stopwords.\nstop_words = stopwords.words('english')\nsentence_obama = [w for w in sentence_obama if w not in stop_words]\nsentence_president = [w for w in sentence_president if w not in stop_words]\nsentence_orange = [w for w in sentence_orange if w not in stop_words]\n\n# Prepare a dictionary and a corpus.\nfrom gensim import corpora\ndocuments = [sentence_obama, sentence_president, sentence_orange]\ndictionary = corpora.Dictionary(documents)\n\n# Convert the sentences into bag-of-words vectors.\nsentence_obama = dictionary.doc2bow(sentence_obama)\nsentence_president = dictionary.doc2bow(sentence_president)\nsentence_orange = dictionary.doc2bow(sentence_orange)\n```\n\n----------------------------------------\n\nTITLE: Applying TF-IDF Transformation to Entire Corpus in Gensim\nDESCRIPTION: Shows how to apply the TF-IDF transformation to the entire corpus and print the results.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_topics_and_transformations.rst#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncorpus_tfidf = tfidf[corpus]\nfor doc in corpus_tfidf:\n    print(doc)\n```\n\n----------------------------------------\n\nTITLE: Implementing C_V Coherence\nDESCRIPTION: Sets up coherence models using the c_v metric for comparing topic model quality.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence_tutorial.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ngoodcm = CoherenceModel(model=goodLdaModel, texts=texts, dictionary=dictionary, coherence='c_v')\nbadcm = CoherenceModel(model=badLdaModel, texts=texts, dictionary=dictionary, coherence='c_v')\n```\n\n----------------------------------------\n\nTITLE: Training NumPy Models for WordNet Link Prediction in Python\nDESCRIPTION: Training NumPy implementation models for the link prediction task with default parameters. The code creates models of various dimensions and stores their file paths for later evaluation.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Evaluation.ipynb#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nlp_model_files['numpy'] = {}\n# Train numpy models with default params\nmodel_name, files = train_model_with_params(default_params, wordnet_train_file, model_sizes, 'np_lp_model', 'numpy')\nlp_model_files['numpy'][model_name] = {}\nfor dim, filepath in files.items():\n    lp_model_files['numpy'][model_name][dim] = filepath\n```\n\n----------------------------------------\n\nTITLE: Running Distributed LSA on a Large Corpus in Python\nDESCRIPTION: This code shows how to perform distributed LSA on a corpus of one million documents using Gensim.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/dist_lsi.rst#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> # inflate the corpus to 1M documents, by repeating its documents over&over\n>>> corpus1m = utils.RepeatCorpus(corpus, 1000000)\n>>> # run distributed LSA on 1 million documents\n>>> lsi1m = models.LsiModel(corpus1m, id2word=id2word, num_topics=200, chunksize=10000, distributed=True)\n>>>\n>>> lsi1m.print_topics(num_topics=2, num_words=5)\ntopic #0(1113.628): 0.644*\"system\" + 0.404*\"user\" + 0.301*\"eps\" + 0.265*\"time\" + 0.265*\"response\"\ntopic #1(847.233): 0.623*\"graph\" + 0.490*\"trees\" + 0.451*\"minors\" + 0.274*\"survey\" + -0.167*\"system\"\n```\n\n----------------------------------------\n\nTITLE: Starting Gensim LSI Dispatcher\nDESCRIPTION: This command starts the LSI dispatcher, which coordinates the distributed LSA computation across worker nodes.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/dist_lsi.rst#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m gensim.models.lsi_dispatcher &\n```\n\n----------------------------------------\n\nTITLE: Building Dictionary and TF-IDF Model\nDESCRIPTION: Creating a dictionary and TF-IDF model from preprocessed sentences\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_scm.rst#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.corpora import Dictionary\ndocuments = [sentence_obama, sentence_president, sentence_orange]\ndictionary = Dictionary(documents)\n\nsentence_obama = dictionary.doc2bow(sentence_obama)\nsentence_president = dictionary.doc2bow(sentence_president)\nsentence_orange = dictionary.doc2bow(sentence_orange)\n\nfrom gensim.models import TfidfModel\ndocuments = [sentence_obama, sentence_president, sentence_orange]\ntfidf = TfidfModel(documents)\n\nsentence_obama = tfidf[sentence_obama]\nsentence_president = tfidf[sentence_president]\nsentence_orange = tfidf[sentence_orange]\n```\n\n----------------------------------------\n\nTITLE: Initial Word2Vec Model Training\nDESCRIPTION: Trains the initial Word2Vec model using the older Wikipedia dump and creates a backup copy.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/online_w2v_tutorial.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n%%time\nmodel = Word2Vec(oldwiki, min_count = 0, workers=cpu_count())\n# model = Word2Vec.load('oldmodel')\noldmodel = deepcopy(model)\noldmodel.save('oldmodel')\n```\n\n----------------------------------------\n\nTITLE: Creating WordNet Train-Test Split for Link Prediction in Python\nDESCRIPTION: Creating train and test files for the WordNet link prediction task by splitting the original WordNet file. This enables evaluation of the models' ability to predict unseen edges between nodes, testing generalization rather than memorization.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Evaluation.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nwordnet_train_file, wordnet_test_file = train_test_split(wordnet_file)\n```\n\n----------------------------------------\n\nTITLE: Saving a WordRank Model in Python using Gensim\nDESCRIPTION: This code snippet shows how to save a loaded and potentially modified WordRank model using Gensim's standard save method. It creates a temporary file and saves the model to that location.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/WordRank_wrapper_quickstart.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom tempfile import mkstemp\n\nfs, temp_path = mkstemp(\"gensim_temp\")  # creates a temp file\nmodel.save(temp_path)  # save the model\n```\n\n----------------------------------------\n\nTITLE: Implementing Visualization Functions for Topic Model Comparison\nDESCRIPTION: Defines two visualization functions for plotting topic model differences using either Plotly (for interactive Jupyter notebooks) or Matplotlib (for static views), with automatic detection of the execution environment.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_compare_lda.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef plot_difference_plotly(mdiff, title=\"\", annotation=None):\n    \"\"\"Plot the difference between models.\n\n    Uses plotly as the backend.\"\"\"\n    import plotly.graph_objs as go\n    import plotly.offline as py\n\n    annotation_html = None\n    if annotation is not None:\n        annotation_html = [\n            [\n                \"+++ {}<br>--- {}\".format(\", \".join(int_tokens), \", \".join(diff_tokens))\n                for (int_tokens, diff_tokens) in row\n            ]\n            for row in annotation\n        ]\n\n    data = go.Heatmap(z=mdiff, colorscale='RdBu', text=annotation_html)\n    layout = go.Layout(width=950, height=950, title=title, xaxis=dict(title=\"topic\"), yaxis=dict(title=\"topic\"))\n    py.iplot(dict(data=[data], layout=layout))\n\n\ndef plot_difference_matplotlib(mdiff, title=\"\", annotation=None):\n    \"\"\"Helper function to plot difference between models.\n\n    Uses matplotlib as the backend.\"\"\"\n    import matplotlib.pyplot as plt\n    fig, ax = plt.subplots(figsize=(18, 14))\n    data = ax.imshow(mdiff, cmap='RdBu_r', origin='lower')\n    plt.title(title)\n    plt.colorbar(data)\n\n\ntry:\n    get_ipython()\n    import plotly.offline as py\nexcept Exception:\n    #\n    # Fall back to matplotlib if we're not in a notebook, or if plotly is\n    # unavailable for whatever reason.\n    #\n    plot_difference = plot_difference_matplotlib\nelse:\n    py.init_notebook_mode()\n    plot_difference = plot_difference_plotly\n```\n\n----------------------------------------\n\nTITLE: Creating a TF-IDF Scoring Function\nDESCRIPTION: Defines a function that returns model accuracy and document probability scores using Gensim's TfIdfTransformer and scikit-learn's LogisticRegression. This will be used to compare different normalization approaches.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/pivoted_document_length_normalisation.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.sklearn_api.tfidf import TfIdfTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom gensim.matutils import corpus2csc\n\n# This function returns the model accuracy and indivitual document prob values using\n# gensim's TfIdfTransformer and sklearn's LogisticRegression\ndef get_tfidf_scores(kwargs):\n    tfidf_transformer = TfIdfTransformer(**kwargs).fit(train_corpus)\n\n    X_train_tfidf = corpus2csc(tfidf_transformer.transform(train_corpus), num_terms=len(id2word)).T\n    X_test_tfidf = corpus2csc(tfidf_transformer.transform(test_corpus), num_terms=len(id2word)).T\n\n    clf = LogisticRegression().fit(X_train_tfidf, y_train)\n\n    model_accuracy = clf.score(X_test_tfidf, y_test)\n    doc_scores = clf.decision_function(X_test_tfidf)\n\n    return model_accuracy, doc_scores\n```\n\n----------------------------------------\n\nTITLE: Visualizing the Effect of Pivoted Normalization\nDESCRIPTION: Creates histogram plots to visualize how different slope values affect document length bias. It compares slope=1 (equivalent to standard normalization) with slope=0.2 to show how pivoted normalization shifts the distribution of top-scoring documents.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/pivoted_document_length_normalisation.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n%matplotlib inline\nimport matplotlib.pyplot as py\n\nbest_model_accuracy = 0\noptimum_slope = 0\n\nw = 2\nh = 2\nf, axarr = py.subplots(h, w, figsize=(15, 7))\n\nit = 0\nfor slope in [1, 0.2]:\n    params = {\"pivot\": 10, \"slope\": slope}\n\n    model_accuracy, doc_scores = get_tfidf_scores(params)\n\n    if model_accuracy > best_model_accuracy:\n        best_model_accuracy = model_accuracy\n        optimum_slope = slope\n\n    doc_scores, doc_leng = sort_length_by_score(doc_scores, X_test)\n\n    y = abs(doc_scores[:k, np.newaxis])\n    x = doc_leng[:k, np.newaxis]\n\n    py.subplot(1, 2, it+1).bar(x, y, width=20, linewidth=0)\n    py.title(\"slope = \" + str(slope) + \" Model accuracy = \" + str(model_accuracy))\n    py.ylim([0, 4.5])\n    py.xlim([0, 3200])\n    py.xlabel(\"document length\")\n    py.ylabel(\"confidence score\")\n    \n    it += 1\n\npy.tight_layout()\npy.show()\n```\n\n----------------------------------------\n\nTITLE: Defining a function to measure average query time\nDESCRIPTION: Creates a function that measures the average query time for most_similar searches across a specified number of random queries, with option to use an indexer.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef avg_query_time(annoy_index=None, queries=1000):\n    \"\"\"\n    Average query time of a most_similar method over 1000 random queries,\n    uses annoy if given an indexer\n    \"\"\"\n    total_time = 0\n    for _ in range(queries):\n        rand_vec = model.wv.syn0norm[np.random.randint(0, len(model.wv.vocab))]\n        start_time = time.clock()\n        model.most_similar([rand_vec], topn=5, indexer=annoy_index)\n        total_time += time.clock() - start_time\n    return total_time / queries\n```\n\n----------------------------------------\n\nTITLE: Training Two LDA Models with Different Parameters\nDESCRIPTION: Creates two LDA models with different training passes (10 vs 20) using Gensim's LdaMulticore for parallel processing on the prepared 20 Newsgroups dataset.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_compare_lda.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models import LdaMulticore\nnum_topics = 15\n\nlda_fst = LdaMulticore(\n    corpus=d2b_dataset, num_topics=num_topics, id2word=dictionary,\n    workers=4, eval_every=None, passes=10, batch=True,\n)\n\nlda_snd = LdaMulticore(\n    corpus=d2b_dataset, num_topics=num_topics, id2word=dictionary,\n    workers=4, eval_every=None, passes=20, batch=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Calculating Document-Topic Proportions in Python\nDESCRIPTION: Computes and prints the distribution of topics for a specific document using the gamma matrix from the DTM model.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/dtm_example.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndoc_number = 1\nnum_topics = 2\n\nfor i in range(0, num_topics):\n    print (\"Distribution of Topic %d %f\" % (i, model.gamma_[doc_number, i]))\n```\n\n----------------------------------------\n\nTITLE: Creating and Filtering Gensim Dictionary\nDESCRIPTION: Builds a Gensim Dictionary from the processed documents and filters out both frequent words (occurring in >50% of documents) and rare words (occurring <20 times) to improve model quality.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Create a dictionary representation of the documents, and filter out frequent and rare words.\n\nfrom gensim.corpora import Dictionary\ndictionary = Dictionary(docs)\n\n# Remove rare and common tokens.\n# Filter out words that occur too frequently or too rarely.\nmax_freq = 0.5\nmin_wordcount = 20\ndictionary.filter_extremes(no_below=min_wordcount, no_above=max_freq)\n\n_ = dictionary[0]  # This sort of \"initializes\" dictionary.id2token.\n```\n\n----------------------------------------\n\nTITLE: Creating Interactive Scatter Plot with Bokeh in Python\nDESCRIPTION: This snippet sets up an interactive scatter plot using Bokeh. It adds author names and sizes to mouse-over information and creates a figure with various tools for interaction.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# Add author names and sizes to mouse-over info.\nhover = HoverTool(\n        tooltips=[\n        (\"author\", \"@author_names\"),\n        (\"size\", \"@author_sizes\"),\n        ]\n    )\n\np = figure(tools=[hover, 'crosshair,pan,wheel_zoom,box_zoom,reset,save,lasso_select'])\np.scatter('x', 'y', radius='radii', source=source, fill_alpha=0.6, line_color=None)\nshow(p)\n```\n\n----------------------------------------\n\nTITLE: Comparing Topics Within a Model Using Jaccard Distance\nDESCRIPTION: Visualizes how topics within a single LDA model correlate with each other using Jaccard distance, showing topic overlap through a heatmap with annotations showing shared and distinct words.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_compare_lda.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmdiff, annotation = lda_fst.diff(lda_fst, distance='jaccard', num_words=50)\nplot_difference(mdiff, title=\"Topic difference (one model) [jaccard distance]\", annotation=annotation)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages\nDESCRIPTION: These commands install necessary Python packages: wmd, sklearn, and POT for implementing similarity measures and evaluation.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n!pip install wmd\n```\n\nLANGUAGE: python\nCODE:\n```\n!pip install sklearn\n```\n\nLANGUAGE: python\nCODE:\n```\n!pip install POT\n```\n\n----------------------------------------\n\nTITLE: Back-Mapping Translation Experiment\nDESCRIPTION: Implements back-mapping translation between two Doc2Vec models to infer vectors for additional documents. Evaluates classification performance of the inferred vectors.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/translation_matrix.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models import translation_matrix\nbasedir = \"/home/robotcator/doc2vec\"\n\nmodel1 = Doc2Vec.load(os.path.join(basedir, \"small_doc_15000_iter50.bin\"))\nmodel2 = Doc2Vec.load(os.path.join(basedir, \"large_doc_50000_iter50.bin\"))\n\nl = model1.docvecs.count\nl2 = model2.docvecs.count\nm1 = np.array([model1.docvecs[large_corpus[i].tags].flatten() for i in range(l)])\n\nmodel = translation_matrix.BackMappingTranslationMatrix(large_corpus[:15000], model1, model2)\nmodel.train(large_corpus[:15000])\n\nfor i in range(l, l2):\n    infered_vec = model.infer_vector(model2.docvecs[large_corpus[i].tags])\n    m1 = np.vstack((m1, infered_vec.flatten()))\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for EnsembleLda Topic Modeling in Python\nDESCRIPTION: Imports necessary modules from Gensim and standard libraries to work with Ensemble LDA models and the Opinosis corpus. The imports include logging functionality, LDA model variants, masking methods, and corpus handling.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/ensemble_lda_with_opinosis.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nfrom gensim.models import EnsembleLda, LdaMulticore\nfrom gensim.models.ensemblelda import rank_masking\nfrom gensim.corpora import OpinosisCorpus\nimport os\n```\n\n----------------------------------------\n\nTITLE: Finding Similar Words with VarEmbed Model\nDESCRIPTION: Example of using the most_similar method on a loaded VarEmbed model to find words similar to 'government'. This demonstrates one of the standard Gensim KeyedVectors functionalities.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Varembed.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel.most_similar('government')\n```\n\n----------------------------------------\n\nTITLE: Word Vector Embeddings Data\nDESCRIPTION: 50-dimensional word vectors representing semantic relationships between common English words. Each line contains a word followed by its vector representation as space-separated floating point numbers.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/test_glove.txt#2025-04-21_snippet_3\n\nLANGUAGE: text\nCODE:\n```\npeople 0.95281 -0.20608 0.55618 -0.46323 0.73354 0.029137 -0.19367 -0.090066 -0.22958 -0.19058 -0.34857 -1.0231 0.743 -0.5489 0.88484 -0.14051 0.0040139 0.58448 0.10767 -0.44657 -0.43205 0.9868 0.78288 0.51513 0.85788 -1.7713 -0.88259 -0.59728 0.084934 -0.48112 3.9678 0.8893 -0.27064 -0.44094 -0.26213 0.085597 0.022099 -0.58376 0.10908 0.77973 -0.95447 0.40482 0.8941 0.65251 0.39858 0.20884 -1.3281 -0.10882 -0.22822 -0.46303\nn't 0.028702 -0.2163 0.27153 -0.28594 0.42404 -0.18155 -0.85966 0.30447 -0.51645 0.3559 -0.10131 0.8152 -0.77987 -0.044123 1.3768 0.96711 0.59098 -0.16521 0.094372 -1.2292 -0.59056 0.42275 0.52645 0.17536 0.62117 -2.3875 -0.90795 0.26418 1.1507 -1.4289 3.511 0.96796 -0.5905 -0.21382 -0.13049 -0.34336 0.15822 0.2306 0.55332 -0.59173 -0.4403 0.23583 0.082353 0.83847 0.26719 0.063263 -0.080607 0.018159 -0.22789 1.0025\n```\n\n----------------------------------------\n\nTITLE: Loading Document Corpus for Python DTM Analysis\nDESCRIPTION: This snippet defines a sample corpus of documents. Each document is represented as a list of tokens. This pre-processed data is ready for use in the DTM analysis.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/dtm_example.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndocuments = [[u'senior', u'studios', u'studios', u'studios', u'creators', u'award', u'mobile', u'currently', u'challenges', u'senior', u'summary', u'senior', u'motivated', u'creative', u'senior', u'performs', u'engineering', u'tasks', u'infrastructure', u'focusing', u'primarily', u'programming', u'interaction', u'designers', u'engineers', u'leadership', u'teams', u'teams', u'crews', u'responsibilities', u'engineering', u'quality', u'functional', u'functional', u'teams', u'organizing', u'prioritizing', u'technical', u'decisions', u'engineering', u'participates', u'participates', u'reviews', u'participates', u'hiring', u'conducting', u'interviews', u'feedback', u'departments', u'define', u'focusing', u'engineering', u'teams', u'crews', u'facilitate', u'engineering', u'departments', u'deadlines', u'milestones', u'typically', u'spends', u'designing', u'developing', u'updating', u'bugs', u'mentoring', u'engineers', u'define', u'schedules', u'milestones', u'participating', u'reviews', u'interviews', u'sized', u'teams', u'interacts', u'disciplines', u'knowledge', u'skills', u'knowledge', u'knowledge', u'xcode', u'scripting', u'debugging', u'skills', u'skills', u'knowledge', u'disciplines', u'animation', u'networking', u'expertise', u'competencies', u'oral', u'skills', u'management', u'skills', u'proven', u'effectively', u'teams', u'deadline', u'environment', u'bachelor', u'minimum', u'shipped', u'leadership', u'teams', u'location', u'resumes', u'jobs', u'candidates', u'openings', u'jobs'], [u'maryland', u'client', u'producers', u'electricity', u'operates', u'storage', u'utility', u'retail', u'customers', u'engineering', u'consultant', u'maryland', u'summary', u'technical', u'technology', u'departments', u'expertise', u'maximizing', u'output', u'reduces', u'operating', u'participates', u'areas', u'engineering', u'conducts', u'testing', u'solve', u'supports', u'environmental', u'understands', u'objectives', u'operates', u'responsibilities', u'handles', u'complex', u'engineering', u'aspects', u'monitors', u'quality', u'proficiency', u'optimization', u'recommendations', u'supports', u'personnel', u'troubleshooting', u'commissioning', u'startup', u'shutdown', u'supports', u'procedure', u'operating', u'units', u'develops', u'simulations', u'troubleshooting', u'tests', u'enhancing', u'solving', u'develops', u'estimates', u'schedules', u'scopes', u'understands', u'technical', u'management', u'utilize', u'routine', u'conducts', u'hazards', u'utilizing', u'hazard', u'operability', u'methodologies', u'participates', u'startup', u'reviews', u'pssr', u'participate', u'teams', u'participate', u'regulatory', u'audits', u'define', u'scopes', u'budgets', u'schedules', u'technical', u'management', u'environmental', u'awareness', u'interfacing', u'personnel', u'interacts', u'regulatory', u'departments', u'input', u'objectives', u'identifying', u'introducing', u'concepts', u'solutions', u'peers', u'customers', u'coworkers', u'knowledge', u'skills', u'engineering', u'quality', u'engineering', u'commissioning', u'startup', u'knowledge', u'simulators', u'technologies', u'knowledge', u'engineering', u'techniques', u'disciplines', u'leadership', u'skills', u'proven', u'engineers', u'oral', u'skills', u'technical', u'skills', u'analytically', u'solve', u'complex', u'interpret', u'proficiency', u'simulation', u'knowledge', u'applications', u'manipulate', u'applications', u'engineering', u'calculations', u'programs', u'matlab', u'excel', u'independently', u'environment', u'proven', u'skills', u'effectively', u'multiple', u'tasks', u'planning', u'organizational', u'management', u'skills', u'rigzone', u'jobs', u'developer', u'exceptional', u'strategies', u'junction', u'exceptional', u'strategies', u'solutions', u'solutions', u'biggest', u'insurers', u'operates', u'investment'], [u'vegas', u'tasks', u'electrical', u'contracting', u'expertise', u'virtually', u'electrical', u'developments', u'institutional', u'utilities', u'technical', u'experts', u'relationships', u'credibility', u'contractors', u'utility', u'customers', u'customer', u'relationships', u'consistently', u'innovations', u'profile', u'construct', u'envision', u'dynamic', u'complex', u'electrical', u'management', u'grad', u'internship', u'electrical', u'engineering', u'infrastructures', u'engineers', u'documented', u'management', u'engineering', u'quality', u'engineering', u'electrical', u'engineers', u'complex', u'distribution', u'grounding', u'estimation', u'testing', u'procedures', u'voltage', u'engineering', u'troubleshooting', u'installation', u'documentation', u'bsee', u'certification', u'electrical', u'voltage', u'cabling', u'electrical', u'engineering', u'candidates', u'electrical', u'internships', u'oral', u'skills', u'organizational', u'prioritization', u'skills', u'skills', u'excel', u'cadd', u'calculation', u'autocad', u'mathcad', u'skills', u'skills', u'customer', u'relationships', u'solving', u'ethic', u'motivation', u'tasks', u'budget', u'affirmative', u'diversity', u'workforce', u'gender', u'orientation', u'disability', u'disabled', u'veteran', u'vietnam', u'veteran', u'qualifying', u'veteran', u'diverse', u'candidates', u'respond', u'developing', u'workplace', u'reflects', u'diversity', u'communities', u'reviews', u'electrical', u'contracting', u'southwest', u'electrical', u'contractors'], [u'intern', u'electrical', u'engineering', u'idexx', u'laboratories', u'validating', u'idexx', u'integrated', u'hardware', u'entails', u'planning', u'debug', u'validation', u'engineers', u'validation', u'methodologies', u'healthcare', u'platforms', u'brightest', u'solve', u'challenges', u'innovation', u'technology', u'idexx', u'intern', u'idexx', u'interns', u'supplement', u'interns', u'teams', u'roles', u'competitive', u'interns', u'idexx', u'interns', u'participate', u'internships', u'mentors', u'seminars', u'topics', u'leadership', u'workshops', u'relevant', u'planning', u'topics', u'intern', u'presentations', u'mixers', u'applicants', u'ineligible', u'laboratory', u'compliant', u'idexx', u'laboratories', u'healthcare', u'innovation', u'practicing', u'veterinarians', u'diagnostic', u'technology', u'idexx', u'enhance', u'veterinarians', u'efficiency', u'economically', u'idexx', u'worldwide', u'diagnostic', u'tests', u'tests', u'quality', u'headquartered', u'idexx', u'laboratories', u'employs', u'customers', u'qualifications', u'applicants', u'idexx', u'interns', u'potential', u'demonstrated', u'portfolio', u'recommendation', u'resumes', u'marketing', u'location', u'americas', u'verification', u'validation', u'schedule', u'overtime', u'idexx', u'laboratories', u'reviews', u'idexx', u'laboratories', u'nasdaq', u'healthcare', u'innovation', u'practicing', u'veterinarians'], [u'location', u'duration', u'temp', u'verification', u'validation', u'tester', u'verification', u'validation', u'middleware', u'specifically', u'testing', u'applications', u'clinical', u'laboratory', u'regulated', u'environment', u'responsibilities', u'complex', u'hardware', u'testing', u'clinical', u'analyzers', u'laboratory', u'graphical', u'interfaces', u'complex', u'sample', u'sequencing', u'protocols', u'developers', u'correction', u'tracking', u'tool', u'timely', u'troubleshoot', u'testing', u'functional', u'manual', u'automated', u'participate', u'ongoing', u'testing', u'coverage', u'planning', u'documentation', u'testing', u'validation', u'corrections', u'monitor', u'implementation', u'recurrence', u'operating', u'statistical', u'quality', u'testing', u'global', u'multi', u'teams', u'travel', u'skills', u'concepts', u'waterfall', u'agile', u'methodologies', u'debugging', u'skills', u'complex', u'automated', u'instrumentation', u'environment', u'hardware', u'mechanical', u'components', u'tracking', u'lifecycle', u'management', u'quality', u'organize', u'define', u'priorities', u'organize', u'supervision', u'aggressive', u'deadlines', u'ambiguity', u'analyze', u'complex', u'situations', u'concepts', u'technologies', u'verbal', u'skills', u'effectively', u'technical', u'clinical', u'diverse', u'strategy', u'clinical', u'chemistry', u'analyzer', u'laboratory', u'middleware', u'basic', u'automated', u'testing', u'biomedical', u'engineering', u'technologists', u'laboratory', u'technology', u'availability', u'click', u'attach'], [u'scientist', u'linux', u'asrc', u'scientist', u'linux', u'asrc', u'technology', u'solutions', u'subsidiary', u'asrc', u'engineering', u'technology', u'contracts', u'multiple', u'agencies', u'scientists', u'engineers', u'management', u'personnel', u'allows', u'solutions', u'complex', u'aeronautics', u'aviation', u'management', u'aviation', u'engineering', u'hughes', u'technical', u'technical', u'aviation', u'evaluation', u'engineering', u'management', u'technical', u'terminal', u'surveillance', u'programs', u'currently', u'scientist', u'travel', u'responsibilities', u'develops', u'technology', u'modifies', u'technical', u'complex', u'reviews', u'draft', u'conformity', u'completeness', u'testing', u'interface', u'hardware', u'regression', u'impact', u'reliability', u'm\n```\n\n----------------------------------------\n\nTITLE: Running Distributed LSA on Wikipedia Corpus in Python\nDESCRIPTION: This code shows how to perform distributed LSA on the English Wikipedia corpus using Gensim and print the resulting topics.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/dist_lsi.rst#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> # extract 400 LSI topics, using a cluster of nodes\n>>> lsi = gensim.models.lsimodel.LsiModel(corpus=mm, id2word=id2word, num_topics=400, chunksize=20000, distributed=True)\n>>>\n>>> # print the most contributing words (both positively and negatively) for each of the first ten topics\n>>> lsi.print_topics(10)\n```\n\n----------------------------------------\n\nTITLE: Using AuthorTopic Model with Scikit-Learn Pipeline in Python\nDESCRIPTION: This example demonstrates how to use the AuthorTopic model with Scikit-Learn's Pipeline for author clustering. It includes data preparation, model training, and clustering using MiniBatchKMeans.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/sklearn_api.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn import cluster\n\natm_texts = [\n    ['complier', 'system', 'computer'],\n    ['eulerian', 'node', 'cycle', 'graph', 'tree', 'path'],\n    ['graph', 'flow', 'network', 'graph'],\n    ['loading', 'computer', 'system'],\n    ['user', 'server', 'system'],\n    ['tree', 'hamiltonian'],\n    ['graph', 'trees'],\n    ['computer', 'kernel', 'malfunction', 'computer'],\n    ['server', 'system', 'computer'],\n]\natm_dictionary = Dictionary(atm_texts)\natm_corpus = [atm_dictionary.doc2bow(text) for text in atm_texts]\nauthor2doc = {'john': [0, 1, 2, 3, 4, 5, 6], 'jane': [2, 3, 4, 5, 6, 7, 8], 'jack': [0, 2, 4, 6, 8], 'jill': [1, 3, 5, 7]}\n\nmodel = AuthorTopicTransformer(id2word=atm_dictionary, author2doc=author2doc, num_topics=10, passes=100)\nmodel.fit(atm_corpus)\n\n# create and train clustering model\nclstr = cluster.MiniBatchKMeans(n_clusters=2)\nauthors_full = ['john', 'jane', 'jack', 'jill']\nclstr.fit(model.transform(authors_full))\n\n# stack together the two models in a pipeline\ntext_atm = Pipeline([('features', model,), ('cluster', clstr)])\nauthor_list = ['jane', 'jack', 'jill']\nret_val = text_atm.predict(author_list)\n\nprint(ret_val)\n```\n\n----------------------------------------\n\nTITLE: Initializing Basic Dependencies for Poincaré Embeddings\nDESCRIPTION: Sets up required Python imports and logging configuration for working with Poincaré embeddings. Includes path configuration for data and model files.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Tutorial.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport logging\nimport numpy as np\n\nfrom gensim.models.poincare import PoincareModel, PoincareKeyedVectors, PoincareRelations\n\nlogging.basicConfig(level=logging.INFO)\n\npoincare_directory = os.path.join(os.getcwd(), 'docs', 'notebooks', 'poincare')\ndata_directory = os.path.join(poincare_directory, 'data')\nwordnet_mammal_file = os.path.join(data_directory, 'wordnet_mammal_hypernyms.tsv')\n```\n\n----------------------------------------\n\nTITLE: Loading Wikipedia Corpus for Distributed LSA in Python\nDESCRIPTION: This code demonstrates how to load the English Wikipedia corpus for distributed LSA processing using Gensim.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/dist_lsi.rst#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> import logging\n>>> import gensim\n>>>\n>>> logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n>>>\n>>> # load id->word mapping (the dictionary)\n>>> id2word = gensim.corpora.Dictionary.load_from_text('wiki_en_wordids.txt')\n>>> # load corpus iterator\n>>> mm = gensim.corpora.MmCorpus('wiki_en_tfidf.mm')\n>>> # mm = gensim.corpora.MmCorpus('wiki_en_tfidf.mm.bz2') # use this if you compressed the TFIDF output\n>>>\n>>> print(mm)\nMmCorpus(3199665 documents, 100000 features, 495547400 non-zero entries)\n```\n\n----------------------------------------\n\nTITLE: Writing Wiki Text Processing Function\nDESCRIPTION: Defines a function to process and write Wikipedia articles to files while tracking titles.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/online_w2v_tutorial.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef write_wiki(wiki, name, titles = []):\n    with smart_open('{}.wiki'.format(name), 'wb') as f:\n        wiki.metadata = True\n        for text, (page_id, title) in wiki.get_texts():\n            if title not in titles:\n                f.write(b' '.join(text)+b'\\n')\n                titles.append(title)\n    return titles\n```\n\n----------------------------------------\n\nTITLE: Converting SciPy Sparse Matrix to Gensim Corpus (Python)\nDESCRIPTION: Shows how to convert a SciPy sparse matrix to a Gensim corpus and back using the Sparse2Corpus and corpus2csc utility functions.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_corpora_and_vector_spaces.ipynb#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport scipy.sparse\nscipy_sparse_matrix = scipy.sparse.random(5, 2)  # random sparse matrix as example\ncorpus = gensim.matutils.Sparse2Corpus(scipy_sparse_matrix)\nscipy_csc_matrix = gensim.matutils.corpus2csc(corpus)\n```\n\n----------------------------------------\n\nTITLE: Text Cleaning and Sentence Parsing Functions\nDESCRIPTION: Implementation of text preprocessing functions including contraction removal, symbol handling, and sentence splitting\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport re\n\ncontractions = re.compile(r\"'|-|\\\"\")\n# all non alphanumeric\nsymbols = re.compile(r'(\\W+)', re.U)\n# single character removal\nsingles = re.compile(r'(\\s\\S\\s)', re.I|re.U)\n# separators (any whitespace)\nseps = re.compile(r'\\s+')\n\n# cleaner (order matters)\ndef clean(text): \n    text = text.lower()\n    text = contractions.sub('', text)\n    text = symbols.sub(r' \\1 ', text)\n    text = singles.sub(' ', text)\n    text = seps.sub(' ', text)\n    return text\n\n# sentence splitter\nalteos = re.compile(r'([!\\?])')\ndef sentences(l):\n    l = alteos.sub(r' \\1 .', l).rstrip(\"(\\.)*\\n\")\n    return l.split(\".\")\n```\n\n----------------------------------------\n\nTITLE: Evaluating Annoy Indexer Performance in Python\nDESCRIPTION: This code evaluates the performance of Annoy indexer by varying the num_tree parameter and measuring initialization time, accuracy, and query time.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nannoy_x_values = []\nannoy_y_values_init = []\nannoy_y_values_accuracy = []\nannoy_y_values_query = []\n\nfor x in range(100, 401, 50):\n    annoy_x_values.append(x)\n    start_time = time.time()\n    annoy_index = AnnoyIndexer(model, x)\n    annoy_y_values_init.append(time.time() - start_time)\n    approximate_results = model.most_similar([model.wv.syn0norm[0]], topn=100, indexer=annoy_index)\n    top_words = [result[0] for result in approximate_results]\n    annoy_y_values_accuracy.append(len(set(top_words).intersection(exact_results)))\n    annoy_y_values_query.append(avg_query_time(annoy_index, queries=queries))\ncreate_evaluation_graph(annoy_x_values,\n                        annoy_y_values_init, \n                        annoy_y_values_accuracy, \n                        annoy_y_values_query, \n                        \"num_tree\")\n```\n\n----------------------------------------\n\nTITLE: Initializing a Memory-Friendly Corpus\nDESCRIPTION: Creates an instance of the memory-efficient corpus class and demonstrates that it doesn't load all documents into memory at once.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_corpora_and_vector_spaces.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ncorpus_memory_friendly = MyCorpus()  # doesn't load the corpus into memory!\nprint(corpus_memory_friendly)\n```\n\n----------------------------------------\n\nTITLE: Loading a Corpus from Matrix Market Format (Python)\nDESCRIPTION: Shows how to load a corpus iterator from a Matrix Market file using Gensim's MmCorpus class.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_corpora_and_vector_spaces.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ncorpus = corpora.MmCorpus('/tmp/corpus.mm')\n```\n\n----------------------------------------\n\nTITLE: Batch Topic Analysis\nDESCRIPTION: Processes and stores topic information for entire corpus\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_methods.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntopics = model.get_document_topics(corpus, per_word_topics=True)\nall_topics = [(doc_topics, word_topics, word_phis) for doc_topics, word_topics, word_phis in topics]\n```\n\n----------------------------------------\n\nTITLE: Computing Bigrams for NIPS Documents using Gensim in Python\nDESCRIPTION: Computes bigrams in the NIPS documents using Gensim's Phrases model. Bigrams that appear at least 20 times are added to the documents, allowing phrases like 'machine_learning' to be captured.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_lda.rst#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models import Phrases\n\n# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\nbigram = Phrases(docs, min_count=20)\nfor idx in range(len(docs)):\n    for token in bigram[docs[idx]]:\n        if '_' in token:\n            # Token is a bigram, add to document.\n            docs[idx].append(token)\n```\n\n----------------------------------------\n\nTITLE: Loading and Parsing NIPS Document Data\nDESCRIPTION: Crawls through NIPS paper directories, reads the document files, and creates a list of document texts with their corresponding IDs. Each document's whitespace is normalized.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os, re\nfrom smart_open import smart_open\n\n# Folder containing all NIPS papers.\ndata_dir = '/tmp/nipstxt/'  # Set this path to the data on your machine.\n\n# Folders containin individual NIPS papers.\nyrs = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\ndirs = ['nips' + yr for yr in yrs]\n\n# Get all document texts and their corresponding IDs.\ndocs = []\ndoc_ids = []\nfor yr_dir in dirs:\n    files = os.listdir(data_dir + yr_dir)  # List of filenames.\n    for filen in files:\n        # Get document ID.\n        (idx1, idx2) = re.search('[0-9]+', filen).span()  # Matches the indexes of the start end end of the ID.\n        doc_ids.append(yr_dir[4:] + '_' + str(int(filen[idx1:idx2])))\n        \n        # Read document text.\n        # Note: ignoring characters that cause encoding errors.\n        with smart_open(data_dir + yr_dir + '/' + filen, encoding='utf-8', 'rb') as fid:\n            txt = fid.read()\n            \n        # Replace any whitespace (newline, tabs, etc.) by a single space.\n        txt = re.sub('\\s', ' ', txt)\n        \n        docs.append(txt)\n```\n\n----------------------------------------\n\nTITLE: Adjusting Chain Variance in LdaSeq Model with Gensim\nDESCRIPTION: This code shows how to create an LdaSeq model with a custom chain_variance value, which affects how quickly topics evolve over time.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/ldaseqmodel.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nldaseq_chain = ldaseqmodel.LdaSeqModel(corpus=corpus, id2word=dictionary, time_slice=time_slice, num_topics=5, chain_variance=0.05)\n```\n\n----------------------------------------\n\nTITLE: Creating Dictionary and Corpus - Python\nDESCRIPTION: Builds a gensim Dictionary and Corpus from the processed texts\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence-movies.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\ndictionary = Dictionary(texts)\ncorpus = [dictionary.doc2bow(text) for text in texts]\n```\n\n----------------------------------------\n\nTITLE: Text Preparation Function\nDESCRIPTION: Defines a function to prepare text by lowercasing and tokenizing sentences\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/wikinews-bigram-en.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# prepare a text\ndef prepare(txt):\n    # lower case\n    txt = txt.lower()\n    return [tokenizer.tokenize(sent) \n            for sent in sent_tokenize(txt, language=LANG)]\n```\n\n----------------------------------------\n\nTITLE: Test Set Evaluation and Visualization\nDESCRIPTION: Code to evaluate the model on test data and visualize results using matplotlib\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nrevtest = list(YelpReviews(\"test\"))\nprobs = docprob( [r['x'] for r in revtest], starmodels )\n\nimport matplotlib\n%matplotlib inline\n\nprobpos = pd.DataFrame({\"out-of-sample prob positive\":probs[[3,4]].sum(axis=1), \n                        \"true stars\":[r['y'] for r in revtest]})\nprobpos.boxplot(\"out-of-sample prob positive\",by=\"true stars\", figsize=(12,5))\n```\n\n----------------------------------------\n\nTITLE: Displaying WordNet Reconstruction Results in Python\nDESCRIPTION: Calling the display_results function to show the evaluation results for the WordNet reconstruction task in a tabular format. The function organizes metrics by model type and dimension for easy comparison.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Evaluation.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndisplay_results('WordNet Reconstruction', reconstruction_results)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Documents and Creating Dictionary\nDESCRIPTION: Preprocesses the documents using Gensim's preprocess_string function and creates a Dictionary from the training data. The preprocessed documents are then converted to bag-of-words format using doc2bow.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/pivoted_document_length_normalisation.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.parsing.preprocessing import preprocess_string\nfrom gensim.corpora import Dictionary\n\nid2word = Dictionary([preprocess_string(doc) for doc in X_train])\ntrain_corpus = [id2word.doc2bow(preprocess_string(doc)) for doc in X_train]\ntest_corpus = [id2word.doc2bow(preprocess_string(doc)) for doc in X_test]\n```\n\n----------------------------------------\n\nTITLE: Printing Dictionary Token-to-ID Mapping in Python\nDESCRIPTION: Displays the mapping between tokens (words) and their corresponding integer IDs in the dictionary.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_core_concepts.rst#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npprint.pprint(dictionary.token2id)\n```\n\n----------------------------------------\n\nTITLE: Evaluating WordRank Model on Word Analogies in Python\nDESCRIPTION: This snippet demonstrates how to evaluate a WordRank model on word analogy tasks using Gensim's accuracy method. It provides a path to a file containing word analogy questions for testing semantic relationships.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/WordRank_wrapper_quickstart.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nword_analogies_file = 'datasets/questions-words.txt'\nmodel.accuracy(word_analogies_file)\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing the 20 Newsgroups Dataset in Python\nDESCRIPTION: Creates a NewsgroupCorpus instance, filters extreme words (removing rare and common terms), and displays corpus and dictionary statistics.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence_model_selection.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\ncorpus = NewsgroupCorpus()\ncorpus.dictionary.filter_extremes(no_below=5, no_above=0.8)\ndictionary = corpus.dictionary\nprint(len(corpus))\nprint(dictionary)\n```\n\n----------------------------------------\n\nTITLE: Creating Parallel Processes in Python\nDESCRIPTION: This snippet demonstrates how to create and run two parallel processes that share the same index file using Python's multiprocessing module.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\np1 = Process(target=f, args=('1',))\np1.start()\np1.join()\np2 = Process(target=f, args=('2',))\np2.start()\np2.join()\n```\n\n----------------------------------------\n\nTITLE: Selecting the Best Author-Topic Model in Python\nDESCRIPTION: This code selects the model with the highest topic coherence from the previously trained models.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nmodel, tc = max(model_list, key=lambda x: x[1])\nprint('Topic coherence: %.3e' %tc)\n```\n\n----------------------------------------\n\nTITLE: Performing Latent Semantic Analysis on Wikipedia\nDESCRIPTION: Code for creating an LSA model from the Wikipedia corpus. This snippet builds a Latent Semantic Analysis model with 400 topics using the default one-pass algorithm, then prints the top 10 topics with their most significant words and their contribution weights.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/wiki.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> # extract 400 LSI topics; use the default one-pass algorithm\n>>> lsi = gensim.models.lsimodel.LsiModel(corpus=mm, id2word=id2word, num_topics=400)\n>>>\n>>> # print the most contributing words (both positively and negatively) for each of the first ten topics\n>>> lsi.print_topics(10)\ntopic #0(332.762): 0.425*\"utc\" + 0.299*\"talk\" + 0.293*\"page\" + 0.226*\"article\" + 0.224*\"delete\" + 0.216*\"discussion\" + 0.205*\"deletion\" + 0.198*\"should\" + 0.146*\"debate\" + 0.132*\"be\"\ntopic #1(201.852): 0.282*\"link\" + 0.209*\"he\" + 0.145*\"com\" + 0.139*\"his\" + -0.137*\"page\" + -0.118*\"delete\" + 0.114*\"blacklist\" + -0.108*\"deletion\" + -0.105*\"discussion\" + 0.100*\"diff\"\ntopic #2(191.991): -0.565*\"link\" + -0.241*\"com\" + -0.238*\"blacklist\" + -0.202*\"diff\" + -0.193*\"additions\" + -0.182*\"users\" + -0.158*\"coibot\" + -0.136*\"user\" + 0.133*\"he\" + -0.130*\"resolves\"\ntopic #3(141.284): -0.476*\"image\" + -0.255*\"copyright\" + -0.245*\"fair\" + -0.225*\"use\" + -0.173*\"album\" + -0.163*\"cover\" + -0.155*\"resolution\" + -0.141*\"licensing\" + 0.137*\"he\" + -0.121*\"copies\"\ntopic #4(130.909): 0.264*\"population\" + 0.246*\"age\" + 0.243*\"median\" + 0.213*\"income\" + 0.195*\"census\" + -0.189*\"he\" + 0.184*\"households\" + 0.175*\"were\" + 0.167*\"females\" + 0.166*\"males\"\ntopic #5(120.397): 0.304*\"diff\" + 0.278*\"utc\" + 0.213*\"you\" + -0.171*\"additions\" + 0.165*\"talk\" + -0.159*\"image\" + 0.159*\"undo\" + 0.155*\"www\" + -0.152*\"page\" + 0.148*\"contribs\"\ntopic #6(115.414): -0.362*\"diff\" + -0.203*\"www\" + 0.197*\"you\" + -0.180*\"undo\" + -0.180*\"kategori\" + 0.164*\"users\" + 0.157*\"additions\" + -0.150*\"contribs\" + -0.139*\"he\" + -0.136*\"image\"\ntopic #7(111.440): 0.429*\"kategori\" + 0.276*\"categoria\" + 0.251*\"category\" + 0.207*\"kategorija\" + 0.198*\"kategorie\" + -0.188*\"diff\" + 0.163*\"категория\" + 0.153*\"categoría\" + 0.139*\"kategoria\" + 0.133*\"categorie\"\ntopic #8(109.907): 0.385*\"album\" + 0.224*\"song\" + 0.209*\"chart\" + 0.204*\"band\" + 0.169*\"released\" + 0.151*\"music\" + 0.142*\"diff\" + 0.141*\"vocals\" + 0.138*\"she\" + 0.132*\"guitar\"\ntopic #9(102.599): -0.237*\"league\" + -0.214*\"he\" + -0.180*\"season\" + -0.174*\"football\" + -0.166*\"team\" + 0.159*\"station\" + -0.137*\"played\" + -0.131*\"cup\" + 0.131*\"she\" + -0.128*\"utc\"\n```\n\n----------------------------------------\n\nTITLE: Poincaré Embeddings for Graph/Taxonomy Representation\nDESCRIPTION: Demonstrates embedding a graph or taxonomy using Poincaré embeddings, similar to word2vec embedding words\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/CHANGELOG.md#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models.poincare import PoincareRelations, PoincareModel\nfrom gensim.test.utils import datapath\n\ndata = PoincareRelations(datapath('poincare_hypernyms.tsv'))\nmodel = PoincareModel(data)\nmodel.kv.most_similar(\"cat.n.01\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging for Python DTM Analysis\nDESCRIPTION: This code sets up logging for the DTM analysis. It configures the logger to display debug-level messages, which is useful for tracking the progress and debugging issues during the DTM process.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/dtm_example.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nlogger = logging.getLogger()\nlogger.setLevel(logging.DEBUG)\nlogging.debug(\"test\")\n```\n\n----------------------------------------\n\nTITLE: Topic Distribution Calculation\nDESCRIPTION: Extracts topic distributions and terms from the LDA model\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_network.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# get topic distributions\ntopic_dist = lda_fake.state.get_lambda()\n\n# get topic terms\nnum_words = 50\ntopic_terms = [{w for (w, _) in lda_fake.show_topic(topic, topn=num_words)} for topic in range(topic_dist.shape[0])]\n```\n\n----------------------------------------\n\nTITLE: Results Visualization Function in Python\nDESCRIPTION: Matplotlib-based plotting function to visualize training times and accuracy metrics across different models\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef plot(ax, data, corpus_name='brown'):\n    width = 0.25\n    pos = [(i, i + width, i + 2*width) for i in range(len(data))]\n    colors = ['#EE3224', '#F78F1E', '#FFC222']\n    acc_ax = ax.twinx()\n    ax.bar(pos[0], data[0], width, alpha=0.5, color=colors)\n    acc_ax.bar(pos[1], data[1], width, alpha=0.5, color=colors)\n    acc_ax.bar(pos[2], data[2], width, alpha=0.5, color=colors)\n    ax.set_ylabel('Training time (s)')\n    acc_ax.set_ylabel('Accuracy (%)')\n    ax.set_title(corpus_name)\n    acc_ax.set_xticks([p[0] + 1.5 * width for p in pos])\n    acc_ax.set_xticklabels(['Training Time', 'Semantic Accuracy', 'Syntactic Accuracy'])\n    proxies = [ax.bar([0], [0], width=0, color=c, alpha=0.5)[0] for c in colors]\n    models = ('Gensim', 'FastText', 'FastText (no-ngrams)')\n    ax.legend((proxies), models, loc='upper left')\n    ax.set_xlim(pos[0][0]-width, pos[-1][0]+width*4)\n    ax.set_ylim([0, max(data[0])*1.1])\n    acc_ax.set_ylim([0, max(data[1] + data[2])*1.1])\n    plt.grid()\n```\n\n----------------------------------------\n\nTITLE: Analyzing document-topic proportions in Dynamic Topic Model\nDESCRIPTION: Demonstrates how to check the topic proportions for a specific document (number 558) in the trained model.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/ldaseqmodel.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Code for doc_topics function not provided in the snippet\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Corpus Class for Newsgroup Data Processing in Python\nDESCRIPTION: Implements a NewsgroupCorpus class that extends TextCorpus for processing the 20 Newsgroups dataset. Includes helper functions to strip headers, footers, and quote sections from newsgroup text format.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence_model_selection.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn import datasets\n\n\nclass NewsgroupCorpus(TextCorpus):\n    \"\"\"Parse 20 Newsgroups dataset.\"\"\"\n    \n    def __init__(self, *args, **kwargs):\n        super(NewsgroupCorpus, self).__init__(\n            datasets.fetch_20newsgroups(subset='all'), *args, **kwargs)\n\n    def getstream(self):\n        for doc in self.input.data:\n            yield doc  # already unicode\n\n    def preprocess_text(self, text):\n        body = extract_body(text)\n        return super(NewsgroupCorpus, self).preprocess_text(body)\n\n    \ndef extract_body(text):\n    return strip_newsgroup_header(\n        strip_newsgroup_footer(\n            strip_newsgroup_quoting(text)))\n\n\ndef strip_newsgroup_header(text):\n    \"\"\"Given text in \"news\" format, strip the headers, by removing everything\n    before the first blank line.\n    \"\"\"\n    _before, _blankline, after = text.partition('\\n\\n')\n    return after\n\n\n_QUOTE_RE = re.compile(r'(writes in|writes:|wrote:|says:|said:'\n                       r'|^In article|^Quoted from|^\\||^>)')\ndef strip_newsgroup_quoting(text):\n    \"\"\"Given text in \"news\" format, strip lines beginning with the quote\n    characters > or |, plus lines that often introduce a quoted section\n    (for example, because they contain the string 'writes:'.)\n    \"\"\"\n    good_lines = [line for line in text.split('\\n')\n                  if not _QUOTE_RE.search(line)]\n    return '\\n'.join(good_lines)\n\n\n_PGP_SIG_BEGIN = \"-----BEGIN PGP SIGNATURE-----\"\ndef strip_newsgroup_footer(text):\n    \"\"\"Given text in \"news\" format, attempt to remove a signature block.\"\"\"\n    try:\n        return text[:text.index(_PGP_SIG_BEGIN)]\n    except ValueError:\n        return text\n```\n\n----------------------------------------\n\nTITLE: Reclustering EnsembleLda Topics with Custom Parameters in Python\nDESCRIPTION: Applies reclustering to the trained EnsembleLda model with adjusted min_samples and eps parameters to fine-tune topic extraction. This step helps find the optimal balance for identifying meaningful topics in the small, redundant Opinosis dataset.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/ensemble_lda_with_opinosis.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nelda.recluster(min_samples=55, eps=0.14)\npretty_print_topics()\n```\n\n----------------------------------------\n\nTITLE: Tokenizing and Preprocessing NIPS Documents in Python\nDESCRIPTION: Tokenizes the NIPS documents using NLTK's RegexpTokenizer, converts text to lowercase, removes numeric tokens and single-character tokens. This step prepares the text for further processing in the LDA model.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_lda.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom nltk.tokenize import RegexpTokenizer\n\ntokenizer = RegexpTokenizer(r'\\w+')\nfor idx in range(len(docs)):\n    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n\n# Remove numbers, but not words that contain numbers.\ndocs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n\n# Remove words that are only one character.\ndocs = [[token for token in doc if len(token) > 1] for doc in docs]\n```\n\n----------------------------------------\n\nTITLE: Bigram Pattern Analysis\nDESCRIPTION: Analyzes bigrams with same words but different stopwords\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/wikinews-bigram-en.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# did we found any bigram with same words but different stopwords\nimport collections\nby_terms = collections.defaultdict(set)\nfor ngram, score in bigram_ct.export_phrases(corpus):\n    grams = ngram.split()\n    by_terms[(grams[0], grams[-1])].add(ngram)\nfor k, v  in by_terms.items():\n    if len(v) > 1:\n        print(b\"-\".join(k).decode(\"utf-8\"),\" : \", [w.decode(\"utf-8\") for w in v])\n```\n\n----------------------------------------\n\nTITLE: Creating Parallel Processes in Python\nDESCRIPTION: Demonstrates how to create and run two parallel processes that share the same index file using Python's multiprocessing module.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_annoy.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\np1 = Process(target=f, args=('1',))\np1.start()\np1.join()\np2 = Process(target=f, args=('2',))\np2.start()\np2.join()\n```\n\n----------------------------------------\n\nTITLE: Extracting Most Similar Words from Word2Vec Model in Python\nDESCRIPTION: This snippet extracts the top 100 most similar words to the first word vector in the model's vocabulary.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nexact_results = [element[0] for element in model.most_similar([model.wv.syn0norm[0]], topn=100)]\n```\n\n----------------------------------------\n\nTITLE: Creating a Text Corpus in Python\nDESCRIPTION: Defines a corpus as a list of documents, each being a single sentence string.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_core_concepts.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntext_corpus = [\n    \"Human machine interface for lab abc computer applications\",\n    \"A survey of user opinion of computer system response time\",\n    \"The EPS user interface management system\",\n    \"System and human system engineering testing of EPS\",\n    \"Relation of user perceived response time to error measurement\",\n    \"The generation of random binary unordered trees\",\n    \"The intersection graph of paths in trees\",\n    \"Graph minors IV Widths of trees and well quasi ordering\",\n    \"Graph minors A survey\",\n]\n```\n\n----------------------------------------\n\nTITLE: Initializing and Training Doc2Vec Model in Python\nDESCRIPTION: This snippet shows the initialization and training of a Doc2Vec model using Gensim. It includes model parameters and training progress logs.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nDoc2Vec<dm/m,d100,n5,w10,mc2,t2>\n```\n\n----------------------------------------\n\nTITLE: Calculating Word Movers Distance with Gensim\nDESCRIPTION: Computes the Word Movers Distance between two preprocessed sentences using gensim's word vectors. The distance metric indicates semantic similarity between texts.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_fasttext.rst#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndistance = wv.wmdistance(sentence_obama, sentence_president)\nprint(f\"Word Movers Distance is {distance} (lower means closer)\")\n```\n\n----------------------------------------\n\nTITLE: Defining Document Preprocessing Function for Yelp Reviews\nDESCRIPTION: Creates a preprocessing function that prepares text documents for WMD by converting to lowercase, tokenizing with NLTK, and removing stopwords and non-alphabetic characters. This is used for preparing Yelp review data.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/WMD_tutorial.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Pre-processing a document.\n\nfrom nltk import word_tokenize\ndownload('punkt')  # Download data for tokenizer.\n\ndef preprocess(doc):\n    doc = doc.lower()  # Lower the text.\n    doc = word_tokenize(doc)  # Split into words.\n    doc = [w for w in doc if not w in stop_words]  # Remove stopwords.\n    doc = [w for w in doc if w.isalpha()]  # Remove numbers and punctuation.\n    return doc\n```\n\n----------------------------------------\n\nTITLE: Processing Wiki Files\nDESCRIPTION: Processes both old and new Wikipedia dumps using the write_wiki function.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/online_w2v_tutorial.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nold_titles = write_wiki(old, 'old')\nall_titles = write_wiki(new, 'new', old_titles)\n```\n\nLANGUAGE: python\nCODE:\n```\noldwiki, newwiki = [LineSentence(f+'.wiki') for f in ['old', 'new']]\n```\n\n----------------------------------------\n\nTITLE: Yelp Review Data Generator\nDESCRIPTION: Generator function to read and process Yelp review data from zip files, yielding reviews with star ratings and tokenized sentences\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom zipfile import ZipFile\nimport json\n\ndef YelpReviews(label):\n    with ZipFile(\"yelp_%s_set.zip\"%label, 'r') as zf:\n        with zf.open(\"yelp_%s_set/yelp_%s_set_review.json\"%(label,label)) as f:\n            for line in f:\n                if type(line) is bytes:\n                    line = line.decode('utf-8')\n                rev = json.loads(line)\n                yield {'y':rev['stars'],\\\n                       'x':[clean(s).split() for s in sentences(rev['text'])]}\n```\n\n----------------------------------------\n\nTITLE: Loading SpaCy NLP Pipeline\nDESCRIPTION: Imports and initializes the SpaCy English language model for text processing tasks such as tokenization, lemmatization, and named entity recognition.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nnlp = spacy.load('en')\n```\n\n----------------------------------------\n\nTITLE: Initializing NMF Model in Python using Gensim\nDESCRIPTION: Example showing how to initialize and use the new Fast Online NMF (Non-negative Matrix Factorization) model with the text8 dataset. Demonstrates corpus preparation, model training, and topic extraction.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/CHANGELOG.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models.nmf import Nmf\nfrom gensim.corpora import Dictionary\nimport gensim.downloader as api\n\ntext8 = api.load('text8')\n\ndictionary = Dictionary(text8)\ndictionary.filter_extremes()\n\ncorpus = [\n    dictionary.doc2bow(doc) for doc in text8\n]\n\nnmf = Nmf(\n    corpus=corpus,\n    num_topics=5,\n    id2word=dictionary,\n    chunksize=2000,\n    passes=5,\n    random_state=42,\n)\n\nnmf.show_topics()\n```\n\n----------------------------------------\n\nTITLE: Plotting Model Evaluation Results with Matplotlib\nDESCRIPTION: Creates a figure with three subplots to visualize evaluation data for different corpus types. Uses matplotlib to generate bar plots comparing model performance metrics.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfig = plt.figure(figsize=(10,15))\nfor corpus, subplot in zip(sorted(evaluation_data.keys()), [311, 312, 313]):\n    ax = fig.add_subplot(subplot)\n    plot(ax, evaluation_data[corpus], corpus)\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Analyzing New Documents with LdaSeq Model in Python\nDESCRIPTION: This code shows how to analyze a new document not in the training set by converting it to the model's dictionary format and passing it to the LdaSeq model.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/ldaseqmodel.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndoc_football_1 = ['economy', 'bank', 'mobile', 'phone', 'markets', 'buy', 'football', 'united', 'giggs']\ndoc_football_1 = dictionary.doc2bow(doc_football_1)\ndoc_football_1 = ldaseq[doc_football_1]\nprint (doc_football_1)\n```\n\n----------------------------------------\n\nTITLE: Serializing a Corpus in Matrix Market Format (Python)\nDESCRIPTION: Creates a toy corpus of 2 documents and serializes it to disk in Matrix Market format using Gensim's MmCorpus class.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_corpora_and_vector_spaces.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ncorpus = [[(1, 0.5)], []]\n\ncorpora.MmCorpus.serialize('/tmp/corpus.mm', corpus)\n```\n\n----------------------------------------\n\nTITLE: Listing Available Resources in Gensim-Data using Python\nDESCRIPTION: Retrieves and prints information about all available corpora and models in gensim-data.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_downloader_api.rst#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport json\ninfo = api.info()\nprint(json.dumps(info, indent=4))\n```\n\n----------------------------------------\n\nTITLE: Creating Example Sentences for Similarity Comparison in Python\nDESCRIPTION: Defines three example sentences to be used for demonstrating document similarity calculations.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsentence_obama = 'Obama speaks to the media in Illinois'.lower().split()\nsentence_president = 'The president greets the press in Chicago'.lower().split()\nsentence_orange = 'Having a tough time finding an orange juice press machine?'.lower().split()\n```\n\n----------------------------------------\n\nTITLE: Evaluating TF-IDF without Pivoted Normalization\nDESCRIPTION: Applies the standard TF-IDF approach without pivoted document length normalization and prints the model accuracy.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/pivoted_document_length_normalisation.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nparams = {}\nmodel_accuracy, doc_scores = get_tfidf_scores(params)\nprint(model_accuracy)\n```\n\n----------------------------------------\n\nTITLE: Loading the Opinosis Corpus in Python\nDESCRIPTION: Creates an OpinosisCorpus object by loading data from the specified path. This process includes text preprocessing with PorterStemmer and stopword removal using NLTK, converting the raw text into a format suitable for topic modeling.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/ensemble_lda_with_opinosis.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nopinosis = OpinosisCorpus(path)\n```\n\n----------------------------------------\n\nTITLE: Network Edge Calculation\nDESCRIPTION: Calculates distances between topics and creates network edges based on a threshold\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_network.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom scipy.spatial.distance import pdist, squareform\nfrom gensim.matutils import jensen_shannon\nimport networkx as nx\nimport itertools as itt\n\n# calculate distance matrix using the input distance metric\ndef distance(X, dist_metric):\n    return squareform(pdist(X, lambda u, v: dist_metric(u, v)))\n\ntopic_distance = distance(topic_dist, jensen_shannon)\n\n# store edges b/w every topic pair along with their distance\nedges = [(i, j, {'weight': topic_distance[i, j]})\n         for i, j in itt.combinations(range(topic_dist.shape[0]), 2)]\n\n# keep edges with distance below the threshold value\nk = np.percentile(np.array([e[2]['weight'] for e in edges]), 20)\nedges = [e for e in edges if e[2]['weight'] < k]\n```\n\n----------------------------------------\n\nTITLE: Loading Trained Model\nDESCRIPTION: Loads the previously saved LDA model\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_network.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlda_fake = LdaModel.load('lda_35')\n```\n\n----------------------------------------\n\nTITLE: Document Topic Analysis\nDESCRIPTION: Analyzes topic distribution for sample documents with different contexts\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_methods.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nbow_water = ['bank','water','bank']\nbow_finance = ['bank','finance','bank']\n\nbow = model.id2word.doc2bow(bow_water) # convert to bag of words format first\ndoc_topics, word_topics, phi_values = model.get_document_topics(bow, per_word_topics=True)\n\nword_topics\n```\n\n----------------------------------------\n\nTITLE: Analyzing Annoy Index Performance\nDESCRIPTION: Builds a dataset to analyze the relationship between num_trees parameter, initialization time, and accuracy of the Annoy index.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_annoy.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nexact_results = [element[0] for element in wv.most_similar([normed_vectors[0]], topn=100)]\n\nx_values = []\ny_values_init = []\ny_values_accuracy = []\n\nfor x in range(1, 300, 10):\n    x_values.append(x)\n    start_time = time.time()\n    annoy_index = AnnoyIndexer(model, x)\n    y_values_init.append(time.time() - start_time)\n    approximate_results = wv.most_similar([normed_vectors[0]], topn=100, indexer=annoy_index)\n    top_words = [result[0] for result in approximate_results]\n    y_values_accuracy.append(len(set(top_words).intersection(exact_results)))\n```\n\n----------------------------------------\n\nTITLE: Calculating Correlations - Python\nDESCRIPTION: Computes Pearson correlations between coherence measures and human scores\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence-movies.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfinal_scores = [\n    score for i, score in enumerate(human_scores)\n    if i not in invalid_topic_indices\n]\nlen(final_scores)\n\nfor our_scores in (u_mass, c_v, c_uci, c_npmi):\n    print(pearsonr(our_scores, final_scores)[0])\n```\n\n----------------------------------------\n\nTITLE: Normalizing Word2Vec Vectors for Improved WMD Calculation\nDESCRIPTION: Normalizes the word vectors in the Word2Vec model to equal length using init_sims(), which improves WMD calculations by ensuring they use Euclidean distance properly. The function times this operation and recalculates the distance.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/WMD_tutorial.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Normalizing word2vec vectors.\nstart = time()\n\nmodel.init_sims(replace=True)  # Normalizes the vectors in the word2vec class.\n\ndistance = model.wmdistance(sentence_obama, sentence_president)  # Compute WMD as normal.\nprint('distance: %r', distance)\nprint('Cell took %.2f seconds to run.' %(time() - start))\n```\n\n----------------------------------------\n\nTITLE: Importing Doc2Vec Model from Gensim in Python\nDESCRIPTION: This snippet demonstrates how to import the Doc2Vec model from Gensim's scikit-learn API. It's necessary for using Doc2Vec in a scikit-learn pipeline.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/sklearn_api.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.sklearn_api import D2VTransformer\n```\n\n----------------------------------------\n\nTITLE: Running Benchmark and Displaying Results for Corpus-to-Corpus Similarity in Python\nDESCRIPTION: This code executes the benchmark for corpus-to-corpus similarity using previously generated configurations and a new set of dense matrices. It processes and displays the results, showing mean values and standard deviations for various metrics.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_benchmark.ipynb#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nnonzero_limits = [1000]\ndense_matrices = []\nfor (model, dictionary), nonzero_limit in tqdm(\n        list(product(zip(models, dictionaries), nonzero_limits)), desc=\"matrices\"):\n    annoy = AnnoyIndexer(model, 1)\n    index = WordEmbeddingSimilarityIndex(model, kwargs={\"indexer\": annoy})\n    matrix = SparseTermSimilarityMatrix(index, dictionary, nonzero_limit=nonzero_limit)\n    matrices.append((matrix, dictionary, nonzero_limit))\n    del annoy\n\nconfigurations = product(matrices + dense_matrices, corpora + [full_corpus], normalization, repetitions)\nresults = benchmark_results(benchmark, configurations, \"matrix_speed.inner-product_results.corpus_corpus\")\n\ndf = pd.DataFrame(results)\ndf[\"speed\"] = df.corpus_actual_size**2 / df.duration\ndel df[\"corpus_actual_size\"]\ndf = df.groupby([\"dictionary_size\", \"corpus_size\", \"nonzero_limit\", \"normalized\"])\n\ndef display(df):\n    df[\"duration\"] = [timedelta(0, duration) for duration in df[\"duration\"]]\n    df[\"speed\"] = [\"%.02f Kdoc pairs / s\" % (speed / 1000) for speed in df[\"speed\"]]\n    return df\n\ndisplay(df.mean()).loc[\n    [1000, 100000], :, [1, 10, 100, 1000], :].loc[\n    :, [\"duration\", \"corpus_nonzero\", \"matrix_nonzero\", \"speed\"]]\n\ndisplay(df.apply(lambda x: (x - x.mean()).std())).loc[\n    [1000, 100000], :, [1, 100], :].loc[\n    :, [\"duration\", \"corpus_nonzero\", \"matrix_nonzero\", \"speed\"]]\n```\n\n----------------------------------------\n\nTITLE: Creating Dictionary and Corpus\nDESCRIPTION: Converts the text data into a Gensim dictionary and document-term matrix format required for topic modeling.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence_tutorial.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndictionary = Dictionary(texts)\ncorpus = [dictionary.doc2bow(text) for text in texts]\n```\n\n----------------------------------------\n\nTITLE: Training Another Doc2Vec Model Configuration in Python\nDESCRIPTION: This snippet shows the initialization of another Doc2Vec model with different parameters, followed by the start of its training process.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nDoc2Vec<dm/c,d100,n5,w5,mc2,t2>\n```\n\n----------------------------------------\n\nTITLE: Filtering Invalid Topics - Python\nDESCRIPTION: Identifies and removes topics containing terms not present in the dictionary\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence-movies.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ninvalid_topic_indices = set(\n    i for i, topic in enumerate(topics)\n    if any(t not in dictionary.token2id for t in topic)\n)\nprint(\"Topics with out-of-vocab terms: %s\" % ', '.join(map(str, invalid_topic_indices)))\nusable_topics = [topic for i, topic in enumerate(topics) if i not in invalid_topic_indices]\n```\n\n----------------------------------------\n\nTITLE: Running Benchmark and Displaying Results for Document-to-Corpus Similarity in Python\nDESCRIPTION: This code executes the benchmark for document-to-corpus similarity using previously generated configurations. It processes and displays the results, showing mean values and standard deviations for various metrics.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_benchmark.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nconfigurations = product(matrices, corpora, normalization, repetitions)\nresults = benchmark_results(benchmark, configurations, \"matrix_speed.inner-product_results.doc_corpus\")\n\ndf = pd.DataFrame(results)\ndf[\"speed\"] = df.corpus_actual_size**2 / df.duration\ndel df[\"corpus_actual_size\"]\ndf = df.groupby([\"dictionary_size\", \"corpus_size\", \"nonzero_limit\", \"normalized\"])\n\ndef display(df):\n    df[\"duration\"] = [timedelta(0, duration) for duration in df[\"duration\"]]\n    df[\"speed\"] = [\"%.02f Kdoc pairs / s\" % (speed / 1000) for speed in df[\"speed\"]]\n    return df\n\ndisplay(df.mean()).loc[\n    [1000, 100000], :, [1, 100], :].loc[\n    :, [\"duration\", \"corpus_nonzero\", \"matrix_nonzero\", \"speed\"]]\n\ndisplay(df.apply(lambda x: (x - x.mean()).std())).loc[\n    [1000, 100000], :, [1, 100], :].loc[\n    :, [\"duration\", \"corpus_nonzero\", \"matrix_nonzero\", \"speed\"]]\n```\n\n----------------------------------------\n\nTITLE: Printing LSI Topics in Python using Gensim\nDESCRIPTION: Prints the top terms and their weights for 2 LSI topics using the print_topics() method. Shows how terms are weighted and grouped into latent topics.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_topics_and_transformations.rst#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlsi_model.print_topics(2)\n```\n\n----------------------------------------\n\nTITLE: Displaying WMD Illustration using Matplotlib in Python\nDESCRIPTION: This snippet loads and displays an image illustrating the Word Mover's Distance concept using matplotlib.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_wmd.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Image from https://vene.ro/images/wmd-obama.png\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimg = mpimg.imread('wmd-obama.png')\nimgplot = plt.imshow(img)\nplt.axis('off')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Converting NumPy Matrix to Gensim Corpus (Python)\nDESCRIPTION: Demonstrates how to convert a NumPy matrix to a Gensim corpus using the Dense2Corpus utility function.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_corpora_and_vector_spaces.ipynb#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport gensim\nimport numpy as np\nnumpy_matrix = np.random.randint(10, size=[5, 2])  # random matrix as an example\ncorpus = gensim.matutils.Dense2Corpus(numpy_matrix)\n# numpy_matrix = gensim.matutils.corpus2dense(corpus, num_terms=number_of_corpus_features)\n```\n\n----------------------------------------\n\nTITLE: Converting Wikipedia Dump Files\nDESCRIPTION: Creates WikiCorpus objects from the downloaded dump files for processing.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/online_w2v_tutorial.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nold, new = [WikiCorpus('enwiki-{}-pages-articles.xml.bz2'.format(ymd)) for ymd in ['20101011', '20160820']]\n```\n\n----------------------------------------\n\nTITLE: Network Graph Layout Setup\nDESCRIPTION: Initializes the network graph and calculates node positions\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_network.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport plotly.offline as py\nfrom plotly.graph_objs import *\nimport plotly.figure_factory as ff\n\npy.init_notebook_mode()\n\n# add nodes and edges to graph layout\nG = nx.Graph()\nG.add_nodes_from(range(topic_dist.shape[0]))\nG.add_edges_from(edges)\n\ngraph_pos = nx.spring_layout(G)\n```\n\n----------------------------------------\n\nTITLE: Setting Up LDA Training with Shell Logging\nDESCRIPTION: Code that sets up logging of LDA training metrics to the shell instead of Visdom visualization, configuring logging levels and defining various metrics as callbacks for training progress monitoring.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Training_visualizations.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nfrom gensim.models.callbacks import CoherenceMetric, DiffMetric, PerplexityMetric, ConvergenceMetric\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\n\n# define perplexity callback for hold_out and test corpus\npl_holdout = PerplexityMetric(corpus=holdout_corpus, logger=\"shell\", title=\"Perplexity (hold_out)\")\npl_test = PerplexityMetric(corpus=test_corpus, logger=\"shell\", title=\"Perplexity (test)\")\n\n# define other remaining metrics available\nch_umass = CoherenceMetric(corpus=training_corpus, coherence=\"u_mass\", logger=\"shell\", title=\"Coherence (u_mass)\")\ndiff_kl = DiffMetric(distance=\"kullback_leibler\", logger=\"shell\", title=\"Diff (kullback_leibler)\")\nconvergence_jc = ConvergenceMetric(distance=\"jaccard\", logger=\"shell\", title=\"Convergence (jaccard)\")\n\ncallbacks = [pl_holdout, pl_test, ch_umass, diff_kl, convergence_jc]\n\n# training LDA model\nmodel = ldamodel.LdaModel(corpus=training_corpus, id2word=dictionary, num_topics=35, passes=2, eval_every=None, callbacks=callbacks)\n```\n\n----------------------------------------\n\nTITLE: Defining SentimentDocument Data Structure\nDESCRIPTION: Creates a named tuple for storing document data including words, tags, split type, and sentiment value.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport collections\n\nSentimentDocument = collections.namedtuple('SentimentDocument', 'words tags split sentiment')\n```\n\n----------------------------------------\n\nTITLE: Using Word2Vec Standalone Script Example - Python\nDESCRIPTION: Example command for running the standalone word2vec script with command line arguments matching the original C implementation. Shows usage with sample parameters for training size, window and sample rate.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/CHANGELOG.md#2025-04-21_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\npython -m gensim.scripts.word2vec_standalone -train data.txt -output trained_vec.txt -size 200 -window 2 -sample 1e-4\n```\n\n----------------------------------------\n\nTITLE: Creating Bag-of-Words Corpus\nDESCRIPTION: Converts the processed documents into a bag-of-words representation using the filtered dictionary, creating the vectorized corpus needed for the author-topic model.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Vectorize data.\n\n# Bag-of-words representation of the documents.\ncorpus = [dictionary.doc2bow(doc) for doc in docs]\n```\n\n----------------------------------------\n\nTITLE: Training Gensim Poincare Embeddings\nDESCRIPTION: This function trains a Poincare embedding using the Gensim implementation. It takes parameters such as data file, output file, dimensions, epochs, negative samples, burn-in, batch size, and regularization coefficient.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Evaluation.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef train_gensim_model(\n    data_file, output_file, dim, epochs, neg, burn_in, batch_size, reg, seed=0):\n    \"\"\"Train a poincare embedding using gensim implementation\n    \n    Args:\n        data_file (str): Path to tsv file containing relation pairs\n        output_file (str): Path to output file containing model\n        dim (int): Number of dimensions of the trained model\n        epochs (int): Number of epochs to use\n        neg (int): Number of negative samples to use\n        burn_in (int): Number of epochs to use for burn-in initialization\n        batch_size (int): Size of batch to use for training\n        reg (float): Coefficient used for l2-regularization while training\n    \n    Notes: \n        If `output_file` already exists, skips training\n    \"\"\"\n    if os.path.exists(output_file):\n        print('File %s exists, skipping' % output_file)\n        return\n    train_data = PoincareRelations(data_file)\n    model = PoincareModel(train_data, size=dim, negative=neg, burn_in=burn_in, regularization_coeff=reg)\n    model.train(epochs=epochs, batch_size=batch_size)\n    model.save(output_file)\n```\n\n----------------------------------------\n\nTITLE: Loading Test and Training Data Paths\nDESCRIPTION: Sets up file paths for the Lee Corpus training and test data files using Gensim's test data directory\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_doc2vec_lee.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport gensim\n# Set file names for train and test data\ntest_data_dir = os.path.join(gensim.__path__[0], 'test', 'test_data')\nlee_train_file = os.path.join(test_data_dir, 'lee_background.cor')\nlee_test_file = os.path.join(test_data_dir, 'lee.cor')\n```\n\n----------------------------------------\n\nTITLE: Loading pre-trained Word2Vec model in Python using Gensim\nDESCRIPTION: Demonstrates how to load a pre-trained Word2Vec model (Google News dataset) using Gensim's downloader API.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_word2vec.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport gensim.downloader as api\nwv = api.load('word2vec-google-news-300')\n```\n\n----------------------------------------\n\nTITLE: Computing Word2Vec Training Loss in Python\nDESCRIPTION: Shows how to enable and retrieve training loss computation during Word2Vec model training.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_word2vec.ipynb#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# instantiating and training the Word2Vec model\nmodel_with_loss = gensim.models.Word2Vec(\n    sentences,\n    min_count=1,\n    compute_loss=True,\n    hs=0,\n    sg=1,\n    seed=42,\n)\n\n# getting the training loss value\ntraining_loss = model_with_loss.get_latest_training_loss()\nprint(training_loss)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Doc2Vec Model in Python\nDESCRIPTION: This snippet shows the evaluation of a trained Doc2Vec model, printing out a score (likely accuracy or similarity measure).\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n0.170880 Doc2Vec<dm/m,d100,n5,w10,mc2,t2>\n```\n\n----------------------------------------\n\nTITLE: Running LDA Model\nDESCRIPTION: Demonstrates fitting and transforming data using LdaTransformer\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/sklearn_api.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel = LdaTransformer(num_topics=2, id2word=dictionary, iterations=20, random_state=1)\nmodel.fit(corpus)\nmodel.transform(corpus)\n```\n\n----------------------------------------\n\nTITLE: Processing and Displaying Benchmark Results in Python\nDESCRIPTION: This code processes the benchmark results, calculating additional metrics like processing and production speed. It then defines a display function to format the results for readable output, including converting durations to timedelta objects and formatting speeds.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_benchmark.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(results)\ndf[\"processing_speed\"] = df.dictionary_size * len(query_terms) / df.production_duration\ndf[\"production_speed\"] = df.nonzero_limit * len(query_terms) / df.production_duration\ndf = df.groupby([\"dictionary_size\", \"nonzero_limit\", \"annoy_n_trees\"])\n\ndef display(df):\n    df[\"constructor_duration\"] = [timedelta(0, duration) for duration in df[\"constructor_duration\"]]\n    df[\"production_duration\"] = [timedelta(0, duration) for duration in df[\"production_duration\"]]\n    df[\"processing_speed\"] = [\"%.02f Kword pairs / s\" % (speed / 1000) for speed in df[\"processing_speed\"]]\n    df[\"production_speed\"] = [\"%.02f Kword pairs / s\" % (speed / 1000) for speed in df[\"production_speed\"]]\n    return df\n```\n\n----------------------------------------\n\nTITLE: Vocabulary Membership Testing\nDESCRIPTION: Shows different methods for checking word presence in vocabulary versus vector existence.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Tests if word present in vocab\nprint(\"word\" in model_wrapper.wv.vocab)\n# Tests if vector present for word\nprint(\"word\" in model_wrapper)\n```\n\n----------------------------------------\n\nTITLE: Displaying Results for Link Prediction Evaluation\nDESCRIPTION: This snippet handles the display of the evaluation results obtained from the link prediction models. It provides a formatted output header followed by the results for easy readability.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Evaluation.ipynb#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndisplay_results('WordNet Link Prediction', lp_results)\n```\n\n----------------------------------------\n\nTITLE: Importing TfIdf Model from Gensim in Python\nDESCRIPTION: This snippet demonstrates how to import the TfIdf model from Gensim's scikit-learn API. It's necessary for using TfIdf in a scikit-learn pipeline.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/sklearn_api.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.sklearn_api import TfIdfTransformer\n```\n\n----------------------------------------\n\nTITLE: Tuning Topic Clustering Parameters\nDESCRIPTION: Analyzes distance matrix and adjusts clustering parameters to optimize topic generation.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_ensemblelda.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nshape = ensemble.asymmetric_distance_matrix.shape\nwithout_diagonal = ensemble.asymmetric_distance_matrix[~np.eye(shape[0], dtype=bool)].reshape(shape[0], -1)\nprint(without_diagonal.min(), without_diagonal.mean(), without_diagonal.max())\n\nensemble.recluster(eps=0.09, min_samples=2, min_cores=2)\n\nprint(len(ensemble.get_topics()))\n```\n\n----------------------------------------\n\nTITLE: Loading Test Data\nDESCRIPTION: Loads 20 Newsgroups dataset for testing\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/sklearn_api.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nrand = np.random.mtrand.RandomState(1) # set seed for getting same result\ncats = ['rec.sport.baseball', 'sci.crypt']\ndata = fetch_20newsgroups(subset='train', categories=cats, shuffle=True)\n```\n\n----------------------------------------\n\nTITLE: Checking BLAS and LAPACK Configuration in Python\nDESCRIPTION: This snippet demonstrates how to check the current BLAS and LAPACK configuration in Python using SciPy. It's useful for verifying the linear algebra libraries being used, which can significantly impact performance.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/distributed.rst#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\npython -c 'import scipy; scipy.show_config()'\n```\n\n----------------------------------------\n\nTITLE: Loading Facebook FastText Models in Python\nDESCRIPTION: New functions for loading Facebook FastText models, including load_facebook_model to load the full model and load_facebook_vectors to load only the embeddings.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/CHANGELOG.md#2025-04-21_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ngensim.models.fasttext.load_facebook_model(path)\n```\n\nLANGUAGE: Python\nCODE:\n```\ngensim.models.fasttext.load_facebook_vectors(path)\n```\n\n----------------------------------------\n\nTITLE: Logging Word2Vec Training Progress in Gensim\nDESCRIPTION: This snippet shows the log output format for Word2Vec training progress in Gensim. It includes timestamp, epoch number, percentage completion, words processed per second, and input/output queue sizes.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_41\n\nLANGUAGE: plaintext\nCODE:\n```\n2023-08-23 13:25:43,917 : INFO : EPOCH 16 - PROGRESS: at 83.88% examples, 315746 words/s, in_qsize 3, out_qsize 0\n```\n\n----------------------------------------\n\nTITLE: Splitting Documents into Training and Test Sets\nDESCRIPTION: Separates the processed documents into training and test sets based on their split attribute.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntrain_docs = [doc for doc in alldocs if doc.split == 'train']\ntest_docs = [doc for doc in alldocs if doc.split == 'test']\nprint(f'{len(alldocs)} docs: {len(train_docs)} train-sentiment, {len(test_docs)} test-sentiment')\n```\n\n----------------------------------------\n\nTITLE: Evaluating WordRank Model on Word Similarity in Python\nDESCRIPTION: This code shows how to evaluate a WordRank model on word similarity benchmarks using Gensim's evaluate_word_pairs method. It tests how well the model captures the semantic similarity between word pairs.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/WordRank_wrapper_quickstart.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nword_similarity_file = 'datasets/ws-353.txt'\nmodel.evaluate_word_pairs(word_similarity_file)\n```\n\n----------------------------------------\n\nTITLE: Monitoring Word2Vec Training Progress in Gensim\nDESCRIPTION: Log output from Gensim's Word2Vec training process showing progression through epochs 13 and 14. Each line shows timestamp, epoch number, completion percentage, processing speed in words per second, and input/output queue sizes.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_40\n\nLANGUAGE: log\nCODE:\n```\n2023-08-23 13:21:29,357 : INFO : EPOCH 13 - PROGRESS: at 36.57% examples, 307459 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:30,372 : INFO : EPOCH 13 - PROGRESS: at 37.97% examples, 307708 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:31,380 : INFO : EPOCH 13 - PROGRESS: at 39.40% examples, 308327 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:32,415 : INFO : EPOCH 13 - PROGRESS: at 40.83% examples, 308606 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:33,438 : INFO : EPOCH 13 - PROGRESS: at 42.18% examples, 308386 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:34,470 : INFO : EPOCH 13 - PROGRESS: at 43.45% examples, 307540 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:35,508 : INFO : EPOCH 13 - PROGRESS: at 44.90% examples, 307819 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:36,510 : INFO : EPOCH 13 - PROGRESS: at 46.30% examples, 308115 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:37,511 : INFO : EPOCH 13 - PROGRESS: at 47.72% examples, 308448 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:38,551 : INFO : EPOCH 13 - PROGRESS: at 49.15% examples, 308676 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:39,557 : INFO : EPOCH 13 - PROGRESS: at 50.50% examples, 308875 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:40,575 : INFO : EPOCH 13 - PROGRESS: at 51.85% examples, 308984 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:41,624 : INFO : EPOCH 13 - PROGRESS: at 53.23% examples, 308627 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:42,633 : INFO : EPOCH 13 - PROGRESS: at 54.54% examples, 308642 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:43,636 : INFO : EPOCH 13 - PROGRESS: at 55.90% examples, 308664 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:44,656 : INFO : EPOCH 13 - PROGRESS: at 57.22% examples, 308272 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:45,658 : INFO : EPOCH 13 - PROGRESS: at 58.64% examples, 308502 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:46,664 : INFO : EPOCH 13 - PROGRESS: at 60.06% examples, 308727 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:47,682 : INFO : EPOCH 13 - PROGRESS: at 61.47% examples, 308811 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:48,705 : INFO : EPOCH 13 - PROGRESS: at 62.92% examples, 309080 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:49,708 : INFO : EPOCH 13 - PROGRESS: at 64.37% examples, 309276 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:50,712 : INFO : EPOCH 13 - PROGRESS: at 65.61% examples, 309074 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:51,728 : INFO : EPOCH 13 - PROGRESS: at 67.00% examples, 309177 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:52,743 : INFO : EPOCH 13 - PROGRESS: at 68.40% examples, 309294 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:53,753 : INFO : EPOCH 13 - PROGRESS: at 69.78% examples, 309629 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:54,766 : INFO : EPOCH 13 - PROGRESS: at 71.16% examples, 309730 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:55,790 : INFO : EPOCH 13 - PROGRESS: at 72.59% examples, 309784 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:56,809 : INFO : EPOCH 13 - PROGRESS: at 74.05% examples, 310058 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:57,862 : INFO : EPOCH 13 - PROGRESS: at 75.47% examples, 310238 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:58,919 : INFO : EPOCH 13 - PROGRESS: at 76.91% examples, 310457 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:21:59,919 : INFO : EPOCH 13 - PROGRESS: at 78.28% examples, 310455 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:00,958 : INFO : EPOCH 13 - PROGRESS: at 79.70% examples, 310572 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:01,971 : INFO : EPOCH 13 - PROGRESS: at 81.11% examples, 310647 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:02,994 : INFO : EPOCH 13 - PROGRESS: at 82.49% examples, 310679 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:04,018 : INFO : EPOCH 13 - PROGRESS: at 83.88% examples, 310817 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:05,034 : INFO : EPOCH 13 - PROGRESS: at 85.29% examples, 310887 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:06,070 : INFO : EPOCH 13 - PROGRESS: at 86.62% examples, 310557 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:07,072 : INFO : EPOCH 13 - PROGRESS: at 87.98% examples, 310562 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:08,113 : INFO : EPOCH 13 - PROGRESS: at 89.44% examples, 310653 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:09,156 : INFO : EPOCH 13 - PROGRESS: at 90.89% examples, 310747 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:10,169 : INFO : EPOCH 13 - PROGRESS: at 92.30% examples, 310812 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:11,169 : INFO : EPOCH 13 - PROGRESS: at 93.66% examples, 310814 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:12,183 : INFO : EPOCH 13 - PROGRESS: at 95.07% examples, 310876 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:13,235 : INFO : EPOCH 13 - PROGRESS: at 96.52% examples, 310918 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:14,257 : INFO : EPOCH 13 - PROGRESS: at 97.92% examples, 310802 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:15,304 : INFO : EPOCH 13 - PROGRESS: at 99.38% examples, 310861 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:15,751 : INFO : EPOCH 13: training on 23279529 raw words (22951015 effective words) took 73.8s, 310940 effective words/s\n2023-08-23 13:22:16,755 : INFO : EPOCH 14 - PROGRESS: at 1.41% examples, 309872 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:17,760 : INFO : EPOCH 14 - PROGRESS: at 2.76% examples, 313599 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:18,777 : INFO : EPOCH 14 - PROGRESS: at 4.21% examples, 316342 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:19,806 : INFO : EPOCH 14 - PROGRESS: at 5.68% examples, 317364 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:20,815 : INFO : EPOCH 14 - PROGRESS: at 7.09% examples, 317417 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:21,839 : INFO : EPOCH 14 - PROGRESS: at 8.41% examples, 314727 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:22,855 : INFO : EPOCH 14 - PROGRESS: at 9.81% examples, 316204 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:23,858 : INFO : EPOCH 14 - PROGRESS: at 11.20% examples, 316441 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:24,863 : INFO : EPOCH 14 - PROGRESS: at 12.58% examples, 315498 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:25,884 : INFO : EPOCH 14 - PROGRESS: at 13.90% examples, 314323 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:26,930 : INFO : EPOCH 14 - PROGRESS: at 15.26% examples, 313408 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:27,939 : INFO : EPOCH 14 - PROGRESS: at 16.60% examples, 312921 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:28,963 : INFO : EPOCH 14 - PROGRESS: at 17.98% examples, 312916 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:29,968 : INFO : EPOCH 14 - PROGRESS: at 19.32% examples, 313049 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:30,984 : INFO : EPOCH 14 - PROGRESS: at 20.68% examples, 313116 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:31,999 : INFO : EPOCH 14 - PROGRESS: at 22.11% examples, 313737 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:33,031 : INFO : EPOCH 14 - PROGRESS: at 23.47% examples, 313501 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:34,072 : INFO : EPOCH 14 - PROGRESS: at 24.81% examples, 312532 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:35,073 : INFO : EPOCH 14 - PROGRESS: at 26.14% examples, 311913 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:36,096 : INFO : EPOCH 14 - PROGRESS: at 27.56% examples, 312361 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:37,110 : INFO : EPOCH 14 - PROGRESS: at 28.93% examples, 312547 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:38,114 : INFO : EPOCH 14 - PROGRESS: at 30.32% examples, 312739 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:39,134 : INFO : EPOCH 14 - PROGRESS: at 31.66% examples, 312400 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:40,184 : INFO : EPOCH 14 - PROGRESS: at 33.10% examples, 312455 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:41,187 : INFO : EPOCH 14 - PROGRESS: at 34.48% examples, 312713 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:42,225 : INFO : EPOCH 14 - PROGRESS: at 35.84% examples, 312180 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:43,236 : INFO : EPOCH 14 - PROGRESS: at 37.27% examples, 312367 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:44,250 : INFO : EPOCH 14 - PROGRESS: at 38.61% examples, 312118 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:45,269 : INFO : EPOCH 14 - PROGRESS: at 39.96% examples, 311789 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:46,273 : INFO : EPOCH 14 - PROGRESS: at 41.29% examples, 311594 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:47,301 : INFO : EPOCH 14 - PROGRESS: at 42.68% examples, 311591 words/s, in_qsize 4, out_qsize 0\n2023-08-23 13:22:48,309 : INFO : EPOCH 14 - PROGRESS: at 43.99% examples, 311163 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:49,326 : INFO : EPOCH 14 - PROGRESS: at 45.41% examples, 311232 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:50,343 : INFO : EPOCH 14 - PROGRESS: at 46.81% examples, 311312 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:51,347 : INFO : EPOCH 14 - PROGRESS: at 48.26% examples, 311762 words/s, in_qsize 4, out_qsize 0\n2023-08-23 13:22:52,350 : INFO : EPOCH 14 - PROGRESS: at 49.57% examples, 311702 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:22:53,352 : INFO : EPOCH 14 - PROGRESS: at 50.87% examples, 311347 words/s, in_qsize 3, out_qsize 0\n```\n\n----------------------------------------\n\nTITLE: Initializing Doc2Vec Model in Python\nDESCRIPTION: Creates a Doc2Vec model with 50-dimensional vectors, minimum word count of 2, and 40 training epochs\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_doc2vec_lee.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmodel = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n```\n\n----------------------------------------\n\nTITLE: Loading Wikipedia Word Vectors\nDESCRIPTION: Loads and initializes a coherence model using pre-trained word vectors from Wikipedia, as an alternative to Google News vectors.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence_model_selection.ipynb#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\nvectors_path = os.path.join(models_dir, 'wiki-en_sgns5-w5-s300.bin.gz')\nkeyed_vectors = models.KeyedVectors.load_word2vec_format(vectors_path, binary=True)\n\n# still need to estimate_probabilities, but corpus is not scanned\ncm = models.CoherenceModel.for_models(\n    trained_models.values(), dictionary, texts=corpus.get_texts(),\n    coherence='c_w2v', keyed_vectors=keyed_vectors)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Author-Topic Distributions using t-SNE in Python\nDESCRIPTION: This code uses t-SNE to reduce the dimensionality of author-topic distributions for visualization purposes.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n%%time\nfrom sklearn.manifold import TSNE\ntsne = TSNE(n_components=2, random_state=0)\nsmallest_author = 0  # Ignore authors with documents less than this.\nauthors = [model.author2id[a] for a in model.author2id.keys() if len(model.author2doc[a]) >= smallest_author]\n_ = tsne.fit_transform(model.state.gamma[authors, :])  # Result stored in tsne.embedding_\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Topic Modeling in Python\nDESCRIPTION: Sets up the required imports for performing topic modeling, including logging configuration to disable warning messages. Imports include standard libraries and Gensim components for corpus handling and modeling.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence_model_selection.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import print_function\n\nimport os\nimport re\nimport logging\nfrom collections import OrderedDict\n\nfrom gensim.corpora import TextCorpus, MmCorpus\nfrom gensim import utils, models\n\nlogging.basicConfig(level=logging.ERROR)  # disable warning logging\n```\n\n----------------------------------------\n\nTITLE: Setting Top-K Documents Parameter\nDESCRIPTION: Defines a parameter k to analyze approximately the top 10% most scored documents, which will be used for comparing document length effects.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/pivoted_document_length_normalisation.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# We perform our analysis on top k documents which is almost top 10% most scored documents\nk = len(X_test) // 10\n```\n\n----------------------------------------\n\nTITLE: Listing Available Corpora\nDESCRIPTION: Demonstrates how to iterate through and display information about available corpora, including name, number of records, and description.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_downloader_api.rst#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfor corpus_name, corpus_data in sorted(info['corpora'].items()):\n    print(\n        '%s (%d records): %s' % (\n            corpus_name,\n            corpus_data.get('num_records', -1),\n            corpus_data['description'][:40] + '...',\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Document Probability Calculator\nDESCRIPTION: Function to calculate class probabilities for documents using trained Word2Vec models\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndef docprob(docs, mods):\n    sentlist = [s for d in docs for s in d]\n    llhd = np.array( [ m.score(sentlist, len(sentlist)) for m in mods ] )\n    lhd = np.exp(llhd - llhd.max(axis=0))\n    prob = pd.DataFrame( (lhd/lhd.sum(axis=0)).transpose() )\n    prob[\"doc\"] = [i for i,d in enumerate(docs) for s in d]\n    prob = prob.groupby(\"doc\").mean()\n    return prob\n```\n\n----------------------------------------\n\nTITLE: Setting Up LDA Model Configuration\nDESCRIPTION: Configures basic parameters for LDA model training including model class, workers, and topic settings.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_ensemblelda.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models import LdaModel\ntopic_model_class = LdaModel\n\nensemble_workers = 4\nnum_models = 8\n\ndistance_workers = 4\n\nnum_topics = 20\npasses = 2\n```\n\n----------------------------------------\n\nTITLE: Importing SVMlight Corpus Module in Python\nDESCRIPTION: This snippet shows how to import the SVMlight corpus module from gensim. It allows working with corpora in SVMlight format, which is commonly used in machine learning applications.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/corpora/svmlightcorpus.rst#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom gensim.corpora import svmlightcorpus\n```\n\n----------------------------------------\n\nTITLE: Setting Up JSON Processing with Smart_open\nDESCRIPTION: Imports the JSON module and smart_open library for efficiently reading and processing the Yelp review data stored in JSON format.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/WMD_tutorial.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nstart = time()\n\nimport json\nfrom smart_open import smart_open\n```\n\n----------------------------------------\n\nTITLE: Analyzing Translation Matrix Creation Time\nDESCRIPTION: Measures and plots the time taken to create translation matrices of different sizes.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/translation_matrix.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\ntest_case = 10\nword_pair_length = len(word_pair)\nstep = word_pair_length / test_case\n\nduration = []\nsizeofword = []\n\nfor idx in range(0, test_case):\n    sub_pair = word_pair[: (idx + 1) * step]\n\n    startTime = time.time()\n    transmat = translation_matrix.TranslationMatrix(source_word_vec, target_word_vec, sub_pair)\n    transmat.train(sub_pair)\n    endTime = time.time()\n    \n    sizeofword.append(len(sub_pair))\n    duration.append(endTime - startTime)\n```\n\n----------------------------------------\n\nTITLE: Listing Available Data Resources\nDESCRIPTION: Shows how to print the available types of data resources (corpora and models) using the info dictionary keys.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_downloader_api.rst#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprint(info.keys())\n```\n\n----------------------------------------\n\nTITLE: Getting Detailed Resource Information\nDESCRIPTION: Demonstrates how to retrieve and display detailed information about a specific resource (fake-news dataset in this case).\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_downloader_api.rst#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfake_news_info = api.info('fake-news')\nprint(json.dumps(fake_news_info, indent=4))\n```\n\n----------------------------------------\n\nTITLE: Optimized LSI Model Training with CSC Matrix\nDESCRIPTION: Example showing how to train an LSI model using CSC matrix format and single precision for improved memory efficiency and speed. This approach can be 3x faster while using 4x less memory when the corpus fits in RAM.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/CHANGELOG.md#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# just an example; the corpus stream is up to you\nstreaming_corpus = gensim.corpora.MmCorpus(\"my_tfidf_corpus.mm.gz\")\n\n# convert your corpus to a CSC sparse matrix (assumes the entire corpus fits in RAM)\nin_memory_csc_matrix = gensim.matutils.corpus2csc(streaming_corpus, dtype=np.float32)\n\n# then pass the CSC to LsiModel directly\nmodel = LsiModel(corpus=in_memory_csc_matrix, num_topics=500, dtype=np.float32)\n```\n\n----------------------------------------\n\nTITLE: Loading the Text8 Corpus\nDESCRIPTION: Downloads and loads the text8 corpus into memory automatically using Gensim's downloader API.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/downloader_api_tutorial.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncorpus = api.load('text8')\n```\n\n----------------------------------------\n\nTITLE: Creating a Corpus for Similarity Queries\nDESCRIPTION: Demonstrates corpus creation by tokenizing documents, removing stopwords, and creating a dictionary and bag-of-words representation.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_similarity_queries.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom collections import defaultdict\nfrom gensim import corpora\n\ndocuments = [\n    \"Human machine interface for lab abc computer applications\",\n    \"A survey of user opinion of computer system response time\",\n    \"The EPS user interface management system\",\n    \"System and human system engineering testing of EPS\",\n    \"Relation of user perceived response time to error measurement\",\n    \"The generation of random binary unordered trees\",\n    \"The intersection graph of paths in trees\",\n    \"Graph minors IV Widths of trees and well quasi ordering\",\n    \"Graph minors A survey\",\n]\n\n# remove common words and tokenize\nstoplist = set('for a of the and to in'.split())\ntexts = [\n    [word for word in document.lower().split() if word not in stoplist]\n    for document in documents\n]\n\n# remove words that appear only once\nfrequency = defaultdict(int)\nfor text in texts:\n    for token in text:\n        frequency[token] += 1\n\ntexts = [\n    [token for token in text if frequency[token] > 1]\n    for text in texts\n]\n\ndictionary = corpora.Dictionary(texts)\ncorpus = [dictionary.doc2bow(text) for text in texts]\n```\n\n----------------------------------------\n\nTITLE: Explaining Corpus Flexibility in Gensim\nDESCRIPTION: Comment explaining that Gensim models can work with any iterable object that yields documents, allowing for memory-efficient processing.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_corpora_and_vector_spaces.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# This flexibility allows you to create your own corpus classes that stream the\n# documents directly from disk, network, database, dataframes... The models\n# in Gensim are implemented such that they don't require all vectors to reside\n# in RAM at once. You can even create the documents on the fly!\n```\n\n----------------------------------------\n\nTITLE: Converting New Document to Bag-of-Words Vector in Gensim\nDESCRIPTION: Demonstrates how to convert a new document into a bag-of-words vector representation using the created dictionary.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_core_concepts.rst#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nnew_doc = \"Human computer interaction\"\nnew_vec = dictionary.doc2bow(new_doc.lower().split())\nprint(new_vec)\n```\n\n----------------------------------------\n\nTITLE: Initializing WordNet Reconstruction Evaluation in Python\nDESCRIPTION: Setting up the WordNet reconstruction evaluation task by creating an OrderedDict to store evaluation results for different models and metrics. The code defines the metrics 'mean_rank' and 'MAP' (Mean Average Precision) for measuring the embedding quality.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Evaluation.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nreconstruction_results = OrderedDict()\nmetrics = ['mean_rank', 'MAP']\n```\n\n----------------------------------------\n\nTITLE: Benchmarking WordEmbeddingSimilarityIndex in Python\nDESCRIPTION: This function benchmarks the WordEmbeddingSimilarityIndex constructor and query performance. It measures the time taken to build the index and perform similarity queries, supporting both precise and ANNOY-based approximate nearest neighbor search.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_benchmark.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef benchmark(configuration):\n    (model, dictionary), nonzero_limit, annoy_n_trees, query_terms, repetition = configuration\n    use_annoy = annoy_n_trees > 0\n    model.init_sims()\n    \n    start_time = time()\n    if use_annoy:\n        annoy = AnnoyIndexer(model, annoy_n_trees)\n        kwargs = {\"indexer\": annoy}\n    else:\n        kwargs = {}\n    index = WordEmbeddingSimilarityIndex(model, kwargs=kwargs)\n    end_time = time()\n    constructor_duration = end_time - start_time\n    \n    start_time = time()\n    for term in query_terms:\n        for _j, _k in zip(index.most_similar(term, topn=nonzero_limit), range(nonzero_limit)):\n            pass\n    end_time = time()\n    production_duration = end_time - start_time\n    \n    return {\n        \"dictionary_size\": len(dictionary),\n        \"mean_query_term_length\": np.mean([len(term) for term in query_terms]),\n        \"nonzero_limit\": nonzero_limit,\n        \"use_annoy\": use_annoy,\n        \"annoy_n_trees\": annoy_n_trees,\n        \"repetition\": repetition,\n        \"constructor_duration\": constructor_duration,\n        \"production_duration\": production_duration, }\n```\n\n----------------------------------------\n\nTITLE: Getting the File Path to a Downloaded Model\nDESCRIPTION: Shows how to get the local file path to a downloaded model without loading it into memory by using the return_path parameter.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/downloader_api_tutorial.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(api.load('glove-wiki-gigaword-50', return_path=True))\n```\n\n----------------------------------------\n\nTITLE: Analyzing Word Frequency and Model Performance in Python\nDESCRIPTION: This snippet defines functions to analyze the relationship between word frequency and model performance on analogy tasks. It computes accuracies for different frequency buckets and prepares data for plotting.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Wordrank_comparisons.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import division\nimport matplotlib.pyplot as plt\nimport copy\nimport multiprocessing\nimport numpy as np\nfrom smart_open import smart_open\n\n\ndef compute_accuracies(model, freq):\n    # mean_freq will contain analogies together with the mean frequency of 4 words involved\n    mean_freq = {}\n    with smart_open(word_analogies_file, 'r') as r:\n        for i, line in enumerate(r):\n            if ':' not in line:\n                analogy = tuple(line.split())\n            else:\n                continue\n            try:\n                mfreq = sum([int(freq[x.lower()]) for x in analogy])/4\n                mean_freq['a%d'%i] = [analogy, mfreq]\n            except KeyError:\n                continue\n    \n    # compute model's accuracy\n    model = KeyedVectors.load_word2vec_format(model)\n    acc = model.accuracy(word_analogies_file)\n    \n    sem_correct = [acc[i]['correct'] for i in range(5)]\n    sem_total = [acc[i]['correct'] + acc[i]['incorrect'] for i in range(5)]\n    syn_correct = [acc[i]['correct'] for i in range(5, len(acc)-1)]\n    syn_total = [acc[i]['correct'] + acc[i]['incorrect'] for i in range(5, len(acc)-1)]\n    total_correct = sem_correct + syn_correct\n    total_total = sem_total + syn_total\n\n    sem_x, sem_y = calc_axis(sem_correct, sem_total, mean_freq)\n    syn_x, syn_y = calc_axis(syn_correct, syn_total, mean_freq)\n    total_x, total_y = calc_axis(total_correct, total_total, mean_freq)\n    return ((sem_x, sem_y), (syn_x, syn_y), (total_x, total_y))\n\ndef calc_axis(correct, total, mean_freq):\n    # make flat lists\n    correct_analogies = []\n    for i in range(len(correct)):\n        for analogy in correct[i]:\n            correct_analogies.append(analogy)            \n    total_analogies = []\n    for i in range(len(total)):\n        for analogy in total[i]:\n            total_analogies.append(analogy)\n\n    copy_mean_freq = copy.deepcopy(mean_freq)\n    # delete other case's analogy from total analogies  \n    for key, value in copy_mean_freq.items():\n        value[0] = tuple(x.upper() for x in value[0])\n        if value[0] not in total_analogies:\n            del copy_mean_freq[key]\n\n    # append 0 or 1 for incorrect or correct analogy\n    for key, value in copy_mean_freq.iteritems():\n        value[0] = tuple(x.upper() for x in value[0])\n        if value[0] in correct_analogies:\n            copy_mean_freq[key].append(1)\n        else:\n            copy_mean_freq[key].append(0)\n\n    x = []\n    y = []\n    bucket_size = int(len(copy_mean_freq) * 0.06)\n    # sort analogies according to their mean frequences \n    copy_mean_freq = sorted(copy_mean_freq.items(), key=lambda x: x[1][1])\n    # prepare analogies buckets according to given size\n    for centre_p in range(bucket_size//2, len(copy_mean_freq), bucket_size):\n        bucket = copy_mean_freq[centre_p-bucket_size//2:centre_p+bucket_size//2]\n        b_acc = 0\n        # calculate current bucket accuracy with b_acc count\n        for analogy in bucket:\n            if analogy[1][2]==1:\n                b_acc+=1\n        y.append(b_acc/bucket_size)\n        x.append(np.log(copy_mean_freq[centre_p][1][1]))\n    return x, y\n```\n\n----------------------------------------\n\nTITLE: Model Training Execution\nDESCRIPTION: Executes the training function on the Text8 corpus to generate word embeddings using all three models.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Wordrank_comparisons.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntrain_models(corpus_file='text8', output_name='text8')\n```\n\n----------------------------------------\n\nTITLE: Stopwords Filtering\nDESCRIPTION: Creates a filtered version of the corpus without stopwords\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/wikinews-bigram-en.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# a version of corups without stop words\nstop_words = frozenset(stopwords.words(LANG))\ndef stopwords_filter(txt):\n    return [w for w in txt if w not in stop_words]\nst_corpus = [stopwords_filter(txt) for txt in corpus]\n```\n\n----------------------------------------\n\nTITLE: Setting up Gensim and Pyro for Distributed Computing\nDESCRIPTION: This snippet shows how to install Gensim with distributed computing support and configure Pyro serialization settings.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/dist_lsi.rst#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ sudo easy_install gensim[distributed]\n$ export PYRO_SERIALIZERS_ACCEPTED=pickle\n$ export PYRO_SERIALIZER=pickle\n```\n\n----------------------------------------\n\nTITLE: Logging Word2Vec Training Progress in Gensim\nDESCRIPTION: This snippet shows the log output format for Word2Vec training progress in Gensim. It includes the epoch number, percentage of examples processed, words processed per second, and input/output queue sizes.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\n2023-08-23 12:42:08,013 : INFO : EPOCH 3 - PROGRESS: at 31.93% examples, 812452 words/s, in_qsize 3, out_qsize 0\n```\n\n----------------------------------------\n\nTITLE: Preparing Models and Configurations for Benchmarking in Python\nDESCRIPTION: This code prepares models and configurations for benchmarking. It creates subsets of a full model, sets up ANNOY configurations, and generates query terms. The configurations are then used to run the benchmark function across various parameters.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_benchmark.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmodels = []\nfor dictionary in tqdm(dictionaries, desc=\"models\"):\n    if dictionary == full_dictionary:\n        models.append(full_model)\n        continue\n    model = full_model.__class__(full_model.vector_size)\n    model.vocab = {word: deepcopy(full_model.vocab[word]) for word in dictionary.values()}\n    model.index2entity = []\n    vector_indices = []\n    for index, word in enumerate(full_model.index2entity):\n        if word in model.vocab.keys():\n            model.index2entity.append(word)\n            model.vocab[word].index = len(vector_indices)\n            vector_indices.append(index)\n    model.vectors = full_model.vectors[vector_indices]\n    models.append(model)\nannoy_n_trees = [0] + [10**k for k in range(3)]\nseed(RANDOM_SEED)\nquery_terms = sample(list(min_dictionary.values()), 1000)\n\nconfigurations = product(zip(models, dictionaries), nonzero_limits, annoy_n_trees, [query_terms], repetitions)\nresults = benchmark_results(benchmark, configurations, \"matrix_speed.builder_results.wordembeddings\")\n```\n\n----------------------------------------\n\nTITLE: Locating Corpus Class File in Python\nDESCRIPTION: Prints the file path of the corpus class definition.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_downloader_api.rst#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint(inspect.getfile(corpus.__class__))\n```\n\n----------------------------------------\n\nTITLE: Setting Word2Vec min_count Parameter in Python\nDESCRIPTION: Example of initializing Word2Vec model with min_count parameter to filter out rare words.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_word2vec.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nmodel = gensim.models.Word2Vec(sentences, min_count=10)\n```\n\n----------------------------------------\n\nTITLE: Removing Stopwords from Example Sentences using NLTK\nDESCRIPTION: Imports and downloads stopwords from NLTK, then filters out common English stopwords from the example sentences to improve WMD accuracy by focusing on meaningful words.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/WMD_tutorial.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Import and download stopwords from NLTK.\nfrom nltk.corpus import stopwords\nfrom nltk import download\ndownload('stopwords')  # Download stopwords list.\n\n# Remove stopwords.\nstop_words = stopwords.words('english')\nsentence_obama = [w for w in sentence_obama if w not in stop_words]\nsentence_president = [w for w in sentence_president if w not in stop_words]\n```\n\n----------------------------------------\n\nTITLE: Updated Word2Vec Training Call in Python\nDESCRIPTION: New required syntax for training Word2Vec/Doc2Vec models that now requires explicit epochs parameter and corpus size estimation.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/CHANGELOG.md#2025-04-21_snippet_24\n\nLANGUAGE: Python\nCODE:\n```\nvec_model.train(sentences, total_examples=self.corpus_count, epochs=self.iter)\n```\n\n----------------------------------------\n\nTITLE: Defining Time Sequence for DTM in Python\nDESCRIPTION: Defines the time sequence for documents in the corpus, specifying how many documents belong to each time slice for Dynamic Topic Modeling.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/dtm_example.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntime_seq = [3, 7]  # first 3 documents are from time slice one \n#  and the other 7 are from the second time slice.\n```\n\n----------------------------------------\n\nTITLE: Iterating Through a Memory-Efficient Corpus\nDESCRIPTION: Demonstrates how to iterate through the memory-friendly corpus, loading and printing one document vector at a time.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_corpora_and_vector_spaces.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfor vector in corpus_memory_friendly:  # load one vector into memory at a time\n    print(vector)\n```\n\n----------------------------------------\n\nTITLE: Importing Text2Bow Model from Gensim in Python\nDESCRIPTION: This snippet shows how to import the Text2Bow model from Gensim's scikit-learn API. It's required for using Text2Bow in a scikit-learn pipeline.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/sklearn_api.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.sklearn_api import Text2BowTransformer\n```\n\n----------------------------------------\n\nTITLE: Exporting Word2Vec Model to Text Format in Python\nDESCRIPTION: This code exports the Word2Vec model to a text file in word2vec format.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nmodel.wv.save_word2vec_format('/tmp/vectors.txt', binary=False)\n```\n\n----------------------------------------\n\nTITLE: Computing Document Similarity Rankings\nDESCRIPTION: Calculates similarity rankings between documents using the trained model\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_doc2vec_lee.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nranks = []\nsecond_ranks = []\nfor doc_id in range(len(train_corpus)):\n    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n    rank = [docid for docid, sim in sims].index(doc_id)\n    ranks.append(rank)\n\n    second_ranks.append(sims[1])\n```\n\n----------------------------------------\n\nTITLE: Importing required libraries for Dynamic Topic Modeling in Python\nDESCRIPTION: Imports the necessary modules from Gensim and other libraries to work with Dynamic Topic Models.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/ldaseqmodel.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models import ldaseqmodel\nfrom gensim.corpora import Dictionary, bleicorpus\nimport numpy\nfrom gensim.matutils import hellinger\n```\n\n----------------------------------------\n\nTITLE: Initializing Random Projections Model in Python\nDESCRIPTION: Creates a Random Projections model to reduce vector space dimensionality using TF-IDF corpus with 500 target topics. This is an efficient approach for approximating TF-IDF distances between documents.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_topics_and_transformations.rst#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmodel = models.RpModel(tfidf_corpus, num_topics=500)\n```\n\n----------------------------------------\n\nTITLE: Importing Gensim's Downloader API and Setting Up Logging\nDESCRIPTION: Sets up the environment by importing Gensim's downloader API and configuring basic logging to display timestamps, log levels, and messages.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/downloader_api_tutorial.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport gensim.downloader as api\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n```\n\n----------------------------------------\n\nTITLE: Displaying Word-to-ID Mapping\nDESCRIPTION: Prints the mapping between words and their integer IDs from the dictionary.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_corpora_and_vector_spaces.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(dictionary.token2id)\n```\n\n----------------------------------------\n\nTITLE: Displaying LDA Model diff() Method Documentation\nDESCRIPTION: Prints the documentation string for the diff() method of the LdaMulticore class, which explains how to interpret the distance matrix and annotation results.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_compare_lda.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(LdaMulticore.diff.__doc__)\n```\n\n----------------------------------------\n\nTITLE: Importing LDA Model Wrapper\nDESCRIPTION: Basic import of the LdaTransformer class from gensim.sklearn_api\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/sklearn_api.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.sklearn_api import LdaTransformer\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Imports necessary Python libraries for topic modeling, data processing and visualization\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_network.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models.ldamodel import LdaModel\nfrom gensim.corpora import Dictionary\nimport pandas as pd\nimport re\nfrom gensim.parsing.preprocessing import remove_stopwords, strip_punctuation\n\nimport numpy as np\n```\n\n----------------------------------------\n\nTITLE: Executing Cross-Validation for Similarity Measures in Python\nDESCRIPTION: This code performs cross-validation for different similarity measures on SemEval 2016 and 2017 datasets using multiprocessing for parallel execution.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n%%time\nfrom multiprocessing import Pool\n\nargs_list = [\n    (dataset, technique)\n    for dataset in (\"2016-test\", \"2017-test\")\n    for technique in (\"softcossim\", \"wmd-gensim\", \"wmd-relax\", \"cossim\")]\nwith Pool() as pool:\n    results = pool.map(crossvalidate, args_list)\n```\n\n----------------------------------------\n\nTITLE: Importing and Configuring Matplotlib for Jupyter\nDESCRIPTION: Configures Matplotlib to display plots inline in Jupyter notebooks.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_similarity_queries.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%matplotlib inline\n```\n\n----------------------------------------\n\nTITLE: Logging Doc2Vec Model Evaluation in Python\nDESCRIPTION: Log output showing the evaluation result of a trained Doc2Vec model. It displays the model configuration and a numeric evaluation score.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_16\n\nLANGUAGE: text\nCODE:\n```\nEvaluating Doc2Vec<dbow,d100,n5,mc2,t2>\n\n0.105240 Doc2Vec<dbow,d100,n5,mc2,t2>\n```\n\n----------------------------------------\n\nTITLE: Testing Number Translations\nDESCRIPTION: Tests the translation matrix with English number words to Italian.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/translation_matrix.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwords = [(\"one\", \"uno\"), (\"two\", \"due\"), (\"three\", \"tre\"), (\"four\", \"quattro\"), (\"five\", \"cinque\")]\nsource_word, target_word = zip(*words)\ntranslated_word = transmat.translate(source_word, 5)\n```\n\n----------------------------------------\n\nTITLE: Expanding Ensemble with Additional Models\nDESCRIPTION: Demonstrates how to add new LDA models to an existing ensemble and reclustering topics.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_ensemblelda.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models import LdaMulticore\n\nmodel1 = LdaMulticore(\n    corpus=corpus,\n    id2word=dictionary,\n    num_topics=9,\n    passes=4,\n)\n\nmodel2 = LdaModel(\n    corpus=corpus,\n    id2word=dictionary,\n    num_topics=11,\n    passes=2,\n)\n\nensemble.add_model(model1)\nensemble.add_model(model2)\n\nensemble.recluster()\n\nprint(len(ensemble.ttda))\nprint(len(ensemble.get_topics()))\n```\n\n----------------------------------------\n\nTITLE: Topic Analysis Methods\nDESCRIPTION: Demonstrates various methods for analyzing topics including get_term_topics and get_document_topics\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_methods.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel.show_topics()\n\nmodel.get_term_topics('hell')\n\nmodel.get_term_topics('firearm')\n\nmodel.get_term_topics('car')\n```\n\n----------------------------------------\n\nTITLE: Logging Gensim Training Progress in Plaintext\nDESCRIPTION: Log entries showing the progress of a Gensim model training process. Each line includes a timestamp, epoch number, percentage of examples processed, words per second, and input/output queue sizes.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_36\n\nLANGUAGE: plaintext\nCODE:\n```\n2023-08-23 13:15:48,556 : INFO : EPOCH 8 - PROGRESS: at 84.73% examples, 293723 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:49,601 : INFO : EPOCH 8 - PROGRESS: at 86.10% examples, 293782 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:50,602 : INFO : EPOCH 8 - PROGRESS: at 87.39% examples, 293754 words/s, in_qsize 3, out_qsize 0\n```\n\n----------------------------------------\n\nTITLE: Setting up Document Corpus\nDESCRIPTION: Defines a sample corpus with two distinct document classes - human-computer interaction and graphs. Used for demonstrating topic coherence differences.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence_tutorial.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntexts = [['human', 'interface', 'computer'],\n         ['survey', 'user', 'computer', 'system', 'response', 'time'],\n         ['eps', 'user', 'interface', 'system'],\n         ['system', 'human', 'system', 'eps'],\n         ['user', 'response', 'time'],\n         ['trees'],\n         ['graph', 'trees'],\n         ['graph', 'minors', 'trees'],\n         ['graph', 'minors', 'survey']]\n```\n\n----------------------------------------\n\nTITLE: Loading Source Language Word Vectors\nDESCRIPTION: Loads pre-trained English word vectors from a file using KeyedVectors.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/translation_matrix.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsource_word_vec_file = \"EN.200K.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt\"\nsource_word_vec = KeyedVectors.load_word2vec_format(source_word_vec_file, binary=False)\n```\n\n----------------------------------------\n\nTITLE: Initializing Gensim and Basic Setup\nDESCRIPTION: Sets up the required imports from Gensim library and configures logging\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_methods.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.corpora import Dictionary\nfrom gensim.models import ldamodel\nimport numpy\n%matplotlib inline\n\nimport logging\nlogging.basicConfig(level=logging.INFO)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Rank Distribution\nDESCRIPTION: Counts and displays the distribution of document ranks using Counter\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_doc2vec_lee.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport collections\n\ncounter = collections.Counter(ranks)\nprint(counter)\n```\n\n----------------------------------------\n\nTITLE: Calculating Word Similarity with VarEmbed Model\nDESCRIPTION: Example of using the similarity method on a loaded VarEmbed model to calculate the similarity between 'peace' and 'grim'. This demonstrates another standard KeyedVectors functionality.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Varembed.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel.similarity('peace', 'grim')\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies and Setting DTM Path for Python DTM Analysis\nDESCRIPTION: This snippet imports necessary libraries and checks for the DTM_PATH environment variable. It's essential for setting up the environment to use the DTM wrapper in Python.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/dtm_example.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport os\nfrom gensim import corpora, utils\nfrom gensim.models.wrappers.dtmmodel import DtmModel\nimport numpy as np\n\nif not os.environ.get('DTM_PATH', None):\n    raise ValueError(\"SKIP: You need to set the DTM path\")\n```\n\n----------------------------------------\n\nTITLE: Evaluating NMSLIB Indexer Performance in Python\nDESCRIPTION: This function evaluates the performance of NMSLIB indexer by varying a specified parameter and measuring initialization time, accuracy, and query time.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_nmslib_performance(parameter, is_parameter_query, parameter_start, parameter_end, parameter_step):\n    nmslib_x_values = []\n    nmslib_y_values_init = []\n    nmslib_y_values_accuracy = []\n    nmslib_y_values_query = []\n    index_params = {'M': 100, 'indexThreadQty': 10, 'efConstruction': 100, 'post': 0}\n    query_params = {'efSearch': 100}\n    \n    for x in range(parameter_start, parameter_end, parameter_step):\n        nmslib_x_values.append(x)\n        start_time = time.time()\n        if is_parameter_query:\n            query_params[parameter] = x\n        else:\n            index_params[parameter] = x\n        nmslib_index = NmslibIndexer(model\n                                , index_params\n                                , query_params)\n        nmslib_y_values_init.append(time.time() - start_time)\n        approximate_results = model.most_similar([model.wv.syn0norm[0]], topn=100, indexer=nmslib_index)\n        top_words = [result[0] for result in approximate_results]\n        nmslib_y_values_accuracy.append(len(set(top_words).intersection(exact_results)))\n        nmslib_y_values_query.append(avg_query_time(nmslib_index, queries=queries))\n    create_evaluation_graph(nmslib_x_values,\n                            nmslib_y_values_init, \n                            nmslib_y_values_accuracy, \n                            nmslib_y_values_query, \n                            parameter)\n```\n\n----------------------------------------\n\nTITLE: Evaluation Data Initialization in Python\nDESCRIPTION: Initializes training time data in seconds for different corpus sizes (brown, text8, text9)\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nevaluation_data['brown'] = [(18, 54.3, 32.5)]\nevaluation_data['text8'] = [(402, 942, 496)]\nevaluation_data['text9'] = [(3218, 6589, 3550)]\n```\n\n----------------------------------------\n\nTITLE: Preparing Dictionary and Corpus\nDESCRIPTION: Creates dictionary and corpus from the loaded test data\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/sklearn_api.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndata_texts = [_.split() for _ in data.data]\nid2word = Dictionary(data_texts)\ncorpus = [id2word.doc2bow(i.split()) for i in data.data]\n```\n\n----------------------------------------\n\nTITLE: Comparing Document Length Distribution After Pivoted Normalization\nDESCRIPTION: Analyzes how pivoted normalization affects the mean document length of top-scoring documents, showing that it reduces the bias toward short documents.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/pivoted_document_length_normalisation.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprint(\n   \"With pivoted normalisation top {} docs have mean length of {:.3f} \"\n   \"which is much closer to the corpus mean doc length of {:.3f}\"\n   .format(\n       k, sort_length_by_score(doc_scores, X_test)[1][:k].mean(), \n       sort_length_by_score(doc_scores, X_test)[1].mean()\n   )\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Plotly Dependencies\nDESCRIPTION: Installs required version of Plotly for visualization support\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_network.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install plotly>=2.0.16 # 2.0.16 need for support 'hovertext' argument from create_dendrogram function\n```\n\n----------------------------------------\n\nTITLE: Custom Tokenizer Setup\nDESCRIPTION: Creates a custom regular expression tokenizer for word tokenization\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/wikinews-bigram-en.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# make a custom tokenizer\nimport re\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer('\\w[\\w-]*|\\d[\\d,]*')\n```\n\n----------------------------------------\n\nTITLE: Doc2Vec Model Initialization\nDESCRIPTION: Initializes two Doc2Vec models with different corpus sizes and parameters for translation matrix experiments.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/translation_matrix.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ncores = multiprocessing.cpu_count()\nmodel1 = Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=2, workers=cores)\nmodel2 = Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=2, workers=cores)\n\nsmall_train_docs = train_docs[:15000]\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging for EnsembleLda in Python\nDESCRIPTION: Sets up the logging configuration for the EnsembleLda model to display information about the model's current operations. The logger is set to INFO level with a StreamHandler to output messages to the console.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/ensemble_lda_with_opinosis.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nelda_logger = logging.getLogger(EnsembleLda.__module__)\nelda_logger.setLevel(logging.INFO)\nelda_logger.addHandler(logging.StreamHandler())\n```\n\n----------------------------------------\n\nTITLE: 300-Dimensional Word Embedding Vector for 'apple'\nDESCRIPTION: This snippet contains a 300-dimensional vector representation of the word 'apple' using word embedding techniques. Each float value represents a coordinate in the high-dimensional space, encoding semantic and syntactic information about the word.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/EN.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_14\n\nLANGUAGE: plaintext\nCODE:\n```\napple -4.707000218331813812e-03 -1.116089969873428345e-01 -2.327380031347274780e-01 2.159049957990646362e-01 -2.440609931945800781e-01 -1.557100005447864532e-02 -1.485549956560134888e-01 2.165099978446960449e-02 9.909100085496902466e-02 -2.301730066537857056e-01 -5.695400014519691467e-02 -1.435200031846761703e-02 1.046660020947456360e-01 -3.978500142693519592e-02 1.912519931793212891e-01 -2.741119861602783203e-01 3.595010042190551758e-01 -7.682199776172637939e-02 1.878399960696697235e-02 -2.711049914360046387e-01 -1.381520032882690430e-01 1.634999923408031464e-02 2.711099945008754730e-02 4.109900072216987610e-02 -1.017969995737075806e-01 4.515190124511718750e-01 -3.759299963712692261e-02 -3.206880092620849609e-01 -1.682640016078948975e-01 -1.552810072898864746e-01 -1.565299928188323975e-01 8.492399752140045166e-02 8.492899686098098755e-02 1.703100055456161499e-01 -2.083179950714111328e-01 -8.059100061655044556e-02 1.787919998168945312e-01 -4.957300052046775818e-02 -1.996169984340667725e-01 -1.280589997768402100e-01 -1.166910007596015930e-01 -4.054259955883026123e-01 -1.991959959268569946e-01 1.062550023198127747e-01 -2.245900034904479980e-02 -3.622199967503547668e-02 4.465720057487487793e-01 -1.150979995727539062e-01 2.108179926872253418e-01 -1.236999966204166412e-02 3.507660031318664551e-01 2.016299962997436523e-02 -5.179600045084953308e-02 2.572000026702880859e-01 1.691479980945587158e-01 2.332170009613037109e-01 -2.746120095252990723e-01 2.046599984169006348e-02 4.230400174856185913e-02 -1.446470022201538086e-01 -1.665340065956115723e-01 -3.835099935531616211e-02 1.113490015268325806e-01 1.403010040521621704e-01 2.023099921643733978e-02 -2.948609888553619385e-01 8.977999910712242126e-03 -5.283899977803230286e-02 -2.332379966974258423e-01 2.457270026206970215e-01 -2.331900000572204590e-01 -5.154999904334545135e-03 3.109419941902160645e-01 -2.664020061492919922e-01 -6.479999981820583344e-03 2.858700044453144073e-02 -2.722249925136566162e-01 2.390509992837905884e-01 -1.245789974927902222e-01 -4.275200143456459045e-02 -4.819599911570549011e-02 6.261400133371353149e-02 -6.220100075006484985e-02 1.667799986898899078e-02 -1.266130059957504272e-01 -2.006369978189468384e-01 8.258099853992462158e-02 1.798290014266967773e-01 -2.480709999799728394e-01 -5.932800099253654480e-02 -2.641399949789047241e-02 -4.753600060939788818e-01 -2.575280070304870605e-01 2.718369960784912109e-01 -1.827590018510818481e-01 -1.295900046825408936e-01 2.551620006561279297e-01 4.109709858894348145e-01 1.416379958391189575e-01 1.040140017867088318e-01 -1.028119996190071106e-01 -2.301740050315856934e-01 -1.795410066843032837e-01 -4.642099887132644653e-02 -7.029499858617782593e-02 2.069589942693710327e-01 1.099900007247924805e-01 -2.156029939651489258e-01 6.177999824285507202e-02 5.905599892139434814e-02 -7.455199956893920898e-02 6.865099817514419556e-02 4.914000164717435837e-03 -2.305909991264343262e-01 -1.801179945468902588e-01 -1.253719925880432129e-01 -8.738499879837036133e-02 3.656469881534576416e-01 -7.560499757528305054e-02 2.422299981117248535e-01 -2.170549929141998291e-01 2.711359858512878418e-01 -1.451420038938522339e-01 1.099819988012313843e-01 1.002060025930404663e-01 -2.342460006475448608e-01 -7.393600046634674072e-02 -3.642809987068176270e-01 1.775539964437484741e-01 -5.211300030350685120e-02 -2.840399928390979767e-02 8.513599634170532227e-02 2.490000054240226746e-02 -5.228599905967712402e-02 -8.793500065803527832e-02 2.287459969520568848e-01 -1.328749954700469971e-01 5.210800096392631531e-02 1.117099970579147339e-01 -2.032199949026107788e-01 -2.979280054569244385e-01 5.824000015854835510e-02 1.416050046682357788e-01 -9.411200135946273804e-02 -2.769999904558062553e-03 2.013729959726333618e-01 -1.706639975309371948e-01 -1.456299982964992523e-02 1.161520034074783325e-01 -1.550009995698928833e-01 3.844700008630752563e-02 2.892999909818172455e-02 1.305560022592544556e-01 -9.443499892950057983e-02 5.892200022935867310e-02 -2.544980049133300781e-01 -1.423449963331222534e-01 -2.275999933481216431e-01 -2.633799985051155090e-02 1.159520000219345093e-01 3.909999970346689224e-03 -2.024970054626464844e-01 -5.634399875998497009e-02 -4.145099967718124390e-02 3.066070079803466797e-01 2.603690028190612793e-01 3.092879951000213623e-01 -2.421849966049194336e-01 -8.304300159215927124e-02 1.725219935178756714e-01 4.237300157546997070e-02 -2.368029952049255371e-01 -1.213589981198310852e-01 1.126490011811256409e-01 -1.720319986343383789e-01 2.487390041351318359e-01 1.436219960451126099e-01 1.371839940547943115e-01 3.344919979572296143e-01 -3.288500010967254639e-02 3.158600032329559326e-01 7.409200072288513184e-02 -3.906489908695220947e-01 -2.706240117549896240e-01 1.860889941453933716e-01 -5.836199969053268433e-02 7.295899838209152222e-02 -1.010520011186599731e-01 6.033299863338470459e-02 7.420299947261810303e-02 1.494610011577606201e-01 -1.041930019855499268e-01 5.143300071358680725e-02 3.674589991569519043e-01 -6.570000201463699341e-02 4.436400160193443298e-02 1.249009966850280762e-01 -1.010409966111183167e-01 -1.447679996490478516e-01 -8.580300211906433105e-02 -9.635300189256668091e-02 5.462000146508216858e-03 2.002989947795867920e-01 3.479300066828727722e-02 -6.426200270652770996e-02 -8.808200061321258545e-02 2.488829940557479858e-01 9.120800346136093140e-02 -2.956619858741760254e-01 -3.141100108623504639e-01 8.964800089597702026e-02 3.520099818706512451e-02 2.512099966406822205e-02 -1.785909980535507202e-01 -2.858600020408630371e-02 -1.843640059232711792e-01 3.111500144004821777e-01 -2.885569930076599121e-01 -2.963699959218502045e-02 -1.744540035724639893e-01 1.150790005922317505e-01 -2.438649982213973999e-01 -3.940099850296974182e-02 -4.103839993476867676e-01 -4.103429913520812988e-01 -1.455600000917911530e-02 -2.096289992332458496e-01 1.233770027756690979e-01 -1.977300085127353668e-02 4.461640119552612305e-01 -3.150250017642974854e-01 -3.334040045738220215e-01 -4.311819970607757568e-01 -8.810299634933471680e-02 2.407900057733058929e-02 -1.636970043182373047e-01 -2.998709976673126221e-01 1.444150060415267944e-01 2.927800081670284271e-02 4.028600081801414490e-02 3.577800095081329346e-02 1.822620034217834473e-01 -3.052240014076232910e-01 2.275449931621551514e-01 3.649200126528739929e-02 8.072700351476669312e-02 -2.080900073051452637e-01 2.927199937403202057e-02 5.049859881401062012e-01 -8.107999712228775024e-02 -2.220730036497116089e-01 5.399699881672859192e-02 3.037899918854236603e-02 -2.229620069265365601e-01 -8.185400068759918213e-02 -4.208999872207641602e-02 -6.200600042939186096e-02 1.435820013284683228e-01 3.532019853591918945e-01 3.942880034446716309e-01 6.878499686717987061e-02 4.884560108184814453e-01 -1.887660026550292969e-01 -1.036719977855682373e-01 8.707799762487411499e-02 -2.807630002498626709e-01 -1.679700054228305817e-02 -1.411740034818649292e-01 7.213900238275527954e-02 -1.934529989957809448e-01 -1.127969995141029358e-01 2.904010117053985596e-01 -2.482119947671890259e-01 9.452600032091140747e-02 -2.322109937667846680e-01 6.758999824523925781e-02 -1.852010041475296021e-01 1.012770012021064758e-01 -3.967289924621582031e-01 1.655099913477897644e-02 7.471700012683868408e-02 9.217499941587448120e-02 1.055699959397315979e-02 -1.212529987096786499e-01 -2.211209982633590698e-01 3.940599858760833740e-01 -4.075999837368726730e-03 4.354799985885620117e-01 2.377800047397613525e-01 -3.511000052094459534e-02 -2.849479913711547852e-01 -3.537100180983543396e-02 -1.034509986639022827e-01 -2.422170042991638184e-01 -1.887760013341903687e-01 5.209099873900413513e-02 -5.649200081825256348e-02 4.406499862670898438e-02 -4.894000012427568436e-03 -6.830299645662307739e-02\n```\n\n----------------------------------------\n\nTITLE: Plotting Author-Topic Distributions using Bokeh in Python\nDESCRIPTION: This code uses Bokeh to create an interactive scatter plot of author-topic distributions, with author names and document counts as hover information.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom bokeh.models import HoverTool\nfrom bokeh.plotting import figure, show, ColumnDataSource\n\nx = tsne.embedding_[:, 0]\ny = tsne.embedding_[:, 1]\nauthor_names = [model.id2author[a] for a in authors]\n\n# Radius of each point corresponds to the number of documents attributed to that author.\nscale = 0.1\nauthor_sizes = [len(model.author2doc[a]) for a in author_names]\nradii = [size * scale for size in author_sizes]\n\nsource = ColumnDataSource(\n        data=dict(\n            x=x,\n            y=y,\n            author_names=author_names,\n            author_sizes=author_sizes,\n            radii=radii,\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Displaying Gensim Installation Information in Python\nDESCRIPTION: Shows how to use a new method to display Gensim installation parameters, which is useful for debugging and reporting issues.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/CHANGELOG.md#2025-04-21_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npython -m gensim.scripts.package_info --info\n```\n\n----------------------------------------\n\nTITLE: Reassigning Processed Documents\nDESCRIPTION: Replaces the original documents with the processed versions and frees memory by deleting the processed_docs variable.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndocs = processed_docs\ndel processed_docs\n```\n\n----------------------------------------\n\nTITLE: Checking Word Frequency in Doc2Vec Model\nDESCRIPTION: Demonstrates how to check the frequency of a specific word in the training corpus\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_doc2vec_lee.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"Word 'penalty' appeared {model.wv.get_vecattr('penalty', 'count')} times in the training corpus.\")\n```\n\n----------------------------------------\n\nTITLE: Printing LDA Topics in Python\nDESCRIPTION: This snippet demonstrates how to print the top 20 topics from a trained LDA model using Gensim. Each topic is represented by its most significant words and their corresponding weights.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/dist_lda.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> lda.print_topics(20)\ntopic #0: 0.007*disease + 0.006*medical + 0.005*treatment + 0.005*cells + 0.005*cell + 0.005*cancer + 0.005*health + 0.005*blood + 0.004*patients + 0.004*drug\ntopic #1: 0.024*king + 0.013*ii + 0.013*prince + 0.013*emperor + 0.008*duke + 0.008*empire + 0.007*son + 0.007*china + 0.007*dynasty + 0.007*iii\ntopic #2: 0.031*film + 0.017*films + 0.005*movie + 0.005*directed + 0.004*man + 0.004*episode + 0.003*character + 0.003*cast + 0.003*father + 0.003*mother\ntopic #3: 0.022*user + 0.012*edit + 0.009*wikipedia + 0.007*block + 0.007*my + 0.007*here + 0.007*edits + 0.007*blocked + 0.006*revert + 0.006*me\ntopic #4: 0.045*air + 0.026*aircraft + 0.021*force + 0.018*airport + 0.011*squadron + 0.010*flight + 0.010*military + 0.008*wing + 0.007*aviation + 0.007*f\ntopic #5: 0.025*sun + 0.022*star + 0.018*moon + 0.015*light + 0.013*stars + 0.012*planet + 0.011*camera + 0.010*mm + 0.009*earth + 0.008*lens\ntopic #6: 0.037*radio + 0.026*station + 0.022*fm + 0.014*news + 0.014*stations + 0.014*channel + 0.013*am + 0.013*racing + 0.011*tv + 0.010*broadcasting\ntopic #7: 0.122*image + 0.099*jpg + 0.046*file + 0.038*uploaded + 0.024*png + 0.014*contribs + 0.013*notify + 0.013*logs + 0.013*picture + 0.013*flag\ntopic #8: 0.036*russian + 0.030*soviet + 0.028*polish + 0.024*poland + 0.022*russia + 0.013*union + 0.012*czech + 0.011*republic + 0.011*moscow + 0.010*finland\ntopic #9: 0.031*language + 0.014*word + 0.013*languages + 0.009*term + 0.009*words + 0.008*example + 0.007*names + 0.007*meaning + 0.006*latin + 0.006*form\ntopic #10: 0.029*w + 0.029*toronto + 0.023*l + 0.020*hockey + 0.019*nhl + 0.014*ontario + 0.012*calgary + 0.011*edmonton + 0.011*hamilton + 0.010*season\ntopic #11: 0.110*wikipedia + 0.110*articles + 0.030*library + 0.029*wikiproject + 0.028*project + 0.019*data + 0.016*archives + 0.012*needing + 0.009*reference + 0.009*statements\ntopic #12: 0.032*http + 0.030*your + 0.022*request + 0.017*sources + 0.016*archived + 0.016*modify + 0.015*changes + 0.015*creation + 0.014*www + 0.013*try\ntopic #13: 0.011*your + 0.010*my + 0.009*we + 0.008*don + 0.008*get + 0.008*know + 0.007*me + 0.006*think + 0.006*question + 0.005*find\ntopic #14: 0.073*r + 0.066*japanese + 0.062*japan + 0.018*tokyo + 0.008*prefecture + 0.005*osaka + 0.004*j + 0.004*sf + 0.003*kyoto + 0.003*manga\ntopic #15: 0.045*da + 0.045*fr + 0.027*kategori + 0.026*pl + 0.024*nl + 0.021*pt + 0.017*en + 0.015*categoria + 0.014*es + 0.012*kategorie\ntopic #16: 0.010*death + 0.005*died + 0.005*father + 0.004*said + 0.004*himself + 0.004*took + 0.004*son + 0.004*killed + 0.003*murder + 0.003*wife\ntopic #17: 0.027*book + 0.021*published + 0.020*books + 0.014*isbn + 0.010*author + 0.010*magazine + 0.009*press + 0.009*novel + 0.009*writers + 0.008*story\ntopic #18: 0.027*football + 0.024*players + 0.023*cup + 0.019*club + 0.017*fc + 0.017*footballers + 0.017*league + 0.011*season + 0.007*teams + 0.007*goals\ntopic #19: 0.032*band + 0.024*album + 0.014*albums + 0.013*guitar + 0.013*rock + 0.011*records + 0.011*vocals + 0.009*live + 0.008*bass + 0.008*track\n```\n\n----------------------------------------\n\nTITLE: Loading Word Pairs from Training File\nDESCRIPTION: Reads English-Italian word pairs from the OPUS collection training file and creates tuple pairs.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/translation_matrix.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntrain_file = \"OPUS_en_it_europarl_train_5K.txt\"\n\nwith smart_open.open(train_file, \"r\") as f:\n    word_pair = [tuple(utils.to_unicode(line).strip().split()) for line in f]\nprint(word_pair[:10])\n```\n\n----------------------------------------\n\nTITLE: Displaying Test Corpus Sample\nDESCRIPTION: Prints the first two documents from the test corpus to inspect the tokenized format\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_doc2vec_lee.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(test_corpus[:2])\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging for Gensim\nDESCRIPTION: Sets up basic logging configuration for Gensim to display informational messages.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_similarity_queries.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n```\n\n----------------------------------------\n\nTITLE: Star-Filtered Sentence Generator\nDESCRIPTION: Generator function that yields sentences from reviews filtered by specific star ratings\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef StarSentences(reviews, stars=[1,2,3,4,5]):\n    for r in reviews:\n        if r['y'] in stars:\n            for s in r['x']:\n                yield s\n```\n\n----------------------------------------\n\nTITLE: Commented Code for Model Persistence in Python\nDESCRIPTION: Contains commented code for saving and loading the trained models. This is provided as an option but not executed in the current notebook flow.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence_model_selection.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# save_models(trained_models)\n# trained_models = load_models()\n```\n\n----------------------------------------\n\nTITLE: Computing SCM Similarity\nDESCRIPTION: Computing SCM similarity between related sentences using inner_product method\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_scm.rst#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsimilarity = termsim_matrix.inner_product(sentence_obama, sentence_president, normalized=(True, True))\nprint('similarity = %.4f' % similarity)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Logging and Preparing Example Sentences for WMD\nDESCRIPTION: Initializes logging and creates two sample sentences about Obama and the president, converting them to lowercase and splitting them into words for WMD comparison.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/WMD_tutorial.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Initialize logging.\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n\nsentence_obama = 'Obama speaks to the media in Illinois'\nsentence_president = 'The president greets the press in Chicago'\nsentence_obama = sentence_obama.lower().split()\nsentence_president = sentence_president.lower().split()\n```\n\n----------------------------------------\n\nTITLE: Displaying Results for Lexical Entailment Evaluation\nDESCRIPTION: This snippet displays the results of the lexical entailment evaluations. It ensures that the results are presented in a clear and informative manner.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Evaluation.ipynb#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndisplay_results('Lexical Entailment (HyperLex)', entailment_results)\n```\n\n----------------------------------------\n\nTITLE: Creating Similarity Index\nDESCRIPTION: Creates a similarity index from the TF-IDF transformed corpus for performing similarity queries.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_core_concepts.rst#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim import similarities\n\nindex = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=12)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries - Python\nDESCRIPTION: Imports necessary Python libraries for text processing, statistical analysis, and coherence modeling\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence-movies.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import print_function\n\nimport re\nimport os\n\nfrom scipy.stats import pearsonr\nfrom datetime import datetime\n\nfrom gensim.models import CoherenceModel\nfrom gensim.corpora.dictionary import Dictionary\nfrom smart_open import smart_open\n```\n\n----------------------------------------\n\nTITLE: Inferring Vector from Text\nDESCRIPTION: Shows how to infer a vector from a list of words using the trained model\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_doc2vec_lee.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nvector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\nprint(vector)\n```\n\n----------------------------------------\n\nTITLE: Using KeyedVectors most_similar with String Arguments in Python\nDESCRIPTION: Demonstrates the new ability to pass single strings directly to the positive and negative parameters of the most_similar method in KeyedVectors, instead of requiring lists.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/CHANGELOG.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nmodel.most_similar(positive='war', negative='peace')\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Query\nDESCRIPTION: Performs a similarity query between a test document and all documents in the corpus using the similarity index.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_core_concepts.rst#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nquery_document = 'system engineering'.split()\nquery_bow = dictionary.doc2bow(query_document)\nsims = index[tfidf[query_bow]]\nprint(list(enumerate(sims)))\n```\n\n----------------------------------------\n\nTITLE: Initializing HDP Model in Python\nDESCRIPTION: Creates a Hierarchical Dirichlet Process model, a non-parametric Bayesian method for topic modeling. Uses dictionary for word mappings and operates on a document corpus. Implementation based on online variational inference.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_topics_and_transformations.rst#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmodel = models.HdpModel(corpus, id2word=dictionary)\n```\n\n----------------------------------------\n\nTITLE: Creating Corpus and Dictionary for Topic Modeling\nDESCRIPTION: Defines functions to create the document corpus and dictionary with filtering for both training and test data. The first function handles training data and generates a new dictionary, while the second creates a corpus for test data using the existing dictionary.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_prediction_tutorial.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef create_corpus_dictionary(docs, max_freq=0.5, min_wordcount=20):\n    # Create a dictionary representation of the documents, and filter out frequent and rare words.\n    from gensim.corpora import Dictionary\n    dictionary = Dictionary(docs)\n\n    # Remove rare and common tokens.\n    # Filter out words that occur too frequently or too rarely.\n    max_freq = max_freq\n    min_wordcount = min_wordcount\n    dictionary.filter_extremes(no_below=min_wordcount, no_above=max_freq)\n\n    _ = dictionary[0]  # This sort of \"initializes\" dictionary.id2token.\n\n    # Vectorize data.\n    # Bag-of-words representation of the documents.\n    corpus = [dictionary.doc2bow(doc) for doc in docs]\n\n    return corpus, dictionary\n\ndef create_test_corpus(train_dictionary, docs):\n    # Create test corpus using the dictionary from the train data.\n    return [train_dictionary.doc2bow(doc) for doc in docs]\n```\n\n----------------------------------------\n\nTITLE: Initializing Gensim Models Dictionary for Link Prediction in Python\nDESCRIPTION: Creating an empty dictionary to store Gensim implementation model files for the link prediction task. This prepares for training Gensim models on the WordNet training data.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Evaluation.ipynb#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nlp_model_files['gensim'] = {}\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Logging and Auto-reload\nDESCRIPTION: Optional configuration for logging setup and auto-reload functionality during development\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# ### uncomment below if you want...\n# ## ... copious amounts of logging info\n# import logging\n# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n# rootLogger = logging.getLogger()\n# rootLogger.setLevel(logging.INFO)\n# ## ... or auto-reload of gensim during development\n# %load_ext autoreload\n# %autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple Document Corpus\nDESCRIPTION: Defines a small corpus of nine documents, each being a single sentence, for text analysis demonstration.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_corpora_and_vector_spaces.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndocuments = [\n    \"Human machine interface for lab abc computer applications\",\n    \"A survey of user opinion of computer system response time\",\n    \"The EPS user interface management system\",\n    \"System and human system engineering testing of EPS\",\n    \"Relation of user perceived response time to error measurement\",\n    \"The generation of random binary unordered trees\",\n    \"The intersection graph of paths in trees\",\n    \"Graph minors IV Widths of trees and well quasi ordering\",\n    \"Graph minors A survey\",\n]\n```\n\n----------------------------------------\n\nTITLE: Stopwords Display\nDESCRIPTION: Displays stopwords for the specified language\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/wikinews-bigram-en.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# which are the stop words we will use\nfrom nltk.corpus import stopwords\n\" \".join(stopwords.words(LANG))\n```\n\n----------------------------------------\n\nTITLE: Monitoring Word2Vec Training Progress with Gensim Logging\nDESCRIPTION: Log output from Gensim showing the training progress of a Word2Vec model across multiple epochs. The logs track completion percentage, processing speed (words/second), and queue statuses at regular intervals. Complete epoch statistics are shown at the end of each epoch.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_25\n\nLANGUAGE: log\nCODE:\n```\n2023-08-23 13:01:35,954 : INFO : EPOCH 16 - PROGRESS: at 73.10% examples, 553628 words/s, in_qsize 4, out_qsize 0\n2023-08-23 13:01:36,957 : INFO : EPOCH 16 - PROGRESS: at 75.47% examples, 553490 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:01:37,961 : INFO : EPOCH 16 - PROGRESS: at 77.95% examples, 554032 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:01:38,964 : INFO : EPOCH 16 - PROGRESS: at 80.36% examples, 553904 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:01:39,968 : INFO : EPOCH 16 - PROGRESS: at 82.74% examples, 553754 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:01:40,989 : INFO : EPOCH 16 - PROGRESS: at 85.16% examples, 553623 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:01:41,993 : INFO : EPOCH 16 - PROGRESS: at 87.74% examples, 554390 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:01:43,019 : INFO : EPOCH 16 - PROGRESS: at 90.30% examples, 554724 words/s, in_qsize 4, out_qsize 0\n2023-08-23 13:01:44,034 : INFO : EPOCH 16 - PROGRESS: at 92.86% examples, 555210 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:01:45,036 : INFO : EPOCH 16 - PROGRESS: at 95.31% examples, 555361 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:01:46,049 : INFO : EPOCH 16 - PROGRESS: at 97.74% examples, 554873 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:01:46,946 : INFO : EPOCH 16: training on 23279529 raw words (22951015 effective words) took 41.3s, 555250 effective words/s\n2023-08-23 13:01:47,950 : INFO : EPOCH 17 - PROGRESS: at 2.45% examples, 551192 words/s, in_qsize 2, out_qsize 1\n2023-08-23 13:01:48,974 : INFO : EPOCH 17 - PROGRESS: at 4.86% examples, 543989 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:01:49,981 : INFO : EPOCH 17 - PROGRESS: at 7.34% examples, 548902 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:01:51,007 : INFO : EPOCH 17 - PROGRESS: at 9.66% examples, 543517 words/s, in_qsize 4, out_qsize 0\n2023-08-23 13:01:52,038 : INFO : EPOCH 17 - PROGRESS: at 12.16% examples, 545699 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:01:53,039 : INFO : EPOCH 17 - PROGRESS: at 14.61% examples, 549717 words/s, in_qsize 4, out_qsize 0\n2023-08-23 13:01:54,044 : INFO : EPOCH 17 - PROGRESS: at 16.99% examples, 549641 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:01:55,064 : INFO : EPOCH 17 - PROGRESS: at 19.40% examples, 550628 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:01:56,079 : INFO : EPOCH 17 - PROGRESS: at 21.82% examples, 550731 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:01:57,090 : INFO : EPOCH 17 - PROGRESS: at 24.27% examples, 552099 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:01:58,090 : INFO : EPOCH 17 - PROGRESS: at 26.64% examples, 551244 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:01:59,106 : INFO : EPOCH 17 - PROGRESS: at 29.02% examples, 550572 words/s, in_qsize 4, out_qsize 0\n2023-08-23 13:02:00,109 : INFO : EPOCH 17 - PROGRESS: at 31.41% examples, 550543 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:01,117 : INFO : EPOCH 17 - PROGRESS: at 33.69% examples, 548272 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:02,147 : INFO : EPOCH 17 - PROGRESS: at 35.98% examples, 545637 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:03,159 : INFO : EPOCH 17 - PROGRESS: at 38.39% examples, 545652 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:04,188 : INFO : EPOCH 17 - PROGRESS: at 40.70% examples, 543810 words/s, in_qsize 3, out_qsize 1\n2023-08-23 13:02:05,189 : INFO : EPOCH 17 - PROGRESS: at 43.10% examples, 544174 words/s, in_qsize 4, out_qsize 0\n2023-08-23 13:02:06,196 : INFO : EPOCH 17 - PROGRESS: at 45.33% examples, 541822 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:07,205 : INFO : EPOCH 17 - PROGRESS: at 47.77% examples, 542108 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:08,206 : INFO : EPOCH 17 - PROGRESS: at 50.02% examples, 541598 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:09,217 : INFO : EPOCH 17 - PROGRESS: at 52.42% examples, 541686 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:10,251 : INFO : EPOCH 17 - PROGRESS: at 54.87% examples, 542276 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:11,267 : INFO : EPOCH 17 - PROGRESS: at 57.35% examples, 542672 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:12,288 : INFO : EPOCH 17 - PROGRESS: at 59.67% examples, 541419 words/s, in_qsize 4, out_qsize 0\n2023-08-23 13:02:13,320 : INFO : EPOCH 17 - PROGRESS: at 62.24% examples, 542228 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:14,323 : INFO : EPOCH 17 - PROGRESS: at 64.80% examples, 543562 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:15,325 : INFO : EPOCH 17 - PROGRESS: at 67.29% examples, 544834 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:16,326 : INFO : EPOCH 17 - PROGRESS: at 69.70% examples, 545409 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:17,329 : INFO : EPOCH 17 - PROGRESS: at 72.19% examples, 546203 words/s, in_qsize 4, out_qsize 0\n2023-08-23 13:02:18,345 : INFO : EPOCH 17 - PROGRESS: at 74.60% examples, 545857 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:19,352 : INFO : EPOCH 17 - PROGRESS: at 77.00% examples, 546494 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:20,360 : INFO : EPOCH 17 - PROGRESS: at 79.39% examples, 546544 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:21,371 : INFO : EPOCH 17 - PROGRESS: at 81.79% examples, 546265 words/s, in_qsize 4, out_qsize 0\n2023-08-23 13:02:22,373 : INFO : EPOCH 17 - PROGRESS: at 84.11% examples, 546029 words/s, in_qsize 4, out_qsize 0\n2023-08-23 13:02:23,384 : INFO : EPOCH 17 - PROGRESS: at 86.53% examples, 546058 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:24,386 : INFO : EPOCH 17 - PROGRESS: at 88.96% examples, 546264 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:25,408 : INFO : EPOCH 17 - PROGRESS: at 91.48% examples, 546599 words/s, in_qsize 4, out_qsize 0\n2023-08-23 13:02:26,410 : INFO : EPOCH 17 - PROGRESS: at 93.86% examples, 546500 words/s, in_qsize 3, out_qsize 1\n2023-08-23 13:02:27,424 : INFO : EPOCH 17 - PROGRESS: at 96.39% examples, 546935 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:28,424 : INFO : EPOCH 17 - PROGRESS: at 98.93% examples, 547510 words/s, in_qsize 4, out_qsize 0\n2023-08-23 13:02:28,839 : INFO : EPOCH 17: training on 23279529 raw words (22951015 effective words) took 41.9s, 547865 effective words/s\n2023-08-23 13:02:29,843 : INFO : EPOCH 18 - PROGRESS: at 2.41% examples, 541842 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:30,850 : INFO : EPOCH 18 - PROGRESS: at 4.86% examples, 548560 words/s, in_qsize 4, out_qsize 0\n2023-08-23 13:02:31,853 : INFO : EPOCH 18 - PROGRESS: at 7.37% examples, 555955 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:32,863 : INFO : EPOCH 18 - PROGRESS: at 9.74% examples, 553429 words/s, in_qsize 4, out_qsize 0\n2023-08-23 13:02:33,867 : INFO : EPOCH 18 - PROGRESS: at 12.07% examples, 548884 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:34,876 : INFO : EPOCH 18 - PROGRESS: at 14.45% examples, 548441 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:35,898 : INFO : EPOCH 18 - PROGRESS: at 16.85% examples, 548571 words/s, in_qsize 4, out_qsize 0\n2023-08-23 13:02:36,909 : INFO : EPOCH 18 - PROGRESS: at 19.29% examples, 550327 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:37,914 : INFO : EPOCH 18 - PROGRESS: at 21.68% examples, 551082 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:38,924 : INFO : EPOCH 18 - PROGRESS: at 24.09% examples, 551585 words/s, in_qsize 4, out_qsize 0\n2023-08-23 13:02:39,945 : INFO : EPOCH 18 - PROGRESS: at 26.52% examples, 550578 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:40,958 : INFO : EPOCH 18 - PROGRESS: at 28.81% examples, 548436 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:41,968 : INFO : EPOCH 18 - PROGRESS: at 31.15% examples, 547487 words/s, in_qsize 4, out_qsize 0\n2023-08-23 13:02:42,978 : INFO : EPOCH 18 - PROGRESS: at 33.57% examples, 547460 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:43,993 : INFO : EPOCH 18 - PROGRESS: at 36.05% examples, 548614 words/s, in_qsize 4, out_qsize 0\n2023-08-23 13:02:45,012 : INFO : EPOCH 18 - PROGRESS: at 38.65% examples, 550613 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:46,019 : INFO : EPOCH 18 - PROGRESS: at 41.07% examples, 550815 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:47,056 : INFO : EPOCH 18 - PROGRESS: at 43.53% examples, 550271 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:48,058 : INFO : EPOCH 18 - PROGRESS: at 45.96% examples, 550236 words/s, in_qsize 4, out_qsize 0\n2023-08-23 13:02:49,074 : INFO : EPOCH 18 - PROGRESS: at 48.44% examples, 550387 words/s, in_qsize 4, out_qsize 0\n2023-08-23 13:02:50,097 : INFO : EPOCH 18 - PROGRESS: at 50.87% examples, 550739 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:51,099 : INFO : EPOCH 18 - PROGRESS: at 53.23% examples, 550244 words/s, in_qsize 4, out_qsize 0\n2023-08-23 13:02:52,110 : INFO : EPOCH 18 - PROGRESS: at 55.60% examples, 550185 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:53,122 : INFO : EPOCH 18 - PROGRESS: at 58.00% examples, 549500 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:54,157 : INFO : EPOCH 18 - PROGRESS: at 60.55% examples, 549617 words/s, in_qsize 3, out_qsize 1\n2023-08-23 13:02:55,171 : INFO : EPOCH 18 - PROGRESS: at 63.10% examples, 550415 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:56,177 : INFO : EPOCH 18 - PROGRESS: at 65.57% examples, 551075 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:57,201 : INFO : EPOCH 18 - PROGRESS: at 68.08% examples, 551312 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:58,208 : INFO : EPOCH 18 - PROGRESS: at 70.47% examples, 551570 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:02:59,217 : INFO : EPOCH 18 - PROGRESS: at 72.92% examples, 551728 words/s, in_qsize 4, out_qsize 0\n```\n\n----------------------------------------\n\nTITLE: Running Benchmark and Displaying Results for Document-to-Document Similarity in Python\nDESCRIPTION: This code executes the benchmark for document-to-document similarity using the configurations generated earlier. It then processes and displays the results, showing mean values and standard deviations for various metrics.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_benchmark.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nconfigurations = product(matrices, corpora, normalization, repetitions)\nresults = benchmark_results(benchmark, configurations, \"matrix_speed.inner-product_results.doc_doc\")\n\ndf = pd.DataFrame(results)\ndf[\"speed\"] = df.corpus_actual_size**2 / df.duration\ndel df[\"corpus_actual_size\"]\ndf = df.groupby([\"dictionary_size\", \"corpus_size\", \"nonzero_limit\", \"normalized\"])\n\ndef display(df):\n    df[\"duration\"] = [timedelta(0, duration) for duration in df[\"duration\"]]\n    df[\"speed\"] = [\"%.02f Kdoc pairs / s\" % (speed / 1000) for speed in df[\"speed\"]]\n    return df\n\ndisplay(df.mean()).loc[\n    [1000, 100000], :, [1, 100], :].loc[\n    :, [\"duration\", \"corpus_nonzero\", \"matrix_nonzero\", \"speed\"]]\n\ndisplay(df.apply(lambda x: (x - x.mean()).std())).loc[\n    [1000, 100000], :, [1, 100], :].loc[\n    :, [\"duration\", \"corpus_nonzero\", \"matrix_nonzero\", \"speed\"]]\n```\n\n----------------------------------------\n\nTITLE: Loading Metadata and Accessing Document Information in Python\nDESCRIPTION: This snippet demonstrates how to load metadata from a pickle file and access document information such as title. It uses the pickle library to deserialize data and print the title of the first document in the corpus.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/wiki.rst#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport pickle  # noqa: E402\n\n# Get an article and its topic distribution\nwith open(\"wiki_en_bow.mm.metadata.cpickle\", 'rb') as meta_file:\n    docno2metadata = pickle.load(meta_file)\n\ndoc_num = 0\nprint(\"Title: {}\".format(docno2metadata[doc_num][1]))  # take the first article as an example\nTitle: Anarchism\n\nvec = mm[doc_num]  # get tf-idf vector\nlda.get_document_topics(vec)\n[(1, 0.028828567), (10, 0.32766217), (36, 0.021675354), (55, 0.2521854), (57, 0.27154338)]\n```\n\n----------------------------------------\n\nTITLE: Displaying Training Corpus Sample\nDESCRIPTION: Prints the first two documents from the training corpus to inspect the tagged document format\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_doc2vec_lee.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint(train_corpus[:2])\n```\n\n----------------------------------------\n\nTITLE: PCA and Plotly Visualization Setup\nDESCRIPTION: Prepares document vectors for visualization by applying PCA dimensionality reduction to reduce vectors to 2D, setting up Plotly visualization parameters.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/translation_matrix.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.decomposition import PCA\nimport plotly\nfrom plotly.graph_objs import Scatter, Layout, Figure\nplotly.offline.init_notebook_mode(connected=True)\n\nm1_part = m1[14995: 15000]\nm2_part = m2[14995: 15000]\n\nm1_part = np.array(m1_part).reshape(len(m1_part), 100)\nm2_part = np.array(m2_part).reshape(len(m2_part), 100)\n\npca = PCA(n_components=2)\nreduced_vec1 = pca.fit_transform(m1_part)\nreduced_vec2 = pca.fit_transform(m2_part)\n```\n\n----------------------------------------\n\nTITLE: Creating Sample Corpus\nDESCRIPTION: Creates a test corpus using gensim Dictionary class and sample texts\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/sklearn_api.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.corpora import Dictionary\ntexts = [\n    ['complier', 'system', 'computer'],\n    ['eulerian', 'node', 'cycle', 'graph', 'tree', 'path'],\n    ['graph', 'flow', 'network', 'graph'],\n    ['loading', 'computer', 'system'],\n    ['user', 'server', 'system'],\n    ['tree', 'hamiltonian'],\n    ['graph', 'trees'],\n    ['computer', 'kernel', 'malfunction', 'computer'],\n    ['server', 'system', 'computer']\n]\ndictionary = Dictionary(texts)\ncorpus = [dictionary.doc2bow(text) for text in texts]\n```\n\n----------------------------------------\n\nTITLE: Accessing Stored LDA Training Metrics\nDESCRIPTION: A simple line of code showing how to access the stored metrics from a trained LDA model instance for custom use.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Training_visualizations.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel.metrics\n```\n\n----------------------------------------\n\nTITLE: Displaying Standard Deviation of Benchmark Results in Python\nDESCRIPTION: This code snippet calculates and displays the standard deviation of the benchmark results for specific configurations. It uses the same filtering as the mean results display, providing insight into the variability of the performance metrics across repeated measurements.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_benchmark.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndisplay(df.apply(lambda x: (x - x.mean()).std())).loc[\n    [1000000, len(full_dictionary)], [1, 100], [0, 1, 100]].loc[\n    :, [\"constructor_duration\", \"production_duration\", \"production_speed\", \"processing_speed\"]]\n```\n\n----------------------------------------\n\nTITLE: Sklearn Integration Imports\nDESCRIPTION: Imports required for integrating with scikit-learn and loading test data\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/sklearn_api.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom gensim import matutils\nfrom gensim.models.ldamodel import LdaModel\nfrom sklearn.datasets import fetch_20newsgroups\nfrom gensim.sklearn_api.ldamodel import LdaTransformer\n```\n\n----------------------------------------\n\nTITLE: Setting Up Logging for Model Evaluation\nDESCRIPTION: Configures logging to track the evaluation process of word embeddings. This will display information about the analogical reasoning task during model evaluation.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n```\n\n----------------------------------------\n\nTITLE: Improved Phrases Detection with Common Terms\nDESCRIPTION: Example showing how to use common terms (like stopwords) in phrase detection to better identify multi-word expressions.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/CHANGELOG.md#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nphr_old = Phrases(corpus)\nphr_new = Phrases(corpus, common_terms=stopwords.words('en'))\n\nprint(phr_old[[\"we\", \"provide\", \"car\", \"with\", \"driver\"]])  # [\"we\", \"provide\", \"car_with\", \"driver\"]\nprint(phr_new[[\"we\", \"provide\", \"car\", \"with\", \"driver\"]])  # [\"we\", \"provide\", \"car_with_driver\"]\n```\n\n----------------------------------------\n\nTITLE: Loading Topics and Human Scores - Python\nDESCRIPTION: Reads topic lists and human-assigned coherence scores from reference files\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence-movies.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntopics = []  # list of 100 topics\nwith smart_open(topics_path, 'rb') as f:\n    topics = [line.split() for line in f if line]\nlen(topics)\n\nhuman_scores = []\nwith smart_open(human_scores_path, 'rb') as f:\n    for line in f:\n        human_scores.append(float(line.strip()))\nlen(human_scores)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Data Paths - Python\nDESCRIPTION: Configures file paths for accessing the movie corpus and reference data\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence-movies.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nbase_dir = os.path.join(os.path.expanduser('~'), \"workshop/nlp/data/\")\ndata_dir = os.path.join(base_dir, 'wiki-movie-subset')\nif not os.path.exists(data_dir):\n    raise ValueError(\"SKIP: Please download the movie corpus.\")\n\nref_dir = os.path.join(base_dir, 'reference')\ntopics_path = os.path.join(ref_dir, 'topicsMovie.txt')\nhuman_scores_path = os.path.join(ref_dir, 'goldMovie.txt')\n```\n\n----------------------------------------\n\nTITLE: Initializing Logging Configuration in Python\nDESCRIPTION: Sets up basic logging configuration with timestamp, level and message format.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing Newsgroup Dataset in Python\nDESCRIPTION: Downloads the 20-newsgroups dataset using Gensim's downloader API and splits it into training and test sets for two specific categories: 'sci.electronics' and 'sci.space', with corresponding labels 0 and 1.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/pivoted_document_length_normalisation.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#\n# Download our dataset\n#\nimport gensim.downloader as api\nnws = api.load(\"20-newsgroups\")\n\n#\n# Pick texts from relevant newsgroups, split into training and test set.\n#\ncat1, cat2 = ('sci.electronics', 'sci.space')\n\n#\n# X_* contain the actual texts as strings.\n# Y_* contain labels, 0 for cat1 (sci.electronics) and 1 for cat2 (sci.space)\n#\nX_train = []\nX_test = []\ny_train = []\ny_test = []\n\nfor i in nws:\n    if i[\"set\"] == \"train\" and i[\"topic\"] == cat1:\n        X_train.append(i[\"data\"])\n        y_train.append(0)\n    elif i[\"set\"] == \"train\" and i[\"topic\"] == cat2:\n        X_train.append(i[\"data\"])\n        y_train.append(1)\n    elif i[\"set\"] == \"test\" and i[\"topic\"] == cat1:\n        X_test.append(i[\"data\"])\n        y_test.append(0)\n    elif i[\"set\"] == \"test\" and i[\"topic\"] == cat2:\n        X_test.append(i[\"data\"])\n        y_test.append(1)\n```\n\n----------------------------------------\n\nTITLE: Downloading NIPS Dataset with wget in Python\nDESCRIPTION: Downloads the NIPS conference papers dataset from Sam Roweis' website using wget command.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!wget -O - 'http://www.cs.nyu.edu/~roweis/data/nips12raw_str602.tgz' > /tmp/nips12raw_str602.tgz\n```\n\n----------------------------------------\n\nTITLE: Logging Word2Vec Training Progress in Gensim\nDESCRIPTION: This log snippet shows the progress of training a Word2Vec model using Gensim. It logs the epoch number, percentage completion, words processed per second, and input/output queue sizes at regular intervals.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_20\n\nLANGUAGE: log\nCODE:\n```\n2023-08-23 12:54:35,354 : INFO : EPOCH 6 - PROGRESS: at 67.98% examples, 551483 words/s, in_qsize 4, out_qsize 0\n2023-08-23 12:54:36,371 : INFO : EPOCH 6 - PROGRESS: at 70.39% examples, 551536 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:54:37,381 : INFO : EPOCH 6 - PROGRESS: at 72.87% examples, 552004 words/s, in_qsize 3, out_qsize 0\n```\n\n----------------------------------------\n\nTITLE: File Existence Check and Data Loading\nDESCRIPTION: Test data file existence and load training reviews into memory with randomization\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    next(YelpReviews(\"test\"))\nexcept FileNotFoundError:\n    raise ValueError(\"SKIP: Please download the yelp_test_set.zip\")\n\nrevtrain = list(YelpReviews(\"training\"))\nprint(len(revtrain), \"training reviews\")\n\n## and shuffle just in case they are ordered\nimport numpy as np\nnp.random.shuffle(revtrain)\n```\n\n----------------------------------------\n\nTITLE: Downloading Text8 Corpus for Word2Vec training\nDESCRIPTION: Downloads the Text8 corpus if it doesn't exist in the current directory. This corpus will be used to train the Word2Vec model.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os.path\nif not os.path.isfile('text8'):\n    !wget -c https://mattmahoney.net/dc/text8.zip\n    !unzip text8.zip\n```\n\n----------------------------------------\n\nTITLE: Defining reStructuredText Documentation Block for Gensim Interfaces\nDESCRIPTION: A reStructuredText documentation block that configures the autodoc extension to generate documentation for the gensim.interfaces module, including all members, inherited members, undocumented members, and inheritance diagrams.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/interfaces.rst#2025-04-21_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. automodule:: gensim.interfaces\n    :synopsis: Core gensim interfaces\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Generating Benchmark Configurations in Python\nDESCRIPTION: This code snippet generates various configurations for benchmarking, including different dictionary sizes, corpus sizes, models, and matrix configurations. It prepares the data for subsequent benchmark runs.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_benchmark.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nseed(RANDOM_SEED)\ndictionary_sizes = [1000, 100000]\ndictionaries = []\nfor size in tqdm(dictionary_sizes, desc=\"dictionaries\"):\n    dictionary = Dictionary([sample(list(full_dictionary.values()), size)])\n    dictionaries.append(dictionary)\nmin_dictionary = sorted((len(dictionary), dictionary) for dictionary in dictionaries)[0][1]\n\ncorpus_sizes = [100, 1000]\ncorpora = []\nfor size in tqdm(corpus_sizes, desc=\"corpora\"):\n    corpus = sample(full_corpus, size)\n    corpora.append(corpus)\n\nmodels = []\nfor dictionary in tqdm(dictionaries, desc=\"models\"):\n    if dictionary == full_dictionary:\n        models.append(full_model)\n        continue\n    model = full_model.__class__(full_model.vector_size)\n    model.vocab = {word: deepcopy(full_model.vocab[word]) for word in dictionary.values()}\n    model.index2entity = []\n    vector_indices = []\n    for index, word in enumerate(full_model.index2entity):\n        if word in model.vocab.keys():\n            model.index2entity.append(word)\n            model.vocab[word].index = len(vector_indices)\n            vector_indices.append(index)\n    model.vectors = full_model.vectors[vector_indices]\n    models.append(model)\n\nnonzero_limits = [1, 10, 100]\nmatrices = []\nfor (model, dictionary), nonzero_limit in tqdm(\n        list(product(zip(models, dictionaries), nonzero_limits)), desc=\"matrices\"):\n    annoy = AnnoyIndexer(model, 1)\n    index = WordEmbeddingSimilarityIndex(model, kwargs={\"indexer\": annoy})\n    matrix = SparseTermSimilarityMatrix(index, dictionary, nonzero_limit=nonzero_limit)\n    matrices.append((matrix, dictionary, nonzero_limit))\n    del annoy\n\nnormalization = (True, False)\nrepetitions = range(10)\n```\n\n----------------------------------------\n\nTITLE: Finding Most Similar Words with WordRank Model in Python\nDESCRIPTION: This code snippet shows how to use the most_similar method from the Keyed Vector interface on a trained WordRank model to find words that are semantically closest to 'President'.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/WordRank_wrapper_quickstart.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel.most_similar('President')\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Similarity Index\nDESCRIPTION: Demonstrates how to save and load a similarity index for persistence.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_similarity_queries.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nindex.save('/tmp/deerwester.index')\nindex = similarities.MatrixSimilarity.load('/tmp/deerwester.index')\n```\n\n----------------------------------------\n\nTITLE: Importing HashDictionary Module in Python\nDESCRIPTION: This snippet shows how to import the hashdictionary module from gensim.corpora. The module provides functionality for constructing word-to-id mappings using the hashing trick.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/corpora/hashdictionary.rst#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom gensim.corpora import hashdictionary\n```\n\n----------------------------------------\n\nTITLE: Reading Interlinks Data from segment_wiki Output (Python)\nDESCRIPTION: This Python snippet shows how to read and process the interlinks data generated by the segment_wiki script. It uses the smart_open library to handle compressed files and demonstrates extracting source and destination nodes from the interlinks JSON structure.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/CHANGELOG.md#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom smart_open import smart_open\n\nwith smart_open(\"enwiki-latest.jsonl.gz\") as infile:\n    for doc in infile:\n        doc = json.loads(doc)\n\n        src_node = doc['title']\n        dst_nodes = doc['interlinks'].keys()\n\n        print(u\"Source node: {}\".format(src_node))\n        print(u\"Destination nodes: {}\".format(u\", \".join(dst_nodes)))\n        break\n```\n\n----------------------------------------\n\nTITLE: 300-Dimensional Vector Representation of 'uccelli'\nDESCRIPTION: This snippet shows a 300-dimensional vector representation of the Italian word 'uccelli' (meaning 'birds' in English). Each number represents a dimension in the vector space, capturing semantic information about the word based on its usage in a large corpus of text.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/IT.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_11\n\nLANGUAGE: plaintext\nCODE:\n```\nuccelli -3.331049978733062744e-01 -9.601099789142608643e-02 -6.954299658536911011e-02 3.531200066208839417e-02 2.957499921321868896e-01 -7.897999882698059082e-02 -2.876169979572296143e-01 4.708899930119514465e-02 3.079000115394592285e-01 -3.115369975566864014e-01 3.984000161290168762e-02 -7.210999727249145508e-02 4.129600152373313904e-02 -2.965039908885955811e-01 4.613199830055236816e-02 -1.354209929704666138e-01 -3.247400000691413879e-02 1.255819946527481079e-01 -2.034990042448043823e-01 -4.146690070629119873e-01 1.338569968938827515e-01 -1.152260005474090576e-01 -3.956000134348869324e-02 2.153040021657943726e-01 9.510999917984008789e-02 -9.473799914121627808e-02 1.399100013077259064e-02 -4.308930039405822754e-01 2.681280076503753662e-01 -1.338990032672882080e-01 5.364350080490112305e-01 -2.161199972033500671e-02 1.163220033049583435e-01 -1.295560002326965332e-01 -8.369400352239608765e-02 -2.189999967813491821e-01 1.467400044202804565e-01 2.587920129299163818e-01 1.750539988279342651e-01 9.529100358486175537e-02 4.649269878864288330e-01 3.134899958968162537e-02 -4.488919973373413086e-01 -4.968599975109100342e-02 -1.429959982633590698e-01 2.653029859066009521e-01 1.477209925651550293e-01 3.328499943017959595e-02 -1.430409997701644897e-01 8.135200291872024536e-02 2.193620055913925171e-01 1.045090034604072571e-01 -6.168600171804428101e-02 1.117710024118423462e-01 -8.164600282907485962e-02 -2.124769985675811768e-01 2.087769955396652222e-01 -3.400470018386840820e-01 1.519600022584199905e-02 1.300159990787506104e-01 -4.800809919834136963e-01 -3.079200088977813721e-01 -9.266999550163745880e-03 -3.282670080661773682e-01 -2.588360011577606201e-01 -1.483370065689086914e-01 -7.729499787092208862e-02 -3.157170116901397705e-01 2.570099942386150360e-02 -8.629799634218215942e-02 9.986100345849990845e-02 -1.539559960365295410e-01 -1.359499990940093994e-01 -2.977809906005859375e-01 1.668410003185272217e-01 -2.198349982500076294e-01 7.017000112682580948e-03 9.971699863672256470e-02 -3.173870146274566650e-01 -2.281350046396255493e-01 1.875189989805221558e-01 -3.628070056438446045e-01 -1.881850063800811768e-01 1.263349950313568115e-01 -1.327500026673078537e-02 1.012099981307983398e-01 -9.766899794340133667e-02 -7.066600024700164795e-02 1.094919964671134949e-01 2.425339967012405396e-01 -4.038900136947631836e-02 2.096289992332458496e-01 -9.719099849462509155e-02 -2.574380040168762207e-01 3.507500141859054565e-02 3.829340040683746338e-01 1.725440025329589844e-01 1.994269937276840210e-01 -5.725200101733207703e-02 -2.856909930706024170e-01 3.011599928140640259e-02 2.392130047082901001e-01 -3.253059983253479004e-01 9.254299849271774292e-02 1.265559941530227661e-01 1.833370029926300049e-01 -1.861000061035156250e-01 -3.221090137958526611e-01 1.659480035305023193e-01 2.253520041704177856e-01 -3.450070023536682129e-01 -1.658110022544860840e-01 -1.766020059585571289e-01 2.506470084190368652e-01 3.707709908485412598e-01 5.919099971652030945e-02 -2.023119926452636719e-01 -1.919160038232803345e-01 -1.894740015268325806e-01 2.452600002288818359e-02 1.852310001850128174e-01 -1.278859972953796387e-01 -2.344570010900497437e-01 -1.017979979515075684e-01 3.442150056362152100e-01 -1.598369926214218140e-01 2.375269979238510132e-01 3.616189956665039062e-01 2.643559873104095459e-01 9.045299887657165527e-02 4.911199957132339478e-02 1.607549935579299927e-01 3.649620115756988525e-01 8.436899632215499878e-02 -7.736799865961074829e-02 1.332300007343292236e-01 -3.493990004062652588e-01 -4.259929955005645752e-01 -4.918399825692176819e-02 -1.486549973487854004e-01 4.221700131893157959e-01 -2.611400000751018524e-02 9.130199998617172241e-02 7.876499742269515991e-02 -1.853920072317123413e-01 1.180889979004859924e-01 2.706989943981170654e-01 -1.373769938945770264e-01 -1.109310016036033630e-01 3.116970062255859375e-01 3.584159910678863525e-01 -2.284329980611801147e-01 4.275999963283538818e-03 -3.846900165081024170e-02 1.767150014638900757e-01 -1.115600019693374634e-01 -1.246879994869232178e-01 2.866500057280063629e-02 -2.774699926376342773e-01 1.550800055265426636e-01 -9.399999678134918213e-02 -4.924599826335906982e-02 9.475599974393844604e-02 5.554600059986114502e-02 -1.751999952830374241e-03 -8.117900043725967407e-02 -2.618739902973175049e-01 1.883659958839416504e-01 1.413269937038421631e-01 1.337019950151443481e-01 2.684690058231353760e-01 -1.189590021967887878e-01 -2.660140097141265869e-01 -1.740629971027374268e-01 -1.923909932374954224e-01 2.095389962196350098e-01 -1.230439990758895874e-01 -2.443809956312179565e-01 -3.618530035018920898e-01 2.169370055198669434e-01 -4.279339909553527832e-01 -9.456600248813629150e-02 1.351269930601119995e-01 -3.032180070877075195e-01 5.257000215351581573e-03 -2.157890051603317261e-01 -6.300000241026282310e-04 1.208600029349327087e-02 -6.269600242376327515e-02 -2.566109895706176758e-01 -2.868179976940155029e-01 1.519590020179748535e-01 -5.084400177001953125e-01 -1.848919987678527832e-01 4.492590129375457764e-01 -3.534809947013854980e-01 -9.374400228261947632e-02 -5.246900022029876709e-02 -9.229599684476852417e-02 4.734390079975128174e-01 7.727800309658050537e-02 -1.366139948368072510e-01 6.825300306081771851e-02 -1.306439936161041260e-01 4.139300063252449036e-02 8.166900277137756348e-02 -2.013819962739944458e-01 -2.582800015807151794e-02 1.982350051403045654e-01 4.376429915428161621e-01 6.081999838352203369e-02 2.585459947586059570e-01 6.625299900770187378e-02 -3.610499948263168335e-02 -3.996099904179573059e-02 2.376810014247894287e-01 2.698099985718727112e-02 -3.058500029146671295e-02 2.206400036811828613e-02 -2.278410047292709351e-01 1.027610003948211670e-01 -1.497949957847595215e-01 -5.102990269660949707e-01 -1.011999975889921188e-02 3.664100170135498047e-02 7.027400285005569458e-02 4.306110143661499023e-01 6.858800351619720459e-02 -1.209639981389045715e-01 -2.593829929828643799e-01 1.348499953746795654e-02 -8.403900265693664551e-02 -9.867800027132034302e-02 -2.498780041933059692e-01 3.747199848294258118e-02 3.303599953651428223e-01 3.084869980812072754e-01 8.563099801540374756e-02 3.525060117244720459e-01 9.544800221920013428e-02 -1.043099984526634216e-01 3.417559862136840820e-01 -1.345000043511390686e-02 1.912830024957656860e-01 -4.532999917864799500e-02 5.855200067162513733e-02 -2.499200031161308289e-02 1.141230016946792603e-01 2.303300052881240845e-02 2.676039934158325195e-01 4.262929856777191162e-01 -1.223189979791641235e-01 7.781100273132324219e-02 -1.600600033998489380e-02 -2.256090044975280762e-01 1.659609973430633545e-01 2.302799969911575317e-01 2.678200043737888336e-02 2.832239866256713867e-01 -5.625300109386444092e-02 -8.059500157833099365e-02 2.096600085496902466e-02 -2.355339974164962769e-01 -4.820000007748603821e-02 -3.018999937921762466e-03 2.921310067176818848e-01 7.519000209867954254e-03 1.367129981517791748e-01 2.716699987649917603e-02 1.626400090754032135e-02 -1.383139938116073608e-01 1.982440054416656494e-01 8.290500193834304810e-02 2.055460065603256226e-01 5.686499923467636108e-02 -4.808320105075836182e-01 3.167929947376251221e-01 -3.898839950561523438e-01 -2.035640031099319458e-01 5.449600145220756531e-02 1.084479987621307373e-01 -4.270999878644943237e-02 -1.344500035047531128e-01 -1.788599938154220581e-01 -1.348129957914352417e-01 -2.298520058393478394e-01 6.226399913430213928e-02 -9.067899733781814575e-02 -1.955600082874298096e-02 4.424500092864036560e-02 1.410300005227327347e-02 -1.150519996881484985e-01 -1.231240034103393555e-01 -1.136320009827613831e-01 1.164780035614967346e-01 -2.791160047054290771e-01 6.602100282907485962e-02 2.332739979028701782e-01 1.194470003247261047e-01 -8.205399662256240845e-02\n```\n\n----------------------------------------\n\nTITLE: UniformTermSimilarityIndex Benchmark Function\nDESCRIPTION: Measures performance of UniformTermSimilarityIndex by tracking constructor and term similarity production duration. Takes dictionary size, nonzero limit and repetition as configuration parameters.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_benchmark.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef benchmark(configuration):\n    dictionary, nonzero_limit, repetition = configuration\n    \n    start_time = time()\n    index = UniformTermSimilarityIndex(dictionary)\n    end_time = time()\n    constructor_duration = end_time - start_time\n    \n    start_time = time()\n    for term in dictionary.values():\n        for _j, _k in zip(index.most_similar(term, topn=nonzero_limit), range(nonzero_limit)):\n            pass\n    end_time = time()\n    production_duration = end_time - start_time\n    \n    return {\n        \"dictionary_size\": len(dictionary),\n        \"nonzero_limit\": nonzero_limit,\n        \"repetition\": repetition,\n        \"constructor_duration\": constructor_duration,\n        \"production_duration\": production_duration, }\n```\n\n----------------------------------------\n\nTITLE: Initializing Example Sentences\nDESCRIPTION: Setup of logging and definition of example sentences for SCM comparison\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_scm.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Initialize logging.\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\nsentence_obama = 'Obama speaks to the media in Illinois'\nsentence_president = 'The president greets the press in Chicago'\nsentence_orange = 'Oranges are my favorite fruit'\n```\n\n----------------------------------------\n\nTITLE: Monitoring Word2Vec Training Progress in Gensim\nDESCRIPTION: This log output shows training progress from a Gensim Word2Vec model training session. The logs track multiple training epochs (15-18), displaying completion percentage, processing speed (words/s), and input/output queue status at regular intervals.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_13\n\nLANGUAGE: log\nCODE:\n```\n2023-08-23 12:47:41,186 : INFO : EPOCH 15 - PROGRESS: at 25.60% examples, 837803 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:47:42,188 : INFO : EPOCH 15 - PROGRESS: at 29.27% examples, 838220 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:47:43,195 : INFO : EPOCH 15 - PROGRESS: at 32.97% examples, 839116 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:47:44,215 : INFO : EPOCH 15 - PROGRESS: at 36.57% examples, 836070 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:47:45,219 : INFO : EPOCH 15 - PROGRESS: at 40.29% examples, 837033 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:47:46,226 : INFO : EPOCH 15 - PROGRESS: at 43.65% examples, 831229 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:47:47,235 : INFO : EPOCH 15 - PROGRESS: at 47.33% examples, 830732 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:47:48,241 : INFO : EPOCH 15 - PROGRESS: at 50.87% examples, 829753 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:47:49,244 : INFO : EPOCH 15 - PROGRESS: at 54.49% examples, 830481 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:47:50,262 : INFO : EPOCH 15 - PROGRESS: at 58.17% examples, 829595 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:47:51,271 : INFO : EPOCH 15 - PROGRESS: at 61.81% examples, 828738 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:47:52,282 : INFO : EPOCH 15 - PROGRESS: at 65.35% examples, 827341 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:47:53,288 : INFO : EPOCH 15 - PROGRESS: at 69.02% examples, 827911 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:47:54,295 : INFO : EPOCH 15 - PROGRESS: at 72.64% examples, 827848 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:47:55,319 : INFO : EPOCH 15 - PROGRESS: at 76.34% examples, 828525 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:47:56,328 : INFO : EPOCH 15 - PROGRESS: at 80.01% examples, 828863 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:47:57,335 : INFO : EPOCH 15 - PROGRESS: at 83.72% examples, 829928 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:47:58,336 : INFO : EPOCH 15 - PROGRESS: at 87.44% examples, 830531 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:47:59,340 : INFO : EPOCH 15 - PROGRESS: at 91.09% examples, 830497 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:00,343 : INFO : EPOCH 15 - PROGRESS: at 94.50% examples, 828331 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:01,346 : INFO : EPOCH 15 - PROGRESS: at 97.96% examples, 826291 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:01,907 : INFO : EPOCH 15: training on 23279529 raw words (22951015 effective words) took 27.8s, 826319 effective words/s\n2023-08-23 12:48:02,910 : INFO : EPOCH 16 - PROGRESS: at 3.74% examples, 849544 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:03,926 : INFO : EPOCH 16 - PROGRESS: at 7.29% examples, 820671 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:04,942 : INFO : EPOCH 16 - PROGRESS: at 11.03% examples, 832892 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:05,944 : INFO : EPOCH 16 - PROGRESS: at 14.54% examples, 825124 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:06,955 : INFO : EPOCH 16 - PROGRESS: at 18.14% examples, 826695 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:07,956 : INFO : EPOCH 16 - PROGRESS: at 21.55% examples, 821953 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:08,976 : INFO : EPOCH 16 - PROGRESS: at 25.26% examples, 825241 words/s, in_qsize 4, out_qsize 0\n2023-08-23 12:48:09,989 : INFO : EPOCH 16 - PROGRESS: at 28.89% examples, 824858 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:10,991 : INFO : EPOCH 16 - PROGRESS: at 32.51% examples, 825542 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:11,994 : INFO : EPOCH 16 - PROGRESS: at 35.88% examples, 820405 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:13,001 : INFO : EPOCH 16 - PROGRESS: at 39.52% examples, 820927 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:14,004 : INFO : EPOCH 16 - PROGRESS: at 43.10% examples, 820715 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:15,009 : INFO : EPOCH 16 - PROGRESS: at 46.76% examples, 821259 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:16,014 : INFO : EPOCH 16 - PROGRESS: at 50.29% examples, 820279 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:17,020 : INFO : EPOCH 16 - PROGRESS: at 53.83% examples, 820827 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:18,033 : INFO : EPOCH 16 - PROGRESS: at 57.50% examples, 820846 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:19,040 : INFO : EPOCH 16 - PROGRESS: at 61.19% examples, 820642 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:20,041 : INFO : EPOCH 16 - PROGRESS: at 64.80% examples, 820654 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:21,043 : INFO : EPOCH 16 - PROGRESS: at 68.21% examples, 818658 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:22,049 : INFO : EPOCH 16 - PROGRESS: at 71.83% examples, 820114 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:23,063 : INFO : EPOCH 16 - PROGRESS: at 75.42% examples, 819681 words/s, in_qsize 4, out_qsize 0\n2023-08-23 12:48:24,073 : INFO : EPOCH 16 - PROGRESS: at 79.09% examples, 820855 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:25,088 : INFO : EPOCH 16 - PROGRESS: at 82.66% examples, 819965 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:26,097 : INFO : EPOCH 16 - PROGRESS: at 86.10% examples, 818575 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:27,097 : INFO : EPOCH 16 - PROGRESS: at 89.53% examples, 816888 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:28,099 : INFO : EPOCH 16 - PROGRESS: at 93.03% examples, 816014 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:29,100 : INFO : EPOCH 16 - PROGRESS: at 96.44% examples, 814522 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:30,076 : INFO : EPOCH 16: training on 23279529 raw words (22951015 effective words) took 28.2s, 814826 effective words/s\n2023-08-23 12:48:31,084 : INFO : EPOCH 17 - PROGRESS: at 3.78% examples, 854301 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:32,104 : INFO : EPOCH 17 - PROGRESS: at 7.67% examples, 859621 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:33,112 : INFO : EPOCH 17 - PROGRESS: at 11.38% examples, 858027 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:34,132 : INFO : EPOCH 17 - PROGRESS: at 14.89% examples, 842363 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:35,136 : INFO : EPOCH 17 - PROGRESS: at 18.46% examples, 839696 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:36,146 : INFO : EPOCH 17 - PROGRESS: at 22.04% examples, 836642 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:37,160 : INFO : EPOCH 17 - PROGRESS: at 25.60% examples, 834307 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:38,162 : INFO : EPOCH 17 - PROGRESS: at 29.35% examples, 837507 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:39,173 : INFO : EPOCH 17 - PROGRESS: at 32.89% examples, 833856 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:40,179 : INFO : EPOCH 17 - PROGRESS: at 36.61% examples, 835454 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:41,180 : INFO : EPOCH 17 - PROGRESS: at 40.25% examples, 834906 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:42,192 : INFO : EPOCH 17 - PROGRESS: at 44.03% examples, 836955 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:43,212 : INFO : EPOCH 17 - PROGRESS: at 47.89% examples, 838257 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:44,224 : INFO : EPOCH 17 - PROGRESS: at 51.31% examples, 834903 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:45,225 : INFO : EPOCH 17 - PROGRESS: at 54.95% examples, 835503 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:46,232 : INFO : EPOCH 17 - PROGRESS: at 58.64% examples, 834853 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:47,242 : INFO : EPOCH 17 - PROGRESS: at 62.46% examples, 835915 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:48,249 : INFO : EPOCH 17 - PROGRESS: at 66.00% examples, 834335 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:49,254 : INFO : EPOCH 17 - PROGRESS: at 69.50% examples, 833046 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:50,257 : INFO : EPOCH 17 - PROGRESS: at 73.10% examples, 832422 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:51,275 : INFO : EPOCH 17 - PROGRESS: at 76.53% examples, 830339 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:52,281 : INFO : EPOCH 17 - PROGRESS: at 80.11% examples, 829412 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:53,288 : INFO : EPOCH 17 - PROGRESS: at 83.80% examples, 830464 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:54,289 : INFO : EPOCH 17 - PROGRESS: at 87.56% examples, 831421 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:55,296 : INFO : EPOCH 17 - PROGRESS: at 91.44% examples, 833210 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:56,308 : INFO : EPOCH 17 - PROGRESS: at 95.11% examples, 832854 words/s, in_qsize 2, out_qsize 1\n2023-08-23 12:48:57,324 : INFO : EPOCH 17 - PROGRESS: at 99.01% examples, 834167 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:57,592 : INFO : EPOCH 17: training on 23279529 raw words (22951015 effective words) took 27.5s, 834124 effective words/s\n2023-08-23 12:48:58,600 : INFO : EPOCH 18 - PROGRESS: at 3.61% examples, 816344 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:48:59,621 : INFO : EPOCH 18 - PROGRESS: at 7.33% examples, 821273 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:49:00,627 : INFO : EPOCH 18 - PROGRESS: at 10.88% examples, 820413 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:49:01,634 : INFO : EPOCH 18 - PROGRESS: at 14.54% examples, 823890 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:49:02,645 : INFO : EPOCH 18 - PROGRESS: at 18.27% examples, 831531 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:49:03,648 : INFO : EPOCH 18 - PROGRESS: at 22.00% examples, 837022 words/s, in_qsize 3, out_qsize 0\n```\n\n----------------------------------------\n\nTITLE: Corpus Building\nDESCRIPTION: Builds the corpus by processing all texts from the iterator\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/wikinews-bigram-en.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# we put all data in ram, it's not so much\ncorpus = []\nfor txt in iter_texts():\n    corpus.extend(prepare(txt))\n```\n\n----------------------------------------\n\nTITLE: Documenting Translation Matrix Module in Python\nDESCRIPTION: This snippet uses Sphinx automodule directive to generate documentation for the translation_matrix module in Gensim. It includes all members, inherited members, and undocumented members, providing a complete API reference.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/models/translation_matrix.rst#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: gensim.models.translation_matrix\n    :synopsis: Translation Matrix\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Serializing a Corpus in Multiple Formats (Python)\nDESCRIPTION: Demonstrates how to serialize a corpus in SVMlight, LDA-C, and GibbsLDA++ formats using Gensim's respective corpus classes.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_corpora_and_vector_spaces.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ncorpora.SvmLightCorpus.serialize('/tmp/corpus.svmlight', corpus)\ncorpora.BleiCorpus.serialize('/tmp/corpus.lda-c', corpus)\ncorpora.LowCorpus.serialize('/tmp/corpus.low', corpus)\n```\n\n----------------------------------------\n\nTITLE: Setting Word2Vec Worker Threads in Python\nDESCRIPTION: Demonstrates how to configure parallel processing for Word2Vec training using the workers parameter.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_word2vec.ipynb#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# default value of workers=3 (tutorial says 1...)\nmodel = gensim.models.Word2Vec(sentences, workers=4)\n```\n\n----------------------------------------\n\nTITLE: Initializing DIM Model in Gensim\nDESCRIPTION: Creates a Document Influence Model using Gensim's DtmModel with 'fixed' mode parameter. The model requires a corpus, time sequence, and dictionary while setting the number of topics to 2.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/dtm_example.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmodel = DtmModel(dtm_path, corpus, time_seq, num_topics=2,\n                 id2word=corpus.dictionary, initialize_lda=True, model='fixed')\n```\n\n----------------------------------------\n\nTITLE: Implementing HDP Model Support\nDESCRIPTION: Demonstrates coherence calculation for Hierarchical Dirichlet Process (HDP) topic model.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence_tutorial.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nhm = HdpModel(corpus=corpus, id2word=dictionary)\n\ntopics = []\nfor topic_id, topic in hm.show_topics(num_topics=10, formatted=False):\n    topic = [word for word, _ in topic]\n    topics.append(topic)\ntopics[:2]\n\ncm = CoherenceModel(topics=topics, corpus=corpus, dictionary=dictionary, coherence='u_mass')\ncm.get_coherence()\n```\n\n----------------------------------------\n\nTITLE: Word Coloring Visualization\nDESCRIPTION: Implements a visualization function to color words based on their topic distribution\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_methods.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef color_words(model, doc):\n    import matplotlib.pyplot as plt\n    import matplotlib.patches as patches\n    \n    doc = model.id2word.doc2bow(doc)\n    doc_topics, word_topics, phi_values = model.get_document_topics(doc, per_word_topics=True)\n\n    topic_colors = { 1:'red', 0:'blue'}\n    \n    fig = plt.figure()\n    ax = fig.add_axes([0,0,1,1])\n\n    word_pos = 1/len(doc)\n    \n    for word, topics in word_topics:\n        ax.text(word_pos, 0.8, model.id2word[word],\n                horizontalalignment='center',\n                verticalalignment='center',\n                fontsize=20, color=topic_colors[topics[0]],\n                transform=ax.transAxes)\n        word_pos += 0.2\n\n    ax.set_axis_off()\n    plt.show()\n```\n\n----------------------------------------\n\nTITLE: Demonstrating inefficient multiprocessing with separate Nmslib indices\nDESCRIPTION: Shows an inefficient approach where two processes each load their own copy of the Word2Vec model and create separate Nmslib indices, leading to higher memory usage.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\nmodel.save('/tmp/mymodel.pkl')\n\ndef f(process_id):\n    print('Process Id: {}'.format(os.getpid()))\n    process = psutil.Process(os.getpid())\n    new_model = Word2Vec.load('/tmp/mymodel.pkl')\n    vector = new_model[\"science\"]\n    nmslib_index = NmslibIndexer(new_model, {'M': 100, 'indexThreadQty': 1, 'efConstruction': 100}, {'efSearch': 10})\n    approximate_neighbors = new_model.most_similar([vector], topn=5, indexer=nmslib_index)\n    print('\\nMemory used by process {}: {}\\n---'.format(os.getpid(), process.memory_info()))\n\n# Creating and running two parallel process to share the same index file.\np1 = Process(target=f, args=('1',))\np1.start()\np1.join()\np2 = Process(target=f, args=('2',))\np2.start()\np2.join()\n```\n\n----------------------------------------\n\nTITLE: Installing Gensim via pip\nDESCRIPTION: Command to install or upgrade Gensim using Python's package manager pip.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/intro.rst#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade gensim\n```\n\n----------------------------------------\n\nTITLE: Downloading and Streaming NLP Datasets with Gensim Downloader API\nDESCRIPTION: Demonstrates downloading large NLP datasets using the new Gensim downloader API with memory-efficient data streaming\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/CHANGELOG.md#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport gensim.downloader as api\n\nfor article in api.load(\"wiki-english-20171001\"):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Word Vector for 'pig' in 300-dimensional space\nDESCRIPTION: This snippet shows the word vector representation for the word 'pig' in a 300-dimensional space. Each number represents a dimension in the vector space, capturing semantic and syntactic properties of the word.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/EN.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_11\n\nLANGUAGE: text\nCODE:\n```\npig 2.526780068874359131e-01 -2.333520054817199707e-01 -7.205899804830551147e-02 9.321000427007675171e-03 2.669680118560791016e-01 2.444050014019012451e-01 3.295849859714508057e-01 2.996869981288909912e-01 -4.999859929084777832e-01 -1.733999997377395630e-01 -8.441200107336044312e-02 1.385930031538009644e-01 -7.854899764060974121e-02 -1.893630027770996094e-01 1.484649926424026489e-01 -2.250050008296966553e-01 -1.125520020723342896e-01 -3.876300156116485596e-02 9.051799774169921875e-02 -1.425870060920715332e-01 -5.589599907398223877e-02 1.538179963827133179e-01 2.456899918615818024e-02 -2.150049954652786255e-01 5.772300064563751221e-02 -8.324000239372253418e-02 -1.633519977331161499e-01 -2.928879857063293457e-01 2.855600044131278992e-02 5.919099971652030945e-02 -1.974000036716461182e-02 -9.345799684524536133e-02 -2.732850015163421631e-01 -3.720600157976150513e-02 1.075410023331642151e-01 3.532930016517639160e-01 4.523000214248895645e-03 6.715299934148788452e-02 -4.710179865360260010e-01 -3.192639946937561035e-01 5.028399825096130371e-02 1.720390021800994873e-01 -1.519149988889694214e-01 4.375199973583221436e-02 2.976529896259307861e-01 -8.874099701642990112e-02 2.182849943637847900e-01 6.221999879926443100e-03 4.928100109100341797e-02 8.361999876797199249e-03 1.107290014624595642e-01 -4.856200143694877625e-02 -1.089809983968734741e-01 5.616899952292442322e-02 6.593999918550252914e-03 4.747100174427032471e-02 -7.300200313329696655e-02 1.892219930887222290e-01 -3.943499922752380371e-02 2.165410071611404419e-01 -8.160199970006942749e-02 -8.598099648952484131e-02 1.257870048284530640e-01 -9.597799926996231079e-02 -1.709080040454864502e-01 -2.630769908428192139e-01 -8.011200278997421265e-02 -1.920949965715408325e-01 -1.463650017976760864e-01 3.350499868392944336e-01 -2.495999913662672043e-03 1.504900027066469193e-02 -1.834270060062408447e-01 -2.574810087680816650e-01 -1.094430014491081238e-01 1.108200009912252426e-02 -6.206699833273887634e-02 -1.156340017914772034e-01 2.482270002365112305e-01 -2.005989998579025269e-01 7.883899658918380737e-02 1.701849997043609619e-01 -7.164300233125686646e-02 6.639400124549865723e-02 2.755600027740001678e-02 4.733999818563461304e-02 1.211310029029846191e-01 7.151699811220169067e-02 -9.665600210428237915e-02 -8.368799835443496704e-02 1.215369999408721924e-01 3.271900117397308350e-02 -1.820099949836730957e-01 7.919699698686599731e-02 1.168200001120567322e-02 1.035000011324882507e-02 7.340000011026859283e-04 1.006309986114501953e-01 -2.891800105571746826e-01 -3.397800028324127197e-02 -2.300959974527359009e-01 2.314359992742538452e-01 -2.682270109653472900e-01 2.145960032939910889e-01 -8.529900014400482178e-02 -2.952539920806884766e-01 1.054529994726181030e-01 7.395000010728836060e-02 2.666589915752410889e-01 4.747499898076057434e-02 -3.624680042266845703e-01 8.808100223541259766e-02 7.353500276803970337e-02 -1.134890019893646240e-01 2.392649948596954346e-01 -3.802900016307830811e-02 1.402940005064010620e-01 -5.680000176653265953e-04 1.599369943141937256e-01 1.750699989497661591e-02 -7.528399676084518433e-02 1.543209999799728394e-01 -4.432089924812316895e-01 -3.978500142693519592e-02 -2.374619990587234497e-01 -3.067800030112266541e-02 -5.628700181841850281e-02 -2.823100052773952484e-02 2.228309959173202515e-01 -9.621500223875045776e-02 3.645739853382110596e-01 6.342200189828872681e-02 3.290759921073913574e-01 -5.921500176191329956e-02 1.091279983520507812e-01 2.102759927511215210e-01 -2.309329956769943237e-01 -9.988100081682205200e-02 7.383199781179428101e-02 1.801300048828125000e-01 -4.202400147914886475e-02 8.974900096654891968e-02 -2.494180053472518921e-01 2.280319929122924805e-01 6.686600297689437866e-02 1.608889997005462646e-01 -3.576499968767166138e-02 -4.376200139522552490e-01 2.610319852828979492e-01 -2.340899966657161713e-02 -1.629699952900409698e-02 -2.550419867038726807e-01 -1.064150035381317139e-01 1.231819987297058105e-01 -9.714099764823913574e-02 2.271919995546340942e-01 2.320300042629241943e-02 2.155259996652603149e-01 -1.267839968204498291e-01 -6.458400189876556396e-02 -8.124899864196777344e-02 -3.182000014930963516e-03 -1.133119985461235046e-01 3.934900090098381042e-02 -3.039869964122772217e-01 3.553799912333488464e-02 4.825099930167198181e-02 4.199999943375587463e-02 -3.079729974269866943e-01 1.221510022878646851e-01 1.236120015382766724e-01 -1.366300042718648911e-02 1.146200019866228104e-02 -2.377589941024780273e-01 -2.343999920412898064e-03 -2.344100028276443481e-01 -3.269299864768981934e-02 1.067200005054473877e-01 3.204520046710968018e-01 -1.255369931459426880e-01 -2.767490148544311523e-01 -2.394399940967559814e-01 -1.374939978122711182e-01 1.245660036802291870e-01 -8.946800231933593750e-02 -2.568410038948059082e-01 -9.087099879980087280e-02 2.384420037269592285e-01 -1.008260026574134827e-01 -1.057749986648559570e-01 3.606880009174346924e-01 -1.333730071783065796e-01 2.786859869956970215e-01 2.353879958391189575e-01 -1.490519940853118896e-01 -2.855500020086765289e-02 1.621479988098144531e-01 3.637399896979331970e-02 -1.233149990439414978e-01 3.948900103569030762e-02 -2.097470015287399292e-01 -2.260780036449432373e-01 -2.851399965584278107e-02 -2.523350119590759277e-01 -2.429019957780838013e-01 7.007499784231185913e-02 4.153450131416320801e-01 -5.018420219421386719e-01 -1.381330043077468872e-01 1.407539993524551392e-01 3.086180090904235840e-01 -8.018500357866287231e-02 -5.141900107264518738e-02 -2.854720056056976318e-01 6.528300046920776367e-02 3.562000114470720291e-03 -2.372359931468963623e-01 5.004199966788291931e-02 3.000300005078315735e-02 -4.052290022373199463e-01 2.784529924392700195e-01 1.679670065641403198e-01 -2.675729990005493164e-01 -1.142920032143592834e-01 -2.627370059490203857e-01 -1.954790055751800537e-01 -1.752019971609115601e-01 -2.012200057506561279e-01 3.229900002479553223e-01 4.096480011940002441e-01 -3.324150145053863525e-01 -1.223009973764419556e-01 -1.562800072133541107e-02 -5.506499856710433960e-02 9.428200125694274902e-02 -1.842209994792938232e-01 -1.280950009822845459e-01 -1.688430011272430420e-01 1.395339965820312500e-01 -1.592629998922348022e-01 3.527710139751434326e-01 -2.582530081272125244e-01 2.282099984586238861e-02 -1.221750006079673767e-01 -7.807999849319458008e-03 -1.107319965958595276e-01 3.875999944284558296e-03 3.631469905376434326e-01 9.551700204610824585e-02 1.620790064334869385e-01 1.272079944610595703e-01 -1.273529976606369019e-01 1.261609941720962524e-01 -2.678259909152984619e-01 4.396000131964683533e-02 -1.648889929056167603e-01 2.568629980087280273e-01 -9.027600288391113281e-02 5.338599905371665955e-02 1.108250021934509277e-01 -1.435980051755905151e-01 1.576679944992065430e-01 6.182600185275077820e-02 -1.158640012145042419e-01 5.361000075936317444e-02 -9.339299798011779785e-02 -3.131800144910812378e-02 2.608210146427154541e-01 -3.612700104713439941e-02 2.302179932594299316e-01 -1.036660000681877136e-01 2.648650109767913818e-01 5.822800099849700928e-02 2.442100048065185547e-01 -9.789100289344787598e-02 2.722300030291080475e-02 3.856379985809326172e-01 1.171249970793724060e-01 2.564699947834014893e-01 3.206549882888793945e-01 -4.213599860668182373e-02 2.128549963235855103e-01 -7.727500051259994507e-02 -1.089570000767707825e-01 -1.423539966344833374e-01 7.856500148773193359e-02 5.836300179362297058e-02 -1.582899987697601318e-01 2.638959884643554688e-01 -2.775129973888397217e-01 1.160399988293647766e-01 -1.322550028562545776e-01 1.831009984016418457e-01 -3.140339851379394531e-01 3.097400069236755371e-02 1.132239997386932373e-01 -2.114190012216567993e-01 2.135719954967498779e-01 -6.695500016212463379e-02 1.805610060691833496e-01\n```\n\n----------------------------------------\n\nTITLE: Academic Citation BibTeX Entry\nDESCRIPTION: BibTeX entry for citing Gensim in academic publications, referencing the original 2010 LREC workshop paper.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/intro.rst#2025-04-21_snippet_1\n\nLANGUAGE: bibtex\nCODE:\n```\n@inproceedings{rehurek_lrec,\n        title = {{Software Framework for Topic Modelling with Large Corpora}},\n        author = {Radim {\\v R}eh{\\r u}{\\v r}ek and Petr Sojka},\n        booktitle = {{Proceedings of the LREC 2010 Workshop on New\n             Challenges for NLP Frameworks}},\n        pages = {45--50},\n        year = 2010,\n        month = May,\n        day = 22,\n        publisher = {ELRA},\n        address = {Valletta, Malta},\n        note={\\url{http://is.muni.cz/publication/884893/en}},\n        language={English}\n  }\n```\n\n----------------------------------------\n\nTITLE: Viewing Exported Word2Vec Text File in Python\nDESCRIPTION: This code reads and prints the first three lines of the exported word2vec text file.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nfrom smart_open import open\nwith open('/tmp/vectors.txt') as myfile:\n    for i in range(3):\n        print(myfile.readline().strip())\n```\n\n----------------------------------------\n\nTITLE: Loading SemEval Datasets in Python\nDESCRIPTION: This snippet loads the validation and test datasets from SemEval 2016 and 2017 tasks using an API.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndatasets = api.load(\"semeval-2016-2017-task3-subtaskBC\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Logging for Gensim in Python\nDESCRIPTION: Sets up basic logging configuration for Gensim, including format and level.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_topics_and_transformations.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n```\n\n----------------------------------------\n\nTITLE: Configuring logging for Gensim in Python\nDESCRIPTION: Sets up basic logging configuration for Gensim, including timestamp, log level, and message format.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_word2vec.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n```\n\n----------------------------------------\n\nTITLE: Sorting Similarity Results\nDESCRIPTION: Sorts and prints similarity scores in descending order for better readability.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_core_concepts.rst#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfor document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n    print(document_number, score)\n```\n\n----------------------------------------\n\nTITLE: Text Iterator Implementation\nDESCRIPTION: Creates an iterator to extract titles and text from the gzipped JSON file\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/wikinews-bigram-en.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# iterator\nimport gzip\nimport json\n\nFDATE = 20170327\nFNAME = \"enwikinews-%s-cirrussearch-content.json.gz\" % FDATE\n\ndef iter_texts(fpath=FNAME):\n    with gzip.open(fpath, \"rt\") as f:\n        for l in f:\n            data = json.loads(l)\n            if \"title\" in data:\n                yield data[\"title\"]\n                yield data[\"text\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Word2Vec Vector Size in Python\nDESCRIPTION: Shows how to set the dimension size of word vectors in Word2Vec model initialization.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_word2vec.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# The default value of vector_size is 100.\nmodel = gensim.models.Word2Vec(sentences, vector_size=200)\n```\n\n----------------------------------------\n\nTITLE: Installing NLTK Package using pip in Python\nDESCRIPTION: Installs the Natural Language Toolkit (NLTK) package using pip, which is required for stopword removal.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n!pip install nltk\n```\n\n----------------------------------------\n\nTITLE: Common Terms Bigram Analysis\nDESCRIPTION: Analyzes and displays bigrams containing common terms\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/wikinews-bigram-en.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# grams that have more than 2 terms, are those with common terms\nct_ngrams = set((g[1], g[0].decode(\"utf-8\"))\n                     for g in bigram_ct.export_phrases(corpus) \n                     if len(g[0].split()) > 2)\nct_ngrams = sorted(list(ct_ngrams))\nprint(len(ct_ngrams), \"grams with common terms found\")\n# highest scores\nct_ngrams[-20:]\n```\n\n----------------------------------------\n\nTITLE: Creating a Function to Display Coherence Rankings in Python\nDESCRIPTION: Defines a function that prints the ranking of LDA models based on their coherence scores. This helps identify which number of topics produces the most coherent and interpretable topics.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence_model_selection.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef print_coherence_rankings(coherences):\n    avg_coherence = \\\n        [(num_topics, avg_coherence)\n         for num_topics, (_, avg_coherence) in coherences.items()]\n    ranked = sorted(avg_coherence, key=lambda tup: tup[1], reverse=True)\n    print(\"Ranked by average '%s' coherence:\\n\" % cm.coherence)\n    for item in ranked:\n        print(\"num_topics=%d:\\t%.4f\" % item)\n    print(\"\\nBest: %d\" % ranked[0][0])\n```\n\n----------------------------------------\n\nTITLE: Displaying Newsgroup Category Names in Python\nDESCRIPTION: Prints the target category names from the 20 Newsgroups dataset, showing the 20 different newsgroup categories that make up the corpus.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence_model_selection.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint('\\n'.join(corpus.input.target_names))\n```\n\n----------------------------------------\n\nTITLE: Executing Word Coloring Function for LDA Model and Dictionary in Python\nDESCRIPTION: This code snippet calls the color_words_dict function with a pre-defined LDA model and dictionary to generate a visualization of words colored by their topic associations.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_methods.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ncolor_words_dict(model, dictionary)\n```\n\n----------------------------------------\n\nTITLE: Training C++ Poincare Models with Non-Default Parameters\nDESCRIPTION: This snippet iterates through non-default parameters to train C++ Poincare models. It updates the default parameters with new values and stores the resulting model files.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Evaluation.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfor param, values in non_default_params.items():\n    params = default_params.copy()\n    for value in values:\n        params[param] = value\n        model_name, files = train_model_with_params(params, wordnet_file, model_sizes, 'cpp_model', 'c++')\n        model_files['c++'][model_name] = {}\n        for dim, filepath in files.items():\n            model_files['c++'][model_name][dim] = filepath\n```\n\n----------------------------------------\n\nTITLE: Monitoring Word2Vec Training Progress in Gensim\nDESCRIPTION: Log output showing the progress of training a Word2Vec model in Gensim. Each line shows timestamp, epoch number, completion percentage, processing speed in words/second, and queue statuses. The logs cover epochs 4-6 of training, with each epoch processing about 23 million words at speeds between 540K-550K words per second.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_19\n\nLANGUAGE: log\nCODE:\n```\n2023-08-23 12:53:10,226 : INFO : EPOCH 4 - PROGRESS: at 64.95% examples, 544620 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:11,254 : INFO : EPOCH 4 - PROGRESS: at 67.46% examples, 545345 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:12,260 : INFO : EPOCH 4 - PROGRESS: at 69.81% examples, 545489 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:13,286 : INFO : EPOCH 4 - PROGRESS: at 72.33% examples, 545888 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:14,303 : INFO : EPOCH 4 - PROGRESS: at 74.78% examples, 546064 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:15,309 : INFO : EPOCH 4 - PROGRESS: at 77.20% examples, 546755 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:16,321 : INFO : EPOCH 4 - PROGRESS: at 79.48% examples, 545889 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:17,333 : INFO : EPOCH 4 - PROGRESS: at 81.95% examples, 546165 words/s, in_qsize 4, out_qsize 0\n2023-08-23 12:53:18,342 : INFO : EPOCH 4 - PROGRESS: at 84.32% examples, 546091 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:19,371 : INFO : EPOCH 4 - PROGRESS: at 86.88% examples, 546667 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:20,398 : INFO : EPOCH 4 - PROGRESS: at 89.31% examples, 546456 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:21,401 : INFO : EPOCH 4 - PROGRESS: at 91.72% examples, 546326 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:22,406 : INFO : EPOCH 4 - PROGRESS: at 94.13% examples, 546423 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:23,408 : INFO : EPOCH 4 - PROGRESS: at 96.61% examples, 546778 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:24,418 : INFO : EPOCH 4 - PROGRESS: at 99.16% examples, 547248 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:24,781 : INFO : EPOCH 4: training on 23279529 raw words (22951015 effective words) took 41.9s, 547120 effective words/s\n2023-08-23 12:53:25,788 : INFO : EPOCH 5 - PROGRESS: at 2.41% examples, 540089 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:26,802 : INFO : EPOCH 5 - PROGRESS: at 4.82% examples, 540969 words/s, in_qsize 4, out_qsize 0\n2023-08-23 12:53:27,807 : INFO : EPOCH 5 - PROGRESS: at 7.29% examples, 547429 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:28,853 : INFO : EPOCH 5 - PROGRESS: at 9.78% examples, 549294 words/s, in_qsize 3, out_qsize 1\n2023-08-23 12:53:29,884 : INFO : EPOCH 5 - PROGRESS: at 12.11% examples, 542678 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:30,894 : INFO : EPOCH 5 - PROGRESS: at 14.41% examples, 539963 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:31,912 : INFO : EPOCH 5 - PROGRESS: at 16.73% examples, 538947 words/s, in_qsize 4, out_qsize 0\n2023-08-23 12:53:32,923 : INFO : EPOCH 5 - PROGRESS: at 19.00% examples, 537203 words/s, in_qsize 4, out_qsize 0\n2023-08-23 12:53:33,943 : INFO : EPOCH 5 - PROGRESS: at 21.29% examples, 536293 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:34,946 : INFO : EPOCH 5 - PROGRESS: at 23.55% examples, 534862 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:35,946 : INFO : EPOCH 5 - PROGRESS: at 25.83% examples, 533676 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:36,955 : INFO : EPOCH 5 - PROGRESS: at 28.14% examples, 533252 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:37,970 : INFO : EPOCH 5 - PROGRESS: at 30.48% examples, 533282 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:38,986 : INFO : EPOCH 5 - PROGRESS: at 32.89% examples, 534012 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:40,006 : INFO : EPOCH 5 - PROGRESS: at 35.21% examples, 533299 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:41,010 : INFO : EPOCH 5 - PROGRESS: at 37.65% examples, 534417 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:42,018 : INFO : EPOCH 5 - PROGRESS: at 40.17% examples, 536732 words/s, in_qsize 4, out_qsize 0\n2023-08-23 12:53:43,035 : INFO : EPOCH 5 - PROGRESS: at 42.56% examples, 536960 words/s, in_qsize 4, out_qsize 0\n2023-08-23 12:53:44,046 : INFO : EPOCH 5 - PROGRESS: at 44.99% examples, 537387 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:45,047 : INFO : EPOCH 5 - PROGRESS: at 47.28% examples, 536665 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:46,058 : INFO : EPOCH 5 - PROGRESS: at 49.65% examples, 537062 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:47,073 : INFO : EPOCH 5 - PROGRESS: at 51.97% examples, 536859 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:48,074 : INFO : EPOCH 5 - PROGRESS: at 54.37% examples, 537554 words/s, in_qsize 4, out_qsize 0\n2023-08-23 12:53:49,080 : INFO : EPOCH 5 - PROGRESS: at 56.87% examples, 538407 words/s, in_qsize 4, out_qsize 0\n2023-08-23 12:53:50,096 : INFO : EPOCH 5 - PROGRESS: at 59.26% examples, 538552 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:51,104 : INFO : EPOCH 5 - PROGRESS: at 61.59% examples, 537776 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:52,134 : INFO : EPOCH 5 - PROGRESS: at 64.07% examples, 537988 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:53,168 : INFO : EPOCH 5 - PROGRESS: at 66.40% examples, 537538 words/s, in_qsize 4, out_qsize 0\n2023-08-23 12:53:54,181 : INFO : EPOCH 5 - PROGRESS: at 68.89% examples, 538473 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:55,190 : INFO : EPOCH 5 - PROGRESS: at 71.40% examples, 539971 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:56,212 : INFO : EPOCH 5 - PROGRESS: at 73.97% examples, 540678 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:57,234 : INFO : EPOCH 5 - PROGRESS: at 76.45% examples, 541802 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:58,264 : INFO : EPOCH 5 - PROGRESS: at 78.88% examples, 541921 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:53:59,279 : INFO : EPOCH 5 - PROGRESS: at 81.41% examples, 542570 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:54:00,281 : INFO : EPOCH 5 - PROGRESS: at 83.72% examples, 542453 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:54:01,288 : INFO : EPOCH 5 - PROGRESS: at 86.19% examples, 542905 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:54:02,316 : INFO : EPOCH 5 - PROGRESS: at 88.69% examples, 543320 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:54:03,330 : INFO : EPOCH 5 - PROGRESS: at 91.18% examples, 543600 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:54:04,336 : INFO : EPOCH 5 - PROGRESS: at 93.62% examples, 543762 words/s, in_qsize 4, out_qsize 0\n2023-08-23 12:54:05,343 : INFO : EPOCH 5 - PROGRESS: at 96.04% examples, 543873 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:54:06,343 : INFO : EPOCH 5 - PROGRESS: at 98.38% examples, 543386 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:54:07,036 : INFO : EPOCH 5: training on 23279529 raw words (22951015 effective words) took 42.3s, 543171 effective words/s\n2023-08-23 12:54:08,039 : INFO : EPOCH 6 - PROGRESS: at 2.45% examples, 551673 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:54:09,055 : INFO : EPOCH 6 - PROGRESS: at 4.90% examples, 551131 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:54:10,089 : INFO : EPOCH 6 - PROGRESS: at 7.33% examples, 545631 words/s, in_qsize 3, out_qsize 1\n2023-08-23 12:54:11,096 : INFO : EPOCH 6 - PROGRESS: at 9.74% examples, 548445 words/s, in_qsize 4, out_qsize 0\n2023-08-23 12:54:12,112 : INFO : EPOCH 6 - PROGRESS: at 12.20% examples, 549275 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:54:13,122 : INFO : EPOCH 6 - PROGRESS: at 14.61% examples, 550344 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:54:14,131 : INFO : EPOCH 6 - PROGRESS: at 16.99% examples, 549929 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:54:15,152 : INFO : EPOCH 6 - PROGRESS: at 19.44% examples, 551865 words/s, in_qsize 4, out_qsize 1\n2023-08-23 12:54:16,177 : INFO : EPOCH 6 - PROGRESS: at 21.99% examples, 554496 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:54:17,184 : INFO : EPOCH 6 - PROGRESS: at 24.43% examples, 555691 words/s, in_qsize 4, out_qsize 0\n2023-08-23 12:54:18,195 : INFO : EPOCH 6 - PROGRESS: at 26.81% examples, 553972 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:54:19,206 : INFO : EPOCH 6 - PROGRESS: at 29.16% examples, 552540 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:54:20,217 : INFO : EPOCH 6 - PROGRESS: at 31.41% examples, 549807 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:54:21,244 : INFO : EPOCH 6 - PROGRESS: at 33.77% examples, 548191 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:54:22,261 : INFO : EPOCH 6 - PROGRESS: at 36.23% examples, 548615 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:54:23,271 : INFO : EPOCH 6 - PROGRESS: at 38.74% examples, 549711 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:54:24,271 : INFO : EPOCH 6 - PROGRESS: at 41.14% examples, 550147 words/s, in_qsize 4, out_qsize 0\n2023-08-23 12:54:25,279 : INFO : EPOCH 6 - PROGRESS: at 43.61% examples, 550531 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:54:26,279 : INFO : EPOCH 6 - PROGRESS: at 46.01% examples, 550067 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:54:27,288 : INFO : EPOCH 6 - PROGRESS: at 48.52% examples, 550882 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:54:28,296 : INFO : EPOCH 6 - PROGRESS: at 50.95% examples, 551557 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:54:29,297 : INFO : EPOCH 6 - PROGRESS: at 53.39% examples, 551973 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:54:30,300 : INFO : EPOCH 6 - PROGRESS: at 55.77% examples, 551989 words/s, in_qsize 3, out_qsize 1\n2023-08-23 12:54:31,313 : INFO : EPOCH 6 - PROGRESS: at 58.27% examples, 552016 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:54:32,314 : INFO : EPOCH 6 - PROGRESS: at 60.73% examples, 552017 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:54:33,323 : INFO : EPOCH 6 - PROGRESS: at 63.19% examples, 552095 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:54:34,336 : INFO : EPOCH 6 - PROGRESS: at 65.52% examples, 551486 words/s, in_qsize 3, out_qsize 0\n```\n\n----------------------------------------\n\nTITLE: Initializing Dependencies for Topic Coherence Demo\nDESCRIPTION: Imports required libraries and sets up basic configuration for the topic coherence demonstration. Includes error handling for pyLDAvis visualization dependency.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence_tutorial.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import print_function\n\nimport os\nimport logging\nimport json\nimport warnings\n\ntry:\n    raise ImportError\n    import pyLDAvis.gensim\n    CAN_VISUALIZE = True\n    pyLDAvis.enable_notebook()\n    from IPython.display import display\nexcept ImportError:\n    ValueError(\"SKIP: please install pyLDAvis\")\n    CAN_VISUALIZE = False\n\nimport numpy as np\n\nfrom gensim.models import CoherenceModel, LdaModel, HdpModel\nfrom gensim.models.wrappers import LdaVowpalWabbit, LdaMallet\nfrom gensim.corpora import Dictionary\n\nwarnings.filterwarnings('ignore')\n```\n\n----------------------------------------\n\nTITLE: Analyzing List of Programming Languages\nDESCRIPTION: This snippet contains a list of programming languages.  This information can be used for tasks such as language popularity analysis, language categorization, or building language-aware tools.  The list includes languages such as 'java', 'ruby', 'erlang', 'cplusplus', etc.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/ldavowpalwabbit.txt#2025-04-21_snippet_13\n\nLANGUAGE: text\nCODE:\n```\n\"java java ruby erlang cplusplus haskell java python ruby java erlang java scala haskell erlang ruby cplusplus cplusplus erlang python csharp python erlang haskell java go cplusplus c cplusplus go scala scala haskell ruby haskell go scala python c go haskell python csharp java python haskell cplusplus scala cplusplus go haskell scala c haskell cplusplus go haskell go go haskell ruby ruby haskell go c java scala csharp c scala ruby go csharp haskell erlang haskell haskell go erlang go csharp haskell java go go ruby ruby ruby haskell java\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Model Files Dictionary for Link Prediction in Python\nDESCRIPTION: Creating an empty dictionary to store model files for the link prediction task. This dictionary will be populated with paths to models trained on the WordNet training set for subsequent evaluation.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Evaluation.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Training models for link prediction\nlp_model_files = {}\n```\n\n----------------------------------------\n\nTITLE: Creating a Docstring for Gensim Documentation in Python\nDESCRIPTION: This snippet demonstrates how to create a properly formatted docstring at the beginning of a Gensim documentation script. It includes a title and a brief description, which will be used in the documentation gallery.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nr\"\"\"\nTitle\n=====\n\nBrief description.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Querying Gensim Model Lifecycle Events\nDESCRIPTION: Prints the lifecycle events of a Gensim model to help diagnose issues with specific models like word2vec, LSI, doc2vec, fasttext, or LDA.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/ISSUE_TEMPLATE.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nprint(my_model.lifecycle_events)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Logging Configuration\nDESCRIPTION: Configures basic logging with timestamp, level and message format at INFO level.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_compare_lda.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# sphinx_gallery_thumbnail_number = 2\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n```\n\n----------------------------------------\n\nTITLE: Setting Dataset Path in Python\nDESCRIPTION: Defines the path to the Opinosis dataset directory by expanding the user home directory. This path will be used to load the corpus in subsequent steps.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/ensemble_lda_with_opinosis.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npath = os.path.expanduser('~/opinosis/')\n```\n\n----------------------------------------\n\nTITLE: Extracting Dataset Zip File\nDESCRIPTION: Uses the zipfile module to extract the downloaded C50.zip file containing the Reuters dataset to a temporary directory.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_prediction_tutorial.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport zipfile\n\nfilename = '/tmp/C50.zip'\n\nzip_ref = zipfile.ZipFile(filename, 'r')\nzip_ref.extractall(\"/tmp/\")\nzip_ref.close()\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies and Setting Up Logging for Doc2Vec Training in Python\nDESCRIPTION: This snippet imports necessary modules and sets up logging for the Doc2Vec training process. It requires Python 3.7+ and Gensim 4.0+.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport multiprocessing\nfrom pprint import pprint\n\nimport smart_open\nfrom gensim.corpora.wikicorpus import WikiCorpus, tokenize\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging for Word2Vec Training in Python\nDESCRIPTION: Sets up optional logging for the Word2Vec training process. Logging is disabled by default but can be enabled by setting LOGS to True.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_annoy.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nLOGS = False  # Set to True if you want to see progress in logs.\nif LOGS:\n    import logging\n    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n```\n\n----------------------------------------\n\nTITLE: Printing System and Package Version Information\nDESCRIPTION: Prints comprehensive system information including platform details, Python version, architecture, and versions of key dependencies (NumPy, SciPy, Gensim).\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/ISSUE_TEMPLATE.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport platform; print(platform.platform())\nimport sys; print(\"Python\", sys.version)\nimport struct; print(\"Bits\", 8 * struct.calcsize(\"P\"))\nimport numpy; print(\"NumPy\", numpy.__version__)\nimport scipy; print(\"SciPy\", scipy.__version__)\nimport gensim; print(\"gensim\", gensim.__version__)\nfrom gensim.models import word2vec;print(\"FAST_VERSION\", word2vec.FAST_VERSION)\n```\n\n----------------------------------------\n\nTITLE: Gensim Phrases Import\nDESCRIPTION: Imports Gensim's Phrases model for bigram analysis\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/wikinews-bigram-en.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models.phrases import Phrases\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Translation Matrix\nDESCRIPTION: Imports necessary Python libraries including gensim for word vectors and translation matrix functionality.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/translation_matrix.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom gensim import utils\nfrom gensim.models import translation_matrix\nfrom gensim.models import KeyedVectors\nimport smart_open\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting the Opinosis Dataset in Python\nDESCRIPTION: Downloads the Opinosis dataset from GitHub and extracts it to a local directory. This code uses shell commands through Python to retrieve the dataset ZIP file and unpack it for further processing.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/ensemble_lda_with_opinosis.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n!mkdir ~/opinosis\n!wget -P ~/opinosis https://github.com/kavgan/opinosis/raw/master/OpinosisDataset1.0_0.zip\n!unzip ~/opinosis/OpinosisDataset1.0_0.zip -d ~/opinosis\n```\n\n----------------------------------------\n\nTITLE: Bigram Model Creation\nDESCRIPTION: Creates two bigram models - one standard and one with common terms\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/wikinews-bigram-en.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# bigram std\n%time bigram = Phrases(st_corpus)\n# bigram with common terms\n%time bigram_ct = Phrases(corpus, common_terms=stopwords.words(LANG))\n```\n\n----------------------------------------\n\nTITLE: Displaying SCM Example Image\nDESCRIPTION: Code to display an illustration of SCM concept using matplotlib\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_scm.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimg = mpimg.imread('scm-hello.png')\nimgplot = plt.imshow(img)\nplt.axis('off')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Analyzing Gensim Word2Vec Training Progress Logs\nDESCRIPTION: Logging output from Gensim's Word2Vec model training showing progress across multiple epochs. Each log entry displays the timestamp, epoch number, percentage of examples processed, processing speed in words per second, and input/output queue sizes. The model processes approximately 23 million raw words (about 22.95 million effective words) per epoch with performance improving in later epochs.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_9\n\nLANGUAGE: log\nCODE:\n```\n2023-08-23 12:40:45,796 : INFO : EPOCH 0 - PROGRESS: at 42.35% examples, 691178 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:40:46,801 : INFO : EPOCH 0 - PROGRESS: at 45.77% examples, 697023 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:40:47,802 : INFO : EPOCH 0 - PROGRESS: at 49.15% examples, 701199 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:40:48,821 : INFO : EPOCH 0 - PROGRESS: at 52.50% examples, 705150 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:40:49,835 : INFO : EPOCH 0 - PROGRESS: at 55.95% examples, 709661 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:40:50,848 : INFO : EPOCH 0 - PROGRESS: at 59.44% examples, 713459 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:40:51,867 : INFO : EPOCH 0 - PROGRESS: at 62.97% examples, 716717 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:40:52,877 : INFO : EPOCH 0 - PROGRESS: at 66.23% examples, 718185 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:40:53,879 : INFO : EPOCH 0 - PROGRESS: at 69.66% examples, 721577 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:40:54,884 : INFO : EPOCH 0 - PROGRESS: at 73.10% examples, 724121 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:40:55,902 : INFO : EPOCH 0 - PROGRESS: at 76.49% examples, 726480 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:40:56,912 : INFO : EPOCH 0 - PROGRESS: at 79.88% examples, 728160 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:40:57,930 : INFO : EPOCH 0 - PROGRESS: at 83.28% examples, 729732 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:40:58,946 : INFO : EPOCH 0 - PROGRESS: at 86.45% examples, 729181 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:40:59,948 : INFO : EPOCH 0 - PROGRESS: at 89.74% examples, 729761 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:00,958 : INFO : EPOCH 0 - PROGRESS: at 93.11% examples, 730757 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:01,975 : INFO : EPOCH 0 - PROGRESS: at 96.21% examples, 729591 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:03,000 : INFO : EPOCH 0 - PROGRESS: at 99.55% examples, 729553 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:03,151 : INFO : EPOCH 0: training on 23279529 raw words (22951015 effective words) took 31.5s, 729418 effective words/s\n2023-08-23 12:41:04,155 : INFO : EPOCH 1 - PROGRESS: at 3.57% examples, 810365 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:05,161 : INFO : EPOCH 1 - PROGRESS: at 7.13% examples, 804760 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:06,165 : INFO : EPOCH 1 - PROGRESS: at 10.54% examples, 800233 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:07,177 : INFO : EPOCH 1 - PROGRESS: at 14.04% examples, 798576 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:08,185 : INFO : EPOCH 1 - PROGRESS: at 17.60% examples, 804000 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:09,186 : INFO : EPOCH 1 - PROGRESS: at 21.20% examples, 810978 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:10,195 : INFO : EPOCH 1 - PROGRESS: at 24.81% examples, 812910 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:11,210 : INFO : EPOCH 1 - PROGRESS: at 28.22% examples, 807981 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:12,213 : INFO : EPOCH 1 - PROGRESS: at 31.70% examples, 807193 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:13,215 : INFO : EPOCH 1 - PROGRESS: at 35.24% examples, 807789 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:14,227 : INFO : EPOCH 1 - PROGRESS: at 38.74% examples, 805782 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:15,230 : INFO : EPOCH 1 - PROGRESS: at 42.22% examples, 805033 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:16,245 : INFO : EPOCH 1 - PROGRESS: at 45.82% examples, 805429 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:17,259 : INFO : EPOCH 1 - PROGRESS: at 49.18% examples, 801775 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:18,266 : INFO : EPOCH 1 - PROGRESS: at 52.77% examples, 803273 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:19,286 : INFO : EPOCH 1 - PROGRESS: at 56.52% examples, 806058 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:20,291 : INFO : EPOCH 1 - PROGRESS: at 60.14% examples, 806759 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:21,308 : INFO : EPOCH 1 - PROGRESS: at 63.74% examples, 806209 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:22,311 : INFO : EPOCH 1 - PROGRESS: at 67.17% examples, 805498 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:23,317 : INFO : EPOCH 1 - PROGRESS: at 70.68% examples, 805659 words/s, in_qsize 4, out_qsize 0\n2023-08-23 12:41:24,320 : INFO : EPOCH 1 - PROGRESS: at 74.23% examples, 805547 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:25,323 : INFO : EPOCH 1 - PROGRESS: at 77.81% examples, 807062 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:26,334 : INFO : EPOCH 1 - PROGRESS: at 81.28% examples, 806116 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:27,355 : INFO : EPOCH 1 - PROGRESS: at 84.77% examples, 805630 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:28,363 : INFO : EPOCH 1 - PROGRESS: at 88.18% examples, 804268 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:29,377 : INFO : EPOCH 1 - PROGRESS: at 91.98% examples, 805704 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:30,396 : INFO : EPOCH 1 - PROGRESS: at 95.66% examples, 806498 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:31,396 : INFO : EPOCH 1 - PROGRESS: at 99.42% examples, 807815 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:31,569 : INFO : EPOCH 1: training on 23279529 raw words (22951015 effective words) took 28.4s, 807647 effective words/s\n2023-08-23 12:41:32,574 : INFO : EPOCH 2 - PROGRESS: at 3.69% examples, 838866 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:33,579 : INFO : EPOCH 2 - PROGRESS: at 7.34% examples, 829306 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:34,582 : INFO : EPOCH 2 - PROGRESS: at 10.72% examples, 813650 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:35,586 : INFO : EPOCH 2 - PROGRESS: at 14.41% examples, 821914 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:36,592 : INFO : EPOCH 2 - PROGRESS: at 18.05% examples, 827046 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:37,607 : INFO : EPOCH 2 - PROGRESS: at 21.73% examples, 830028 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:38,620 : INFO : EPOCH 2 - PROGRESS: at 25.26% examples, 827368 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:39,621 : INFO : EPOCH 2 - PROGRESS: at 29.02% examples, 831575 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:40,638 : INFO : EPOCH 2 - PROGRESS: at 32.73% examples, 832256 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:41,640 : INFO : EPOCH 2 - PROGRESS: at 36.53% examples, 836182 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:42,646 : INFO : EPOCH 2 - PROGRESS: at 40.17% examples, 835265 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:43,651 : INFO : EPOCH 2 - PROGRESS: at 43.95% examples, 837760 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:44,664 : INFO : EPOCH 2 - PROGRESS: at 47.72% examples, 838012 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:45,682 : INFO : EPOCH 2 - PROGRESS: at 51.36% examples, 837709 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:46,690 : INFO : EPOCH 2 - PROGRESS: at 55.00% examples, 837743 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:47,698 : INFO : EPOCH 2 - PROGRESS: at 58.69% examples, 836927 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:48,698 : INFO : EPOCH 2 - PROGRESS: at 62.37% examples, 836597 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:49,699 : INFO : EPOCH 2 - PROGRESS: at 66.04% examples, 836884 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:50,704 : INFO : EPOCH 2 - PROGRESS: at 69.58% examples, 835959 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:51,717 : INFO : EPOCH 2 - PROGRESS: at 73.28% examples, 835754 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:52,722 : INFO : EPOCH 2 - PROGRESS: at 76.91% examples, 836330 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:53,734 : INFO : EPOCH 2 - PROGRESS: at 80.62% examples, 836196 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:54,742 : INFO : EPOCH 2 - PROGRESS: at 84.28% examples, 836475 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:55,756 : INFO : EPOCH 2 - PROGRESS: at 87.98% examples, 836357 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:56,775 : INFO : EPOCH 2 - PROGRESS: at 91.85% examples, 837150 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:57,785 : INFO : EPOCH 2 - PROGRESS: at 95.66% examples, 838175 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:41:58,794 : INFO : EPOCH 2 - PROGRESS: at 99.50% examples, 838829 words/s, in_qsize 4, out_qsize 0\n2023-08-23 12:41:58,949 : INFO : EPOCH 2: training on 23279529 raw words (22951015 effective words) took 27.4s, 838282 effective words/s\n2023-08-23 12:41:59,958 : INFO : EPOCH 3 - PROGRESS: at 3.53% examples, 797332 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:42:00,958 : INFO : EPOCH 3 - PROGRESS: at 7.25% examples, 819966 words/s, in_qsize 4, out_qsize 0\n2023-08-23 12:42:01,977 : INFO : EPOCH 3 - PROGRESS: at 10.88% examples, 822523 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:42:02,980 : INFO : EPOCH 3 - PROGRESS: at 14.41% examples, 819097 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:42:03,988 : INFO : EPOCH 3 - PROGRESS: at 17.98% examples, 820622 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:42:04,991 : INFO : EPOCH 3 - PROGRESS: at 21.68% examples, 827878 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:42:06,003 : INFO : EPOCH 3 - PROGRESS: at 24.98% examples, 817363 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:42:07,006 : INFO : EPOCH 3 - PROGRESS: at 28.39% examples, 813039 words/s, in_qsize 3, out_qsize 0\n```\n\n----------------------------------------\n\nTITLE: Importing time and numpy modules for benchmarking\nDESCRIPTION: Imports the necessary modules for time measurement and random vector selection in the benchmark functions.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport numpy as np\n```\n\n----------------------------------------\n\nTITLE: Analyzing Fourth List of Car Parts\nDESCRIPTION: This snippet contains a list of car-related terms. Analysis of this data involves understanding and documenting the terms for usage in automotive applications or natural language processing tasks. The list consists of parts such as 'wheel', 'clutch', 'tyre', 'engine', etc.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/ldavowpalwabbit.txt#2025-04-21_snippet_15\n\nLANGUAGE: text\nCODE:\n```\n\"wheel clutch tyre engine wheel tyre tyre brakes exhaust engine exhaust cylinder car motor suspension clutch tyre cylinder car suspension exhaust exhaust tyre motor clutch wheel motor clutch clutch wheel brakes motor cylinder tyre brakes suspension cylinder engine tyre brakes tyre clutch suspension exhaust motor motor wheel cylinder exhaust tyre car cylinder\"\n```\n\n----------------------------------------\n\nTITLE: Setting Language Variable\nDESCRIPTION: Defines the language variable for processing English text\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/wikinews-bigram-en.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nLANG=\"english\"\n```\n\n----------------------------------------\n\nTITLE: Loading Pre-trained Word Embeddings in Python using Gensim\nDESCRIPTION: This snippet loads pre-trained word embeddings (Word2Vec model) using Gensim's downloader API.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_wmd.rst#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport gensim.downloader as api\nmodel = api.load('word2vec-google-news-300')\n```\n\n----------------------------------------\n\nTITLE: Word2Vec Training Progress Log\nDESCRIPTION: Log entries showing training progress of a Word2Vec model during epochs 8-10. Each line shows timestamp, epoch number, completion percentage, processing speed (words/sec), and input/output queue sizes. Processing speed averages around 545-550K words/second.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_22\n\nLANGUAGE: log\nCODE:\n```\n2023-08-23 12:56:00,294 : INFO : EPOCH 8 - PROGRESS: at 70.11% examples, 549605 words/s, in_qsize 4, out_qsize 0\n2023-08-23 12:56:01,301 : INFO : EPOCH 8 - PROGRESS: at 72.41% examples, 548598 words/s, in_qsize 3, out_qsize 0\n[...]\n2023-08-23 12:57:23,083 : INFO : EPOCH 10 - PROGRESS: at 67.50% examples, 547724 words/s, in_qsize 4, out_qsize 0\n```\n\n----------------------------------------\n\nTITLE: Initializing Logging for FastText Model in Python\nDESCRIPTION: Sets up basic logging configuration for the FastText model training process.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_fasttext.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n```\n\n----------------------------------------\n\nTITLE: Implementing TextsIterable for Word2Vec Coherence Model\nDESCRIPTION: Creates a wrapper class that enables multiple iterations over a text corpus for Word2Vec model training. This is necessary because the Word2Vec model needs to scan texts multiple times.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence_model_selection.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\nclass TextsIterable(object):\n    \"\"\"Wrap a TextCorpus in something that yields texts from its __iter__.\n    \n    It's necessary to use this because the Word2Vec model is built by scanning\n    over the texts several times. Passing in corpus.get_texts() would result in\n    an empty iterable on passes after the first.\n    \"\"\"\n    \n    def __init__(self, corpus):\n        self.corpus = corpus\n    \n    def __iter__(self):\n        return self.corpus.get_texts()\n\n\ncm = models.CoherenceModel.for_models(\n    trained_models.values(), dictionary, texts=TextsIterable(corpus), coherence='c_w2v')\n```\n\n----------------------------------------\n\nTITLE: Random Document Comparison\nDESCRIPTION: Compares a randomly selected document with its second most similar document\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_doc2vec_lee.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport random\ndoc_id = random.randint(0, len(train_corpus) - 1)\n\nprint('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\nsim_id = second_ranks[doc_id]\nprint('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))\n```\n\n----------------------------------------\n\nTITLE: Setting Number of Queries for Performance Evaluation in Python\nDESCRIPTION: This code sets the number of queries to be used for calculating query time in performance evaluation.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nqueries = 1000\n```\n\n----------------------------------------\n\nTITLE: Evaluating Word2Vec Analogies in Python\nDESCRIPTION: Shows how to evaluate word analogies using built-in Word2Vec evaluation method.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_word2vec.ipynb#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nmodel.wv.evaluate_word_analogies(datapath('questions-words.txt'))\n```\n\n----------------------------------------\n\nTITLE: LevenshteinSimilarityIndex Benchmark Function\nDESCRIPTION: Measures performance of LevenshteinSimilarityIndex with constructor and production duration tracking. Includes additional query terms parameter compared to Uniform benchmark.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_benchmark.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef benchmark(configuration):\n    dictionary, nonzero_limit, query_terms, repetition = configuration\n    \n    start_time = time()\n    index = LevenshteinSimilarityIndex(dictionary)\n    end_time = time()\n    constructor_duration = end_time - start_time\n    \n    start_time = time()\n    for term in query_terms:\n        for _j, _k in zip(index.most_similar(term, topn=nonzero_limit), range(nonzero_limit)):\n            pass\n    end_time = time()\n    production_duration = end_time - start_time\n    \n    return {\n        \"dictionary_size\": len(dictionary),\n        \"mean_query_term_length\": np.mean([len(term) for term in query_terms]),\n        \"nonzero_limit\": nonzero_limit,\n        \"repetition\": repetition,\n        \"constructor_duration\": constructor_duration,\n        \"production_duration\": production_duration, }\n```\n\n----------------------------------------\n\nTITLE: Extracting Interlinks from Wikipedia Pages with segment_wiki Script (Bash)\nDESCRIPTION: This snippet demonstrates how to use the gensim.scripts.segment_wiki script to extract interlinks from a Wikipedia dump file. It includes the new --include-interlinks option to output additional JSON data containing interlink information for each article.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/CHANGELOG.md#2025-04-21_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npython -m gensim.scripts.segment_wiki --include-interlinks --file ~/Downloads/enwiki-latest-pages-articles.xml.bz2 --output ~/Desktop/enwiki-latest.jsonl.gz\n```\n\n----------------------------------------\n\nTITLE: Downloading Reuters Dataset using wget\nDESCRIPTION: Downloads the C50 dataset from UCI Machine Learning Repository which contains 50 authors with 50 documents each for training and testing.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_prediction_tutorial.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!wget -O - \"https://archive.ics.uci.edu/ml/machine-learning-databases/00217/C50.zip\" > /tmp/C50.zip\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging for Gensim\nDESCRIPTION: Sets up basic logging configuration to display timestamp, log level, and messages at INFO level.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_corpora_and_vector_spaces.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n```\n\n----------------------------------------\n\nTITLE: Logging Word2Vec Epoch Completion in Gensim\nDESCRIPTION: This snippet shows the log output format for completed epochs in Word2Vec training using Gensim. It includes the total number of raw and effective words processed, training duration, and effective words processed per second.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_11\n\nLANGUAGE: plaintext\nCODE:\n```\n2023-08-23 12:42:26,947 : INFO : EPOCH 3: training on 23279529 raw words (22951015 effective words) took 28.0s, 819806 effective words/s\n```\n\n----------------------------------------\n\nTITLE: Setting up Logging Configuration\nDESCRIPTION: Configures basic logging with timestamp, level and message format.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_ensemblelda.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n```\n\n----------------------------------------\n\nTITLE: Word2Vec Training Progress Log Output\nDESCRIPTION: Log entries showing Word2Vec model training progress including epoch number, completion percentage, processing speed (words/second), and queue sizes. The training processes approximately 23 million words per epoch at speeds around 550K words/second.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_23\n\nLANGUAGE: log\nCODE:\n```\n2023-08-23 12:57:24,083 : INFO : EPOCH 10 - PROGRESS: at 69.81% examples, 547558 words/s, in_qsize 4, out_qsize 0\n2023-08-23 12:57:25,091 : INFO : EPOCH 10 - PROGRESS: at 72.07% examples, 546288 words/s, in_qsize 3, out_qsize 0\n```\n\n----------------------------------------\n\nTITLE: Initializing Python Time Module for Execution Tracking\nDESCRIPTION: Sets up a timer to track how long the notebook takes to run by importing the time module and marking the start time.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/WMD_tutorial.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom time import time\nstart_nb = time()\n```\n\n----------------------------------------\n\nTITLE: Printing Dataset Dimensions\nDESCRIPTION: Displays the key dimensions of the dataset: number of authors, unique tokens in the vocabulary, and total number of documents for analysis.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nprint('Number of authors: %d' % len(author2doc))\nprint('Number of unique tokens: %d' % len(dictionary))\nprint('Number of documents: %d' % len(corpus))\n```\n\n----------------------------------------\n\nTITLE: Logging Doc2Vec Training Completion in Python\nDESCRIPTION: Log output showing the completion of a Doc2Vec training epoch. It displays the epoch number, total raw and effective words processed, training duration, and effective words processed per second.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_15\n\nLANGUAGE: text\nCODE:\n```\n2023-08-23 12:49:25,196 : INFO : EPOCH 18: training on 23279529 raw words (22951015 effective words) took 27.6s, 831487 effective words/s\n```\n\n----------------------------------------\n\nTITLE: Initializing Logging and Defining Sample Sentences in Python\nDESCRIPTION: This snippet sets up logging and defines two sample sentences for WMD comparison.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_wmd.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Initialize logging.\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\nsentence_obama = 'Obama speaks to the media in Illinois'\nsentence_president = 'The president greets the press in Chicago'\n```\n\n----------------------------------------\n\nTITLE: Inspecting Corpus Class Definition in Python\nDESCRIPTION: Prints the source code of the corpus class to understand its implementation.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_downloader_api.rst#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport inspect\nprint(inspect.getsource(corpus.__class__))\n```\n\n----------------------------------------\n\nTITLE: Enabling Matplotlib Inline Display in Jupyter\nDESCRIPTION: Configures Matplotlib to display plots inline within a Jupyter notebook environment.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_compare_lda.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%matplotlib inline\n```\n\n----------------------------------------\n\nTITLE: Training EnsembleLda Model\nDESCRIPTION: Creates and trains the EnsembleLda model with specified parameters and corpus.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_ensemblelda.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models import EnsembleLda\nensemble = EnsembleLda(\n    corpus=corpus,\n    id2word=dictionary,\n    num_topics=num_topics,\n    passes=passes,\n    num_models=num_models,\n    topic_model_class=LdaModel,\n    ensemble_workers=ensemble_workers,\n    distance_workers=distance_workers\n)\n\nprint(len(ensemble.ttda))\nprint(len(ensemble.get_topics()))\n```\n\n----------------------------------------\n\nTITLE: LevenshteinSimilarityIndex Benchmark Configuration\nDESCRIPTION: Configures Levenshtein benchmark with random query terms sampling and configuration setup.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_benchmark.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nnonzero_limits = [1, 10, 100]\nseed(RANDOM_SEED)\nmin_dictionary = sorted((len(dictionary), dictionary) for dictionary in dictionaries)[0][1]\nquery_terms = sample(list(min_dictionary.values()), 10)\n\nconfigurations = product(dictionaries, nonzero_limits, [query_terms], repetitions)\nresults = benchmark_results(benchmark, configurations, \"matrix_speed.builder_results.levenshtein\")\n```\n\n----------------------------------------\n\nTITLE: Vector Representation Data Format for WordNet Senses\nDESCRIPTION: Shows the data format for word sense vectors. The first line (2 2) indicates dimensions. Each subsequent line contains a WordNet sense (e.g., kangaroo.n.01) followed by its coordinates in 2D vector space.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/high_precision.kv.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n2 2\nkangaroo.n.01 -0.0007369244245224787 -8.269973595356034e-05\nhorse.n.01 -0.0008546282343595379 0.0007694142576316829\n```\n\n----------------------------------------\n\nTITLE: Loading Wikipedia Corpus and Dictionary for LDA\nDESCRIPTION: Code to load the preprocessed Wikipedia corpus and dictionary for Latent Dirichlet Allocation. This snippet is similar to the LSA setup, configuring logging and loading the corpus data needed before performing LDA topic modeling.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/wiki.rst#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> import logging\n>>> import gensim\n>>>\n>>> logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n>>>\n>>> # load id->word mapping (the dictionary), one of the results of step 2 above\n>>> id2word = gensim.corpora.Dictionary.load_from_text('wiki_en_wordids.txt')\n>>> # load corpus iterator\n>>> mm = gensim.corpora.MmCorpus('wiki_en_tfidf.mm')\n>>> # mm = gensim.corpora.MmCorpus('wiki_en_tfidf.mm.bz2') # use this if you compressed the TFIDF output\n>>>\n>>> print(mm)\nMmCorpus(3931787 documents, 100000 features, 756379027 non-zero entries)\n```\n\n----------------------------------------\n\nTITLE: Tokenizing Sentences in Python\nDESCRIPTION: Creates two sentence variables and splits them into lowercase tokens for comparison. The sentences contain text about Obama and a president speaking to media.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_fasttext.rst#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsentence_obama = 'Obama speaks to the media in Illinois'.lower().split()\nsentence_president = 'The president greets the press in Chicago'.lower().split()\n```\n\n----------------------------------------\n\nTITLE: Logging Word2Vec Epoch Completion in Gensim\nDESCRIPTION: This log entry shows the completion of an epoch during Word2Vec model training using Gensim. It includes the total number of raw and effective words processed, training duration, and words processed per second.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_21\n\nLANGUAGE: log\nCODE:\n```\n2023-08-23 12:54:48,549 : INFO : EPOCH 6: training on 23279529 raw words (22951015 effective words) took 41.5s, 552882 effective words/s\n```\n\n----------------------------------------\n\nTITLE: Setting up logging for gensim\nDESCRIPTION: Configures the logging system for detailed output from gensim operations. This is disabled by default in the notebook to avoid verbose output.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nLOGS = False\n\nif LOGS:\n    import logging\n    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n```\n\n----------------------------------------\n\nTITLE: UniformTermSimilarityIndex Benchmark Configuration\nDESCRIPTION: Sets up benchmark configurations by creating product of dictionaries, nonzero limits and repetitions.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_benchmark.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nnonzero_limits = [1, 10, 100, 1000]\n\nconfigurations = product(dictionaries, nonzero_limits, repetitions)\nresults = benchmark_results(benchmark, configurations, \"matrix_speed.builder_results.uniform\")\n```\n\n----------------------------------------\n\nTITLE: Importing indexedcorpus Module in Python\nDESCRIPTION: This snippet demonstrates how to import the indexedcorpus module from the gensim.corpora package. The module provides functionality for random access to corpus documents in Gensim.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/corpora/indexedcorpus.rst#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom gensim.corpora import indexedcorpus\n```\n\n----------------------------------------\n\nTITLE: Monitoring Word2Vec Training Progress in Gensim Log Output\nDESCRIPTION: This log snippet shows the progress of Word2Vec model training in Gensim. It displays regular updates including completion percentage, processing speed (words/s), and queue sizes. The log spans epochs 7-8 of training with timestamp information.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_35\n\nLANGUAGE: log\nCODE:\n```\n2023-08-23 13:14:21,711 : INFO : EPOCH 7 - PROGRESS: at 73.53% examples, 294642 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:22,715 : INFO : EPOCH 7 - PROGRESS: at 74.86% examples, 294855 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:23,749 : INFO : EPOCH 7 - PROGRESS: at 76.22% examples, 295116 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:24,789 : INFO : EPOCH 7 - PROGRESS: at 77.54% examples, 295169 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:25,810 : INFO : EPOCH 7 - PROGRESS: at 78.69% examples, 294525 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:26,873 : INFO : EPOCH 7 - PROGRESS: at 79.88% examples, 293867 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:27,885 : INFO : EPOCH 7 - PROGRESS: at 81.11% examples, 293596 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:28,905 : INFO : EPOCH 7 - PROGRESS: at 82.45% examples, 293754 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:29,941 : INFO : EPOCH 7 - PROGRESS: at 83.80% examples, 293953 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:30,942 : INFO : EPOCH 7 - PROGRESS: at 85.16% examples, 294193 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:31,982 : INFO : EPOCH 7 - PROGRESS: at 86.62% examples, 294553 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:33,016 : INFO : EPOCH 7 - PROGRESS: at 87.98% examples, 294653 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:34,064 : INFO : EPOCH 7 - PROGRESS: at 89.36% examples, 294673 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:35,118 : INFO : EPOCH 7 - PROGRESS: at 90.75% examples, 294804 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:36,166 : INFO : EPOCH 7 - PROGRESS: at 92.07% examples, 294558 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:37,177 : INFO : EPOCH 7 - PROGRESS: at 93.37% examples, 294596 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:38,210 : INFO : EPOCH 7 - PROGRESS: at 94.72% examples, 294672 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:39,249 : INFO : EPOCH 7 - PROGRESS: at 96.13% examples, 294854 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:40,258 : INFO : EPOCH 7 - PROGRESS: at 97.48% examples, 294887 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:41,273 : INFO : EPOCH 7 - PROGRESS: at 98.81% examples, 294908 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:42,204 : INFO : EPOCH 7: training on 23279529 raw words (22951015 effective words) took 77.8s, 294859 effective words/s\n2023-08-23 13:14:43,221 : INFO : EPOCH 8 - PROGRESS: at 1.28% examples, 277149 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:44,246 : INFO : EPOCH 8 - PROGRESS: at 2.52% examples, 280060 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:45,286 : INFO : EPOCH 8 - PROGRESS: at 3.82% examples, 282428 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:46,303 : INFO : EPOCH 8 - PROGRESS: at 5.16% examples, 285489 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:47,311 : INFO : EPOCH 8 - PROGRESS: at 6.41% examples, 284359 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:48,325 : INFO : EPOCH 8 - PROGRESS: at 7.71% examples, 286323 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:49,335 : INFO : EPOCH 8 - PROGRESS: at 8.94% examples, 286368 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:50,347 : INFO : EPOCH 8 - PROGRESS: at 10.03% examples, 281784 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:51,357 : INFO : EPOCH 8 - PROGRESS: at 11.15% examples, 279222 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:52,359 : INFO : EPOCH 8 - PROGRESS: at 12.44% examples, 280213 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:53,395 : INFO : EPOCH 8 - PROGRESS: at 13.83% examples, 282872 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:54,419 : INFO : EPOCH 8 - PROGRESS: at 15.14% examples, 284457 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:55,439 : INFO : EPOCH 8 - PROGRESS: at 16.52% examples, 286723 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:56,476 : INFO : EPOCH 8 - PROGRESS: at 17.87% examples, 287672 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:57,483 : INFO : EPOCH 8 - PROGRESS: at 19.16% examples, 288790 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:58,524 : INFO : EPOCH 8 - PROGRESS: at 20.48% examples, 289308 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:14:59,527 : INFO : EPOCH 8 - PROGRESS: at 21.87% examples, 290897 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:00,555 : INFO : EPOCH 8 - PROGRESS: at 23.18% examples, 291495 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:01,560 : INFO : EPOCH 8 - PROGRESS: at 24.35% examples, 290349 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:02,577 : INFO : EPOCH 8 - PROGRESS: at 25.56% examples, 289638 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:03,579 : INFO : EPOCH 8 - PROGRESS: at 26.89% examples, 290109 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:04,588 : INFO : EPOCH 8 - PROGRESS: at 28.14% examples, 290008 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:05,641 : INFO : EPOCH 8 - PROGRESS: at 29.40% examples, 289353 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:06,669 : INFO : EPOCH 8 - PROGRESS: at 30.70% examples, 289452 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:07,721 : INFO : EPOCH 8 - PROGRESS: at 32.06% examples, 289696 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:08,729 : INFO : EPOCH 8 - PROGRESS: at 33.23% examples, 288902 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:09,778 : INFO : EPOCH 8 - PROGRESS: at 34.39% examples, 287759 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:10,791 : INFO : EPOCH 8 - PROGRESS: at 35.59% examples, 287090 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:11,850 : INFO : EPOCH 8 - PROGRESS: at 37.01% examples, 287637 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:12,895 : INFO : EPOCH 8 - PROGRESS: at 38.39% examples, 288245 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:13,907 : INFO : EPOCH 8 - PROGRESS: at 39.74% examples, 288781 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:14,910 : INFO : EPOCH 8 - PROGRESS: at 40.99% examples, 288758 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:15,947 : INFO : EPOCH 8 - PROGRESS: at 42.40% examples, 289324 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:16,976 : INFO : EPOCH 8 - PROGRESS: at 43.78% examples, 289947 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:17,977 : INFO : EPOCH 8 - PROGRESS: at 45.16% examples, 290492 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:19,012 : INFO : EPOCH 8 - PROGRESS: at 46.55% examples, 290983 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:20,052 : INFO : EPOCH 8 - PROGRESS: at 47.90% examples, 290940 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:21,054 : INFO : EPOCH 8 - PROGRESS: at 49.18% examples, 291140 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:22,082 : INFO : EPOCH 8 - PROGRESS: at 50.46% examples, 291138 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:23,110 : INFO : EPOCH 8 - PROGRESS: at 51.81% examples, 291604 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:24,131 : INFO : EPOCH 8 - PROGRESS: at 53.14% examples, 291674 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:25,163 : INFO : EPOCH 8 - PROGRESS: at 54.45% examples, 291916 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:26,177 : INFO : EPOCH 8 - PROGRESS: at 55.81% examples, 292251 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:27,181 : INFO : EPOCH 8 - PROGRESS: at 57.18% examples, 292571 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:28,214 : INFO : EPOCH 8 - PROGRESS: at 58.55% examples, 292736 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:29,230 : INFO : EPOCH 8 - PROGRESS: at 59.97% examples, 293214 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:30,236 : INFO : EPOCH 8 - PROGRESS: at 61.31% examples, 293310 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:31,270 : INFO : EPOCH 8 - PROGRESS: at 62.67% examples, 293414 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:32,290 : INFO : EPOCH 8 - PROGRESS: at 63.99% examples, 293412 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:33,291 : INFO : EPOCH 8 - PROGRESS: at 65.22% examples, 293374 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:34,309 : INFO : EPOCH 8 - PROGRESS: at 66.31% examples, 292481 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:35,310 : INFO : EPOCH 8 - PROGRESS: at 67.50% examples, 292060 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:36,337 : INFO : EPOCH 8 - PROGRESS: at 68.85% examples, 292262 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:37,339 : INFO : EPOCH 8 - PROGRESS: at 70.20% examples, 292749 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:38,362 : INFO : EPOCH 8 - PROGRESS: at 71.51% examples, 292909 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:39,369 : INFO : EPOCH 8 - PROGRESS: at 72.87% examples, 293020 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:40,422 : INFO : EPOCH 8 - PROGRESS: at 74.27% examples, 293066 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:41,448 : INFO : EPOCH 8 - PROGRESS: at 75.47% examples, 292852 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:42,461 : INFO : EPOCH 8 - PROGRESS: at 76.74% examples, 292930 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:43,469 : INFO : EPOCH 8 - PROGRESS: at 78.07% examples, 293030 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:44,493 : INFO : EPOCH 8 - PROGRESS: at 79.39% examples, 293179 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:45,540 : INFO : EPOCH 8 - PROGRESS: at 80.77% examples, 293220 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:46,548 : INFO : EPOCH 8 - PROGRESS: at 82.12% examples, 293456 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:15:47,554 : INFO : EPOCH 8 - PROGRESS: at 83.37% examples, 293358 words/s, in_qsize 3, out_qsize 0\n```\n\n----------------------------------------\n\nTITLE: Running Gensim LSI Worker Nodes\nDESCRIPTION: This command starts a Gensim LSI worker node, which will participate in distributed LSA computations.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/dist_lsi.rst#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m gensim.models.lsi_worker &\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Performance Improvements in Gensim 4.0\nDESCRIPTION: Comparison of performance metrics between Gensim 3.8.3 and 4.0.0 for fastText and word2vec models, showing improvements in processing time and memory usage.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/CHANGELOG.md#2025-04-21_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n| model | 3.8.3: wall time / peak RAM / throughput | 4.0.0: wall time / peak RAM / throughput |\n|----------|------------|--------|\n| fastText | 2.9h / 4.11 GB / 822k words/s | 2.3h / **1.26 GB** / 914k words/s |\n| word2vec | 1.7h / 0.36 GB / 1685k words/s | **1.2h** / 0.33 GB / 1762k words/s |\n```\n\n----------------------------------------\n\nTITLE: Gensim Word2Vec Training Log Output\nDESCRIPTION: Training progress log showing word processing speed, completion percentage, and queue sizes during Word2Vec model training. The log spans multiple epochs, showing metrics like words processed per second and training progress percentage.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_30\n\nLANGUAGE: text\nCODE:\n```\n2023-08-23 13:05:42,637 : INFO : EPOCH 1 - PROGRESS: at 23.68% examples, 266660 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:43,684 : INFO : EPOCH 1 - PROGRESS: at 24.93% examples, 267131 words/s, in_qsize 3, out_qsize 0\n[...truncated...]\n2023-08-23 13:07:08,211 : INFO : EPOCH 2 - PROGRESS: at 26.89% examples, 276418 words/s, in_qsize 3, out_qsize 0\n```\n\n----------------------------------------\n\nTITLE: Batch Processing Word Clouds for Multiple Tutorials in Python\nDESCRIPTION: This function processes multiple tutorials, creating word clouds for each using predefined masks and texts. It saves the resulting images with the same name as the input Python files but with a .png extension.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/tools/wordcloud.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef make_clouds(subdir):\n    masks = ('one.png', 'two.png', 'three.png', 'four.png')\n    py_files = [P.join(subdir, f) for f in os.listdir(subdir) if f.endswith('.py')]\n    text = [\n        'core concepts document corpus model vector',\n        'corpora vector spaces corpus streaming corpus formats serialization',\n        'topics transformations model',\n        'similarity queries query similar documents'\n    ]\n    for m, p, t in zip(masks, py_files, text):\n        make_cloud(m, t, re.sub('.py$', '.png', p))\n        \nmake_clouds('../gallery/core/')\n```\n\n----------------------------------------\n\nTITLE: Importing Hierarchical Dirichlet Process Model in Python\nDESCRIPTION: This code snippet demonstrates how to import the hdpmodel module from gensim.models. The Hierarchical Dirichlet Process model is used for topic modeling with an automatically determined number of topics.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/models/hdpmodel.rst#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom gensim.models import hdpmodel\n```\n\n----------------------------------------\n\nTITLE: Preparing Text Corpus and Dictionary\nDESCRIPTION: Downloads text corpus using gensim downloader, processes text with lemmatization, and creates dictionary with frequency filtering.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_ensemblelda.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport gensim.downloader as api\nfrom gensim.corpora import Dictionary\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\ndocs = api.load('text8')\n\ndictionary = Dictionary()\nfor doc in docs:\n    dictionary.add_documents([[lemmatizer.lemmatize(token) for token in doc]])\ndictionary.filter_extremes(no_below=20, no_above=0.5)\n\ncorpus = [dictionary.doc2bow(doc) for doc in docs]\n```\n\n----------------------------------------\n\nTITLE: Logging Doc2Vec Training Progress in Python\nDESCRIPTION: Log output showing the progress of training a Doc2Vec model using Gensim. It displays the epoch number, percentage of examples processed, words processed per second, and input/output queue sizes.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_14\n\nLANGUAGE: text\nCODE:\n```\n2023-08-23 12:49:04,655 : INFO : EPOCH 18 - PROGRESS: at 25.64% examples, 838133 words/s, in_qsize 3, out_qsize 0\n```\n\n----------------------------------------\n\nTITLE: Displaying 300-dimensional Word Embedding Vector for 'grape'\nDESCRIPTION: This is a word embedding vector representation for the word 'grape', containing 300 floating-point values. These values represent the word in a high-dimensional vector space where semantically similar words would have similar vectors.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/EN.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_16\n\nLANGUAGE: text\nCODE:\n```\ngrape 1.703300029039382935e-01 -2.813819944858551025e-01 1.890030056238174438e-01 2.653760015964508057e-01 -2.627939879894256592e-01 1.461230069398880005e-01 -4.704200103878974915e-02 5.474700033664703369e-02 -4.394490122795104980e-01 2.382599934935569763e-02 -1.546880006790161133e-01 -5.356099829077720642e-02 3.106350004673004150e-01 -4.683319926261901855e-01 6.644840240478515625e-01 -7.095900177955627441e-02 -3.737299889326095581e-02 2.012149989604949951e-01 9.581899642944335938e-02 -6.499729752540588379e-01 -4.086639881134033203e-01 7.358799874782562256e-02 1.253930032253265381e-01 -1.635999977588653564e-02 -7.683599740266799927e-02 5.464500188827514648e-01 2.461940050125122070e-01 3.296200186014175415e-02 5.820599943399429321e-02 2.619560062885284424e-01 2.623890042304992676e-01 -1.497620046138763428e-01 3.447199985384941101e-02 -8.359900116920471191e-02 -1.824869960546493530e-01 -4.652500152587890625e-02 1.820109933614730835e-01 -2.147109955549240112e-01 1.146029978990554810e-01 -1.548870056867599487e-01 -3.045620024204254150e-01 -2.776199877262115479e-01 -8.692300319671630859e-02 2.310760021209716797e-01 3.947980105876922607e-01 1.588599979877471924e-01 4.812209904193878174e-01 -3.603990077972412109e-01 -2.822900116443634033e-01 2.512109875679016113e-01 1.439789980649948120e-01 -3.490149974822998047e-01 -6.247700005769729614e-02 1.581429988145828247e-01 -3.460870087146759033e-01 2.426199987530708313e-02 -1.994449943304061890e-01 7.595299929380416870e-02 -1.192269995808601379e-01 -1.576749980449676514e-01 -1.676409989595413208e-01 -2.005320042371749878e-01 1.391820013523101807e-01 1.840700022876262665e-02 1.290380060672760010e-01 9.167200326919555664e-02 6.364499777555465698e-02 1.190999988466501236e-02 8.233100175857543945e-02 2.232639938592910767e-01 2.828089892864227295e-01 3.405499830842018127e-02 7.938800007104873657e-02 -5.704599991440773010e-02 -8.877100050449371338e-02 4.262950122356414795e-01 -4.410000145435333252e-01 5.193499848246574402e-02 -5.344799906015396118e-02 -1.289539933204650879e-01 -2.527540028095245361e-01 1.091269999742507935e-01 3.220329880714416504e-01 4.174169898033142090e-01 -5.224600061774253845e-02 1.847999989986419678e-01 -1.514669954776763916e-01 -2.890659868717193604e-01 -8.009900152683258057e-02 -1.884949952363967896e-01 2.203720062971115112e-01 -2.796289920806884766e-01 -6.868399679660797119e-02 2.723800018429756165e-02 -5.661100149154663086e-02 -2.129199914634227753e-02 1.018280014395713806e-01 3.213300183415412903e-02 7.093200087547302246e-02 -1.568540036678314209e-01 -3.206380009651184082e-01 -1.479029953479766846e-01 -3.058739900588989258e-01 9.646099805831909180e-02 -1.733499951660633087e-02 4.315100014209747314e-01 -3.422929942607879639e-01 1.292600035667419434e-01 -1.547500044107437134e-01 -2.079090029001235962e-01 -2.946969866752624512e-01 -1.489000022411346436e-02 -2.758179903030395508e-01 2.187969982624053955e-01 -1.516859978437423706e-01 -2.216349989175796509e-01 2.010170072317123413e-01 -2.883199974894523621e-02 -2.425699979066848755e-01 -2.358200028538703918e-02 -1.596830040216445923e-01 -7.677999790757894516e-03 3.478069901466369629e-01 -2.426699995994567871e-01 2.732819914817810059e-01 -2.667070031166076660e-01 1.304800063371658325e-01 1.098880022764205933e-01 4.397900030016899109e-02 -4.329900071024894714e-02 2.167599946260452271e-01 -1.391980051994323730e-01 2.468120008707046509e-01 -2.106100022792816162e-01 3.568530082702636719e-01 -2.510350048542022705e-01 -3.793970048427581787e-01 3.296490013599395752e-01 5.824999883770942688e-03 -2.272000070661306381e-03 -4.238060116767883301e-01 5.382800102233886719e-02 -4.413819909095764160e-01 1.460300013422966003e-02 6.047600135207176208e-02 -8.071999996900558472e-03 7.933899760246276855e-02 5.338200181722640991e-02 -8.285500109195709229e-02 -3.078019917011260986e-01 2.104499936103820801e-01 1.412360072135925293e-01 -4.137369990348815918e-01 1.293029934167861938e-01 8.652299642562866211e-02 -1.945589929819107056e-01 -7.339300215244293213e-02 3.518289923667907715e-01 -3.676249980926513672e-01 -1.919389963150024414e-01 1.572000049054622650e-02 -2.157869935035705566e-01 8.087299764156341553e-02 -1.786019951105117798e-01 4.173089861869812012e-01 3.170999884605407715e-01 2.715269923210144043e-01 -3.429839909076690674e-01 -3.795410096645355225e-01 7.271999865770339966e-02 1.617499999701976776e-02 1.325269937515258789e-01 3.648529946804046631e-01 9.411999955773353577e-03 -1.562680006027221680e-01 2.553429901599884033e-01 -1.793220043182373047e-01 2.035080045461654663e-01 2.416889965534210205e-01 3.994410037994384766e-01 -2.748299948871135712e-02 -2.911959886550903320e-01 6.235000211745500565e-03 1.462550014257431030e-01 9.708400070667266846e-02 -5.545359849929809570e-01 -1.358100026845932007e-01 -2.558819949626922607e-01 -3.434590101242065430e-01 1.222500018775463104e-02 4.010900110006332397e-02 3.967930078506469727e-01 -8.305799961090087891e-02 -7.053600251674652100e-02 -1.425119936466217041e-01 5.046100169420242310e-02 1.161599997431039810e-02 6.214499846100807190e-02 5.078300088644027710e-02 3.201780021190643311e-01 3.284310102462768555e-01 1.700779944658279419e-01 1.553889960050582886e-01 1.092099957168102264e-02 -3.123489916324615479e-01 -1.089299991726875305e-01 9.374599903821945190e-02 -2.045309990644454956e-01 -3.133749961853027344e-01 -5.543130040168762207e-01 8.658800274133682251e-02 -4.389000125229358673e-03 -2.154729962348937988e-01 -2.487729936838150024e-01 -3.288590013980865479e-01 -7.619900256395339966e-02 1.847369968891143799e-01 1.281290054321289062e-01 -1.460530012845993042e-01 1.106849983334541321e-01 2.492160052061080933e-01 -4.598199948668479919e-02 -2.129939943552017212e-01 -4.291700124740600586e-01 -2.101400047540664673e-01 -2.337020039558410645e-01 -2.707929909229278564e-01 -2.572309970855712891e-01 9.194800257682800293e-02 3.452459871768951416e-01 -4.841490089893341064e-01 8.791999891400337219e-03 7.448399811983108521e-02 -1.996799930930137634e-02 -3.237119913101196289e-01 -3.656999906525015831e-03 -3.698880076408386230e-01 -8.205900341272354126e-02 1.225749999284744263e-01 4.295700043439865112e-02 3.007250130176544189e-01 -1.495999982580542564e-03 1.183279976248741150e-01 2.754229903221130371e-01 2.338800020515918732e-02 9.848400205373764038e-02 -8.960500359535217285e-02 7.157900184392929077e-02 1.339720040559768677e-01 1.399060040712356567e-01 -5.462020039558410645e-01 -1.957419961690902710e-01 4.436399936676025391e-01 1.493999967351555824e-03 -2.397299930453300476e-02 -6.707999855279922485e-02 1.490850001573562622e-01 2.861200086772441864e-02 -2.076950073242187500e-01 8.799199759960174561e-02 -2.472579926252365112e-01 1.772090047597885132e-01 -7.742500305175781250e-02 -8.793400228023529053e-02 7.822299748659133911e-02 -3.055419921875000000e-01 -8.260799944400787354e-02 -2.196329981088638306e-01 -9.920199960470199585e-02 -2.126359939575195312e-01 6.547299772500991821e-02 3.060970008373260498e-01 6.243500113487243652e-02 -8.124000392854213715e-03 2.540830075740814209e-01 -2.715440094470977783e-01 -9.201499819755554199e-02 -1.164750009775161743e-01 1.217000000178813934e-02 7.449900358915328979e-02 -5.007800087332725525e-02 1.677870005369186401e-01 2.456820011138916016e-01 -1.314619928598403931e-01 -2.382339984178543091e-01 -3.458299860358238220e-02 -1.073300018906593323e-01 3.594500124454498291e-01 5.744099989533424377e-02 5.452400073409080505e-02 1.049939990043640137e-01 -4.482199996709823608e-02 2.318129986524581909e-01 -3.526580035686492920e-01 -2.049009948968887329e-01 4.019499942660331726e-02 7.344499975442886353e-02 2.059099972248077393e-01 4.263999871909618378e-03 2.034180015325546265e-01\n```\n\n----------------------------------------\n\nTITLE: Checking if Word is in FastText Model Vocabulary\nDESCRIPTION: Example of how to check if a word is in the known vocabulary of a FastText model after changes in out-of-vocabulary word handling.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/CHANGELOG.md#2025-04-21_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n'any_word' in ft_model.wv.vocab\n```\n\n----------------------------------------\n\nTITLE: Matplotlib Import for Visualization\nDESCRIPTION: Imports the matplotlib.pyplot module for creating visualizations.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_annoy.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n```\n\n----------------------------------------\n\nTITLE: Logging Word2Vec Training Progress in Gensim\nDESCRIPTION: This log snippet shows the progress of training a Word2Vec model using Gensim. It includes information about the epoch number, percentage of examples processed, words processed per second, and input/output queue sizes.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_43\n\nLANGUAGE: log\nCODE:\n```\n2023-08-23 13:27:08,670 : INFO : EPOCH 17: training on 23279529 raw words (22951015 effective words) took 73.3s, 313195 effective words/s\n2023-08-23 13:27:09,677 : INFO : EPOCH 18 - PROGRESS: at 1.41% examples, 308523 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:27:10,684 : INFO : EPOCH 18 - PROGRESS: at 2.80% examples, 317553 words/s, in_qsize 3, out_qsize 0\n// ... additional log lines omitted for brevity ...\n2023-08-23 13:28:32,251 : INFO : EPOCH 19 - PROGRESS: at 15.42% examples, 313968 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:28:33,270 : INFO : EPOCH 19 - PROGRESS: at 16.81% examples, 314007 words/s, in_qsize 3, out_qsize 0\n```\n\n----------------------------------------\n\nTITLE: Disabling logging for multiprocessing examples\nDESCRIPTION: Temporarily disables logging to avoid verbose output during the multiprocessing demonstrations that follow.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Remove verbosity from code below (if logging active)\n\nif LOGS:\n    logging.disable(logging.CRITICAL)\n```\n\n----------------------------------------\n\nTITLE: Importing HDP Model from Gensim in Python\nDESCRIPTION: This snippet shows how to import the HDP (Hierarchical Dirichlet Process) model from Gensim's scikit-learn API. It's required for using HDP in a scikit-learn pipeline.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/sklearn_api.ipynb#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.sklearn_api import HdpTransformer\n```\n\n----------------------------------------\n\nTITLE: Using RST for Structuring Gensim Documentation\nDESCRIPTION: ReStructuredText markup used to organize the how-to guides section of Gensim documentation, including section headings, tooltips, image references, and hidden toctree elements.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/index.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _sphx_glr_auto_examples_howtos:\n\nHow-to Guides: Solve a Problem\n------------------------------\n\nThese **goal-oriented guides** demonstrate how to **solve a specific problem** using gensim.\n\n\n\n.. raw:: html\n\n    <div class=\"sphx-glr-thumbnails\">\n\n\n.. raw:: html\n\n    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"Demonstrates simple and quick access to common corpora and pretrained models.\">\n\n.. only:: html\n\n  .. image:: /auto_examples/howtos/images/thumb/sphx_glr_run_downloader_api_thumb.png\n    :alt: How to download pre-trained models and corpora\n\n  :ref:`sphx_glr_auto_examples_howtos_run_downloader_api.py`\n\n.. raw:: html\n\n      <div class=\"sphx-glr-thumbnail-title\">How to download pre-trained models and corpora</div>\n    </div>\n\n\n.. raw:: html\n\n    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"How to author documentation for Gensim.\">\n\n.. only:: html\n\n  .. image:: /auto_examples/howtos/images/thumb/sphx_glr_run_doc_thumb.png\n    :alt: How to Author Gensim Documentation\n\n  :ref:`sphx_glr_auto_examples_howtos_run_doc.py`\n\n.. raw:: html\n\n      <div class=\"sphx-glr-thumbnail-title\">How to Author Gensim Documentation</div>\n    </div>\n\n\n.. raw:: html\n\n    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"Shows how to reproduce results of the &quot;Distributed Representation of Sentences and Documents&quot; p...\">\n\n.. only:: html\n\n  .. image:: /auto_examples/howtos/images/thumb/sphx_glr_run_doc2vec_imdb_thumb.png\n    :alt: How to reproduce the doc2vec 'Paragraph Vector' paper\n\n  :ref:`sphx_glr_auto_examples_howtos_run_doc2vec_imdb.py`\n\n.. raw:: html\n\n      <div class=\"sphx-glr-thumbnail-title\">How to reproduce the doc2vec 'Paragraph Vector' paper</div>\n    </div>\n\n\n.. raw:: html\n\n    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"Demonstrates how you can visualize and compare trained topic models.\">\n\n.. only:: html\n\n  .. image:: /auto_examples/howtos/images/thumb/sphx_glr_run_compare_lda_thumb.png\n    :alt: How to Compare LDA Models\n\n  :ref:`sphx_glr_auto_examples_howtos_run_compare_lda.py`\n\n.. raw:: html\n\n      <div class=\"sphx-glr-thumbnail-title\">How to Compare LDA Models</div>\n    </div>\n\n\n.. raw:: html\n\n    </div>\n\n\n.. toctree::\n   :hidden:\n\n   /auto_examples/howtos/run_downloader_api\n   /auto_examples/howtos/run_doc\n   /auto_examples/howtos/run_doc2vec_imdb\n   /auto_examples/howtos/run_compare_lda\n```\n\n----------------------------------------\n\nTITLE: Topic Model Training Log Output\nDESCRIPTION: Log entries showing the evolution of topic distributions and alpha optimizations across multiple training passes. Each topic shows word distributions with associated weights, focusing on neural network and machine learning concepts.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_lda.rst#2025-04-21_snippet_9\n\nLANGUAGE: log\nCODE:\n```\n2022-04-22 17:43:44,821 : INFO : topic #6 (0.054): 0.011*\"image\" + 0.008*\"object\" + 0.007*\"visual\" + 0.007*\"motion\" + 0.006*\"field\" + 0.006*\"cell\" + 0.005*\"direction\" + 0.005*\"filter\" + 0.004*\"signal\" + 0.004*\"response\"\n2022-04-22 17:43:44,822 : INFO : topic #8 (0.065): 0.005*\"rule\" + 0.005*\"hidden\" + 0.003*\"sequence\" + 0.003*\"generalization\" + 0.003*\"net\" + 0.003*\"bound\" + 0.003*\"prediction\" + 0.003*\"hidden_unit\" + 0.003*\"optimal\" + 0.003*\"machine\"\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Module Documentation Header\nDESCRIPTION: Module documentation header defining the Levenshtein similarity module and its capabilities for string similarity searches using Soft-Cosine Semantic Similarity.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/similarities/levenshtein.rst#2025-04-21_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n:mod:`similarities.levenshtein` -- Fast soft-cosine semantic similarity search\n==============================================================================\n\n.. automodule:: gensim.similarities.levenshtein\n    :synopsis: Fast fuzzy search between strings, using the Soft-Cosine Semantic Similarity\n    :members:\n    :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Visualizing an Ideal Topic Difference Matrix\nDESCRIPTION: Creates and visualizes an ideal topic difference matrix where all topics are completely decorrelated (except with themselves), represented as a diagonal matrix.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_compare_lda.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nmdiff = np.ones((num_topics, num_topics))\nnp.fill_diagonal(mdiff, 0.)\nplot_difference(mdiff, title=\"Topic difference (one model) in ideal world\")\n```\n\n----------------------------------------\n\nTITLE: Results Processing Function\nDESCRIPTION: Processes benchmark results to calculate processing and production speeds, formatting durations into human-readable format.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_benchmark.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(results)\ndf[\"processing_speed\"] = df.dictionary_size ** 2 / df.production_duration\ndf[\"production_speed\"] = df.dictionary_size * df.nonzero_limit / df.production_duration\ndf = df.groupby([\"dictionary_size\", \"nonzero_limit\"])\n\ndef display(df):\n    df[\"constructor_duration\"] = [timedelta(0, duration) for duration in df[\"constructor_duration\"]]\n    df[\"production_duration\"] = [timedelta(0, duration) for duration in df[\"production_duration\"]]\n    df[\"processing_speed\"] = [\"%.02f Kword pairs / s\" % (speed / 1000) for speed in df[\"processing_speed\"]]\n    df[\"production_speed\"] = [\"%.02f Kword pairs / s\" % (speed / 1000) for speed in df[\"production_speed\"]]\n    return df\n```\n\n----------------------------------------\n\nTITLE: Word Vector Representation for 'otto'\nDESCRIPTION: This snippet shows the 300-dimensional word vector for the word 'otto'. Each number represents a dimension in the embedding space, capturing semantic and syntactic properties of the word.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/IT.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\notto 1.320369988679885864e-01 1.641600020229816437e-02 6.165799871087074280e-02 -1.292760074138641357e-01 1.684850007295608521e-01 1.311109960079193115e-01 2.061000093817710876e-03 4.189499840140342712e-02 2.453000051900744438e-03 -2.797299996018409729e-02 -1.555349975824356079e-01 1.691300049424171448e-02 -2.706960141658782959e-01 -1.353699993342161179e-02 -9.377200156450271606e-02 1.105270013213157654e-01 -1.017249971628189087e-01 -4.786999896168708801e-02 -2.013790011405944824e-01 3.714999929070472717e-02 5.748200044035911560e-02 -1.084330007433891296e-01 1.129200011491775513e-01 1.387629956007003784e-01 -2.728889882564544678e-01 1.544969975948333740e-01 4.337000194936990738e-03 -4.630399867892265320e-02 -2.252900041639804840e-02 1.169359982013702393e-01 -5.283499881625175476e-02 1.595840007066726685e-01 3.407900035381317139e-02 -2.303099930286407471e-01 1.982029974460601807e-01 -6.289999932050704956e-03 1.784929931163787842e-01 1.521240025758743286e-01 1.971890032291412354e-01 -9.789600223302841187e-02 4.595300182700157166e-02 -9.703000076115131378e-03 1.088699977844953537e-02 2.196550071239471436e-01 8.713999763131141663e-03 -1.775499992072582245e-02 -1.133100036531686783e-02 1.433770060539245605e-01 -6.713400036096572876e-02 7.484000176191329956e-02 6.442800164222717285e-02 2.322099916636943817e-02 2.998699992895126343e-02 -2.073940038681030273e-01 1.577799953520298004e-02 -4.085100069642066956e-02 2.317990064620971680e-01 -3.586849868297576904e-01 -6.348499655723571777e-02 2.766140103340148926e-01 2.392479926347732544e-01 -7.351200282573699951e-02 -5.814100056886672974e-02 4.344600066542625427e-02 -1.390929967164993286e-01 -1.376789957284927368e-01 1.016419976949691772e-01 2.579100057482719421e-02 -2.521199919283390045e-02 1.810760051012039185e-01 1.565960049629211426e-01 -4.778999835252761841e-02 1.336960047483444214e-01 -3.621299937367439270e-02 -4.492900148034095764e-02 2.924199961125850677e-02 2.237300015985965729e-02 1.281400024890899658e-01 -6.751500070095062256e-02 -5.503499880433082581e-02 7.716099917888641357e-02 7.180699706077575684e-02 1.194619983434677124e-01 -1.173399984836578369e-01 -9.007000364363193512e-03 2.205099910497665405e-02 7.876200228929519653e-02 1.224120035767555237e-01 -2.838909924030303955e-01 -4.377000033855438232e-03 -6.628000177443027496e-03 6.557399779558181763e-02 -8.445999585092067719e-03 3.240400180220603943e-02 -1.913769990205764771e-01 1.237939968705177307e-01 -4.253799840807914734e-02 3.607200086116790771e-01 1.195290014147758484e-01 -1.188379973173141479e-01 4.651200026273727417e-02 6.997799873352050781e-02 2.365729957818984985e-01 -1.483399979770183563e-02 -7.456500083208084106e-02 2.365200035274028778e-02 -6.221399828791618347e-02 2.724100090563297272e-02 1.555799972265958786e-02 -7.792600244283676147e-02 -1.923600025475025177e-02 -1.347250044345855713e-01 2.176429927349090576e-01 -2.052900008857250214e-02 3.445459902286529541e-01 4.545899853110313416e-02 -6.349100172519683838e-02 -4.095000214874744415e-03 -2.288099974393844604e-01 -6.223500147461891174e-02 -3.460500016808509827e-02 2.560940086841583252e-01 1.271000038832426071e-02 -6.395599991083145142e-02 3.348299860954284668e-02 7.015000283718109131e-02 -2.285369932651519775e-01 1.091600023210048676e-02 1.294230073690414429e-01 -1.101579964160919189e-01 2.641809880733489990e-01 3.891900181770324707e-02 -1.664980053901672363e-01 -5.822000093758106232e-03 -9.901000186800956726e-03 1.789280027151107788e-01 -6.961800158023834229e-02 -1.205350011587142944e-01 1.801200024783611298e-02 2.002299949526786804e-02 1.385099999606609344e-02 1.102109998464584351e-01 1.386809945106506348e-01 8.364599943161010742e-02 -2.651470005512237549e-01 -6.424800306558609009e-02 -4.187700152397155762e-02 1.635099947452545166e-01 5.877999961376190186e-02 3.361000120639801025e-02 4.129999876022338867e-02 -8.341799676418304443e-02 -4.697600007057189941e-02 -6.024900078773498535e-02 -2.099030017852783203e-01 -3.727100044488906860e-02 -7.540900260210037231e-02 -4.821000155061483383e-03 -7.681000232696533203e-02 7.263699918985366821e-02 2.167000062763690948e-02 7.855100184679031372e-02 -1.661100052297115326e-02 1.920099928975105286e-02 -2.220589965581893921e-01 1.670069992542266846e-01 4.325300082564353943e-02 1.188319995999336243e-01 -3.623700141906738281e-02 1.672299951314926147e-01 1.285000005736947060e-03 2.571200020611286163e-02 1.693789958953857422e-01 -2.265059947967529297e-01 1.582140028476715088e-01 1.402049958705902100e-01 -1.624760031700134277e-01 -1.713919937610626221e-01 -9.549500048160552979e-02 6.510200351476669312e-02 -8.551900088787078857e-02 -4.360299929976463318e-02 -3.559299930930137634e-02 -7.918799668550491333e-02 2.366900071501731873e-02 1.792919933795928955e-01 5.709299817681312561e-02 2.633699961006641388e-02 -2.709789872169494629e-01 -9.914600104093551636e-02 -1.390829980373382568e-01 1.391450017690658569e-01 1.817570030689239502e-01 1.402350068092346191e-01 -7.948599755764007568e-02 4.142700135707855225e-02 -4.319600015878677368e-02 -8.942899852991104126e-02 2.239000052213668823e-02 -1.985519975423812866e-01 4.439780116081237793e-01 7.910200208425521851e-02 -8.677799999713897705e-02 7.465100288391113281e-02 2.110000059474259615e-04 1.951950043439865112e-01 7.502000033855438232e-02 -7.107099890708923340e-02 -2.210500091314315796e-02 2.123280018568038940e-01 1.820340007543563843e-01 2.665379941463470459e-01 1.304209977388381958e-01 6.510899960994720459e-02 1.071510016918182373e-01 -6.106400117278099060e-02 -5.578500032424926758e-02 1.186679974198341370e-01 -1.369199994951486588e-02 -1.616220027208328247e-01 1.046179980039596558e-01 9.972999803721904755e-03 1.023079976439476013e-01 1.807290017604827881e-01 2.004099935293197632e-01 1.871179938316345215e-01 5.754800140857696533e-02 1.813939958810806274e-01 1.373299956321716309e-02 -8.665000088512897491e-03 1.688109934329986572e-01 -1.997150033712387085e-01 6.110100075602531433e-02 2.800709903240203857e-01 -5.088999867439270020e-03 1.435749977827072144e-01 -8.806200325489044189e-02 1.699600070714950562e-01 1.181209981441497803e-01 -1.307700015604496002e-02 -1.268399972468614578e-02 -8.390799909830093384e-02 -1.093410030007362366e-01 3.305900096893310547e-02 6.961700320243835449e-02 8.745899796485900879e-02 2.249290049076080322e-01 7.138299942016601562e-02 3.678499907255172729e-02 -1.734000071883201599e-02 1.997599937021732330e-02 -9.439600259065628052e-02 8.495900034904479980e-02 1.909170001745223999e-01 1.683890074491500854e-01 -6.606099754571914673e-02 7.674200087785720825e-02 7.266899943351745605e-02 -1.905799955129623413e-01 4.283300042152404785e-02 -9.350699931383132935e-02 -2.495370060205459595e-01 7.028999924659729004e-02 -4.612999968230724335e-03 -2.385479956865310669e-01 -1.236630007624626160e-01 1.807889938354492188e-01 2.393800020217895508e-02 -1.222500018775463104e-02 1.330270022153854370e-01 -2.248400077223777771e-02 3.306600078940391541e-02 -3.978000022470951080e-03 1.184870004653930664e-01 6.112400069832801819e-02 2.185630053281784058e-01 -4.134999960660934448e-02 -8.099199831485748291e-02 -4.866100102663040161e-02 2.109680026769638062e-01 2.201399952173233032e-02 1.398629993200302124e-01 1.766999950632452965e-03 -4.886899888515472412e-02 -1.479649990797042847e-01 1.386860013008117676e-01 -3.101699985563755035e-02 5.867699906229972839e-02 4.262400045990943909e-02 9.840399771928787231e-02 -7.180699706077575684e-02 -5.993999913334846497e-02 5.104700103402137756e-02 1.131519973278045654e-01 1.975339949131011963e-01 -3.561399877071380615e-02 -4.652800038456916809e-02 -2.171519994735717773e-01 7.769899815320968628e-02 1.078409999608993530e-01\n```\n\n----------------------------------------\n\nTITLE: Downloading Wikinews Data\nDESCRIPTION: Bash script to download Wikinews cirrussearch dump if not already present\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/wikinews-bigram-en.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n%%bash\n\nfdate=20170327\nfname=enwikinews-$fdate-cirrussearch-content.json.gz\nif [ ! -e  $fname ]\nthen\n    wget \"https://dumps.wikimedia.org/other/cirrussearch/$fdate/$fname\"\nfi\n```\n\n----------------------------------------\n\nTITLE: Word2Vec Training Progress Log Output\nDESCRIPTION: Training progress log showing metrics for Word2Vec model training across multiple epochs. Each line shows the timestamp, epoch number, percent complete, processing speed (words/s), and queue sizes.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_12\n\nLANGUAGE: log\nCODE:\n```\n2023-08-23 12:46:17,807 : INFO : EPOCH 12 - PROGRESS: at 31.66% examples, 804486 words/s, in_qsize 3, out_qsize 0\n2023-08-23 12:46:18,809 : INFO : EPOCH 12 - PROGRESS: at 35.21% examples, 805316 words/s, in_qsize 3, out_qsize 0\n[...additional log lines omitted for brevity...]\n2023-08-23 12:47:40,174 : INFO : EPOCH 15 - PROGRESS: at 21.95% examples, 837269 words/s, in_qsize 3, out_qsize 0\n```\n\n----------------------------------------\n\nTITLE: Analyzing List of Animals (Third Time)\nDESCRIPTION: This snippet contains animal names, serving as input for applications such as language training models or animal identification tools. The animal names include 'cheetah', 'lion', 'kitten', 'jaguar', etc.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/ldavowpalwabbit.txt#2025-04-21_snippet_22\n\nLANGUAGE: text\nCODE:\n```\n\"cheetah lion kitten kitten jaguar lynx jaguar puppy tiger mouse leopard lynx puppy lynx cat cat lion leopard leopard leopard\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Dictionary Size\nDESCRIPTION: Prints the number of unique tokens in the training dictionary to verify the effectiveness of preprocessing and filtering steps.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_prediction_tutorial.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint('Number of unique tokens: %d' % len(train_dictionary_50_20))\n```\n\n----------------------------------------\n\nTITLE: Generating API Documentation for similarities.annoy Module in Python\nDESCRIPTION: This snippet uses Sphinx directives to automatically generate API documentation for the gensim.similarities.annoy module. It includes synopsis information and specifies that all members and inherited members should be documented.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/similarities/annoy.rst#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: gensim.similarities.annoy\n    :synopsis: Fast Approximate Nearest Neighbor Similarity with the Annoy package\n    :members:\n    :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries in Python\nDESCRIPTION: Imports the pprint library for pretty-printing Python data structures.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_core_concepts.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pprint\n```\n\n----------------------------------------\n\nTITLE: Word2Vec Training Progress Log\nDESCRIPTION: Detailed training log output showing epoch progress, processing speed (words/s), and queue states during Word2Vec model training. Shows consistent processing speeds around 300k words/s with input and output queue monitoring.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_37\n\nLANGUAGE: log\nCODE:\n```\n2023-08-23 13:17:14,904 : INFO : EPOCH 9 - PROGRESS: at 98.24% examples, 302218 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:17:15,930 : INFO : EPOCH 9 - PROGRESS: at 99.66% examples, 302342 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:17:16,168 : INFO : EPOCH 9: training on 23279529 raw words (22951015 effective words) took 75.9s, 302410 effective words/s\n```\n\n----------------------------------------\n\nTITLE: Monitoring Gensim Model Training Progress via Log Output\nDESCRIPTION: Log output from Gensim's word embedding model training showing training progress across epochs. Each line displays timestamp, epoch number, percentage completion, processing speed (words/second), and internal queue statuses.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_29\n\nLANGUAGE: log\nCODE:\n```\n2023-08-23 13:04:16,419 : INFO : EPOCH 0 - PROGRESS: at 23.64% examples, 231296 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:17,428 : INFO : EPOCH 0 - PROGRESS: at 24.85% examples, 233185 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:18,474 : INFO : EPOCH 0 - PROGRESS: at 26.05% examples, 234253 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:19,513 : INFO : EPOCH 0 - PROGRESS: at 27.26% examples, 235637 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:20,517 : INFO : EPOCH 0 - PROGRESS: at 28.43% examples, 236909 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:21,553 : INFO : EPOCH 0 - PROGRESS: at 29.62% examples, 237795 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:22,581 : INFO : EPOCH 0 - PROGRESS: at 30.77% examples, 238681 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:23,589 : INFO : EPOCH 0 - PROGRESS: at 31.97% examples, 239701 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:24,616 : INFO : EPOCH 0 - PROGRESS: at 33.15% examples, 240484 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:25,621 : INFO : EPOCH 0 - PROGRESS: at 34.31% examples, 241383 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:26,656 : INFO : EPOCH 0 - PROGRESS: at 35.46% examples, 241755 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:27,666 : INFO : EPOCH 0 - PROGRESS: at 36.61% examples, 242285 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:28,667 : INFO : EPOCH 0 - PROGRESS: at 37.81% examples, 243095 words/s, in_qsize 2, out_qsize 1\n2023-08-23 13:04:29,678 : INFO : EPOCH 0 - PROGRESS: at 39.05% examples, 243998 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:30,684 : INFO : EPOCH 0 - PROGRESS: at 40.21% examples, 244655 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:31,735 : INFO : EPOCH 0 - PROGRESS: at 41.41% examples, 245217 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:32,747 : INFO : EPOCH 0 - PROGRESS: at 42.60% examples, 245792 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:33,790 : INFO : EPOCH 0 - PROGRESS: at 43.82% examples, 246384 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:34,859 : INFO : EPOCH 0 - PROGRESS: at 45.08% examples, 246796 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:35,864 : INFO : EPOCH 0 - PROGRESS: at 46.26% examples, 247305 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:36,890 : INFO : EPOCH 0 - PROGRESS: at 47.52% examples, 247945 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:37,899 : INFO : EPOCH 0 - PROGRESS: at 48.70% examples, 248408 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:38,919 : INFO : EPOCH 0 - PROGRESS: at 49.82% examples, 248780 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:39,946 : INFO : EPOCH 0 - PROGRESS: at 51.02% examples, 249279 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:40,976 : INFO : EPOCH 0 - PROGRESS: at 52.23% examples, 249767 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:42,021 : INFO : EPOCH 0 - PROGRESS: at 53.42% examples, 249987 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:43,037 : INFO : EPOCH 0 - PROGRESS: at 54.58% examples, 250364 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:44,037 : INFO : EPOCH 0 - PROGRESS: at 55.73% examples, 250583 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:45,067 : INFO : EPOCH 0 - PROGRESS: at 56.99% examples, 251001 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:46,076 : INFO : EPOCH 0 - PROGRESS: at 58.22% examples, 251501 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:47,080 : INFO : EPOCH 0 - PROGRESS: at 59.44% examples, 252016 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:48,097 : INFO : EPOCH 0 - PROGRESS: at 60.69% examples, 252299 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:49,134 : INFO : EPOCH 0 - PROGRESS: at 61.89% examples, 252612 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:50,148 : INFO : EPOCH 0 - PROGRESS: at 63.15% examples, 253023 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:51,148 : INFO : EPOCH 0 - PROGRESS: at 64.41% examples, 253504 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:52,184 : INFO : EPOCH 0 - PROGRESS: at 65.52% examples, 253652 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:53,215 : INFO : EPOCH 0 - PROGRESS: at 66.74% examples, 253973 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:54,253 : INFO : EPOCH 0 - PROGRESS: at 67.98% examples, 254249 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:55,307 : INFO : EPOCH 0 - PROGRESS: at 69.18% examples, 254468 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:56,336 : INFO : EPOCH 0 - PROGRESS: at 70.29% examples, 254457 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:57,347 : INFO : EPOCH 0 - PROGRESS: at 71.48% examples, 254809 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:58,369 : INFO : EPOCH 0 - PROGRESS: at 72.71% examples, 254989 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:04:59,374 : INFO : EPOCH 0 - PROGRESS: at 73.97% examples, 255378 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:00,376 : INFO : EPOCH 0 - PROGRESS: at 75.02% examples, 255278 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:01,389 : INFO : EPOCH 0 - PROGRESS: at 76.12% examples, 255335 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:02,444 : INFO : EPOCH 0 - PROGRESS: at 77.25% examples, 255222 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:03,456 : INFO : EPOCH 0 - PROGRESS: at 78.37% examples, 255150 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:04,473 : INFO : EPOCH 0 - PROGRESS: at 79.44% examples, 255041 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:05,534 : INFO : EPOCH 0 - PROGRESS: at 80.65% examples, 255042 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:06,552 : INFO : EPOCH 0 - PROGRESS: at 81.79% examples, 255080 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:07,609 : INFO : EPOCH 0 - PROGRESS: at 83.00% examples, 255196 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:08,669 : INFO : EPOCH 0 - PROGRESS: at 84.20% examples, 255326 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:09,704 : INFO : EPOCH 0 - PROGRESS: at 85.45% examples, 255672 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:10,727 : INFO : EPOCH 0 - PROGRESS: at 86.70% examples, 255931 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:11,767 : INFO : EPOCH 0 - PROGRESS: at 87.98% examples, 256260 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:12,770 : INFO : EPOCH 0 - PROGRESS: at 89.14% examples, 256320 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:13,797 : INFO : EPOCH 0 - PROGRESS: at 90.30% examples, 256295 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:14,816 : INFO : EPOCH 0 - PROGRESS: at 91.48% examples, 256426 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:15,849 : INFO : EPOCH 0 - PROGRESS: at 92.73% examples, 256621 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:16,851 : INFO : EPOCH 0 - PROGRESS: at 93.82% examples, 256569 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:17,889 : INFO : EPOCH 0 - PROGRESS: at 95.07% examples, 256736 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:18,907 : INFO : EPOCH 0 - PROGRESS: at 96.26% examples, 256853 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:19,909 : INFO : EPOCH 0 - PROGRESS: at 97.44% examples, 256893 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:20,927 : INFO : EPOCH 0 - PROGRESS: at 98.60% examples, 256902 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:21,928 : INFO : EPOCH 0 - PROGRESS: at 99.74% examples, 256944 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:22,140 : INFO : EPOCH 0: training on 23279529 raw words (22951015 effective words) took 89.3s, 256988 effective words/s\n2023-08-23 13:05:23,163 : INFO : EPOCH 1 - PROGRESS: at 1.21% examples, 256915 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:24,202 : INFO : EPOCH 1 - PROGRESS: at 2.37% examples, 258851 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:25,229 : INFO : EPOCH 1 - PROGRESS: at 3.57% examples, 263246 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:26,288 : INFO : EPOCH 1 - PROGRESS: at 4.78% examples, 261229 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:27,297 : INFO : EPOCH 1 - PROGRESS: at 6.02% examples, 264557 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:28,344 : INFO : EPOCH 1 - PROGRESS: at 7.22% examples, 263821 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:29,376 : INFO : EPOCH 1 - PROGRESS: at 8.41% examples, 264826 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:30,391 : INFO : EPOCH 1 - PROGRESS: at 9.57% examples, 265122 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:31,452 : INFO : EPOCH 1 - PROGRESS: at 10.80% examples, 265244 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:32,452 : INFO : EPOCH 1 - PROGRESS: at 11.98% examples, 265717 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:33,457 : INFO : EPOCH 1 - PROGRESS: at 13.12% examples, 265190 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:34,487 : INFO : EPOCH 1 - PROGRESS: at 14.28% examples, 264956 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:35,523 : INFO : EPOCH 1 - PROGRESS: at 15.52% examples, 266139 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:36,529 : INFO : EPOCH 1 - PROGRESS: at 16.64% examples, 265739 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:37,548 : INFO : EPOCH 1 - PROGRESS: at 17.82% examples, 265825 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:38,552 : INFO : EPOCH 1 - PROGRESS: at 19.00% examples, 266491 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:39,581 : INFO : EPOCH 1 - PROGRESS: at 20.18% examples, 266789 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:40,591 : INFO : EPOCH 1 - PROGRESS: at 21.33% examples, 266817 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:05:41,611 : INFO : EPOCH 1 - PROGRESS: at 22.52% examples, 266766 words/s, in_qsize 3, out_qsize 0\n```\n\n----------------------------------------\n\nTITLE: Importing LDASeqModel Module in Python\nDESCRIPTION: This code snippet demonstrates how to import the ldaseqmodel module from gensim.models. It is used to access the Dynamic Topic Modeling functionality in Python.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/models/ldaseqmodel.rst#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom gensim.models import ldaseqmodel\n```\n\n----------------------------------------\n\nTITLE: Downloading Dataset\nDESCRIPTION: Downloads the fake news dataset from Kaggle\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_network.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n!wget https://www.kaggle.com/mrisdal/fake-news/downloads/fake-news.zip/1 -O fake.news.zip\n```\n\n----------------------------------------\n\nTITLE: Installing and Starting Visdom Server\nDESCRIPTION: Commands to install Visdom using pip and start the Visdom server to access the visualization dashboard at http://localhost:8097.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Training_visualizations.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install visdom\n```\n\nLANGUAGE: bash\nCODE:\n```\npython -m visdom.server\n```\n\n----------------------------------------\n\nTITLE: Updated Usage of epochs Parameter in doc2vec's infer_vector Method in Python\nDESCRIPTION: Shows the correct usage of the 'epochs' parameter in doc2vec's infer_vector method, replacing the deprecated 'steps' parameter.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/CHANGELOG.md#2025-04-21_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nmodel.infer_vector(..., epochs=123)\n```\n\n----------------------------------------\n\nTITLE: Gensim Word2Vec Training Progress Output\nDESCRIPTION: Log output showing training progress of a Word2Vec model, including epoch number, completion percentage, processing speed (words/s), and queue sizes. Shows training on approximately 23M raw words with processing speeds around 550K words/second.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_24\n\nLANGUAGE: log\nCODE:\n```\n2023-08-23 13:00:12,434 : INFO : EPOCH 14 - PROGRESS: at 72.59% examples, 549275 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:00:13,450 : INFO : EPOCH 14 - PROGRESS: at 74.94% examples, 548733 words/s, in_qsize 4, out_qsize 0\n2023-08-23 13:00:23,915 : INFO : EPOCH 14: training on 23279529 raw words (22951015 effective words) took 41.9s, 548392 effective words/s\n```\n\n----------------------------------------\n\nTITLE: Sphinx Documentation Directive for Matrix Market Corpus\nDESCRIPTION: Sphinx documentation directive that specifies the module to document (gensim.corpora.mmcorpus) and documentation generation options including synopsis, members, inherited members, and inheritance information.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/corpora/mmcorpus.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: gensim.corpora.mmcorpus\n    :synopsis: Corpus in Matrix Market format\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Starting Pyro Name Server for Distributed Computing\nDESCRIPTION: This command starts the Pyro name server, which is required for distributed computing in Gensim.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/dist_lsi.rst#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m Pyro4.naming -n 0.0.0.0 &\n```\n\n----------------------------------------\n\nTITLE: Setting DTM Executable Path in Python\nDESCRIPTION: Configures the path to the DTM executable, either using an environment variable or direct path specification.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/dtm_example.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# path to dtm home folder\ndtm_home = os.environ.get('DTM_HOME', \"dtm-master\")\n# path to the binary. on my PC the executable file is dtm-master/bin/dtm\ndtm_path = os.path.join(dtm_home, 'bin', 'dtm') if dtm_home else None\n# you can also copy the path down directly. Change this variable to your DTM executable before running.\ndtm_path = \"/home/bhargav/dtm/main\"\n```\n\n----------------------------------------\n\nTITLE: Installing Pyro4 for Distributed Computing in Python\nDESCRIPTION: This command installs Pyro4, a library for remote procedure calls and distributed computing, which is required for Gensim's distributed features.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/distributed.md#2025-04-21_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsudo easy_install Pyro4\n```\n\n----------------------------------------\n\nTITLE: Gensim Training Progress Log Output\nDESCRIPTION: Log entries showing training progress for word2vec model training in Gensim. Shows metrics including epoch number, completion percentage, processing speed in words/second, and input/output queue sizes.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_34\n\nLANGUAGE: log\nCODE:\n```\n2023-08-23 13:12:55,212 : INFO : EPOCH 6 - PROGRESS: at 62.15% examples, 296613 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:12:56,224 : INFO : EPOCH 6 - PROGRESS: at 63.53% examples, 296792 words/s, in_qsize 3, out_qsize 0\n[...truncated for brevity...]\n2023-08-23 13:14:20,658 : INFO : EPOCH 7 - PROGRESS: at 72.16% examples, 294628 words/s, in_qsize 3, out_qsize 0\n```\n\n----------------------------------------\n\nTITLE: Importing Gensim Utils Module\nDESCRIPTION: This code snippet demonstrates how to import the utils module from Gensim. It uses the automodule directive to generate documentation for the module, including all members, inherited members, and undocumented members.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/utils.rst#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: gensim.utils\n    :synopsis: Various utility functions\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Extracting Tarball Archive in Python\nDESCRIPTION: Uses the tarfile module to extract the downloaded NIPS dataset archive to the /tmp directory.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport tarfile\n\nfilename = '/tmp/nips12raw_str602.tgz'\ntar = tarfile.open(filename, 'r:gz')\nfor item in tar:\n    tar.extract(item, path='/tmp')\n```\n\n----------------------------------------\n\nTITLE: Word2Vec Training Progress Log Output\nDESCRIPTION: Log entries showing training progress of a Word2Vec model in Gensim, tracking completion percentage and performance metrics like words processed per second and queue sizes. Shows progress through epochs 11-12 with timestamp, completion percentage, processing speed (~300k words/s), and queue states.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_38\n\nLANGUAGE: log\nCODE:\n```\n2023-08-23 13:18:39,022 : INFO : EPOCH 11 - PROGRESS: at 9.37% examples, 299865 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:18:40,023 : INFO : EPOCH 11 - PROGRESS: at 10.72% examples, 301215 words/s, in_qsize 3, out_qsize 0\n[...]\n2023-08-23 13:20:03,524 : INFO : EPOCH 12 - PROGRESS: at 21.82% examples, 307586 words/s, in_qsize 3, out_qsize 0\n```\n\n----------------------------------------\n\nTITLE: Logging Word2Vec Training Progress in Gensim\nDESCRIPTION: This snippet shows the log output format for Gensim Word2Vec training. It includes timestamp, epoch number, completion percentage, processing speed, and queue sizes. The log is updated frequently to show progress throughout the training process.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_39\n\nLANGUAGE: plaintext\nCODE:\n```\n2023-08-23 13:20:04,552 : INFO : EPOCH 12 - PROGRESS: at 23.06% examples, 306138 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:20:05,593 : INFO : EPOCH 12 - PROGRESS: at 24.43% examples, 306129 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:20:06,609 : INFO : EPOCH 12 - PROGRESS: at 25.74% examples, 305518 words/s, in_qsize 3, out_qsize 0\n```\n\n----------------------------------------\n\nTITLE: Importing and Using Gensim KeyedVectors Module\nDESCRIPTION: Shows the module import path for the keyedvectors component in Gensim. This module provides functionality for storing and querying word vectors with special methods for interaction.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/models/keyedvectors.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: gensim.models.keyedvectors\n    :synopsis: Store and query word vectors\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n    :special-members: __getitem__, __setitem__, __contains__\n```\n\n----------------------------------------\n\nTITLE: Setting up Matplotlib for inline plotting in Python\nDESCRIPTION: Configures Matplotlib to display plots inline, typically used in Jupyter notebooks.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_word2vec.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%matplotlib inline\n```\n\n----------------------------------------\n\nTITLE: Sphinx Module Documentation Directive for Topic Coherence Segmentation\nDESCRIPTION: ReStructuredText directive for documenting the topic_coherence.segmentation module in Gensim. Includes configuration for showing all members, inherited members, undocumented members and inheritance relationships.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/topic_coherence/segmentation.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: gensim.topic_coherence.segmentation\n    :synopsis: Segmentation module\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx AutoModule Documentation for Gensim Models\nDESCRIPTION: Sphinx documentation directive that automatically generates API documentation for the gensim.models package. Includes all members and inherited members, with a synopsis describing the package purpose.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/models/models.rst#2025-04-21_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. automodule:: gensim.models\n    :synopsis: Package for transformation models\n    :members:\n    :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Documenting Phrases Module with reStructuredText\nDESCRIPTION: Sphinx documentation configuration for the gensim.models.phrases module. This directive automatically generates documentation from the module's docstrings and includes all members, inherited members, and undocumented members.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/models/phrases.rst#2025-04-21_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. automodule:: gensim.models.phrases\n    :synopsis: Phrase (collocation) detection\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Importing GloVe to Word2Vec Conversion Module in Python\nDESCRIPTION: This code snippet demonstrates how to import the glove2word2vec module from the gensim.scripts package. The module provides functionality to convert GloVe format embeddings to Word2Vec format.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/scripts/glove2word2vec.rst#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom gensim.scripts import glove2word2vec\n```\n\n----------------------------------------\n\nTITLE: Corpus Statistics\nDESCRIPTION: Calculates and displays corpus statistics including word and sentence counts\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/wikinews-bigram-en.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# how many sentences and words ?\nwords_count = sum(len(s) for s in corpus)\nprint(\"Corpus has %d words in %d sentences\" % (words_count, len(corpus)))\n```\n\n----------------------------------------\n\nTITLE: Initializing Logging for LDA Model in Python\nDESCRIPTION: Sets up basic logging configuration for the LDA model tutorial. This snippet configures the logging format and level to INFO.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_lda.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n```\n\n----------------------------------------\n\nTITLE: Converting Matrix Market to LDA-C Format (Python)\nDESCRIPTION: Shows how to load a Matrix Market document stream and save it in Blei's LDA-C format using Gensim's BleiCorpus class.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_corpora_and_vector_spaces.ipynb#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ncorpora.BleiCorpus.serialize('/tmp/corpus.lda-c', corpus)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Wikipedia Data with Gensim script\nDESCRIPTION: Command to run the Gensim script that processes Wikipedia markup into plain text and TF-IDF vectors. The script makes two passes over the compressed wiki dump and requires about 35GB of free disk space for the output vectors.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/wiki.rst#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m gensim.scripts.make_wiki\n```\n\n----------------------------------------\n\nTITLE: Gensim Training Progress Log Output\nDESCRIPTION: Log entries showing training progress for word embeddings model using Gensim, including metrics like epoch number, completion percentage, processing speed, and queue sizes.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_33\n\nLANGUAGE: log\nCODE:\n```\n2023-08-23 13:11:28,655 : INFO : EPOCH 5 - PROGRESS: at 50.29% examples, 290002 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:11:29,679 : INFO : EPOCH 5 - PROGRESS: at 51.60% examples, 290287 words/s, in_qsize 3, out_qsize 0\n```\n\n----------------------------------------\n\nTITLE: Running Distributed LSA on a Small Corpus in Python\nDESCRIPTION: This code demonstrates how to perform distributed LSA on a small corpus of nine documents using Gensim.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/dist_lsi.rst#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from gensim import corpora, models\n>>> import logging\n>>>\n>>> logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n>>>\n>>> corpus = corpora.MmCorpus('/tmp/deerwester.mm')  # load a corpus of nine documents, from the Tutorials\n>>> id2word = corpora.Dictionary.load('/tmp/deerwester.dict')\n>>>\n>>> # run distributed LSA on nine documents\n>>> lsi = models.LsiModel(corpus, id2word=id2word, num_topics=200, chunksize=1, distributed=True)\n>>>\n>>> lsi.print_topics(num_topics=2, num_words=5)\ntopic #0(3.341): 0.644*\"system\" + 0.404*\"user\" + 0.301*\"eps\" + 0.265*\"time\" + 0.265*\"response\"\ntopic #1(2.542): 0.623*\"graph\" + 0.490*\"trees\" + 0.451*\"minors\" + 0.274*\"survey\" + -0.167*\"system\"\n```\n\n----------------------------------------\n\nTITLE: Logging Word2Vec Epoch Completion in Gensim\nDESCRIPTION: This snippet shows the log output format for Word2Vec epoch completion in Gensim. It includes the epoch number, total raw words, effective words, training time, and effective words processed per second.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_42\n\nLANGUAGE: plaintext\nCODE:\n```\n2023-08-23 13:25:55,388 : INFO : EPOCH 16: training on 23279529 raw words (22951015 effective words) took 72.6s, 316217 effective words/s\n```\n\n----------------------------------------\n\nTITLE: Logging Word2Vec Training Progress in Gensim\nDESCRIPTION: This snippet shows the format of progress log messages during Word2Vec model training using Gensim. It includes timestamp, epoch number, percentage completion, processing speed, and queue sizes.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_31\n\nLANGUAGE: plaintext\nCODE:\n```\n2023-08-23 13:08:35,761 : INFO : EPOCH 3 - PROGRESS: at 35.80% examples, 289361 words/s, in_qsize 3, out_qsize 0\n```\n\n----------------------------------------\n\nTITLE: Downloading Wikipedia Dump Files\nDESCRIPTION: Bash commands to download Wikipedia dump files from different years for comparison training.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/online_w2v_tutorial.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nwget https://dumps.wikimedia.org/archive/2010/2010-11/enwiki/20101011/enwiki-20101011-pages-articles.xml.bz2\nwget https://dumps.wikimedia.org/enwiki/20160820/enwiki-20160820-pages-articles.xml.bz2\n```\n\n----------------------------------------\n\nTITLE: Initializing Matplotlib Inline Display\nDESCRIPTION: Enables inline display of matplotlib plots in Jupyter notebook.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_ensemblelda.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%matplotlib inline\n```\n\n----------------------------------------\n\nTITLE: Deprecated Usage of steps Parameter in doc2vec's infer_vector Method in Python\nDESCRIPTION: Illustrates the deprecated use of the 'steps' parameter in doc2vec's infer_vector method, which should be replaced with 'epochs'.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/CHANGELOG.md#2025-04-21_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nmodel.infer_vector(..., steps=123)\n```\n\n----------------------------------------\n\nTITLE: Defining Sphinx Documentation for Ensemble LDA Module in RST\nDESCRIPTION: This RST (reStructuredText) snippet configures how Sphinx should generate documentation for the ensemblelda module in Gensim. It specifies that all module members should be documented, including inherited and undocumented members, with inheritance relationships displayed.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/models/ensemblelda.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: gensim.models.ensemblelda\n    :synopsis: Ensemble Latent Dirichlet Allocation\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Module Documentation Directive in RST\nDESCRIPTION: ReStructuredText directive specifying the documentation configuration for the gensim.topic_coherence.text_analysis module, including synopsis, member visibility, and inheritance settings.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/topic_coherence/text_analysis.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: gensim.topic_coherence.text_analysis\n    :synopsis: Analyzing the texts of a corpus to accumulate statistical information about word occurrences\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n    :special-members: __getitem__\n```\n\n----------------------------------------\n\nTITLE: Importing make_wikicorpus Module in Python\nDESCRIPTION: This code snippet demonstrates how to import the make_wikicorpus module from the gensim.scripts package. The module provides functionality to convert Wikipedia articles to vector representations.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/scripts/make_wikicorpus.rst#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom gensim.scripts import make_wikicorpus\n```\n\n----------------------------------------\n\nTITLE: Validating Gensim Code Changes\nDESCRIPTION: Commands for checking code quality, building documentation, running tests, and committing changes to a Gensim pull request.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/CONTRIBUTING.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nflake8 --ignore E12,W503 --max-line-length 120 --show-source gensim\n```\n\nLANGUAGE: bash\nCODE:\n```\nmake -C docs/src html\n```\n\nLANGUAGE: bash\nCODE:\n```\npytest -v gensim/test\n```\n\nLANGUAGE: bash\nCODE:\n```\ngit add ... ; git commit -m \"my commit message\"; git push origin my-feature\n```\n\n----------------------------------------\n\nTITLE: Displaying an Image Using Matplotlib (Python)\nDESCRIPTION: Loads and displays an image file using Matplotlib's imread and imshow functions, with the axis turned off.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_corpora_and_vector_spaces.ipynb#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimg = mpimg.imread('run_corpora_and_vector_spaces.png')\nimgplot = plt.imshow(img)\n_ = plt.axis('off')\n```\n\n----------------------------------------\n\nTITLE: Printing a Corpus (Python)\nDESCRIPTION: Demonstrates two ways to print the contents of a corpus: loading it entirely into memory and printing one document at a time using the streaming interface.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_corpora_and_vector_spaces.ipynb#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nprint(list(corpus))\n```\n\nLANGUAGE: python\nCODE:\n```\nfor doc in corpus:\n    print(doc)\n```\n\n----------------------------------------\n\nTITLE: Importing AuthorTopic Model from Gensim in Python\nDESCRIPTION: This snippet shows how to import the AuthorTopic model from Gensim's scikit-learn API. It's a prerequisite for using the AuthorTopic model in a scikit-learn pipeline.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/sklearn_api.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.sklearn_api import AuthorTopicTransformer\n```\n\n----------------------------------------\n\nTITLE: Word Frequency Data Sample\nDESCRIPTION: Tab-delimited data showing word tokens and their frequency counts. Each line contains a word and a number indicating how many times it appears in the corpus. The data is sorted alphabetically.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/head500.noblanks.cor_wordids.txt#2025-04-21_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n2667\ta\t4\n5026\ta-b\t1\n16598\ta-bomb\t1\n22319\ta-class\t1\n```\n\n----------------------------------------\n\nTITLE: Training C++ Models for WordNet Link Prediction in Python\nDESCRIPTION: Training C++ implementation models for the link prediction task with both default and non-default parameters. The code loops through different parameter configurations and model sizes, storing the resulting model files in a dictionary.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Evaluation.ipynb#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nlp_model_files['c++'] = {}\n# Train c++ models with default params\nmodel_name, files = train_model_with_params(default_params, wordnet_train_file, model_sizes, 'cpp_lp_model', 'c++')\nlp_model_files['c++'][model_name] = {}\nfor dim, filepath in files.items():\n    lp_model_files['c++'][model_name][dim] = filepath\n# Train c++ models with non-default params\nfor param, values in non_default_params.items():\n    params = default_params.copy()\n    for value in values:\n        params[param] = value\n        model_name, files = train_model_with_params(params, wordnet_train_file, model_sizes, 'cpp_lp_model', 'c++')\n        lp_model_files['c++'][model_name] = {}\n        for dim, filepath in files.items():\n            lp_model_files['c++'][model_name][dim] = filepath\n```\n\n----------------------------------------\n\nTITLE: Setting Up Python Environment and Dependencies\nDESCRIPTION: Installs required Python libraries and downloads WordNet data. It also sets up logging and downloads NLTK's WordNet corpus.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Evaluation.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n! pip install click>=6.7 nltk>=3.2.5 prettytable>=0.7.2 pygtrie>=2.2\n```\n\nLANGUAGE: python\nCODE:\n```\nimport csv\nfrom collections import OrderedDict\nfrom IPython.display import display, HTML\nimport logging\nimport os\nimport pickle\nimport random\nimport re\n\nimport click\nfrom gensim.models.poincare import PoincareModel, PoincareRelations, \\\n    ReconstructionEvaluation, LinkPredictionEvaluation, \\\n    LexicalEntailmentEvaluation, PoincareKeyedVectors\nfrom gensim.utils import check_output\nimport nltk\nfrom prettytable import PrettyTable\nfrom smart_open import smart_open\n\nlogging.basicConfig(level=logging.INFO)\nnltk.download('wordnet')\n```\n\n----------------------------------------\n\nTITLE: Importing Gensim Downloader API in Python\nDESCRIPTION: Imports the Gensim downloader API module for accessing pre-trained models and corpora.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_downloader_api.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport gensim.downloader as api\n```\n\n----------------------------------------\n\nTITLE: Installing Pyro4 for Distributed Computing in Python\nDESCRIPTION: This command installs Pyro4, a library required for distributed computing features in Gensim. Pyro4 is used for low-level socket communication and remote procedure calls between nodes in a distributed setup.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/distributed.rst#2025-04-21_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\npip install Pyro4\n```\n\n----------------------------------------\n\nTITLE: Gensim Word2Vec Training Progress Log Output\nDESCRIPTION: Progress log showing Word2Vec model training metrics including epoch number, completion percentage, processing speed (words/second), and queue sizes. The log demonstrates consistent processing of ~285-290K words/second with stable input/output queue sizes.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_32\n\nLANGUAGE: log\nCODE:\n```\n2023-08-23 13:10:02,219 : INFO : EPOCH 4 - PROGRESS: at 41.58% examples, 283845 words/s, in_qsize 3, out_qsize 0\n2023-08-23 13:10:03,219 : INFO : EPOCH 4 - PROGRESS: at 42.86% examples, 284053 words/s, in_qsize 4, out_qsize 0\n2023-08-23 13:10:48,756 : INFO : EPOCH 4: training on 23279529 raw words (22951015 effective words) took 80.3s, 285888 effective words/s\n2023-08-23 13:10:49,759 : INFO : EPOCH 5 - PROGRESS: at 1.33% examples, 290411 words/s, in_qsize 3, out_qsize 0\n```\n\n----------------------------------------\n\nTITLE: Using KeyedVectors most_similar with List Arguments in Python\nDESCRIPTION: Shows the previous method of passing lists of strings to the positive and negative parameters of the most_similar method in KeyedVectors, which is still supported.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/CHANGELOG.md#2025-04-21_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nmodel.most_similar(positive=['war'], negative=['peace'])\n```\n\n----------------------------------------\n\nTITLE: RST Module Documentation Block\nDESCRIPTION: ReStructuredText documentation directive that sets up automated documentation generation for the Poincare embeddings module, including method signatures, inheritance info, and special methods.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/models/poincare.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: gensim.models.poincare\n    :synopsis: Train and use Poincare embeddings\n    :members:\n    :inherited-members:\n    :special-members: __iter__, __getitem__, __contains__\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Importing gensim.corpora.dictionary Module\nDESCRIPTION: This snippet demonstrates how to import the gensim.corpora.dictionary module. The module is used for constructing word<->id mappings in the gensim library.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/corpora/dictionary.rst#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: gensim.corpora.dictionary\n```\n\n----------------------------------------\n\nTITLE: Printing Corpus Statistics in Python\nDESCRIPTION: Displays the number of unique tokens in the dictionary and total number of documents in the corpus.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_lda.rst#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprint('Number of unique tokens: %d' % len(dictionary))\nprint('Number of documents: %d' % len(corpus))\n```\n\n----------------------------------------\n\nTITLE: Displaying Image with Matplotlib\nDESCRIPTION: Loads and displays an image using matplotlib library with axis turned off. Used to show a logo at the end of the tutorial.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_fasttext.rst#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimg = mpimg.imread('fasttext-logo-color-web.png')\nimgplot = plt.imshow(img)\n_ = plt.axis('off')\n```\n\n----------------------------------------\n\nTITLE: Retrieving Detailed Information About a Model\nDESCRIPTION: Demonstrates how to get detailed information about a specific model or corpus (in this case 'fake-news') from the API and display it as formatted JSON.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/downloader_api_tutorial.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfake_news_info = api.info('fake-news')\nprint(json.dumps(fake_news_info, indent=4))\n```\n\n----------------------------------------\n\nTITLE: Vector Representation of the Word 'nine' in a Word Embedding\nDESCRIPTION: This is a 300-dimensional vector representation of the word 'nine' in a word embedding space. Each floating-point value represents the word's position along a different dimension in the embedding space, capturing semantic relationships and meaning.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/EN.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_8\n\nLANGUAGE: text\nCODE:\n```\nnine 4.910999909043312073e-02 1.079630032181739807e-01 -1.467400044202804565e-01 -4.344100132584571838e-02 -2.438499964773654938e-02 1.322560012340545654e-01 1.113810017704963684e-01 -6.493800133466720581e-02 9.406699985265731812e-02 -1.189590021967887878e-01 2.624900080263614655e-02 3.120670020580291748e-01 -1.914599984884262085e-01 8.548100292682647705e-02 1.205210015177726746e-01 -4.193900153040885925e-02 -4.992200061678886414e-02 1.426469981670379639e-01 1.509110033512115479e-01 -1.519519984722137451e-01 1.369199994951486588e-02 -3.724399954080581665e-02 -3.467699885368347168e-02 3.103099949657917023e-02 1.486230045557022095e-01 1.327639967203140259e-01 -1.981890052556991577e-01 3.955999854952096939e-03 4.902800172567367554e-02 2.063809931278228760e-01 -1.815630048513412476e-01 5.328999832272529602e-02 1.352470070123672485e-01 -2.501299977302551270e-02 -3.469699993729591370e-02 2.584400027990341187e-02 -4.171599820256233215e-02 1.137029975652694702e-01 -3.962999954819679260e-02 8.397900313138961792e-02 -1.389269977807998657e-01 -1.304930001497268677e-01 -2.426230013370513916e-01 -6.070499867200851440e-02 2.345860004425048828e-01 -4.836000036448240280e-03 3.456580042839050293e-01 8.261899650096893311e-02 9.904000163078308105e-02 -2.454999983310699463e-01 1.697099953889846802e-01 -3.648619949817657471e-01 -1.250530034303665161e-01 -4.283300042152404785e-02 5.121599882841110229e-02 2.311940044164657593e-01 2.181989997625350952e-01 -7.077100127935409546e-02 -2.208600006997585297e-02 5.917200073599815369e-02 -9.488800168037414551e-02 -2.125000022351741791e-02 1.211439967155456543e-01 -1.185099966824054718e-02 9.480000007897615433e-04 7.056000083684921265e-02 1.037980020046234131e-01 3.253100067377090454e-02 1.359229981899261475e-01 -1.120690032839775085e-01 -1.584780067205429077e-01 1.676920056343078613e-01 -8.110599964857101440e-02 -3.274599835276603699e-02 2.091300040483474731e-01 -5.480099841952323914e-02 -6.653899699449539185e-02 -7.721199840307235718e-02 -1.720750033855438232e-01 4.598740041255950928e-01 -1.319520026445388794e-01 5.455600097775459290e-02 -1.088839992880821228e-01 1.960770040750503540e-01 -6.461999844759702682e-03 3.209599852561950684e-02 -2.853000070899724960e-03 2.004819959402084351e-01 -7.619400322437286377e-02 -9.330400079488754272e-02 2.149900048971176147e-02 1.563789993524551392e-01 -1.656160056591033936e-01 -1.781730055809020996e-01 -1.604589968919754028e-01 -1.511169970035552979e-01 2.415999956429004669e-02 3.562400117516517639e-02 -6.539700180292129517e-02 -8.148600161075592041e-02 -2.906680107116699219e-01 -2.409799955785274506e-02 -1.648470014333724976e-01 1.788000017404556274e-02 -1.093060001730918884e-01 -3.739099949598312378e-02 2.141859978437423706e-01 5.018400028347969055e-02 1.538899987936019897e-01 -1.234930008649826050e-01 -1.216970011591911316e-01 -4.647000133991241455e-02 -1.428600028157234192e-02 -1.061450019478797913e-01 8.762399852275848389e-02 5.068000033497810364e-02 -4.710999876260757446e-02 -1.049999967217445374e-01 8.241499960422515869e-02 -1.928289979696273804e-01 9.947399795055389404e-02 5.070399865508079529e-02 -1.279999967664480209e-02 1.343999989330768585e-03 9.689100086688995361e-02 -2.476390004158020020e-01 -8.177100121974945068e-02 -1.848099939525127411e-02 1.297599971294403076e-01 1.024279966950416565e-01 1.886509954929351807e-01 1.073020026087760925e-01 1.552840024232864380e-01 -7.197300344705581665e-02 -9.171500056982040405e-02 2.420799992978572845e-02 -1.124180033802986145e-01 -7.965499907732009888e-02 2.197200059890747070e-02 -1.358170062303543091e-01 -2.158280014991760254e-01 3.709800168871879578e-02 -1.499940007925033569e-01 -7.195399701595306396e-02 -1.160899996757507324e-01 -6.869000196456909180e-02 -1.745110005140304565e-01 4.553399980068206787e-02 1.825969964265823364e-01 1.435189992189407349e-01 -7.072400301694869995e-02 -1.027429997920989990e-01 -1.690730005502700806e-01 -5.280000157654285431e-03 2.617819905281066895e-01 1.639400050044059753e-02 7.142200320959091187e-02 -2.861349880695343018e-01 -6.588900089263916016e-02 1.026199981570243835e-01 2.211800031363964081e-02 -1.800570040941238403e-01 9.891200065612792969e-02 -1.487929970026016235e-01 2.713900059461593628e-02 1.047519966959953308e-01 1.403139978647232056e-01 1.524560004472732544e-01 8.883000165224075317e-02 -2.429980039596557617e-01 -2.221670001745223999e-01 -1.533930003643035889e-01 -2.256280034780502319e-01 -7.271400094032287598e-02 5.328400060534477234e-02 -3.025499917566776276e-02 -6.819000001996755600e-03 2.183780074119567871e-01 1.215500012040138245e-01 -2.595999976620078087e-03 -2.846280038356781006e-01 1.431269943714141846e-01 -4.234600067138671875e-02 3.830200061202049255e-02 -3.079500049352645874e-02 2.135969996452331543e-01 -2.165099978446960449e-02 -2.019000053405761719e-02 6.662300229072570801e-02 -5.510999821126461029e-03 1.715599931776523590e-02 -9.716799855232238770e-02 2.124090045690536499e-01 -3.469099998474121094e-01 -5.182199925184249878e-02 9.992899745702743530e-02 8.649899810552597046e-02 3.045099973678588867e-02 1.764840036630630493e-01 7.986199855804443359e-02 3.811400011181831360e-02 1.161120012402534485e-01 -5.367999896407127380e-02 1.171109974384307861e-01 -2.626200020313262939e-02 -1.285970062017440796e-01 1.871760040521621704e-01 -1.046890020370483398e-01 -1.791370064020156860e-01 1.059660017490386963e-01 5.279099941253662109e-02 6.046500056982040405e-02 -5.880000069737434387e-03 -7.585400342941284180e-02 1.428990066051483154e-01 -2.226420044898986816e-01 9.709899872541427612e-02 -3.899899870157241821e-02 -8.442900329828262329e-02 -1.420360058546066284e-01 5.527300015091896057e-02 8.310399949550628662e-02 2.558259963989257812e-01 2.380700036883354187e-02 6.530000246129930019e-04 3.455299884080886841e-02 1.325400024652481079e-01 -1.021590009331703186e-01 5.385899916291236877e-02 -1.425870060920715332e-01 5.514999851584434509e-02 1.117210015654563904e-01 4.032399877905845642e-02 1.091820001602172852e-01 -1.541209965944290161e-01 -1.108739972114562988e-01 -1.203399989753961563e-02 1.961430013179779053e-01 -3.376299887895584106e-02 4.687799885869026184e-02 -9.430500119924545288e-02 -2.736900001764297485e-02 5.770000279881060123e-04 -1.767449975013732910e-01 -2.541499957442283630e-02 1.366640031337738037e-01 6.155899912118911743e-02 1.531590074300765991e-01 -1.023809984326362610e-01 2.583600021898746490e-02 4.193399846553802490e-02 -8.173099905252456665e-02 3.668999997898936272e-03 -4.293400049209594727e-02 -2.289099991321563721e-02 -9.109199792146682739e-02 -2.114599943161010742e-01 -1.334940046072006226e-01 1.370189934968948364e-01 3.276669979095458984e-01 6.755500286817550659e-02 3.607299923896789551e-02 -2.116699963808059692e-01 7.105399668216705322e-02 9.283500164747238159e-02 3.245449960231781006e-01 -7.055299729108810425e-02 -8.097399771213531494e-02 3.676199913024902344e-02 3.110099956393241882e-02 -1.206580027937889099e-01 7.032600045204162598e-02 -2.011609971523284912e-01 -2.307509928941726685e-01 -1.003649979829788208e-01 -1.428160071372985840e-01 3.240900114178657532e-02 1.794700026512145996e-01 5.094299837946891785e-02 -4.918900132179260254e-02 -1.243259981274604797e-01 1.440310031175613403e-01 -4.863800108432769775e-02 8.394999802112579346e-02 9.850999712944030762e-02 -1.149820014834403992e-01 -2.465150058269500732e-01 -9.680499881505966187e-02 -4.457499831914901733e-02 -3.197000175714492798e-02 -4.719600081443786621e-02 5.587799847126007080e-02 1.940940022468566895e-01 -4.862099885940551758e-02 -5.144799873232841492e-02 1.254439949989318848e-01 -8.347100019454956055e-02 7.282400131225585938e-02 -1.346100028604269028e-02 2.750200033187866211e-02\n```\n\n----------------------------------------\n\nTITLE: Installing Gensim via pip\nDESCRIPTION: Command to install the latest version of Gensim using pip package manager.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade gensim\n```\n\n----------------------------------------\n\nTITLE: Word Frequency Data Format\nDESCRIPTION: Tab-delimited data showing word tokens and their frequency counts in a corpus. Format is: ID<tab>word<tab>frequency\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/head500.noblanks.cor_wordids.txt#2025-04-21_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n25705\tsalti\t1\n3495\tsaltwat\t2\n23052\tsalut\t2\n15493\tsalvador\t5\n```\n\n----------------------------------------\n\nTITLE: Importing Topic Coherence Probability Estimation Module in Python\nDESCRIPTION: This code snippet shows how to import the probability estimation module from the gensim.topic_coherence package. This module is used for estimating probabilities in topic coherence calculations.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/topic_coherence/probability_estimation.rst#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom gensim.topic_coherence import probability_estimation\n```\n\n----------------------------------------\n\nTITLE: Displaying Sample Document Structure\nDESCRIPTION: Prints an example document to demonstrate the structure of processed movie reviews.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprint(alldocs[27])\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx AutoModule Documentation for Gensim Downloader\nDESCRIPTION: reStructuredText configuration block that sets up automatic documentation generation for the gensim.downloader module. It includes settings for synopsis, members, inherited members, undocumented members, and inheritance diagrams.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/downloader.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: gensim.downloader\n    :synopsis: Downloader API for gensim\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Setting up Logging Configuration\nDESCRIPTION: Configures the logging module to display formatted log messages with timestamps and levels for debugging purposes.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_prediction_tutorial.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nlogging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG, datefmt='%I:%M:%S')\n```\n\n----------------------------------------\n\nTITLE: Initializing Logging for Gensim in Python\nDESCRIPTION: Sets up basic logging configuration for the Gensim library to output informational messages.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n```\n\n----------------------------------------\n\nTITLE: Word Embedding Vector for 'six'\nDESCRIPTION: A 300-dimensional vector representation of the word 'six' from a word embedding model like word2vec or GloVe. Each floating-point number represents a coordinate in the embedding space that captures semantic properties of the word.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/EN.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\nsix 7.617100328207015991e-02 3.068000078201293945e-03 -1.196120008826255798e-01 -1.046240031719207764e-01 2.566800080239772797e-02 1.121750026941299438e-01 1.216220036149024963e-01 -9.985899925231933594e-02 9.444499760866165161e-02 -4.521400108933448792e-02 1.011580005288124084e-01 1.080090031027793884e-01 -1.734600029885768890e-02 3.783600032329559326e-02 7.117299735546112061e-02 -9.675300121307373047e-02 7.677999883890151978e-02 1.485790014266967773e-01 1.757310032844543457e-01 -1.105339974164962769e-01 -1.195750012993812561e-01 3.331299871206283569e-02 -1.462399959564208984e-02 1.199600007385015488e-02 4.707999899983406067e-02 1.552270054817199707e-01 -1.630980074405670166e-01 1.034300029277801514e-02 4.286799952387809753e-02 2.423159927129745483e-01 -1.435109972953796387e-01 9.519999730400741100e-04 1.441790014505386353e-01 -6.107499822974205017e-02 4.014499858021736145e-02 1.510199997574090958e-02 -7.239899784326553345e-02 1.378159970045089722e-01 5.285799875855445862e-02 -7.340999785810709000e-03 -9.909799695014953613e-02 -1.171339973807334900e-01 -2.190330028533935547e-01 -2.441800013184547424e-02 2.109149992465972900e-01 7.780700176954269409e-02 2.437770068645477295e-01 5.901699885725975037e-02 5.755100026726722717e-02 -2.822839915752410889e-01 1.574369966983795166e-01 -3.319050073623657227e-01 -1.734309941530227661e-01 -7.660599797964096069e-02 9.215100109577178955e-02 2.188899964094161987e-01 2.349810004234313965e-01 -5.618700012564659119e-02 9.790000040084123611e-04 6.277400255203247070e-02 -5.353099852800369263e-02 -7.666300237178802490e-02 1.942960023880004883e-01 -4.515900090336799622e-02 -7.864200323820114136e-02 1.863510012626647949e-01 3.558500111103057861e-02 7.285299897193908691e-02 7.783100008964538574e-02 -1.705040037631988525e-01 -5.835999920964241028e-02 1.738349944353103638e-01 -2.610000083222985268e-03 -8.147799968719482422e-02 2.881050109863281250e-01 -4.947499930858612061e-02 7.910000160336494446e-03 -4.918000195175409317e-03 -1.226880028843879700e-01 3.624320030212402344e-01 -1.985580027103424072e-01 4.241700097918510437e-02 -2.429929971694946289e-01 1.918019950389862061e-01 -1.857100054621696472e-02 4.690200090408325195e-02 -1.084420010447502136e-01 2.304890006780624390e-01 -2.945899963378906250e-02 -1.286489963531494141e-01 -1.133999973535537720e-01 8.333099633455276489e-02 -5.708999931812286377e-02 -1.991270035505294800e-01 -1.296849995851516724e-01 -9.475199878215789795e-02 1.771200075745582581e-02 8.987700194120407104e-02 1.306600030511617661e-02 -2.492900006473064423e-02 -2.824470102787017822e-01 -8.225099742412567139e-02 -6.443999707698822021e-02 4.333000164479017258e-03 -1.173620000481605530e-01 2.051199972629547119e-02 1.335690021514892578e-01 2.930999966338276863e-03 2.347719967365264893e-01 -1.325239986181259155e-01 -1.090079993009567261e-01 -1.877900026738643646e-02 1.622699946165084839e-02 -7.559700310230255127e-02 -3.569699823856353760e-02 1.648700051009654999e-02 -7.051999866962432861e-02 1.384500041604042053e-02 -1.749699935317039490e-02 -1.326189935207366943e-01 1.438740044832229614e-01 1.458339989185333252e-01 -1.777400076389312744e-02 7.828199863433837891e-02 7.642800360918045044e-02 -1.859389990568161011e-01 -3.448000177741050720e-02 -2.086500078439712524e-02 6.470499932765960693e-02 1.891610026359558105e-01 1.587049961090087891e-01 1.183430030941963196e-01 1.137169972062110901e-01 -1.088780015707015991e-01 -8.449199795722961426e-02 6.804899871349334717e-02 -3.091800026595592499e-02 -9.132300317287445068e-02 7.443899661302566528e-02 -1.241289973258972168e-01 -1.673090010881423950e-01 -5.127599835395812988e-02 -1.002549976110458374e-01 -8.793299645185470581e-02 -2.043890058994293213e-01 -1.156629994511604309e-01 -2.126709967851638794e-01 8.944900333881378174e-02 2.165279984474182129e-01 1.423050016164779663e-01 -6.449099630117416382e-02 -1.157919988036155701e-01 -8.976300060749053955e-02 -9.773699939250946045e-02 2.576479911804199219e-01 7.020899653434753418e-02 -2.156200073659420013e-02 -1.858150064945220947e-01 -5.261899903416633606e-02 1.565099954605102539e-01 1.896100044250488281e-01 -1.282230019569396973e-01 6.666900217533111572e-02 -1.837469935417175293e-01 8.544799685478210449e-02 5.333599820733070374e-02 1.048960015177726746e-01 1.057540029287338257e-01 1.976799964904785156e-02 -1.424939930438995361e-01 -6.422600150108337402e-02 -1.534689962863922119e-01 -2.010760009288787842e-01 -1.244029998779296875e-01 1.182020008563995361e-01 -1.018289998173713684e-01 3.284300118684768677e-02 1.931239962577819824e-01 1.839209944009780884e-01 -2.115000039339065552e-01 -1.422110050916671753e-01 1.396169960498809814e-01 3.307300060987472534e-02 -4.950100183486938477e-02 -2.523400075733661652e-02 1.619659960269927979e-01 -4.379799962043762207e-02 -9.441299736499786377e-02 2.155000111088156700e-03 1.531029939651489258e-01 9.032700210809707642e-02 -9.751199930906295776e-02 1.514379978179931641e-01 -3.531410098075866699e-01 4.548000171780586243e-03 1.604500040411949158e-02 3.451000154018402100e-02 -7.749000098556280136e-03 7.244700193405151367e-02 1.706299930810928345e-02 3.466000035405158997e-02 7.240899652242660522e-02 -1.191430017352104187e-01 1.845339983701705933e-01 -2.315700054168701172e-02 -1.336150020360946655e-01 2.188369929790496826e-01 -3.886000020429491997e-03 -6.196000054478645325e-02 9.144999831914901733e-02 2.087499946355819702e-02 -5.662000179290771484e-02 1.583399996161460876e-02 -8.002699911594390869e-02 1.360470056533813477e-01 -1.545899957418441772e-01 4.329999908804893494e-03 -8.431000262498855591e-02 1.483200024813413620e-02 -2.190899997949600220e-01 8.171500265598297119e-02 1.357640027999877930e-01 3.505519926548004150e-01 5.774600058794021606e-02 -1.424400042742490768e-02 1.791000016964972019e-03 6.877999752759933472e-02 -7.660599797964096069e-02 2.207989990711212158e-01 -9.704499691724777222e-02 6.208299845457077026e-02 5.445399880409240723e-02 6.756199896335601807e-02 8.268799632787704468e-02 -1.189490035176277161e-01 -1.050790026783943176e-01 -1.150230020284652710e-01 1.709450036287307739e-01 -1.834899932146072388e-02 -1.274800021201372147e-02 -9.471900016069412231e-02 -3.819499909877777100e-02 -3.663900122046470642e-02 -2.640100121498107910e-01 -1.501599978655576706e-02 8.532299846410751343e-02 8.708599954843521118e-02 1.250659972429275513e-01 -8.299600332975387573e-02 -2.838400006294250488e-02 9.713999927043914795e-02 -3.079300001263618469e-02 7.635799795389175415e-02 -1.284459978342056274e-01 5.371100082993507385e-02 -4.865900054574012756e-02 -2.333710044622421265e-01 -1.070090010762214661e-01 1.816029995679855347e-01 2.822189927101135254e-01 6.971299648284912109e-02 5.248500034213066101e-02 -9.345199912786483765e-02 -4.956000018864870071e-03 1.213449984788894653e-01 2.431180030107498169e-01 -7.568600028753280640e-02 -1.334840059280395508e-01 1.096389964222908020e-01 8.952000178396701813e-03 -9.354700148105621338e-02 -9.380199760198593140e-02 -1.309570074081420898e-01 -2.360219955444335938e-01 -8.184999972581863403e-02 -1.639070063829421997e-01 3.979400172829627991e-02 8.235800266265869141e-02 4.944499954581260681e-02 -2.249400019645690918e-01 -9.783300012350082397e-02 6.775800138711929321e-02 1.040470004081726074e-01 3.780300170183181763e-02 2.122689932584762573e-01 -1.883150041103363037e-01 -1.220450028777122498e-01 -8.399400115013122559e-02 -2.124500088393688202e-02 1.843300089240074158e-02 3.635599836707115173e-02 9.380699694156646729e-02 1.395390033721923828e-01 -1.239890009164810181e-01 4.561499878764152527e-02 2.038989961147308350e-01 -1.114590018987655640e-01 1.693200021982192993e-01 -1.135770007967948914e-01 1.581599935889244080e-02\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for Gensim\nDESCRIPTION: This snippet lists the required Python packages and their specific versions for the Gensim project. It includes libraries for various functionalities such as remote object communication, documentation, machine learning, memory profiling, and data analysis.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/requirements_docs.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nPyro4==4.77\nSphinx==3.5.2\nannoy==1.16.2\nmemory-profiler==0.55.0\nnltk==3.4.5\nnmslib==2.1.1\npandas==1.2.3\nPOT==0.8.1\nscikit-learn==0.24.1\nsphinx-gallery==0.8.2\nsphinxcontrib-napoleon==0.7\nsphinxcontrib-programoutput==0.15\nstatsmodels==0.12.2\ntestfixtures==6.17.1\n```\n\n----------------------------------------\n\nTITLE: Numeric Vector Data\nDESCRIPTION: A sequence of floating point numbers in scientific notation, likely representing vector embeddings or model weights. The data appears to be space-separated values with high precision.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/EN.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_15\n\nLANGUAGE: txt\nCODE:\n```\norange 7.760900259017944336e-02 -7.072699815034866333e-02 -1.308480054140090942e-01 2.794040143489837646e-01 -2.380000054836273193e-02 1.349709928035736084e-01 8.348000235855579376e-03 3.347399830818176270e-02 3.087230026721954346e-01 -2.781839966773986816e-01 [...]\n```\n\n----------------------------------------\n\nTITLE: Setting up Jupyter notebook environment with watermark\nDESCRIPTION: Loads the watermark extension to display version information for the libraries used in this tutorial.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# pip install watermark\n%reload_ext watermark\n%watermark -v -m -p gensim,numpy,scipy,psutil,matplotlib\n```\n\n----------------------------------------\n\nTITLE: Initializing Logging for Gensim in Python\nDESCRIPTION: Sets up basic logging configuration for Gensim, including timestamp, log level, and message format.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_downloader_api.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n```\n\n----------------------------------------\n\nTITLE: Logging Doc2Vec Training Initialization in Python\nDESCRIPTION: Log output showing the initialization of a new Doc2Vec model training session. It includes model parameters such as workers, vocabulary size, features, and training algorithm settings.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_doc2vec_imdb.rst#2025-04-21_snippet_17\n\nLANGUAGE: text\nCODE:\n```\n2023-08-23 12:49:53,240 : INFO : Doc2Vec lifecycle event {'msg': 'training model with 2 workers on 265408 vocabulary and 100 features, using sg=0 hs=0 sample=0 negative=5 window=10 shrink_windows=True', 'datetime': '2023-08-23T12:49:53.240069', 'gensim': '4.3.1.dev0', 'python': '3.8.17 (default, Jun  7 2023, 12:29:39) \\n[GCC 9.4.0]', 'platform': 'Linux-5.15.0-1042-azure-x86_64-with-glibc2.2.5', 'event': 'train'}\n```\n\n----------------------------------------\n\nTITLE: Displaying Topic Transformation Image in Python\nDESCRIPTION: Loads and displays an image related to topic transformations using matplotlib. Disables axis display for cleaner visualization.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_topics_and_transformations.rst#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimg = mpimg.imread('run_topics_and_transformations.png')\nimgplot = plt.imshow(img)\n_ = plt.axis('off')\n```\n\n----------------------------------------\n\nTITLE: Word Embedding Vector for 'mela'\nDESCRIPTION: This snippet shows the 500-dimensional vector representation for the word 'mela'. Each number represents a coordinate in the high-dimensional space, capturing semantic and syntactic properties of the word.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/IT.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_12\n\nLANGUAGE: plaintext\nCODE:\n```\nmela -6.023100018501281738e-02 1.960379928350448608e-01 1.464810073375701904e-01 -4.085500165820121765e-02 3.170599937438964844e-01 3.696820139884948730e-01 1.711049973964691162e-01 1.484300047159194946e-01 -3.316420018672943115e-01 1.388549953699111938e-01 2.044440060853958130e-01 4.757089912891387939e-01 1.329299956560134888e-01 3.059479892253875732e-01 -1.845230013132095337e-01 -2.861120104789733887e-01 -2.750959992408752441e-01 -5.952899903059005737e-02 -4.407379925251007080e-01 -4.867399856448173523e-02 2.901090085506439209e-01 1.484889984130859375e-01 1.463789939880371094e-01 2.522639930248260498e-01 1.506579965353012085e-01 -2.701400034129619598e-02 -1.507229954004287720e-01 1.625799946486949921e-02 2.973899990320205688e-02 1.676010042428970337e-01 -4.470089972019195557e-01 1.109210029244422913e-01 -6.470300257205963135e-02 -5.076000001281499863e-03 1.537059992551803589e-01 -1.224000006914138794e-03 -1.555850058794021606e-01 -1.110759973526000977e-01 1.480379998683929443e-01 1.422079950571060181e-01 -4.255700111389160156e-02 3.169789910316467285e-01 -1.675180047750473022e-01 2.622199952602386475e-01 2.126390039920806885e-01 1.365420073270797729e-01 1.123889982700347900e-01 -1.879699975252151489e-01 1.227699965238571167e-01 -8.757899701595306396e-02 2.103579938411712646e-01 -4.553499817848205566e-02 2.013480067253112793e-01 -3.016110062599182129e-01 2.973369956016540527e-01 -2.507100105285644531e-01 3.193640112876892090e-01 -6.291999667882919312e-02 6.485699862241744995e-02 -3.070200048387050629e-02 1.726640015840530396e-01 -1.957409977912902832e-01 2.850750088691711426e-01 -9.464000351727008820e-03 -1.301880031824111938e-01 2.238779962062835693e-01 1.137470006942749023e-01 -1.003599986433982849e-01 -5.797100067138671875e-02 -9.107299894094467163e-02 2.168399989604949951e-01 4.754300042986869812e-02 -4.026950001716613770e-01 2.785700000822544098e-02 2.280309945344924927e-01 -7.066699862480163574e-02 1.923599988222122192e-01 4.266330003738403320e-01 -1.232919991016387939e-01 -3.743380010128021240e-01 5.470800027251243591e-02 -1.525049954652786255e-01 2.912300080060958862e-02 -8.362299948930740356e-02 -7.949700206518173218e-02 1.429730057716369629e-01 -2.951999893411993980e-03 -6.741800159215927124e-02 -2.043959945440292358e-01 8.151099830865859985e-02 1.191570013761520386e-01 4.037089943885803223e-01 9.473999962210655212e-03 -1.754229962825775146e-01 1.523240059614181519e-01 -2.013369947671890259e-01 2.428880035877227783e-01 1.144179999828338623e-01 1.639769971370697021e-01 1.448269933462142944e-01 -5.801599845290184021e-02 1.964999921619892120e-02 2.459899932146072388e-01 2.820309996604919434e-01 1.327860057353973389e-01 6.947000324726104736e-02 1.983489990234375000e-01 3.363600000739097595e-02 1.977600008249282837e-01 1.409009993076324463e-01 1.082229986786842346e-01 2.588280141353607178e-01 -6.401599943637847900e-02 -4.786499962210655212e-02 2.409580051898956299e-01 1.067669987678527832e-01 -1.535889953374862671e-01 -3.295460045337677002e-01 -1.477659940719604492e-01 1.364980041980743408e-01 -1.567140072584152222e-01 2.125930041074752808e-01 2.016550004482269287e-01 1.482400000095367432e-01 1.214229986071586609e-01 2.278199940919876099e-01 -2.095070034265518188e-01 2.773349881172180176e-01 2.736659944057464600e-01 -1.294649988412857056e-01 -2.335550040006637573e-01 -3.093899972736835480e-02 -1.173209995031356812e-01 -8.403199911117553711e-02 -1.125240027904510498e-01 1.318529993295669556e-01 -8.794999867677688599e-02 -4.422000050544738770e-02 1.667360067367553711e-01 5.753400176763534546e-02 2.943139970302581787e-01 2.402340024709701538e-01 2.593910098075866699e-01 -6.526699662208557129e-02 1.298570036888122559e-01 3.654209971427917480e-01 -1.487089991569519043e-01 1.701099984347820282e-02 4.040399938821792603e-02 3.859669864177703857e-01 1.958889961242675781e-01 5.210100114345550537e-02 -6.482099741697311401e-02 1.136199980974197388e-01 -8.101499825716018677e-02 -3.026719987392425537e-01 1.918900012969970703e-02 1.862699985504150391e-01 -2.206130027770996094e-01 -6.854999810457229614e-03 -1.426379978656768799e-01 -6.175000220537185669e-03 5.511799827218055725e-02 -1.115880012512207031e-01 -1.832579970359802246e-01 -1.365000009536743164e-02 -7.496300339698791504e-02 8.513499796390533447e-02 -9.311799705028533936e-02 1.632120013236999512e-01 -5.465200170874595642e-02 -2.656999975442886353e-03 -2.186730057001113892e-01 -6.427100300788879395e-02 -3.846200183033943176e-02 8.878000080585479736e-02 2.090339958667755127e-01 3.044449985027313232e-01 -1.372559964656829834e-01 5.652600154280662537e-02 1.129830032587051392e-01 -1.503700017929077148e-02 1.866309940814971924e-01 -9.863399714231491089e-02 5.456999875605106354e-03 -9.129899740219116211e-02 -2.476399950683116913e-02 1.271429955959320068e-01 -1.886119991540908813e-01 -1.340689957141876221e-01 3.234669864177703857e-01 -2.434399910271167755e-02 1.035950034856796265e-01 4.021900147199630737e-02 -4.857600107789039612e-02 1.248029991984367371e-01 2.413540035486221313e-01 -1.625660061836242676e-01 -4.132699966430664062e-01 2.441309988498687744e-01 -6.408099830150604248e-02 -2.970440089702606201e-01 -4.679270088672637939e-01 1.288329958915710449e-01 7.308399677276611328e-02 -6.395599991083145142e-02 2.401469945907592773e-01 7.094399631023406982e-02 9.541000239551067352e-03 1.669829934835433960e-01 -1.779329925775527954e-01 9.336300194263458252e-02 -4.856399819254875183e-02 -1.932950019836425781e-01 4.520900174975395203e-02 -1.341419965028762817e-01 3.566199913620948792e-02 2.924600057303905487e-02 1.567949950695037842e-01 -8.386799693107604980e-02 -4.556899890303611755e-02 -3.236300125718116760e-02 -2.918139994144439697e-01 1.516489982604980469e-01 1.942950040102005005e-01 3.431180119514465332e-01 1.978130042552947998e-01 -1.195710003376007080e-01 -8.865299820899963379e-02 4.051899909973144531e-02 -6.502600014209747314e-02 5.452900007367134094e-02 -2.871379852294921875e-01 1.804690062999725342e-01 3.262929916381835938e-01 -4.882600158452987671e-02 1.531230062246322632e-01 1.188199967145919800e-01 8.770599961280822754e-02 1.878699958324432373e-01 -9.768799692392349243e-02 -1.017239987850189209e-01 -6.581100076436996460e-02 -2.133290022611618042e-01 2.691299915313720703e-01 2.096460014581680298e-01 -2.987239956855773926e-01 -6.232900172472000122e-02 -1.052360013127326965e-01 2.723000012338161469e-02 -6.789299845695495605e-02 5.217710137367248535e-01 2.519110143184661865e-01 -2.259399928152561188e-02 2.481400035321712494e-02 6.381999701261520386e-02 -7.649300247430801392e-02 4.976080060005187988e-01 1.576080024242401123e-01 -1.272740066051483154e-01 1.640129983425140381e-01 -1.250479966402053833e-01 4.486199840903282166e-02 6.688100099563598633e-02 1.918029934167861938e-01 -1.156560033559799194e-01 6.240800023078918457e-02 1.629710048437118530e-01 -2.282260060310363770e-01 -9.089799970388412476e-02 1.281149983406066895e-01 -5.110299959778785706e-02 -1.808310002088546753e-01 7.528000324964523315e-02 -4.956100136041641235e-02 -2.597799897193908691e-01 5.602699890732765198e-02 -1.071899980306625366e-01 -5.702999979257583618e-02 -2.296849936246871948e-01 -2.980079948902130127e-01 3.707399964332580566e-02 1.402799971401691437e-02 -1.282929927110671997e-01 1.820500008761882782e-02 -2.184530049562454224e-01 -1.778959929943084717e-01 9.468799829483032227e-02 -5.659700185060501099e-02 -4.900100082159042358e-02 9.804400056600570679e-02 3.124429881572723389e-01 7.586099952459335327e-02 -1.526689976453781128e-01 2.876000013202428818e-03 -2.377859950065612793e-01 2.100249975919723511e-01 3.286490142345428467e-01 -1.634030044078826904e-01 1.838099956512451172e-01\n```\n\n----------------------------------------\n\nTITLE: Sphinx Module Documentation Directive for Gensim Callbacks\nDESCRIPTION: ReStructuredText directive that configures the documentation generation for the gensim.models.callbacks module. Includes settings for synopsis, member inclusion, and inheritance display.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/models/callbacks.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: gensim.models.callbacks\n    :synopsis: Callbacks for track and viz LDA train process\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Word Frequency Data Format\nDESCRIPTION: Tab-separated data showing word stems and their frequency counts. Format appears to be: ID, word stem, frequency count.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/head500.noblanks.cor_wordids.txt#2025-04-21_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n9468\tccd\t4\n2444\tccn\t1\n12659\tcctv\t1\n1454\tcd\t8\n```\n\n----------------------------------------\n\nTITLE: Documenting Text Preprocessing Module in Python\nDESCRIPTION: This snippet uses Sphinx autodoc directives to generate documentation for the gensim.parsing.preprocessing module. It includes all members, inherited members, and undocumented members, while also showing inheritance information.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/parsing/preprocessing.rst#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: gensim.parsing.preprocessing\n    :synopsis: Functions to preprocess raw text\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Initializing Matplotlib Display\nDESCRIPTION: Configures matplotlib for inline display in a Jupyter notebook environment\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_doc2vec_lee.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%matplotlib inline\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Word Cloud Generation in Python\nDESCRIPTION: This snippet imports necessary Python libraries for file handling, image processing, and word cloud generation. It includes matplotlib, numpy, PIL, and wordcloud.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/tools/wordcloud.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport os.path as P\nimport re\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport PIL\nimport wordcloud\n```\n\n----------------------------------------\n\nTITLE: Word Vector Numerical Data\nDESCRIPTION: A 250-dimensional word vector representation for the word 'two', containing floating point values that encode semantic information about the word in vector space\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/EN.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_1\n\nLANGUAGE: txt\nCODE:\n```\ntwo -2.195999957621097565e-03 1.386159956455230713e-01 -1.767520010471343994e-01 -1.459649950265884399e-01 -8.652199804782867432e-02 1.975750029087066650e-01 1.121199969202280045e-02 -1.989099942147731781e-02 1.931319981813430786e-01 5.783800035715103149e-02 [...]\n```\n\n----------------------------------------\n\nTITLE: Inline Code References in Markdown\nDESCRIPTION: Inline code formatting showing package names and PR status indicators used in the documentation\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/HACKTOBERFEST.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n`gensim`\n```\n\nLANGUAGE: markdown\nCODE:\n```\n`smart_open`\n```\n\nLANGUAGE: markdown\nCODE:\n```\n`hacktoberfest`\n```\n\nLANGUAGE: markdown\nCODE:\n```\n\"[WIP]\"\n```\n\n----------------------------------------\n\nTITLE: Word Vector Representation - Numerical Data\nDESCRIPTION: A 300-dimensional vector representation of the word 'three' containing floating point values that encode semantic meaning in a vector space. Each number represents a different semantic feature or dimension.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/EN.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nthree 2.787099964916706085e-02 1.304740011692047119e-01 -1.716769933700561523e-01 -6.451299786567687988e-02 [...additional values...]\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Block for LDA Multicore Module\nDESCRIPTION: ReStructuredText documentation directive that configures the autodoc generation for the ldamulticore module. It specifies module synopsis, member inclusion, and inheritance display settings.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/models/ldamulticore.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: gensim.models.ldamulticore\n    :synopsis: Latent Dirichlet Allocation\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Checking Dataset Size\nDESCRIPTION: Prints the number of documents in the training and test sets to understand the dataset distribution.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/pivoted_document_length_normalisation.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprint(len(X_train), len(X_test))\n```\n\n----------------------------------------\n\nTITLE: Importing Mallet Corpus Module in Python\nDESCRIPTION: Module import path for the Mallet corpus functionality in gensim. This module provides capabilities for handling corpora in Mallet format within the gensim library.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/corpora/malletcorpus.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ngensim.corpora.malletcorpus\n```\n\n----------------------------------------\n\nTITLE: Getting Model File Path\nDESCRIPTION: Shows how to get the filesystem path to a model without loading it into memory.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/run_downloader_api.rst#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nprint(api.load('glove-wiki-gigaword-50', return_path=True))\n```\n\n----------------------------------------\n\nTITLE: Enabling Matplotlib Inline Display in Jupyter\nDESCRIPTION: Sets up the Jupyter notebook environment to display matplotlib visualizations inline within the notebook.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_corpora_and_vector_spaces.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%matplotlib inline\n```\n\n----------------------------------------\n\nTITLE: Displaying Word Embedding for 'from'\nDESCRIPTION: A 50-dimensional vector representation of the word 'from'.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/test_glove.txt#2025-04-21_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nfrom 0.41037 0.11342 0.051524 -0.53833 -0.12913 0.22247 -0.9494 -0.18963 -0.36623 -0.067011 0.19356 -0.33044 0.11615 -0.58585 0.36106 0.12555 -0.3581 -0.023201 -1.2319 0.23383 0.71256 0.14824 0.50874 -0.12313 -0.20353 -1.82 0.22291 0.020291 -0.081743 -0.27481 3.7343 -0.01874 -0.084522 -0.30364 0.27959 0.043328 -0.24621 0.015373 0.49751 0.15108 -0.01619 0.40132 0.23067 -0.10743 -0.36625 -0.051135 0.041474 -0.36064 -0.19616 -0.81066\n```\n\n----------------------------------------\n\nTITLE: Sphinx Documentation Configuration for LSI Model in Gensim\nDESCRIPTION: Sphinx reStructuredText directives for generating documentation for the Latent Semantic Indexing model in gensim. The configuration includes all members, special members like __getitem__, inherited members, undocumented members, and inheritance information.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/models/lsimodel.rst#2025-04-21_snippet_0\n\nLANGUAGE: reST\nCODE:\n```\n.. automodule:: gensim.models.lsimodel\n    :synopsis: Latent Semantic Indexing\n    :members:\n    :special-members: __getitem__\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Defining Sphinx Documentation for gensim.corpora Module\nDESCRIPTION: This reStructuredText snippet defines the documentation structure for the gensim.corpora module. It sets up an orphaned page, specifies the module title, and uses the automodule directive to generate documentation for all members and inherited members of the module.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/corpora/corpora.rst#2025-04-21_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n:orphan:\n\n:mod:`corpora` -- Package for corpora I/O\n==========================================\n\n.. automodule:: gensim.corpora\n    :synopsis: Package for corpora I/O\n    :members:\n    :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Defining Sphinx Autodoc Module for Document Similarity Queries in RST\nDESCRIPTION: This RST (reStructuredText) snippet defines the Sphinx documentation configuration for the gensim.similarities.docsim module. It specifies that all members, including inherited members, should be included in the generated documentation.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/similarities/docsim.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: gensim.similarities.docsim\n    :synopsis: Document similarity queries\n    :members:\n    :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Importing Matplotlib for Data Visualization in Python\nDESCRIPTION: This code imports the matplotlib library and enables inline plotting for Jupyter notebooks.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n%matplotlib inline\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Documentation for LogEntropy Model\nDESCRIPTION: Sphinx documentation configuration block that specifies the module path and documentation settings for the LogEntropy model. It includes settings for showing inherited members, undocumented members, and inheritance relationships.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/models/logentropy_model.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: gensim.models.logentropy_model\n    :synopsis: LogEntropy model\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Word Vector Embedding Data for 'mango'\nDESCRIPTION: 300-dimensional floating point vector representation of the word 'mango', likely from a trained word embedding model. Each number represents a coordinate in the high-dimensional semantic space.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/IT.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_15\n\nLANGUAGE: txt\nCODE:\n```\nmango 3.826000029221177101e-03 2.255710065364837646e-01 -2.374259978532791138e-01 -1.500129997730255127e-01 7.453300058841705322e-02 -2.356800064444541931e-02 2.245409935712814331e-01 -2.093469947576522827e-01 9.478300064802169800e-02 1.832839995622634888e-01 9.553100168704986572e-02 1.610029935836791992e-01 6.954699754714965820e-02 1.340830028057098389e-01 2.614199928939342499e-02 -3.125590085983276367e-01 -1.442299969494342804e-02 -8.107899874448776245e-02 -1.965370029211044312e-01 -2.889539897441864014e-01 3.071199916303157806e-02 1.760599948465824127e-02 3.602999961003661156e-03 3.394699841737747192e-02 -4.513100162148475647e-02 -1.537900045514106750e-02 -3.656499832868576050e-02 -1.114460006356239319e-01 -1.572369933128356934e-01 1.564480066299438477e-01 -1.137230023741722107e-01 7.249800115823745728e-02 2.572650015354156494e-01 -4.235090017318725586e-01 -5.013100057840347290e-02 -6.864099949598312378e-02 1.391199976205825806e-02 -1.849689930677413940e-01 7.650099694728851318e-02 -5.335599929094314575e-02 1.566170006990432739e-01 1.169919967651367188e-01 -3.319799900054931641e-02 2.161429971456527710e-01 6.139399856328964233e-02 1.513800024986267090e-01 1.154090017080307007e-01 -1.928150057792663574e-01 -1.279999967664480209e-02 -1.617480069398880005e-01 2.006790041923522949e-01 -3.492000047117471695e-03 3.358999965712428093e-03 -2.587609887123107910e-01 -3.167099878191947937e-02 -5.580899864435195923e-02 -6.481999997049570084e-03 -6.189800053834915161e-02 -9.440799802541732788e-02 -9.436800330877304077e-02 1.316619962453842163e-01 -2.570570111274719238e-01 1.700389981269836426e-01 2.309360057115554810e-01 -3.132840096950531006e-01 1.292680054903030396e-01 -1.166899967938661575e-02 -1.091699972748756409e-01 -5.237999930977821350e-02 -1.534959971904754639e-01 2.316540032625198364e-01 -2.480130046606063843e-01 -1.841080039739608765e-01 -9.530899673700332642e-02 5.624800175428390503e-02 -1.663859933614730835e-01 1.318069994449615479e-01 -1.357229948043823242e-01 -1.364939957857131958e-01 -2.042119950056076050e-01 -3.302060067653656006e-01 -4.348300024867057800e-02 4.756699874997138977e-02 -4.917800053954124451e-02 -8.572100102901458740e-02 -3.220900148153305054e-02 -2.269929945468902588e-01 5.324900150299072266e-02 1.424599997699260712e-02 -2.608929872512817383e-01 3.880400061607360840e-01 9.061000309884548187e-03 -6.170599907636642456e-02 -1.352600008249282837e-01 -1.326189935207366943e-01 -2.033890038728713989e-01 -9.491900354623794556e-02 -1.222560033202171326e-01 8.488500118255615234e-02 5.305700004100799561e-02 8.787500113248825073e-02 2.358100004494190216e-02 -9.198500216007232666e-02 1.718300022184848785e-02 6.629200279712677002e-02 -1.079970002174377441e-01 2.310539931058883667e-01 -2.199530005455017090e-01 -1.871429979801177979e-01 1.161020025610923767e-01 2.450679987668991089e-01 2.710149884223937988e-01 -1.046089977025985718e-01 2.515810132026672363e-01 2.986700087785720825e-02 5.645800009369850159e-02 -2.186570018529891968e-01 -1.790950000286102295e-01 2.567300014197826385e-02 3.678600117564201355e-02 5.094499886035919189e-02 2.619369924068450928e-01 8.335900306701660156e-02 7.939500361680984497e-02 4.957899823784828186e-02 -3.413600102066993713e-02 -6.387700140476226807e-02 5.273500084877014160e-02 -2.013130038976669312e-01 2.400559931993484497e-01 -3.178359866142272949e-01 -7.864899933338165283e-02 1.937890052795410156e-01 -2.424550056457519531e-01 1.084090024232864380e-01 7.080499827861785889e-02 1.093199998140335083e-01 -1.549659967422485352e-01 -5.059000104665756226e-03 -8.166000247001647949e-02 4.902999848127365112e-02 3.241899907588958740e-01 2.074179947376251221e-01 -8.013699948787689209e-02 -1.447499990463256836e-01 -7.994499802589416504e-02 -1.721439957618713379e-01 -1.465999931097030640e-01 1.718560010194778442e-01 2.732760012149810791e-01 -1.784310042858123779e-01 -3.106500022113323212e-02 -1.185199990868568420e-01 1.387719959020614624e-01 -2.108059972524642944e-01 -2.774890065193176270e-01 -1.129900012165307999e-02 1.904099993407726288e-02 -1.279820054769515991e-01 -2.382600009441375732e-01 7.412700355052947998e-02 -3.692980110645294189e-01 5.996200069785118103e-02 -9.483599662780761719e-02 2.070700004696846008e-02 -1.547860056161880493e-01 -1.508270055055618286e-01 -1.816900074481964111e-01 -2.133889943361282349e-01 7.565899938344955444e-02 -1.117269992828369141e-01 2.148019969463348389e-01 -8.137699961662292480e-02 -1.992110013961791992e-01 8.511099964380264282e-02 2.350150048732757568e-01 3.084900043904781342e-02 1.390050053596496582e-01 -2.016340047121047974e-01 2.313449978828430176e-01 7.612899690866470337e-02 1.184749975800514221e-01 7.429800182580947876e-02 1.892669945955276489e-01 2.774290144443511963e-01 7.610000204294919968e-03 1.660200022161006927e-02 2.493959963321685791e-01 1.422870010137557983e-01 -7.628700137138366699e-02 1.198270022869110107e-01 -1.002100035548210144e-01 -1.261560022830963135e-01 5.926499888300895691e-02 8.706700056791305542e-02 -1.332159936428070068e-01 1.390909999608993530e-01 -3.996500000357627869e-02 1.339450031518936157e-01 -9.747599810361862183e-02 -6.191699951887130737e-02 2.554499916732311249e-02 -1.866659969091415405e-01 3.439390063285827637e-01 -2.205779999494552612e-01 3.599699959158897400e-02 3.695310056209564209e-01 1.246609985828399658e-01 -8.486600220203399658e-02 1.680699922144412994e-02 -2.441800013184547424e-02 9.128499776124954224e-02 1.572149991989135742e-01 -3.395819962024688721e-01 9.169899672269821167e-02 -2.447649985551834106e-01 5.143000185489654541e-02 -2.702799998223781586e-02 -1.799199916422367096e-02 -8.661000058054924011e-03 6.826999783515930176e-02 -1.323599964380264282e-01 -1.245749965310096741e-01 -2.781310081481933594e-01 2.365799993276596069e-02 -9.742400050163269043e-02 8.053600043058395386e-02 -3.346800059080123901e-02 -1.825869977474212646e-01 -7.273100316524505615e-02 1.756380051374435425e-01 -6.293699890375137329e-02 5.430699884891510010e-02 2.542470097541809082e-01 7.788100093603134155e-02 -1.831340044736862183e-01 3.311000065878033638e-03 3.326500058174133301e-01 1.551569998264312744e-01 1.200000042445026338e-05 -8.028399944305419922e-02 -1.467790007591247559e-01 4.944700002670288086e-02 3.023869991302490234e-01 9.073899686336517334e-02 2.573530077934265137e-01 1.500090062618255615e-01 -3.164179921150207520e-01 -3.647330105304718018e-01 1.148789972066879272e-01 6.147000007331371307e-03 2.029529958963394165e-01 1.186939999461174011e-01 -1.004360020160675049e-01 -1.357029974460601807e-01 1.671900041401386261e-02 1.237239986658096313e-01 2.699750065803527832e-01 -1.425610035657882690e-01 9.528899937868118286e-02 1.232530027627944946e-01 6.358399987220764160e-02 -6.837599724531173706e-02 2.668399922549724579e-02 7.762300223112106323e-02 1.786639988422393799e-01 -3.646340072154998779e-01 -3.766199946403503418e-02 -1.105230003595352173e-01 1.099070012569427490e-01 -1.301469951868057251e-01 1.578879952430725098e-01 1.338340044021606445e-01 8.716700226068496704e-02 -1.306249946355819702e-01 -1.286219954490661621e-01 1.939110010862350464e-01 -8.712100237607955933e-02 3.731150031089782715e-01 -2.127580046653747559e-01 7.627000100910663605e-03 1.417880058288574219e-01 -6.656400114297866821e-02 2.081000059843063354e-02 4.928499832749366760e-02 -2.210170030593872070e-01 -2.731130123138427734e-01 2.148849964141845703e-01 -2.496300041675567627e-01 -1.538639962673187256e-01 1.701650023460388184e-01 -2.680850028991699219e-01 1.016680002212524414e-01 5.560500174760818481e-02 -2.761529982089996338e-01 -6.317400187253952026e-02 -1.326919943094253540e-01 2.399830073118209839e-01 1.087749972939491272e-01 1.319539994001388550e-01\n```\n\n----------------------------------------\n\nTITLE: Importing make_wiki_online Module in Python\nDESCRIPTION: This snippet demonstrates how to import the make_wiki_online module from the gensim.scripts package. The module is used for converting Wikipedia dump articles.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/scripts/make_wiki_online.rst#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom gensim.scripts import make_wiki_online\n```\n\n----------------------------------------\n\nTITLE: Word Vector Data - Numerical Representation\nDESCRIPTION: A 300-dimensional word vector embedding for the word 'banana', containing floating point values that represent semantic features in a vector space model\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/IT.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_14\n\nLANGUAGE: txt\nCODE:\n```\nbanana 2.379999961704015732e-03 3.165549933910369873e-01 7.732599973678588867e-02 -3.196229934692382812e-01 2.690129876136779785e-01 3.809899836778640747e-02 -6.910300254821777344e-02 -5.879800021648406982e-02 -2.868080139160156250e-01 4.187500104308128357e-02 [...]\n```\n\n----------------------------------------\n\nTITLE: Importing and Documenting Term Similarity Module in Python\nDESCRIPTION: This snippet uses Sphinx documentation syntax to automatically generate documentation for the gensim.similarities.termsim module. It includes a synopsis and specifies that all members and inherited members should be documented.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/similarities/termsim.rst#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: gensim.similarities.termsim\n    :synopsis: Term similarity queries\n    :members:\n    :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: NLTK Resource Download\nDESCRIPTION: Downloads required NLTK resources for text processing\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/wikinews-bigram-en.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# also prepare nltk\nimport nltk\nnltk.download(\"punkt\")\nnltk.download(\"stopwords\")\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Porter Stemming\nDESCRIPTION: Sphinx/RST documentation structure defining the auto-documentation settings for the Porter Stemming Algorithm module. Includes module reference, synopsis, and documentation display options.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/parsing/porter.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n:mod:`parsing.porter` -- Porter Stemming Algorithm\n==================================================\n\n.. automodule:: gensim.parsing.porter\n    :synopsis: Porter Stemming Algorithm\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Setting up Gensim Development Environment\nDESCRIPTION: Commands for cloning the Gensim repository, creating a feature branch, setting up a virtual environment, and installing Gensim in development mode with test dependencies.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/CONTRIBUTING.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/<YOUR_GITHUB_USERNAME>/gensim.git\n```\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout -b my-feature develop\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install virtualenv; virtualenv gensim_env\n```\n\nLANGUAGE: bash\nCODE:\n```\nsource gensim_env/bin/activate\n```\n\nLANGUAGE: bash\nCODE:\n```\ngensim_env\\Scripts\\activate\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .[test]\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .[test-win]\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx ToC Tree Structure\nDESCRIPTION: Sphinx toctree directive that defines the documentation navigation structure. The toctree is hidden from direct view, has a maximum depth of 1 level, and includes key documentation sections like intro, examples, API reference, support and team information.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/indextoc.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :hidden:\n   :maxdepth: 1\n\n   intro\n   auto_examples/index\n   apiref\n   support\n   people\n```\n\n----------------------------------------\n\nTITLE: 300-Dimensional Word Vector Data\nDESCRIPTION: A single line of 300 space-separated floating-point numbers representing a word vector. Each number corresponds to a dimension in the vector space, capturing semantic information about the word.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/IT.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nnove 2.449080049991607666e-01 1.620510071516036987e-01 1.361050009727478027e-01 -4.976600036025047302e-02 7.134400308132171631e-02 4.716499894857406616e-02 7.954499870538711548e-02 1.507609933614730835e-01 -4.455300047993659973e-02 -1.096099987626075745e-02 -5.745999887585639954e-02 7.864599674940109253e-02 -1.762440055608749390e-01 2.024099975824356079e-01 -2.828249931335449219e-01 -5.119000095874071121e-03 -9.035900235176086426e-02 -8.165299892425537109e-02 -2.272900007665157318e-02 -4.633799940347671509e-02 5.454700067639350891e-02 -1.679919958114624023e-01 1.605439931154251099e-01 2.012050002813339233e-01 -2.086829990148544312e-01 1.309600025415420532e-01 1.697999984025955200e-02 -2.112309932708740234e-01 -2.227340042591094971e-01 9.273199737071990967e-02 -7.732199877500534058e-02 2.023870050907135010e-01 -1.207400020211935043e-02 -1.111169978976249695e-01 1.324580013751983643e-01 -1.691289991140365601e-01 1.796330064535140991e-01 2.061340063810348511e-01 3.192009925842285156e-01 -8.569099754095077515e-02 2.392089962959289551e-01 5.258100107312202454e-02 1.460819989442825317e-01 2.033350020647048950e-01 5.455699935555458069e-02 1.282210052013397217e-01 -2.795499935746192932e-02 -3.052599914371967316e-02 -1.636620014905929565e-01 1.475819945335388184e-01 5.762099847197532654e-02 8.503299951553344727e-02 -3.395299986004829407e-02 -2.163179963827133179e-01 6.389100104570388794e-02 -1.801490038633346558e-01 3.587960004806518555e-01 -2.137670069932937622e-01 -1.476799976080656052e-02 9.508500248193740845e-02 2.253540009260177612e-01 -1.479219943284988403e-01 -1.084970012307167053e-01 4.114799946546554565e-02 -7.869499921798706055e-02 -2.877799980342388153e-02 9.517700225114822388e-02 1.259970068931579590e-01 4.377299919724464417e-02 1.348550021648406982e-01 1.384059935808181763e-01 -9.128700196743011475e-02 1.021580025553703308e-01 -2.495200000703334808e-02 -1.085269972681999207e-01 -1.113680005073547363e-01 3.838700056076049805e-02 1.370559930801391602e-01 -6.449799984693527222e-02 -1.612550020217895508e-01 -1.300889998674392700e-01 2.573299966752529144e-02 1.453000009059906006e-01 2.532199956476688385e-02 1.202120035886764526e-01 -3.528900071978569031e-02 -4.524800181388854980e-02 1.127749979496002197e-01 -3.135010004043579102e-01 9.677999652922153473e-03 -1.503939926624298096e-01 7.675100117921829224e-02 2.572799921035766602e-01 2.263469994068145752e-01 -1.670109927654266357e-01 4.999800026416778564e-02 -2.409199997782707214e-02 2.918109893798828125e-01 1.409150063991546631e-01 -6.843599677085876465e-02 3.913100063800811768e-02 -5.960699915885925293e-02 5.598400160670280457e-02 7.828500121831893921e-02 -8.038700371980667114e-02 -1.308609992265701294e-01 -1.553130000829696655e-01 -1.012399978935718536e-02 -1.067619994282722473e-01 -7.761999964714050293e-02 -1.345700025558471680e-02 -4.312799870967864990e-02 1.805170029401779175e-01 -9.841000288724899292e-02 4.282810091972351074e-01 6.684199720621109009e-02 3.629299998283386230e-02 -2.623900026082992554e-02 -1.878259927034378052e-01 -1.331090033054351807e-01 -1.576279997825622559e-01 2.020090073347091675e-01 -7.662799954414367676e-02 -2.186689972877502441e-01 -3.198700025677680969e-02 -3.104699961841106415e-02 -1.013420000672340393e-01 1.629730015993118286e-01 1.665740013122558594e-01 -1.690520048141479492e-01 8.300700038671493530e-02 -5.926999822258949280e-02 -1.899019926786422729e-01 -2.178760021924972534e-01 -1.426799967885017395e-02 1.471180021762847900e-01 -6.145099923014640808e-02 -3.310000058263540268e-03 9.533099830150604248e-02 -1.062529981136322021e-01 8.457600325345993042e-02 3.605999983847141266e-03 2.227990031242370605e-01 5.740699917078018188e-02 -3.709189891815185547e-01 -5.188500136137008667e-02 -5.834599956870079041e-02 1.510960012674331665e-01 -1.673000049777328968e-03 4.290400072932243347e-02 6.187200173735618591e-02 -8.547099679708480835e-02 9.604999795556068420e-03 -9.156399965286254883e-02 -4.304900020360946655e-02 8.351000025868415833e-03 -7.654599845409393311e-02 2.196500077843666077e-02 2.251799963414669037e-02 2.405900061130523682e-01 2.440699934959411621e-02 1.364669948816299438e-01 5.155900120735168457e-02 -6.982299685478210449e-02 -2.230390012264251709e-01 2.092619985342025757e-01 1.763579994440078735e-01 -2.526099979877471924e-02 -6.801699846982955933e-02 1.173259988427162170e-01 -6.213299930095672607e-02 -2.238820046186447144e-01 1.484169960021972656e-01 -1.656360030174255371e-01 1.610700041055679321e-02 5.574800074100494385e-02 -1.509159952402114868e-01 -1.956779956817626953e-01 -6.788399815559387207e-02 1.308159977197647095e-01 1.808100007474422455e-02 5.821299925446510315e-02 -1.894939988851547241e-01 2.785000018775463104e-03 -7.532600313425064087e-02 1.811700016260147095e-01 -1.171159967780113220e-01 -5.940000060945749283e-03 -1.727360039949417114e-01 -9.495200216770172119e-02 -1.415269970893859863e-01 2.524769902229309082e-01 4.492300003767013550e-02 7.451699674129486084e-02 -5.143399909138679504e-02 8.222799748182296753e-02 -8.903799951076507568e-02 -5.580800026655197144e-02 1.216809973120689392e-01 1.623470038175582886e-01 3.249990046024322510e-01 9.980999678373336792e-02 -2.443069964647293091e-01 8.786199986934661865e-02 1.037029996514320374e-01 7.435999810695648193e-02 1.640550047159194946e-01 -2.273949980735778809e-01 -2.618860006332397461e-01 1.305959969758987427e-01 -7.605499774217605591e-02 2.432080060243606567e-01 2.631030082702636719e-01 7.955999672412872314e-02 9.391500055789947510e-02 -1.830469965934753418e-01 -2.116500027477741241e-02 2.557739913463592529e-01 -1.127220019698143005e-01 -7.999300211668014526e-02 -9.336499869823455811e-02 -1.657200045883655548e-02 6.585899740457534790e-02 1.826290041208267212e-01 2.257609963417053223e-01 1.133920028805732727e-01 5.132599920034408569e-02 1.631940007209777832e-01 1.909100078046321869e-02 4.781400039792060852e-02 2.929239869117736816e-01 -1.859540045261383057e-01 3.427499905228614807e-02 1.071190014481544495e-01 -1.082660034298896790e-01 1.358509957790374756e-01 7.502800226211547852e-02 1.735289990901947021e-01 -5.938100069761276245e-02 3.281100094318389893e-02 1.363369971513748169e-01 -1.474300026893615723e-02 1.879899948835372925e-02 -2.471699938178062439e-02 -1.935300044715404510e-02 2.947599999606609344e-02 2.139820009469985962e-01 1.271799951791763306e-01 -7.238700240850448608e-02 2.331699989736080170e-02 2.066999906674027443e-03 2.935199998319149017e-02 4.234699904918670654e-02 1.111190021038055420e-01 2.080820053815841675e-01 1.234299968928098679e-02 -9.011399745941162109e-02 2.455590069293975830e-01 -1.611199975013732910e-01 -1.175229996442794800e-01 -1.315300017595291138e-01 -2.580890059471130371e-01 1.887020021677017212e-01 4.077500104904174805e-02 -1.542010009288787842e-01 -2.000209987163543701e-01 1.084410026669502258e-01 4.524299874901771545e-02 7.195500284433364868e-02 4.718799889087677002e-02 -7.164000067859888077e-03 1.043599992990493774e-01 6.721999961882829666e-03 8.221600204706192017e-02 7.253800332546234131e-02 1.994549930095672607e-01 -5.939200147986412048e-02 -7.181899994611740112e-02 1.027679964900016785e-01 1.885869950056076050e-01 1.861199922859668732e-02 1.423889994621276855e-01 8.728300034999847412e-02 6.890299916267395020e-02 -1.195880025625228882e-01 7.625500112771987915e-02 8.749900013208389282e-02 1.164159998297691345e-01 1.853399910032749176e-02 1.401560008525848389e-01 -1.157910004258155823e-01 -8.970099687576293945e-02 1.215929985046386719e-01 1.002010032534599304e-01 2.439759969711303711e-01 -1.387770026922225952e-01 -4.896400123834609985e-02 -1.601539999246597290e-01 -3.164999932050704956e-02 2.050130069255828857e-01\n```\n\n----------------------------------------\n\nTITLE: Word Embedding Vector for 'birds'\nDESCRIPTION: A 300-dimensional word embedding vector for the word 'birds'. Each number represents a dimension in the embedding space, capturing semantic and syntactic properties of the word.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/EN.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_13\n\nLANGUAGE: Data\nCODE:\n```\nbirds -1.470880061388015747e-01 2.359730005264282227e-01 1.041430011391639709e-01 -6.319800019264221191e-02 -1.204909980297088623e-01 -2.190199941396713257e-01 1.390160024166107178e-01 3.164860010147094727e-01 -2.427330017089843750e-01 1.587489992380142212e-01 2.432669997215270996e-01 1.319800019264221191e-01 1.335960030555725098e-01 -2.762440145015716553e-01 5.250500142574310303e-02 1.215759962797164917e-01 -4.060199856758117676e-02 4.727999866008758545e-02 6.807699799537658691e-02 2.180269956588745117e-01 -2.160280048847198486e-01 6.341599673032760620e-02 -1.715780049562454224e-01 -9.899900108575820923e-02 -1.656669974327087402e-01 6.506499648094177246e-02 -1.141799986362457275e-01 -6.958399713039398193e-02 -1.878740042448043823e-01 1.819700002670288086e-01 3.072699904441833496e-01 3.145790100097656250e-01 6.825999915599822998e-02 2.602339982986450195e-01 1.390450000762939453e-01 -3.118270039558410645e-01 1.243100035935640335e-02 2.075359970331192017e-01 -3.089520037174224854e-01 -5.705999955534934998e-02 8.292900025844573975e-02 -3.861499950289726257e-02 -4.911100119352340698e-02 -1.971399933099746704e-01 -5.413600057363510132e-02 4.658799991011619568e-02 5.418799817562103271e-02 -9.632699936628341675e-02 3.626500070095062256e-02 -3.151479959487915039e-01 1.092170029878616333e-01 -2.891799993813037872e-02 1.907179951667785645e-01 -1.475299987941980362e-02 1.186480000615119934e-01 2.765800058841705322e-02 5.417300015687942505e-02 1.545590013265609741e-01 -6.985999643802642822e-02 -1.845760047435760498e-01 -1.279319971799850464e-01 -1.987030059099197388e-01 -1.958670020103454590e-01 2.566449940204620361e-01 -2.206449955701828003e-01 -1.566019952297210693e-01 9.293200075626373291e-02 -8.593399822711944580e-02 -2.767499908804893494e-02 3.980059921741485596e-01 1.676899939775466919e-02 1.266050040721893311e-01 -1.008170023560523987e-01 -2.047140002250671387e-01 1.513170003890991211e-01 -1.327459961175918579e-01 -2.589870095252990723e-01 2.354220002889633179e-01 9.258600324392318726e-02 7.447999902069568634e-03 -2.935839891433715820e-01 1.982319951057434082e-01 3.274000063538551331e-02 1.658000051975250244e-03 -8.090099692344665527e-02 -1.145019978284835815e-01 8.687400072813034058e-02 2.512649893760681152e-01 -5.927800014615058899e-02 -2.174839973449707031e-01 1.349920034408569336e-01 -6.611800193786621094e-02 -8.209700137376785278e-02 -6.515300273895263672e-02 6.693500280380249023e-02 6.455000024288892746e-03 -6.325300037860870361e-02 -1.107500027865171432e-02 -3.952749967575073242e-01 2.722960114479064941e-01 -5.657399818301200867e-02 8.980099856853485107e-02 -2.433830052614212036e-01 -5.447600036859512329e-02 -1.190870031714439392e-01 -6.175699830055236816e-02 -2.365860044956207275e-01 -1.382400002330541611e-02 4.374589920043945312e-01 -1.932419985532760620e-01 -1.816460043191909790e-01 -2.204989939928054810e-01 -1.203989982604980469e-01 8.240500092506408691e-02 -1.233899965882301331e-01 1.810739934444427490e-01 1.898680031299591064e-01 -3.746400028467178345e-02 1.865189969539642334e-01 1.008960008621215820e-01 1.073540002107620239e-01 -3.594199940562248230e-02 -1.463969945907592773e-01 -9.837000072002410889e-02 1.297319978475570679e-01 -7.030099630355834961e-02 2.691200003027915955e-02 2.488570064306259155e-01 -1.686899922788143158e-02 -5.156499892473220825e-02 -2.316300012171268463e-02 -3.382800146937370300e-02 -1.372599974274635315e-02 1.534930020570755005e-01 3.062089979648590088e-01 -1.267160028219223022e-01 2.693899907171726227e-02 2.102500051259994507e-01 8.243000134825706482e-03 -1.929160058498382568e-01 -1.076570004224777222e-01 -5.048959851264953613e-01 -9.877400100231170654e-02 2.559800073504447937e-02 -1.192250028252601624e-01 -8.786600083112716675e-02 -2.798680067062377930e-01 2.900299988687038422e-02 6.575600057840347290e-02 6.360999774187803268e-03 -3.062970042228698730e-01 -1.825370043516159058e-01 -9.459199756383895874e-02 -1.562149971723556519e-01 1.129050031304359436e-01 2.499350011348724365e-01 3.909150063991546631e-01 9.892299771308898926e-02 -7.604800164699554443e-02 1.519590020179748535e-01 -2.687639892101287842e-01 -2.348009943962097168e-01 9.347499907016754150e-02 -1.851229965686798096e-01 -1.499210000038146973e-01 -3.435000078752636909e-03 -1.972019970417022705e-01 5.732430219650268555e-01 -1.186029985547065735e-01 4.744400084018707275e-02 2.808330059051513672e-01 1.282850056886672974e-01 1.237690001726150513e-01 -3.934999927878379822e-03 -1.330450028181076050e-01 3.733519911766052246e-01 1.189640015363693237e-01 -8.038300275802612305e-02 4.265899956226348877e-01 4.635910093784332275e-01 6.174999848008155823e-02 -2.695200033485889435e-02 -2.063000109046697617e-03 -3.141399845480918884e-02 9.727200120687484741e-02 -2.333620041608810425e-01 -8.094900101423263550e-02 9.368199855089187622e-02 1.627019941806793213e-01 1.809210032224655151e-01 -5.690000019967556000e-03 2.155000111088156700e-03 -8.278100192546844482e-02 -1.376560032367706299e-01 1.313569992780685425e-01 7.865399867296218872e-02 -6.140299886465072632e-02 1.223799958825111389e-02 1.076269969344139099e-01 -1.073630005121231079e-01 -6.032000109553337097e-02 5.767099931836128235e-02 7.189399749040603638e-02 -2.502129971981048584e-01 -2.691490054130554199e-01 1.366419941186904907e-01 4.221399873495101929e-02 -6.663200259208679199e-01 4.429600015282630920e-02 -3.299080133438110352e-01 -2.703840136528015137e-01 -4.624899849295616150e-02 2.839600145816802979e-01 -4.805399850010871887e-02 -1.247629970312118530e-01 1.878229975700378418e-01 -3.641200065612792969e-02 -1.489699929952621460e-01 1.378059983253479004e-01 -8.027900010347366333e-02 3.307810127735137939e-01 3.615399822592735291e-02 -2.718980014324188232e-01 9.511099755764007568e-02 -2.249290049076080322e-01 -2.600539922714233398e-01 -1.518409997224807739e-01 -2.220599912106990814e-02 2.534619867801666260e-01 2.053509950637817383e-01 2.351740002632141113e-01 4.792400076985359192e-02 -1.532229930162429810e-01 -1.146299988031387329e-01 -1.336890012025833130e-01 -3.124319911003112793e-01 3.238280117511749268e-01 -9.839399904012680054e-02 3.081479966640472412e-01 1.137099973857402802e-02 1.962739974260330200e-01 -3.263370096683502197e-01 1.487240046262741089e-01 -1.375560015439987183e-01 4.515900090336799622e-02 3.112260103225708008e-01 1.245549991726875305e-01 -9.213799983263015747e-02 7.292199879884719849e-02 1.145999995060265064e-03 -1.105479970574378967e-01 3.437700122594833374e-02 1.771250069141387939e-01 1.546909958124160767e-01 -5.173999816179275513e-02 -1.600770056247711182e-01 -1.753599941730499268e-02 -1.589329987764358521e-01 -6.717099994421005249e-02 9.810200333595275879e-02 -2.352969944477081299e-01 -7.496800273656845093e-02 -8.115500211715698242e-02 -4.276080131530761719e-01 1.752379983663558960e-01 -1.406259983777999878e-01 7.631599903106689453e-02 1.059610024094581604e-01 5.876999814063310623e-03 2.663129866123199463e-01 -1.092839986085891724e-01 1.743790060281753540e-01 7.512100040912628174e-02 -1.678700000047683716e-02 -1.485600043088197708e-02 -1.841899938881397247e-02 3.724839985370635986e-01 2.619380056858062744e-01 -4.735700041055679321e-02 1.518020033836364746e-01 5.414700135588645935e-02 5.230800062417984009e-02 2.288040071725845337e-01 -1.832229942083358765e-01 8.433099836111068726e-02 2.872700095176696777e-01 -7.803600281476974487e-02 -2.468999940901994705e-03 3.950079977512359619e-01 7.519699633121490479e-02 1.489959955215454102e-01 -2.561190128326416016e-01 4.979100078344345093e-02 -7.104899734258651733e-02 -7.917000353336334229e-02 -1.335979998111724854e-01 9.185999631881713867e-03 1.669099926948547363e-02 -3.321389853954315186e-01 3.535729944705963135e-01\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Documentation for Word2Vec Module\nDESCRIPTION: ReStructuredText directives for configuring Sphinx documentation of the word2vec module. Specifies module path, synopsis, and documentation display options including inherited members and undocumented members.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/models/word2vec.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n:mod:`models.word2vec` -- Word2vec embeddings\n=============================================\n\n.. automodule:: gensim.models.word2vec\n    :synopsis: Word2vec embeddings\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx AutoDoc for Topic Coherence Module (RST)\nDESCRIPTION: RST directives for configuring Sphinx documentation generation for the topic_coherence.aggregation module. Sets up module synopsis and controls member visibility including inherited and undocumented members.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/topic_coherence/aggregation.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: gensim.topic_coherence.aggregation\n    :synopsis: Aggregation module\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Analyzing Fourth List of Names\nDESCRIPTION: This snippet contains a list of names, which are analyzed for potential use in applications like name generation, user data creation for software testing, or social network analysis. The list includes names such as 'sue', 'tim', 'dave', 'harry', etc.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/ldavowpalwabbit.txt#2025-04-21_snippet_11\n\nLANGUAGE: text\nCODE:\n```\n\"sue tim dave harry bob bob jim rachel rachel harry sue alex tim sue harry rachel tim tim alex jim sue dave bob sue sue sue dave jim harry sue alice harry alice bob bob rachel robert robert jim bob rachel tim tim rachel bob bob alice alice alex dave alex bob alex rachel robert alice bob alex alice dave sue tim jim alice robert tim robert harry sue alice rachel\"\n```\n\n----------------------------------------\n\nTITLE: Documentation Header in RST\nDESCRIPTION: ReStructuredText header defining the documentation section and gallery reference label\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/gallery/README.txt#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nDocumentation\n=============\n\n.. _gallery_top:\n```\n\n----------------------------------------\n\nTITLE: Scientific Notation Number Matrix\nDESCRIPTION: A sequence of 300 space-separated floating point numbers in scientific notation format (e.g., 3.069799952208995819e-02). Appears to be numerical data for analysis or machine learning, possibly vector/matrix data.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/EN.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_7\n\nLANGUAGE: text\nCODE:\n```\neight 3.069799952208995819e-02 1.424860060214996338e-01 -1.306300014257431030e-01 -6.515599787235260010e-02 5.075399950146675110e-02 1.101830005645751953e-01 3.049699962139129639e-02 1.365000032819807529e-03 1.373160034418106079e-01 [...continues]\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Documentation for Gensim Base Model\nDESCRIPTION: Sphinx documentation configuration block that specifies how to document the gensim.models.basemodel module. It includes settings to show all members, inherited members, undocumented members and inheritance relationships.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/models/basemodel.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: gensim.models.basemodel\n    :synopsis: Core TM interface\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Defining RST Documentation for NMF Module in Gensim\nDESCRIPTION: ReStructuredText documentation configuration for the NMF module in Gensim. The configuration specifies the automodule directive to generate documentation for the Non-Negative Matrix Factorization module, including all members, inherited members, undocumented members, and inheritance relationships.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/models/nmf.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: gensim.models.nmf\n    :synopsis: Non-Negative Matrix Factorization\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Sphinx Documentation Directive for GibbsLda++ Corpus Module\nDESCRIPTION: Sphinx documentation configuration directive that sets up autodoc generation for the gensim.corpora.lowcorpus module. Includes configuration for showing all members, inherited members, undocumented members and inheritance relationships.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/corpora/lowcorpus.rst#2025-04-21_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. automodule:: gensim.corpora.lowcorpus\n    :synopsis: Corpus in GibbsLda++ format\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Module Documentation Directive\nDESCRIPTION: Sphinx documentation directive that specifies the module to document and its configuration options. Sets up auto-documentation for sharded_corpus module with synopsis, members, inheritance and other options.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/corpora/sharded_corpus.rst#2025-04-21_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. automodule:: gensim.corpora.sharded_corpus\n    :synopsis: Numpy arrays on disk for iterative processing \n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Extracting Dataset\nDESCRIPTION: Unzips the downloaded dataset file\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_network.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n!unzip fake.news.zip\n```\n\n----------------------------------------\n\nTITLE: 300-Dimensional Vector Data\nDESCRIPTION: A 300-dimensional vector represented as space-separated floating point numbers in scientific notation. Common format used in word embeddings or other high-dimensional numerical data.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/EN.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nfive 7.488500326871871948e-02 1.285589933395385742e-01 -9.260199964046478271e-02 -5.428700149059295654e-02 9.875999763607978821e-03 2.371640056371688843e-01 6.615199893712997437e-02 -1.256580054759979248e-01 7.377599924802780151e-02 [...] 7.279799878597259521e-02\n```\n\n----------------------------------------\n\nTITLE: Word Embedding Vector Data in TRE Format\nDESCRIPTION: This is a single word embedding vector consisting of 400 dimensions represented as floating-point numbers. Each number represents the value of the word along a particular dimension in the embedding space. This format is commonly used in Gensim and other NLP libraries for word vector storage.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/IT.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ntre 1.336809992790222168e-01 8.114799857139587402e-02 -8.692000061273574829e-03 -8.047299832105636597e-02 1.111629977822303772e-01 1.484729945659637451e-01 -3.643000125885009766e-02 9.789700061082839966e-02 1.573600061237812042e-02 4.670299962162971497e-02 -1.409810036420822144e-01 1.435389965772628784e-01 -1.951999962329864502e-01 5.727700144052505493e-02 -3.955300152301788330e-02 7.996600121259689331e-02 -1.079820021986961365e-01 -8.450999855995178223e-02 -1.265549957752227783e-01 -3.460700064897537231e-02 -1.174819990992546082e-01 -1.777040064334869385e-01 -9.233999997377395630e-03 1.189050003886222839e-01 -9.861200302839279175e-02 1.150579974055290222e-01 3.644900023937225342e-02 7.916999980807304382e-03 -8.952300250530242920e-02 -3.685399889945983887e-02 -6.980600208044052124e-02 1.810040026903152466e-01 7.054500281810760498e-02 -2.843160033226013184e-01 1.795150041580200195e-01 -5.620000138878822327e-02 7.113499939441680908e-02 1.395999938249588013e-01 2.418330013751983643e-01 -1.397500038146972656e-01 6.048100069165229797e-02 1.249200012534856796e-02 1.300709992647171021e-01 1.313720047473907471e-01 -3.342900052666664124e-02 8.401100337505340576e-02 3.820999991148710251e-03 -1.318799983710050583e-02 -6.395299732685089111e-02 -3.663900122046470642e-02 1.993750035762786865e-01 3.146500140428543091e-02 -5.095800012350082397e-02 -2.163829952478408813e-01 5.475600063800811768e-02 -8.635800331830978394e-02 2.981230020523071289e-01 -1.393489986658096313e-01 1.913800090551376343e-02 1.434240043163299561e-01 7.273200154304504395e-02 -8.101300150156021118e-02 -9.876699745655059814e-02 -8.788199722766876221e-02 -5.335700139403343201e-02 -1.230869963765144348e-01 2.124180048704147339e-01 -4.868200048804283142e-02 1.035379990935325623e-01 1.644739955663681030e-01 8.184599876403808594e-02 -8.907999843358993530e-02 7.211399823427200317e-02 2.074999921023845673e-02 -8.391900360584259033e-02 1.532499939203262329e-01 -2.462900057435035706e-02 4.976000171154737473e-03 -4.777999967336654663e-02 -7.276599854230880737e-02 2.812300063669681549e-02 2.192589938640594482e-01 1.567300036549568176e-02 -9.391699731349945068e-02 1.385900005698204041e-02 7.976900041103363037e-02 2.264099940657615662e-02 1.929430067539215088e-01 -2.469539940357208252e-01 -4.450299963355064392e-02 -1.512430012226104736e-01 -3.893600031733512878e-02 4.582799971103668213e-02 1.169599965214729309e-01 -1.106880009174346924e-01 -7.089999853633344173e-04 5.464699864387512207e-02 9.432599693536758423e-02 2.102770060300827026e-01 -1.503400038927793503e-02 6.700800359249114990e-02 -1.847900077700614929e-02 1.801089942455291748e-01 1.027499977499246597e-02 -8.247999846935272217e-02 5.301100015640258789e-02 -2.005919963121414185e-01 -2.013700082898139954e-02 3.181499987840652466e-02 -8.113600313663482666e-02 9.486000053584575653e-03 -5.202899873256683350e-02 9.760300070047378540e-02 2.811999991536140442e-03 3.968130052089691162e-01 -1.419329941272735596e-01 -2.227599918842315674e-02 -2.314499951899051666e-02 -1.064890027046203613e-01 -8.687700331211090088e-02 -7.521899789571762085e-02 1.686570048332214355e-01 -2.820100076496601105e-02 -1.965470016002655029e-01 2.903800085186958313e-02 -3.662599995732307434e-02 -1.287100017070770264e-01 1.911399960517883301e-01 1.783680021762847900e-01 -9.438200294971466064e-02 6.240599974989891052e-02 3.383100032806396484e-02 -1.413500010967254639e-01 -5.105499923229217529e-02 1.499000005424022675e-03 1.415199972689151764e-02 -1.127469986677169800e-01 -5.433999933302402496e-03 1.233640015125274658e-01 3.998999949544668198e-03 -3.561599925160408020e-02 -1.225709989666938782e-01 1.579399965703487396e-02 1.611869931221008301e-01 -2.639659941196441650e-01 -7.716300338506698608e-02 -1.006070002913475037e-01 1.739419996738433838e-01 1.529169976711273193e-01 1.682959944009780884e-01 1.472039967775344849e-01 4.659700021147727966e-02 -6.180000025779008865e-03 -1.204280033707618713e-01 1.054900046437978745e-02 4.475200176239013672e-02 -1.480260044336318970e-01 1.281000021845102310e-02 7.648999802768230438e-03 4.688800126314163208e-02 -2.562700025737285614e-02 3.262399882078170776e-02 7.219699770212173462e-02 1.359020024538040161e-01 -7.339400053024291992e-02 2.306469976902008057e-01 -8.880000095814466476e-04 2.886199951171875000e-02 -5.197700113058090210e-02 2.065880000591278076e-01 -2.373999916017055511e-02 -2.944100089371204376e-02 8.268799632787704468e-02 -1.469569951295852661e-01 1.371449977159500122e-01 -1.226000022143125534e-02 -1.208660006523132324e-01 -5.180700123310089111e-02 6.909400224685668945e-02 1.369780004024505615e-01 -1.123999943956732750e-03 1.184200029820203781e-02 -1.522780060768127441e-01 8.351000025868415833e-03 -4.256900027394294739e-02 1.580339968204498291e-01 9.612999856472015381e-03 2.764900028705596924e-02 -1.372189968824386597e-01 -2.400909960269927979e-01 3.218099847435951233e-02 1.690129935741424561e-01 1.252759993076324463e-01 1.246789991855621338e-01 -3.568400070071220398e-02 3.611199930310249329e-02 -1.905239969491958618e-01 -6.847800314426422119e-02 1.862500049173831940e-02 -1.354610025882720947e-01 3.161889910697937012e-01 1.100199967622756958e-01 -9.746400266885757446e-02 -3.151199966669082642e-02 7.823000103235244751e-02 1.432539969682693481e-01 2.718389928340911865e-01 -1.753080040216445923e-01 -8.900099992752075195e-02 2.168149948120117188e-01 -8.276999928057193756e-03 2.293699979782104492e-01 1.079080030322074890e-01 -9.774000383913516998e-03 2.988000027835369110e-02 -1.538699958473443985e-02 -1.022659987211227417e-01 2.577849924564361572e-01 -1.713500022888183594e-01 -5.699900165200233459e-02 9.037200361490249634e-02 -2.090390026569366455e-01 4.945899918675422668e-02 2.434989959001541138e-01 2.105509936809539795e-01 1.609369963407516479e-01 -3.966299816966056824e-02 7.521999999880790710e-03 -6.060900166630744934e-02 -2.841399982571601868e-02 1.271079927682876587e-01 -2.717999974265694618e-03 5.241800099611282349e-02 1.188860014081001282e-01 -1.872999942861497402e-03 2.110150009393692017e-01 -3.237000107765197754e-02 1.299120038747787476e-01 -4.120799899101257324e-02 9.417899698019027710e-02 -2.975999936461448669e-02 1.734199933707714081e-02 -1.241089999675750732e-01 1.192899979650974274e-02 4.869300127029418945e-02 8.236099779605865479e-02 1.917439997196197510e-01 1.071230024099349976e-01 6.898199766874313354e-02 -1.412820070981979370e-01 -8.955100178718566895e-02 -6.563200056552886963e-02 -2.928999951109290123e-03 1.893000006675720215e-01 1.238529980182647705e-01 4.676299914717674255e-02 8.784600347280502319e-02 4.693400114774703979e-02 -1.180810034275054932e-01 -6.162000074982643127e-02 -4.128599911928176880e-02 -1.297959983348846436e-01 3.549300134181976318e-02 4.640199989080429077e-02 -2.571609914302825928e-01 -1.978619992733001709e-01 1.912889927625656128e-01 1.837770044803619385e-01 2.330200001597404480e-02 1.133849993348121643e-01 -1.015190035104751587e-01 6.213299930095672607e-02 -3.614699840545654297e-02 1.085909977555274963e-01 9.966400265693664551e-02 2.288579940795898438e-01 -8.990500122308731079e-02 1.262999954633414745e-03 -4.458900168538093567e-02 1.228360012173652649e-01 2.049499936401844025e-02 1.039730012416839600e-01 2.567409873008728027e-01 -4.930699989199638367e-02 9.568399935960769653e-02 1.364549994468688965e-01 -2.482000039890408516e-03 2.527999924495816231e-03 -3.375500068068504333e-02 -8.231800049543380737e-02 4.581000190228223801e-03 -1.629999955184757710e-03 7.889200001955032349e-02 1.295859962701797485e-01 2.034009993076324463e-01 -4.721799865365028381e-02 -5.172700062394142151e-02 -2.196840047836303711e-01 5.842199921607971191e-02 2.154140025377273560e-01\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Documentation for Author-Topic Model Module\nDESCRIPTION: Sphinx configuration directives for documenting the gensim.models.atmodel module. Sets up automodule documentation with inherited members, undocumented members, and inheritance diagrams.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/models/atmodel.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: gensim.models.atmodel\n    :synopsis: Author-topic model\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Wikicorpus Module\nDESCRIPTION: Sphinx documentation structure defining the module layout and automodule directives for the wikicorpus component. Includes configuration for member visibility and inheritance documentation.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/corpora/wikicorpus.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n:mod:`corpora.wikicorpus` -- Corpus from a Wikipedia dump\n==========================================================\n\n.. automodule:: gensim.corpora.wikicorpus\n    :synopsis: Corpus from a Wikipedia dump\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Deprecating Code Paths in Gensim\nDESCRIPTION: List of code paths that are being deprecated and will be removed in the next major release, including wrappers, examples, scripts and utility functions being moved to new locations.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/CHANGELOG.md#2025-04-21_snippet_11\n\nLANGUAGE: text\nCODE:\n```\nRemove:\n- gensim.models.wrappers.fasttext\n- gensim.examples\n- gensim.nosy\n- gensim.scripts.word2vec_standalone\n- gensim.scripts.make_wiki_lemma\n- gensim.scripts.make_wiki_online\n- gensim.scripts.make_wiki_online_lemma\n- gensim.scripts.make_wiki_online_nodebug\n- gensim.scripts.make_wiki\n\nMove:\n- gensim.scripts.make_wikicorpus → gensim.scripts.make_wiki.py\n- gensim.summarization → gensim.models.summarization\n- gensim.topic_coherence → gensim.models._coherence\n- gensim.utils → gensim.utils.utils\n- gensim.parsing.* → gensim.utils.text_utils\n```\n\n----------------------------------------\n\nTITLE: Python Module Import Reference\nDESCRIPTION: Sphinx automodule directive for documenting the opinosiscorpus module from gensim.corpora package. Includes inherited members, undocumented members and inheritance information.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/corpora/opinosiscorpus.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: gensim.corpora.opinosiscorpus\n    :synopsis: Topic related review sentences\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Word Frequency List Format\nDESCRIPTION: Each line contains a frequency count followed by a word/term and an additional numeric value, separated by tabs. The format appears to be: frequency word count\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/head500.noblanks.cor_wordids.txt#2025-04-21_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n25906\tenduringli\t1\n19577\tendzon\t1\n9438\tenema\t1\n3826\tenemi\t25\n```\n\n----------------------------------------\n\nTITLE: Analyzing Fifth List of Car Parts\nDESCRIPTION: This snippet includes another list of car-related terms, similar to prior lists. The goal is to analyze and document this dataset for potential use in automotive applications, data analysis, or natural language processing.  The list contains car parts like 'car', 'tyre', 'cylinder', 'exhaust', etc.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/ldavowpalwabbit.txt#2025-04-21_snippet_17\n\nLANGUAGE: text\nCODE:\n```\n\"car tyre cylinder exhaust motor exhaust motor car car exhaust brakes wheel car tyre clutch motor tyre car wheel exhaust brakes car motor tyre cylinder wheel wheel car suspension clutch car exhaust clutch cylinder car wheel cylinder exhaust clutch cylinder engine brakes engine wheel cylinder tyre clutch engine wheel clutch cylinder cylinder suspension tyre car clutch engine engine tyre motor wheel motor exhaust suspension clutch wheel suspension engine engine engine tyre brakes wheel engine tyre tyre cylinder tyre cylinder cylinder car tyre motor exhaust suspension motor motor clutch car exhaust wheel clutch brakes exhaust engine brakes car engine clutch\"\n```\n\n----------------------------------------\n\nTITLE: Word Frequency Data Format\nDESCRIPTION: Tab-separated word frequency data showing word ID, word, and frequency count. Format: [ID]\\t[word]\\t[count]\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/head500.noblanks.cor_wordids.txt#2025-04-21_snippet_10\n\nLANGUAGE: text\nCODE:\n```\n28444\tweinbaum\t1\n23815\tweinberg\t1\n25858\tweiner\t1\n```\n\n----------------------------------------\n\nTITLE: Displaying Tutorial Image\nDESCRIPTION: Loads and displays an image related to the core concepts tutorial using matplotlib.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_core_concepts.rst#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimg = mpimg.imread('run_core_concepts.png')\nimgplot = plt.imshow(img)\n_ = plt.axis('off')\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Documentation for Gensim matutils Module\nDESCRIPTION: RST directive for configuring Sphinx documentation to auto-document the matutils module. Sets up module synopsis, member documentation, inherited members display, undocumented members inclusion, and inheritance diagram display.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/matutils.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: gensim.matutils\n    :synopsis: Math utils\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Formatting for Contributors and Sponsors Page\nDESCRIPTION: RST markup defining the structure and content of the contributors and sponsors documentation page, including section headers, links, images and figure directives.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/people.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _people:\n\nPeople behind Gensim\n====================\n\n.. _contributors:\n\nTop Contributors\n----------------\n\n.. figure:: _static/images/misha_radim.jpeg\n   :width: 100%\n   :alt: Misha (left) and Radim (right) got together in Prague for some open source and badminton :)\n\n   RARE photo: Misha (left) and Radim (right) got together in Prague, for some open source hacking & badminton :)\n\n.. _gold-sponsors:\n\nGold Sponsors\n-------------\n\n.. _silver-sponsors:\n\nSilver Sponsors\n---------------\n\n.. figure:: _static/images/wilabs-logo.png\n   :target: https://wilabs.com/\n   :width: 50%\n   :alt: WiLabs\n\n.. _bronze-sponsors:\n\nBronze Sponsors\n---------------\n\n.. figure:: _static/images/route4me-logo.png\n   :target: https://route4me.com\n   :width: 50%\n   :alt: Route Optimizer and Route Planner Software\n```\n\n----------------------------------------\n\nTITLE: Using FastText Model with Russian Language Support\nDESCRIPTION: Demonstration of improved FastText compatibility including proper handling of non-latin languages, loading pre-trained models, and continued training with new corpus data.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/CHANGELOG.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models import FastText\n\nmodel = FastText.load_fasttext_format(\"cc.ru.300.bin\")\n\nassert \"мяу\" in m.wv.vocab\nmodel.wv.most_similar(\"мяу\")\n\nassert \"котогород\" not in m.wv.vocab\nmodel.wv.most_similar(\"котогород\", topn=3)\n\nfrom gensim.test.utils import datapath\nfrom smart_open import smart_open\n\nwith smart_open(datapath(\"crime-and-punishment.txt\"), encoding=\"utf-8\") as infile:\n    corpus = [line.strip().split() for line in infile]\n\nmodel.train(corpus, total_examples=len(corpus), epochs=5)\n```\n\n----------------------------------------\n\nTITLE: Deleting Serialized Model File in Python\nDESCRIPTION: This snippet shows how to delete the serialized model file after use, which is important for cleaning up temporary files and managing disk space.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n# Delete the file, once you're done using it.\nimport os\nos.remove('/tmp/model_serialization.mm')\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx AutoModule for Coherence Model\nDESCRIPTION: Sphinx documentation directive for automatically generating API documentation from the gensim.models.coherencemodel module. Includes inherited members, undocumented members, and inheritance diagrams.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/models/coherencemodel.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: gensim.models.coherencemodel\n    :synopsis: Topic coherence pipeline\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Word Vector Representation in Numerical Format\nDESCRIPTION: A 300-dimensional vector representation for the word 'acino', with each value being a floating-point number representing the coordinate in a specific semantic dimension of the embedding space. This format is common in word embedding models like Word2Vec, GloVe, or FastText.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/IT.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_13\n\nLANGUAGE: text\nCODE:\n```\nacino 2.490399964153766632e-02 1.230349987745285034e-01 -6.001099944114685059e-02 -1.807399988174438477e-01 1.134570017457008362e-01 -1.079230010509490967e-01 -4.417299851775169373e-02 -1.037489995360374451e-01 1.600030064582824707e-01 -7.738299667835235596e-02 -6.553400307893753052e-02 1.762609928846359253e-01 -2.005700021982192993e-02 -9.716999717056751251e-03 -5.894900113344192505e-02 1.465570032596588135e-01 -8.646299690008163452e-02 6.585200130939483643e-02 -5.841000005602836609e-03 -2.438200078904628754e-02 -8.115000277757644653e-02 1.019219979643821716e-01 1.049899961799383163e-02 -2.048429995775222778e-01 -1.501930058002471924e-01 -1.636500060558319092e-01 -2.252359986305236816e-01 -9.940999560058116913e-03 -5.436199903488159180e-02 4.484000056982040405e-02 -5.943100154399871826e-02 1.781989932060241699e-01 7.648000027984380722e-03 -2.291679978370666504e-01 2.411970049142837524e-01 -3.315000096336007118e-03 -9.079999872483313084e-04 -3.070000000298023224e-02 2.027689963579177856e-01 -1.124359965324401855e-01 1.612569987773895264e-01 -6.350100040435791016e-02 -1.468649953603744507e-01 2.600109875202178955e-01 8.589600026607513428e-02 1.734150052070617676e-01 -2.684899978339672089e-02 -4.283080101013183594e-01 -1.528590023517608643e-01 2.069099992513656616e-02 3.834640085697174072e-01 5.463000014424324036e-02 -5.257999990135431290e-03 -2.184039950370788574e-01 -2.282889932394027710e-01 -3.216260075569152832e-01 7.029599696397781372e-02 -1.623110026121139526e-01 1.617899909615516663e-02 3.598999977111816406e-02 -3.163000103086233139e-03 -1.973620057106018066e-01 -7.133799791336059570e-02 8.935599774122238159e-02 4.806699976325035095e-02 4.337299987673759460e-02 1.170170009136199951e-01 -1.219969987869262695e-01 -9.943699836730957031e-02 -1.513099968433380127e-01 3.530399873852729797e-02 -4.614499956369400024e-02 -1.805129945278167725e-01 -4.649839997291564941e-01 2.641209959983825684e-01 -7.192700356245040894e-02 1.598909944295883179e-01 1.309960037469863892e-01 -6.966800242662429810e-02 1.593800075352191925e-02 -1.286700069904327393e-01 -1.288949996232986450e-01 -1.626749932765960693e-01 6.491900235414505005e-02 -1.477639973163604736e-01 1.241279989480972290e-01 5.153299868106842041e-02 3.500400111079216003e-02 -1.440050005912780762e-01 6.363400071859359741e-02 1.035429984331130981e-01 1.102589964866638184e-01 -6.680600345134735107e-02 -1.536499988287687302e-02 1.572130024433135986e-01 -1.650419980287551880e-01 3.715299814939498901e-02 -1.576299965381622314e-02 -7.885999977588653564e-03 2.002159953117370605e-01 -7.895000278949737549e-02 -1.612789928913116455e-01 7.310199737548828125e-02 -8.984500169754028320e-02 -1.122030019760131836e-01 -1.689499989151954651e-02 1.596270054578781128e-01 -1.684450060129165649e-01 -1.250299960374832153e-01 2.099799960851669312e-01 4.076899960637092590e-02 2.343659996986389160e-01 -1.885109990835189819e-01 6.762199848890304565e-02 2.347780019044876099e-01 -4.725300148129463196e-02 -6.668999791145324707e-03 -1.325590014457702637e-01 6.990000256337225437e-04 1.702309995889663696e-01 2.668300084769725800e-02 1.705140024423599243e-01 -1.092599984258413315e-02 1.312669962644577026e-01 -1.158310025930404663e-01 1.740480065345764160e-01 1.180700026452541351e-02 7.366500049829483032e-02 -1.956039965152740479e-01 1.259530037641525269e-01 -2.041860073804855347e-01 -2.741999924182891846e-03 1.259579956531524658e-01 -7.056599855422973633e-02 -7.559999823570251465e-03 1.917420029640197754e-01 -1.005069985985755920e-01 -4.332299903035163879e-02 2.205800078809261322e-02 -2.396160066127777100e-01 6.221599876880645752e-02 2.123430073261260986e-01 -1.442960053682327271e-01 1.475100032985210419e-02 8.519200235605239868e-02 1.675499975681304932e-02 -6.589700281620025635e-02 7.104700058698654175e-02 2.086389958858489990e-01 3.553700074553489685e-02 1.780699938535690308e-02 1.645810008049011230e-01 -1.840499974787235260e-02 1.841500028967857361e-02 1.613669991493225098e-01 -8.376400172710418701e-02 -1.298629939556121826e-01 -1.526200026273727417e-02 -1.408009976148605347e-01 3.275009989738464355e-01 1.385699957609176636e-02 -1.543049961328506470e-01 1.898069977760314941e-01 -8.792400360107421875e-02 2.667899988591670990e-02 -2.304240018129348755e-01 -1.634790003299713135e-01 -8.432500064373016357e-02 -1.136749982833862305e-01 2.251529991626739502e-01 7.991000078618526459e-03 6.202799826860427856e-02 -4.961099848151206970e-02 6.202299892902374268e-02 -2.051379978656768799e-01 -6.469500064849853516e-02 -5.738399922847747803e-02 2.561230063438415527e-01 -3.875999897718429565e-02 -8.593200147151947021e-02 2.262699976563453674e-02 1.075479984283447266e-01 1.413999963551759720e-02 4.406000021845102310e-03 -4.029839932918548584e-01 1.350419968366622925e-01 -7.019300013780593872e-02 2.522439956665039062e-01 -1.144770011305809021e-01 -6.003599986433982849e-02 -1.513459980487823486e-01 5.675100162625312805e-02 -1.126760020852088928e-01 1.286710053682327271e-01 1.848389953374862671e-01 -1.837619990110397339e-01 9.939099848270416260e-02 2.852799929678440094e-02 -3.715199977159500122e-02 3.534400090575218201e-02 -1.234389990568161011e-01 -1.158400028944015503e-01 6.196200102567672729e-02 1.234100013971328735e-02 -3.004699945449829102e-02 -7.292999885976314545e-03 2.412160038948059082e-01 5.641499906778335571e-02 2.869060039520263672e-01 1.631000079214572906e-02 -1.114469990134239197e-01 1.770250052213668823e-01 -1.522839963436126709e-01 -1.974609941244125366e-01 -1.716800034046173096e-02 -1.318000024184584618e-03 -1.112049967050552368e-01 -1.904589980840682983e-01 6.968200206756591797e-02 2.104710042476654053e-01 1.640810072422027588e-01 -8.919899910688400269e-02 -2.997799962759017944e-02 -6.347200274467468262e-02 -3.766199946403503418e-02 1.278380006551742554e-01 1.360069960355758667e-01 4.891499876976013184e-02 7.199800014495849609e-02 -2.034400030970573425e-02 7.334099709987640381e-02 4.474399983882904053e-02 -4.509900137782096863e-02 8.915700018405914307e-02 1.328639984130859375e-01 -1.516170054674148560e-01 1.339100021868944168e-02 1.286500040441751480e-02 -1.470910012722015381e-01 8.198799937963485718e-02 1.830900087952613831e-02 -3.189999982714653015e-02 -2.266180068254470825e-01 6.275000050663948059e-03 1.054420024156570435e-01 4.245999827980995178e-02 -1.696040034294128418e-01 -1.627009958028793335e-01 -1.926030069589614868e-01 9.463100135326385498e-02 1.579800061881542206e-02 -1.923000067472457886e-02 -4.200600087642669678e-02 -2.393399924039840698e-02 -1.116769984364509583e-01 2.820029854774475098e-01 -8.763000369071960449e-03 5.323800072073936462e-02 -6.600999832153320312e-02 1.282130032777786255e-01 5.064700171351432800e-02 -1.217600032687187195e-01 1.263920068740844727e-01 -3.839299827814102173e-02 -5.363500118255615234e-02 -1.015250012278556824e-01 -2.460799925029277802e-02 1.116480007767677307e-01 -1.015129983425140381e-01 1.692579984664916992e-01 -1.537100039422512054e-02 -1.021390035748481750e-01 -4.521400108933448792e-02 -1.043339967727661133e-01 1.343670040369033813e-01 6.096899881958961487e-02 3.481830060482025146e-01 4.927999898791313171e-02 9.221900254487991333e-02 -2.541900053620338440e-02 -2.022020071744918823e-01 1.114009991288185120e-01 -2.149200066924095154e-02 -1.356370002031326294e-01 -2.295999974012374878e-02 -2.339520007371902466e-01 -1.115899998694658279e-02 8.500099927186965942e-02 2.549199946224689484e-02 1.449200045317411423e-02 -5.313099920749664307e-02 1.110000014305114746e-01 6.314000114798545837e-03 -1.310150027275085449e-01 -1.121850013732910156e-01 -1.982049942016601562e-01 3.275199979543685913e-02 1.767940074205398560e-01 9.490499645471572876e-02 9.364800155162811279e-02\n```\n\n----------------------------------------\n\nTITLE: Word Frequency Data in Tab-Separated Format\nDESCRIPTION: A tab-separated data file containing word entries and their frequency counts. Each line follows the format: ID, word, frequency count.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/head500.noblanks.cor_wordids.txt#2025-04-21_snippet_7\n\nLANGUAGE: text\nCODE:\n```\n12435\tself-propag\t1\n22992\tself-propel\t2\n23713\tself-publish\t1\n...\n```\n\n----------------------------------------\n\nTITLE: Installing Gensim 4.0 Pre-release Using pip\nDESCRIPTION: Command to install the pre-release version of Gensim 4.0 using pip package manager. This allows users to access cutting edge performance and bug fixes before the final release.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/CHANGELOG.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install --pre --upgrade gensim\n```\n\n----------------------------------------\n\nTITLE: Analyzing List of Animal Names\nDESCRIPTION: This snippet contains a list of animal names. The purpose is to process and document this data, which could be used in various applications, such as educational software, animal classification systems, or text analysis related to wildlife. The list includes animals like 'kitten', 'lion', 'puppy', 'mouse', etc.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/ldavowpalwabbit.txt#2025-04-21_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n\"kitten lion puppy mouse cheetah jaguar puppy cat cat lynx cat mouse cheetah cheetah cat lion mouse leopard kitten jaguar kitten mouse lion mouse tiger tiger puppy cat cheetah mouse\"\n```\n\n----------------------------------------\n\nTITLE: Analyzing List of Animals (Again)\nDESCRIPTION: This snippet includes a list of animals, similar to a previous list. It will be analyzed and documented for potential usage in applications like wildlife tracking, educational programs, or language models using animal vocabulary. The animal names include 'leopard', 'tiger', 'jaguar', 'lion', etc.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/ldavowpalwabbit.txt#2025-04-21_snippet_16\n\nLANGUAGE: text\nCODE:\n```\n\"leopard tiger jaguar lion tiger mouse cat leopard cat jaguar cheetah mouse leopard cheetah lion lion mouse puppy leopard cat cat kitten puppy kitten cat cheetah kitten mouse kitten mouse leopard lynx leopard jaguar kitten puppy jaguar leopard leopard mouse tiger leopard jaguar kitten cat lion puppy puppy lion cat mouse cheetah lynx cheetah cheetah cheetah jaguar leopard tiger tiger cheetah kitten cheetah jaguar lynx mouse tiger kitten leopard kitten jaguar cheetah lynx lion cat kitten lynx leopard cat lynx cat tiger cheetah jaguar leopard lion cheetah puppy kitten jaguar lion lynx lynx puppy kitten leopard\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Tutorial Image\nDESCRIPTION: Loads and displays an image related to the similarity queries tutorial using Matplotlib.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_similarity_queries.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimg = mpimg.imread('run_similarity_queries.png')\nimgplot = plt.imshow(img)\n_ = plt.axis('off')\n```\n\n----------------------------------------\n\nTITLE: Defining module documentation for Wikipedia articles conversion in RST\nDESCRIPTION: RST documentation syntax for the 'gensim.scripts.make_wiki_online_nodebug' module, which handles the conversion of Wikipedia dump articles. The automodule directive includes synopsis and various documentation flags.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/scripts/make_wiki_online_nodebug.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n:mod:`scripts.make_wiki_online_nodebug` -- Convert articles from a Wikipedia dump\n=================================================================================\n\n.. automodule:: gensim.scripts.make_wiki_online_nodebug\n    :synopsis: Convert articles from a Wikipedia dump\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Analyzing Second List of Programming Languages\nDESCRIPTION: This snippet contains a list of programming languages.  This can be leveraged for language analysis, tool development, or generating sample language lists for application purposes.  The list includes languages such as 'c', 'csharp', 'go', 'erlang', etc.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/ldavowpalwabbit.txt#2025-04-21_snippet_19\n\nLANGUAGE: text\nCODE:\n```\n\"c csharp go c csharp erlang erlang csharp csharp erlang c java haskell haskell python go c scala csharp erlang c c\"\n```\n\n----------------------------------------\n\nTITLE: Installing Gensim from source\nDESCRIPTION: Commands to install Gensim from a downloaded source tarball, including extracting the archive and installing via pip.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntar -xvzf gensim-X.X.X.tar.gz\ncd gensim-X.X.X/\npip install .\n```\n\n----------------------------------------\n\nTITLE: Tab-delimited Word Frequency Data\nDESCRIPTION: A tab-separated data format showing word IDs, words, and their frequency counts. Each line follows the pattern: ID<tab>word<tab>count\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/head500.noblanks.cor_wordids.txt#2025-04-21_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n18879\tdryden\t2\n16079\tdryland\t1\n18485\tdsc\t1\n13685\tdsch\t1\n...\n```\n\n----------------------------------------\n\nTITLE: Sphinx Autodoc Configuration for Gensim's LDA Module\nDESCRIPTION: Sphinx reStructuredText directive that configures automatic documentation generation for the gensim.models.ldamodel module. It includes settings to document all members, special methods like __getitem__, inherited members, and show the class inheritance hierarchy.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/models/ldamodel.rst#2025-04-21_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. automodule:: gensim.models.ldamodel\n    :synopsis: Latent Dirichlet Allocation\n    :members:\n    :special-members: __getitem__\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Word Vector Representation for 'quattro' in Numerical Format\nDESCRIPTION: This is a 400-dimensional vector representation for the Italian word 'quattro' (meaning 'four'). Each floating-point number represents a dimension in the embedding space, capturing semantic relationships with other words in the corpus.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/IT.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nquattro 4.651600122451782227e-02 4.845299944281578064e-02 1.987499929964542389e-02 -1.681739985942840576e-01 1.387290060520172119e-01 1.352979987859725952e-01 -1.169029995799064636e-01 1.010489985346794128e-01 -2.846200019121170044e-02 9.062699973583221436e-02 -1.938879936933517456e-01 1.164480000734329224e-01 -2.375320047140121460e-01 3.478299826383590698e-02 -9.973999857902526855e-02 1.323499977588653564e-01 -1.406359970569610596e-01 -1.539109945297241211e-01 -1.963170021772384644e-01 -1.440329998731613159e-01 -1.039099972695112228e-02 -2.290180027484893799e-01 1.514200028032064438e-02 9.771999716758728027e-02 -7.012700289487838745e-02 6.482999771833419800e-02 -5.028299987316131592e-02 2.409099973738193512e-02 -8.052100241184234619e-02 1.831500045955181122e-02 -9.932500123977661133e-02 1.712599992752075195e-01 1.464200019836425781e-02 -2.211699932813644409e-01 1.245580017566680908e-01 -1.102840006351470947e-01 2.704999968409538269e-02 1.636089980602264404e-01 2.083790004253387451e-01 -2.045660018920898438e-01 -5.458100140094757080e-02 -2.479399926960468292e-02 1.431539952754974365e-01 1.737370043992996216e-01 4.846800118684768677e-02 7.673499733209609985e-02 2.992700040340423584e-02 -2.780300006270408630e-02 -6.713899970054626465e-02 8.393199741840362549e-02 2.114900052547454834e-01 1.068700011819601059e-02 -9.488999843597412109e-02 -2.646549940109252930e-01 4.673000052571296692e-02 -2.252599969506263733e-02 2.333119958639144897e-01 -2.356660068035125732e-01 -4.605299979448318481e-02 7.708299905061721802e-02 7.057999819517135620e-02 -4.641500115394592285e-02 -1.628759950399398804e-01 -1.158700045198202133e-02 -1.222729980945587158e-01 -5.572000145912170410e-02 1.262609958648681641e-01 -4.997399821877479553e-02 2.318159937858581543e-01 1.248079985380172729e-01 2.089989930391311646e-01 -3.374199941754341125e-02 5.044399946928024292e-02 3.993700072169303894e-02 -1.289319992065429688e-01 -8.900000248104333878e-04 -9.095799922943115234e-02 5.527599900960922241e-02 -7.822699844837188721e-02 -8.878000080585479736e-03 -1.910999999381601810e-03 1.684899926185607910e-01 7.808999717235565186e-02 -8.543999865651130676e-03 -8.697400242090225220e-02 1.018410027027130127e-01 -7.100799679756164551e-02 1.125930026173591614e-01 -1.953130066394805908e-01 4.931300133466720581e-02 -1.395290046930313110e-01 -1.252400036901235580e-02 3.298600018024444580e-02 5.595700070261955261e-02 -1.316680014133453369e-01 -1.009699981659650803e-02 8.893100172281265259e-02 1.273030042648315430e-01 1.707579940557479858e-01 -5.322400107979774475e-02 4.623400047421455383e-02 8.430000161752104759e-04 2.960239946842193604e-01 5.496399849653244019e-02 -2.008499950170516968e-02 1.772999949753284454e-02 -2.712500095367431641e-01 2.177399955689907074e-02 -2.247199974954128265e-02 -4.876799881458282471e-02 -5.900099873542785645e-02 -8.224699646234512329e-02 1.970729976892471313e-01 -1.075130030512809753e-01 3.946779966354370117e-01 -1.005099993199110031e-02 5.089999758638441563e-04 -1.177159994840621948e-01 -8.872699737548828125e-02 -1.885900087654590607e-02 -1.947299949824810028e-02 1.733849942684173584e-01 -7.504399865865707397e-02 -1.434389948844909668e-01 2.851900085806846619e-02 -9.826400130987167358e-02 -1.052279993891716003e-01 1.220389977097511292e-01 2.034610062837600708e-01 -3.322799876332283020e-02 9.280099719762802124e-02 -1.265999954193830490e-02 -1.341360062360763550e-01 1.199999969685450196e-04 5.150000099092721939e-03 4.800000041723251343e-02 -1.127969995141029358e-01 1.071500033140182495e-02 1.190489977598190308e-01 -8.562199771404266357e-02 3.176200017333030701e-02 -6.141500174999237061e-02 9.306299686431884766e-02 1.478150039911270142e-01 -2.016289979219436646e-01 -7.763399928808212280e-02 -1.182959973812103271e-01 1.619119942188262939e-01 6.188299879431724548e-02 2.359430044889450073e-01 1.105640009045600891e-01 1.194700039923191071e-02 -1.690600067377090454e-02 -1.507340073585510254e-01 2.043999964371323586e-03 6.747300177812576294e-02 -1.899860054254531860e-01 -3.283699974417686462e-02 -8.132299780845642090e-02 5.784400179982185364e-02 4.521000199019908905e-03 3.454599902033805847e-02 5.600399896502494812e-02 -5.263499915599822998e-02 -1.622219979763031006e-01 2.612940073013305664e-01 2.263199910521507263e-02 5.358700081706047058e-02 -5.312300100922584534e-02 1.291310042142868042e-01 6.493800133466720581e-02 -3.414199873805046082e-02 2.269459962844848633e-01 -1.695179939270019531e-01 1.547220051288604736e-01 -5.316499993205070496e-02 -8.936399966478347778e-02 -7.587199658155441284e-02 7.596199959516525269e-02 1.298400014638900757e-01 -2.640499919652938843e-02 -6.538800150156021118e-02 -1.518779993057250977e-01 -1.687000039964914322e-03 5.167299881577491760e-02 8.779300004243850708e-02 -2.976699918508529663e-02 1.933100074529647827e-02 -1.820199936628341675e-01 -2.309959977865219116e-01 1.260100025683641434e-02 1.763769984245300293e-01 1.419959962368011475e-01 1.116179972887039185e-01 -2.566199935972690582e-02 -7.155000232160091400e-03 -1.866900026798248291e-01 -1.242889985442161560e-01 5.526300147175788879e-02 -5.331299826502799988e-02 3.601740002632141113e-01 1.010740026831626892e-01 -2.065059989690780640e-01 -5.882599949836730957e-02 4.512599855661392212e-02 4.549799859523773193e-02 1.703979969024658203e-01 -1.111700013279914856e-01 -2.825400047004222870e-02 1.943109929561614990e-01 9.592200070619583130e-02 2.015729993581771851e-01 1.117149963974952698e-01 5.147499963641166687e-02 4.517999943345785141e-03 -2.872800081968307495e-02 -5.835900083184242249e-02 2.866590023040771484e-01 -1.715330034494400024e-01 -1.691280007362365723e-01 1.338350027799606323e-01 -1.624300032854080200e-01 1.164690032601356506e-01 1.462949961423873901e-01 2.138559967279434204e-01 9.382999688386917114e-02 -5.995199829339981079e-02 -2.434399910271167755e-02 -5.183700099587440491e-02 8.509399741888046265e-02 1.305519938468933105e-01 -5.640999972820281982e-02 2.877900004386901855e-02 6.235000118613243103e-02 -4.017399996519088745e-02 2.296569943428039551e-01 1.621000003069639206e-03 1.467639952898025513e-01 -3.294400125741958618e-02 6.983099877834320068e-02 5.198799818754196167e-02 7.493899762630462646e-02 -1.235660016536712646e-01 3.379999892786145210e-03 9.490799903869628906e-02 1.618400029838085175e-02 2.251040041446685791e-01 8.565700054168701172e-02 4.591599851846694946e-02 -1.120890006422996521e-01 5.342000164091587067e-03 9.647899866104125977e-02 4.800000169780105352e-05 1.798779964447021484e-01 1.353570073843002319e-01 7.412599772214889526e-02 1.689199917018413544e-02 4.608500003814697266e-02 -1.234209984540939331e-01 -1.660400070250034332e-02 -2.835300005972385406e-02 -1.895730048418045044e-01 -4.729500040411949158e-02 1.823000027798116207e-03 -2.643460035324096680e-01 -2.761839926242828369e-01 2.096650004386901855e-01 1.924889981746673584e-01 2.308199927210807800e-02 1.148839965462684631e-01 -1.214910000562667847e-01 1.023769974708557129e-01 -4.508199915289878845e-02 1.843860000371932983e-01 1.141370013356208801e-01 3.002609908580780029e-01 -9.392599761486053467e-02 2.460899949073791504e-02 -6.654000282287597656e-02 1.911430060863494873e-01 -1.399799995124340057e-02 8.718899637460708618e-02 1.866730004549026489e-01 -1.681100018322467804e-02 3.833900019526481628e-02 -1.348000019788742065e-03 -2.085099928081035614e-02 3.513999981805682182e-03 -4.861300066113471985e-02 2.177900075912475586e-02 -2.040000073611736298e-02 2.063200064003467560e-02 1.316689997911453247e-01 2.398129999637603760e-01 1.612419933080673218e-01 -4.454100131988525391e-02 -9.627199918031692505e-02 -2.779360115528106689e-01 7.444500178098678589e-02 2.818489968776702881e-01\n```\n\n----------------------------------------\n\nTITLE: Word Vector Representation for 'seven'\nDESCRIPTION: This snippet shows the 300-dimensional vector representation for the word 'seven' in a word embedding model. Each number represents a dimension in the embedding space, capturing semantic and syntactic properties of the word.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/EN.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nseven -2.243199944496154785e-02 1.085729971528053284e-01 -7.895100116729736328e-02 -1.354880034923553467e-01 -9.380999952554702759e-03 1.983869969844818115e-01 8.388700336217880249e-02 -5.792199820280075073e-02 1.257629990577697754e-01 -8.846600353717803955e-02 -3.950000100303441286e-04 2.561219930648803711e-01 -1.624699980020523071e-01 1.459040045738220215e-01 5.567799881100654602e-02 -7.611600309610366821e-02 2.672700025141239166e-02 -2.219399996101856232e-02 9.414599835872650146e-02 -1.131360009312629700e-01 -1.073900014162063599e-01 -2.440500073134899139e-02 -5.653800070285797119e-02 2.095299959182739258e-02 1.850579977035522461e-01 5.490300059318542480e-02 -2.064740061759948730e-01 1.048580035567283630e-01 9.444999694824218750e-02 2.513999938964843750e-01 -2.098769992589950562e-01 5.883799865841865540e-02 1.606619954109191895e-01 -5.610999837517738342e-02 3.017899952828884125e-02 -4.324800148606300354e-02 3.412500023841857910e-02 1.102849990129470825e-01 2.050100080668926239e-02 1.930199936032295227e-02 -2.755500003695487976e-02 -7.937700301408767700e-02 -9.019999951124191284e-02 -3.366199880838394165e-02 2.268480062484741211e-01 5.225700139999389648e-02 3.490419983863830566e-01 1.384020000696182251e-01 8.339399844408035278e-02 -2.178660035133361816e-01 2.668820023536682129e-01 -2.623820006847381592e-01 -1.208769977092742920e-01 -1.395130008459091187e-01 1.301749944686889648e-01 1.858869940042495728e-01 1.654939949512481689e-01 -1.067299954593181610e-02 1.100500021129846573e-02 1.251710057258605957e-01 -8.635099977254867554e-02 -2.782100066542625427e-02 1.960950046777725220e-01 -2.773699909448623657e-02 1.765000075101852417e-02 9.026899933815002441e-02 1.518449932336807251e-01 3.244199976325035095e-02 1.427969932556152344e-01 -3.048799932003021240e-02 -6.245800107717514038e-02 1.730900071561336517e-02 -4.701500013470649719e-02 -4.907799884676933289e-02 2.771129906177520752e-01 -1.440259963274002075e-01 -2.677300013601779938e-02 -1.361249983310699463e-01 -1.811639964580535889e-01 4.561710059642791748e-01 -1.934739947319030762e-01 -1.443700026720762253e-02 -1.912789940834045410e-01 2.088990062475204468e-01 1.490500010550022125e-02 7.295300066471099854e-02 -1.733009964227676392e-01 1.759440004825592041e-01 -5.758599936962127686e-02 -1.188990026712417603e-01 1.243000035174190998e-03 4.907400161027908325e-02 -6.731200218200683594e-02 -1.199460029602050781e-01 -6.899599730968475342e-02 -1.364859938621520996e-01 1.366499997675418854e-02 -7.134199887514114380e-02 -4.794799908995628357e-02 -7.410299777984619141e-02 -2.978740036487579346e-01 -1.168999969959259033e-01 -5.801900103688240051e-02 -7.259099930524826050e-02 -5.465399846434593201e-02 -1.273099984973669052e-02 1.917729973793029785e-01 1.974499970674514771e-02 1.697639971971511841e-01 -1.212740018963813782e-01 -1.868519932031631470e-01 -1.463199965655803680e-02 2.179799973964691162e-02 -4.616999998688697815e-02 -6.038400158286094666e-02 6.835900247097015381e-02 -2.886999957263469696e-02 -1.095100026577711105e-02 -1.138199958950281143e-02 -7.734999805688858032e-02 1.154830008745193481e-01 1.461140066385269165e-01 3.509400039911270142e-02 6.979499757289886475e-02 9.479899704456329346e-02 -2.324209958314895630e-01 -6.741700321435928345e-02 1.996699906885623932e-02 1.418610066175460815e-01 7.507800310850143433e-02 1.785710006952285767e-01 6.264500319957733154e-02 7.130400091409683228e-02 -1.007200032472610474e-01 -7.613900303840637207e-02 2.840499952435493469e-02 1.966799981892108917e-02 -6.894499808549880981e-02 1.769799925386905670e-02 -9.941700100898742676e-02 -2.621859908103942871e-01 3.576000034809112549e-02 -6.568399816751480103e-02 -1.366209983825683594e-01 -1.156909987330436707e-01 -5.324300006031990051e-02 -2.089190036058425903e-01 -1.128299999982118607e-02 2.586849927902221680e-01 1.777610033750534058e-01 -4.726900160312652588e-02 -3.389999968931078911e-03 -2.053450047969818115e-01 -1.480500027537345886e-02 2.820590138435363770e-01 8.630800247192382812e-02 1.514299958944320679e-02 -1.190079972147941589e-01 -1.286550015211105347e-01 1.718879938125610352e-01 5.495600029826164246e-02 -1.281380057334899902e-01 1.134549975395202637e-01 -2.525070011615753174e-01 9.139300137758255005e-02 7.762599736452102661e-02 1.449840068817138672e-01 1.090259999036788940e-01 9.178300201892852783e-02 -1.649210005998611450e-01 -4.017899930477142334e-02 -8.476799726486206055e-02 -2.124319970607757568e-01 -6.863400340080261230e-02 1.333860009908676147e-01 -4.354000091552734375e-02 4.727400094270706177e-02 2.556209862232208252e-01 1.540630012750625610e-01 -1.057799998670816422e-02 -2.098210006952285767e-01 4.460699856281280518e-02 3.219500184059143066e-02 8.155599981546401978e-02 -2.540000132285058498e-04 1.497640013694763184e-01 -2.404999919235706329e-02 -7.587300240993499756e-02 -3.540699928998947144e-02 1.243479996919631958e-01 5.544799938797950745e-02 -2.458699978888034821e-02 1.672669947147369385e-01 -5.101799964904785156e-01 2.802399918437004089e-02 1.682000048458576202e-02 7.888200134038925171e-02 -4.507900029420852661e-02 1.505320072174072266e-01 1.193350031971931458e-01 3.375700116157531738e-02 1.173549965023994446e-01 -8.057200163602828979e-02 2.497000060975551605e-02 9.403999894857406616e-03 -1.105360016226768494e-01 2.115589976310729980e-01 -6.637699902057647705e-02 -4.362599924206733704e-02 8.022899925708770752e-02 1.174900028854608536e-02 -5.289600044488906860e-02 -3.065899945795536041e-02 -6.478100270032882690e-02 1.915079951286315918e-01 -1.541139930486679077e-01 5.049100145697593689e-02 -1.985339969396591187e-01 -1.001719981431961060e-01 -2.051440030336380005e-01 4.812100157141685486e-02 7.739199697971343994e-02 3.152459859848022461e-01 1.277299970388412476e-02 1.395499985665082932e-02 2.089000074192881584e-03 5.075599998235702515e-02 -5.648500099778175354e-02 1.725350022315979004e-01 -1.975789964199066162e-01 8.319299668073654175e-02 7.258699834346771240e-02 1.091379970312118530e-01 6.976400315761566162e-02 -1.990280002355575562e-01 -1.611669957637786865e-01 -6.420399993658065796e-02 2.601180076599121094e-01 -1.222949996590614319e-01 8.055999875068664551e-02 -4.248699918389320374e-02 3.958600014448165894e-02 7.039699703454971313e-02 -1.524489969015121460e-01 -5.304199829697608948e-02 1.656849980354309082e-01 5.622300133109092712e-02 3.165899962186813354e-02 2.178000053390860558e-03 -1.852600090205669403e-02 7.047999650239944458e-02 -7.363600283861160278e-02 1.368550062179565430e-01 -6.348600238561630249e-02 -5.254700034856796265e-02 -1.200440004467964172e-01 -2.152470052242279053e-01 5.255999974906444550e-03 2.409030050039291382e-01 2.772859930992126465e-01 6.190799921751022339e-02 7.488999981433153152e-03 -1.517540067434310913e-01 2.812499925494194031e-02 1.298310011625289917e-01 3.546949923038482666e-01 1.802100054919719696e-02 -1.353009939193725586e-01 6.736399978399276733e-02 -3.348999889567494392e-03 -5.784599855542182922e-02 3.730500116944313049e-02 -1.907929927110671997e-01 -6.792999804019927979e-02 -1.147060021758079529e-01 -1.381320059299468994e-01 -1.176400016993284225e-02 1.654269993305206299e-01 -2.990199998021125793e-02 -1.853200048208236694e-01 -1.500010043382644653e-01 1.139670014381408691e-01 -2.005100063979625702e-02 1.050940006971359253e-01 2.407529950141906738e-01 -2.307900041341781616e-01 -1.897370070219039917e-01 -2.104499936103820801e-02 3.541199862957000732e-02 -2.812400087714195251e-02 1.102129966020584106e-01 5.201999843120574951e-02 1.603430062532424927e-01 -9.340099990367889404e-02 -8.282399922609329224e-02 1.597719937562942505e-01 -6.164300069212913513e-02 1.355659961700439453e-01 -5.900299921631813049e-02 2.718299999833106995e-02\n```\n\n----------------------------------------\n\nTITLE: Sphinx Documentation for UCI Corpus Module\nDESCRIPTION: ReStructuredText documentation configuration that exposes the gensim.corpora.ucicorpus module documentation including all members, inherited members, undocumented members and inheritance tree.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/corpora/ucicorpus.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n:mod:`corpora.ucicorpus` -- Corpus in UCI format\n================================================\n\n.. automodule:: gensim.corpora.ucicorpus\n    :synopsis: Corpus in UCI format\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Analyzing Fifth List of Names\nDESCRIPTION: This snippet contains a list of names, used for data processing and documentation for applications that use sample names, such as software testing and demographic analysis. The list of names include 'bob', 'harry', 'alex', 'tim', etc.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/ldavowpalwabbit.txt#2025-04-21_snippet_14\n\nLANGUAGE: text\nCODE:\n```\n\"bob harry alex tim robert dave alex rachel sue rachel robert jim sue bob alex tim dave jim alice bob robert robert jim sue alex rachel harry alex alice jim robert\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Wrappers\nDESCRIPTION: Sets up paths for Vowpal Wabbit and Mallet installations to demonstrate wrapper support.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence_tutorial.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nvw_path = '/usr/local/bin/vw'\n\nhome = os.path.expanduser('~')\nmallet_path = os.path.join(home, 'mallet-2.0.8', 'bin', 'mallet')\n```\n\n----------------------------------------\n\nTITLE: Sphinx RST Table of Execution Times\nDESCRIPTION: A reStructuredText table displaying execution times and memory usage for Gensim example scripts, including run_doc2vec_imdb.py, run_compare_lda.py, run_doc.py, and run_downloader_api.py.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/howtos/sg_execution_times.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n+----------------------------------------------------------------------------------------+-----------+-----------+\n| :ref:`sphx_glr_auto_examples_howtos_run_doc2vec_imdb.py` (``run_doc2vec_imdb.py``)     | 56:58.813 | 3772.5 MB |\n+----------------------------------------------------------------------------------------+-----------+-----------+\n| :ref:`sphx_glr_auto_examples_howtos_run_compare_lda.py` (``run_compare_lda.py``)       | 00:00.000 | 0.0 MB    |\n+----------------------------------------------------------------------------------------+-----------+-----------+\n| :ref:`sphx_glr_auto_examples_howtos_run_doc.py` (``run_doc.py``)                       | 00:00.000 | 0.0 MB    |\n+----------------------------------------------------------------------------------------+-----------+-----------+\n| :ref:`sphx_glr_auto_examples_howtos_run_downloader_api.py` (``run_downloader_api.py``) | 00:00.000 | 0.0 MB    |\n+----------------------------------------------------------------------------------------+-----------+-----------+\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure\nDESCRIPTION: ReStructuredText markup defining the tutorial documentation structure with image thumbnails and links to individual tutorials.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/index.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _sphx_glr_auto_examples_tutorials:\n\nTutorials: Learning Oriented Lessons\n------------------------------------\n\nLearning-oriented lessons that introduce a particular gensim feature, e.g. a model (Word2Vec, FastText) or technique (similarity queries or text summarization).\n```\n\n----------------------------------------\n\nTITLE: Word Vector Representation for 'maiale'\nDESCRIPTION: A 300-dimensional word vector embedding representing the word 'maiale'. Each number represents a coordinate in the multidimensional semantic space learned by the word embedding model.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/IT.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_9\n\nLANGUAGE: txt\nCODE:\n```\nmaiale -2.038999944925308228e-01 6.999440193176269531e-01 3.538599982857704163e-02 -2.776629924774169922e-01 [...continues]\n```\n\n----------------------------------------\n\nTITLE: Analyzing Fifth List of Food Items\nDESCRIPTION: This snippet consists of another list of food items. This data is analyzed to facilitate applications such as menu planning, inventory management, or food analysis. The data consists of terms such as 'beans', 'sausages', 'mushrooms', 'eggs', etc.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/ldavowpalwabbit.txt#2025-04-21_snippet_10\n\nLANGUAGE: text\nCODE:\n```\n\"beans sausages sausages sausages mushrooms eggs coffee beans sausages tea sausages cereal mushrooms tea ham eggs bacon ham bacon bacon bacon eggs beans beans juice tea beans beans mushrooms tea juice mushrooms cereal juice tea sausages bacon eggs juice juice eggs ham eggs ham sausages beans mushrooms\"\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure\nDESCRIPTION: Sphinx documentation table of contents tree structure defining the organization of Gensim's API reference documentation. Lists all available modules and their nested structure.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/apiref.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _apiref:\n\nAPI Reference\n=============\n\nModules:\n\n.. toctree::\n    :maxdepth: 0\n\n    interfaces\n    utils\n    matutils\n    downloader\n    corpora/bleicorpus\n    corpora/csvcorpus\n    corpora/dictionary\n    corpora/hashdictionary\n    corpora/indexedcorpus\n    corpora/lowcorpus\n    corpora/malletcorpus\n    corpora/mmcorpus\n    corpora/opinosiscorpus\n    corpora/sharded_corpus\n    corpora/svmlightcorpus\n    corpora/textcorpus\n    corpora/ucicorpus\n    corpora/wikicorpus\n    models/ldamodel\n    models/ldamulticore\n    models/ensemblelda\n    models/nmf\n    models/lsimodel\n    models/ldaseqmodel\n    models/tfidfmodel\n    models/rpmodel\n    models/hdpmodel\n    models/logentropy_model\n    models/normmodel\n    models/translation_matrix\n    models/lsi_dispatcher\n    models/lsi_worker\n    models/lda_dispatcher\n    models/lda_worker\n    models/atmodel\n    models/word2vec\n    models/keyedvectors\n    models/doc2vec\n    models/fasttext\n    models/_fasttext_bin\n    models/phrases\n    models/poincare\n    models/coherencemodel\n    models/basemodel\n    models/callbacks\n    models/word2vec_inner\n    models/doc2vec_inner\n    models/fasttext_inner\n    similarities/docsim\n    similarities/termsim\n    similarities/annoy\n    similarities/nmslib\n    similarities/levenshtein\n    similarities/fastss\n    test/utils\n    topic_coherence/aggregation\n    topic_coherence/direct_confirmation_measure\n    topic_coherence/indirect_confirmation_measure\n    topic_coherence/probability_estimation\n    topic_coherence/segmentation\n    topic_coherence/text_analysis\n    scripts/package_info\n    scripts/glove2word2vec\n    scripts/make_wikicorpus\n    scripts/word2vec_standalone\n    scripts/make_wiki_online\n    scripts/make_wiki_online_nodebug\n    scripts/word2vec2tensor\n    scripts/segment_wiki\n    parsing/porter\n    parsing/preprocessing\n```\n\n----------------------------------------\n\nTITLE: Analyzing List of Food Items\nDESCRIPTION: This snippet contains a list of food items. The aim is to analyze and document this data, suitable for applications like recipe databases, nutritional analysis software, or text analysis of culinary content. The food items include 'bacon', 'cereal', 'beans', 'sausages', etc.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/ldavowpalwabbit.txt#2025-04-21_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n\"bacon cereal beans sausages ham sausages tea bacon coffee cereal bacon beans beans cereal cereal bacon eggs beans tea ham coffee mushrooms tea tea bacon bacon eggs mushrooms cereal bacon tea juice cereal coffee bacon juice mushrooms juice juice juice ham eggs mushrooms tea bacon bacon eggs bacon sausages beans cereal juice eggs sausages coffee beans bacon juice bacon ham ham sausages eggs sausages tea cereal eggs mushrooms juice coffee beans bacon mushrooms cereal beans bacon mushrooms ham juice coffee cereal cereal tea tea bacon sausages bacon eggs tea mushrooms sausages cereal beans mushrooms mushrooms ham juice eggs coffee\"\n```\n\n----------------------------------------\n\nTITLE: Displaying LDA Topic Model Results with Keyword Distributions in Gensim Log Output\nDESCRIPTION: Log output from a Gensim LDA topic modeling process showing the evolution of topics across multiple iterations. Each topic is represented by its top 10 keywords with associated probability weights, and the optimization process includes alpha value refinement and topic convergence metrics.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_lda.rst#2025-04-21_snippet_10\n\nLANGUAGE: log\nCODE:\n```\n2022-04-22 17:44:23,825 : INFO : topic #7 (0.033): 0.013*\"chip\" + 0.012*\"circuit\" + 0.011*\"analog\" + 0.010*\"neuron\" + 0.008*\"signal\" + 0.007*\"memory\" + 0.007*\"voltage\" + 0.006*\"vlsi\" + 0.005*\"noise\" + 0.005*\"implementation\"\n2022-04-22 17:44:23,825 : INFO : topic #9 (0.033): 0.021*\"neuron\" + 0.014*\"spike\" + 0.014*\"cell\" + 0.011*\"stimulus\" + 0.010*\"signal\" + 0.010*\"response\" + 0.008*\"firing\" + 0.007*\"noise\" + 0.006*\"channel\" + 0.006*\"frequency\"\n2022-04-22 17:44:23,826 : INFO : topic #4 (0.051): 0.009*\"matrix\" + 0.006*\"gradient\" + 0.006*\"solution\" + 0.005*\"convergence\" + 0.004*\"distance\" + 0.004*\"let\" + 0.004*\"minimum\" + 0.004*\"eq\" + 0.004*\"optimization\" + 0.003*\"descent\"\n2022-04-22 17:44:23,826 : INFO : topic #8 (0.053): 0.008*\"rule\" + 0.006*\"hidden\" + 0.005*\"generalization\" + 0.004*\"hidden_unit\" + 0.004*\"sequence\" + 0.004*\"net\" + 0.004*\"prediction\" + 0.004*\"tree\" + 0.003*\"machine\" + 0.003*\"trained\"\n2022-04-22 17:44:23,826 : INFO : topic #3 (0.061): 0.007*\"class\" + 0.006*\"gaussian\" + 0.005*\"classifier\" + 0.005*\"sample\" + 0.005*\"estimate\" + 0.005*\"classification\" + 0.004*\"density\" + 0.004*\"prior\" + 0.004*\"mixture\" + 0.004*\"bayesian\"\n2022-04-22 17:44:23,827 : INFO : topic diff=0.163348, rho=0.288675\n2022-04-22 17:44:23,834 : INFO : PROGRESS: pass 11, at document #1740/1740\n2022-04-22 17:44:29,135 : INFO : optimized alpha [0.040188633, 0.03646946, 0.038880475, 0.06112813, 0.05245481, 0.04061286, 0.047049697, 0.03229136, 0.05290524, 0.03296597]\n2022-04-22 17:44:29,141 : INFO : topic #7 (0.032): 0.013*\"chip\" + 0.013*\"circuit\" + 0.011*\"analog\" + 0.010*\"neuron\" + 0.008*\"signal\" + 0.007*\"memory\" + 0.007*\"voltage\" + 0.006*\"vlsi\" + 0.005*\"noise\" + 0.005*\"implementation\"\n2022-04-22 17:44:29,141 : INFO : topic #9 (0.033): 0.021*\"neuron\" + 0.014*\"spike\" + 0.014*\"cell\" + 0.011*\"signal\" + 0.011*\"stimulus\" + 0.010*\"response\" + 0.008*\"firing\" + 0.007*\"noise\" + 0.006*\"frequency\" + 0.006*\"channel\"\n2022-04-22 17:44:29,142 : INFO : topic #4 (0.052): 0.009*\"matrix\" + 0.006*\"gradient\" + 0.006*\"solution\" + 0.005*\"convergence\" + 0.004*\"distance\" + 0.004*\"let\" + 0.004*\"minimum\" + 0.004*\"eq\" + 0.004*\"optimization\" + 0.003*\"optimal\"\n2022-04-22 17:44:29,142 : INFO : topic #8 (0.053): 0.008*\"rule\" + 0.006*\"hidden\" + 0.005*\"generalization\" + 0.004*\"hidden_unit\" + 0.004*\"sequence\" + 0.004*\"prediction\" + 0.004*\"net\" + 0.004*\"tree\" + 0.003*\"machine\" + 0.003*\"trained\"\n2022-04-22 17:44:29,142 : INFO : topic #3 (0.061): 0.007*\"class\" + 0.006*\"gaussian\" + 0.005*\"classifier\" + 0.005*\"sample\" + 0.005*\"estimate\" + 0.005*\"classification\" + 0.005*\"density\" + 0.004*\"prior\" + 0.004*\"mixture\" + 0.004*\"bayesian\"\n2022-04-22 17:44:29,142 : INFO : topic diff=0.153485, rho=0.277350\n2022-04-22 17:44:29,150 : INFO : PROGRESS: pass 12, at document #1740/1740\n2022-04-22 17:44:33,545 : INFO : optimized alpha [0.04036388, 0.03635188, 0.038611963, 0.061483774, 0.05345723, 0.040894084, 0.04736741, 0.03211178, 0.05297828, 0.03274891]\n2022-04-22 17:44:33,551 : INFO : topic #7 (0.032): 0.013*\"circuit\" + 0.013*\"chip\" + 0.011*\"analog\" + 0.010*\"neuron\" + 0.008*\"signal\" + 0.007*\"voltage\" + 0.007*\"memory\" + 0.006*\"vlsi\" + 0.005*\"implementation\" + 0.005*\"noise\"\n2022-04-22 17:44:33,552 : INFO : topic #9 (0.033): 0.021*\"neuron\" + 0.014*\"spike\" + 0.014*\"cell\" + 0.011*\"signal\" + 0.011*\"stimulus\" + 0.011*\"response\" + 0.009*\"firing\" + 0.007*\"noise\" + 0.006*\"frequency\" + 0.006*\"channel\"\n2022-04-22 17:44:33,552 : INFO : topic #8 (0.053): 0.008*\"rule\" + 0.006*\"hidden\" + 0.006*\"generalization\" + 0.004*\"hidden_unit\" + 0.004*\"sequence\" + 0.004*\"prediction\" + 0.004*\"net\" + 0.004*\"tree\" + 0.003*\"machine\" + 0.003*\"trained\"\n2022-04-22 17:44:33,552 : INFO : topic #4 (0.053): 0.009*\"matrix\" + 0.006*\"gradient\" + 0.006*\"solution\" + 0.005*\"convergence\" + 0.004*\"distance\" + 0.004*\"let\" + 0.004*\"minimum\" + 0.004*\"eq\" + 0.003*\"optimization\" + 0.003*\"optimal\"\n2022-04-22 17:44:33,552 : INFO : topic #3 (0.061): 0.007*\"class\" + 0.006*\"gaussian\" + 0.005*\"sample\" + 0.005*\"classifier\" + 0.005*\"estimate\" + 0.005*\"density\" + 0.005*\"classification\" + 0.004*\"prior\" + 0.004*\"mixture\" + 0.004*\"bayesian\"\n2022-04-22 17:44:33,553 : INFO : topic diff=0.143831, rho=0.267261\n2022-04-22 17:44:33,562 : INFO : PROGRESS: pass 13, at document #1740/1740\n2022-04-22 17:44:39,235 : INFO : optimized alpha [0.040587135, 0.03631959, 0.03839379, 0.061911535, 0.05453887, 0.041285977, 0.047773384, 0.032027513, 0.05315258, 0.03261802]\n2022-04-22 17:44:39,246 : INFO : topic #7 (0.032): 0.014*\"circuit\" + 0.013*\"chip\" + 0.011*\"analog\" + 0.010*\"neuron\" + 0.008*\"signal\" + 0.008*\"voltage\" + 0.007*\"memory\" + 0.006*\"vlsi\" + 0.005*\"implementation\" + 0.005*\"noise\"\n2022-04-22 17:44:39,246 : INFO : topic #9 (0.033): 0.021*\"neuron\" + 0.014*\"spike\" + 0.014*\"cell\" + 0.011*\"signal\" + 0.011*\"stimulus\" + 0.011*\"response\" + 0.009*\"firing\" + 0.007*\"noise\" + 0.007*\"frequency\" + 0.006*\"channel\"\n2022-04-22 17:44:39,247 : INFO : topic #8 (0.053): 0.008*\"rule\" + 0.006*\"hidden\" + 0.006*\"generalization\" + 0.004*\"hidden_unit\" + 0.004*\"sequence\" + 0.004*\"prediction\" + 0.004*\"net\" + 0.004*\"tree\" + 0.004*\"machine\" + 0.003*\"trained\"\n2022-04-22 17:44:39,247 : INFO : topic #4 (0.055): 0.009*\"matrix\" + 0.007*\"gradient\" + 0.006*\"solution\" + 0.005*\"convergence\" + 0.004*\"distance\" + 0.004*\"let\" + 0.004*\"minimum\" + 0.004*\"eq\" + 0.003*\"optimization\" + 0.003*\"optimal\"\n2022-04-22 17:44:39,247 : INFO : topic #3 (0.062): 0.007*\"class\" + 0.006*\"gaussian\" + 0.005*\"sample\" + 0.005*\"classifier\" + 0.005*\"estimate\" + 0.005*\"density\" + 0.005*\"classification\" + 0.004*\"prior\" + 0.004*\"mixture\" + 0.004*\"bayesian\"\n2022-04-22 17:44:39,248 : INFO : topic diff=0.134602, rho=0.258199\n2022-04-22 17:44:39,258 : INFO : PROGRESS: pass 14, at document #1740/1740\n2022-04-22 17:44:46,319 : INFO : optimized alpha [0.040821876, 0.036360793, 0.03824259, 0.062456302, 0.055688635, 0.041737743, 0.048259463, 0.032020763, 0.05343126, 0.03254091]\n2022-04-22 17:44:46,325 : INFO : topic #7 (0.032): 0.014*\"circuit\" + 0.013*\"chip\" + 0.012*\"analog\" + 0.010*\"neuron\" + 0.008*\"signal\" + 0.008*\"voltage\" + 0.007*\"memory\" + 0.006*\"vlsi\" + 0.005*\"implementation\" + 0.005*\"noise\"\n2022-04-22 17:44:46,326 : INFO : topic #9 (0.033): 0.021*\"neuron\" + 0.014*\"spike\" + 0.014*\"cell\" + 0.011*\"signal\" + 0.011*\"response\" + 0.011*\"stimulus\" + 0.009*\"firing\" + 0.007*\"noise\" + 0.007*\"frequency\" + 0.006*\"channel\"\n2022-04-22 17:44:46,327 : INFO : topic #8 (0.053): 0.008*\"rule\" + 0.006*\"hidden\" + 0.006*\"generalization\" + 0.004*\"hidden_unit\" + 0.004*\"sequence\" + 0.004*\"prediction\" + 0.004*\"net\" + 0.004*\"tree\" + 0.004*\"machine\" + 0.003*\"trained\"\n2022-04-22 17:44:46,327 : INFO : topic #4 (0.056): 0.009*\"matrix\" + 0.007*\"gradient\" + 0.006*\"solution\" + 0.005*\"convergence\" + 0.004*\"distance\" + 0.004*\"let\" + 0.004*\"minimum\" + 0.004*\"eq\" + 0.004*\"optimal\" + 0.003*\"optimization\"\n2022-04-22 17:44:46,327 : INFO : topic #3 (0.062): 0.007*\"class\" + 0.006*\"gaussian\" + 0.005*\"sample\" + 0.005*\"classifier\" + 0.005*\"estimate\" + 0.005*\"density\" + 0.004*\"mixture\" + 0.004*\"classification\" + 0.004*\"prior\" + 0.004*\"bayesian\"\n2022-04-22 17:44:46,328 : INFO : topic diff=0.125871, rho=0.250000\n2022-04-22 17:44:46,338 : INFO : PROGRESS: pass 15, at document #1740/1740\n2022-04-22 17:44:53,655 : INFO : optimized alpha [0.04109236, 0.036467522, 0.0381424, 0.06306473, 0.056903645, 0.04227092, 0.04874864, 0.032058466, 0.053792715, 0.03251973]\n2022-04-22 17:44:53,666 : INFO : topic #7 (0.032): 0.014*\"circuit\" + 0.013*\"chip\" + 0.012*\"analog\" + 0.010*\"neuron\" + 0.008*\"signal\" + 0.008*\"voltage\" + 0.007*\"memory\" + 0.006*\"vlsi\" + 0.005*\"implementation\" + 0.005*\"bit\"\n2022-04-22 17:44:53,666 : INFO : topic #9 (0.033): 0.021*\"neuron\" + 0.014*\"spike\" + 0.014*\"cell\" + 0.011*\"signal\" + 0.011*\"response\" + 0.011*\"stimulus\" + 0.009*\"firing\" + 0.007*\"frequency\" + 0.007*\"noise\" + 0.006*\"channel\"\n2022-04-22 17:44:53,667 : INFO : topic #8 (0.054): 0.008*\"rule\" + 0.006*\"hidden\" + 0.006*\"generalization\" + 0.004*\"hidden_unit\" + 0.004*\"prediction\" + 0.004*\"sequence\" + 0.004*\"net\" + 0.004*\"tree\" + 0.004*\"machine\" + 0.003*\"trained\"\n2022-04-22 17:44:53,667 : INFO : topic #4 (0.057): 0.009*\"matrix\" + 0.007*\"gradient\" + 0.006*\"solution\" + 0.005*\"convergence\" + 0.004*\"distance\" + 0.004*\"let\" + 0.004*\"minimum\" + 0.004*\"eq\" + 0.004*\"optimal\" + 0.003*\"optimization\"\n2022-04-22 17:44:53,667 : INFO : topic #3 (0.063): 0.007*\"class\" + 0.006*\"gaussian\" + 0.005*\"sample\" + 0.005*\"estimate\" + 0.005*\"classifier\" + 0.005*\"density\" + 0.005*\"mixture\" + 0.005*\"prior\" + 0.004*\"classification\" + 0.004*\"bayesian\"\n2022-04-22 17:44:53,667 : INFO : topic diff=0.117670, rho=0.242536\n2022-04-22 17:44:53,679 : INFO : PROGRESS: pass 16, at document #1740/1740\n2022-04-22 17:45:00,393 : INFO : optimized alpha [0.041376065, 0.03660367, 0.0380804, 0.06374838, 0.058118302, 0.0428449, 0.049285352, 0.03212048, 0.054208644, 0.032528903]\n2022-04-22 17:45:00,403 : INFO : topic #7 (0.032): 0.014*\"circuit\" + 0.013*\"chip\" + 0.012*\"analog\" + 0.011*\"neuron\" + 0.008*\"signal\" + 0.008*\"voltage\" + 0.008*\"memory\" + 0.006*\"vlsi\" + 0.006*\"implementation\" + 0.005*\"bit\"\n2022-04-22 17:45:00,403 : INFO : topic #9 (0.033): 0.021*\"neuron\" + 0.014*\"cell\" + 0.014*\"spike\" + 0.012*\"signal\" + 0.011*\"response\" + 0.011*\"stimulus\" + 0.009*\"firing\" + 0.007*\"frequency\" + 0.007*\"noise\" + 0.007*\"channel\"\n2022-04-22 17:45:00,404 : INFO : topic #8 (0.054): 0.008*\"rule\" + 0.006*\"hidden\" + 0.006*\"generalization\" + 0.004*\"hidden_unit\" + 0.004*\"prediction\" + 0.004*\"sequence\" + 0.004*\"net\" + 0.004*\"tree\" + 0.004*\"machine\" + 0.003*\"trained\"\n2022-04-22 17:45:00,404 : INFO : topic #4 (0.058): 0.009*\"matrix\" + 0.007*\"gradient\" + 0.006*\"solution\" + 0.005*\"convergence\" + 0.004*\"distance\" + 0.004*\"let\" + 0.004*\"minimum\" + 0.004*\"eq\" + 0.004*\"optimal\" + 0.003*\"optimization\"\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Block for bleicorpus Module\nDESCRIPTION: ReStructuredText documentation block defining the module's structure, scope and inheritance details for the bleicorpus module which handles Blei's LDA-C format corpus.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/corpora/bleicorpus.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n:mod:`corpora.bleicorpus` -- Corpus in Blei's LDA-C format\n==========================================================\n\n.. automodule:: gensim.corpora.bleicorpus\n    :synopsis: Corpus in Blei's LDA-C format\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Analyzing List of Names\nDESCRIPTION: This snippet contains a list of names. The purpose is to process and document this list, which could be used in various applications such as data generation for software testing, demographic analysis, or name recognition in text processing. The names include 'alex', 'rachel', 'dave', 'bob', etc.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/ldavowpalwabbit.txt#2025-04-21_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n\"alex rachel dave rachel bob jim dave harry harry dave sue alice bob tim alice alice harry harry alice bob dave dave robert robert harry tim dave rachel jim tim sue alex sue tim alice robert harry robert tim robert jim robert jim alex bob sue alex dave rachel harry rachel rachel alex alice dave sue alice\"\n```\n\n----------------------------------------\n\nTITLE: Word Embedding Vector Representation in 300 Dimensions\nDESCRIPTION: This snippet shows the vector representation for the word 'uno' in a 300-dimensional embedding space. Each number represents the coordinate of the word along a specific dimension in the embedding space.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/IT.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_0\n\nLANGUAGE: data\nCODE:\n```\n20 300\nuno -1.105509996414184570e-01 9.477700293064117432e-02 3.271999955177307129e-02 -3.734639883041381836e-01 -4.962399974465370178e-02 6.096800044178962708e-02 -3.497200086712837219e-02 -7.580299675464630127e-02 -2.305780053138732910e-01 3.037399984896183014e-02 -1.362410038709640503e-01 2.524420022964477539e-01 9.408500045537948608e-02 -1.673370003700256348e-01 -8.943200111389160156e-02 2.401899918913841248e-02 -1.070640012621879578e-01 2.020870000123977661e-01 -1.564449965953826904e-01 1.400749981403350830e-01 -8.892899751663208008e-02 -1.060110032558441162e-01 -8.315499871969223022e-02 2.785980105400085449e-01 -3.359999954700469971e-01 1.247479990124702454e-01 -9.182699769735336304e-02 -1.750040054321289062e-01 -1.194700039923191071e-02 -2.003770023584365845e-01 2.500100061297416687e-02 -6.559299677610397339e-02 2.129299938678741455e-02 -1.643330007791519165e-01 2.357040047645568848e-01 7.093299925327301025e-02 -1.813800036907196045e-01 -1.151980012655258179e-01 1.235499978065490723e-01 3.614300116896629333e-02 -4.945000167936086655e-03 2.767499908804893494e-02 -8.490999788045883179e-02 -3.068199940025806427e-02 1.355150043964385986e-01 -5.919000133872032166e-02 -1.974249929189682007e-01 -1.901300065219402313e-02 5.373099818825721741e-02 -7.814399898052215576e-02 4.281599819660186768e-02 1.200200021266937256e-01 -2.313599921762943268e-02 3.840800002217292786e-02 -3.067499957978725433e-02 9.747499972581863403e-02 2.316360026597976685e-01 -2.523930072784423828e-01 8.110000053420662880e-04 1.401560008525848389e-01 -1.231279969215393066e-01 -2.241529971361160278e-01 2.936399914324283600e-02 -3.592399880290031433e-02 -2.779639959335327148e-01 1.988109946250915527e-01 1.251800060272216797e-01 6.647000089287757874e-03 2.447949945926666260e-01 2.701000077649950981e-03 -1.479469984769821167e-01 -2.143069952726364136e-01 2.018399909138679504e-02 2.368749976158142090e-01 -9.729900211095809937e-02 8.870000019669532776e-03 -2.394000068306922913e-03 -5.868799984455108643e-02 -1.688760071992874146e-01 -1.106320023536682129e-01 -1.521909981966018677e-01 3.102600015699863434e-02 4.273000173270702362e-03 6.963200122117996216e-02 -8.534000068902969360e-02 -1.762129962444305420e-01 2.307839989662170410e-01 -1.239330023527145386e-01 -2.280409932136535645e-01 7.888799905776977539e-02 3.829599916934967041e-02 1.675709933042526245e-01 8.715000003576278687e-03 -2.057499997317790985e-02 -8.175700157880783081e-02 -1.041830033063888550e-01 8.093100041151046753e-02 -7.543499767780303955e-02 1.636839956045150757e-01 1.879329979419708252e-01 2.346999943256378174e-01 1.137510016560554504e-01 7.030499726533889771e-02 -1.722459942102432251e-01 -2.460159957408905029e-01 -1.189569979906082153e-01 -1.484199985861778259e-02 -1.246109977364540100e-01 -7.703900337219238281e-02 -2.273910045623779297e-01 -7.068599760532379150e-02 8.908899873495101929e-02 3.972399979829788208e-02 8.957000449299812317e-03 1.593369990587234497e-01 -1.041909977793693542e-01 5.521799996495246887e-02 1.528500020503997803e-02 -7.500000065192580223e-04 -2.265099994838237762e-02 -2.329590022563934326e-01 3.347500041127204895e-02 1.920000067912042141e-04 -1.722140014171600342e-01 8.192899823188781738e-02 1.937900036573410034e-01 -2.764889895915985107e-01 1.314159929752349854e-01 3.961000125855207443e-03 1.625549942255020142e-01 8.520200103521347046e-02 -9.434999898076057434e-03 -3.002420067787170410e-01 -1.238199975341558456e-02 -4.585000127553939819e-02 -2.182479947805404663e-01 -1.434209942817687988e-01 -4.285499826073646545e-02 2.590800076723098755e-02 3.432799875736236572e-02 -6.443499773740768433e-02 5.239799991250038147e-02 2.799910008907318115e-01 2.378450036048889160e-01 9.632299840450286865e-02 1.673009991645812988e-01 1.186980009078979492e-01 -6.155199930071830750e-02 1.512250006198883057e-01 1.302569955587387085e-01 1.459320038557052612e-01 -9.717900305986404419e-02 -7.166899740695953369e-02 1.177330017089843750e-01 -6.883200258016586304e-02 -1.684959977865219116e-01 -2.322549968957901001e-01 -3.564399853348731995e-02 7.427199929952621460e-02 -5.000799894332885742e-02 1.789560019969940186e-01 1.326999976299703121e-03 1.110899969935417175e-01 4.892899841070175171e-02 -2.565680146217346191e-01 4.041920006275177002e-01 -2.165240049362182617e-01 9.056600183248519897e-02 -5.614899843931198120e-02 -8.905100077390670776e-02 -6.460399925708770752e-02 -3.231439888477325439e-01 -2.250770032405853271e-01 -1.060030013322830200e-01 1.526959985494613647e-01 1.953100040555000305e-02 -7.242199778556823730e-02 1.810840070247650146e-01 2.943300083279609680e-02 -8.188100159168243408e-02 -2.138220071792602539e-01 4.682400077581405640e-02 4.609800130128860474e-02 3.815500065684318542e-02 -5.090000107884407043e-02 -2.209500074386596680e-01 1.147909983992576599e-01 -2.489230036735534668e-01 -1.099999994039535522e-02 5.182600021362304688e-02 -1.189030036330223083e-01 -1.068980023264884949e-01 -2.303200028836727142e-02 2.666400000452995300e-02 -1.152419969439506531e-01 9.420999884605407715e-02 -2.339890003204345703e-01 -1.492500007152557373e-01 -7.406900078058242798e-02 -1.702419966459274292e-01 4.091500118374824524e-02 1.459400057792663574e-01 8.527100086212158203e-02 1.135879978537559509e-01 -3.450300171971321106e-02 -2.995200082659721375e-02 7.965300232172012329e-02 -1.389950066804885864e-01 6.089700013399124146e-02 -4.710799828171730042e-02 -4.100299999117851257e-02 1.620119959115982056e-01 5.368899926543235779e-02 -8.989699929952621460e-02 1.425900030881166458e-02 -4.750299826264381409e-02 -2.041379958391189575e-01 1.105400025844573975e-01 -8.100900053977966309e-02 -4.103700071573257446e-02 -1.742899976670742035e-02 -2.549130022525787354e-01 -1.109950020909309387e-01 5.676599964499473572e-02 2.949250042438507080e-01 -9.326700121164321899e-02 1.069969981908798218e-01 -2.894699946045875549e-02 2.291619926691055298e-01 2.130270004272460938e-01 3.704499825835227966e-02 1.950599998235702515e-02 2.458900026977062225e-02 8.221100270748138428e-02 2.237000036984682083e-03 -8.341799676418304443e-02 -1.301659941673278809e-01 -1.354579925537109375e-01 1.967599987983703613e-02 -1.103039979934692383e-01 -1.052419990301132202e-01 4.138499870896339417e-02 2.189699932932853699e-02 -1.953999977558851242e-03 -1.258960068225860596e-01 1.119100023061037064e-02 -1.081200037151575089e-02 -4.133000038564205170e-03 -2.238709926605224609e-01 -2.135300077497959137e-02 1.762460023164749146e-01 -4.811999853700399399e-03 3.156599998474121094e-01 2.599380016326904297e-01 2.401169985532760620e-01 -3.112800046801567078e-02 7.251399755477905273e-02 -3.061600029468536377e-02 2.270220071077346802e-01 -1.274639964103698730e-01 -6.967200338840484619e-02 2.042530030012130737e-01 6.347399950027465820e-02 -1.392779946327209473e-01 -2.224610000848770142e-01 -1.005999967455863953e-01 2.483000047504901886e-02 1.589999999850988388e-03 -2.713399939239025116e-02 4.437899962067604065e-02 8.354000002145767212e-02 4.784899950027465820e-02 -1.350580006837844849e-01 -7.152699679136276245e-02 4.085300117731094360e-02 3.624499961733818054e-02 -1.346990019083023071e-01 3.305099904537200928e-02 -3.212200105190277100e-02 1.342729926109313965e-01 2.104610055685043335e-01 -6.442999932914972305e-03 -3.545299917459487915e-02 3.994499891996383667e-02 1.772499978542327881e-01 -9.614799916744232178e-02 -4.571500048041343689e-02 1.327529996633529663e-01 2.993899956345558167e-02 -1.192549988627433777e-01 -1.811610013246536255e-01 2.291199937462806702e-02 7.092200219631195068e-02 3.258950114250183105e-01 -3.556700050830841064e-02 -8.715000003576278687e-02 -4.995000082999467850e-03 1.141999964602291584e-03 1.652950048446655273e-01 -3.259399905800819397e-02\n```\n\n----------------------------------------\n\nTITLE: Analyzing Sixth List of Food Items\nDESCRIPTION: This snippet provides a list of food items.  The purpose is to document this data for potential usage in food-related applications such as recipe parsing or nutritional analysis. The list contains food items such as 'bacon', 'mushrooms', 'sausages', etc.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/ldavowpalwabbit.txt#2025-04-21_snippet_12\n\nLANGUAGE: text\nCODE:\n```\n\"bacon mushrooms mushrooms sausages sausages cereal bacon tea beans sausages ham mushrooms eggs mushrooms bacon eggs tea coffee cereal bacon ham sausages juice ham beans beans tea cereal eggs bacon tea beans bacon coffee sausages beans sausages coffee cereal tea sausages ham cereal eggs juice coffee juice mushrooms mushrooms juice beans ham tea mushrooms ham beans tea tea\"\n```\n\n----------------------------------------\n\nTITLE: Numeric Vector Data in Plaintext Format\nDESCRIPTION: A single line of space-separated floating-point numbers representing a high-dimensional vector. This format is commonly used for storing feature vectors, word embeddings, or other numerical data for machine learning models.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/EN.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_12\n\nLANGUAGE: plaintext\nCODE:\n```\ncat 3.831920027732849121e-01 3.878200054168701172e-02 1.998000079765915871e-03 -1.787899993360042572e-02 -4.471100121736526489e-02 1.967930048704147339e-01 2.135490030050277710e-01 1.552910059690475464e-01 2.212260067462921143e-01 7.695899903774261475e-02 -2.935270071029663086e-01 1.319720000028610229e-01 -1.369220018386840820e-01 8.062000386416912079e-03 2.516599893569946289e-01 -6.765300035476684570e-02 1.350969970226287842e-01 -1.598159968852996826e-01 1.476300060749053955e-01 -1.189759969711303711e-01 -3.563979864120483398e-01 1.676049977540969849e-01 8.230099827051162720e-02 -4.239760041236877441e-01 -5.059799924492835999e-02 2.273599952459335327e-01 -8.762499690055847168e-02 -3.711960017681121826e-01 -5.574699863791465759e-02 1.805350035429000854e-01 -3.790500015020370483e-02 2.844999916851520538e-02 -4.687000066041946411e-03 1.111800000071525574e-01 1.043300032615661621e-01 1.675069928169250488e-01 3.428599983453750610e-02 2.661240100860595703e-01 -1.276330053806304932e-01 -1.910440027713775635e-01 3.928299993276596069e-02 2.355580031871795654e-01 -2.848299965262413025e-02 1.786420047283172607e-01 2.379499971866607666e-01 -1.585080027580261230e-01 8.174700289964675903e-02 1.066310033202171326e-01 4.838999826461076736e-03 -5.078599974513053894e-02 9.467200189828872681e-02 -9.592799842357635498e-02 6.535000167787075043e-03 9.688899666070938110e-02 3.313519954681396484e-01 -2.486629933118820190e-01 6.940000224858522415e-03 6.413699686527252197e-02 -2.672510147094726562e-01 3.842800110578536987e-02 -2.519730031490325928e-01 -2.542899921536445618e-02 -1.132209971547126770e-01 -5.884199962019920349e-02 -1.732050031423568726e-01 -3.523000050336122513e-03 -9.980700165033340454e-02 7.859099656343460083e-02 -1.657300069928169250e-02 9.285199642181396484e-02 -1.980960071086883545e-01 2.276750057935714722e-01 1.316609978675842285e-01 -1.900669932365417480e-01 1.918260008096694946e-01 -1.430439949035644531e-01 3.379890024662017822e-01 -1.348900049924850464e-01 2.392410039901733398e-01 -2.239290028810501099e-01 5.893399938941001892e-02 1.772560030221939087e-01 2.266589999198913574e-01 1.226579993963241577e-01 -1.640720069408416748e-01 2.982899919152259827e-02 2.411340028047561646e-01 2.242709994316101074e-01 -9.086599946022033691e-02 -7.349199801683425903e-02 5.474599823355674744e-02 1.855120062828063965e-01 -1.582069993019104004e-01 1.449320018291473389e-01 9.309999877586960793e-04 -1.286790072917938232e-01 -1.603299938142299652e-02 8.051899820566177368e-02 -4.477719962596893311e-01 2.132979929447174072e-01 3.859000047668814659e-03 1.058819964528083801e-01 2.191399969160556793e-02 -1.571410000324249268e-01 -6.839700043201446533e-02 -1.387130022048950195e-01 -1.064670011401176453e-01 3.590099886059761047e-02 2.066539973020553589e-01 -7.152199745178222656e-02 1.001169979572296143e-01 -3.704290091991424561e-01 1.078270003199577332e-01 1.485299970954656601e-02 9.256500005722045898e-02 9.664800018072128296e-02 -1.439709961414337158e-01 -3.877799957990646362e-02 3.209860026836395264e-01 6.344299763441085815e-02 3.509190082550048828e-01 -2.559500001370906830e-02 -2.572000026702880859e-03 -4.468199983239173889e-02 -1.707800030708312988e-01 -2.260970026254653931e-01 -2.125730067491531372e-01 -6.218200176954269409e-02 -2.929200045764446259e-02 2.967300079762935638e-02 8.027300238609313965e-02 -1.827480047941207886e-01 -6.996800005435943604e-02 3.099899888038635254e-01 -8.379000239074230194e-03 -8.371400088071823120e-02 4.590599983930587769e-02 5.994400009512901306e-02 2.591999946162104607e-03 9.355000220239162445e-03 -3.002100065350532532e-02 -9.301999583840370178e-03 -2.960810065269470215e-01 1.585859954357147217e-01 8.464100211858749390e-02 1.587309986352920532e-01 5.679799988865852356e-02 -2.776000089943408966e-02 6.050499901175498962e-02 1.405930072069168091e-01 -6.917700171470642090e-02 5.989100039005279541e-02 2.229699939489364624e-01 -2.076999992132186890e-01 1.405220031738281250e-01 -7.593099772930145264e-02 1.066320016980171204e-01 2.430869936943054199e-01 1.097130030393600464e-01 -5.985999945551156998e-03 6.358899921178817749e-02 -2.248400002717971802e-01 -1.137140020728111267e-01 1.415320038795471191e-01 -1.159099955111742020e-02 2.234549969434738159e-01 -1.031140014529228210e-01 5.671599879860877991e-02 -2.200479954481124878e-01 2.170879989862442017e-01 1.463789939880371094e-01 1.354549974203109741e-01 -9.290499985218048096e-02 -1.471900045871734619e-01 1.310400012880563736e-02 4.158100113272666931e-02 -7.367599755525588989e-02 -9.703599661588668823e-02 2.302599996328353882e-01 1.668900065124034882e-02 -1.833560019731521606e-01 7.879699766635894775e-02 -4.785199835896492004e-02 -2.762700058519840240e-02 2.782600000500679016e-02 -2.549160122871398926e-01 1.498129963874816895e-01 2.301110029220581055e-01 7.887399941682815552e-02 1.530079990625381470e-01 -5.229799821972846985e-02 4.104200005531311035e-02 3.219130039215087891e-01 2.132700011134147644e-02 4.424000158905982971e-03 2.263520061969757080e-01 9.222699701786041260e-02 -6.081299856305122375e-02 6.755200028419494629e-02 3.494200110435485840e-02 -1.095509976148605347e-01 3.198799863457679749e-02 1.073400005698204041e-01 -1.130419969558715820e-01 -1.029279977083206177e-01 2.616960108280181885e-01 3.762600123882293701e-01 -1.114590018987655640e-01 -1.759389936923980713e-01 -9.720000438392162323e-03 8.283399790525436401e-02 1.426330059766769409e-01 2.094929963350296021e-01 -3.359400108456611633e-02 9.636899828910827637e-02 1.932599954307079315e-02 -4.703700169920921326e-02 -1.595830023288726807e-01 2.310039997100830078e-01 -1.524060070514678955e-01 1.022119969129562378e-01 -3.410600125789642334e-01 -1.103530004620552063e-01 6.158899888396263123e-02 -2.257779985666275024e-01 -3.526400029659271240e-02 -5.423900112509727478e-02 -7.201299816370010376e-02 1.118440032005310059e-01 1.912470012903213501e-01 4.761999938637018204e-03 -1.061130017042160034e-01 -1.523540019989013672e-01 -1.176590025424957275e-01 1.908300071954727173e-02 -2.797729969024658203e-01 -1.372990012168884277e-01 8.454400300979614258e-02 8.822400122880935669e-02 -2.103909999132156372e-01 -5.466800183057785034e-02 -5.058399960398674011e-02 1.550099998712539673e-02 -3.030860126018524170e-01 -8.347799628973007202e-02 -1.454229950904846191e-01 -7.387000322341918945e-02 -1.130400039255619049e-02 6.179400160908699036e-02 -6.316500157117843628e-02 1.210720017552375793e-01 -1.109299995005130768e-02 -3.864400088787078857e-02 6.486999802291393280e-03 -4.103949964046478271e-01 -8.023499697446823120e-02 9.014199674129486084e-02 -5.861400067806243896e-02 1.193599998950958252e-01 1.153969988226890564e-01 -1.655219942331314087e-01 3.796310126781463623e-01 1.977599970996379852e-02 -9.230799973011016846e-02 1.550759971141815186e-01 -3.610400110483169556e-02 -1.856790035963058472e-01 2.660549879074096680e-01 1.413400005549192429e-02 6.764099746942520142e-02 2.382999984547495842e-03 -1.153509989380836487e-01 -2.458900026977062225e-02 6.340199708938598633e-02 1.090980023145675659e-01 1.180030032992362976e-01 1.805370002985000610e-01 -1.643469929695129395e-01 5.229210257530212402e-01 3.800239861011505127e-01 8.183900266885757446e-02 3.052999973297119141e-01 7.809999864548444748e-03 -5.768999923020601273e-03 -1.163510009646415710e-01 3.323499858379364014e-02 5.433500185608863831e-02 -2.051260024309158325e-01 1.338479965925216675e-01 -2.900519967079162598e-01 6.853300333023071289e-02 -3.621110022068023682e-01 1.401460021734237671e-01 -1.026849970221519470e-01 1.783419996500015259e-01 3.309400007128715515e-02 -2.566690146923065186e-01 3.570900112390518188e-02 -5.068400129675865173e-02 1.886519938707351685e-01\n```\n\n----------------------------------------\n\nTITLE: Word Vector Numerical Data\nDESCRIPTION: 300-dimensional vector representation of the word 'cane' with floating-point values in scientific notation. Each number represents a dimension in the word embedding space.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/IT.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_8\n\nLANGUAGE: txt\nCODE:\n```\ncane -9.584199637174606323e-02 3.684430122375488281e-01 1.073990017175674438e-01 -4.641990065574645996e-01 3.561689853668212891e-01 -3.067879974842071533e-01 -5.447300150990486145e-02 4.398899897933006287e-02 1.051689982414245605e-01 3.137289881706237793e-01 -9.935300052165985107e-02 2.755099907517433167e-02 2.827500104904174805e-01 -1.429640054702758789e-01 1.107199955731630325e-02 1.490619927644729614e-01 3.655340075492858887e-01 -1.731220036745071411e-01 -1.987369954586029053e-01 -3.359960019588470459e-01 6.794000044465065002e-03 -1.012099999934434891e-02 -1.442200038582086563e-02 9.850899875164031982e-02 -1.190489977598190308e-01 -3.490110039710998535e-01 -2.346220016479492188e-01 8.984999731183052063e-03 2.408459931612014771e-01 -7.467299699783325195e-02 -5.531900003552436829e-02 -1.112409979104995728e-01 -2.302299998700618744e-02 1.008879989385604858e-01 1.778469979763031006e-01 -1.563690006732940674e-01 -7.829000055789947510e-02 -1.092429980635643005e-01 1.714989989995956421e-01 -1.127339974045753479e-01 1.550070047378540039e-01 7.345099747180938721e-02 -1.541659981012344360e-01 2.634750008583068848e-01 -2.141959965229034424e-01 -1.564999967813491821e-01 -6.144999992102384567e-03 1.586199924349784851e-02 -2.832849919795989990e-01 -5.541799962520599365e-02 7.039300352334976196e-02 4.355950057506561279e-01 -2.137539982795715332e-01 3.706600144505500793e-02 3.792909979820251465e-01 -2.831000089645385742e-01 1.230020001530647278e-01 -2.430620044469833374e-01 -1.101009994745254517e-01 -1.941799931228160858e-02 5.955500155687332153e-02 -2.681269943714141846e-01 -4.898799955844879150e-02 -3.147999942302703857e-02 -3.033600002527236938e-02 -4.195719957351684570e-01 -1.412799954414367676e-01 -6.868000142276287079e-03 1.389710009098052979e-01 1.138079985976219177e-01 -1.737129986286163330e-01 -1.932169944047927856e-01 -3.904999932274222374e-03 -1.907150000333786011e-01 1.376039981842041016e-01 -1.203309968113899231e-01 -1.983910053968429565e-01 1.572909951210021973e-01 -3.503499925136566162e-02 3.431470096111297607e-01 -1.514479964971542358e-01 -1.694699935615062714e-02 1.312890052795410156e-01 -1.515560001134872437e-01 -5.396800115704536438e-02 6.140200048685073853e-02 -7.207699865102767944e-02 2.803069949150085449e-01 3.324100002646446228e-02 1.543360054492950439e-01 -1.593330055475234985e-01 1.889780014753341675e-01 2.288610041141510010e-01 -2.665669918060302734e-01 -2.072339951992034912e-01 -3.560699895024299622e-02 -2.600359916687011719e-01 5.541999824345111847e-03 -7.783800363540649414e-02 -1.656300015747547150e-02 6.977800279855728149e-02 -3.859550058841705322e-01 -3.436000086367130280e-03 -2.221900038421154022e-02 -1.023039966821670532e-01 -1.829000003635883331e-02 -3.011209964752197266e-01 -2.650859951972961426e-01 -1.023000013083219528e-02 2.809990048408508301e-01 -3.938960134983062744e-01 -4.411900043487548828e-02 -2.257679998874664307e-01 1.152829974889755249e-01 -1.229989975690841675e-01 -3.744199872016906738e-02 2.281280010938644409e-01 6.757599860429763794e-02 -3.376000095158815384e-03 3.742200136184692383e-02 -1.350540071725845337e-01 1.075420007109642029e-01 -2.076330035924911499e-01 -1.526219993829727173e-01 -4.965599998831748962e-02 -3.002919852733612061e-01 3.397069871425628662e-01 9.593699872493743896e-02 -2.017620056867599487e-01 6.466999650001525879e-02 -3.416689932346343994e-01 1.385210007429122925e-01 1.492299977689981461e-02 -1.631029993295669556e-01 2.737000025808811188e-02 2.051090002059936523e-01 -3.590419888496398926e-01 -6.355699896812438965e-02 -1.111840009689331055e-01 -1.151299998164176941e-01 -2.699300087988376617e-02 7.611700147390365601e-02 1.444699987769126892e-02 1.571960002183914185e-01 1.337999943643808365e-03 4.559060037136077881e-01 -2.631349861621856689e-01 -6.835999898612499237e-03 -2.408239990472793579e-01 1.409579962491989136e-01 1.676100045442581177e-01 8.260200172662734985e-02 -5.929299816489219666e-02 -1.349859982728958130e-01 3.379699960350990295e-02 7.387000136077404022e-03 -2.456099987030029297e-01 4.928699880838394165e-02 4.847399890422821045e-02 3.400849997997283936e-01 -1.920710057020187378e-01 2.226970046758651733e-01 1.717730015516281128e-01 -2.180800028145313263e-02 1.318459957838058472e-01 1.443890035152435303e-01 -1.163119971752166748e-01 1.660639941692352295e-01 9.505300223827362061e-02 5.784000083804130554e-02 7.254800200462341309e-02 9.754800051450729370e-02 -1.985049992799758911e-01 1.963880062103271484e-01 -1.945780068635940552e-01 9.426700323820114136e-02 1.347270011901855469e-01 1.671549975872039795e-01 1.197429969906806946e-01 -2.806299924850463867e-02 -1.493709981441497803e-01 -1.737930029630661011e-01 6.180600076913833618e-02 6.661300361156463623e-02 5.998599901795387268e-02 -6.538300216197967529e-02 2.207770049571990967e-01 1.441379934549331665e-01 -1.866919994354248047e-01 -1.504289954900741577e-01 -3.330810070037841797e-01 3.438889980316162109e-01 -4.134200140833854675e-02 1.707870066165924072e-01 3.815180063247680664e-01 1.631239950656890869e-01 4.292359948158264160e-01 -1.893430054187774658e-01 3.297590017318725586e-01 2.382739931344985962e-01 -9.226199984550476074e-02 -6.184599921107292175e-02 -1.017450019717216492e-01 3.539200127124786377e-02 -1.125650033354759216e-01 3.807719945907592773e-01 2.827499993145465851e-02 7.258500158786773682e-02 1.805530041456222534e-01 3.912099823355674744e-02 -2.758539915084838867e-01 -2.896399982273578644e-02 -1.141379997134208679e-01 -9.418900310993194580e-02 -3.944169878959655762e-01 6.855600327253341675e-02 1.604599952697753906e-01 1.958500035107135773e-02 4.931000061333179474e-03 1.887599937617778778e-02 -2.038599923253059387e-02 4.554319977760314941e-01 -1.673080027103424072e-01 1.239020004868507385e-01 2.110649943351745605e-01 4.766499996185302734e-02 3.164600133895874023e-01 1.704199984669685364e-02 -1.187649965286254883e-01 4.480000119656324387e-03 -1.184739992022514343e-01 -2.061460018157958984e-01 -4.201000183820724487e-02 -3.749499842524528503e-02 1.389539986848831177e-01 1.429339945316314697e-01 -2.688699960708618164e-01 -2.563180029392242432e-01 1.957979947328567505e-01 -1.873400062322616577e-02 -5.626399815082550049e-02 2.430039942264556885e-01 9.251000359654426575e-03 1.231560036540031433e-01 2.339050024747848511e-01 9.805999696254730225e-03 -7.001200318336486816e-02 4.246300086379051208e-02 -3.572649955749511719e-01 7.016299664974212646e-02 1.456239968538284302e-01 -6.145750284194946289e-01 2.122820019721984863e-01 -2.273299917578697205e-02 1.120750010013580322e-01 1.833830028772354126e-01 -3.136549890041351318e-01 7.303799688816070557e-02 -2.546999976038932800e-02 -1.975339949131011963e-01 -1.042789965867996216e-01 -2.438000030815601349e-02 -2.247119992971420288e-01 -3.412680029869079590e-01 8.786299824714660645e-02 -6.043300032615661621e-02 -2.993400022387504578e-02 3.802349865436553955e-01 -8.728999644517898560e-03 -1.246230006217956543e-01 2.343759983777999878e-01 1.154960021376609802e-01 -3.071100078523159027e-02 8.395300060510635376e-02 1.977750062942504883e-01 2.135940045118331909e-01 2.166319936513900757e-01 5.604999884963035583e-02 8.934699743986129761e-02 8.498600125312805176e-02 -3.304780125617980957e-01 1.530790030956268311e-01 -2.517499960958957672e-02 -1.562359929084777832e-01 -1.696099936962127686e-01 -7.367199659347534180e-02 -2.563600055873394012e-02 2.444999990984797478e-03 -9.576500207185745239e-02 6.402300298213958740e-02 5.217700079083442688e-02 -8.487399667501449585e-02 -1.418479979038238525e-01 -2.984599955379962921e-02 -2.906360030174255371e-01 -5.188879966735839844e-01 -2.995100021362304688e-01 -1.272660046815872192e-01 1.815400086343288422e-02 2.053530067205429077e-01\n```\n\n----------------------------------------\n\nTITLE: Retrieving Coherence Metric Value for Trained LDA Model\nDESCRIPTION: Code snippet that demonstrates how to retrieve a specific metric value (in this case, u_mass coherence) for a trained LDA model.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Training_visualizations.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# to get a metric value on a trained model\nprint(CoherenceMetric(corpus=training_corpus, coherence=\"u_mass\").get_value(model=model))\n```\n\n----------------------------------------\n\nTITLE: Checking BLAS and LAPACK Configuration in Python\nDESCRIPTION: This command shows the current BLAS and LAPACK configuration for SciPy, which is important for optimizing linear algebra performance in Gensim.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/distributed.md#2025-04-21_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython -c 'import scipy; scipy.show_config()'\n```\n\n----------------------------------------\n\nTITLE: Sample Word Frequency Entries\nDESCRIPTION: Examples of word frequency entries from the list, showing the format of index number, word, and frequency count.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/head500.noblanks.cor_wordids.txt#2025-04-21_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\n28918\tgaston\t1\n12849\tgastric\t1\n25325\tgastroenterolog\t1\n1687\tgastrointestin\t1\n3868\tgate\t21\n8839\tgatewai\t2\n520\tgather\t26\n```\n\n----------------------------------------\n\nTITLE: Word Frequency Data Format - TSV\nDESCRIPTION: Tab-separated values (TSV) data showing word frequencies. Format: <frequency count><tab><word><tab><number of documents>\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/head500.noblanks.cor_wordids.txt#2025-04-21_snippet_5\n\nLANGUAGE: data\nCODE:\n```\n28996\tpocotaligo\t1\n22432\tpodium\t1\n18078\tpodocarpu\t1\n16776\tpodolski\t1\n26024\tpoe\t3\n```\n\n----------------------------------------\n\nTITLE: Word Vector Data Array\nDESCRIPTION: A large array of floating-point numbers representing word vectors. Each number is a dimension in the vector space, capturing semantic and syntactic properties of words.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/IT.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_3\n\nLANGUAGE: Data\nCODE:\n```\n1.862020045518875122e-01 1.546850055456161499e-01 -1.394080072641372681e-01 -4.315299913287162781e-02 7.415100187063217163e-02 -4.303999990224838257e-02 -5.765699967741966248e-02 -7.232400029897689819e-02 1.931199990212917328e-02 1.915369927883148193e-01 -1.099790036678314209e-01 2.701799944043159485e-02 -4.774099960923194885e-02 5.883499979972839355e-02 -4.312900081276893616e-02 1.518180072307586670e-01 1.953710019588470459e-01 -1.376049965620040894e-01 -2.233130037784576416e-01 3.040600009262561798e-02 -3.635700047016143799e-02 -6.112299859523773193e-02 1.258189976215362549e-01 2.061650007963180542e-01 6.777600198984146118e-02 -1.201919987797737122e-01 -3.409210145473480225e-01 1.076930016279220581e-01 -5.554699897766113281e-02 1.224730014801025391e-01 -7.156399637460708618e-02 1.941799931228160858e-02 -8.437799662351608276e-02 -1.396380066871643066e-01 -1.489100009202957153e-01 2.144300006330013275e-02 -7.561999838799238205e-03 1.904999953694641590e-03 3.594799935817718506e-01 -1.176239997148513794e-01 1.468240022659301758e-01 -4.381199926137924194e-02 1.422629952430725098e-01 2.269999980926513672e-01 7.655200362205505371e-02 3.071800060570240021e-02 -7.390400022268295288e-02 8.714199811220169067e-02 2.124820053577423096e-01 -5.251200124621391296e-02 2.018039971590042114e-01 9.406200051307678223e-02 7.088500261306762695e-02 -1.715729981660842896e-01 -1.085250005125999451e-01 2.998000010848045349e-02 2.462009936571121216e-01 -2.888030111789703369e-01 -2.393240034580230713e-01 1.229379996657371521e-01 1.211050003767013550e-01 -2.493920028209686279e-01 -1.513639986515045166e-01 -7.821699976921081543e-02 -1.366010010242462158e-01 1.013469994068145752e-01 4.123499989509582520e-02 3.354580104351043701e-01 8.135099709033966064e-02 1.050209999084472656e-01 -4.731800034642219543e-02 -1.345629990100860596e-01 3.284479975700378418e-01 8.596300333738327026e-02 -1.311360001564025879e-01 -1.930059939622879028e-01 -5.660299956798553467e-02 1.001930013298988342e-01 -2.056200057268142700e-01 -4.051100090146064758e-02 -1.131239980459213257e-01 -3.650000086054205894e-03 1.009149998426437378e-01 1.282439976930618286e-01 6.182799860835075378e-02 -1.023939996957778931e-01 3.036110103130340576e-01 7.825300097465515137e-02 -3.207789957523345947e-01 -8.378200232982635498e-02 -1.626159995794296265e-01 -7.223399728536605835e-02 9.197899699211120605e-02 -9.770999848842620850e-02 -5.524599924683570862e-02 -4.615699872374534607e-02 -2.602429986000061035e-01 3.203599900007247925e-02 1.995369940996170044e-01 4.035700112581253052e-02 -1.896319985389709473e-01 -8.821500092744827271e-02 3.307599946856498718e-02 -6.485600024461746216e-02 -5.052800104022026062e-02 2.673040032386779785e-01 -1.112880036234855652e-01 -3.243799880146980286e-02 2.385099977254867554e-02 -1.283469945192337036e-01 8.374600112438201904e-02 5.374300107359886169e-02 2.067369967699050903e-01 -1.029689982533454895e-01 4.446189999580383301e-01 -1.559610068798065186e-01 8.133800327777862549e-02 2.007170021533966064e-01 -2.617230117321014404e-01 4.637499898672103882e-02 -2.645530104637145996e-01 3.719860017299652100e-01 -8.652400225400924683e-02 1.297760009765625000e-01 3.679599985480308533e-02 -7.812300324440002441e-02 -1.313620060682296753e-01 1.884510070085525513e-01 3.977620005607604980e-01 1.317599974572658539e-02 1.165549978613853455e-01 2.034250050783157349e-01 -3.180220127105712891e-01 -3.585100173950195312e-02 -9.426400065422058105e-02 3.644499927759170532e-02 1.417160034179687500e-01 -6.109099835157394409e-02 -3.465000167489051819e-02 -8.591800183057785034e-02 3.852400183677673340e-02 1.477990001440048218e-01 2.402469962835311890e-01 1.756709963083267212e-01 -2.295819967985153198e-01 8.692800253629684448e-02 -5.799899995326995850e-02 -8.911599963903427124e-02 1.238759979605674744e-01 -3.570999950170516968e-03 -1.049899980425834656e-01 -1.349660009145736694e-01 2.175300009548664093e-02 -1.502000028267502785e-03 3.667600080370903015e-02 -5.897599831223487854e-02 -2.651399970054626465e-01 -1.415699999779462814e-02 -6.798899918794631958e-02 5.265200138092041016e-02 1.194799970835447311e-02 6.201500073075294495e-02 9.258200228214263916e-02 -6.676399707794189453e-02 -1.443410068750381470e-01 2.345460057258605957e-01 -6.909999996423721313e-02 -1.789999939501285553e-02 -6.122099980711936951e-02 1.660470068454742432e-01 3.031089901924133301e-01 -9.150999598205089569e-03 8.524800091981887817e-02 -3.181599825620651245e-02 -5.779900029301643372e-02 1.180669963359832764e-01 -5.317000206559896469e-03 2.068150043487548828e-01 6.597299873828887939e-02 1.824879944324493408e-01 -3.321399912238121033e-02 -5.575799942016601562e-02 -7.238499820232391357e-02 1.030100043863058090e-02 -6.041000131517648697e-03 2.785559892654418945e-01 7.355400174856185913e-02 2.543999999761581421e-02 -2.885049879550933838e-01 -8.296900242567062378e-02 2.152499929070472717e-02 9.941700100898742676e-02 1.386190056800842285e-01 2.647869884967803955e-01 -8.399499952793121338e-02 1.641820073127746582e-01 -1.665299944579601288e-02 -4.961099848151206970e-02 -4.608099907636642456e-02 -3.188199922442436218e-02 3.874689936637878418e-01 6.733900308609008789e-02 -3.997679948806762695e-01 8.833900094032287598e-02 -6.337500363588333130e-02 6.680300086736679077e-02 3.045539855957031250e-01 -2.793259918689727783e-01 -4.183200001716613770e-02 3.360449969768524170e-01 -1.140700001269578934e-02 1.611119955778121948e-01 5.234700068831443787e-02 -6.100599840283393860e-02 6.358800083398818970e-02 -2.250490039587020874e-01 -1.823370009660720825e-01 2.671279907226562500e-01 -8.160299807786941528e-02 -1.157829985022544861e-01 1.968560069799423218e-01 -1.032529994845390320e-01 1.662899926304817200e-02 1.826940029859542847e-01 4.438599944114685059e-02 2.645099908113479614e-02 2.237379997968673706e-01 -4.138699918985366821e-02 -1.270619928836822510e-01 -9.752099961042404175e-02 -9.593999944627285004e-03 1.085589975118637085e-01 5.590000073425471783e-04 1.678490042686462402e-01 -1.760060042142868042e-01 -8.882600069046020508e-02 -7.667600363492965698e-02 -1.557399984449148178e-02 -6.673999875783920288e-02 1.036930009722709656e-01 1.883199997246265411e-02 2.894620001316070557e-01 2.464500069618225098e-02 -8.243499696254730225e-02 2.365660071372985840e-01 1.665440052747726440e-01 1.935539990663528442e-01 -2.142689973115921021e-01 3.355700150132179260e-02 3.013700060546398163e-02 3.560800105333328247e-02 8.434700220823287964e-02 -8.516299724578857422e-02 -3.613800182938575745e-02 5.873800069093704224e-02 -3.473500162363052368e-02 -2.036470025777816772e-01 1.664099991321563721e-01 -2.045309990644454956e-01 -1.488910019397735596e-01 1.164830029010772705e-01 -1.231779977679252625e-01 2.469210028648376465e-01 -4.570199921727180481e-02 -1.576890051364898682e-01 -1.247939988970756531e-01 1.098420023918151855e-01 7.101099938154220581e-02 -2.351099997758865356e-02 -1.389990001916885376e-01 -2.342669963836669922e-01 2.314029932022094727e-01 4.973899945616722107e-02 8.325000293552875519e-03 2.515400014817714691e-02 1.123569980263710022e-01 1.428000032901763916e-01 9.951400011777877808e-02 1.973769962787628174e-01 2.987300045788288116e-02 -9.268300235271453857e-02 1.572699993848800659e-01 1.365109980106353760e-01 -1.016269996762275696e-01 1.053499989211559296e-02 1.241640001535415649e-01 1.313959956169128418e-01 2.296320050954818726e-01 1.521040052175521851e-01 1.088299974799156189e-01 -1.508370041847229004e-01 1.825110018253326416e-01 6.123699992895126343e-02 1.044839993119239807e-01 2.603399939835071564e-02 -2.848420143127441406e-01 -8.482900261878967285e-02 -1.414580047130584717e-01 1.750279963016510010e-01 1.921119987964630127e-01\n```\n\n----------------------------------------\n\nTITLE: Computing SCM for Unrelated Sentences\nDESCRIPTION: Computing SCM similarity between unrelated sentences for comparison\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_scm.rst#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsimilarity = termsim_matrix.inner_product(sentence_obama, sentence_orange, normalized=(True, True))\nprint('similarity = %.4f' % similarity)\n```\n\n----------------------------------------\n\nTITLE: Word Vector Representation for 'mango' in Text Format\nDESCRIPTION: This is a 300-dimensional vector representation of the word 'mango' from a word embedding model (likely from Gensim). Each value represents a coordinate in the multidimensional semantic space that captures the meaning of the word.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/EN.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_18\n\nLANGUAGE: plaintext\nCODE:\n```\nmango -1.386030018329620361e-01 3.652789890766143799e-01 -1.973980069160461426e-01 1.514700055122375488e-01 -1.098240017890930176e-01 1.486230045557022095e-01 1.081719994544982910e-01 -2.000650018453598022e-01 3.456839919090270996e-01 1.666039973497390747e-01 -2.321999967098236084e-01 -1.063190028071403503e-01 1.309799998998641968e-01 -1.620409935712814331e-01 8.199799805879592896e-02 -1.532810032367706299e-01 1.633670032024383545e-01 1.236299984157085419e-02 -1.375789940357208252e-01 -4.353210031986236572e-01 -5.797199904918670654e-02 1.148600038141012192e-02 1.829000003635883331e-02 -1.093150004744529724e-01 -2.990400046110153198e-02 2.633930146694183350e-01 -2.656010091304779053e-01 -5.840000137686729431e-02 1.303859949111938477e-01 -1.282500009983778000e-02 -1.060300022363662720e-01 3.147169947624206543e-01 1.672820001840591431e-01 1.906999945640563965e-02 -8.840800076723098755e-02 -1.340039968490600586e-01 1.096889972686767578e-01 -2.709749937057495117e-01 5.469600111246109009e-02 -1.711619943380355835e-01 6.063299998641014099e-02 -1.837310045957565308e-01 -1.193969994783401489e-01 1.501100044697523117e-02 2.243299968540668488e-02 4.122700169682502747e-02 2.250889986753463745e-01 2.240640074014663696e-01 -1.991869956254959106e-01 2.214950025081634521e-01 1.016689985990524292e-01 -4.855409860610961914e-01 1.020319983363151550e-01 2.965930104255676270e-01 8.800000068731606007e-04 4.945800080895423889e-02 -2.635810077190399170e-01 -9.606199711561203003e-02 2.113579958677291870e-01 -3.070429861545562744e-01 -2.171149998903274536e-01 4.905600100755691528e-02 2.288530021905899048e-01 -2.115370035171508789e-01 1.672999933362007141e-02 -4.114730060100555420e-01 7.319000363349914551e-02 9.730000048875808716e-03 -3.110550045967102051e-01 1.317130029201507568e-01 -2.302899956703186035e-02 1.049069985747337341e-01 3.085370063781738281e-01 -2.195519953966140747e-01 1.185629963874816895e-01 -1.123109981417655945e-01 1.208819970488548279e-01 3.439640104770660400e-01 7.498399913311004639e-02 2.646199986338615417e-02 -2.036329954862594604e-01 3.854500129818916321e-02 2.014299929141998291e-01 2.330580055713653564e-01 2.236739993095397949e-01 -1.025839969515800476e-01 7.957299798727035522e-02 -1.512919962406158447e-01 1.497800052165985107e-01 -9.612800180912017822e-02 2.706860005855560303e-01 -5.176309943199157715e-01 -2.310030013322830200e-01 -1.084380000829696655e-01 1.902340054512023926e-01 1.660120040178298950e-01 3.525319993495941162e-01 3.583419919013977051e-01 -5.201400071382522583e-02 -5.894999951124191284e-02 7.369499653577804565e-02 1.954510062932968140e-01 2.377659976482391357e-01 -5.557999946177005768e-03 2.478699982166290283e-01 1.068020015954971313e-01 -1.284350007772445679e-01 -6.591799855232238770e-02 2.637800015509128571e-02 -5.247999913990497589e-03 -8.880099654197692871e-02 1.245760023593902588e-01 -2.433329969644546509e-01 -3.888199850916862488e-02 -1.389189958572387695e-01 1.544789969921112061e-01 8.225700259208679199e-02 2.903749942779541016e-01 -5.324700102210044861e-02 1.094190031290054321e-01 3.629799932241439819e-02 1.813289970159530640e-01 1.851859986782073975e-01 -1.289000064134597778e-01 9.175399690866470337e-02 -1.577579975128173828e-01 -3.281829953193664551e-01 3.052799962460994720e-02 -1.542869955301284790e-01 -1.066740006208419800e-01 2.138590067625045776e-01 -8.647400140762329102e-02 -2.354670017957687378e-01 -1.610880047082901001e-01 -6.986399739980697632e-02 6.406400352716445923e-02 -3.130060136318206787e-01 1.881000027060508728e-02 2.955999923869967461e-03 -5.793169736862182617e-01 8.251400291919708252e-02 -8.971200138330459595e-02 2.075179964303970337e-01 -2.736799977719783783e-02 -2.226819992065429688e-01 3.838700056076049805e-02 -3.367210030555725098e-01 -1.291150003671646118e-01 9.774100035429000854e-02 1.556230038404464722e-01 -2.408999949693679810e-03 4.112460017204284668e-01 -2.400799989700317383e-01 4.726700112223625183e-02 -6.836699694395065308e-02 6.228199973702430725e-02 -6.582300364971160889e-02 -1.091229990124702454e-01 -2.703059911727905273e-01 -8.506999909877777100e-02 7.288700342178344727e-02 -2.098259925842285156e-01 6.913000345230102539e-02 6.679499894380569458e-02 2.906889915466308594e-01 -6.892299652099609375e-02 2.857000008225440979e-02 -6.587000098079442978e-03 -2.767649888992309570e-01 1.373530030250549316e-01 -4.248600080609321594e-02 -1.750710010528564453e-01 2.806819975376129150e-01 1.212129965424537659e-01 -9.062000364065170288e-02 2.788650095462799072e-01 -1.282590031623840332e-01 1.918320059776306152e-01 2.118539959192276001e-01 -8.906199783086776733e-02 -7.994099706411361694e-02 -1.313640028238296509e-01 1.480029970407485962e-01 -8.302400261163711548e-02 -8.200599998235702515e-02 1.139229983091354370e-01 -1.878899931907653809e-01 -1.334220021963119507e-01 -2.854889929294586182e-01 -1.778289973735809326e-01 -1.121729984879493713e-01 9.327899664640426636e-02 1.984740048646926880e-01 -9.105999767780303955e-02 -1.836619973182678223e-01 1.759420037269592285e-01 -1.340010017156600952e-01 -6.797199696302413940e-02 -4.525300115346908569e-02 1.435260027647018433e-01 -6.861100345849990845e-02 6.883800029754638672e-02 -2.073909938335418701e-01 8.721800148487091064e-02 -2.382279932498931885e-01 1.549859941005706787e-01 1.805749982595443726e-01 -1.922550052404403687e-01 -6.866499781608581543e-02 -6.006300076842308044e-02 4.451899975538253784e-02 3.080329895019531250e-01 1.330659985542297363e-01 3.469769954681396484e-01 -1.185230016708374023e-01 -1.755999960005283356e-02 5.598200112581253052e-02 -5.122590065002441406e-01 1.467369943857192993e-01 2.437330037355422974e-01 3.961659967899322510e-01 1.794749945402145386e-01 -1.763789951801300049e-01 6.718999892473220825e-02 -2.655340135097503662e-01 -5.150989890098571777e-01 -5.580100044608116150e-02 4.499100148677825928e-02 3.107869923114776611e-01 2.033469974994659424e-01 8.126500248908996582e-02 8.552099764347076416e-02 1.128569990396499634e-01 -2.437849938869476318e-01 -8.485700190067291260e-02 -3.527599945664405823e-02 -1.376020014286041260e-01 3.298400044441223145e-01 -2.531200051307678223e-01 1.947100088000297546e-02 -6.550599634647369385e-02 -1.035730019211769104e-01 -2.051299996674060822e-02 -1.384509950876235962e-01 4.648600146174430847e-02 -3.138399869203567505e-02 -6.060000043362379074e-03 -2.713499963283538818e-01 1.270090043544769287e-01 3.966600075364112854e-02 -3.661719858646392822e-01 6.895899772644042969e-02 -5.756000056862831116e-03 -9.676600247621536255e-02 -3.749749958515167236e-01 -3.758159875869750977e-01 -2.134619951248168945e-01 -1.330250054597854614e-01 4.655800014734268188e-02 1.613229960203170776e-01 -5.309800058603286743e-02 2.219330072402954102e-01 -5.841099843382835388e-02 1.206600014120340347e-02 1.923789978027343750e-01 -1.787260025739669800e-01 6.573799997568130493e-02 1.055980026721954346e-01 1.372099965810775757e-01 3.668500110507011414e-02 -1.616239994764328003e-01 4.648400098085403442e-02 6.830500066280364990e-02 3.410480022430419922e-01 -3.474000096321105957e-02 -1.459780037403106689e-01 -2.730100043118000031e-02 1.439100056886672974e-01 1.512399967759847641e-02 -1.201139986515045166e-01 -1.408649981021881104e-01 -5.101599916815757751e-02 -1.263210028409957886e-01 2.707299888134002686e-01 -1.684630066156387329e-01 1.568859964609146118e-01 6.800500303506851196e-02 2.809469997882843018e-01 6.362599879503250122e-02 -1.890980005264282227e-01 1.016250029206275940e-01 -3.419060111045837402e-01 -1.402599960565567017e-01 9.124000370502471924e-02 -2.520599961280822754e-01 -6.394299864768981934e-02 -4.505300149321556091e-02 2.927690148353576660e-01 4.197100177407264709e-02 2.991499900817871094e-01\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Table of Execution Times\nDESCRIPTION: ReStructuredText table showing execution times and memory usage for Gensim example files, including file names, execution duration, and memory consumption in MB.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/sg_execution_times.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n+--------------------------------------------------------------------------------------------------------------+-----------+---------+\n| :ref:`sphx_glr_auto_examples_core_run_topics_and_transformations.py` (``run_topics_and_transformations.py``) | 00:01.658 | 58.1 MB |\n+--------------------------------------------------------------------------------------------------------------+-----------+---------+\n| :ref:`sphx_glr_auto_examples_core_run_core_concepts.py` (``run_core_concepts.py``)                           | 00:00.000 | 0.0 MB  |\n+--------------------------------------------------------------------------------------------------------------+-----------+---------+\n| :ref:`sphx_glr_auto_examples_core_run_corpora_and_vector_spaces.py` (``run_corpora_and_vector_spaces.py``)   | 00:00.000 | 0.0 MB  |\n+--------------------------------------------------------------------------------------------------------------+-----------+---------+\n| :ref:`sphx_glr_auto_examples_core_run_similarity_queries.py` (``run_similarity_queries.py``)                 | 00:00.000 | 0.0 MB  |\n+--------------------------------------------------------------------------------------------------------------+-----------+---------+\n```\n\n----------------------------------------\n\nTITLE: Word Frequency Data Format\nDESCRIPTION: Tab-delimited data showing word ID, word, and frequency count. Format: <id>[tab]<word>[tab]<count>\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/head500.noblanks.cor_wordids.txt#2025-04-21_snippet_8\n\nLANGUAGE: text\nCODE:\n```\n24059\ttatsuya\t1\n24060\ttatter\t1\n21690\ttau\t1\n27301\ttauern\t1\n5434\ttaught\t22\n```\n\n----------------------------------------\n\nTITLE: Floating Point Vector Data\nDESCRIPTION: A series of scientific notation floating point numbers, likely representing vector embeddings or weights. Values range approximately between -0.35 and 0.35.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/EN.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_9\n\nLANGUAGE: text\nCODE:\n```\nten 9.471800178289413452e-02 1.374779939651489258e-01 -1.950179934501647949e-01 -9.249199926853179932e-02 6.842199712991714478e-02 5.899000167846679688e-02 1.016160026192665100e-01 -4.558200016617774963e-02 5.040999874472618103e-02 [...] 8.190599828958511353e-02\n```\n\n----------------------------------------\n\nTITLE: 300-Dimensional Vector Representation\nDESCRIPTION: This snippet presents a 300-dimensional vector, likely representing a word embedding or feature vector used in natural language processing or machine learning tasks. Each value is a floating-point number, providing a dense numerical representation of some entity or concept.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/IT.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n1.434729993343353271e-01 7.033900171518325806e-02 1.101759970188140869e-01 -1.138620004057884216e-01 2.081400007009506226e-01 1.513379961252212524e-01 -1.386899966746568680e-02 -1.595100015401840210e-02 -7.329399883747100830e-02 1.543529927730560303e-01 -9.655100107192993164e-02 4.044700041413307190e-02 -2.505779862403869629e-01 7.246199995279312134e-02 -9.061300009489059448e-02 1.360190063714981079e-01 -2.387080043554306030e-01 -5.145600065588951111e-02 -1.802629977464675903e-01 -1.620589941740036011e-01 4.811099916696548462e-02 -7.070600241422653198e-02 3.484600037336349487e-02 8.955699950456619263e-02 -4.913499951362609863e-02 6.034199893474578857e-02 3.112399950623512268e-02 -4.570600017905235291e-02 -1.677560061216354370e-01 2.224599942564964294e-02 -3.588600084185600281e-02 1.046160012483596802e-01 -2.222900092601776123e-02 -1.671780049800872803e-01 2.002299949526786804e-02 3.271099925041198730e-02 1.131379976868629456e-01 2.079720050096511841e-01 2.506160140037536621e-01 -1.619050055742263794e-01 1.373009979724884033e-01 3.618400171399116516e-02 1.280930042266845703e-01 1.895940005779266357e-01 6.466999650001525879e-02 1.112810000777244568e-01 1.039839982986450195e-01 4.036999773234128952e-03 -3.613800182938575745e-02 9.063699841499328613e-02 1.281429976224899292e-01 4.736600071191787720e-02 4.581499844789505005e-02 -1.095260009169578552e-01 -5.235200002789497375e-02 -7.534199953079223633e-02 1.353109925985336304e-01 -3.173109889030456543e-01 8.395399898290634155e-02 1.443739980459213257e-01 1.927579939365386963e-01 -6.348299980163574219e-02 -9.367799758911132812e-02 2.735299989581108093e-02 -1.337330043315887451e-01 -1.476449966430664062e-01 8.685500174760818481e-02 6.092299893498420715e-02 1.284859925508499146e-01 9.933099895715713501e-02 1.878339946269989014e-01 -1.220159977674484253e-01 1.987980008125305176e-01 -5.805699899792671204e-02 3.260999917984008789e-02 -4.561600089073181152e-02 -3.835000097751617432e-02 2.326100021600723267e-01 -1.489350050687789917e-01 -9.933900088071823120e-02 -4.377400130033493042e-02 7.662700116634368896e-02 1.711429953575134277e-01 -1.567450016736984253e-01 4.986099898815155029e-02 5.026600137352943420e-02 -3.993799909949302673e-02 6.766100227832794189e-02 -3.474200069904327393e-01 -1.099400036036968231e-02 -1.336009949445724487e-01 -1.230529993772506714e-01 1.041119992733001709e-01 1.197450011968612671e-01 -1.549910008907318115e-01 9.438499808311462402e-02 2.359100058674812317e-02 2.569249868392944336e-01 1.645469963550567627e-01 -5.003999918699264526e-03 1.071199998259544373e-01 -2.903700061142444611e-02 2.315499931573867798e-01 2.598199993371963501e-02 -1.399870067834854126e-01 4.410700127482414246e-02 -8.800700306892395020e-02 1.211600005626678467e-02 7.124300301074981689e-02 -1.366250067949295044e-01 -3.340800106525421143e-02 -7.735099643468856812e-02 1.210739985108375549e-01 -7.616099715232849121e-02 3.765600025653839111e-01 -3.766800090670585632e-02 -7.211100310087203979e-02 -2.373299933969974518e-02 -1.855120062828063965e-01 -7.199999876320362091e-03 1.093970015645027161e-01 2.698040008544921875e-01 -6.932400166988372803e-02 -2.852579951286315918e-01 9.846799820661544800e-02 -1.104300003498792648e-02 -1.938789933919906616e-01 1.833399981260299683e-01 2.110159993171691895e-01 -6.720499694347381592e-02 2.631100118160247803e-01 7.789699733257293701e-02 -1.101000010967254639e-01 -1.212819963693618774e-01 -3.150799870491027832e-02 6.836199760437011719e-02 -1.190949976444244385e-01 -6.844899803400039673e-02 1.221110001206398010e-01 -3.604499995708465576e-02 4.449300095438957214e-02 -8.828900009393692017e-02 1.324959993362426758e-01 8.395700156688690186e-02 -2.406439930200576782e-01 -1.392060071229934692e-01 1.881800033152103424e-02 1.873320043087005615e-01 1.077729985117912292e-01 1.107719987630844116e-01 -3.656899929046630859e-02 -1.170390024781227112e-01 2.218399941921234131e-02 -1.590780019760131836e-01 -1.392199993133544922e-01 -4.739899933338165283e-02 -2.133429944515228271e-01 -2.523499913513660431e-02 -2.123299986124038696e-02 1.585209965705871582e-01 2.603000029921531677e-02 1.577699929475784302e-01 6.090699881315231323e-02 6.563500314950942993e-02 -5.206000059843063354e-02 2.375150024890899658e-01 3.218699991703033447e-02 8.309400081634521484e-02 8.337000384926795959e-03 1.365679949522018433e-01 -7.184500247240066528e-02 -3.313300013542175293e-02 2.512229979038238525e-01 -1.161829978227615356e-01 1.205559968948364258e-01 -4.606000147759914398e-03 -4.429600015282630920e-02 -1.069319993257522583e-01 -7.169199734926223755e-02 4.335099831223487854e-02 -5.059799924492835999e-02 7.810500264167785645e-02 -7.578899711370468140e-02 -7.642299681901931763e-02 7.910999655723571777e-02 1.753190010786056519e-01 -7.288700342178344727e-02 1.362490057945251465e-01 -1.920900046825408936e-01 -1.307490020990371704e-01 -1.231499984860420227e-01 1.566189974546432495e-01 2.594060003757476807e-01 1.728260070085525513e-01 -1.815270036458969116e-01 -2.032300084829330444e-02 -6.446000188589096069e-03 -1.221800036728382111e-02 6.798599660396575928e-02 -1.032989993691444397e-01 3.517040014266967773e-01 1.108789965510368347e-01 -1.581220030784606934e-01 9.157399833202362061e-02 1.268849968910217285e-01 1.460029929876327515e-01 2.510580122470855713e-01 -1.699209958314895630e-01 -4.877000115811824799e-03 1.177870035171508789e-01 1.034699976444244385e-01 2.890219986438751221e-01 9.308200329542160034e-02 6.330499798059463501e-02 2.830499969422817230e-02 -1.552110016345977783e-01 7.700999826192855835e-02 1.708749979734420776e-01 -8.567799627780914307e-02 -2.153800055384635925e-02 -8.144999854266643524e-03 2.571000019088387489e-03 1.021260023117065430e-01 1.494410037994384766e-01 1.449809968471527100e-01 1.990280002355575562e-01 -1.895689964294433594e-01 6.614600121974945068e-02 -4.827199876308441162e-02 1.171199977397918701e-02 1.412380039691925049e-01 -1.301070004701614380e-01 -1.290619969367980957e-01 1.832769960165023804e-01 -7.764899730682373047e-02 1.278460025787353516e-01 1.317649930715560913e-01 1.466919928789138794e-01 -1.121359989047050476e-01 3.030800074338912964e-02 -8.369900286197662354e-02 -1.499440073966979980e-01 -1.436949968338012695e-01 -2.259700000286102295e-02 8.095899969339370728e-02 8.958400040864944458e-02 1.974450051784515381e-01 4.370000096969306469e-04 -2.599699981510639191e-02 -1.612579971551895142e-01 -1.177019998431205750e-01 -8.015000075101852417e-02 7.277499884366989136e-02 1.915600001811981201e-01 4.545899853110313416e-02 6.452199816703796387e-02 8.877599984407424927e-02 2.437759935855865479e-01 -9.606800228357315063e-02 -3.115300089120864868e-02 -2.022900059819221497e-02 -2.312140017747879028e-01 7.338300347328186035e-02 1.087990030646324158e-01 -1.642439961433410645e-01 -1.911730021238327026e-01 1.628849953413009644e-01 8.350600302219390869e-02 -9.356900304555892944e-02 6.587900221347808838e-02 -1.440179944038391113e-01 6.529200077056884766e-02 -2.012650072574615479e-01 1.063790023326873779e-01 7.697399705648422241e-02 3.940900042653083801e-02 8.990000002086162567e-03 1.143810003995895386e-01 4.343099892139434814e-02 1.981980055570602417e-01 1.204520016908645630e-01 1.855230033397674561e-01 8.436899632215499878e-02 1.167200040072202682e-02 -9.245000034570693970e-02 1.728270053863525391e-01 7.206500321626663208e-02 5.402600020170211792e-02 -1.743299961090087891e-01 -5.863900110125541687e-02 -1.506229937076568604e-01 -7.351599633693695068e-02 7.429800182580947876e-02 2.826460003852844238e-01 1.307529956102371216e-01 -7.732100039720535278e-02 -3.964500129222869873e-02 -1.969940066337585449e-01 1.494379937648773193e-01 2.163670063018798828e-01\n```\n\n----------------------------------------\n\nTITLE: Setting Up LDA Training Visualization with Visdom\nDESCRIPTION: Code that defines various evaluation metrics (Coherence, Perplexity, Diff and Convergence) as callbacks for LDA model training visualization and initiates the model training process with visualization enabled through Visdom.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Training_visualizations.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models.callbacks import CoherenceMetric, DiffMetric, PerplexityMetric, ConvergenceMetric\n\n# define perplexity callback for hold_out and test corpus\npl_holdout = PerplexityMetric(corpus=holdout_corpus, logger=\"visdom\", title=\"Perplexity (hold_out)\")\npl_test = PerplexityMetric(corpus=test_corpus, logger=\"visdom\", title=\"Perplexity (test)\")\n\n# define other remaining metrics available\nch_umass = CoherenceMetric(corpus=training_corpus, coherence=\"u_mass\", logger=\"visdom\", title=\"Coherence (u_mass)\")\nch_cv = CoherenceMetric(corpus=training_corpus, texts=training_texts, coherence=\"c_v\", logger=\"visdom\", title=\"Coherence (c_v)\")\ndiff_kl = DiffMetric(distance=\"kullback_leibler\", logger=\"visdom\", title=\"Diff (kullback_leibler)\")\nconvergence_kl = ConvergenceMetric(distance=\"jaccard\", logger=\"visdom\", title=\"Convergence (jaccard)\")\n\ncallbacks = [pl_holdout, pl_test, ch_umass, ch_cv, diff_kl, convergence_kl]\n\n# training LDA model\nmodel = ldamodel.LdaModel(corpus=training_corpus, id2word=dictionary, num_topics=35, passes=50, chunksize=1500, iterations=200, alpha='auto', callbacks=callbacks)\n```\n\n----------------------------------------\n\nTITLE: Word Frequency Data Format\nDESCRIPTION: Tab-separated format showing word frequency data. Each line contains a numeric ID, word form, and frequency count.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/head500.noblanks.cor_wordids.txt#2025-04-21_snippet_9\n\nLANGUAGE: text\nCODE:\n```\n21568\tvanish\t3\n5460\tvaniti\t5\n23641\tvann\t1\n```\n\n----------------------------------------\n\nTITLE: Displaying Word Embedding for ')'\nDESCRIPTION: A 50-dimensional vector representation of the closing parenthesis symbol.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/test_glove.txt#2025-04-21_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n) -0.28314 1.0028 0.14746 0.22262 0.0070985 0.23108 0.57082 -1.2767 -0.72415 -0.7527 0.52624 0.39498 0.0018922 -0.39396 0.44859 -0.019057 0.068143 0.45082 -1.2849 0.68088 -0.48318 -0.45829 0.85504 0.47712 -0.16152 -0.74784 0.40742 -0.97385 -0.7258 -0.17232 3.8901 -0.46535 -0.61925 0.63584 -0.20339 -0.080612 0.64959 -0.51208 0.91193 0.036208 1.0099 0.18802 0.59359 -0.61313 -0.66839 0.67479 0.40625 -0.6959 0.14553 0.37339\n```\n\n----------------------------------------\n\nTITLE: Displaying Word Embedding for '('\nDESCRIPTION: A 50-dimensional vector representation of the opening parenthesis symbol.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/test_glove.txt#2025-04-21_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n( -0.24978 1.0476 0.21602 0.23278 0.12371 0.2761 0.51184 -1.36 -0.6902 -0.66679 0.49105 0.51671 -0.027218 -0.52056 0.49539 -0.097307 0.12779 0.44388 -1.2612 0.66209 -0.55461 -0.43498 0.81247 0.40855 -0.094327 -0.622 0.36498 -1.0038 -0.77693 -0.22408 3.6533 -0.52004 -0.57384 0.72381 -0.24887 -0.14347 0.69169 -0.51861 1.0806 0.20382 1.1045 0.31045 0.60765 -0.64538 -0.60249 0.60803 0.34393 -0.79411 0.15177 0.45779\n```\n\n----------------------------------------\n\nTITLE: Preprocessing and Preparing Data for LDA Training\nDESCRIPTION: Code to load and preprocess the fake news dataset, remove stopwords and punctuation, and prepare the data in the required format for LDA model training with separate training, holdout, and test corpus sets.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Training_visualizations.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models import ldamodel\nfrom gensim.corpora import Dictionary\nimport pandas as pd\nimport re\nfrom gensim.parsing.preprocessing import remove_stopwords, strip_punctuation\n\nimport numpy as np\n\ndf_fake = pd.read_csv('fake.csv')\ndf_fake[['title', 'text', 'language']].head()\ndf_fake = df_fake.loc[(pd.notnull(df_fake.text)) & (df_fake.language == 'english')]\n\n# remove stopwords and punctuations\ndef preprocess(row):\n    return strip_punctuation(remove_stopwords(row.lower()))\n    \ndf_fake['text'] = df_fake['text'].apply(preprocess)\n\n# Convert data to required input format by LDA\ntexts = []\nfor line in df_fake.text:\n    lowered = line.lower()\n    words = re.findall(r'\\w+', lowered, flags = re.UNICODE | re.LOCALE)\n    texts.append(words)\n\ndictionary = Dictionary(texts)\n\ntraining_texts = texts[:5000]\nholdout_texts = texts[5000:7500]\ntest_texts = texts[7500:10000]\n\ntraining_corpus = [dictionary.doc2bow(text) for text in training_texts]\nholdout_corpus = [dictionary.doc2bow(text) for text in holdout_texts]\ntest_corpus = [dictionary.doc2bow(text) for text in test_texts]\n```\n\n----------------------------------------\n\nTITLE: Evaluating Word Embedding Models on Analogy and Similarity Tasks in Python\nDESCRIPTION: This snippet defines functions to calculate and print accuracy scores for word analogy and similarity tasks. It also loads different embedding models and evaluates their performance on various datasets.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Wordrank_comparisons.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\ndef print_analogy_accuracy(model, questions_file):\n    acc = model.accuracy(questions_file)\n\n    sem_correct = sum((len(acc[i]['correct']) for i in range(5)))\n    sem_total = sum((len(acc[i]['correct']) + len(acc[i]['incorrect'])) for i in range(5))\n    sem_acc = 100*float(sem_correct)/sem_total\n    print('\\nSemantic: {:d}/{:d}, Accuracy: {:.2f}%'.format(sem_correct, sem_total, sem_acc))\n    \n    syn_correct = sum((len(acc[i]['correct']) for i in range(5, len(acc)-1)))\n    syn_total = sum((len(acc[i]['correct']) + len(acc[i]['incorrect'])) for i in range(5,len(acc)-1))\n    syn_acc = 100*float(syn_correct)/syn_total\n    print('Syntactic: {:d}/{:d}, Accuracy: {:.2f}%\\n'.format(syn_correct, syn_total, syn_acc))\n    \ndef print_similarity_accuracy(model, similarity_file):\n    acc = model.evaluate_word_pairs(similarity_file)\n    print('Pearson correlation coefficient: {:.2f}'.format(acc[0][0]))\n    print('Spearman rank correlation coefficient: {:.2f}'.format(acc[1][0]))\n```\n\n----------------------------------------\n\nTITLE: Analyzing List of Car Parts\nDESCRIPTION: This snippet contains a list of words related to car parts.  The analysis involves identifying and documenting the type of data and its potential usage in applications related to automotive information or natural language processing.  The content consists of terms such as 'suspension', 'exhaust', 'cylinder', 'brakes', etc.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/ldavowpalwabbit.txt#2025-04-21_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n\"suspension exhaust cylinder brakes cylinder engine exhaust tyre car engine motor brakes motor clutch suspension suspension tyre suspension car engine motor wheel suspension brakes tyre clutch clutch exhaust tyre motor exhaust clutch motor clutch tyre motor clutch wheel tyre suspension suspension clutch suspension exhaust cylinder car brakes brakes exhaust car cylinder engine brakes wheel engine suspension wheel wheel car tyre tyre engine car tyre suspension clutch brakes tyre motor exhaust engine tyre suspension tyre cylinder suspension brakes suspension cylinder cylinder suspension motor suspension tyre car tyre brakes exhaust clutch car suspension tyre cylinder\"\n```\n\n----------------------------------------\n\nTITLE: Analyzing Second List of Food Items\nDESCRIPTION: This snippet contains another list of food items. The purpose is to analyze and document this data, which could be used in various applications such as dietary planning tools, menu generation software, or text processing of food-related documents. The list contains items like 'juice', 'coffee', 'cereal', 'mushrooms', etc.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/ldavowpalwabbit.txt#2025-04-21_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n\"juice coffee cereal coffee coffee mushrooms sausages ham sausages eggs bacon beans mushrooms juice bacon tea beans beans juice cereal bacon mushrooms ham bacon mushrooms tea bacon sausages juice cereal tea ham mushrooms mushrooms ham ham bacon sausages mushrooms eggs sausages cereal tea eggs beans cereal coffee bacon coffee tea cereal mushrooms mushrooms ham cereal bacon eggs cereal sausages tea bacon cereal sausages tea eggs coffee beans beans mushrooms ham coffee bacon tea mushrooms juice sausages bacon beans coffee cereal cereal tea bacon beans coffee ham juice ham tea sausages coffee sausages sausages beans eggs tea coffee ham tea eggs\"\n```\n\n----------------------------------------\n\nTITLE: Training NumPy Poincare Embeddings\nDESCRIPTION: This function trains a Poincare embedding using an external NumPy implementation. It takes various parameters such as script path, data file, output file, dimensions, epochs, and negative samples.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Evaluation.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef train_external_numpy_model(\n    script_path, data_file, output_file, dim, epochs, neg, seed=0):\n    \"\"\"Train a poincare embedding using an external numpy implementation\n    \n    Args:\n        script_path (str): Path to the Python training script\n        data_file (str): Path to tsv file containing relation pairs\n        output_file (str): Path to output file containing model\n        dim (int): Number of dimensions of the trained model\n        epochs (int): Number of epochs to use\n        neg (int): Number of negative samples to use\n    \n    Notes: \n        If `output_file` already exists, skips training\n    \"\"\"\n    if os.path.exists(output_file):\n        print('File %s exists, skipping' % output_file)\n        return\n    args = {\n        'input-file': data_file,\n        'output-file': output_file,\n        'dimensions': dim,\n        'epochs': epochs,\n        'learning-rate': 0.01,\n        'num-negative': neg,\n    }\n    cmd = ['python', script_path]\n    for option, value in args.items():\n        cmd.append(\"--%s\" % option)\n        cmd.append(str(value))\n    \n    return check_output(args=cmd)\n```\n\n----------------------------------------\n\nTITLE: Word Embedding Vector for 'dog'\nDESCRIPTION: This is a 300-dimensional word embedding vector for the word 'dog'. Each dimension represents a different semantic feature extracted during machine learning training. These vectors are used in natural language processing for measuring word similarity and other semantic relationships.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/EN.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_10\n\nLANGUAGE: text\nCODE:\n```\ndog 3.225910067558288574e-01 1.040479987859725952e-01 -9.455999732017517090e-02 2.135719954967498779e-01 -4.748100042343139648e-02 1.039099972695112228e-02 1.930319964885711670e-01 1.516800001263618469e-02 9.286399930715560913e-02 7.242900133132934570e-02 -3.529919981956481934e-01 7.164099812507629395e-02 -6.708600372076034546e-02 -2.029549926519393921e-01 2.544830143451690674e-01 -1.549140065908432007e-01 3.560230135917663574e-01 -4.033749997615814209e-01 1.107620000839233398e-01 -1.343639940023422241e-01 -1.617980003356933594e-01 1.524839997291564941e-01 2.974629998207092285e-01 -1.305029988288879395e-01 1.584949940443038940e-01 4.008919894695281982e-01 -9.224999696016311646e-03 -3.204430043697357178e-01 1.032579988241195679e-01 4.616520106792449951e-01 -2.131700068712234497e-01 -3.487899899482727051e-02 1.519380062818527222e-01 1.018180027604103088e-01 3.541130125522613525e-01 1.810259968042373657e-01 1.858370006084442139e-01 2.146899998188018799e-01 -2.128939926624298096e-01 -2.834430038928985596e-01 1.864000037312507629e-02 1.447049975395202637e-01 1.462700031697750092e-02 2.405229955911636353e-01 3.541849851608276367e-01 -9.546200186014175415e-02 -2.268099971115589142e-02 -8.408500254154205322e-02 2.057050019502639771e-01 -5.336299911141395569e-02 9.473899751901626587e-02 -1.766629964113235474e-01 -1.595900058746337891e-01 -6.777100265026092529e-02 1.513800024986267090e-01 -9.947499632835388184e-02 -2.031800001859664917e-01 5.774699896574020386e-02 -4.753189980983734131e-01 1.355029940605163574e-01 -1.734800077974796295e-02 -2.753629982471466064e-01 2.651200070977210999e-02 -1.431380063295364380e-01 -1.778980046510696411e-01 -3.308419883251190186e-01 -6.661900132894515991e-02 -1.912200003862380981e-01 -7.369899749755859375e-02 2.406470030546188354e-01 5.080699920654296875e-02 8.167800307273864746e-02 1.937689930200576782e-01 -2.345580011606216431e-01 1.228210031986236572e-01 -7.286699861288070679e-02 1.879799962043762207e-01 -1.771569997072219849e-01 1.331550031900405884e-01 -2.901139855384826660e-01 6.073899939656257629e-02 -1.522300019860267639e-02 5.047199875116348267e-02 2.841100096702575684e-01 -1.507360041141510010e-01 -4.384100064635276794e-02 1.417399942874908447e-01 2.885160148143768311e-01 -1.566449999809265137e-01 1.572999954223632812e-01 2.197900041937828064e-02 1.962150037288665771e-01 -6.166499853134155273e-02 -1.869500055909156799e-02 1.225090026855468750e-01 5.296299979090690613e-02 8.646000176668167114e-02 2.198790013790130615e-01 -2.411340028047561646e-01 5.496099963784217834e-02 1.567150056362152100e-01 1.234280019998550415e-01 -1.223120018839836121e-01 8.339700102806091309e-02 9.300000220537185669e-02 -1.596540063619613647e-01 -2.887789905071258545e-01 -8.606100082397460938e-02 8.617900311946868896e-02 4.377700015902519226e-02 7.956299930810928345e-02 -1.452839970588684082e-01 1.586599946022033691e-01 1.068800017237663269e-01 1.063290014863014221e-01 2.565889954566955566e-01 -1.305460035800933838e-01 7.442799955606460571e-02 8.566600084304809570e-02 1.332290023565292358e-01 1.474000066518783569e-01 1.384200006723403931e-01 9.389000013470649719e-03 -1.023999997414648533e-03 -3.033249974250793457e-01 -1.466169953346252441e-01 -2.400300055742263794e-01 -1.677699983119964600e-01 2.684300020337104797e-02 1.811760067939758301e-01 8.740999735891819000e-03 1.463250070810317993e-01 6.726399809122085571e-02 3.232460021972656250e-01 6.013200059533119202e-02 4.063799977302551270e-02 1.251370012760162354e-01 2.066300064325332642e-01 7.170200347900390625e-02 3.811320066452026367e-01 1.089899986982345581e-02 2.287499979138374329e-02 -2.916989922523498535e-01 1.510789990425109863e-01 3.317109942436218262e-01 -3.957799822092056274e-02 -8.791999891400337219e-03 -2.474099956452846527e-02 -2.230400033295154572e-02 1.742289960384368896e-01 -5.978500097990036011e-02 -7.774300128221511841e-02 -5.715699866414070129e-02 -1.434150040149688721e-01 -1.295710057020187378e-01 1.493030041456222534e-01 2.428400069475173950e-01 4.130089879035949707e-01 1.312050074338912964e-01 -5.196300148963928223e-02 2.048839926719665527e-01 -4.181360006332397461e-01 -1.444100029766559601e-02 5.020999908447265625e-02 -9.075599908828735352e-02 2.987090051174163818e-01 -9.519500285387039185e-02 2.341900020837783813e-02 -3.219819962978363037e-01 2.162380069494247437e-01 2.507719993591308594e-01 1.691630035638809204e-01 -1.956200040876865387e-02 -2.059949934482574463e-01 1.127839982509613037e-01 5.047199875116348267e-02 -1.505949944257736206e-01 -3.418599814176559448e-02 3.253270089626312256e-01 7.482600212097167969e-02 -1.717510074377059937e-01 5.522900074720382690e-02 1.103099994361400604e-02 -1.373230069875717163e-01 6.649999995715916157e-04 -3.486619889736175537e-01 9.761100262403488159e-02 1.314570009708404541e-01 -4.414900019764900208e-02 5.709600076079368591e-02 3.192500025033950806e-02 -9.812000207602977753e-03 1.395200043916702271e-01 8.909299969673156738e-02 8.613000065088272095e-02 2.917349934577941895e-01 4.844399914145469666e-02 -3.795199841260910034e-02 7.389000058174133301e-02 -7.066000252962112427e-02 -6.087300181388854980e-02 -3.440000116825103760e-02 1.157729998230934143e-01 -1.455010026693344116e-01 -1.365810036659240723e-01 -1.885969936847686768e-01 2.467889934778213501e-01 -1.655579954385757446e-01 -1.526799984276294708e-02 -7.105500251054763794e-02 3.720799833536148071e-02 2.652399986982345581e-02 1.425330042839050293e-01 -4.398800060153007507e-02 1.608719974756240845e-01 1.801660060882568359e-01 1.875489950180053711e-01 8.896499872207641602e-02 1.416289955377578735e-01 -3.170349895954132080e-01 6.860599666833877563e-02 -1.609670072793960571e-01 4.991099983453750610e-02 -1.974759995937347412e-01 1.432999968528747559e-03 -3.909400105476379395e-01 -1.061189994215965271e-01 -1.462620049715042114e-01 3.446530103683471680e-01 1.513479948043823242e-01 2.355599962174892426e-02 -2.965099923312664032e-02 2.178519964218139648e-01 -3.444499894976615906e-02 -6.090300157666206360e-02 -3.313910067081451416e-01 1.050190031528472900e-01 -1.057000015862286091e-03 1.704169958829879761e-01 -3.662919998168945312e-01 7.753700017929077148e-02 -7.105100154876708984e-02 1.339500024914741516e-02 -2.921069860458374023e-01 3.903700038790702820e-02 -6.489899754524230957e-02 2.880000101868063211e-04 -3.878799825906753540e-02 -6.605499982833862305e-02 -6.450999993830919266e-03 1.089520007371902466e-01 9.650999680161476135e-03 4.966099932789802551e-02 -1.955379992723464966e-01 -1.356189996004104614e-01 6.996099650859832764e-02 4.612800106406211853e-02 -1.060110032558441162e-01 1.472959965467453003e-01 9.419000148773193359e-02 -3.208189904689788818e-01 5.471599847078323364e-02 -2.050540000200271606e-01 -1.472759991884231567e-01 3.397800028324127197e-02 1.044699996709823608e-01 6.230400130152702332e-02 2.061389982700347900e-01 -1.507440060377120972e-01 6.711799651384353638e-02 -1.103179976344108582e-01 1.646240055561065674e-01 -3.976000007241964340e-03 -1.455609947443008423e-01 2.126570045948028564e-01 -1.147119998931884766e-01 2.793720066547393799e-01 -2.220170050859451294e-01 3.736090064048767090e-01 3.508000075817108154e-01 -3.109399974346160889e-02 -5.834199860692024231e-02 6.972700357437133789e-02 -1.171040013432502747e-01 9.581600129604339600e-02 -4.191000014543533325e-02 2.241900004446506500e-02 -3.106760084629058838e-01 2.529579997062683105e-01 5.021899938583374023e-02 -9.259399771690368652e-02 -2.003629952669143677e-01 2.394549995660781860e-01 -3.943900018930435181e-02 1.592200063169002533e-02 3.891099989414215088e-02 8.634000085294246674e-03 -8.326700329780578613e-02 -4.958299919962882996e-02 2.080229967832565308e-01\n```\n\n----------------------------------------\n\nTITLE: Analyzing Fourth List of Food Items\nDESCRIPTION: This snippet contains a list of food items.  The purpose is to document and analyze this information for use in applications such as food tracking, meal recommendation, or dietary analysis software.  The list includes items such as 'beans', 'eggs', 'cereal', 'bacon', etc.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/ldavowpalwabbit.txt#2025-04-21_snippet_9\n\nLANGUAGE: text\nCODE:\n```\n\"beans eggs beans cereal bacon coffee sausages coffee juice coffee sausages bacon mushrooms bacon sausages juice ham tea juice coffee mushrooms coffee juice coffee cereal beans juice sausages beans coffee mushrooms ham ham bacon ham mushrooms cereal beans ham juice bacon juice beans sausages mushrooms cereal coffee eggs mushrooms sausages tea sausages mushrooms ham\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Documentation for FastText Model in gensim\nDESCRIPTION: This is a reStructuredText directive that configures how Sphinx should document the FastText model. It specifies that inherited members, special methods, and undocumented members should be included in the generated documentation.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/models/fasttext.rst#2025-04-21_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. automodule:: gensim.models.fasttext\n    :synopsis: FastText model\n    :members:\n    :inherited-members:\n    :special-members: __getitem__, __contains__\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Word Vector Numerical Data\nDESCRIPTION: A 300-dimensional word vector for the word 'gatto', containing floating point values that represent semantic features in an embedding space. The vector starts with the word followed by space-separated numerical coefficients.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/IT.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_10\n\nLANGUAGE: txt\nCODE:\n```\ngatto -6.385300308465957642e-02 3.576999902725219727e-01 1.793400011956691742e-02 -7.251100242137908936e-02 5.944349765777587891e-01 1.438099984079599380e-02 -8.167999796569347382e-03 -1.336279958486557007e-01 6.965000182390213013e-02 3.317329883575439453e-01 -5.137100070714950562e-02 3.006669878959655762e-01 3.514220118522644043e-01 3.108200058341026306e-02 -4.011299833655357361e-02 2.131620049476623535e-01 2.136919945478439331e-01 -8.712299913167953491e-02 -2.524200081825256348e-01 -5.555019974708557129e-01 [...]\n```\n\n----------------------------------------\n\nTITLE: Analyzing Third List of Programming Languages\nDESCRIPTION: This snippet provides a list of programming languages.  The data can be used for language recognition, generating random language lists for testing, or identifying potential languages for development projects. The languages in the list include 'go', 'ruby', 'cplusplus', 'c', 'erlang', etc.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/ldavowpalwabbit.txt#2025-04-21_snippet_20\n\nLANGUAGE: text\nCODE:\n```\n\"go ruby cplusplus c cplusplus erlang ruby cplusplus ruby c ruby csharp haskell cplusplus go erlang ruby scala java erlang python scala erlang java go haskell c go scala haskell haskell csharp csharp java python python\"\n```\n\n----------------------------------------\n\nTITLE: Importing LSI Dispatcher Module in Python\nDESCRIPTION: This code snippet demonstrates how to import the lsi_dispatcher module from gensim.models. The module provides functionality for distributed Latent Semantic Indexing.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/models/lsi_dispatcher.rst#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom gensim.models import lsi_dispatcher\n```\n\n----------------------------------------\n\nTITLE: Sphinx Module Documentation for LDA Dispatcher in restructuredtext\nDESCRIPTION: This code defines the Sphinx documentation setup for the lda_dispatcher module in Gensim. It specifies the module name, synopsis, and documentation options to include members and inherited members.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/models/lda_dispatcher.rst#2025-04-21_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n:mod:`models.lda_dispatcher` -- Dispatcher for distributed LDA\n================================================================\n\n.. automodule:: gensim.models.lda_dispatcher\n    :synopsis: Dispatcher for distributed LDA\n    :members:\n    :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Analyzing Third List of Names\nDESCRIPTION: This snippet contains a list of names. The purpose is to process and document this list of names, for applications related to data sampling or name analysis. The list of names include 'tim', 'alex', 'jim', 'harry', etc.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/ldavowpalwabbit.txt#2025-04-21_snippet_7\n\nLANGUAGE: text\nCODE:\n```\n\"tim alex alex jim harry sue sue sue rachel tim alice harry robert bob rachel harry alex robert harry robert jim harry tim robert jim bob bob jim rachel alex jim rachel rachel robert robert alex bob robert robert dave harry dave alice alice dave bob sue rachel sue rachel harry dave alex rachel alex sue alex sue tim tim dave rachel alice tim tim sue alex tim rachel sue dave alex jim bob bob tim jim rachel rachel harry jim alex jim jim dave sue robert robert rachel alice sue jim rachel tim\"\n```\n\n----------------------------------------\n\nTITLE: Sphinx Documentation Directive for CSV Corpus Module\nDESCRIPTION: Sphinx documentation directive for the gensim.corpora.csvcorpus module. Configures documentation to include all members, inherited members, undocumented members and show inheritance relationships.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/corpora/csvcorpus.rst#2025-04-21_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. automodule:: gensim.corpora.csvcorpus\n    :synopsis: Corpus in CSV format\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Image Output Function in Python\nDESCRIPTION: This function creates and saves an image without borders using matplotlib. It takes data, output filename, size, and DPI as parameters.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/tools/wordcloud.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef make_image(data, outputname, size=(1, 1), dpi=80):\n    #\n    # https://stackoverflow.com/questions/9295026/matplotlib-plots-removing-axis-legends-and-white-spaces\n    #\n    # Argh, getting the image saved without any borders is such a PITA\n    #\n    fig = plt.figure()\n    fig.set_size_inches(size)\n    ax = plt.Axes(fig, [0., 0., 1., 1.])\n    ax.set_axis_off()\n    fig.add_axes(ax)\n    plt.set_cmap('hot')\n    ax.imshow(data, aspect='equal')\n    plt.savefig(outputname, dpi=dpi)\n```\n\n----------------------------------------\n\nTITLE: Enabling Inline Matplotlib Plotting in Python\nDESCRIPTION: This magic command enables inline plotting for Matplotlib in Jupyter notebooks.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_annoy.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%matplotlib inline\n```\n\n----------------------------------------\n\nTITLE: Analyzing Second List of Names\nDESCRIPTION: This snippet contains another list of names.  The purpose is to analyze and document this data, potentially for use in generating sample data for testing, or for name frequency analysis.  The list contains names such as 'alice', 'bob', 'jim', 'dave', etc.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/ldavowpalwabbit.txt#2025-04-21_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n\"alice bob jim bob alice jim dave bob alice dave rachel harry sue sue alex tim alex tim bob rachel rachel harry harry alex dave sue jim harry harry tim dave dave sue harry sue rachel tim harry dave alex sue rachel dave alex rachel alex rachel harry harry alice bob bob harry rachel robert sue alice robert bob rachel sue bob alex bob robert dave dave alex harry alex bob bob jim harry bob bob bob harry rachel harry harry alice dave sue tim sue robert\"\n```\n\n----------------------------------------\n\nTITLE: Analyzing Second List of Car Parts\nDESCRIPTION: This snippet contains another list of words related to car parts. Similar to the first list, the goal is to analyze and document the purpose and usage of this data. This could involve automotive applications, data analysis, or natural language processing tasks. The list includes terms such as 'brakes', 'wheel', 'engine', 'clutch', etc.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/ldavowpalwabbit.txt#2025-04-21_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n\"brakes wheel engine clutch cylinder engine car exhaust brakes suspension wheel motor wheel exhaust engine motor motor suspension clutch motor suspension cylinder car wheel engine wheel wheel suspension brakes brakes clutch engine suspension suspension suspension cylinder exhaust exhaust clutch wheel exhaust brakes car clutch brakes suspension wheel exhaust suspension suspension engine suspension brakes exhaust motor engine motor clutch exhaust motor car suspension motor clutch cylinder car tyre cylinder clutch tyre engine tyre exhaust wheel brakes brakes brakes clutch brakes motor exhaust tyre suspension clutch motor cylinder motor suspension brakes engine tyre engine\"\n```\n\n----------------------------------------\n\nTITLE: Analyzing List of Animals (Fourth Time)\nDESCRIPTION: This snippet contains yet another list of animal names. This dataset can be used to train language models, create educational games, or perform animal-related text analysis.  The animal names include 'mouse', 'kitten', 'lion', 'tiger', etc.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/ldavowpalwabbit.txt#2025-04-21_snippet_23\n\nLANGUAGE: text\nCODE:\n```\n\"mouse kitten lion tiger mouse cat cheetah puppy tiger cheetah cheetah mouse puppy kitten leopard puppy lion jaguar cat jaguar mouse tiger kitten cheetah jaguar puppy cat jaguar cat jaguar kitten cat cat tiger mouse leopard cat lion cheetah lynx tiger jaguar cat leopard leopard leopard tiger leopard mouse jaguar leopard tiger cat jaguar lynx cheetah tiger cheetah puppy cat puppy lion lynx cheetah jaguar kitten leopard puppy jaguar mouse lion cat cat puppy tiger jaguar puppy kitten kitten lion mouse lynx puppy puppy puppy\"\n```\n\n----------------------------------------\n\nTITLE: Analyzing Third List of Food Items\nDESCRIPTION: This snippet consists of a list of food items. The intention is to analyze and document this dataset for potential usage in applications such as meal planning, dietary analysis, or food inventory management. The list includes items like 'coffee', 'cereal', 'juice', 'bacon', etc.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/ldavowpalwabbit.txt#2025-04-21_snippet_8\n\nLANGUAGE: text\nCODE:\n```\n\"coffee cereal juice bacon coffee cereal beans ham juice mushrooms beans mushrooms cereal sausages beans tea sausages juice coffee sausages eggs eggs sausages tea eggs tea cereal tea eggs juice cereal eggs bacon juice bacon eggs beans beans mushrooms mushrooms ham eggs sausages cereal beans juice bacon beans tea coffee bacon sausages juice bacon mushrooms bacon cereal eggs sausages juice ham sausages sausages beans ham ham beans cereal bacon eggs eggs juice bacon ham bacon sausages mushrooms ham bacon cereal mushrooms beans beans ham coffee tea sausages ham ham tea cereal mushrooms beans mushrooms bacon\"\n```\n\n----------------------------------------\n\nTITLE: Generating Word Cloud with Custom Mask in Python\nDESCRIPTION: This function creates a word cloud using a custom mask image and input text. It uses the wordcloud library to generate the cloud and saves it to a file.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/tools/wordcloud.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef make_cloud(mask_path, text, out_path):\n    mask = 255 - np.array(PIL.Image.open(mask_path))\n    \n    cloud = wordcloud.WordCloud(\n        mask=mask,\n        contour_width=3, \n        contour_color='steelblue',\n        max_words=50,\n        repeat=True,\n    ).generate(text)\n    \n    cloud.to_file(out_path)\n    \n    plt.axis(\"off\")\n    plt.imshow(cloud, interpolation=\"bilinear\")\n    plt.show()\n```\n\n----------------------------------------\n\nTITLE: Cloning and Patching External Poincare Embedding Implementations\nDESCRIPTION: Clones the C++ and NumPy implementations of Poincare embeddings from GitHub repositories and applies patches to them. It also compiles the C++ source code.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Poincare Evaluation.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncurrent_directory = os.getcwd()\n\n# Change this variable to `False` to not remove and re-download repos for external implementations\nforce_setup = False\n\n# The poincare datasets, models and source code for external models are downloaded to this directory\nparent_directory = os.path.join(current_directory, 'poincare')\n! mkdir -p {parent_directory}\n\n%cd {parent_directory}\n\n# Clone repos\nnp_repo_name = 'poincare-np-embedding'\nif force_setup and os.path.exists(np_repo_name):\n    ! rm -rf {np_repo_name}\nclone_np_repo = not os.path.exists(np_repo_name)\nif clone_np_repo:\n    ! git clone https://github.com/nishnik/poincare_embeddings.git {np_repo_name}\n\ncpp_repo_name = 'poincare-cpp-embedding'\nif force_setup and os.path.exists(cpp_repo_name):\n    ! rm -rf {cpp_repo_name}\nclone_cpp_repo = not os.path.exists(cpp_repo_name)\nif clone_cpp_repo:\n    ! git clone https://github.com/TatsuyaShirakawa/poincare-embedding.git {cpp_repo_name}\n\npatches_applied = False\n\n# Apply patches\nif clone_cpp_repo and not patches_applied:\n    %cd {cpp_repo_name}\n    ! git apply ../poincare_burn_in_eps.patch\n\nif clone_np_repo and not patches_applied:\n    %cd ../{np_repo_name}\n    ! git apply ../poincare_numpy.patch\n    \npatches_applied = True\n\n# Compile the code for the external c++ implementation into a binary\n%cd {parent_directory}/{cpp_repo_name}\n!mkdir -p work\n%cd work\n!cmake ..\n!make\n%cd {current_directory}\n\ncpp_binary_path = os.path.join(parent_directory, cpp_repo_name, 'work', 'poincare_embedding')\nassert(os.path.exists(cpp_binary_path)), 'Binary file doesnt exist at %s' % cpp_binary_path\n```\n\n----------------------------------------\n\nTITLE: Analyzing Seventh List of Food Items\nDESCRIPTION: This snippet contains another list of food items, which will be documented for applications related to food analysis and inventory. The food items include 'beans', 'tea', 'cereal', 'bacon', etc.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/ldavowpalwabbit.txt#2025-04-21_snippet_21\n\nLANGUAGE: text\nCODE:\n```\n\"beans tea cereal bacon tea bacon beans sausages cereal tea bacon bacon beans cereal sausages ham beans eggs tea ham bacon ham beans eggs ham cereal tea mushrooms coffee sausages coffee coffee bacon tea ham cereal ham coffee ham juice mushrooms eggs sausages juice eggs mushrooms\"\n```\n\n----------------------------------------\n\nTITLE: Analyzing Sixth List of Car Parts\nDESCRIPTION: This snippet represents yet another list of words relating to car parts.  The information can be used for creating automotive-related applications or for text analysis of automotive content.  The list includes terms such as 'car', 'engine', 'motor', 'wheel', etc.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/ldavowpalwabbit.txt#2025-04-21_snippet_18\n\nLANGUAGE: text\nCODE:\n```\n\"car engine motor engine car exhaust motor wheel clutch car tyre car suspension wheel wheel cylinder cylinder brakes wheel motor motor wheel exhaust clutch engine wheel wheel cylinder suspension tyre clutch brakes tyre exhaust suspension car cylinder clutch motor motor\"\n```\n\n----------------------------------------\n\nTITLE: Loading VarEmbed Model in Gensim\nDESCRIPTION: Code to load a pre-trained VarEmbed model into Gensim using a pickle file containing word vectors. This demonstrates the basic loading functionality without morphological information.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/Varembed.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom gensim.models.wrappers import varembed\n\nvector_file = '../../gensim/test/test_data/varembed_leecorpus_vectors.pkl'\nmodel = varembed.VarEmbed.load_varembed_format(vectors=vector_file)\n```\n\n----------------------------------------\n\nTITLE: Comparing Document Similarities\nDESCRIPTION: Prints and compares document similarities for different ranking levels\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_doc2vec_lee.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nprint('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\nprint(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\nfor label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))\n```\n\n----------------------------------------\n\nTITLE: Calculating Document Influence in DIM\nDESCRIPTION: Demonstrates how to access the influence score of a specific document on a particular topic within a given time slice using the influences_time attribute of the model.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/dtm_example.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndocument_no = 1 #document 2\ntopic_no = 1 #topic number 2\ntime_slice = 0 #time slice 1\n\nmodel.influences_time[time_slice][document_no][topic_no]\n```\n\n----------------------------------------\n\nTITLE: Sphinx Module Documentation Directive for TextCorpus\nDESCRIPTION: Sphinx documentation directive that configures the autodoc generation for the gensim.corpora.textcorpus module, including all members, inherited members, and undocumented members.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/corpora/textcorpus.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: gensim.corpora.textcorpus\n    :synopsis: Tools for building corpora with dictionaries\n    :members:\n    :inherited-members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Updating S3 Bucket Lifecycle Configuration using AWS CLI\nDESCRIPTION: AWS CLI command to update the bucket lifecycle configuration for gensim-wheels bucket using a local JSON configuration file. Requires AWS CLI with smart_open profile configured.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/continuous_integration/BucketLifecycleConfiguration.txt#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\naws --profile smart_open s3api put-bucket-lifecycle-configuration --bucket gensim-wheels --lifecycle-configuration file://continuous_integration/BucketLifecycleConfiguration.json\n```\n\n----------------------------------------\n\nTITLE: 300-Dimensional Word Vector for 'banana'\nDESCRIPTION: This snippet represents a 300-dimensional vector for the word 'banana'. Each number corresponds to a dimension in the vector space, capturing semantic information about the word. Such vectors are typically used in word embedding models for natural language processing tasks.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/EN.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_17\n\nLANGUAGE: text\nCODE:\n```\nbanana 1.681350022554397583e-01 4.239000007510185242e-02 -4.317559897899627686e-01 -2.153149992227554321e-01 -3.559599816799163818e-02 1.562460064888000488e-01 -4.426000174134969711e-03 1.236099973320960999e-01 -2.279200032353401184e-02 2.076599933207035065e-02 -1.967789977788925171e-01 2.440280020236968994e-01 -1.584240049123764038e-01 -3.443630039691925049e-01 -7.949399948120117188e-02 -1.934749931097030640e-01 -1.268299967050552368e-01 -2.487000077962875366e-02 4.290900006890296936e-02 -4.859659969806671143e-01 -6.306900084018707275e-02 3.224999904632568359e-01 -5.505299940705299377e-02 -1.721099950373172760e-02 1.816260069608688354e-01 4.017759859561920166e-01 -1.164480000734329224e-01 -1.510629951953887939e-01 -2.099999925121665001e-03 1.657480001449584961e-01 -1.085790023207664490e-01 4.477500170469284058e-02 -2.155050039291381836e-01 3.052300028502941132e-02 -6.105799973011016846e-02 2.524600028991699219e-01 8.036399632692337036e-02 -8.784099668264389038e-02 -1.470140069723129272e-01 -6.580299884080886841e-02 2.311299927532672882e-02 -2.684330046176910400e-01 -2.651599943637847900e-01 -1.685920059680938721e-01 2.492100000381469727e-01 4.358999896794557571e-03 3.933010101318359375e-01 -9.549000300467014313e-03 -1.339319944381713867e-01 1.272390037775039673e-01 -1.152390018105506897e-01 -1.898410022258758545e-01 1.235250011086463928e-01 4.560599848628044128e-02 -2.909590005874633789e-01 -9.001799672842025757e-02 -6.960599869489669800e-02 4.678000137209892273e-02 -1.963199973106384277e-01 -1.609210073947906494e-01 -5.649200081825256348e-02 -1.983319967985153198e-01 2.749699912965297699e-02 -6.798599660396575928e-02 -1.584549993276596069e-01 -7.656600326299667358e-02 9.133999794721603394e-02 3.903099894523620605e-02 3.201099857687950134e-02 1.430640071630477905e-01 -1.476499997079372406e-02 -1.052270010113716125e-01 1.057370007038116455e-01 -3.838140070438385010e-01 -3.570999950170516968e-03 -9.158699959516525269e-02 2.101089954376220703e-01 2.645500004291534424e-02 -4.153899848461151123e-02 -1.454219967126846313e-01 -2.839339971542358398e-01 3.353230059146881104e-01 1.407279968261718750e-01 3.768219947814941406e-01 2.361010015010833740e-01 -6.590200215578079224e-02 2.231310009956359863e-01 -8.986599743366241455e-02 2.869400009512901306e-02 -1.758150011301040649e-01 1.689449995756149292e-01 -2.820209860801696777e-01 -5.417799949645996094e-02 1.255020052194595337e-01 -1.662250012159347534e-01 1.153990030288696289e-01 4.620319902896881104e-01 3.680630028247833252e-01 -4.244709908962249756e-01 -2.745999954640865326e-02 -1.178229972720146179e-01 -3.518230020999908447e-01 -1.579329967498779297e-01 4.539300128817558289e-02 -3.459800034761428833e-02 -1.119379997253417969e-01 2.127199992537498474e-02 -2.089460045099258423e-01 7.342900335788726807e-02 -1.133930012583732605e-01 8.111499994993209839e-02 6.359000224620103836e-03 -1.515900045633316040e-01 -4.787600040435791016e-02 -1.034950017929077148e-01 -2.036200016736984253e-01 1.404130011796951294e-01 -2.146600000560283661e-02 4.761099815368652344e-02 -7.508300244808197021e-02 -7.811400294303894043e-02 2.212000079452991486e-03 4.225900024175643921e-02 -9.715200215578079224e-02 1.062519997358322144e-01 -2.929599955677986145e-02 -1.440999954938888550e-01 1.254789978265762329e-01 3.198200091719627380e-02 -1.156760007143020630e-01 4.158020019531250000e-01 -5.022599920630455017e-02 -1.638710051774978638e-01 -1.775189936161041260e-01 -6.445299834012985229e-02 1.785009950399398804e-01 -2.096620053052902222e-01 1.084380000829696655e-01 -1.220699995756149292e-01 -2.797580063343048096e-01 -1.091829985380172729e-01 7.639700174331665039e-02 -5.619399994611740112e-02 1.387120038270950317e-01 -2.138810008764266968e-01 2.634280025959014893e-01 -2.963999984785914421e-03 -2.276329994201660156e-01 -6.339999963529407978e-04 -4.817299917340278625e-02 2.723029851913452148e-01 2.939350008964538574e-01 -3.418610095977783203e-01 3.303899988532066345e-02 1.021879985928535461e-01 9.271500259637832642e-02 -1.438689976930618286e-01 -3.559789955615997314e-01 -1.345680058002471924e-01 2.413699962198734283e-02 2.488840073347091675e-01 -1.949460059404373169e-01 -1.261049956083297729e-01 3.325400128960609436e-02 1.820099949836730957e-01 1.181749999523162842e-01 8.873199671506881714e-02 -1.467290073633193970e-01 -1.757459938526153564e-01 2.684690058231353760e-01 -3.584799915552139282e-02 -1.111679971218109131e-01 3.656879961490631104e-01 2.966799959540367126e-02 -1.038789972662925720e-01 3.046469986438751221e-01 1.439000014215707779e-03 8.898899704217910767e-02 1.219089999794960022e-01 -2.761499956250190735e-02 1.462150067090988159e-01 -3.556399941444396973e-01 -7.613999769091606140e-03 -2.243819981813430786e-01 -7.707200199365615845e-02 -1.730770021677017212e-01 -2.504230141639709473e-01 1.290399953722953796e-02 -6.587799638509750366e-02 6.026399880647659302e-02 1.606789976358413696e-01 1.819400042295455933e-01 1.500889956951141357e-01 1.020219996571540833e-01 -1.523209959268569946e-01 -2.711830139160156250e-01 -1.769569963216781616e-01 3.045949935913085938e-01 -3.871209919452667236e-01 -1.955299973487854004e-01 -2.865040004253387451e-01 -1.592209935188293457e-01 9.539200365543365479e-02 -1.188179999589920044e-01 -3.241289854049682617e-01 -1.847000047564506531e-02 4.518440067768096924e-01 -2.615849971771240234e-01 3.100699931383132935e-02 1.222779974341392517e-01 1.581490039825439453e-01 -9.039899706840515137e-02 2.182770073413848877e-01 1.015689969062805176e-01 -1.913550049066543579e-01 1.228040009737014771e-01 9.706600010395050049e-02 -3.515500128269195557e-01 -3.541599959135055542e-02 5.642800033092498779e-02 1.081049963831901550e-01 7.913199812173843384e-02 -6.892400234937667847e-02 -2.461000112816691399e-03 -3.253290057182312012e-01 -1.852950006723403931e-01 -1.108189970254898071e-01 -2.741329967975616455e-01 1.436849981546401978e-01 3.026350140571594238e-01 -1.501310020685195923e-01 8.609700202941894531e-02 6.328999996185302734e-02 -2.397679984569549561e-01 2.922900021076202393e-02 -1.233860030770301819e-01 -1.055570021271705627e-01 6.989099830389022827e-02 -8.719000220298767090e-02 -3.109399974346160889e-02 1.521490067243576050e-01 -1.909720003604888916e-01 -1.092469990253448486e-01 -7.005999796092510223e-03 2.557890117168426514e-01 -1.032780036330223083e-01 -5.136400088667869568e-02 -8.267399668693542480e-02 -1.543409973382949829e-01 1.269810050725936890e-01 -1.618340015411376953e-01 -2.749280035495758057e-01 6.683000177145004272e-02 -2.379879951477050781e-01 9.348000399768352509e-03 -1.854900084435939789e-02 -1.861399970948696136e-02 2.155200019478797913e-02 2.568190097808837891e-01 -7.048999890685081482e-03 8.013200014829635620e-02 -1.556369960308074951e-01 5.841000005602836609e-03 2.010199986398220062e-02 1.334680020809173584e-01 -1.940329968929290771e-01 -2.967210114002227783e-01 8.934099972248077393e-02 1.320019960403442383e-01 -4.420800134539604187e-02 -6.053400039672851562e-02 -2.730100043118000031e-02 -8.021400123834609985e-02 8.333200216293334961e-02 2.963699959218502045e-02 6.170700117945671082e-02 -1.696999999694526196e-03 -1.083519980311393738e-01 1.548739969730377197e-01 -1.871069967746734619e-01 -5.718199908733367920e-02 -7.110700011253356934e-02 8.039599657058715820e-02 2.568059861660003662e-01 1.345510035753250122e-01 2.893350124359130859e-01 8.333200216293334961e-02 1.366799976676702499e-02 6.942600011825561523e-02 -2.174399979412555695e-02 2.027399986982345581e-01 -2.093620002269744873e-01 -3.132300078868865967e-02 -9.503900259733200073e-02 3.464100137352943420e-02 -7.952699810266494751e-02 2.000500075519084930e-02 8.464500308036804199e-02 2.436379939317703247e-01 -1.887110024690628052e-01\n```\n\n----------------------------------------\n\nTITLE: Word Vector Data Row\nDESCRIPTION: A single line from a word vector file showing the word 'four' followed by 300-dimensional vector components represented as floating point numbers in scientific notation.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/EN.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nfour -2.373999916017055511e-03 7.336699962615966797e-02 -7.257899641990661621e-02 -8.272500336170196533e-02 1.300700008869171143e-02 1.547800004482269287e-01 6.543699651956558228e-02 -3.014099970459938049e-02 1.431650072336196899e-01 3.217000048607587814e-03 1.437170058488845825e-01 2.542549967765808105e-01 -1.573299989104270935e-02 1.126160025596618652e-01 3.773299977183341980e-02 -4.634699970483779907e-02 1.747499965131282806e-02 1.523849964141845703e-01 1.600250005722045898e-01 ...\n```\n\n----------------------------------------\n\nTITLE: Importing multiprocessing and psutil modules\nDESCRIPTION: Imports necessary modules for demonstrating and measuring memory usage in multiprocessing scenarios with Nmslib indices.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmslibtutorial.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom multiprocessing import Process\nimport psutil\n```\n\n----------------------------------------\n\nTITLE: Word Embedding Vector Representation in Text Format\nDESCRIPTION: A 300-dimensional word embedding vector for the word 'one'. The file begins with a header '20 300' indicating there are 20 words with 300 dimensions each, though only one word is shown in this snippet.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/EN.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n20 300\none -1.671300083398818970e-02 7.239600270986557007e-02 -6.052500009536743164e-02 -1.076750010251998901e-01 -2.389900013804435730e-02 1.234359964728355408e-01 -1.060480028390884399e-01 -2.284700050950050354e-02 1.972600072622299194e-02 -1.655009984970092773e-01 1.506949961185455322e-01 1.362709999084472656e-01 -6.062199920415878296e-02 -2.170900069177150726e-02 6.102799996733665466e-02 -4.963700100779533386e-02 -1.533399987965822220e-02 -2.378099970519542694e-02 7.065500319004058838e-02 -2.022099941968917847e-01 -4.202999919652938843e-02 1.386999990791082382e-02 3.320400044322013855e-02 -8.138500154018402100e-02 -5.338999908417463303e-03 1.249400004744529724e-01 -5.267399922013282776e-02 -3.647600114345550537e-02 4.680000129155814648e-04 7.500600069761276245e-02 -9.056700021028518677e-02 -1.285099983215332031e-02 4.935999959707260132e-02 1.208909973502159119e-01 1.411760002374649048e-01 1.143629997968673706e-01 3.310500085353851318e-02 1.263629943132400513e-01 -1.153459995985031128e-01 -1.699080020189285278e-01 -7.231000065803527832e-02 -1.481499969959259033e-01 2.031099982559680939e-02 -2.160000003641471267e-04 -1.367000024765729904e-02 -6.461100280284881592e-02 1.737120002508163452e-01 7.375399768352508545e-02 -2.499799989163875580e-02 -1.646919995546340942e-01 9.968000464141368866e-03 -6.150399893522262573e-02 -8.114799857139587402e-02 -2.592400088906288147e-02 3.627600148320198059e-02 1.112699974328279495e-02 9.238199889659881592e-02 1.957800053060054779e-02 -3.782700002193450928e-02 6.264999974519014359e-03 -3.378900140523910522e-02 7.094999775290489197e-03 3.512499853968620300e-02 -9.201000444591045380e-03 -1.342889964580535889e-01 1.259389966726303101e-01 -1.059999968856573105e-02 1.114490032196044922e-01 -2.489200048148632050e-02 4.227500036358833313e-02 -7.338900119066238403e-02 1.692380011081695557e-01 3.674200177192687988e-02 -7.658500224351882935e-02 2.284550070762634277e-01 5.876000039279460907e-03 1.240499969571828842e-02 -1.557669937610626221e-01 -1.047440022230148315e-01 8.771699666976928711e-02 -1.761599928140640259e-01 -8.594000339508056641e-02 -2.514399960637092590e-02 1.050760000944137573e-01 1.488899998366832733e-02 8.937399834394454956e-02 -2.758700028061866760e-02 7.015900313854217529e-02 -8.793000131845474243e-02 -1.265799999237060547e-02 1.976200006902217865e-02 1.188530027866363525e-01 -7.451800256967544556e-02 -3.582900017499923706e-02 -1.434000022709369659e-02 -7.825800031423568726e-02 8.541300147771835327e-02 7.579399645328521729e-02 -5.767299979925155640e-02 1.657470017671585083e-01 3.662800043821334839e-02 -1.534530073404312134e-01 7.708799839019775391e-02 1.888000033795833588e-02 -5.443599820137023926e-02 -4.567100107669830322e-02 1.559599936008453369e-01 -7.609999924898147583e-02 6.928600370883941650e-02 -2.282740026712417603e-01 -1.260109990835189819e-01 1.147430017590522766e-01 -1.320360004901885986e-01 -8.095700293779373169e-02 7.371500134468078613e-02 8.162199705839157104e-02 -1.262319982051849365e-01 -5.962000228464603424e-03 -1.748500019311904907e-02 4.819999914616346359e-03 -6.047699972987174988e-02 2.601700089871883392e-02 1.082879975438117981e-01 -1.732099987566471100e-02 5.485000088810920715e-02 7.255599647760391235e-02 4.840600118041038513e-02 1.932200044393539429e-02 3.987399861216545105e-02 5.295000039041042328e-03 3.871199861168861389e-02 6.900000153109431267e-05 2.053519934415817261e-01 -1.246969997882843018e-01 -1.201739981770515442e-01 3.654599934816360474e-02 -6.165599822998046875e-02 -2.034779936075210571e-01 1.024150028824806213e-01 -7.133100181818008423e-02 -7.750099897384643555e-02 -7.030700147151947021e-02 5.472999997437000275e-03 -6.108099967241287231e-02 -1.066310033202171326e-01 5.899399891495704651e-02 -7.210200279951095581e-02 6.202999874949455261e-02 1.307439953088760376e-01 1.033020019531250000e-01 9.240999817848205566e-03 -2.285999990999698639e-02 -1.878979951143264771e-01 1.752799935638904572e-02 1.011520028114318848e-01 6.858699768781661987e-02 6.353200227022171021e-02 -6.002100184559822083e-02 -1.896450072526931763e-01 1.074080020189285278e-01 9.291899949312210083e-02 -1.812009960412979126e-01 -3.820899873971939087e-02 -1.032280027866363525e-01 -7.671599835157394409e-02 -3.802800178527832031e-02 1.175540015101432800e-01 4.937000107020139694e-03 1.019729971885681152e-01 -4.910499975085258484e-02 -3.667600080370903015e-02 3.371600061655044556e-02 -7.602000236511230469e-02 -8.279500156641006470e-02 1.138999983668327332e-01 4.635399952530860901e-02 3.553999960422515869e-02 1.460999995470046997e-02 4.136899858713150024e-02 -3.484399989247322083e-02 2.009099908173084259e-02 8.379399776458740234e-02 -2.716300077736377716e-02 -3.489299863576889038e-02 -7.507500052452087402e-02 1.316349953413009644e-01 -1.374319940805435181e-01 -9.518399834632873535e-02 -6.918399780988693237e-02 4.416900128126144409e-02 -1.338630020618438721e-01 5.714900046586990356e-02 5.605000071227550507e-03 -6.328500062227249146e-02 -3.968900069594383240e-02 1.044460013508796692e-01 1.012490019202232361e-01 -3.939199820160865784e-02 1.371890008449554443e-01 -1.866200007498264313e-02 1.071960031986236572e-01 5.449999868869781494e-02 4.136399924755096436e-02 6.846400350332260132e-02 4.723300039768218994e-02 8.769000321626663208e-02 1.366309970617294312e-01 6.490000057965517044e-03 -1.244799979031085968e-02 -1.196409985423088074e-01 1.414700001478195190e-01 -1.147020012140274048e-01 9.805899858474731445e-02 -1.143919974565505981e-01 -3.485000133514404297e-02 -9.628300368785858154e-02 -3.636699914932250977e-02 -1.358730047941207886e-01 -4.812299832701683044e-02 7.660000119358301163e-03 9.196899831295013428e-02 -2.058500051498413086e-02 1.874150037765502930e-01 7.090000063180923462e-02 -7.355999946594238281e-02 2.234599925577640533e-02 5.268900096416473389e-02 -2.327699959278106689e-02 4.333399981260299683e-02 -1.515990048646926880e-01 4.122500121593475342e-02 1.313100010156631470e-02 2.707800082862377167e-02 1.882199943065643311e-02 -4.537900164723396301e-02 5.262000020593404770e-03 -1.256200019270181656e-02 1.470400020480155945e-02 1.770800054073333740e-01 -1.333499979227781296e-02 -2.009700052440166473e-02 -4.736400023102760315e-02 -4.740000003948807716e-04 -6.520999968051910400e-02 -3.003999963402748108e-02 8.188500255346298218e-02 9.359000250697135925e-03 -3.931999951601028442e-02 -9.453999809920787811e-03 -2.900899946689605713e-02 5.978700146079063416e-02 -1.900559961795806885e-01 1.063809990882873535e-01 -1.447069942951202393e-01 7.213500142097473145e-02 -1.224069967865943909e-01 -9.964600205421447754e-02 -7.471299916505813599e-02 2.089589983224868774e-01 1.635919958353042603e-01 -5.477600172162055969e-02 -6.420700252056121826e-02 -2.158560007810592651e-01 -2.318840026855468750e-01 -5.500800162553787231e-02 6.673300266265869141e-02 4.069999791681766510e-03 -4.269099980592727661e-02 2.347799949347972870e-02 -7.604599744081497192e-02 -1.148160025477409363e-01 -1.903899945318698883e-02 6.285899877548217773e-02 -9.319200366735458374e-02 -6.497500091791152954e-02 -1.119590029120445251e-01 -3.645600005984306335e-02 -6.783299893140792847e-02 7.802599668502807617e-02 -3.742900118231773376e-02 -4.273900017142295837e-02 4.238300025463104248e-02 -4.730999842286109924e-02 5.203400179743766785e-02 -2.157000079751014709e-02 -1.444009989500045776e-01 9.435500204563140869e-02 -9.086599946022033691e-02 5.617399886250495911e-02 6.904000043869018555e-02 4.339899867773056030e-02 5.562400072813034058e-02 4.579500108957290649e-02 5.447100102901458740e-02 6.177999824285507202e-02 1.594299972057342529e-01 -2.531999954953789711e-03 2.091609984636306763e-01 -2.417000010609626770e-02 -1.405800040811300278e-02\n```\n\n----------------------------------------\n\nTITLE: Sorting and Displaying Similarity Results\nDESCRIPTION: Sorts the similarity results in descending order and prints them with the corresponding documents.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_similarity_queries.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsims = sorted(enumerate(sims), key=lambda item: -item[1])\nfor doc_position, doc_score in sims:\n    print(doc_score, documents[doc_position])\n```\n\n----------------------------------------\n\nTITLE: Training DTM Model in Python\nDESCRIPTION: Initializes and trains the Dynamic Topic Model using the prepared corpus, time sequence, and DTM executable path.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/dtm_example.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmodel = DtmModel(dtm_path, corpus, time_seq, num_topics=2,\n                 id2word=corpus.dictionary, initialize_lda=True)\n```\n\n----------------------------------------\n\nTITLE: RST Table of Contents\nDESCRIPTION: Hidden table of contents directive listing all tutorial pages.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/index.rst#2025-04-21_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :hidden:\n\n   /auto_examples/tutorials/run_word2vec\n   /auto_examples/tutorials/run_doc2vec_lee\n   /auto_examples/tutorials/run_fasttext\n   /auto_examples/tutorials/run_ensemblelda\n   /auto_examples/tutorials/run_annoy\n   /auto_examples/tutorials/run_lda\n   /auto_examples/tutorials/run_wmd\n   /auto_examples/tutorials/run_scm\n```\n\n----------------------------------------\n\nTITLE: Computing Word Mover's Distance between Sentences in Python\nDESCRIPTION: This snippet calculates the Word Mover's Distance between two preprocessed sentences using the loaded word embeddings.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_wmd.rst#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndistance = model.wmdistance(sentence_obama, sentence_president)\nprint('distance = %.4f' % distance)\n```\n\n----------------------------------------\n\nTITLE: Numerical Vector Data\nDESCRIPTION: A space-separated sequence of floating point numbers representing vector data, likely used for machine learning or numerical analysis purposes. Each number is in scientific notation format with high precision.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/gensim/test/test_data/IT.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt#2025-04-21_snippet_7\n\nLANGUAGE: txt\nCODE:\n```\ndieci 9.988400340080261230e-02 2.124339938163757324e-01 -1.951700076460838318e-02 -7.538100332021713257e-02 7.276199758052825928e-02 4.682099819183349609e-02 5.015600100159645081e-02 3.456699848175048828e-02 2.432700060307979584e-02 8.391999639570713043e-03 -1.664800010621547699e-02 6.816300004720687866e-02 -1.184040009975433350e-01 1.121269986033439636e-01 -1.136149987578392029e-01 8.978699892759323120e-02 -2.279389947652816772e-01 -1.091480031609535217e-01 -1.988600045442581177e-01...\n```\n\n----------------------------------------\n\nTITLE: Loading Corpus Data\nDESCRIPTION: Loads the 20-newsgroups dataset using Gensim's downloader\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_methods.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport gensim.downloader\ncorpus = gensim.downloader.load(\"20-newsgroups\")\n```\n\n----------------------------------------\n\nTITLE: Setting up Logging Configuration\nDESCRIPTION: Configures basic logging with timestamp, level and message format\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/tutorials/run_doc2vec_lee.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n```\n\n----------------------------------------\n\nTITLE: Defining a Document in Python\nDESCRIPTION: Creates a single document as a string, representing the basic unit of text in Gensim.\nSOURCE: https://github.com/piskvorky/gensim/blob/develop/docs/src/auto_examples/core/run_core_concepts.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndocument = \"Human machine interface for lab abc computer applications\"\n```"
  }
]
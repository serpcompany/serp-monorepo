[
  {
    "owner": "kedro-org",
    "repo": "kedro",
    "content": "TITLE: Complete Kedro Pipeline Example\nDESCRIPTION: Final implementation showing the complete data pipeline using Kedro for configuration and data management\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook.ipynb#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Kedro setup for data management and configuration\n\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\n\nfrom kedro.config import OmegaConfigLoader\nfrom kedro.io import DataCatalog\n\nconf_loader = OmegaConfigLoader(conf_source=\".\")\nconf_catalog = conf_loader[\"catalog\"]\nconf_params = conf_loader[\"parameters\"]\n\n# Create the DataCatalog instance from the configuration\ncatalog = DataCatalog.from_config(conf_catalog)\n\n# Load the datasets\ncompanies = catalog.load(\"companies\")\nreviews = catalog.load(\"reviews\")\nshuttles = catalog.load(\"shuttles\")\n\n# Load the configuration data\ntest_size = conf_params[\"model_options\"][\"test_size\"]\nrandom_state = conf_params[\"model_options\"][\"random_state\"]\n\n\n####################\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Companies and Shuttles Data in Python\nDESCRIPTION: This code snippet defines functions to preprocess companies and shuttles data. It includes helper functions to parse boolean, percentage, and money values, and main functions to preprocess the entire datasets.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/create_a_pipeline.md#2025-04-19_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\n\n\ndef _is_true(x: pd.Series) -> pd.Series:\n    return x == \"t\"\n\n\ndef _parse_percentage(x: pd.Series) -> pd.Series:\n    x = x.str.replace(\"%\", \"\")\n    x = x.astype(float) / 100\n    return x\n\n\ndef _parse_money(x: pd.Series) -> pd.Series:\n    x = x.str.replace(\"$\", \"\").str.replace(\",\", \"\")\n    x = x.astype(float)\n    return x\n\n\ndef preprocess_companies(companies: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Preprocesses the data for companies.\n\n    Args:\n        companies: Raw data.\n    Returns:\n        Preprocessed data, with `company_rating` converted to a float and\n        `iata_approved` converted to boolean.\n    \"\"\"\n    companies[\"iata_approved\"] = _is_true(companies[\"iata_approved\"])\n    companies[\"company_rating\"] = _parse_percentage(companies[\"company_rating\"])\n    return companies\n\n\ndef preprocess_shuttles(shuttles: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Preprocesses the data for shuttles.\n\n    Args:\n        shuttles: Raw data.\n    Returns:\n        Preprocessed data, with `price` converted to a float and `d_check_complete`,\n        `moon_clearance_complete` converted to boolean.\n    \"\"\"\n    shuttles[\"d_check_complete\"] = _is_true(shuttles[\"d_check_complete\"])\n    shuttles[\"moon_clearance_complete\"] = _is_true(shuttles[\"moon_clearance_complete\"])\n    shuttles[\"price\"] = _parse_money(shuttles[\"price\"])\n    return shuttles\n```\n\n----------------------------------------\n\nTITLE: Implementing Namespaced Pipeline Architecture in Python\nDESCRIPTION: Redefines the data science pipeline to create two instances of the same pipeline with different namespaces, allowing parallel execution with different parameters and comparing model performance.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/add_another_pipeline.md#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.pipeline import Pipeline, node, pipeline\n\nfrom .nodes import evaluate_model, split_data, train_model\n\n\ndef create_pipeline(**kwargs) -> Pipeline:\n    pipeline_instance = pipeline(\n        [\n            node(\n                func=split_data,\n                inputs=[\"model_input_table\", \"params:model_options\"],\n                outputs=[\"X_train\", \"X_test\", \"y_train\", \"y_test\"],\n                name=\"split_data_node\",\n            ),\n            node(\n                func=train_model,\n                inputs=[\"X_train\", \"y_train\"],\n                outputs=\"regressor\",\n                name=\"train_model_node\",\n            ),\n            node(\n                func=evaluate_model,\n                inputs=[\"regressor\", \"X_test\", \"y_test\"],\n                outputs=None,\n                name=\"evaluate_model_node\",\n            ),\n        ]\n    )\n    ds_pipeline_1 = pipeline(\n        pipe=pipeline_instance,\n        inputs=\"model_input_table\",\n        namespace=\"active_modelling_pipeline\",\n    )\n    ds_pipeline_2 = pipeline(\n        pipe=pipeline_instance,\n        inputs=\"model_input_table\",\n        namespace=\"candidate_modelling_pipeline\",\n    )\n\n    return ds_pipeline_1 + ds_pipeline_2\n```\n\n----------------------------------------\n\nTITLE: Registering Preprocessed Data in Kedro Data Catalog\nDESCRIPTION: This YAML snippet shows how to register preprocessed datasets in the Kedro Data Catalog. It defines the storage type and filepath for preprocessed companies and shuttles data.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/create_a_pipeline.md#2025-04-19_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\npreprocessed_companies:\n  type: pandas.ParquetDataset\n  filepath: data/02_intermediate/preprocessed_companies.parquet\n\npreprocessed_shuttles:\n  type: pandas.ParquetDataset\n  filepath: data/02_intermediate/preprocessed_shuttles.parquet\n```\n\n----------------------------------------\n\nTITLE: Configuring S3-stored CSV Dataset with Credentials in Kedro\nDESCRIPTION: Example showing how to load a CSV file from a specific S3 bucket with credentials and custom load arguments like row skipping and NA value handling in Kedro's catalog.yml.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog_yaml_examples.md#2025-04-19_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nmotorbikes:\n  type: pandas.CSVDataset\n  filepath: s3://your_bucket/data/02_intermediate/company/motorbikes.csv\n  credentials: dev_s3\n  load_args:\n    sep: ','\n    skiprows: 5\n    skipfooter: 1\n    na_values: ['#NA', NA]\n```\n\n----------------------------------------\n\nTITLE: Building a Variance Calculation Pipeline in Python with Kedro\nDESCRIPTION: This snippet demonstrates how to construct a simple pipeline that computes the variance of a set of numbers using Kedro's pipeline and node classes. It defines functions for mean, mean sum of squares, and variance calculations, then combines them into a pipeline.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/pipeline_introduction.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.pipeline import pipeline, node\n\ndef mean(xs, n):\n    return sum(xs) / n\n\ndef mean_sos(xs, n):\n    return sum(x**2 for x in xs) / n\n\ndef variance(m, m2):\n    return m2 - m * m\n\n\nvariance_pipeline = pipeline(\n    [\n        node(len, \"xs\", \"n\"),\n        node(mean, [\"xs\", \"n\"], \"m\", name=\"mean_node\"),\n        node(mean_sos, [\"xs\", \"n\"], \"m2\", name=\"mean_sos\"),\n        node(variance, [\"m\", \"m2\"], \"v\", name=\"variance_node\"),\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Creating ML Pipeline Nodes with Generator-based Predictions in Python\nDESCRIPTION: Implementation of machine learning pipeline nodes including data splitting, prediction generation using DecisionTreeClassifier with generator pattern, and accuracy reporting. Uses scikit-learn for model training and prediction.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/nodes.md#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nfrom typing import Any, Dict, Tuple, Iterator, Generator\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nimport pandas as pd\n\n\ndef split_data(\n    data: pd.DataFrame, parameters: Dict[str, Any]\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n    \"\"\"Splits data into features and target training and test sets.\n\n    Args:\n        data: Data containing features and target.\n        parameters: Parameters defined in parameters.yml.\n    Returns:\n        Split data.\n    \"\"\"\n\n    data_train = data.sample(\n        frac=parameters[\"train_fraction\"], random_state=parameters[\"random_state\"]\n    )\n    data_test = data.drop(data_train.index)\n\n    X_train = data_train.drop(columns=parameters[\"target_column\"])\n    X_test = data_test.drop(columns=parameters[\"target_column\"])\n    y_train = data_train[parameters[\"target_column\"]]\n    y_test = data_test[parameters[\"target_column\"]]\n\n    label_encoder = LabelEncoder()\n    label_encoder.fit(pd.concat([y_train, y_test]))\n    y_train = label_encoder.transform(y_train)\n\n    return X_train, X_test, y_train, y_test\n\n\ndef make_predictions(\n    X_train: pd.DataFrame, X_test: pd.DataFrame, y_train: pd.Series\n) -> Generator[pd.Series, None, None]:\n    \"\"\"Use a DecisionTreeClassifier model to make prediction.\"\"\"\n    model = DecisionTreeClassifier()\n    model.fit(X_train, y_train)\n\n    for chunk in X_test:\n        y_pred = model.predict(chunk)\n        y_pred = pd.DataFrame(y_pred)\n        yield y_pred\n\n\ndef report_accuracy(y_pred: pd.Series, y_test: pd.Series):\n    \"\"\"Calculates and logs the accuracy.\n\n    Args:\n        y_pred: Predicted target.\n        y_test: True target.\n    \"\"\"\n    accuracy = accuracy_score(y_test, y_pred)\n    logger = logging.getLogger(__name__)\n    logger.info(\"Model has accuracy of %.3f on test data.\", accuracy)\n```\n\n----------------------------------------\n\nTITLE: Using the 'describe' Method to Examine Pipeline Structure in Kedro\nDESCRIPTION: This code snippet shows how to use the 'describe' method to get an overview of the nodes in a Kedro pipeline and their execution order. It prints a description of the previously defined variance_pipeline.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/pipeline_introduction.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprint(variance_pipeline.describe())\n```\n\n----------------------------------------\n\nTITLE: Tagging a Kedro Node (Python)\nDESCRIPTION: Demonstrates how to tag a Kedro node, which can be useful for running specific parts of a pipeline.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/nodes.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nnode(func=add, inputs=[\"a\", \"b\"], outputs=\"sum\", name=\"adding_a_and_b\", tags=\"node_tag\")\n```\n\n----------------------------------------\n\nTITLE: Defining Hook Implementation for after_catalog_created in Python\nDESCRIPTION: Example of implementing the after_catalog_created hook to log the contents of a data catalog after it is created. This snippet demonstrates how to create a custom hook class with a logger and implement the hook using the @hook_impl decorator.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/introduction.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# src/<package_name>/hooks.py\nimport logging\n\nfrom kedro.framework.hooks import hook_impl\nfrom kedro.io import DataCatalog\n\n\nclass DataCatalogHooks:\n    @property\n    def _logger(self):\n        return logging.getLogger(__name__)\n\n    @hook_impl\n    def after_catalog_created(self, catalog: DataCatalog) -> None:\n        self._logger.info(catalog.list())\n```\n\n----------------------------------------\n\nTITLE: Merging Multiple Pipelines in Kedro\nDESCRIPTION: This example demonstrates how to merge multiple pipelines in Kedro. It creates two separate pipelines (pipeline_de and pipeline_ds), adds a final node, and then combines them all into a single pipeline (pipeline_all).\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/pipeline_introduction.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npipeline_de = pipeline([node(len, \"xs\", \"n\"), node(mean, [\"xs\", \"n\"], \"m\")])\n\npipeline_ds = pipeline(\n    [node(mean_sos, [\"xs\", \"n\"], \"m2\"), node(variance, [\"m\", \"m2\"], \"v\")]\n)\n\nlast_node = node(print, \"v\", None)\n\npipeline_all = pipeline([pipeline_de, pipeline_ds, last_node])\nprint(pipeline_all.describe())\n```\n\n----------------------------------------\n\nTITLE: Creating Data Processing Pipeline in Python for Kedro Project\nDESCRIPTION: This function creates a Kedro pipeline with three nodes: preprocess_companies, preprocess_shuttles, and create_model_input_table. It defines the inputs and outputs for each node in the data processing pipeline.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/create_a_pipeline.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.pipeline import Pipeline, node, pipeline\n\nfrom .nodes import create_model_input_table, preprocess_companies, preprocess_shuttles\n\n\ndef create_pipeline(**kwargs) -> Pipeline:\n    return pipeline(\n        [\n            node(\n                func=preprocess_companies,\n                inputs=\"companies\",\n                outputs=\"preprocessed_companies\",\n                name=\"preprocess_companies_node\",\n            ),\n            node(\n                func=preprocess_shuttles,\n                inputs=\"shuttles\",\n                outputs=\"preprocessed_shuttles\",\n                name=\"preprocess_shuttles_node\",\n            ),\n            node(\n                func=create_model_input_table,\n                inputs=[\"preprocessed_shuttles\", \"preprocessed_companies\", \"reviews\"],\n                outputs=\"model_input_table\",\n                name=\"create_model_input_table_node\",\n            ),\n        ]\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Dask-based Runner for Kedro Pipelines\nDESCRIPTION: Complete implementation of DaskRunner and DaskDataset classes that enable distributed execution of Kedro pipeline nodes across a Dask cluster. Includes methods for handling dataset persistence, node execution, and dependency management.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/dask.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"``DaskRunner`` is an ``AbstractRunner`` implementation. It can be\nused to distribute execution of ``Node``s in the ``Pipeline`` across\na Dask cluster, taking into account the inter-``Node`` dependencies.\n\"\"\"\nfrom collections import Counter\nfrom itertools import chain\nfrom typing import Any, Dict\n\nfrom distributed import Client, as_completed, worker_client\nfrom kedro.framework.hooks.manager import (\n    _create_hook_manager,\n    _register_hooks,\n    _register_hooks_entry_points,\n)\nfrom kedro.framework.project import settings\nfrom kedro.io import AbstractDataset, DataCatalog\nfrom kedro.pipeline import Pipeline\nfrom kedro.pipeline.node import Node\nfrom kedro.runner import AbstractRunner, run_node\nfrom pluggy import PluginManager\n\n\nclass _DaskDataset(AbstractDataset):\n    \"\"\"``_DaskDataset`` publishes/gets named datasets to/from the Dask\n    scheduler.\"\"\"\n\n    def __init__(self, name: str):\n        self._name = name\n\n    def _load(self) -> Any:\n        try:\n            with worker_client() as client:\n                return client.get_dataset(self._name)\n        except ValueError:\n            # Upon successfully executing the pipeline, the runner loads\n            # free outputs on the scheduler (as opposed to on a worker).\n            Client.current().get_dataset(self._name)\n\n    def _save(self, data: Any) -> None:\n        with worker_client() as client:\n            client.publish_dataset(data, name=self._name, override=True)\n\n    def _exists(self) -> bool:\n        return self._name in Client.current().list_datasets()\n\n    def _release(self) -> None:\n        Client.current().unpublish_dataset(self._name)\n\n    def _describe(self) -> Dict[str, Any]:\n        return dict(name=self._name)\n\n\nclass DaskRunner(AbstractRunner):\n    \"\"\"``DaskRunner`` is an ``AbstractRunner`` implementation. It can be\n    used to distribute execution of ``Node``s in the ``Pipeline`` across\n    a Dask cluster, taking into account the inter-``Node`` dependencies.\n    \"\"\"\n\n    def __init__(self, client_args: Dict[str, Any] = {}, is_async: bool = False):\n        \"\"\"Instantiates the runner by creating a ``distributed.Client``.\n\n        Args:\n            client_args: Arguments to pass to the ``distributed.Client``\n                constructor.\n            is_async: If True, the node inputs and outputs are loaded and saved\n                asynchronously with threads. Defaults to False.\n        \"\"\"\n        super().__init__(is_async=is_async)\n        Client(**client_args)\n\n    def __del__(self):\n        Client.current().close()\n\n    def create_default_dataset(self, ds_name: str) -> _DaskDataset:\n        \"\"\"Factory method for creating the default dataset for the runner.\n\n        Args:\n            ds_name: Name of the missing dataset.\n\n        Returns:\n            An instance of ``_DaskDataset`` to be used for all\n            unregistered datasets.\n        \"\"\"\n        return _DaskDataset(ds_name)\n\n    @staticmethod\n    def _run_node(\n        node: Node,\n        catalog: DataCatalog,\n        is_async: bool = False,\n        session_id: str = None,\n        *dependencies: Node,\n    ) -> Node:\n        \"\"\"Run a single `Node` with inputs from and outputs to the `catalog`.\n\n        Wraps ``run_node`` to accept the set of ``Node``s that this node\n        depends on. When ``dependencies`` are futures, Dask ensures that\n        the upstream node futures are completed before running ``node``.\n\n        A ``PluginManager`` instance is created on each worker because the\n        ``PluginManager`` can't be serialised.\n\n        Args:\n            node: The ``Node`` to run.\n            catalog: A ``DataCatalog`` containing the node's inputs and outputs.\n            is_async: If True, the node inputs and outputs are loaded and saved\n                asynchronously with threads. Defaults to False.\n            session_id: The session id of the pipeline run.\n            dependencies: The upstream ``Node``s to allow Dask to handle\n                dependency tracking. Their values are not actually used.\n\n        Returns:\n            The node argument.\n        \"\"\"\n        hook_manager = _create_hook_manager()\n        _register_hooks(hook_manager, settings.HOOKS)\n        _register_hooks_entry_points(hook_manager, settings.DISABLE_HOOKS_FOR_PLUGINS)\n\n        return run_node(node, catalog, hook_manager, is_async, session_id)\n\n    def _run(\n        self,\n        pipeline: Pipeline,\n        catalog: DataCatalog,\n        hook_manager: PluginManager,\n        session_id: str = None,\n    ) -> None:\n        nodes = pipeline.nodes\n        load_counts = Counter(chain.from_iterable(n.inputs for n in nodes))\n        node_dependencies = pipeline.node_dependencies\n        node_futures = {}\n\n        client = Client.current()\n        for node in nodes:\n            dependencies = (\n                node_futures[dependency] for dependency in node_dependencies[node]\n            )\n            node_futures[node] = client.submit(\n                DaskRunner._run_node,\n                node,\n                catalog,\n                self._is_async,\n                session_id,\n                *dependencies,\n            )\n\n        for i, (_, node) in enumerate(\n            as_completed(node_futures.values(), with_results=True)\n        ):\n            self._logger.info(\"Completed node: %s\", node.name)\n            self._logger.info(\"Completed %d out of %d tasks\", i + 1, len(nodes))\n\n            # Decrement load counts, and release any datasets we\n            # have finished with. This is particularly important\n            # for the shared, default datasets we created above.\n            for dataset in node.inputs:\n                load_counts[dataset] -= 1\n                if load_counts[dataset] < 1 and dataset not in pipeline.inputs():\n                    catalog.release(dataset)\n            for dataset in node.outputs:\n                if load_counts[dataset] < 1 and dataset not in pipeline.outputs():\n                    catalog.release(dataset)\n\n    def run_only_missing(\n        self, pipeline: Pipeline, catalog: DataCatalog\n    ) -> Dict[str, Any]:\n        \"\"\"Run only the missing outputs from the ``Pipeline`` using the\n        datasets provided by ``catalog``, and save results back to the\n        same objects.\n\n        Args:\n            pipeline: The ``Pipeline`` to run.\n            catalog: The ``DataCatalog`` from which to fetch data.\n        Raises:\n            ValueError: Raised when ``Pipeline`` inputs cannot be\n                satisfied.\n\n        Returns:\n            Any node outputs that cannot be processed by the\n            ``DataCatalog``. These are returned in a dictionary, where\n            the keys are defined by the node outputs.\n        \"\"\"\n        free_outputs = pipeline.outputs() - set(catalog.list())\n        missing = {ds for ds in catalog.list() if not catalog.exists(ds)}\n        to_build = free_outputs | missing\n        to_rerun = pipeline.only_nodes_with_outputs(*to_build) + pipeline.from_inputs(\n            *to_build\n        )\n\n        # We also need any missing datasets that are required to run the\n        # `to_rerun` pipeline, including any chains of missing datasets.\n        unregistered_ds = pipeline.datasets() - set(catalog.list())\n        # Some of the unregistered datasets could have been published to\n        # the scheduler in a previous run, so we need not recreate them.\n        missing_unregistered_ds = {\n            ds_name\n            for ds_name in unregistered_ds\n            if not self.create_default_dataset(ds_name).exists()\n        }\n        output_to_unregistered = pipeline.only_nodes_with_outputs(\n            *missing_unregistered_ds\n        )\n        input_from_unregistered = to_rerun.inputs() & missing_unregistered_ds\n        to_rerun += output_to_unregistered.to_outputs(*input_from_unregistered)\n\n        # We need to add any previously-published, unregistered datasets\n        # to the catalog passed to the `run` method, so that it does not\n        # think that the `to_rerun` pipeline's inputs are not satisfied.\n        catalog = catalog.shallow_copy()\n        for ds_name in unregistered_ds - missing_unregistered_ds:\n            catalog.add(ds_name, self.create_default_dataset(ds_name))\n\n        return self.run(to_rerun, catalog)\n```\n\n----------------------------------------\n\nTITLE: Implementing Pipeline Monitoring Hooks with statsd\nDESCRIPTION: Implementation of before_node_run, after_node_run, and after_pipeline_run hooks to collect metrics (dataset size and node execution time) using statsd for visualization in Grafana.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/examples.md#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# src/<package_name>/hooks.py\nimport sys\nfrom typing import Any, Dict\n\nimport statsd\nfrom kedro.framework.hooks import hook_impl\nfrom kedro.pipeline.node import Node\n\n\nclass PipelineMonitoringHooks:\n    def __init__(self):\n        self._timers = {}\n        self._client = statsd.StatsClient(prefix=\"kedro\")\n\n    @hook_impl\n    def before_node_run(self, node: Node) -> None:\n        node_timer = self._client.timer(node.name)\n        node_timer.start()\n        self._timers[node.short_name] = node_timer\n\n    @hook_impl\n    def after_node_run(self, node: Node, inputs: Dict[str, Any]) -> None:\n        self._timers[node.short_name].stop()\n        for dataset_name, dataset_value in inputs.items():\n            self._client.gauge(dataset_name + \"_size\", sys.getsizeof(dataset_value))\n\n    @hook_impl\n    def after_pipeline_run(self):\n        self._client.incr(\"run\")\n```\n\n----------------------------------------\n\nTITLE: Debugging Pipelines with PDB using Kedro Hooks\nDESCRIPTION: Implementation of a hook that activates the Python debugger (PDB) when an error occurs in a pipeline. The hook uses the 'on_pipeline_error' hook point to start a post-mortem debugging session with the traceback information.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/common_use_cases.md#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport pdb\nimport sys\nimport traceback\n\nfrom kedro.framework.hooks import hook_impl\n\n\nclass PDBPipelineDebugHook:\n    \"\"\"A hook class for creating a post mortem debugging with the PDB debugger\n    whenever an error is triggered within a pipeline. The local scope from when the\n    exception occured is available within this debugging session.\n    \"\"\"\n\n    @hook_impl\n    def on_pipeline_error(self):\n        # We don't need the actual exception since it is within this stack frame\n        _, _, traceback_object = sys.exc_info()\n\n        #  Print the traceback information for debugging ease\n        traceback.print_tb(traceback_object)\n\n        # Drop you into a post mortem debugging session\n        pdb.post_mortem(traceback_object)\n```\n\n----------------------------------------\n\nTITLE: Basic Data Catalog Configuration in YAML\nDESCRIPTION: Example of a basic `catalog.yml` file that registers two CSV datasets and one Excel dataset with minimal configuration. The file shows the required fields for each dataset: name, type, and filepath.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog.md#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ncompanies:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/companies.csv\n\nreviews:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/reviews.csv\n\nshuttles:\n  type: pandas.ExcelDataset\n  filepath: data/01_raw/shuttles.xlsx\n  load_args:\n    engine: openpyxl # Use modern Excel engine (the default since Kedro 0.18.0)\n```\n\n----------------------------------------\n\nTITLE: Accessing Configuration in Kedro Code\nDESCRIPTION: Demonstrates how to access Kedro configuration directly in code for debugging purposes using OmegaConfigLoader.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/configuration_basics.md#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.config import OmegaConfigLoader\nfrom kedro.framework.project import settings\n\nconf_path = str(project_path / settings.CONF_SOURCE)\nconf_loader = OmegaConfigLoader(conf_source=conf_path)\n\nconf_catalog = conf_loader[\"catalog\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Load and Save Arguments for CSV Datasets\nDESCRIPTION: Example showing how to configure load_args and save_args for a CSV dataset. This demonstrates setting the separator, controlling index inclusion, and formatting date and decimal values.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog.md#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ncars:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/company/cars.csv\n  load_args:\n    sep: ','\n  save_args:\n    index: False\n    date_format: '%Y-%m-%d %H:%M'\n    decimal: .\n```\n\n----------------------------------------\n\nTITLE: Initializing SparkSession with Kedro Hook\nDESCRIPTION: Implementation of a Kedro hook to initialize SparkSession using configuration from spark.yml. This ensures SparkSession is created before the pipeline runs.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/pyspark_integration.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.framework.hooks import hook_impl\nfrom pyspark import SparkConf\nfrom pyspark.sql import SparkSession\n\n\nclass SparkHooks:\n    @hook_impl\n    def after_context_created(self, context) -> None:\n        \"\"\"Initialises a SparkSession using the config\n        defined in project's conf folder.\n        \"\"\"\n\n        # Load the spark configuration in spark.yaml using the config loader\n        parameters = context.config_loader[\"spark\"]\n        spark_conf = SparkConf().setAll(parameters.items())\n\n        # Initialise the spark session\n        spark_session_conf = (\n            SparkSession.builder.appName(context.project_path.name)\n            .enableHiveSupport()\n            .config(conf=spark_conf)\n        )\n        _spark_session = spark_session_conf.getOrCreate()\n        _spark_session.sparkContext.setLogLevel(\"WARN\")\n```\n\n----------------------------------------\n\nTITLE: Accessing Node Information in a Kedro Pipeline\nDESCRIPTION: This snippet shows how to access information about the nodes in a Kedro pipeline, including their inputs and outputs. It demonstrates accessing the nodes list and inspecting individual node properties.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/pipeline_introduction.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nnodes = variance_pipeline.nodes\nnodes\n\nnodes[0].inputs\n```\n\n----------------------------------------\n\nTITLE: Defining Data Science Pipeline in Python\nDESCRIPTION: This snippet defines the data science pipeline using Kedro's Pipeline and node classes, connecting the previously defined node functions.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/add_another_pipeline.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.pipeline import Pipeline, node, pipeline\n\nfrom .nodes import evaluate_model, split_data, train_model\n\n\ndef create_pipeline(**kwargs) -> Pipeline:\n    return pipeline(\n        [\n            node(\n                func=split_data,\n                inputs=[\"model_input_table\", \"params:model_options\"],\n                outputs=[\"X_train\", \"X_test\", \"y_train\", \"y_test\"],\n                name=\"split_data_node\",\n            ),\n            node(\n                func=train_model,\n                inputs=[\"X_train\", \"y_train\"],\n                outputs=\"regressor\",\n                name=\"train_model_node\",\n            ),\n            node(\n                func=evaluate_model,\n                inputs=[\"regressor\", \"X_test\", \"y_test\"],\n                outputs=None,\n                name=\"evaluate_model_node\",\n            ),\n        ]\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Data Science Node Functions in Python\nDESCRIPTION: This snippet defines three functions for a data science pipeline: split_data for splitting the dataset, train_model for training a linear regression model, and evaluate_model for calculating the R^2 score.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/add_another_pipeline.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nfrom typing import dict, Tuple\n\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\n\n\ndef split_data(data: pd.DataFrame, parameters: dict[str, Any]) -> Tuple:\n    \"\"\"Splits data into features and targets training and test sets.\n\n    Args:\n        data: Data containing features and target.\n        parameters: Parameters defined in parameters_data_science.yml.\n    Returns:\n        Split data.\n    \"\"\"\n    X = data[parameters[\"features\"]]\n    y = data[\"price\"]\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=parameters[\"test_size\"], random_state=parameters[\"random_state\"]\n    )\n    return X_train, X_test, y_train, y_test\n\n\ndef train_model(X_train: pd.DataFrame, y_train: pd.Series) -> LinearRegression:\n    \"\"\"Trains the linear regression model.\n\n    Args:\n        X_train: Training data of independent features.\n        y_train: Training data for price.\n\n    Returns:\n        Trained model.\n    \"\"\"\n    regressor = LinearRegression()\n    regressor.fit(X_train, y_train)\n    return regressor\n\n\ndef evaluate_model(\n    regressor: LinearRegression, X_test: pd.DataFrame, y_test: pd.Series\n):\n    \"\"\"Calculates and logs the coefficient of determination.\n\n    Args:\n        regressor: Trained model.\n        X_test: Testing data of independent features.\n        y_test: Testing data for price.\n    \"\"\"\n    y_pred = regressor.predict(X_test)\n    score = r2_score(y_test, y_pred)\n    logger = logging.getLogger(__name__)\n    logger.info(\"Model has a coefficient R^2 of %.3f on test data.\", score)\n```\n\n----------------------------------------\n\nTITLE: Linear Regression Model Training and Evaluation in Python\nDESCRIPTION: Functions to train a linear regression model on the prepared data and evaluate its performance using R-squared score. These functions handle the model fitting process and output the evaluation metric to the console.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook.ipynb#2025-04-19_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef train_model(X_train: pd.DataFrame, y_train: pd.Series) -> LinearRegression:\n    regressor = LinearRegression()\n    regressor.fit(X_train, y_train)\n    return regressor\n\n\ndef evaluate_model(\n    regressor: LinearRegression, X_test: pd.DataFrame, y_test: pd.Series\n):\n    y_pred = regressor.predict(X_test)\n    print(r2_score(y_test, y_pred))\n```\n\n----------------------------------------\n\nTITLE: Creating Model Input Table by Merging Datasets in Python\nDESCRIPTION: A function that combines shuttle, company, and review data into a single table for model training. It merges the datasets on their respective IDs and removes any rows with missing values to ensure data quality.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook.ipynb#2025-04-19_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef create_model_input_table(\n    shuttles: pd.DataFrame, companies: pd.DataFrame, reviews: pd.DataFrame\n) -> pd.DataFrame:\n    rated_shuttles = shuttles.merge(reviews, left_on=\"id\", right_on=\"shuttle_id\")\n    model_input_table = rated_shuttles.merge(\n        companies, left_on=\"company_id\", right_on=\"id\"\n    )\n    model_input_table = model_input_table.dropna()\n    return model_input_table\n```\n\n----------------------------------------\n\nTITLE: Complete Data Science Pipeline Test Implementation\nDESCRIPTION: Full implementation of test file showing fixtures, pipeline tests, and data splitting tests. Includes error handling and logging verification.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/test_a_project.md#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# tests/pipelines/test_data_science_pipeline.py\n\nimport logging\nimport pandas as pd\nimport pytest\n\nfrom kedro.io import DataCatalog\nfrom kedro.runner import SequentialRunner\nfrom spaceflights.pipelines.data_science import create_pipeline as create_ds_pipeline\nfrom spaceflights.pipelines.data_science.nodes import split_data\n\n@pytest.fixture\ndef dummy_data():\n    return pd.DataFrame(\n        {\n            \"engines\": [1, 2, 3],\n            \"crew\": [4, 5, 6],\n            \"passenger_capacity\": [5, 6, 7],\n            \"price\": [120, 290, 30],\n        }\n    )\n\n@pytest.fixture\ndef dummy_parameters():\n    parameters = {\n        \"model_options\": {\n            \"test_size\": 0.2,\n            \"random_state\": 3,\n            \"features\": [\"engines\", \"passenger_capacity\", \"crew\"],\n        }\n    }\n    return parameters\n\n\ndef test_split_data(dummy_data, dummy_parameters):\n    X_train, X_test, y_train, y_test = split_data(\n        dummy_data, dummy_parameters[\"model_options\"]\n    )\n    assert len(X_train) == 2\n    assert len(y_train) == 2\n    assert len(X_test) == 1\n    assert len(y_test) == 1\n\ndef test_split_data_missing_price(dummy_data, dummy_parameters):\n    dummy_data_missing_price = dummy_data.drop(columns=\"price\")\n    with pytest.raises(KeyError) as e_info:\n        X_train, X_test, y_train, y_test = split_data(dummy_data_missing_price, dummy_parameters[\"model_options\"])\n\n    assert \"price\" in str(e_info.value)\n\ndef test_data_science_pipeline(caplog, dummy_data, dummy_parameters):\n    pipeline = (\n        create_ds_pipeline()\n        .from_nodes(\"split_data_node\")\n        .to_nodes(\"evaluate_model_node\")\n    )\n    catalog = DataCatalog()\n    catalog.add_feed_dict(\n        {\n            \"model_input_table\" : dummy_data,\n            \"params:model_options\": dummy_parameters[\"model_options\"],\n        }\n    )\n\n    caplog.set_level(logging.DEBUG, logger=\"kedro\")\n    successful_run_msg = \"Pipeline execution completed successfully.\"\n\n    SequentialRunner().run(pipeline, catalog)\n\n    assert successful_run_msg in caplog.text\n```\n\n----------------------------------------\n\nTITLE: Creating and Running Kedro Session\nDESCRIPTION: Creates a KedroSession object as a context manager and runs a pipeline inside the context. The session automatically closes after exit.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/kedro_project_setup/session.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.framework.session import KedroSession\nfrom kedro.framework.startup import bootstrap_project\nfrom pathlib import Path\n\nbootstrap_project(Path.cwd())\nwith KedroSession.create() as session:\n    session.run()\n```\n\n----------------------------------------\n\nTITLE: Registering Pipelines Manually in Kedro (Python)\nDESCRIPTION: This snippet demonstrates how to manually register pipelines in a Kedro project using the `register_pipelines()` function. It imports pipeline modules, creates pipeline objects, and returns a dictionary mapping pipeline names to Pipeline objects.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/pipeline_registry.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport spaceflights.pipelines.data_processing as dp\nimport spaceflights.pipelines.data_science as ds\n\n\ndef register_pipelines() -> Dict[str, Pipeline]:\n    \"\"\"Register the project's pipelines.\n\n    Returns:\n        A mapping from pipeline names to ``Pipeline`` objects.\n    \"\"\"\n    data_processing_pipeline = dp.create_pipeline()\n    data_science_pipeline = ds.create_pipeline()\n\n    return {\n        \"__default__\": data_processing_pipeline + data_science_pipeline,\n        \"data_processing\": data_processing_pipeline,\n        \"data_science\": data_science_pipeline,\n    }\n```\n\n----------------------------------------\n\nTITLE: Constructing a Data Processing Pipeline in Kedro\nDESCRIPTION: This code snippet demonstrates how to construct a data processing pipeline in Kedro. It creates nodes for preprocessing companies and shuttles data, and combines them into a pipeline.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/create_a_pipeline.md#2025-04-19_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom kedro.pipeline import Pipeline, node, pipeline\n\nfrom .nodes import preprocess_companies, preprocess_shuttles\n\n...\n\n\ndef create_pipeline(**kwargs) -> Pipeline:\n    return pipeline(\n        [\n            node(\n                func=preprocess_companies,\n                inputs=\"companies\",\n                outputs=\"preprocessed_companies\",\n                name=\"preprocess_companies_node\",\n            ),\n            node(\n                func=preprocess_shuttles,\n                inputs=\"shuttles\",\n                outputs=\"preprocessed_shuttles\",\n                name=\"preprocess_shuttles_node\",\n            ),\n            ...,\n        ]\n    )\n```\n\n----------------------------------------\n\nTITLE: Overriding Configuration with Runtime Parameters in YAML\nDESCRIPTION: Demonstrates how to use the runtime_params resolver in a parameters.yml file to allow overriding values with CLI parameters.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/advanced_configuration.md#2025-04-19_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_options:\n  random_state: \"${runtime_params:random}\"\n```\n\n----------------------------------------\n\nTITLE: Loading Parameters in Python Code\nDESCRIPTION: Shows how to load parameters using the OmegaConfigLoader in Python code, including error handling for missing configurations.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/parameters.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.config import OmegaConfigLoader\nfrom kedro.framework.project import settings\n\nconf_path = str(project_path / settings.CONF_SOURCE)\nconf_loader = OmegaConfigLoader(conf_source=conf_path)\nparameters = conf_loader[\"parameters\"]\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.config import OmegaConfigLoader, MissingConfigException\nfrom kedro.framework.project import settings\n\nconf_path = str(project_path / settings.CONF_SOURCE)\nconf_loader = OmegaConfigLoader(conf_source=conf_path)\n\ntry:\n    parameters = conf_loader[\"parameters\"]\nexcept MissingConfigException:\n    parameters = {}\n```\n\n----------------------------------------\n\nTITLE: Using Dataset Factory with Pipeline Nodes in Python\nDESCRIPTION: Example of how pipeline nodes reference datasets that match the factory pattern. This code shows a node connecting two datasets that will be resolved by the dataset factory pattern.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/kedro_dataset_factories.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nnode(\n    func=process_factory,\n    inputs=\"factory_data\",\n    outputs=\"process_data\",\n),\n\n...\n```\n\n----------------------------------------\n\nTITLE: Tagging Pipelines and Nodes in Kedro\nDESCRIPTION: These examples show how to tag pipelines and nodes in Kedro. The first example tags all nodes in a pipeline, while the second combines pipeline tagging with individual node tagging.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/pipeline_introduction.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npipeline = pipeline(\n    [node(..., name=\"node1\"), node(..., name=\"node2\")], tags=\"pipeline_tag\"\n)\n\npipeline = pipeline(\n    [node(..., name=\"node1\"), node(..., name=\"node2\", tags=\"node_tag\")],\n    tags=\"pipeline_tag\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Kedro Nodes\nDESCRIPTION: Demonstrates how to create two simple Kedro nodes - one that returns a greeting and another that joins statements together. Shows node creation with and without inputs.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/get_started/kedro_concepts.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.pipeline import node\n\n\n# First node\ndef return_greeting():\n    return \"Hello\"\n\n\nreturn_greeting_node = node(func=return_greeting, inputs=None, outputs=\"my_salutation\")\n\n\n# Second node\ndef join_statements(greeting):\n    return f\"{greeting} Kedro!\"\n\n\njoin_statements_node = node(\n    join_statements, inputs=\"my_salutation\", outputs=\"my_message\"\n)\n```\n\n----------------------------------------\n\nTITLE: Using Runtime Parameters Resolver in Python Configuration\nDESCRIPTION: Example of runtime parameters syntax for overriding configuration keys using the runtime_params resolver with OmegaConfigLoader.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlayer: ${runtime_params:param_name}\n```\n\n----------------------------------------\n\nTITLE: Basic Sequential Pipeline Execution in Bash\nDESCRIPTION: Commands to run Kedro pipeline using the default SequentialRunner or explicitly specifying it.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/run_a_pipeline.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkedro run\n```\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --runner=SequentialRunner\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Kedro Pipeline\nDESCRIPTION: Python code to define a simple Kedro pipeline in pipeline_registry.py. It includes a dummy function and registers a pipeline with a single node.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/get_started/minimal_kedro_project.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.pipeline import pipeline, node\n\ndef foo():\n    return \"dummy\"\n\ndef register_pipelines():\n    return {\"__default__\": pipeline([node(foo, None, \"dummy_output\")])}\n```\n\n----------------------------------------\n\nTITLE: Loading Credentials from Azure KeyVault using Kedro Hooks\nDESCRIPTION: Implementation of a hook that loads credentials from Azure KeyVault and injects them into the Kedro context configuration. The hook uses the 'after_context_created' hook point to fetch secrets and add them to the config loader.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/common_use_cases.md#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# hooks.py\n\nfrom kedro.framework.hooks import hook_impl\nfrom azure.keyvault.secrets import SecretClient\nfrom azure.identity import DefaultAzureCredential\n\n\nclass AzureSecretsHook:\n    @hook_impl\n    def after_context_created(self, context) -> None:\n        keyVaultName = \"keyvault-0542abb\"  # or os.environ[\"KEY_VAULT_NAME\"] if you would like to provide it through environment variables\n        KVUri = f\"https://{keyVaultName}.vault.azure.net\"\n\n        my_credential = DefaultAzureCredential()\n        client = SecretClient(vault_url=KVUri, credential=my_credential)\n\n        secrets = {\n            \"abs_creds\": \"azure-blob-store\",\n            \"s3_creds\": \"s3-bucket-creds\",\n        }\n        azure_creds = {\n            cred_name: client.get_secret(secret_name).value\n            for cred_name, secret_name in secrets.items()\n        }\n\n        context.config_loader[\"credentials\"] = {\n            **context.config_loader[\"credentials\"],\n            **azure_creds,\n        }\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic Kedro Node (Python)\nDESCRIPTION: Demonstrates how to create a Kedro node using the previously defined addition function, specifying inputs and outputs.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/nodes.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nadder_node = node(func=add, inputs=[\"a\", \"b\"], outputs=\"sum\")\nadder_node\n```\n\n----------------------------------------\n\nTITLE: Implementing Pipeline Autodiscovery in Python\nDESCRIPTION: This code snippet demonstrates how to implement autodiscovery of project pipelines in Kedro. It uses the `find_pipelines()` function to automatically discover and register pipelines, including a default pipeline that combines all discovered pipelines.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef register_pipelines() -> Dict[str, Pipeline]:\n    \"\"\"Register the project's pipelines.\n\n    Returns:\n        A mapping from pipeline names to ``Pipeline`` objects.\n    \"\"\"\n    pipelines = find_pipelines()\n    pipelines[\"__default__\"] = sum(pipelines.values())\n    return pipelines\n```\n\n----------------------------------------\n\nTITLE: Configuring Filesystem Arguments for Google Cloud Storage\nDESCRIPTION: Example showing how to configure fs_args to provide project information to the underlying GCSFileSystem when working with Google Cloud Storage.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog.md#2025-04-19_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ntest_dataset:\n  type: ...\n  fs_args:\n    project: test_project\n```\n\n----------------------------------------\n\nTITLE: Using Global Variables with OmegaConfigLoader in YAML and Python\nDESCRIPTION: This example shows how to use global variables for variable interpolation across different configuration types. It includes a globals file, and examples of how to access these global variables in catalog and parameters files.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/advanced_configuration.md#2025-04-19_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nmy_global_value: 45\ndataset_type:\n  csv: pandas.CSVDataset\n```\n\nLANGUAGE: yaml\nCODE:\n```\nmy_param : \"${globals:my_global_value}\"\n```\n\nLANGUAGE: yaml\nCODE:\n```\ncompanies:\n  filepath: data/01_raw/companies.csv\n  type: \"${globals:dataset_type.csv}\"\n```\n\nLANGUAGE: yaml\nCODE:\n```\nmy_param: \"${globals: nonexistent_global, 23}\"\n```\n\n----------------------------------------\n\nTITLE: Running Kedro Pipeline and Installing Kedro-Viz\nDESCRIPTION: These Bash commands demonstrate how to run the Kedro pipeline, install Kedro-Viz for project visualization, and start the Kedro-Viz server.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/create_a_pipeline.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkedro run\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install kedro-viz\n```\n\nLANGUAGE: bash\nCODE:\n```\nkedro viz run\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced Dataset Parameters in YAML\nDESCRIPTION: Advanced example showing dataset configuration for Excel and Snowflake datasets. This demonstrates how to use dataset-specific parameters and configure underlying library options through load_args and save_args.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog.md#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nshuttles: # Dataset name\n  type: pandas.ExcelDataset # Dataset type\n  filepath: data/01_raw/shuttles.xlsx # pandas.ExcelDataset parameter\n  load_args: # pandas.ExcelDataset parameter\n    engine: openpyxl # Pandas option for loading CSV files\n\nweather: # Dataset name\n  type: snowflake.SnowparkTableDataset # Dataset type\n  table_name: \"weather_data\"\n  database: \"meteorology\"\n  schema: \"observations\"\n  credentials: snowflake_client\n  save_args: # snowflake.SnowparkTableDataset parameter\n    mode: overwrite # Snowpark saveAsTable input option\n    column_order: name\n    table_type: ''\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Pod Operator Configuration\nDESCRIPTION: Python code showing configuration of KubernetesPodOperator for running Kedro nodes in isolated Kubernetes pods.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/airflow.md#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator\n\nKubernetesPodOperator(\n    task_id=node_name,\n    name=node_name,\n    namespace=NAMESPACE,\n    image=DOCKER_IMAGE,\n    cmds=[\"kedro\"],\n    arguments=[\"run\", f\"--nodes={node_name}\"],\n    get_logs=True,\n    is_delete_operator_pod=True,  # Cleanup after execution\n    in_cluster=False, # Set to True if Airflow runs inside the Kubernetes cluster\n    do_xcom_push=False,\n    image_pull_policy=\"Always\",\n    # Uncomment the following lines if Airflow is running outside Kubernetes\n    # cluster_context=\"k3d-your-cluster\",  # Specify the Kubernetes context from your kubeconfig\n    # config_file=\"~/.kube/config\",  # Path to your kubeconfig file\n)\n```\n\n----------------------------------------\n\nTITLE: Integration Testing Kedro Pipeline\nDESCRIPTION: Integration test that validates the complete data science pipeline execution using dummy data and capturing logs to verify successful completion.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/test_a_project.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef test_data_science_pipeline(caplog):\n    # Arrange pipeline\n    pipeline = create_ds_pipeline()\n\n    # Arrange data catalog\n    catalog = DataCatalog()\n\n    dummy_data = pd.DataFrame(\n        {\n            \"engines\": [1, 2, 3],\n            \"crew\": [4, 5, 6],\n            \"passenger_capacity\": [5, 6, 7],\n            \"price\": [120, 290, 30],\n        }\n    )\n\n    duummy_parameters = {\n        \"model_options\": {\n            \"test_size\": 0.2,\n            \"random_state\": 3,\n            \"features\": [\"engines\", \"passenger_capacity\", \"crew\"],\n        }\n    }\n\n    catalog.add_feed_dict(\n        {\n            \"model_input_table\" : dummy_data,\n            \"params:model_options\": dummy_parameters[\"model_options\"],\n        }\n    )\n\n    # Arrange the log testing setup\n    caplog.set_level(logging.DEBUG, logger=\"kedro\")\n    successful_run_msg = \"Pipeline execution completed successfully.\"\n\n    # Act\n    SequentialRunner().run(pipeline, catalog)\n\n    # Assert\n    assert successful_run_msg in caplog.text\n```\n\n----------------------------------------\n\nTITLE: Creating Lambda Handler for Kedro Nodes\nDESCRIPTION: Python script to handle Lambda function execution for individual Kedro nodes, including project configuration and session creation.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/aws_step_functions.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom unittest.mock import patch\n\n\ndef handler(event, context):\n    from kedro.framework.project import configure_project\n\n    configure_project(\"spaceflights_step_functions\")\n    node_to_run = event[\"node_name\"]\n\n    # Since _multiprocessing.SemLock is not implemented on lambda yet,\n    # we mock it out so we could import the session. This has no impact on the correctness\n    # of the pipeline, as each Lambda function runs a single Kedro node, hence no need for Lock\n    # during import. For more information, please see this StackOverflow discussion:\n    # https://stackoverflow.com/questions/34005930/multiprocessing-semlock-is-not-implemented-when-running-on-aws-lambda\n    with patch(\"multiprocessing.Lock\"):\n        from kedro.framework.session import KedroSession\n\n        with KedroSession.create(env=\"aws\") as session:\n            session.run(node_names=[node_to_run])\n```\n\n----------------------------------------\n\nTITLE: Running a Kedro Project\nDESCRIPTION: This command executes the Kedro project, running all the pipelines and nodes defined in the project structure.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/features/steps/test_starter/{{ cookiecutter.repo_name }}/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkedro run\n```\n\n----------------------------------------\n\nTITLE: Implementing AWSBatchRunner Class in Python\nDESCRIPTION: This code defines the AWSBatchRunner class, which extends ThreadRunner to submit and monitor jobs asynchronously on AWS Batch. It includes methods for initialization, creating datasets, and running the pipeline.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/aws_batch.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom concurrent.futures import ThreadPoolExecutor\nfrom time import sleep\nfrom typing import Any, Dict, Set\n\nimport boto3\n\nfrom kedro.io import DataCatalog\nfrom kedro.pipeline.pipeline import Pipeline, Node\nfrom kedro.runner import ThreadRunner\n\n\nclass AWSBatchRunner(ThreadRunner):\n    def __init__(\n        self,\n        max_workers: int = None,\n        job_queue: str = None,\n        job_definition: str = None,\n        is_async: bool = False,\n    ):\n        super().__init__(max_workers, is_async=is_async)\n        self._job_queue = job_queue\n        self._job_definition = job_definition\n        self._client = boto3.client(\"batch\")\n\n    def create_default_dataset(self, ds_name: str):\n        raise NotImplementedError(\"All datasets must be defined in the catalog\")\n\n    def _get_required_workers_count(self, pipeline: Pipeline):\n        if self._max_workers is not None:\n            return self._max_workers\n\n        return super()._get_required_workers_count(pipeline)\n\n    def _run(\n        self,\n        pipeline: Pipeline,\n        catalog: DataCatalog,\n        hook_manager: PluginManager,\n        session_id: str = None,\n    ) -> None:\n        nodes = pipeline.nodes\n        node_dependencies = pipeline.node_dependencies\n        todo_nodes = set(node_dependencies.keys())\n        node_to_job = dict()\n        done_nodes = set()  # type: Set[Node]\n        futures = set()\n        max_workers = self._get_required_workers_count(pipeline)\n\n        self._logger.info(\"Max workers: %d\", max_workers)\n        with ThreadPoolExecutor(max_workers=max_workers) as pool:\n            while True:\n                # Process the nodes that have completed, i.e. jobs that reached\n                # FAILED or SUCCEEDED state\n                done = {fut for fut in futures if fut.done()}\n                futures -= done\n                for future in done:\n                    try:\n                        node = future.result()\n                    except Exception:\n                        self._suggest_resume_scenario(pipeline, done_nodes)\n                        raise\n                    done_nodes.add(node)\n                    self._logger.info(\n                        \"Completed %d out of %d jobs\", len(done_nodes), len(nodes)\n                    )\n\n                # A node is ready to be run if all its upstream dependencies have been\n                # submitted to Batch, i.e. all node dependencies were assigned a job ID\n                ready = {\n                    n for n in todo_nodes if node_dependencies[n] <= node_to_job.keys()\n                }\n                todo_nodes -= ready\n                # Asynchronously submit Batch jobs\n                for node in ready:\n                    future = pool.submit(\n                        self._submit_job,\n                        node,\n                        node_to_job,\n                        node_dependencies[node],\n                        session_id,\n                    )\n                    futures.add(future)\n\n                # If no more nodes left to run, ensure the entire pipeline was run\n                if not futures:\n                    assert not todo_nodes, (todo_nodes, done_nodes, ready, done)\n                    break\n```\n\n----------------------------------------\n\nTITLE: Configuring Hooks in settings.py\nDESCRIPTION: Example of how to specify custom Hook implementations in settings.py, which replaces the previous configuration in .kedro.yml. Shows how to register project hooks and disable hooks for specific plugins.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom <package_name>.hooks import MyCustomHooks, ProjectHooks\n\nHOOKS = (ProjectHooks(), MyCustomHooks())\n\nDISABLE_HOOKS_FOR_PLUGINS = (\"my_plugin1\",)\n```\n\n----------------------------------------\n\nTITLE: Configuring KedroDataCatalog as Default in Python\nDESCRIPTION: Shows how to set KedroDataCatalog as the default catalog for Kedro run and other CLI commands by updating the settings.py file.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/kedro_data_catalog.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.io import KedroDataCatalog\n\nDATA_CATALOG_CLASS = KedroDataCatalog\n```\n\n----------------------------------------\n\nTITLE: Overriding Dataset Configuration in Local Catalog YAML\nDESCRIPTION: Demonstrates how to override a dataset configuration for local development. This example changes the filepath to a local file instead of the S3 bucket used in production.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog.md#2025-04-19_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\ncars:\n  filepath: data/01_raw/cars.csv\n  type: pandas.CSVDataset\n```\n\n----------------------------------------\n\nTITLE: Tracking Kedro Pipeline with MLflow Python API\nDESCRIPTION: Example of setting up MLflow experiment tracking with a Kedro session. The code bootstraps a Kedro project, creates a new MLflow experiment, and logs the session run with the session ID as a tag.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/mlflow.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\n\nimport mlflow\nfrom kedro.framework.session import KedroSession\nfrom kedro.framework.startup import bootstrap_project\n\nbootstrap_project(Path.cwd())\n\nmlflow.set_experiment(\"Kedro Spaceflights test\")\n\nwith KedroSession.create() as session:\n    with mlflow.start_run():\n        mlflow.set_tag(\"session_id\", session.session_id)\n        session.run()\n```\n\n----------------------------------------\n\nTITLE: Implementing MLflow Tracking Hooks in Kedro (Python)\nDESCRIPTION: This code snippet defines a ModelTrackingHooks class with hook implementations for MLflow integration. It includes hooks for starting/ending MLflow runs and logging parameters, models, and metrics at different stages of the Kedro pipeline execution.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/examples.md#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# src/<package_name>/hooks.py\nfrom typing import Any, Dict\n\nimport mlflow\nimport mlflow.sklearn\nfrom kedro.framework.hooks import hook_impl\nfrom kedro.pipeline.node import Node\n\n\nclass ModelTrackingHooks:\n    \"\"\"Namespace for grouping all model-tracking hooks with MLflow together.\"\"\"\n\n    @hook_impl\n    def before_pipeline_run(self, run_params: Dict[str, Any]) -> None:\n        \"\"\"Hook implementation to start an MLflow run\n        with the session_id of the Kedro pipeline run.\n        \"\"\"\n        mlflow.start_run(run_name=run_params[\"session_id\"])\n        mlflow.log_params(run_params)\n\n    @hook_impl\n    def after_node_run(\n        self, node: Node, outputs: Dict[str, Any], inputs: Dict[str, Any]\n    ) -> None:\n        \"\"\"Hook implementation to add model tracking after some node runs.\n        In this example, we will:\n        * Log the parameters after the data splitting node runs.\n        * Log the model after the model training node runs.\n        * Log the model's metrics after the model evaluating node runs.\n        \"\"\"\n        if node._func_name == \"split_data\":\n            mlflow.log_params(\n                {\"split_data_ratio\": inputs[\"params:example_test_data_ratio\"]}\n            )\n\n        elif node._func_name == \"train_model\":\n            model = outputs[\"example_model\"]\n            mlflow.sklearn.log_model(model, \"model\")\n            mlflow.log_params(inputs[\"parameters\"])\n\n    @hook_impl\n    def after_pipeline_run(self) -> None:\n        \"\"\"Hook implementation to end the MLflow run\n        after the Kedro pipeline finishes.\n        \"\"\"\n        mlflow.end_run()\n```\n\n----------------------------------------\n\nTITLE: Creating Spark ML Pipeline with Kedro\nDESCRIPTION: Example of creating a Kedro pipeline with Spark ML components, including a RandomForestClassifier for training and prediction nodes.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/pyspark_integration.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict\n\nfrom kedro.pipeline import node, pipeline\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.sql import DataFrame\n\n\ndef train_model(training_data: DataFrame) -> RandomForestClassifier:\n    \"\"\"Node for training a random forest model to classify the data.\"\"\"\n    classifier = RandomForestClassifier(numTrees=10)\n    return classifier.fit(training_data)\n\n\ndef predict(model: RandomForestClassifier, testing_data: DataFrame) -> DataFrame:\n    \"\"\"Node for making predictions given a pre-trained model and a testing dataset.\"\"\"\n    predictions = model.transform(testing_data)\n    return predictions\n\n\ndef create_pipeline(**kwargs) -> Pipeline:\n    return pipeline(\n        [\n            node(train_model, inputs=[\"training_data\"], outputs=\"example_classifier\"),\n            node(\n                predict,\n                inputs=dict(model=\"example_classifier\", testing_data=\"testing_data\"),\n                outputs=\"example_predictions\",\n            ),\n        ]\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Table Dataset in Kedro\nDESCRIPTION: Example showing how to load and save a SQL table using credentials and custom load/save arguments including index column specification in Kedro's catalog.yml.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog_yaml_examples.md#2025-04-19_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\nscooters:\n  type: pandas.SQLTableDataset\n  credentials: scooters_credentials\n  table_name: scooters\n  load_args:\n    index_col: [name]\n    columns: [name, gear]\n  save_args:\n    if_exists: replace\n```\n\n----------------------------------------\n\nTITLE: Configuring Kedro Tasks in VS Code\nDESCRIPTION: Setting up VS Code tasks.json for common Kedro CLI commands including install, test, run, and package operations.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/set_up_vscode.md#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"version\": \"2.0.0\",\n    \"tasks\": [\n        {\n            \"label\": \"Install\",\n            \"type\": \"shell\",\n            \"command\": \"/path/to/kedro/script\",\n            \"args\": [\n                \"install\"\n            ]\n        },\n        {\n            \"label\": \"Test\",\n            \"group\": \"test\",\n            \"type\": \"shell\",\n            \"command\": \"/path/to/kedro/script\",\n            \"args\": [\n                \"test\"\n            ]\n        },\n        {\n            \"label\": \"Run\",\n            \"type\": \"shell\",\n            \"command\": \"/path/to/kedro/script\",\n            \"args\": [\n                \"run\"\n            ]\n        },\n        {\n            \"label\": \"Package\",\n            \"group\": {\n                \"kind\": \"build\",\n                \"isDefault\": true\n            },\n            \"type\": \"shell\",\n            \"command\": \"/path/to/kedro/script\",\n            \"args\": [\n                \"package\"\n            ],\n            \"dependsOn\": [\n                \"Test\"\n            ]\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Pipeline-Level Namespace in Kedro Data Science Pipeline\nDESCRIPTION: Creates a data processing pipeline with a namespace applied at the pipeline level. The pipeline contains three nodes for preprocessing companies, shuttles and creating a model input table.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/namespaces.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.pipeline import Pipeline, node, pipeline\nfrom .nodes import create_model_input_table, preprocess_companies, preprocess_shuttles\n\ndef create_pipeline(**kwargs) -> Pipeline:\n    return pipeline(\n        [\n            node(\n                func=preprocess_companies,\n                inputs=\"companies\",\n                outputs=\"preprocessed_companies\",\n                name=\"preprocess_companies_node\",\n            ),\n            node(\n                func=preprocess_shuttles,\n                inputs=\"shuttles\",\n                outputs=\"preprocessed_shuttles\",\n                name=\"preprocess_shuttles_node\",\n            ),\n            node(\n                func=create_model_input_table,\n                inputs=[\"preprocessed_shuttles\", \"preprocessed_companies\", \"reviews\"],\n                outputs=\"model_input_table\",\n                name=\"create_model_input_table_node\",\n            ),\n        ],\n        namespace=\"data_processing\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Using Custom Resolvers with OmegaConfigLoader in Python\nDESCRIPTION: Shows how to register and use custom resolvers with OmegaConfigLoader outside of a full Kedro project.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/advanced_configuration.md#2025-04-19_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport polars as pl\nfrom datetime import date\n\nfrom kedro.config import OmegaConfigLoader\n\ncustom_resolvers = {\"polars\": lambda x: getattr(pl, x),\n                    \"today\": lambda: date.today()}\n\n# Register custom resolvers\nconfig_loader = OmegaConfigLoader(conf_source=\".\", custom_resolvers=custom_resolvers)\n\nprint(config_loader[\"parameters\"])\n```\n\n----------------------------------------\n\nTITLE: Node Functions Implementation\nDESCRIPTION: Example implementation of node functions in nodes.py for statistical calculations\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/modular_pipelines.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef mean(xs, n):\n    return sum(xs) / n\n\ndef mean_sos(xs, n):\n    return sum(x**2 for x in xs) / n\n\ndef variance(m, m2):\n    return m2 - m * m\n```\n\n----------------------------------------\n\nTITLE: Implementing Pipeline Autodiscovery in Kedro (Python)\nDESCRIPTION: This code snippet shows how to use Kedro's `find_pipelines()` function to automatically discover and register pipelines. It simplifies the pipeline registration process by automatically finding and combining all available pipelines.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/pipeline_registry.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef register_pipelines() -> Dict[str, Pipeline]:\n    \"\"\"Register the project's pipelines.\n\n    Returns:\n        A mapping from pipeline names to ``Pipeline`` objects.\n    \"\"\"\n    pipelines = find_pipelines()\n    pipelines[\"__default__\"] = sum(pipelines.values())\n    return pipelines\n```\n\n----------------------------------------\n\nTITLE: Running Kedro Pipeline\nDESCRIPTION: This command executes the Kedro pipeline for the project.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/kedro/templates/project/{{ cookiecutter.repo_name }}/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkedro run\n```\n\n----------------------------------------\n\nTITLE: Generalizing Datasets of Same Type with Dataset Factory in YAML\nDESCRIPTION: Example showing how to consolidate multiple datasets of the same type (CSVDataset) using a dataset factory pattern with a suffix identifier (#csv) to ensure proper matching.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/kedro_dataset_factories.md#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nboats:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/shuttles.csv\n\ncars:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/reviews.csv\n\nplanes:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/companies.csv\n```\n\nLANGUAGE: yaml\nCODE:\n```\n\"{dataset_name}#csv\":\n  type: pandas.CSVDataset\n  filepath: data/01_raw/{dataset_name}.csv\n```\n\n----------------------------------------\n\nTITLE: Loading Data Catalog with Templating in Python\nDESCRIPTION: This code demonstrates how to use OmegaConfigLoader to load a data catalog that contains templating. The OmegaConfigLoader resolves any templates automatically, resulting in a properly configured catalog entry.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/advanced_configuration.md#2025-04-19_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n# Example catalog with templating\ncompanies:\n  type: ${_dataset_type}\n  filepath: data/01_raw/companies.csv\n\n_dataset_type: pandas.CSVDataset\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.config import OmegaConfigLoader\nfrom kedro.framework.project import settings\n\n# Instantiate an `OmegaConfigLoader` instance with the location of your project configuration.\nconf_path = str(project_path / settings.CONF_SOURCE)\nconf_loader = OmegaConfigLoader(conf_source=conf_path)\n\nconf_catalog = conf_loader[\"catalog\"]\n# conf_catalog[\"companies\"]\n# Will result in: {'type': 'pandas.CSVDataset', 'filepath': 'data/01_raw/companies.csv'}\n```\n\n----------------------------------------\n\nTITLE: Slicing Pipeline with Tagged Nodes in Python\nDESCRIPTION: This snippet demonstrates how to slice a pipeline using node tags. It shows examples of slicing with nodes that have both 'mean' AND 'variance' tags, and nodes with 'mean' OR 'variance' tags.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/slice_a_pipeline.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(full_pipeline.only_nodes_with_tags(\"mean\", \"variance\").describe())\n\nsliced_pipeline = full_pipeline.only_nodes_with_tags(\n    \"mean\"\n) + full_pipeline.only_nodes_with_tags(\"variance\")\nprint(sliced_pipeline.describe())\n```\n\n----------------------------------------\n\nTITLE: Assembling Kedro Pipeline\nDESCRIPTION: Shows how to create a pipeline by combining multiple nodes. The pipeline automatically determines the execution order based on node dependencies.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/get_started/kedro_concepts.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.pipeline import pipeline\n\n# Assemble nodes into a pipeline\ngreeting_pipeline = pipeline([return_greeting_node, join_statements_node])\n```\n\n----------------------------------------\n\nTITLE: Configuring OmegaConfigLoader in Kedro Settings\nDESCRIPTION: Code diff showing how to set OmegaConfigLoader as the CONFIG_LOADER_CLASS in the Kedro project settings file.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/config_loader_migration.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n+ from kedro.config import OmegaConfigLoader  # new import\n\n+ CONFIG_LOADER_CLASS = OmegaConfigLoader\n```\n\n----------------------------------------\n\nTITLE: Using Nested YAML Anchors for Complex Dataset Configurations in Kedro\nDESCRIPTION: Example showing how to use nested YAML anchors to create hierarchical reusable configurations for datasets with partial overrides in Kedro's catalog.yml.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog_yaml_examples.md#2025-04-19_snippet_20\n\nLANGUAGE: yaml\nCODE:\n```\n_csv: &csv\n  type: spark.SparkDataset\n  file_format: csv\n  load_args: &csv_load_args\n    header: True\n    inferSchema: False\n\nairplanes:\n  <<: *csv\n  filepath: s3a://data/01_raw/airplanes.csv\n  load_args:\n    <<: *csv_load_args\n    sep: ;\n```\n\n----------------------------------------\n\nTITLE: Testing Versioned Dataset in IPython\nDESCRIPTION: IPython commands demonstrating how to load and save versioned data using Kedro's catalog interface.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/how_to_create_a_custom_dataset.md#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# loading works as Kedro automatically find the latest available version inside `pikachu.png` directory\nIn [1]: img = context.catalog.load('pikachu')\n# then saving it should work as well\nIn [2]: context.catalog.save('pikachu', data=img)\n```\n\n----------------------------------------\n\nTITLE: Initializing Kedro OmegaConfigLoader\nDESCRIPTION: Shows how to initialize Kedro's OmegaConfigLoader to load configuration from parameters.yml\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.config import OmegaConfigLoader\n\nconf_loader = OmegaConfigLoader(conf_source=\".\")\n```\n\n----------------------------------------\n\nTITLE: Configuring PartitionedDataset in YAML\nDESCRIPTION: Example of defining a PartitionedDataset in the catalog.yml file, specifying the path, dataset type, and additional arguments.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/partitioned_and_incremental_datasets.md#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmy_partitioned_dataset:\n  type: partitions.PartitionedDataset\n  path: s3://my-bucket-name/path/to/folder  # path to the location of partitions\n  dataset: pandas.CSVDataset  # shorthand notation for the dataset which will handle individual partitions\n  credentials: my_credentials\n  load_args:\n    load_arg1: value1\n    load_arg2: value2\n```\n\n----------------------------------------\n\nTITLE: Implementing Pytest Fixtures for Kedro Testing\nDESCRIPTION: Defines reusable pytest fixtures for dummy data and parameters used in testing data science pipeline nodes. Creates a test DataFrame and model parameters that can be accessed across multiple tests.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/test_a_project.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\n\n@pytest.fixture\ndef dummy_data():\n    return pd.DataFrame(\n        {\n            \"engines\": [1, 2, 3],\n            \"crew\": [4, 5, 6],\n            \"passenger_capacity\": [5, 6, 7],\n            \"price\": [120, 290, 30],\n        }\n    )\n\n@pytest.fixture\ndef dummy_parameters():\n    parameters = {\n        \"model_options\": {\n            \"test_size\": 0.2,\n            \"random_state\": 3,\n            \"features\": [\"engines\", \"passenger_capacity\", \"crew\"],\n        }\n    }\n    return parameters\n```\n\n----------------------------------------\n\nTITLE: Accessing Datasets in KedroDataCatalog using Python\nDESCRIPTION: Demonstrates how to retrieve datasets from the catalog using dictionary-like syntax or the get method.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/kedro_data_catalog.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nreviews_ds = catalog[\"reviews\"]\nreviews_ds = catalog.get(\"reviews\", default=default_ds)\n```\n\n----------------------------------------\n\nTITLE: Combining Globals and Runtime Parameters in YAML\nDESCRIPTION: Shows how to use globals as a default value for runtime parameters in a parameters.yml file.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/advanced_configuration.md#2025-04-19_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_options:\n  random_state: \"${runtime_params:random, ${globals:my_global_value}}\"\n```\n\n----------------------------------------\n\nTITLE: Refactored Data Processing and Model Training Functions\nDESCRIPTION: Modular implementation with separate functions for data preprocessing, model training and evaluation following Kedro best practices\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook.ipynb#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n####################\n# Data processing  #\n####################\n\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\n\n\ndef _is_true(x: pd.Series) -> pd.Series:\n    return x == \"t\"\n\n\ndef _parse_percentage(x: pd.Series) -> pd.Series:\n    x = x.str.replace(\"%\", \"\")\n    x = x.astype(float) / 100\n    return x\n\n\ndef _parse_money(x: pd.Series) -> pd.Series:\n    x = x.str.replace(\"$\", \"\").str.replace(\",\", \"\")\n    x = x.astype(float)\n    return x\n\n\ndef preprocess_companies(companies: pd.DataFrame) -> pd.DataFrame:\n    companies[\"iata_approved\"] = _is_true(companies[\"iata_approved\"])\n    companies[\"company_rating\"] = _parse_percentage(companies[\"company_rating\"])\n    return companies\n\n\ndef preprocess_shuttles(shuttles: pd.DataFrame) -> pd.DataFrame:\n    shuttles[\"d_check_complete\"] = _is_true(shuttles[\"d_check_complete\"])\n    shuttles[\"moon_clearance_complete\"] = _is_true(shuttles[\"moon_clearance_complete\"])\n    shuttles[\"price\"] = _parse_money(shuttles[\"price\"])\n    return shuttles\n\n\ndef create_model_input_table(\n    shuttles: pd.DataFrame, companies: pd.DataFrame, reviews: pd.DataFrame\n) -> pd.DataFrame:\n    rated_shuttles = shuttles.merge(reviews, left_on=\"id\", right_on=\"shuttle_id\")\n    model_input_table = rated_shuttles.merge(\n        companies, left_on=\"company_id\", right_on=\"id\"\n    )\n    model_input_table = model_input_table.dropna()\n    return model_input_table\n\n\n##################################\n# Model training and evaluation  #\n##################################\n\n\ndef split_data(data: pd.DataFrame, parameters: dict) -> tuple:\n    X = data[parameters[\"features\"]]\n    y = data[\"price\"]\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=parameters[\"test_size\"], random_state=parameters[\"random_state\"]\n    )\n    return X_train, X_test, y_train, y_test\n\n\ndef train_model(X_train: pd.DataFrame, y_train: pd.Series) -> LinearRegression:\n    regressor = LinearRegression()\n    regressor.fit(X_train, y_train)\n    return regressor\n\n\ndef evaluate_model(\n    regressor: LinearRegression, X_test: pd.DataFrame, y_test: pd.Series\n):\n    y_pred = regressor.predict(X_test)\n    print(r2_score(y_test, y_pred))\n```\n\n----------------------------------------\n\nTITLE: Configuring API Dataset in Kedro\nDESCRIPTION: Example showing how to load data from a USDA API endpoint with authentication credentials and query parameters in Kedro's catalog.yml.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog_yaml_examples.md#2025-04-19_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\nus_corn_yield_data:\n  type: api.APIDataset\n  url: https://quickstats.nass.usda.gov\n  credentials: usda_credentials\n  params:\n    key: SOME_TOKEN\n    format: JSON\n    commodity_desc: CORN\n    statisticcat_des: YIELD\n    agg_level_desc: STATE\n    year: 2000\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Blob Storage Dataset in Kedro\nDESCRIPTION: Example showing how to load a versioned pickle model from Azure Blob Storage with account credentials in Kedro's catalog.yml.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog_yaml_examples.md#2025-04-19_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\nml_model:\n  type: pickle.PickleDataset\n  filepath: \"abfs://models/ml_models.pickle\"\n  versioned: True\n  credentials: dev_abs\n```\n\n----------------------------------------\n\nTITLE: Recreating Missing Outputs in Python\nDESCRIPTION: This snippet demonstrates how to automatically generate a sliced pipeline from existing node outputs. It shows how to save intermediate output, check for its existence, and run the pipeline avoiding re-calculation of existing results.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/slice_a_pipeline.md#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro_datasets.pandas import JSONDataset\nfrom kedro.io import DataCatalog, MemoryDataset\n\nn_json = JSONDataset(filepath=\"./data/07_model_output/len.json\")\nio = DataCatalog(dict(xs=MemoryDataset([1, 2, 3]), n=n_json))\n\nio.exists(\"n\")\n\nSequentialRunner().run(full_pipeline, io)\n\nio.exists(\"n\")\n\nSequentialRunner().run_only_missing(full_pipeline, io)\n\ntry:\n    os.remove(\"./data/07_model_output/len.json\")\nexcept FileNotFoundError:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Configuring Versioned Dataset in YAML\nDESCRIPTION: Illustrates how to enable versioning for a dataset in the Data Catalog. This configuration allows Kedro to manage different versions of the dataset.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog.md#2025-04-19_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\ncars:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/company/cars.csv\n  versioned: True\n```\n\n----------------------------------------\n\nTITLE: Creating Reusable Pipeline with Parameter Mapping\nDESCRIPTION: Shows how to create a new pipeline based on an existing one by mapping different inputs, outputs and parameters.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/namespaces.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef create_pipeline(**kwargs) -> Pipeline:\n    return pipeline(\n       existing_pipeline, # Name of the existing Pipeline object\n       inputs = {\"old_input_df_name\" : \"new_input_df_name\"},  # Mapping existing Pipeline input to new input\n       outputs = {\"old_output_df_name\" : \"new_output_df_name\"},  # Mapping existing Pipeline output to new output\n       parameters = {\"params: model_options\": \"params: new_model_options\"},  # Updating parameters\n    )\n```\n\n----------------------------------------\n\nTITLE: Running Kedro Pipeline in Jupyter\nDESCRIPTION: Demonstrates how to run a Kedro pipeline using the session.run() method in a Jupyter notebook. This method executes the default project pipeline sequentially.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/kedro_and_notebooks.md#2025-04-19_snippet_10\n\nLANGUAGE: ipython\nCODE:\n```\nsession.run()\n```\n\n----------------------------------------\n\nTITLE: Loading Data Catalog with Credentials in Kedro\nDESCRIPTION: Shows how to load a data catalog with credentials in Kedro code using OmegaConfigLoader and DataCatalog.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/configuration_basics.md#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.config import OmegaConfigLoader\nfrom kedro.framework.project import settings\nfrom kedro.io import DataCatalog\n\nconf_path = str(project_path / settings.CONF_SOURCE)\nconf_loader = OmegaConfigLoader(\n    conf_source=conf_path, base_env=\"base\", default_run_env=\"local\"\n)\n\nconf_catalog = conf_loader[\"catalog\"]\nconf_credentials = conf_loader[\"credentials\"]\n\ncatalog = DataCatalog.from_config(catalog=conf_catalog, credentials=conf_credentials)\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet Dataset with Custom Arguments in Kedro\nDESCRIPTION: Example showing how to load and save a Parquet file on local file system with column selection, compression, and partitioning settings in Kedro's catalog.yml.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog_yaml_examples.md#2025-04-19_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\ntrucks:\n  type: pandas.ParquetDataset\n  filepath: data/02_intermediate/trucks.parquet\n  load_args:\n    columns: [name, gear, disp, wt]\n    categories: list\n    index: name\n  save_args:\n    compression: GZIP\n    file_scheme: hive\n    has_nulls: False\n    partition_on: [name]\n```\n\n----------------------------------------\n\nTITLE: Simplified pipeline with non-returning Delta operations\nDESCRIPTION: Python code showing a streamlined pipeline definition where Delta operations don't need to return any outputs, useful for simple update scenarios.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/deltalake_versioning.md#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\npipeline(\n    [\n        node(func=..., inputs=\"temperature\", outputs=\"weather@spark\"),\n        node(func=..., inputs=\"weather@delta\", outputs=None),\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Registering Memory Profiling Hooks in Kedro Settings\nDESCRIPTION: Code snippet showing how to register the MemoryProfilingHooks implementation in the Kedro settings.py file.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/examples.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nHOOKS = (MemoryProfilingHooks(),)\n```\n\n----------------------------------------\n\nTITLE: Generating Argo Workflow Specifications for Kedro Pipelines in Python\nDESCRIPTION: This Python script generates Argo Workflow specifications for Kedro pipelines. It takes Docker image information and optional pipeline name, converts node dependencies into Argo tasks, and renders a template file to create the final workflow specification.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/argo.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# <project_root>/build_argo_spec.py\nimport re\nfrom pathlib import Path\n\nimport click\nfrom jinja2 import Environment, FileSystemLoader\n\nfrom kedro.framework.project import pipelines\nfrom kedro.framework.startup import bootstrap_project\n\nTEMPLATE_FILE = \"argo_spec.tmpl\"\nSEARCH_PATH = Path(\"templates\")\n\n\n@click.command()\n@click.argument(\"image\", required=True)\n@click.option(\"-p\", \"--pipeline\", \"pipeline_name\", default=None)\n@click.option(\"--env\", \"-e\", type=str, default=None)\ndef generate_argo_config(image, pipeline_name, env):\n    loader = FileSystemLoader(searchpath=SEARCH_PATH)\n    template_env = Environment(loader=loader, trim_blocks=True, lstrip_blocks=True)\n    template = template_env.get_template(TEMPLATE_FILE)\n\n    project_path = Path.cwd()\n    metadata = bootstrap_project(project_path)\n    package_name = metadata.package_name\n\n    pipeline_name = pipeline_name or \"__default__\"\n    pipeline = pipelines.get(pipeline_name)\n\n    tasks = get_dependencies(pipeline.node_dependencies)\n\n    output = template.render(image=image, package_name=package_name, tasks=tasks)\n\n    (SEARCH_PATH / f\"argo-{package_name}.yml\").write_text(output)\n\n\ndef get_dependencies(dependencies):\n    deps_dict = [\n        {\n            \"node\": node.name,\n            \"name\": clean_name(node.name),\n            \"deps\": [clean_name(val.name) for val in parent_nodes],\n        }\n        for node, parent_nodes in dependencies.items()\n    ]\n    return deps_dict\n\n\ndef clean_name(name):\n    return re.sub(r\"[\\W_]+\", \"-\", name).strip(\"-\")\n\n\nif __name__ == \"__main__\":\n    generate_argo_config()\n```\n\n----------------------------------------\n\nTITLE: Loading and Saving Data Using Data Catalog in Python\nDESCRIPTION: This snippet shows how to load data from the catalog, save data to memory, save data to a SQL database, and save data in Parquet format using the programmatically configured Data Catalog.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/advanced_data_catalog_usage.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncars = catalog.load(\"cars\")  # data is now loaded as a DataFrame in 'cars'\ngear = cars[\"gear\"].values\n\nfrom kedro.io import MemoryDataset\n\nmemory = MemoryDataset(data=None)\ncatalog.add(\"cars_cache\", memory)\ncatalog.save(\"cars_cache\", \"Memory can store anything.\")\ncatalog.load(\"cars_cache\")\n\nimport os\n\n# This cleans up the database in case it exists at this point\ntry:\n    os.remove(\"kedro.db\")\nexcept FileNotFoundError:\n    pass\n\ncatalog.save(\"cars_table\", cars)\n\n# rank scooters by their mpg\nranked = catalog.load(\"scooters_query\")[[\"brand\", \"mpg\"]]\n\ncatalog.save(\"ranked\", ranked)\n```\n\n----------------------------------------\n\nTITLE: Pipeline Slicing for Targeted Testing\nDESCRIPTION: Demonstrates how to use pipeline slicing to test specific segments of a data science pipeline. Defines start and end nodes for targeted testing.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/test_a_project.md#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef test_data_science_pipeline(self):\n    # Arrange pipeline\n    pipeline = create_pipeline().from_nodes(\"split_data_node\").to_nodes(\"evaluate_model_node\")\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Load Parameters for Local Binary Files in Kedro\nDESCRIPTION: Example showing how to configure open_args_load and encoding parameters when loading data from a local binary file using utf-8 encoding in Kedro's catalog.yml.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog_yaml_examples.md#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntest_dataset:\n  type: ...\n  fs_args:\n    open_args_load:\n      mode: \"rb\"\n      encoding: \"utf-8\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Data Validation Hooks with Great Expectations V3 API\nDESCRIPTION: Implementation of before_node_run and after_node_run hooks to validate inputs and outputs data using Great Expectations V3 API with checkpoints.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/examples.md#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# src/<package_name>/hooks.py\nfrom typing import Any, Dict\n\nfrom kedro.framework.hooks import hook_impl\nfrom kedro.io import DataCatalog\n\nimport great_expectations as ge\n\n\nclass DataValidationHooks:\n    # Map checkpoint to dataset\n    DATASET_CHECKPOINT_MAPPING = {\n        \"companies\": \"raw_companies_dataset_checkpoint\",\n    }\n\n    @hook_impl\n    def before_node_run(\n        self, catalog: DataCatalog, inputs: Dict[str, Any], session_id: str\n    ) -> None:\n        \"\"\"Validate inputs data to a node based on using great expectation\n        if an expectation suite is defined in ``DATASET_EXPECTATION_MAPPING``.\n        \"\"\"\n        self._run_validation(catalog, inputs, session_id)\n\n    @hook_impl\n    def after_node_run(\n        self, catalog: DataCatalog, outputs: Dict[str, Any], session_id: str\n    ) -> None:\n        \"\"\"Validate outputs data from a node based on using great expectation\n        if an expectation suite is defined in ``DATASET_EXPECTATION_MAPPING``.\n        \"\"\"\n        self._run_validation(catalog, outputs, session_id)\n\n    def _run_validation(\n        self, catalog: DataCatalog, data: Dict[str, Any], session_id: str\n    ):\n        for dataset_name, dataset_value in data.items():\n            if dataset_name not in self.DATASET_CHECKPOINT_MAPPING:\n                continue\n\n            data_context = ge.data_context.DataContext()\n\n            data_context.run_checkpoint(\n                checkpoint_name=self.DATASET_CHECKPOINT_MAPPING[dataset_name],\n                batch_request={\n                    \"runtime_parameters\": {\n                        \"batch_data\": dataset_value,\n                    },\n                    \"batch_identifiers\": {\n                        \"runtime_batch_identifier_name\": dataset_name\n                    },\n                },\n                run_name=session_id,\n            )\n```\n\n----------------------------------------\n\nTITLE: Implementing the Data Processing and Model Training Pipeline in Python\nDESCRIPTION: Code that executes the data processing and model training pipeline by calling the defined functions in sequence. This includes preprocessing the data, creating the model input table, splitting the data, training the model, and evaluating its performance.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook.ipynb#2025-04-19_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# Call data processing functions\npreprocessed_companies = preprocess_companies(companies)\npreprocessed_shuttles = preprocess_shuttles(shuttles)\nmodel_input_table = create_model_input_table(\n    preprocessed_shuttles, preprocessed_companies, reviews\n)\n\n# Call model evaluation functions\nX_train, X_test, y_train, y_test = split_data(\n    model_input_table, conf_params[\"model_options\"]\n)\nregressor = train_model(X_train, y_train)\nevaluate_model(regressor, X_test, y_test)\n```\n\n----------------------------------------\n\nTITLE: Configuring HDF Dataset with Custom Arguments in Kedro\nDESCRIPTION: Example showing how to load and save an HDF file on local file system with column selection and write mode specifications in Kedro's catalog.yml.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog_yaml_examples.md#2025-04-19_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nskateboards:\n  type: pandas.HDFDataset\n  filepath: data/02_intermediate/skateboards.hdf\n  key: name\n  load_args:\n    columns: [brand, length]\n  save_args:\n    mode: w  # Overwrite even when the file already exists\n    dropna: True\n```\n\n----------------------------------------\n\nTITLE: Customizing Kedro CLI Commands in Python\nDESCRIPTION: This Python script template shows how to customize Kedro CLI commands. It includes a sample implementation of the 'run' command with various options and parameters.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/commands_reference.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Command line tools for manipulating a Kedro project.\nIntended to be invoked via `kedro`.\"\"\"\nimport click\nfrom kedro.framework.cli.project import (\n    ASYNC_ARG_HELP,\n    CONFIG_FILE_HELP,\n    CONF_SOURCE_HELP,\n    FROM_INPUTS_HELP,\n    FROM_NODES_HELP,\n    LOAD_VERSION_HELP,\n    NODE_ARG_HELP,\n    PARAMS_ARG_HELP,\n    PIPELINE_ARG_HELP,\n    RUNNER_ARG_HELP,\n    TAG_ARG_HELP,\n    TO_NODES_HELP,\n    TO_OUTPUTS_HELP,\n)\nfrom kedro.framework.cli.utils import (\n    CONTEXT_SETTINGS,\n    _config_file_callback,\n    _split_params,\n    _split_load_versions,\n    env_option,\n    split_string,\n    split_node_names,\n)\nfrom kedro.framework.session import KedroSession\nfrom kedro.utils import load_obj\n\n\n@click.group(context_settings=CONTEXT_SETTINGS, name=__file__)\ndef cli():\n    \"\"\"Command line tools for manipulating a Kedro project.\"\"\"\n\n\n@cli.command()\n@click.option(\n    \"--from-inputs\", type=str, default=\"\", help=FROM_INPUTS_HELP, callback=split_string\n)\n@click.option(\n    \"--to-outputs\", type=str, default=\"\", help=TO_OUTPUTS_HELP, callback=split_string\n)\n@click.option(\n    \"--from-nodes\", type=str, default=\"\", help=FROM_NODES_HELP, callback=split_node_names\n)\n@click.option(\n    \"--to-nodes\", type=str, default=\"\", help=TO_NODES_HELP, callback=split_node_names\n)\n@click.option(\"--nodes\", \"-n\", \"node_names\", type=str, multiple=True, help=NODE_ARG_HELP)\n@click.option(\n    \"--runner\", \"-r\", type=str, default=None, multiple=False, help=RUNNER_ARG_HELP\n)\n@click.option(\"--async\", \"is_async\", is_flag=True, multiple=False, help=ASYNC_ARG_HELP)\n@env_option\n@click.option(\"--tags\", \"-t\", type=str, multiple=True, help=TAG_ARG_HELP)\n@click.option(\n    \"--load-versions\",\n    \"-lv\",\n    type=str,\n    multiple=True,\n    help=LOAD_VERSION_HELP,\n    callback=_split_load_versions,\n)\n@click.option(\"--pipeline\", \"-p\", type=str, default=None, help=PIPELINE_ARG_HELP)\n@click.option(\n    \"--config\",\n    \"-c\",\n    type=click.Path(exists=True, dir_okay=False, resolve_path=True),\n    help=CONFIG_FILE_HELP,\n    callback=_config_file_callback,\n)\n@click.option(\n    \"--conf-source\",\n    type=click.Path(exists=True, file_okay=False, resolve_path=True),\n    help=CONF_SOURCE_HELP,\n)\n@click.option(\n    \"--params\",\n    type=click.UNPROCESSED,\n    default=\"\",\n    help=PARAMS_ARG_HELP,\n    callback=_split_params,\n)\ndef run(\n    tags,\n    env,\n    runner,\n    is_async,\n    node_names,\n    to_nodes,\n    from_nodes,\n    from_inputs,\n    to_outputs,\n    load_versions,\n    pipeline,\n    config,\n    conf_source,\n    params,\n):\n    \"\"\"Run the pipeline.\"\"\"\n\n    runner = load_obj(runner or \"SequentialRunner\", \"kedro.runner\")\n    tags = tuple(tags)\n    node_names = tuple(node_names)\n\n    with KedroSession.create(\n        env=env, conf_source=conf_source, extra_params=params\n    ) as session:\n        session.run(\n            tags=tags,\n            runner=runner(is_async=is_async),\n            node_names=node_names,\n            from_nodes=from_nodes,\n            to_nodes=to_nodes,\n            from_inputs=from_inputs,\n            to_outputs=to_outputs,\n            load_versions=load_versions,\n            pipeline_name=pipeline,\n        )\n\n```\n\n----------------------------------------\n\nTITLE: Versioning Datasets in Python using Kedro's Version Class\nDESCRIPTION: This snippet demonstrates how to version datasets programmatically using Kedro's Version class. It shows examples of automatic versioning and specifying exact versions for load and save operations.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/advanced_data_catalog_usage.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.io import DataCatalog, Version\nfrom kedro_datasets.pandas import CSVDataset\nimport pandas as pd\n\ndata1 = pd.DataFrame({\"col1\": [1, 2], \"col2\": [4, 5], \"col3\": [5, 6]})\ndata2 = pd.DataFrame({\"col1\": [7], \"col2\": [8], \"col3\": [9]})\nversion = Version(\n    load=None,  # load the latest available version\n    save=None,  # generate save version automatically on each save operation\n)\n\ntest_dataset = CSVDataset(\n    filepath=\"data/01_raw/test.csv\", save_args={\"index\": False}, version=version\n)\ncatalog =  DataCatalog({\"test_dataset\": test_dataset})\n\n# save the dataset to data/01_raw/test.csv/<version>/test.csv\ncatalog.save(\"test_dataset\", data1)\n# save the dataset into a new file data/01_raw/test.csv/<version>/test.csv\ncatalog.save(\"test_dataset\", data2)\n\n# load the latest version from data/test.csv/*/test.csv\nreloaded = catalog.load(\"test_dataset\")\nassert data2.equals(reloaded)\n```\n\n----------------------------------------\n\nTITLE: Bypassing Configuration Loading Rules in Python\nDESCRIPTION: This snippet shows how to bypass the configuration patterns and set configuration directly on the instance of a config loader class, overriding the catalog configuration with custom values.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/advanced_configuration.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.config import OmegaConfigLoader\nfrom kedro.framework.project import settings\n\nconf_path = str(project_path / settings.CONF_SOURCE)\nconf_loader = OmegaConfigLoader(conf_source=conf_path)\n\n# Bypass configuration patterns by setting the key and values directly on the config loader instance.\nconf_loader[\"catalog\"] = {\"catalog_config\": \"something_new\"}\n```\n\n----------------------------------------\n\nTITLE: Reading Metadata from DataCatalog using Kedro Hooks\nDESCRIPTION: Implementation of a hook that accesses metadata from the Kedro DataCatalog. The hook uses the 'after_catalog_created' hook point to print metadata for all datasets registered in the catalog.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/common_use_cases.md#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass MetadataHook:\n    @hook_impl\n    def after_catalog_created(\n        self,\n        catalog: DataCatalog,\n    ):\n        for dataset_name, dataset in catalog.datasets.__dict__.items():\n            print(f\"{dataset_name} metadata: \\n  {str(dataset.metadata)}\")\n```\n\n----------------------------------------\n\nTITLE: Accessing Kedro Project Context in a Plugin\nDESCRIPTION: Code showing how plugins can access information about the current Kedro project by creating a session and loading its context.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/extend_kedro/plugins.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\n\nfrom kedro.framework.session import KedroSession\n\n\nproject_path = Path.cwd()\nsession = KedroSession.create(project_path=project_path)\ncontext = session.load_context()\n```\n\n----------------------------------------\n\nTITLE: Pipeline IO Implementation Example\nDESCRIPTION: Python code demonstrating how to use DataCatalog for pipeline IO operations.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/run_a_pipeline.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nio = DataCatalog(dict(xs=MemoryDataset()))\n```\n\nLANGUAGE: python\nCODE:\n```\nio.save(\"xs\", [1, 2, 3])\n```\n\nLANGUAGE: python\nCODE:\n```\nSequentialRunner().run(pipeline, catalog=io)\n```\n\n----------------------------------------\n\nTITLE: Defining Transcoded Datasets in Kedro Catalog YAML\nDESCRIPTION: This snippet demonstrates how to define two DataCatalog entries for the same dataset using transcoding in the catalog.yml file. It shows how to specify both spark.SparkDataset and pandas.ParquetDataset for the same Parquet file.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog_yaml_examples.md#2025-04-19_snippet_21\n\nLANGUAGE: yaml\nCODE:\n```\nmy_dataframe@spark:\n  type: spark.SparkDataset\n  filepath: data/02_intermediate/data.parquet\n  file_format: parquet\n\nmy_dataframe@pandas:\n  type: pandas.ParquetDataset\n  filepath: data/02_intermediate/data.parquet\n```\n\n----------------------------------------\n\nTITLE: Creating Data Science Pipeline in Kedro\nDESCRIPTION: Pipeline definition that combines multiple nodes for data splitting, model training, and evaluation into a complete workflow.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/test_a_project.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef create_pipeline(**kwargs) -> Pipeline:\n    return pipeline(\n        [\n            node(\n                func=split_data,\n                inputs=[\"model_input_table\", \"params:model_options\"],\n                outputs=[\"X_train\", \"X_test\", \"y_train\", \"y_test\"],\n                name=\"split_data_node\",\n            ),\n            node(\n                func=train_model,\n                inputs=[\"X_train\", \"y_train\"],\n                outputs=\"regressor\",\n                name=\"train_model_node\",\n            ),\n            node(\n                func=evaluate_model,\n                inputs=[\"regressor\", \"X_test\", \"y_test\"],\n                outputs=None,\n                name=\"evaluate_model_node\",\n            ),\n        ]\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Kedro CLI Run Function with Runner Support\nDESCRIPTION: Implementation of the run() function in cli.py that handles pipeline execution with configurable runners. The function creates a Kedro session and executes the pipeline with specified parameters.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/dask.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef run(tag, env, ...):\n    \"\"\"Run the pipeline.\"\"\"\n    runner = runner or \"SequentialRunner\"\n\n    tag = _get_values_as_tuple(tag) if tag else tag\n    node_names = _get_values_as_tuple(node_names) if node_names else node_names\n\n    with KedroSession.create(env=env, extra_params=params) as session:\n        context = session.load_context()\n        runner_instance = _instantiate_runner(runner, is_async, context)\n        session.run(\n            tags=tag,\n            runner=runner_instance,\n            node_names=node_names,\n            from_nodes=from_nodes,\n            to_nodes=to_nodes,\n            from_inputs=from_inputs,\n            to_outputs=to_outputs,\n            load_versions=load_version,\n            pipeline_name=pipeline,\n        )\n```\n\n----------------------------------------\n\nTITLE: Adding Labels to Kedro Nodes (Python)\nDESCRIPTION: Shows how to add labels to Kedro nodes, which are used to describe them in logs, and demonstrates the difference in string representation.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/nodes.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nadder_node = node(func=add, inputs=[\"a\", \"b\"], outputs=\"sum\")\nprint(str(adder_node))\n\nadder_node = node(func=add, inputs=[\"a\", \"b\"], outputs=\"sum\", name=\"adding_a_and_b\")\nprint(str(adder_node))\n```\n\n----------------------------------------\n\nTITLE: Defining Parameters in YAML Configuration\nDESCRIPTION: Example of defining simple and nested parameters in a YAML configuration file for Kedro projects.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/parameters.md#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nstep_size: 1\nlearning_rate: 0.01\n```\n\nLANGUAGE: yaml\nCODE:\n```\nstep_size: 1\nmodel_params:\n    learning_rate: 0.01\n    test_data_ratio: 0.2\n    number_of_train_iterations: 10000\n```\n\n----------------------------------------\n\nTITLE: Implementing Data Validation Hooks with Great Expectations V2 API\nDESCRIPTION: Implementation of before_node_run and after_node_run hooks to validate inputs and outputs data using Great Expectations V2 API.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/examples.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# src/<package_name>/hooks.py\nfrom typing import Any, Dict\n\nfrom kedro.framework.hooks import hook_impl\nfrom kedro.io import DataCatalog\n\nimport great_expectations as ge\n\n\nclass DataValidationHooks:\n    # Map expectation to dataset\n    DATASET_EXPECTATION_MAPPING = {\n        \"companies\": \"raw_companies_dataset_expectation\",\n        \"preprocessed_companies\": \"preprocessed_companies_dataset_expectation\",\n    }\n\n    @hook_impl\n    def before_node_run(\n        self, catalog: DataCatalog, inputs: Dict[str, Any], session_id: str\n    ) -> None:\n        \"\"\"Validate inputs data to a node based on using great expectation\n        if an expectation suite is defined in ``DATASET_EXPECTATION_MAPPING``.\n        \"\"\"\n        self._run_validation(catalog, inputs, session_id)\n\n    @hook_impl\n    def after_node_run(\n        self, catalog: DataCatalog, outputs: Dict[str, Any], session_id: str\n    ) -> None:\n        \"\"\"Validate outputs data from a node based on using great expectation\n        if an expectation suite is defined in ``DATASET_EXPECTATION_MAPPING``.\n        \"\"\"\n        self._run_validation(catalog, outputs, session_id)\n\n    def _run_validation(\n        self, catalog: DataCatalog, data: Dict[str, Any], session_id: str\n    ):\n        for dataset_name, dataset_value in data.items():\n            if dataset_name not in self.DATASET_EXPECTATION_MAPPING:\n                continue\n\n            dataset = catalog._get_dataset(dataset_name)\n            dataset_path = str(dataset._filepath)\n            expectation_suite = self.DATASET_EXPECTATION_MAPPING[dataset_name]\n\n            expectation_context = ge.data_context.DataContext()\n            batch = expectation_context.get_batch(\n                {\"path\": dataset_path, \"datasource\": \"files_datasource\"},\n                expectation_suite,\n            )\n            expectation_context.run_validation_operator(\n                \"action_list_operator\",\n                assets_to_validate=[batch],\n                session_id=session_id,\n            )\n```\n\n----------------------------------------\n\nTITLE: Multi-Node Airflow DAG Configuration\nDESCRIPTION: Example DAG configuration for running multiple Kedro nodes within a single Kubernetes container, showing task dependencies and execution order.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/airflow.md#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator\n\nwith DAG(...) as dag:\n    tasks = {\n        \"preprocess-companies-and-shuttles\": KubernetesPodOperator(\n            task_id=\"preprocess-companies-and-shuttles\",\n            name=\"preprocess-companies-and-shuttles\",\n            namespace=NAMESPACE,\n            image=DOCKER_IMAGE,\n            cmds=[\"kedro\"],\n            arguments=[\"run\", \"--nodes=preprocess-companies-node,preprocess-shuttles-node\"],\n            ...\n        ),\n        \"create-model-input-table-node\": KubernetesPodOperator(...),\n        ...\n    }\n\n    tasks[\"preprocess-companies-and-shuttles\"] >> tasks[\"create-model-input-table-node\"]\n    tasks[\"create-model-input-table-node\"] >> tasks[\"split-data-node\"]\n    ...\n```\n\n----------------------------------------\n\nTITLE: Updating CLI run Function for AWSBatchRunner in Python\nDESCRIPTION: This function updates the CLI implementation to correctly instantiate the runner class with the necessary parameters.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/aws_batch.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef run(tag, env, ...):\n    \"\"\"Run the pipeline.\"\"\"\n    runner = runner or \"SequentialRunner\"\n\n    tag = _get_values_as_tuple(tag) if tag else tag\n    node_names = _get_values_as_tuple(node_names) if node_names else node_names\n\n    with KedroSession.create(env=env, extra_params=params) as session:\n        context = session.load_context()\n        runner_instance = _instantiate_runner(runner, is_async, context)\n        session.run(\n            tags=tag,\n            runner=runner_instance,\n            node_names=node_names,\n            from_nodes=from_nodes,\n            to_nodes=to_nodes,\n            from_inputs=from_inputs,\n            to_outputs=to_outputs,\n            load_versions=load_version,\n            pipeline_name=pipeline,\n        )\n```\n\n----------------------------------------\n\nTITLE: Converting Kedro Pipeline to Prefect 2.0 Flow\nDESCRIPTION: A Python script that converts a Kedro pipeline into a Prefect flow and registers it with the Prefect API. It handles Kedro session initialization, node execution, and workflow orchestration.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/prefect.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# <project_root>/register_prefect_flow.py\nimport click\nfrom pathlib import Path\nfrom typing import Dict, List, Union, Callable\n\nfrom kedro.framework.hooks.manager import _create_hook_manager\nfrom kedro.framework.project import pipelines\nfrom kedro.framework.session import KedroSession\nfrom kedro.framework.startup import bootstrap_project\nfrom kedro.io import DataCatalog, MemoryDataset\nfrom kedro.pipeline.node import Node\nfrom kedro.runner import run_node\n\nfrom prefect import flow, task, get_run_logger\nfrom prefect.deployments import Deployment\n\n\n@click.command()\n@click.option(\"-p\", \"--pipeline\", \"pipeline_name\", default=\"__default__\")\n@click.option(\"--env\", \"-e\", type=str, default=\"base\")\n@click.option(\"--deployment_name\", \"deployment_name\", default=\"example\")\n@click.option(\"--work_pool_name\", \"work_pool_name\", default=\"default\")\n@click.option(\"--work_queue_name\", \"work_queue_name\", default=\"default\")\n@click.option(\"--version\", \"version\", default=\"1.0\")\ndef prefect_deploy(\n    pipeline_name, env, deployment_name, work_pool_name, work_queue_name, version\n):\n    \"\"\"Register a Kedro pipeline as a Prefect flow.\"\"\"\n\n    # Pipeline name to execute\n    pipeline_name = pipeline_name or \"__default__\"\n\n    # Use standard deployment configuration for local execution. If you require a different\n    # infrastructure, check the API docs for Deployments at: https://docs.prefect.io/latest/api-ref/prefect/deployments/\n    deployment = Deployment.build_from_flow(\n        flow=my_flow,\n        name=deployment_name,\n        path=str(Path.cwd()),\n        version=version,\n        parameters={\n            \"pipeline_name\": pipeline_name,\n            \"env\": env,\n        },\n        infra_overrides={\"env\": {\"PREFECT_LOGGING_LEVEL\": \"DEBUG\"}},\n        work_pool_name=work_pool_name,\n        work_queue_name=work_queue_name,\n    )\n\n    deployment.apply()\n\n\n@flow(name=\"my_flow\")\ndef my_flow(pipeline_name: str, env: str):\n    logger = get_run_logger()\n    project_path = Path.cwd()\n\n    metadata = bootstrap_project(project_path)\n    logger.info(\"Project name: %s\", metadata.project_name)\n\n    logger.info(\"Initializing Kedro...\")\n    execution_config = kedro_init(\n        pipeline_name=pipeline_name, project_path=project_path, env=env\n    )\n\n    logger.info(\"Building execution layers...\")\n    execution_layers = init_kedro_tasks_by_execution_layer(\n        pipeline_name, execution_config\n    )\n\n    for layer in execution_layers:\n        logger.info(\"Running layer...\")\n        for node_task in layer:\n            logger.info(\"Running node...\")\n            node_task()\n\n\n@task()\ndef kedro_init(\n    pipeline_name: str,\n    project_path: Path,\n    env: str,\n):\n    \"\"\"\n    Initializes a Kedro session and returns the DataCatalog and\n    KedroSession\n    \"\"\"\n    # bootstrap project within task / flow scope\n\n    logger = get_run_logger()\n    logger.info(\"Bootstrapping project\")\n    bootstrap_project(project_path)\n\n    session = KedroSession.create(\n        project_path=project_path,\n        env=env,\n    )\n    # Note that for logging inside a Prefect task logger is used.\n    logger.info(\"Session created with ID %s\", session.session_id)\n    pipeline = pipelines.get(pipeline_name)\n    logger.info(\"Loading context...\")\n    context = session.load_context()\n    catalog = context.catalog\n    logger.info(\"Registering datasets...\")\n    unregistered_ds = pipeline.datasets() - set(catalog.list())\n    for ds_name in unregistered_ds:\n        catalog.add(ds_name, MemoryDataset())\n    return {\"catalog\": catalog, \"sess_id\": session.session_id}\n\n\ndef init_kedro_tasks_by_execution_layer(\n    pipeline_name: str,\n    execution_config: Union[None, Dict[str, Union[DataCatalog, str]]] = None,\n) -> List[List[Callable]]:\n    \"\"\"\n    Inits the Kedro tasks ordered topologically in groups, which implies that an earlier group\n    is the dependency of later one.\n\n    Args:\n        pipeline_name (str): The pipeline name to execute\n        execution_config (Union[None, Dict[str, Union[DataCatalog, str]]], optional):\n        The required execution config for each node. Defaults to None.\n\n    Returns:\n        List[List[Callable]]: A list of topologically ordered task groups\n    \"\"\"\n\n    pipeline = pipelines.get(pipeline_name)\n\n    execution_layers = []\n\n    # Return a list of the pipeline nodes in topologically ordered groups,\n    #  i.e. if node A needs to be run before node B, it will appear in an\n    #  earlier group.\n    for layer in pipeline.grouped_nodes:\n        execution_layer = []\n        for node in layer:\n            # Use a function for task instantiation which avoids duplication of\n            # tasks\n            task = instantiate_task(node, execution_config)\n            execution_layer.append(task)\n        execution_layers.append(execution_layer)\n\n    return execution_layers\n\n\ndef kedro_task(\n    node: Node, task_dict: Union[None, Dict[str, Union[DataCatalog, str]]] = None\n):\n    run_node(\n        node,\n        task_dict[\"catalog\"],\n        _create_hook_manager(),\n        task_dict[\"sess_id\"],\n    )\n\n\ndef instantiate_task(\n    node: Node,\n    execution_config: Union[None, Dict[str, Union[DataCatalog, str]]] = None,\n) -> Callable:\n    \"\"\"\n    Function that wraps a Node inside a task for future execution\n\n    Args:\n        node: Kedro node for which a Prefect task is being created.\n        execution_config: The configurations required for the node to execute\n        that includes catalogs and session id\n\n    Returns: Prefect task for the passed node\n\n    \"\"\"\n    return task(lambda: kedro_task(node, execution_config)).with_options(name=node.name)\n\n\nif __name__ == \"__main__\":\n    prefect_deploy()\n```\n\n----------------------------------------\n\nTITLE: Implementing Base Data Science Pipeline\nDESCRIPTION: Demonstrates how to create a base data science pipeline that can be reused with different parameters.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/namespaces.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.pipeline import Pipeline, node, pipeline\nfrom .nodes import evaluate_model, split_data, train_model\n\nbase_data_science = pipeline(\n        [\n            node(\n                func=split_data,\n                inputs=[\"model_input_table\", \"params:model_options\"],\n                outputs=[\"X_train\", \"X_test\", \"y_train\", \"y_test\"],\n                name=\"split_data_node\",\n            ),\n            node(\n                func=train_model,\n                inputs=[\"X_train\", \"y_train\"],\n                outputs=\"regressor\",\n                name=\"train_model_node\",\n            ),\n            node(\n                func=evaluate_model,\n                inputs=[\"regressor\", \"X_test\", \"y_test\"],\n                outputs=None,\n                name=\"evaluate_model_node\",\n            ),\n        ]\n    )\n\ndef create_pipeline(**kwargs) -> Pipeline:\n    return pipeline(\n        [base_data_science],\n        parameters={\"params:model_options\": \"params:model_options_1\"},\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing the Save Method with fsspec in ImageDataset\nDESCRIPTION: Implementation of the save method for ImageDataset using fsspec and Pillow. The method converts a numpy array to an image and saves it to the specified filepath.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/how_to_create_a_custom_dataset.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass ImageDataset(AbstractDataset[np.ndarray, np.ndarray]):\n    def save(self, data: np.ndarray) -> None:\n        \"\"\"Saves image data to the specified filepath.\"\"\"\n        # using get_filepath_str ensures that the protocol and path are appended correctly for different filesystems\n        save_path = get_filepath_str(self._filepath, self._protocol)\n        with self._fs.open(save_path, \"wb\") as f:\n            image = Image.fromarray(data)\n            image.save(f)\n```\n\n----------------------------------------\n\nTITLE: Using Transcoded Datasets in Kedro Pipeline\nDESCRIPTION: This Python code shows how to use transcoded datasets in a Kedro pipeline. It demonstrates defining nodes that use the same dataset in different formats (Spark and pandas) within the pipeline.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog_yaml_examples.md#2025-04-19_snippet_22\n\nLANGUAGE: python\nCODE:\n```\npipeline(\n    [\n        node(name=\"my_func1_node\", func=my_func1, inputs=\"spark_input\", outputs=\"my_dataframe@spark\"),\n        node(name=\"my_func2_node\", func=my_func2, inputs=\"my_dataframe@pandas\", outputs=\"pipeline_output\"),\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Complete Pipeline Assembly\nDESCRIPTION: Full implementation of pipeline creation showing node assembly, tagging, and optional parameters\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/modular_pipelines.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.pipeline import Pipeline, pipeline, node\n\nfrom .nodes import mean, mean_sos, variance\n\ndef create_pipeline(**kwargs) -> Pipeline:\n    return pipeline(\n        [\n            node(len, \"xs\", \"n\"),\n            node(mean, [\"xs\", \"n\"], \"m\", name=\"mean_node\", tags=\"tag1\"),\n            node(mean_sos, [\"xs\", \"n\"], \"m2\", name=\"mean_sos\", tags=[\"tag1\", \"tag2\"]),\n            node(variance, [\"m\", \"m2\"], \"v\", name=\"variance_node\"),\n        ],\n        tags=\"tag3\",\n        namespace=\"\",\n        inputs={},\n        outputs={},\n        parameters={},\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring pytest-cov in pyproject.toml\nDESCRIPTION: Configuration for pytest to generate a coverage report using pytest-cov, added to the project's pyproject.toml file.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/automated_testing.md#2025-04-19_snippet_8\n\nLANGUAGE: toml\nCODE:\n```\n[tool.pytest.ini_options]\naddopts = \"\"\"\n--cov-report term-missing \\\n--cov src/<package_name> -ra\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-sheet Excel Dataset in Kedro\nDESCRIPTION: Example showing how to load a multi-sheet Excel file from a local file system by specifying multiple sheet names in Kedro's catalog.yml.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog_yaml_examples.md#2025-04-19_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\ntrains:\n  type: pandas.ExcelDataset\n  filepath: data/02_intermediate/company/trains.xlsx\n  load_args:\n    sheet_name: [Sheet1, Sheet2, Sheet3]\n```\n\n----------------------------------------\n\nTITLE: Reading Configuration without a Kedro Project in Python\nDESCRIPTION: Demonstrates how to use OmegaConfigLoader to read configuration without a full Kedro project structure.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/advanced_configuration.md#2025-04-19_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.config import OmegaConfigLoader\nconfig_loader = OmegaConfigLoader(conf_source=\".\")\n\n# Optionally, you can also use environments\n# config_loader = OmegaConfigLoader(conf_source=\".\", base_env=\"base\", default_run_env=\"local\")\n\nprint(config_loader[\"parameters\"])\n```\n\n----------------------------------------\n\nTITLE: Basic Pipeline Template Structure\nDESCRIPTION: Initial template code for pipeline.py showing the basic structure for creating a pipeline\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/modular_pipelines.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.pipeline import Pipeline, pipeline\n\ndef create_pipeline(**kwargs) -> Pipeline:\n    return pipeline([])\n```\n\n----------------------------------------\n\nTITLE: Kedro Pipeline AWS Deployment Script\nDESCRIPTION: Python script using AWS CDK to deploy Kedro pipeline as Step Functions state machine with Lambda functions\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/aws_step_functions.md#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport re\nfrom pathlib import Path\n\nfrom aws_cdk import aws_stepfunctions as sfn\nfrom aws_cdk import aws_s3 as s3\nfrom aws_cdk import core, aws_lambda, aws_ecr\nfrom aws_cdk.aws_lambda import IFunction\nfrom aws_cdk.aws_stepfunctions_tasks import LambdaInvoke\nfrom kedro.framework.project import pipelines\nfrom kedro.framework.session import KedroSession\nfrom kedro.framework.startup import bootstrap_project\nfrom kedro.pipeline.node import Node\n\n\ndef _clean_name(name: str) -> str:\n    \"\"\"Reformat a name to be compliant with AWS requirements for their resources.\n\n    Returns:\n        name: formatted name.\n    \"\"\"\n    return re.sub(r\"[\\W_]+\", \"-\", name).strip(\"-\")[:63]\n\n\nclass KedroStepFunctionsStack(core.Stack):\n    \"\"\"A CDK Stack to deploy a Kedro pipeline to AWS Step Functions.\"\"\"\n\n    env = \"aws\"\n    project_path = Path.cwd()\n    erc_repository_name = project_path.name\n    s3_data_bucket_name = (\n        \"spaceflights-step-functions\"  # this is where the raw data is located\n    )\n\n    def __init__(self, scope: core.Construct, id: str, **kwargs) -> None:\n        super().__init__(scope, id, **kwargs)\n\n        self._parse_kedro_pipeline()\n        self._set_ecr_repository()\n        self._set_ecr_image()\n        self._set_s3_data_bucket()\n        self._convert_kedro_pipeline_to_step_functions_state_machine()\n\n    def _parse_kedro_pipeline(self) -> None:\n        \"\"\"Extract the Kedro pipeline from the project\"\"\"\n        metadata = bootstrap_project(self.project_path)\n\n        self.project_name = metadata.project_name\n        self.pipeline = pipelines.get(\"__default__\")\n\n    def _set_ecr_repository(self) -> None:\n        \"\"\"Set the ECR repository for the Lambda base image\"\"\"\n        self.ecr_repository = aws_ecr.Repository.from_repository_name(\n            self, id=\"ECR\", repository_name=self.erc_repository_name\n        )\n\n    def _set_ecr_image(self) -> None:\n        \"\"\"Set the Lambda base image\"\"\"\n        self.ecr_image = aws_lambda.EcrImageCode(repository=self.ecr_repository)\n\n    def _set_s3_data_bucket(self) -> None:\n        \"\"\"Set the S3 bucket containing the raw data\"\"\"\n        self.s3_bucket = s3.Bucket(\n            self, \"RawDataBucket\", bucket_name=self.s3_data_bucket_name\n        )\n\n    def _convert_kedro_node_to_lambda_function(self, node: Node) -> IFunction:\n        \"\"\"Convert a Kedro node into an AWS Lambda function\"\"\"\n        func = aws_lambda.Function(\n            self,\n            id=_clean_name(f\"{node.name}_fn\"),\n            description=str(node),\n            code=self.ecr_image,\n            handler=aws_lambda.Handler.FROM_IMAGE,\n            runtime=aws_lambda.Runtime.FROM_IMAGE,\n            environment={},\n            function_name=_clean_name(node.name),\n            memory_size=256,\n            reserved_concurrent_executions=10,\n            timeout=core.Duration.seconds(15 * 60),\n        )\n        self.s3_bucket.grant_read_write(func)\n        return func\n\n    def _convert_kedro_node_to_sfn_task(self, node: Node) -> LambdaInvoke:\n        \"\"\"Convert a Kedro node into an AWS Step Functions Task\"\"\"\n        return LambdaInvoke(\n            self,\n            _clean_name(node.name),\n            lambda_function=self._convert_kedro_node_to_lambda_function(node),\n            payload=sfn.TaskInput.from_object({\"node_name\": node.name}),\n        )\n\n    def _convert_kedro_pipeline_to_step_functions_state_machine(self) -> None:\n        \"\"\"Convert Kedro pipeline into an AWS Step Functions State Machine\"\"\"\n        definition = sfn.Pass(self, \"Start\")\n\n        for i, group in enumerate(self.pipeline.grouped_nodes, 1):\n            group_name = f\"Group {i}\"\n            sfn_state = sfn.Parallel(self, group_name)\n            for node in group:\n                sfn_task = self._convert_kedro_node_to_sfn_task(node)\n                sfn_state.branch(sfn_task)\n\n            definition = definition.next(sfn_state)\n\n        sfn.StateMachine(\n            self,\n            self.project_name,\n            definition=definition,\n            timeout=core.Duration.seconds(5 * 60),\n        )\n\n\napp = core.App()\nKedroStepFunctionsStack(app, \"KedroStepFunctionsStack\")\napp.synth()\n```\n\n----------------------------------------\n\nTITLE: Implementing Nested Namespaces in Kedro Pipeline\nDESCRIPTION: Demonstrates how to create nested namespaces by combining pipeline-level and node-level namespacing. The first two nodes are grouped under a 'preprocessing' namespace within the main 'data_processing' namespace.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/namespaces.md#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.pipeline import Pipeline, node, pipeline\nfrom .nodes import create_model_input_table, preprocess_companies, preprocess_shuttles\n\ndef create_pipeline(**kwargs) -> Pipeline:\n    return pipeline(\n        [\n            node(\n                func=preprocess_companies,\n                inputs=\"companies\",\n                outputs=\"preprocessed_companies\",\n                name=\"preprocess_companies_node\",\n                namespace=\"preprocessing\",\n            ),\n            node(\n                func=preprocess_shuttles,\n                inputs=\"shuttles\",\n                outputs=\"preprocessed_shuttles\",\n                name=\"preprocess_shuttles_node\",\n                namespace=\"preprocessing\",\n            ),\n            node(\n                func=create_model_input_table,\n                inputs=[\"preprocessed_shuttles\", \"preprocessed_companies\", \"reviews\"],\n                outputs=\"model_input_table\",\n                name=\"create_model_input_table_node\",\n            ),\n        ],\n        namespace=\"data_processing\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing a Basic Kedro Plugin in Python\nDESCRIPTION: A simple Kedro plugin example that prints a pipeline as JSON. This code creates a command group and defines a to_json command that displays the default pipeline in JSON format.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/extend_kedro/plugins.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport click\nfrom kedro.framework.project import pipelines\n\n\n@click.group(name=\"JSON\")\ndef commands():\n    pass\n\n\n@commands.command(name=\"to_json\")\n@click.pass_obj\ndef to_json(metadata):\n    \"\"\"Display the pipeline in JSON format\"\"\"\n    pipeline = pipelines[\"__default__\"]\n    print(pipeline.to_json())\n```\n\n----------------------------------------\n\nTITLE: Implementing Memory Profiling Hooks in Kedro\nDESCRIPTION: Implementation of before_dataset_loaded and after_dataset_loaded hooks to track memory consumption during dataset loading using memory_profiler.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/examples.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# src/<package_name>/hooks.py\nimport logging\n\nfrom kedro.framework.hooks import hook_impl\nfrom memory_profiler import memory_usage\n\n\ndef _normalise_mem_usage(mem_usage):\n    # memory_profiler < 0.56.0 returns list instead of float\n    return mem_usage[0] if isinstance(mem_usage, (list, tuple)) else mem_usage\n\n\nclass MemoryProfilingHooks:\n    def __init__(self):\n        self._mem_usage = {}\n\n    @hook_impl\n    def before_dataset_loaded(self, dataset_name: str) -> None:\n        before_mem_usage = memory_usage(\n            -1,\n            interval=0.1,\n            max_usage=True,\n            retval=True,\n            include_children=True,\n        )\n        before_mem_usage = _normalise_mem_usage(before_mem_usage)\n        self._mem_usage[dataset_name] = before_mem_usage\n\n    @hook_impl\n    def after_dataset_loaded(self, dataset_name: str) -> None:\n        after_mem_usage = memory_usage(\n            -1,\n            interval=0.1,\n            max_usage=True,\n            retval=True,\n            include_children=True,\n        )\n        # memory_profiler < 0.56.0 returns list instead of float\n        after_mem_usage = _normalise_mem_usage(after_mem_usage)\n\n        logging.getLogger(__name__).info(\n            \"Loading %s consumed %2.2fMiB memory\",\n            dataset_name,\n            after_mem_usage - self._mem_usage[dataset_name],\n        )\n```\n\n----------------------------------------\n\nTITLE: Defining a Full Pipeline in Python\nDESCRIPTION: This snippet defines a full pipeline with four nodes that compute the variance of a set of numbers. It includes functions for mean, mean of squares, and variance calculations.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/slice_a_pipeline.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef mean(xs, n):\n    return sum(xs) / n\n\n\ndef mean_sos(xs, n):\n    return sum(x**2 for x in xs) / n\n\n\ndef variance(m, m2):\n    return m2 - m * m\n\n\nfull_pipeline = pipeline(\n    [\n        node(len, \"xs\", \"n\"),\n        node(mean, [\"xs\", \"n\"], \"m\", name=\"mean_node\", tags=\"mean\"),\n        node(mean_sos, [\"xs\", \"n\"], \"m2\", name=\"mean_sos\", tags=[\"mean\", \"variance\"]),\n        node(variance, [\"m\", \"m2\"], \"v\", name=\"variance_node\", tags=\"variance\"),\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Configuration Loader in Python\nDESCRIPTION: This snippet shows how to create a custom configuration loader by extending the AbstractConfigLoader class. It includes the basic structure with initialization parameters that match the parent class.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/advanced_configuration.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.config import AbstractConfigLoader\n\n\nclass CustomConfigLoader(AbstractConfigLoader):\n    def __init__(\n        self,\n        conf_source: str,\n        env: str = None,\n        runtime_params: Dict[str, Any] = None,\n    ):\n        super().__init__(\n            conf_source=conf_source, env=env, runtime_params=runtime_params\n        )\n\n        # Custom implementation\n```\n\n----------------------------------------\n\nTITLE: Running Sliced Pipeline to Nodes using Kedro CLI\nDESCRIPTION: These bash commands show how to run a sliced pipeline to specified nodes using the Kedro CLI, including examples with multiple start and end nodes.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/slice_a_pipeline.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --to-nodes=mean_node\n\nkedro run --from-nodes=A --to-nodes=Z\n\nkedro run --from-nodes=A,D --to-nodes=X,Y,Z\n```\n\n----------------------------------------\n\nTITLE: Adding Behavior to Nodes by Tag using Kedro Hooks\nDESCRIPTION: Implementation of a hook that adds custom behavior to nodes based on their tags. The hook applies the say_hello function only to nodes that have the 'hello' tag.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/common_use_cases.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# src/<package_name>/hooks.py\n\nfrom kedro.framework.hooks import hook_impl\nfrom kedro.pipeline.node import Node\n\n\nclass ProjectHooks:\n    @hook_impl\n    def before_node_run(self, node: Node):\n        if \"hello\" in node.tags:\n            say_hello(node)\n```\n\n----------------------------------------\n\nTITLE: Configuring Data Catalog with Various Datasets in Python\nDESCRIPTION: This snippet demonstrates how to programmatically configure a DataCatalog object with different types of datasets, including CSV, SQL, and Parquet. It uses pre-built data loaders from kedro-datasets.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/advanced_data_catalog_usage.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.io import DataCatalog\nfrom kedro_datasets.pandas import (\n    CSVDataset,\n    SQLTableDataset,\n    SQLQueryDataset,\n    ParquetDataset,\n)\n\ncatalog =  DataCatalog(\n    {\n        \"bikes\": CSVDataset(filepath=\"../data/01_raw/bikes.csv\"),\n        \"cars\": CSVDataset(filepath=\"../data/01_raw/cars.csv\", load_args=dict(sep=\",\")),\n        \"cars_table\": SQLTableDataset(\n            table_name=\"cars\", credentials=dict(con=\"sqlite:///kedro.db\")\n        ),\n        \"scooters_query\": SQLQueryDataset(\n            sql=\"select * from cars where gear=4\",\n            credentials=dict(con=\"sqlite:///kedro.db\"),\n        ),\n        \"ranked\": ParquetDataset(filepath=\"ranked.parquet\"),\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Debugging Nodes with PDB using Kedro Hooks\nDESCRIPTION: Implementation of a hook that activates the Python debugger (PDB) when an error occurs in a node. The hook uses the 'on_node_error' hook point to start a post-mortem debugging session with the traceback information.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/common_use_cases.md#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport pdb\nimport sys\nimport traceback\n\nfrom kedro.framework.hooks import hook_impl\n\n\nclass PDBNodeDebugHook:\n    \"\"\"A hook class for creating a post mortem debugging with the PDB debugger\n    whenever an error is triggered within a node. The local scope from when the\n    exception occured is available within this debugging session.\n    \"\"\"\n\n    @hook_impl\n    def on_node_error(self):\n        _, _, traceback_object = sys.exc_info()\n\n        #  Print the traceback information for debugging ease\n        traceback.print_tb(traceback_object)\n\n        # Drop you into a post mortem debugging session\n        pdb.post_mortem(traceback_object)\n```\n\n----------------------------------------\n\nTITLE: Slicing Pipeline by Specifying Nodes to Run in Python\nDESCRIPTION: This snippet shows how to create a sliced pipeline by specifying which nodes to run. It demonstrates slicing to include only 'mean_node' and 'mean_sos'.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/slice_a_pipeline.md#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprint(full_pipeline.only_nodes(\"mean_node\", \"mean_sos\").describe())\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom PyIceberg Dataset for Kedro\nDESCRIPTION: Complete implementation of a custom Kedro dataset that wraps PyIceberg functionality, allowing for reading and writing Pandas DataFrames to Iceberg tables with versioning support.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/iceberg_versioning.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\nfrom kedro.io.core import AbstractDataset, DatasetError\nfrom pyiceberg.catalog import load_catalog\nfrom pyiceberg.exceptions import NoSuchTableError\n\nDEFAULT_LOAD_ARGS = {\"load_version\": None}\nDEFAULT_SAVE_ARGS = {\"mode\": \"overwrite\"}\n\nclass PyIcebergDataset(AbstractDataset):\n    def __init__(\n            self,\n            catalog,\n            namespace,\n            table_name,\n            load_args=DEFAULT_LOAD_ARGS,\n            scan_args=None,\n            save_args=DEFAULT_SAVE_ARGS,\n    ):\n        self.table_name = table_name\n        self.namespace = namespace\n        self.catalog = load_catalog(catalog)\n        self.load_args = load_args\n        self.table = self._load_table(namespace, table_name)\n        self.save_args = save_args\n        self.scan_args = scan_args\n\n\n    def load(self):\n        self.table = self.catalog.load_table((self.namespace, self.table_name))\n        if self.scan_args:\n            scan = self.table.scan(**self.scan_args)\n        else:\n            scan = self.table.scan()\n        return scan.to_pandas()\n\n    def _load_table(self, namespace, table_name):\n        try:\n            return self.catalog.load_table((namespace, table_name))\n        except NoSuchTableError:\n            return None\n\n    def save(self, data) -> None:\n        arrow = pa.Table.from_pandas(data)\n        if not self.table:\n            self.catalog.create_namespace_if_not_exists(self.namespace)\n            self.table = self.catalog.create_table((self.namespace, self.table_name), schema=arrow.schema)\n        if self.save_args.get(\"mode\") == \"overwrite\":\n            self.table.overwrite(arrow)\n        elif self.save_args.get(\"mode\") == \"append\":\n            self.table.append(arrow)\n        else:\n            raise DatasetError(\"Mode not supported\")\n\n    def _describe(self) -> dict:\n        return {}\n\n    def exists(self):\n        return self.catalog.table_exists((self.namespace, self.table_name))\n\n    def inspect(self):\n        return self.table.inspect\n```\n\n----------------------------------------\n\nTITLE: Customizing Dataset Load with Performance Logging using Kedro Hooks\nDESCRIPTION: Implementation of hooks that add logging of dataset load times. The hook uses 'before_dataset_loaded' and 'after_dataset_loaded' to track and log how long it takes to load each dataset.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/common_use_cases.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport time\nfrom typing import Any\n\nfrom kedro.framework.hooks import hook_impl\nfrom kedro.pipeline.node import Node\n\n\nclass LoggingHook:\n    \"\"\"A hook that logs how many time it takes to load each dataset.\"\"\"\n\n    def __init__(self):\n        self._timers = {}\n\n    @property\n    def _logger(self):\n        return logging.getLogger(__name__)\n\n    @hook_impl\n    def before_dataset_loaded(self, dataset_name: str, node: Node) -> None:\n        start = time.time()\n        self._timers[dataset_name] = start\n\n    @hook_impl\n    def after_dataset_loaded(self, dataset_name: str, data: Any, node: Node) -> None:\n        start = self._timers[dataset_name]\n        end = time.time()\n        self._logger.info(\n            \"Loading dataset %s before node '%s' takes %0.2f seconds\",\n            dataset_name,\n            node.name,\n            end - start,\n        )\n```\n\n----------------------------------------\n\nTITLE: Implementing Data Split Node Function in Python\nDESCRIPTION: Node function that splits data into training and test sets. Takes a pandas DataFrame and parameters dictionary as input to split data into features and target sets.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/test_a_project.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef split_data(data: pd.DataFrame, parameters: dict[str, Any]) -> Tuple:\n    \"\"\"Splits data into features and targets training and test sets.\n\n    Args:\n        data: Data containing features and target.\n        parameters: Parameters defined in parameters_data_science.yml.\n    Returns:\n        Split data.\n    \"\"\"\n    X = data[parameters[\"features\"]]\n    y = data[\"price\"]\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=parameters[\"test_size\"], random_state=parameters[\"random_state\"]\n    )\n    return X_train, X_test, y_train, y_test\n```\n\n----------------------------------------\n\nTITLE: Loading Credentials from Environment Variables in YAML\nDESCRIPTION: Demonstrates how to use the oc.env resolver to load credentials from environment variables in a credentials.yml file.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/advanced_configuration.md#2025-04-19_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\ndev_s3:\n  client_kwargs:\n    aws_access_key_id: ${oc.env:AWS_ACCESS_KEY_ID}\n    aws_secret_access_key: ${oc.env:AWS_SECRET_ACCESS_KEY}\n```\n\n----------------------------------------\n\nTITLE: Specifying Runtime Parameters via Kedro CLI\nDESCRIPTION: Shows how to pass runtime parameters through the Kedro CLI to override configuration values.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/advanced_configuration.md#2025-04-19_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --params random=3\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Functions for Company and Shuttle Data in Python\nDESCRIPTION: Functions that preprocess company and shuttle datasets by applying the utility conversion functions to specific columns. These functions handle boolean conversions, percentage parsing, and monetary value standardization within pandas DataFrames.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook.ipynb#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef preprocess_companies(companies: pd.DataFrame) -> pd.DataFrame:\n    companies[\"iata_approved\"] = _is_true(companies[\"iata_approved\"])\n    companies[\"company_rating\"] = _parse_percentage(companies[\"company_rating\"])\n    return companies\n\n\ndef preprocess_shuttles(shuttles: pd.DataFrame) -> pd.DataFrame:\n    shuttles[\"d_check_complete\"] = _is_true(shuttles[\"d_check_complete\"])\n    shuttles[\"moon_clearance_complete\"] = _is_true(shuttles[\"moon_clearance_complete\"])\n    shuttles[\"price\"] = _parse_money(shuttles[\"price\"])\n    return shuttles\n```\n\n----------------------------------------\n\nTITLE: Modifying Node Inputs with before_node_run Hook in Kedro (Python)\nDESCRIPTION: This code snippet demonstrates how to use the 'before_node_run' hook to modify node inputs in a Kedro pipeline. It shows an example of replacing the 'first_input' for a specific node named 'my_node' with its file path instead of the actual data.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/examples.md#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# src/<package_name>/hooks.py\nfrom typing import Any, Dict, Optional\n\nfrom kedro.framework.hooks import hook_impl\nfrom kedro.pipeline.node import Node\nfrom kedro.io import DataCatalog\n\n\nclass NodeInputReplacementHook:\n    @hook_impl\n    def before_node_run(\n        self, node: Node, catalog: DataCatalog\n    ) -> dict[str, Any] | None:\n        \"\"\"Replace `first_input` for `my_node`\"\"\"\n        if node.name == \"my_node\":\n            # return the string filepath to the `first_input` dataset\n            # instead of the underlying data\n            dataset_name = \"first_input\"\n            filepath = catalog._get_dataset(dataset_name)._filepath\n            return {\"first_input\": filepath}  # `second_input` is not affected\n        return None\n```\n\n----------------------------------------\n\nTITLE: Comparing DVC Parameter Changes Between Runs\nDESCRIPTION: Shows the output of 'dvc params diff' command that compares parameter values between different runs. Displays changes in model features, random state, and test size parameters from the data science configuration file.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/kedro_dvc_versioning.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nPath                                   Param                       HEAD    workspace\nconf/base/parameters_data_science.yml  model_options.features      -       ['engines', 'passenger_capacity', 'crew', 'd_check_complete', 'moon_clearance_complete', 'iata_approved', 'company_rating', 'review_scores_rating']\nconf/base/parameters_data_science.yml  model_options.random_state  -       3\nconf/base/parameters_data_science.yml  model_options.test_size     -       0.2\n```\n\n----------------------------------------\n\nTITLE: Defining Global Variables in YAML for Kedro OmegaConfigLoader\nDESCRIPTION: Example of a globals.yml file containing shared variables for use across different configuration types and environments in a Kedro project.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/config_loader_migration.md#2025-04-19_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nbucket_name: \"my_s3_bucket\"\nkey_prefix: \"my/key/prefix/\"\n\ndatasets:\n    csv: \"pandas.CSVDataset\"\n    spark: \"spark.SparkDataset\"\n\nfolders:\n    raw: \"01_raw\"\n    int: \"02_intermediate\"\n    pri: \"03_primary\"\n    fea: \"04_feature\"\n```\n\n----------------------------------------\n\nTITLE: Running Namespaced Kedro Pipelines\nDESCRIPTION: Commands to execute namespaced pipelines in Kedro, showing how to run either the entire pipeline or specific namespaced sections.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/namespaces.md#2025-04-19_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --namespace=data_processing\nkedro run --namespace=data_processing.preprocessing\n```\n\n----------------------------------------\n\nTITLE: Printing KedroDataCatalog and Individual Datasets in Python\nDESCRIPTION: Demonstrates how to print the entire catalog or individual datasets in an interactive environment like IPython or JupyterLab.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/kedro_data_catalog.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nIn [1]: catalog\nOut[1]: {'shuttles': kedro_datasets.pandas.excel_dataset.ExcelDataset(filepath=PurePosixPath('/data/01_raw/shuttles.xlsx'), protocol='file', load_args={'engine': 'openpyxl'}, save_args={'index': False}, writer_args={'engine': 'openpyxl'}), 'preprocessed_companies': kedro_datasets.pandas.parquet_dataset.ParquetDataset(filepath=PurePosixPath('/data/02_intermediate/preprocessed_companies.pq'), protocol='file', load_args={}, save_args={}), 'params:model_options.test_size': kedro.io.memory_dataset.MemoryDataset(data='<float>'), 'params:model_options.features': kedro.io.memory_dataset.MemoryDataset(data='<list>')}\n\nIn [2]: catalog[\"shuttles\"]\nOut[2]: kedro_datasets.pandas.excel_dataset.ExcelDataset(filepath=PurePosixPath('/data/01_raw/shuttles.xlsx'), protocol='file', load_args={'engine': 'openpyxl'}, save_args={'index': False}, writer_args={'engine': 'openpyxl'})\n```\n\n----------------------------------------\n\nTITLE: Delta Table catalog configuration with Pandas\nDESCRIPTION: YAML configuration for defining a Delta Table dataset in the Kedro catalog using the pandas.DeltaTableDataset type with overwrite mode.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/deltalake_versioning.md#2025-04-19_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_input_table:\n  type: pandas.DeltaTableDataset\n  filepath: data/03_primary/model_input_table\n  save_args:\n    mode: overwrite\n```\n\n----------------------------------------\n\nTITLE: Applying a Decorator to Node Functions with Tags using Kedro Hooks\nDESCRIPTION: Implementation of a hook that applies the retry decorator from tenacity to node functions that have the 'flaky' tag. This dynamically modifies the node's function at runtime.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/common_use_cases.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# src/<package_name>/hooks.py\nfrom tenacity import retry\n\nfrom kedro.framework.hooks import hook_impl\nfrom kedro.pipeline.node import Node\n\n\nclass ProjectHooks:\n    @hook_impl\n    def before_node_run(self, node: Node):\n        # adding retrying behaviour to nodes tagged as flaky\n        if \"flaky\" in node.tags:\n            node.func = retry(node.func)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Resolvers in Python\nDESCRIPTION: Demonstrates how to define custom resolvers for OmegaConfigLoader in a Kedro project's settings.py file.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/advanced_configuration.md#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport polars as pl\nfrom datetime import date\n\ndef date_today():\n    return date.today()\n\nCONFIG_LOADER_ARGS = {\n    \"custom_resolvers\": {\n        \"add\": lambda *my_list: sum(my_list),\n        \"polars\": lambda x: getattr(pl, x),\n        \"today\": date_today,\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Custom MLflow Hook Implementation\nDESCRIPTION: Python implementation of a custom Hook to track additional metadata in MLflow\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/mlflow.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# src/spaceflights_mlflow/hooks.py\n\nimport typing as t\nimport logging\n\nimport mlflow\nfrom kedro.framework.hooks import hook_impl\n\nlogger = logging.getLogger(__name__)\n\n\nclass ExtraMLflowHooks:\n    @hook_impl\n    def before_pipeline_run(self, run_params: dict[str, t.Any]):\n        logger.info(\"Logging extra metadata to MLflow\")\n        mlflow.set_tags({\n            \"pipeline\": run_params[\"pipeline_name\"] or \"__default__\",\n            \"custom_version\": \"0.1.0\",\n        })\n```\n\n----------------------------------------\n\nTITLE: Skeleton Structure for Custom ImageDataset in Kedro\nDESCRIPTION: Basic structure for a custom ImageDataset class that extends AbstractDataset. Includes constructor, placeholder methods for load, save, and _describe that need to be implemented.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/how_to_create_a_custom_dataset.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict\n\nimport numpy as np\n\nfrom kedro.io import AbstractDataset\n\n\nclass ImageDataset(AbstractDataset[np.ndarray, np.ndarray]):\n    \"\"\"``ImageDataset`` loads / save image data from a given filepath as `numpy` array using Pillow.\n\n    Example:\n    ::\n\n        >>> ImageDataset(filepath='/img/file/path.png')\n    \"\"\"\n\n    def __init__(self, filepath: str):\n        \"\"\"Creates a new instance of ImageDataset to load / save image data at the given filepath.\n\n        Args:\n            filepath: The location of the image file to load / save data.\n        \"\"\"\n        self._filepath = filepath\n\n    def load(self) -> np.ndarray:\n        \"\"\"Loads data from the image file.\n\n        Returns:\n            Data from the image file as a numpy array.\n        \"\"\"\n        ...\n\n    def save(self, data: np.ndarray) -> None:\n        \"\"\"Saves image data to the specified filepath\"\"\"\n        ...\n\n    def _describe(self) -> Dict[str, Any]:\n        \"\"\"Returns a dict that describes the attributes of the dataset\"\"\"\n        ...\n```\n\n----------------------------------------\n\nTITLE: Migrating Configuration Loading to OmegaConfigLoader\nDESCRIPTION: Code diff showing how to update the configuration loading process from ConfigLoader to OmegaConfigLoader, including changes in method calls and syntax.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/config_loader_migration.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n- conf_path = str(project_path / settings.CONF_SOURCE)\n- conf_loader = ConfigLoader(conf_source=conf_path, env=\"local\")\n- catalog = conf_loader.get(\"catalog*\")\n\n+ conf_path = str(project_path / settings.CONF_SOURCE)\n+ config_loader = OmegaConfigLoader(conf_source=conf_path, env=\"local\")\n+ catalog = config_loader[\"catalog\"]\n```\n\n----------------------------------------\n\nTITLE: Kedro Data Catalog Implementation\nDESCRIPTION: Implementation of Kedro's DataCatalog for loading datasets using the catalog configuration.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Using Kedro's DataCatalog\n\nimport yaml\n\nfrom kedro.io import DataCatalog\n\n# load the configuration file\nwith open(\"catalog.yml\") as f:\n    conf_catalog = yaml.safe_load(f)\n\n# Create the DataCatalog instance from the configuration\ncatalog = DataCatalog.from_config(conf_catalog)\n\n# Load the datasets\ncompanies = catalog.load(\"companies\")\nreviews = catalog.load(\"reviews\")\nshuttles = catalog.load(\"shuttles\")\n```\n\n----------------------------------------\n\nTITLE: Defining Default Configuration Patterns in Kedro\nDESCRIPTION: Shows the default configuration patterns used by Kedro to identify and load different types of configuration files including catalog, parameters, credentials, and logging configurations.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/configuration_basics.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nconfig_patterns = {\n    \"catalog\": [\"catalog*\", \"catalog*/**\", \"**/catalog*\"],\n    \"parameters\": [\"parameters*\", \"parameters*/**\", \"**/parameters*\"],\n    \"credentials\": [\"credentials*\", \"credentials*/**\", \"**/credentials*\"],\n    \"logging\": [\"logging*\", \"logging*/**\", \"**/logging*\"],\n}\n```\n\n----------------------------------------\n\nTITLE: Modifying Great Expectations Checkpoint Configuration\nDESCRIPTION: Python code to modify the checkpoint configuration for Great Expectations V3 API by removing data_connector_query from the batch_request.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/examples.md#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nyaml_config = f\"\"\"\nname: {my_checkpoint_name}\nconfig_version: 1.0\nclass_name: SimpleCheckpoint\nrun_name_template: \"%Y%m%d-%H%M%S-my-run-name-template\"\nvalidations:\n  - batch_request:\n      datasource_name: {my_datasource_name}\n      data_connector_name: default_runtime_data_connector_name\n      data_asset_name: my_runtime_asset_name\n      data_connector_query:\n        index: -1\n    expectation_suite_name: {my_expectation_suite_name}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Data Processing and Merging\nDESCRIPTION: Processes and merges multiple dataframes to create the model input table, including data type conversions and cleaning operations.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Data processing\ncompanies[\"iata_approved\"] = companies[\"iata_approved\"] == \"t\"\ncompanies[\"company_rating\"] = (\n    companies[\"company_rating\"].str.replace(\"%\", \"\").astype(float)\n)\nshuttles[\"d_check_complete\"] = shuttles[\"d_check_complete\"] == \"t\"\nshuttles[\"moon_clearance_complete\"] = shuttles[\"moon_clearance_complete\"] == \"t\"\nshuttles[\"price\"] = (\n    shuttles[\"price\"].str.replace(\"$\", \"\").str.replace(\",\", \"\").astype(float)\n)\nrated_shuttles = shuttles.merge(reviews, left_on=\"id\", right_on=\"shuttle_id\")\nmodel_input_table = rated_shuttles.merge(companies, left_on=\"company_id\", right_on=\"id\")\nmodel_input_table = model_input_table.dropna()\nmodel_input_table.head()\n```\n\n----------------------------------------\n\nTITLE: Configuring Kedro Project Settings\nDESCRIPTION: Python code to configure Kedro project settings in settings.py. It overrides default configuration paths to use the current directory for simplicity.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/get_started/minimal_kedro_project.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nCONF_SOURCE = \".\"\nCONFIG_LOADER_ARGS = {\"base_env\": \".\", \"default_run_env\": \".\"}\n```\n\n----------------------------------------\n\nTITLE: Linear Regression Model Training\nDESCRIPTION: Implements model training and prediction using scikit-learn's LinearRegression\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nmodel.predict(X_test)\n\ny_pred = model.predict(X_test)\nr2_score(y_test, y_pred)\n```\n\n----------------------------------------\n\nTITLE: Loading a specific Delta Table version\nDESCRIPTION: YAML configuration that specifies loading a particular version (version 1) of a Delta table dataset in the Kedro catalog.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/deltalake_versioning.md#2025-04-19_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_input_table:\n  type: pandas.DeltaTableDataset\n  filepath: data/03_primary/model_input_table\n  load_args:\n    version: 1\n```\n\n----------------------------------------\n\nTITLE: Kedro Data Catalog Configuration\nDESCRIPTION: YAML configuration for Kedro's Data Catalog defining dataset sources and types.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ncompanies:\n  type: pandas.CSVDataset\n  filepath: data/companies.csv\n\nreviews:\n  type: pandas.CSVDataset\n  filepath: data/reviews.csv\n\nshuttles:\n  type: pandas.ExcelDataset\n  filepath: data/shuttles.xlsx\n```\n\n----------------------------------------\n\nTITLE: Docker Image Management Commands for Container Registry\nDESCRIPTION: Commands for tagging, pushing, and pulling Docker images between local machine and Docker Hub registry.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/single_machine.md#2025-04-19_snippet_0\n\nLANGUAGE: console\nCODE:\n```\ndocker tag <image-name> <DockerID>/<image-name>\n```\n\nLANGUAGE: console\nCODE:\n```\ndocker push <DockerID>/<image-name>\n```\n\nLANGUAGE: console\nCODE:\n```\ndocker pull <DockerID>/<image-name>\n```\n\n----------------------------------------\n\nTITLE: Visualizing Kedro Pipeline in Jupyter\nDESCRIPTION: Demonstrates how to use the %run_viz line magic to display an interactive visualization of the Kedro pipeline directly in a Jupyter notebook.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/kedro_and_notebooks.md#2025-04-19_snippet_13\n\nLANGUAGE: ipython\nCODE:\n```\n%run_viz\n```\n\n----------------------------------------\n\nTITLE: Implementing Kedro Hooks in a Plugin\nDESCRIPTION: Example of implementing hooks in a Kedro plugin. This code defines a hook that runs after the catalog is created, demonstrating how plugins can extend Kedro's core functionality.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/extend_kedro/plugins.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nfrom kedro.framework.hooks import hook_impl\n\n\nclass MyHooks:\n    @hook_impl\n    def after_catalog_created(self, catalog):\n        logging.info(\"Reached after_catalog_created hook\")\n\n\nhooks = MyHooks()\n```\n\n----------------------------------------\n\nTITLE: Using Custom Resolvers in YAML Configuration\nDESCRIPTION: Shows how to use custom resolvers in a parameters.yml file for dynamic value computation.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/advanced_configuration.md#2025-04-19_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_options:\n  test_size: \"${add:1,2,3}\"\n  random_state: 3\n\ndate: \"${today:}\"\n```\n\n----------------------------------------\n\nTITLE: Creating Namespaced Pipelines in Python for Dataset Factory Usage\nDESCRIPTION: Example of creating namespaced modular pipelines that can benefit from dataset factories. This code creates two identical pipelines with different namespaces that will share configuration through a dataset factory.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/kedro_dataset_factories.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.pipeline import Pipeline, node, pipeline\n\nfrom .nodes import evaluate_model, split_data, train_model\n\n\ndef create_pipeline(**kwargs) -> Pipeline:\n    pipeline_instance = pipeline(\n        [\n            node(\n                func=split_data,\n                inputs=[\"model_input_table\", \"params:model_options\"],\n                outputs=[\"X_train\", \"y_train\"],\n                name=\"split_data_node\",\n            ),\n            node(\n                func=train_model,\n                inputs=[\"X_train\", \"y_train\"],\n                outputs=\"regressor\",\n                name=\"train_model_node\",\n            ),\n        ]\n    )\n    ds_pipeline_1 = pipeline(\n        pipe=pipeline_instance,\n        inputs=\"model_input_table\",\n        namespace=\"active_modelling_pipeline\",\n    )\n    ds_pipeline_2 = pipeline(\n        pipe=pipeline_instance,\n        inputs=\"model_input_table\",\n        namespace=\"candidate_modelling_pipeline\",\n    )\n\n    return ds_pipeline_1 + ds_pipeline_2\n```\n\n----------------------------------------\n\nTITLE: Reading Compressed Configuration Files in Kedro\nDESCRIPTION: Demonstrates how to read configuration from compressed tar.gz or zip files using the OmegaConfigLoader in Kedro.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/configuration_basics.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --conf-source=<path-to-compressed-file>.tar.gz\n```\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --conf-source=<path-to-compressed-file>.zip\n```\n\n----------------------------------------\n\nTITLE: Converting Kedro Pipeline to Airflow DAG\nDESCRIPTION: Shell command using kedro-airflow plugin to convert a Kedro pipeline into an Airflow DAG file, specifying the target directory and environment.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/airflow.md#2025-04-19_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nkedro airflow create --target-dir=dags/ --env=airflow\n```\n\n----------------------------------------\n\nTITLE: Using SparkDataset with Python API\nDESCRIPTION: Demonstrates how to use Kedro's SparkDataset to load data into a Spark DataFrame using the Python API instead of YAML configuration.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/pyspark_integration.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport pyspark.sql\nfrom kedro.io import DataCatalog\nfrom kedro_datasets.spark import SparkDataset\n\nspark_ds = SparkDataset(\n    filepath=\"s3a://your_bucket/data/01_raw/weather*\",\n    file_format=\"csv\",\n    load_args={\"header\": True, \"inferSchema\": True},\n    save_args={\"sep\": \"|\", \"header\": True},\n)\ncatalog = DataCatalog({\"weather\": spark_ds})\n\ndf = catalog.load(\"weather\")\nassert isinstance(df, pyspark.sql.DataFrame)\n```\n\n----------------------------------------\n\nTITLE: Applying a Decorator Directly to a Node Function in Python\nDESCRIPTION: Example of applying a decorator directly to a node's function. This shows how to use the retry decorator from the tenacity library to add retry functionality to a flaky node function.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/common_use_cases.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom tenacity import retry\n\n\n@retry\ndef my_flaky_node_function():\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Compressed CSV Dataset in Kedro\nDESCRIPTION: Example showing how to load and save a compressed CSV file on a local file system with compression settings in Kedro's catalog.yml.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog_yaml_examples.md#2025-04-19_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nboats:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/company/boats.csv.gz\n  load_args:\n    sep: ','\n    compression: 'gzip'\n  fs_args:\n    open_args_load:\n      mode: 'rb'\n```\n\n----------------------------------------\n\nTITLE: Configuring Merge Strategy in Python\nDESCRIPTION: Shows how to configure the merge strategy for different configuration types in a Kedro project's settings.py file.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/advanced_configuration.md#2025-04-19_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.config import OmegaConfigLoader\n\nCONFIG_LOADER_CLASS = OmegaConfigLoader\n\nCONFIG_LOADER_ARGS = {\n    \"merge_strategy\": {\n        \"parameters\": \"soft\",\n        \"spark\": \"destructive\",\n        \"mlflow\": \"soft\",\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Transcoding catalog configuration for Delta Tables with Spark\nDESCRIPTION: YAML configuration demonstrating transcoding between Spark and Delta formats for the same dataset, enabling different operations depending on the interface needed.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/deltalake_versioning.md#2025-04-19_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\ntemperature:\n  type: spark.SparkDataset\n  filepath: data/01_raw/data.csv\n  file_format: \"csv\"\n  load_args:\n    header: True\n    inferSchema: True\n  save_args:\n    sep: '|'\n    header: True\n\nweather@spark:\n  type: spark.SparkDataset\n  filepath: s3a://my_bucket/03_primary/weather\n  file_format: \"delta\"\n  save_args:\n    mode: \"overwrite\"\n    versionAsOf: 0\n\nweather@delta:\n  type: spark.DeltaTableDataset\n  filepath: s3a://my_bucket/03_primary/weather\n```\n\n----------------------------------------\n\nTITLE: Deferred Confirmation Pipeline - Python\nDESCRIPTION: Example demonstrating how to implement deferred confirmation of IncrementalDataset processing using multiple pipeline nodes.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/partitioned_and_incremental_datasets.md#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.pipeline import node, pipeline\n\npipeline(\n    [\n        node(\n            func=process_partitions,\n            inputs=\"my_partitioned_dataset\",\n            outputs=\"my_processed_dataset\",\n        ),\n        # do something else\n        node(\n            func=confirm_partitions,\n            # note that the node may not require 'my_partitioned_dataset' as an input\n            inputs=\"my_processed_dataset\",\n            outputs=None,\n            confirms=\"my_partitioned_dataset\",\n        ),\n        # ...\n        node(\n            func=do_something_else_with_partitions,\n            # will return the same partitions even though they were already confirmed\n            inputs=[\"my_partitioned_dataset\", \"my_processed_dataset\"],\n            outputs=None,\n        ),\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Changing Configuration Source at Runtime with CLI\nDESCRIPTION: Shows how to specify an alternative configuration source folder at runtime using the Kedro CLI with the --conf-source flag.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/configuration_basics.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --conf-source=<path-to-new-conf-folder>\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Configuration Loader in Kedro Settings\nDESCRIPTION: This code demonstrates how to set a custom configuration loader class in the project's settings.py file, allowing Kedro to use the custom loader instead of the default one.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/advanced_configuration.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom package_name.custom_configloader import CustomConfigLoader\n\nCONFIG_LOADER_CLASS = CustomConfigLoader\n```\n\n----------------------------------------\n\nTITLE: Disabling Auto-registered Plugin Hooks in Kedro Settings\nDESCRIPTION: Example of how to disable auto-registered plugin hooks in the Kedro project's settings.py file. This snippet demonstrates the use of the DISABLE_HOOKS_FOR_PLUGINS setting to prevent specific plugins from auto-registering their hooks.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/introduction.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# src/<package_name>/settings.py\n\nDISABLE_HOOKS_FOR_PLUGINS = (\"<plugin_name>\",)\n```\n\n----------------------------------------\n\nTITLE: Parallel Pipeline Execution Commands\nDESCRIPTION: Commands to run Kedro pipeline using ParallelRunner or ThreadRunner for concurrent execution.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/run_a_pipeline.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --runner=ParallelRunner\n```\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --runner=ThreadRunner\n```\n\n----------------------------------------\n\nTITLE: Running Kedro with Specific Dataset Version\nDESCRIPTION: Demonstrates how to run Kedro with a specific version of a dataset using the --load-version flag in the command line.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog.md#2025-04-19_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --load-versions=cars:YYYY-MM-DDThh.mm.ss.sssZ\n```\n\n----------------------------------------\n\nTITLE: Implementing ChunkWiseCSVDataset for Kedro\nDESCRIPTION: Custom dataset implementation that extends Kedro's CSVDataset to support chunk-wise data saving. Includes append mode functionality for writing data in chunks.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/nodes.md#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nfrom kedro.io.core import (\n    get_filepath_str,\n)\nfrom kedro_datasets.pandas import CSVDataset\n\n\nclass ChunkWiseCSVDataset(CSVDataset):\n    \"\"\"``ChunkWiseCSVDataset`` loads/saves data from/to a CSV file using an underlying\n    filesystem. It uses pandas to handle the CSV file.\n    \"\"\"\n\n    _overwrite = True\n\n    def _save(self, data: pd.DataFrame) -> None:\n        save_path = get_filepath_str(self._get_save_path(), self._protocol)\n        # Save the header for the first batch\n        if self._overwrite:\n            data.to_csv(save_path, index=False, mode=\"w\")\n            self._overwrite = False\n        else:\n            data.to_csv(save_path, index=False, header=False, mode=\"a\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Dataset Credentials in YAML\nDESCRIPTION: Demonstrates how to set up credentials for dataset access in the 'credentials.yml' file. This example shows AWS S3 access configuration.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog.md#2025-04-19_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\ndev_s3:\n  client_kwargs:\n    aws_access_key_id: key\n    aws_secret_access_key: secret\n```\n\n----------------------------------------\n\nTITLE: Configuring PartitionedDataset in YAML\nDESCRIPTION: YAML configuration for setting up a new partitioned dataset with CSV files in S3, including lazy saving settings.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/partitioned_and_incremental_datasets.md#2025-04-19_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nnew_partitioned_dataset:\n  type: partitions.PartitionedDataset\n  path: s3://my-bucket-name\n  dataset: pandas.CSVDataset\n  filename_suffix: \".csv\"\n  save_lazily: True\n```\n\n----------------------------------------\n\nTITLE: Creating Data Catalog YAML Configuration\nDESCRIPTION: Command to generate a Data Catalog YAML configuration file with MemoryDataset entries for each dataset in a specified pipeline. Supports optional environment specification.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/commands_reference.md#2025-04-19_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nkedro catalog create --pipeline=<pipeline_name>\n```\n\n----------------------------------------\n\nTITLE: Adding Behavior to a Single Node by Name using Kedro Hooks\nDESCRIPTION: Implementation of a hook that adds custom behavior to a specific node based on its name. The hook uses the 'before_node_run' hook point to execute the say_hello function only for the node named 'hello'.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/common_use_cases.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# src/<package_name>/hooks.py\n\nfrom kedro.framework.hooks import hook_impl\nfrom kedro.pipeline.node import Node\n\n\nclass ProjectHooks:\n    @hook_impl\n    def before_node_run(self, node: Node):\n        # adding extra behaviour to a single node\n        if node.name == \"hello\":\n            say_hello(node)\n```\n\n----------------------------------------\n\nTITLE: Using Multiple Dataset Factories in YAML\nDESCRIPTION: Example showing how to define multiple dataset factories in the same catalog to handle different dataset types. Includes factories for both Spark and CSV datasets.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/kedro_dataset_factories.md#2025-04-19_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n\"{namespace}.{dataset_name}@spark\":\n  type: spark.SparkDataset\n  filepath: data/{namespace}/{dataset_name}.parquet\n  file_format: parquet\n\n\"{dataset_name}@csv\":\n  type: pandas.CSVDataset\n  filepath: data/01_raw/{dataset_name}.csv\n```\n\n----------------------------------------\n\nTITLE: Slicing Pipeline by Specifying Nodes in Python\nDESCRIPTION: This snippet shows how to slice a pipeline by specifying the nodes to start from. It demonstrates slicing from the 'mean_node'.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/slice_a_pipeline.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprint(full_pipeline.from_nodes(\"mean_node\").describe())\n```\n\n----------------------------------------\n\nTITLE: Customizing Autodiscovered Pipelines in Kedro (Python)\nDESCRIPTION: This example demonstrates how to modify autodiscovered pipelines in Kedro. It shows adding a new pipeline and modifying existing ones after using `find_pipelines()`, allowing for customization of the default pipeline and individual pipelines.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/pipeline_registry.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef register_pipelines() -> Dict[str, Pipeline]:\n    \"\"\"Register the project's pipelines.\n\n    Returns:\n        A mapping from pipeline names to ``Pipeline`` objects.\n    \"\"\"\n    pipelines = find_pipelines()\n    pipelines[\"__default__\"] = sum(pipelines.values())\n    pipelines[\"data_engineering\"] = pipeline(\n        pipelines[\"data_processing\"], tags=\"data_engineering\"\n    )\n    return pipelines\n```\n\n----------------------------------------\n\nTITLE: Defining Data Catalog Entries with External Credentials in YAML\nDESCRIPTION: YAML configuration for data catalog entries that use external credentials. These entries define datasets that require credentials for access, which will be injected by a hook.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/common_use_cases.md#2025-04-19_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nweather:\n type: spark.SparkDataset\n filepath: s3a://your_bucket/data/01_raw/weather*\n file_format: csv\n credentials: s3_creds\n\ncars:\n type: pandas.CSVDataset\n filepath: https://your_data_store.blob.core.windows.net/data/01_raw/cars.csv\n file_format: csv\n credentials: abs_creds\n```\n\n----------------------------------------\n\nTITLE: Configuring CSV Dataset for Generator Functions (YAML)\nDESCRIPTION: Updates the catalog.yml file to include the chunksize argument for a CSV dataset, enabling the use of generator functions for data loading.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/nodes.md#2025-04-19_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nX_test:\n type: pandas.CSVDataset\n filepath: data/05_model_input/X_test.csv\n load_args:\n   chunksize: 10\n```\n\n----------------------------------------\n\nTITLE: Running Parallel Pipeline Execution\nDESCRIPTION: Demonstrates the new syntax for running Kedro pipelines in parallel using ParallelRunner instead of the deprecated --parallel flag.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --runner=ParallelRunner\n```\n\n----------------------------------------\n\nTITLE: Simplifying Kedro Logging Configuration for Airflow\nDESCRIPTION: YAML configuration that modifies the root handlers to use only the console handler instead of Rich-formatted output, making Kedro logs compatible with Airflow's logging system.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/airflow.md#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nroot:\n  handlers: [console]\n```\n\n----------------------------------------\n\nTITLE: Defining Dataset with Credentials in Data Catalog YAML\nDESCRIPTION: Shows how to reference credentials in the Data Catalog configuration file 'catalog.yml'. This example configures a CSV dataset stored in S3 with specific credentials.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog.md#2025-04-19_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nmotorbikes:\n  type: pandas.CSVDataset\n  filepath: s3://your_bucket/data/02_intermediate/company/motorbikes.csv\n  credentials: dev_s3\n  load_args:\n    sep: ','\n```\n\n----------------------------------------\n\nTITLE: Kedro DataCatalog with ConfigLoader\nDESCRIPTION: Demonstrates using OmegaConfigLoader with DataCatalog to load and process datasets\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Now we are using Kedro's ConfigLoader alongside the DataCatalog\n\nfrom kedro.config import OmegaConfigLoader\nfrom kedro.io import DataCatalog\n\nconf_loader = OmegaConfigLoader(conf_source=\".\")\nconf_catalog = conf_loader[\"catalog\"]\n\n# Create the DataCatalog instance from the configuration\ncatalog = DataCatalog.from_config(conf_catalog)\n\n# Load the datasets\ncompanies = catalog.load(\"companies\")\nreviews = catalog.load(\"reviews\")\nshuttles = catalog.load(\"shuttles\")\n```\n\n----------------------------------------\n\nTITLE: Reloading Kedro Environment in Jupyter\nDESCRIPTION: Shows how to use the %reload_kedro line magic to reload Kedro variables in a Jupyter notebook. This example demonstrates reloading with a specific environment configuration.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/kedro_and_notebooks.md#2025-04-19_snippet_11\n\nLANGUAGE: ipython\nCODE:\n```\n%reload_kedro --env=prod\n```\n\n----------------------------------------\n\nTITLE: Launching Kedro Interactive Development Environments\nDESCRIPTION: Commands to start various interactive development environments including Jupyter Notebook, JupyterLab, and IPython shell. These environments provide access to key Kedro variables.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/commands_reference.md#2025-04-19_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nkedro jupyter notebook\n```\n\nLANGUAGE: bash\nCODE:\n```\nkedro jupyter lab\n```\n\nLANGUAGE: bash\nCODE:\n```\nkedro ipython\n```\n\n----------------------------------------\n\nTITLE: Basic CSV Dataset Configuration in Kedro\nDESCRIPTION: Simple configuration for loading and saving a CSV file from/to a local file system using Kedro's catalog.yml.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog_yaml_examples.md#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nbikes:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/bikes.csv\n```\n\n----------------------------------------\n\nTITLE: Using Resolvers in Catalog Configuration\nDESCRIPTION: Demonstrates the use of custom resolvers in a catalog.yml file for specifying non-primitive types.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/advanced_configuration.md#2025-04-19_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\nmy_polars_dataset:\n  type: polars.CSVDataset\n  filepath: data/01_raw/my_dataset.csv\n  load_args:\n    dtypes:\n      product_age: \"${polars:Float64}\"\n      group_identifier: \"${polars:Utf8}\"\n    try_parse_dates: true\n```\n\n----------------------------------------\n\nTITLE: Registering Trained Model in Kedro Catalog\nDESCRIPTION: This YAML snippet shows how to register the trained model in the Kedro data catalog, enabling versioning for the pickle file.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/add_another_pipeline.md#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nregressor:\n  type: pickle.PickleDataset\n  filepath: data/06_models/regressor.pickle\n  versioned: true\n```\n\n----------------------------------------\n\nTITLE: Using Multiple Placeholders in Dataset Factory Patterns in YAML\nDESCRIPTION: Example showing how to use multiple placeholders in dataset factory patterns to generalize datasets that share type, format, and other configuration but differ in layer and name.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/kedro_dataset_factories.md#2025-04-19_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nprocessing.factory_data:\n  type: spark.SparkDataset\n  filepath: data/processing/factory_data.parquet\n  file_format: parquet\n  save_args:\n    mode: overwrite\n\nprocessing.process_data:\n  type: spark.SparkDataset\n  filepath: data/processing/process_data.parquet\n  file_format: parquet\n  save_args:\n    mode: overwrite\n\nmodelling.metrics:\n  type: spark.SparkDataset\n  filepath: data/modelling/factory_data.parquet\n  file_format: parquet\n  save_args:\n    mode: overwrite\n```\n\nLANGUAGE: yaml\nCODE:\n```\n\"{layer}.{dataset_name}\":\n  type: spark.SparkDataset\n  filepath: data/{layer}/{dataset_name}.parquet\n  file_format: parquet\n  save_args:\n    mode: overwrite\n```\n\n----------------------------------------\n\nTITLE: Updating Kedro Catalog Configuration\nDESCRIPTION: Catalog configuration update to use the custom ChunkWiseCSVDataset for saving predictions.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/nodes.md#2025-04-19_snippet_12\n\nLANGUAGE: diff\nCODE:\n```\n+ y_pred:\n+  type: <package_name>.chunkwise.ChunkWiseCSVDataset\n+  filepath: data/07_model_output/y_pred.csv\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Dataset in Kedro Catalog\nDESCRIPTION: Example of using Kedro's SparkDataset to read a CSV file from S3 into a Spark DataFrame, specified in the catalog.yml file.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/pyspark_integration.md#2025-04-19_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nweather:\n  type: spark.SparkDataset\n  filepath: s3a://your_bucket/data/01_raw/weather*\n  file_format: csv\n  load_args:\n    header: True\n    inferSchema: True\n  save_args:\n    sep: '|'\n    header: True\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Addition Function (Python)\nDESCRIPTION: Creates a basic function that adds two numbers, which will be used to demonstrate node creation.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/nodes.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef add(x, y):\n    return x + y\n```\n\n----------------------------------------\n\nTITLE: Configuring Kedro Project Metadata in pyproject.toml\nDESCRIPTION: Defines the basic project metadata and configuration settings in pyproject.toml file. Includes package name, project name, Kedro version, and source directory settings.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/kedro_project_setup/settings.md#2025-04-19_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[tool.kedro]\npackage_name = \"package_name\"\nproject_name = \"project_name\"\nkedro_init_version = \"kedro_version\"\ntools = \"\"\nexample_pipeline = \"False\"\nsource_dir = \"src\"\n```\n\n----------------------------------------\n\nTITLE: Accessing Dataset Patterns in KedroDataCatalog using Python\nDESCRIPTION: Shows how to access and use the config_resolver for handling dataset patterns in the catalog.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/kedro_data_catalog.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nconfig_resolver = catalog.config_resolver\nds_config = catalog.config_resolver.resolve_pattern(ds_name)  # Resolving a dataset pattern\npatterns = catalog.config_resolver.list_patterns() # Listing all available patterns\n```\n\n----------------------------------------\n\nTITLE: Custom Logging Configuration YAML\nDESCRIPTION: Template for custom logging configuration including handlers for console, file, and rich formatting.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/logging/index.md#2025-04-19_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 1\n\ndisable_existing_loggers: False\n\nformatters:\n  simple:\n    format: \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n\nhandlers:\n  console:\n    class: logging.StreamHandler\n    level: INFO\n    formatter: simple\n    stream: ext://sys.stdout\n\n  info_file_handler:\n    class: logging.handlers.RotatingFileHandler\n    level: INFO\n    formatter: simple\n    filename: info.log\n    maxBytes: 10485760 # 10MB\n    backupCount: 20\n    encoding: utf8\n    delay: True\n\n  rich:\n    class: kedro.logging.RichHandler\n    rich_tracebacks: True\n\nloggers:\n  kedro:\n    level: INFO\n\n  your_python_package:\n    level: INFO\n\nroot:\n  handlers: [rich]\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic ImageDataset in Python for Kedro\nDESCRIPTION: This code snippet defines a basic ImageDataset class that extends AbstractDataset. It provides functionality to load and save image data as numpy arrays using Pillow.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/how_to_create_a_custom_dataset.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import PurePosixPath\nfrom typing import Any, Dict\n\nimport fsspec\nimport numpy as np\nfrom PIL import Image\n\nfrom kedro.io import AbstractDataset\nfrom kedro.io.core import get_filepath_str, get_protocol_and_path\n\n\nclass ImageDataset(AbstractDataset[np.ndarray, np.ndarray]):\n    \"\"\"``ImageDataset`` loads / save image data from a given filepath as `numpy` array using Pillow.\n\n    Example:\n    ::\n\n        >>> ImageDataset(filepath='/img/file/path.png')\n    \"\"\"\n\n    def __init__(self, filepath: str):\n        \"\"\"Creates a new instance of ImageDataset to load / save image data for given filepath.\n\n        Args:\n            filepath: The location of the image file to load / save data.\n        \"\"\"\n        protocol, path = get_protocol_and_path(filepath)\n        self._protocol = protocol\n        self._filepath = PurePosixPath(path)\n        self._fs = fsspec.filesystem(self._protocol)\n\n    def load(self) -> np.ndarray:\n        \"\"\"Loads data from the image file.\n\n        Returns:\n            Data from the image file as a numpy array\n        \"\"\"\n        load_path = get_filepath_str(self._filepath, self._protocol)\n        with self._fs.open(load_path, mode=\"r\") as f:\n            image = Image.open(f).convert(\"RGBA\")\n            return np.asarray(image)\n\n    def save(self, data: np.ndarray) -> None:\n        \"\"\"Saves image data to the specified filepath.\"\"\"\n        save_path = get_filepath_str(self._filepath, self._protocol)\n        with self._fs.open(save_path, mode=\"wb\") as f:\n            image = Image.fromarray(data)\n            image.save(f)\n\n    def _describe(self) -> Dict[str, Any]:\n        \"\"\"Returns a dict that describes the attributes of the dataset.\"\"\"\n        return dict(filepath=self._filepath, protocol=self._protocol)\n```\n\n----------------------------------------\n\nTITLE: Updating Pipeline References to Use Dataset Factory Pattern in Python\nDESCRIPTION: Example showing how to update pipeline node references to work with the dataset factory pattern. This code demonstrates adding a suffix (#csv) to dataset names to match the factory pattern.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/kedro_dataset_factories.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom .nodes import create_model_input_table, preprocess_companies, preprocess_shuttles\n\n\ndef create_pipeline(**kwargs) -> Pipeline:\n    return pipeline(\n        [\n            node(\n                func=preprocess_boats,\n                inputs=\"boats#csv\",\n                outputs=\"preprocessed_boats\",\n                name=\"preprocess_boats_node\",\n            ),\n            node(\n                func=preprocess_cars,\n                inputs=\"cars#csv\",\n                outputs=\"preprocessed_cars\",\n                name=\"preprocess_cars_node\",\n            ),\n            node(\n                func=preprocess_planes,\n                inputs=\"planes#csv\",\n                outputs=\"preprocessed_planes\",\n                name=\"preprocess_planes_node\",\n            ),\n            node(\n                func=create_model_input_table,\n                inputs=[\n                    \"preprocessed_boats\",\n                    \"preprocessed_planes\",\n                    \"preprocessed_cars\",\n                ],\n                outputs=\"model_input_table\",\n                name=\"create_model_input_table_node\",\n            ),\n        ]\n    )\n```\n\n----------------------------------------\n\nTITLE: Custom Comparison Function Configuration - YAML\nDESCRIPTION: YAML configuration showing how to specify a custom comparison function for checkpoint filtering in IncrementalDataset.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/partitioned_and_incremental_datasets.md#2025-04-19_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nmy_partitioned_dataset:\n  type: partitions.IncrementalDataset\n  path: s3://my-bucket-name/path/to/folder\n  dataset: pandas.CSVDataset\n  checkpoint:\n    comparison_func: my_module.path.to.custom_comparison_function  # the path must be importable\n```\n\n----------------------------------------\n\nTITLE: Adding Behavior to All Nodes in a Pipeline using Kedro Hooks\nDESCRIPTION: Implementation of a hook that adds custom behavior to all nodes in a pipeline. The hook applies the say_hello function to every node that runs in the pipeline.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/common_use_cases.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# src/<package_name>/hooks.py\n\nfrom kedro.framework.hooks import hook_impl\nfrom kedro.pipeline.node import Node\n\n\nclass ProjectHooks:\n    @hook_impl\n    def before_node_run(self, node: Node):\n        # adding extra behaviour to all nodes in the pipeline\n        say_hello(node)\n```\n\n----------------------------------------\n\nTITLE: Loading Credentials Configuration Using OmegaConfigLoader in Python\nDESCRIPTION: Demonstrates how to load credentials configuration from project files using the OmegaConfigLoader class. This loads configuration files from conf/base and conf/local directories whose filenames or parent directories start with 'credentials'.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/credentials.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\n\nfrom kedro.config import OmegaConfigLoader\nfrom kedro.framework.project import settings\n\n# Substitute <project_root> with the [root folder for your project](https://docs.kedro.org/en/stable/tutorial/spaceflights_tutorial.html#terminology)\nconf_path = str(Path(<project_root>) / settings.CONF_SOURCE)\nconf_loader = OmegaConfigLoader(conf_source=conf_path)\ncredentials = conf_loader[\"credentials\"]\n```\n\n----------------------------------------\n\nTITLE: Unit Testing Split Data Function\nDESCRIPTION: Test case for the split_data function that validates proper data splitting functionality using dummy data and parameters.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/test_a_project.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef test_split_data():\n    # Arrange\n    dummy_data = pd.DataFrame(\n        {\n            \"engines\": [1, 2, 3],\n            \"crew\": [4, 5, 6],\n            \"passenger_capacity\": [5, 6, 7],\n            \"price\": [120, 290, 30],\n        }\n    )\n\n    dummy_parameters = {\n        \"model_options\": {\n            \"test_size\": 0.2,\n            \"random_state\": 3,\n            \"features\": [\"engines\", \"passenger_capacity\", \"crew\"],\n        }\n    }\n\n    # Act\n    X_train, X_test, y_train, y_test = split_data(dummy_data, dummy_parameters[\"model_options\"])\n\n    # Assert\n    assert len(X_train) == 2\n    assert len(y_train) == 2\n    assert len(X_test) == 1\n    assert len(y_test) == 1\n```\n\n----------------------------------------\n\nTITLE: Creating Pipeline Using Kedro CLI\nDESCRIPTION: Command line instruction to create a new modular pipeline in Kedro\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/modular_pipelines.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkedro pipeline create <pipeline_name>\n```\n\n----------------------------------------\n\nTITLE: DVC Pipeline with Code Tracking\nDESCRIPTION: Extended YAML configuration that includes source code files as dependencies for tracking code changes\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/kedro_dvc_versioning.md#2025-04-19_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nstages:\n  data_processing:\n    cmd: kedro run --pipeline data_processing\n    deps:\n      - data/01_raw/companies.csv\n      - data/01_raw/reviews.csv\n      - data/01_raw/shuttles.xlsx\n      - src/space_dvc/pipelines/data_processing/nodes.py\n      - src/space_dvc/pipelines/data_processing/pipeline.py\n    outs:\n      - data/02_intermediate/preprocessed_companies.parquet\n      - data/02_intermediate/preprocessed_shuttles.parquet\n      - data/03_primary/model_input_table.parquet\n```\n\n----------------------------------------\n\nTITLE: Creating a Kedro Project with a Starter\nDESCRIPTION: This command creates a new Kedro project using a specified starter. The starter can be a local directory path, a remote VCS repository URL, or an alias listed in 'kedro starter list'.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/starters/starters.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkedro new --starter=<path-to-starter>\n```\n\n----------------------------------------\n\nTITLE: Iterating Through Datasets in KedroDataCatalog using Python\nDESCRIPTION: Demonstrates various ways to iterate over dataset names, datasets, and both in the catalog.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/kedro_data_catalog.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfor ds_name in catalog:  # __iter__ defaults to keys\n    pass\n\nfor ds_name in catalog.keys():  # Iterate over dataset names\n    pass\n\nfor ds in catalog.values():  # Iterate over datasets\n    pass\n\nfor ds_name, ds in catalog.items():  # Iterate over (name, dataset) tuples\n    pass\n```\n\n----------------------------------------\n\nTITLE: Custom DryRunner Implementation\nDESCRIPTION: Python implementation of a custom runner that lists nodes to be executed without actually running them, including input validation.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/run_a_pipeline.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# in src/<package_name>/runner.py\nfrom typing import Any, Dict\nfrom kedro.io import AbstractDataset, DataCatalog, MemoryDataset\nfrom kedro.pipeline import Pipeline\nfrom kedro.runner.runner import AbstractRunner\nfrom pluggy import PluginManager\n\n\nclass DryRunner(AbstractRunner):\n    \"\"\"``DryRunner`` is an ``AbstractRunner`` implementation. It can be used to list which\n    nodes would be run without actually executing anything. It also checks if all the\n    necessary data exists.\n    \"\"\"\n\n    def __init__(self, is_async: bool = False, extra_dataset_patterns: Dict[str, Dict[str, Any]] = None):\n        \"\"\"Instantiates the runner class.\n\n        Args:\n            is_async: If True, the node inputs and outputs are loaded and saved\n                asynchronously with threads. Defaults to False.\n            extra_dataset_patterns: Extra dataset factory patterns to be added to the DataCatalog\n                during the run. This is used to set the default datasets.\n        \"\"\"\n        default_dataset_pattern = {\"{default}\": {\"type\": \"MemoryDataset\"}}\n        self._extra_dataset_patterns = extra_dataset_patterns or default_dataset_pattern\n        super().__init__(is_async=is_async, extra_dataset_patterns=self._extra_dataset_patterns)\n\n    def _run(\n        self,\n        pipeline: Pipeline,\n        catalog: DataCatalog,\n        hook_manager: PluginManager = None,\n        session_id: str = None,\n    ) -> None:\n        \"\"\"The method implementing dry pipeline running.\n        Example logs output using this implementation:\n\n            kedro.runner.dry_runner - INFO - Actual run would execute 3 nodes:\n            node3: identity([A]) -> [B]\n            node2: identity([C]) -> [D]\n            node1: identity([D]) -> [E]\n\n        Args:\n            pipeline: The ``Pipeline`` to run.\n            catalog: The ``DataCatalog`` from which to fetch data.\n            hook_manager: The ``PluginManager`` to activate hooks.\n            session_id: The id of the session.\n\n        \"\"\"\n        nodes = pipeline.nodes\n        self._logger.info(\n            \"Actual run would execute %d nodes:\\n%s\",\n            len(nodes),\n            pipeline.describe(),\n        )\n        self._logger.info(\"Checking inputs...\")\n        input_names = pipeline.inputs()\n\n        missing_inputs = [\n            input_name\n            for input_name in input_names\n            if not catalog._get_dataset(input_name).exists()\n        ]\n        if missing_inputs:\n            raise KeyError(f\"Datasets {missing_inputs} not found.\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Dataset for S3 Storage in Kedro\nDESCRIPTION: Example showing how to load and save a Spark table on S3 with schema inference and custom column separators in Kedro's catalog.yml.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog_yaml_examples.md#2025-04-19_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nweather:\n  type: spark.SparkDataset\n  filepath: s3a://your_bucket/data/01_raw/weather*\n  credentials: dev_s3\n  file_format: csv\n  load_args:\n    header: True\n    inferSchema: True\n  save_args:\n    sep: '|'\n    header: True\n```\n\n----------------------------------------\n\nTITLE: Configuring the PyIceberg Dataset in Kedro Catalog\nDESCRIPTION: YAML configuration for adding an Iceberg table to Kedro's catalog by specifying the custom PyIcebergDataset as the type for the model_input_table dataset.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/iceberg_versioning.md#2025-04-19_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_input_table:\n  type: kedro_iceberg.pyiceberg_dataset.PyIcebergDataset\n  catalog: default\n  namespace: default\n  table_name: model_input_table\n```\n\n----------------------------------------\n\nTITLE: Customizing Kedro Jupyter Notebook Command\nDESCRIPTION: Example showing how to override the 'kedro jupyter notebook' command by importing the Jupyter click group and defining a custom implementation. This snippet demonstrates the pattern for customizing Kedro CLI commands from command groups.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/extend_kedro/common_use_cases.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@jupyter.command(\"notebook\")\n@env_option(\n    help=\"Open a notebook\"\n)\ndef notebook_run(...):\n    == ADD YOUR CUSTOM NOTEBOOK COMMAND CODE HERE ==\n```\n\n----------------------------------------\n\nTITLE: Enabling Environment Variable Resolver in Python\nDESCRIPTION: Shows how to enable the oc.env resolver for all configurations in a Kedro project's settings.py file.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/advanced_configuration.md#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom omegaconf.resolvers import oc\n\nCONFIG_LOADER_ARGS = {\n    \"custom_resolvers\": {\n        \"oc.env\": oc.env,\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Catalog with Namespaced Model Outputs in YAML\nDESCRIPTION: Updates the Kedro catalog.yml file to add namespaced outputs for two different model instances (active and candidate) with their respective storage paths.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/add_another_pipeline.md#2025-04-19_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nactive_modelling_pipeline.regressor:\n  type: pickle.PickleDataset\n  filepath: data/06_models/regressor_active.pickle\n  versioned: true\n\ncandidate_modelling_pipeline.regressor:\n  type: pickle.PickleDataset\n  filepath: data/06_models/regressor_candidate.pickle\n  versioned: true\n\n```\n\n----------------------------------------\n\nTITLE: Creating Kedro Pipelines with Dot Notation in Node Names\nDESCRIPTION: This example illustrates the potential issues with using dot notation in node names within Kedro pipelines. It creates a pipeline with a node that has a dot in its output name, which can lead to unexpected behavior.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/pipeline_introduction.md#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npipeline([node(lambda x: x, inputs=\"input1kedro\", outputs=\"output1.kedro\")])\n```\n\n----------------------------------------\n\nTITLE: Creating Nodes with **kwargs Functions (Python)\nDESCRIPTION: Demonstrates how to create Kedro nodes using functions that accept **kwargs, allowing for flexible input specifications.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/nodes.md#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.pipeline import node\n\n\nuk_reporting_node = node(\n    reporting,\n    inputs={\"uk_input1\": \"uk_input1\", \"uk_input2\": \"uk_input2\", ...},\n    outputs=\"uk\",\n)\n\nge_reporting_node = node(\n    reporting,\n    inputs={\"ge_input1\": \"ge_input1\", \"ge_input2\": \"ge_input2\", ...},\n    outputs=\"ge\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring DataCatalog for AWS S3 Storage\nDESCRIPTION: YAML configuration for the DataCatalog to use S3 storage for all datasets in the Kedro pipeline.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/aws_step_functions.md#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ncompanies:\n  type: pandas.CSVDataset\n  filepath: s3://<your-bucket>/companies.csv\n\nreviews:\n  type: pandas.CSVDataset\n  filepath: s3://<your-bucket>/reviews.csv\n\nshuttles:\n  type: pandas.ExcelDataset\n  filepath: s3://<your-bucket>/shuttles.xlsx\n\npreprocessed_companies:\n  type: pandas.CSVDataset\n  filepath: s3://<your-bucket>/preprocessed_companies.csv\n\npreprocessed_shuttles:\n  type: pandas.CSVDataset\n  filepath: s3://<your-bucket>/preprocessed_shuttles.csv\n\nmodel_input_table:\n  type: pandas.CSVDataset\n  filepath: s3://<your-bucket>/model_input_table.csv\n\nregressor:\n  type: pickle.PickleDataset\n  filepath: s3://<your-bucket>/regressor.pickle\n  versioned: true\n\nX_train:\n  type: pickle.PickleDataset\n  filepath: s3://<your-bucket>/X_train.pickle\n\nX_test:\n  type: pickle.PickleDataset\n  filepath: s3://<your-bucket>/X_test.pickle\n\ny_train:\n  type: pickle.PickleDataset\n  filepath: s3://<your-bucket>/y_train.pickle\n\ny_test:\n  type: pickle.PickleDataset\n  filepath: s3://<your-bucket>/y_test.pickle\n```\n\n----------------------------------------\n\nTITLE: Exploring Kedro Pipelines\nDESCRIPTION: Python code to access and explore the pipelines defined in the Kedro project.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/kedro_and_notebooks.md#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npipelines\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Configuration Patterns in Kedro Settings\nDESCRIPTION: This example demonstrates how to add configuration patterns to match additional files beyond the default ones (parameters, credentials, and catalog), specifically for loading Spark configuration files.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/advanced_configuration.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nCONFIG_LOADER_ARGS = {\n    \"config_patterns\": {\n        \"spark\": [\"spark*/\"],\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Data Splitting for Model Training and Evaluation in Python\nDESCRIPTION: A function that splits the preprocessed dataset into training and testing sets for model development. It extracts features and target variables based on parameters provided in a dictionary, allowing for configurable test size and random state.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook.ipynb#2025-04-19_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef split_data(data: pd.DataFrame, parameters: dict) -> tuple:\n    X = data[parameters[\"features\"]]\n    y = data[\"price\"]\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=parameters[\"test_size\"], random_state=parameters[\"random_state\"]\n    )\n    return X_train, X_test, y_train, y_test\n```\n\n----------------------------------------\n\nTITLE: Creating a Spaceflights Project with Kedro Viz (Single Line)\nDESCRIPTION: A single-line command to create a Kedro project named 'spaceflights' with Kedro Viz tools and example code, bypassing the interactive CLI prompts.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/get_started/new_project.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkedro new --name=spaceflights --tools=viz --example=y\n```\n\n----------------------------------------\n\nTITLE: Loading Kedro IPython Extension in Jupyter\nDESCRIPTION: Shows how to load the Kedro IPython extension in a Jupyter environment. This is now the recommended way to work with Kedro in Jupyter or IPython.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n%load_ext kedro.ipython\n```\n\n----------------------------------------\n\nTITLE: Loading a Kedro project in Databricks\nDESCRIPTION: This command loads a Kedro project from its location in either the Databricks workspace or in a Repo. The project path must be specified to properly initialize the project context.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_visualisation.md#2025-04-19_snippet_1\n\nLANGUAGE: ipython\nCODE:\n```\n%reload_kedro <project_root>/iris-databricks\n```\n\n----------------------------------------\n\nTITLE: Configuring SparkSession with Delta Lake Support\nDESCRIPTION: Modified SparkHooks implementation to set up SparkSession with Delta Lake support using the configure_spark_with_delta_pip function.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/pyspark_integration.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.framework.hooks import hook_impl\nfrom pyspark import SparkConf\nfrom pyspark.sql import SparkSession\nfrom delta import configure_spark_with_delta_pip\n\nclass SparkHooks:\n    @hook_impl\n    def after_context_created(self, context) -> None:\n        \"\"\"Initialises a SparkSession using the config\n        defined in project's conf folder.\n        \"\"\"\n\n        # Load the spark configuration in spark.yaml using the config loader\n        parameters = context.config_loader[\"spark\"]\n        spark_conf = SparkConf().setAll(parameters.items())\n\n        # Initialise the spark session\n        spark_session_conf = (\n            SparkSession.builder.appName(context.project_path.name)\n            .enableHiveSupport()\n            .config(conf=spark_conf)\n        )\n        _spark_session = configure_spark_with_delta_pip(spark_session_conf).getOrCreate()\n        _spark_session.sparkContext.setLogLevel(\"WARN\")\n```\n\n----------------------------------------\n\nTITLE: Using Parameters in Kedro Node Definitions\nDESCRIPTION: Demonstrates how to reference parameters in Kedro node definitions using the 'params:' prefix and accessing nested parameters.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/parameters.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef increase_volume(volume, step):\n    return volume + step\n\n\n# in pipeline definition\nnode(\n    func=increase_volume,\n    inputs=[\"input_volume\", \"params:step_size\"],\n    outputs=\"output_volume\",\n)\n```\n\nLANGUAGE: python\nCODE:\n```\ndef train_model(data, model):\n    lr = model[\"learning_rate\"]\n    test_data_ratio = model[\"test_data_ratio\"]\n    iterations = model[\"number_of_train_iterations\"]\n    ...\n\n\n# in pipeline definition\nnode(\n    func=train_model,\n    inputs=[\"input_data\", \"params:model_params\"],\n    outputs=\"output_data\",\n)\n```\n\nLANGUAGE: python\nCODE:\n```\ndef increase_volume(volume, params):\n    step = params[\"step_size\"]\n    return volume + step\n\n\n# in pipeline definition\nnode(\n    func=increase_volume, inputs=[\"input_volume\", \"parameters\"], outputs=\"output_volume\"\n)\n```\n\n----------------------------------------\n\nTITLE: Executing Kedro Pipeline with ThreadRunner\nDESCRIPTION: Command to run a Kedro pipeline using ThreadRunner for concurrent Spark job execution. This enables multiple Spark jobs to run simultaneously through the same SparkSession instance.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/pyspark_integration.md#2025-04-19_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --runner=ThreadRunner\n```\n\n----------------------------------------\n\nTITLE: Comparing Regular Catalog Entries with Dataset Factory Pattern in YAML\nDESCRIPTION: Example showing how traditional catalog configuration can be simplified using dataset factories. This demonstrates the basic pattern syntax that allows multiple similar dataset entries to be consolidated.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/kedro_dataset_factories.md#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nfactory_data:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/factory_data.csv\n\nprocess_data:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/process_data.csv\n```\n\nLANGUAGE: yaml\nCODE:\n```\n\"{name}_data\":\n  type: pandas.CSVDataset\n  filepath: data/01_raw/{name}_data.csv\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 DataCatalog for MWAA\nDESCRIPTION: YAML configuration for setting up S3-based data catalog in AWS MWAA environment.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/airflow.md#2025-04-19_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\ncompanies:\n  type: pandas.CSVDataset\n  filepath: s3://<your_S3_bucket>/data/01_raw/companies.csv\n  credentials: dev_s3\n```\n\n----------------------------------------\n\nTITLE: Ranking Dataset Factories in Kedro Catalog\nDESCRIPTION: Command to display dataset factories in the catalog, ranked by their matching priority.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/commands_reference.md#2025-04-19_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nkedro catalog rank\n```\n\n----------------------------------------\n\nTITLE: Loading Configuration with OmegaConf Directly\nDESCRIPTION: A simple example showing how to load a YAML configuration file directly using OmegaConf's load method. This is suitable for simple configuration needs when not requiring Kedro's more advanced configuration capabilities.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/configuration_basics.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom omegaconf import OmegaConf\n\nparameters = OmegaConf.load(\"/path/to/parameters.yml\")\n```\n\n----------------------------------------\n\nTITLE: Modifying Autodiscovered Pipelines Before Default in Kedro (Python)\nDESCRIPTION: This snippet shows how to modify autodiscovered pipelines before creating the default pipeline in Kedro. It demonstrates updating the 'data_processing' pipeline with a new tag and including this change in the default pipeline.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/pipeline_registry.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef register_pipelines() -> Dict[str, Pipeline]:\n    \"\"\"Register the project's pipelines.\n\n    Returns:\n        A mapping from pipeline names to ``Pipeline`` objects.\n    \"\"\"\n    pipelines = find_pipelines()\n    pipelines[\"data_processing\"] = pipeline(\n        pipelines[\"data_processing\"], tags=\"data_engineering\"\n    )\n    pipelines[\"__default__\"] = sum(pipelines.values())\n    return pipelines\n```\n\n----------------------------------------\n\nTITLE: Implementing the Load Method with fsspec in ImageDataset\nDESCRIPTION: Implementation of the load method for ImageDataset using fsspec for file handling and Pillow for image processing. The method reads an image file and converts it to a numpy array.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/how_to_create_a_custom_dataset.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import PurePosixPath\nfrom typing import Any, Dict\n\nimport fsspec\nimport numpy as np\nfrom PIL import Image\n\nfrom kedro.io import AbstractDataset\nfrom kedro.io.core import get_filepath_str, get_protocol_and_path\n\n\nclass ImageDataset(AbstractDataset[np.ndarray, np.ndarray]):\n    def __init__(self, filepath: str):\n        \"\"\"Creates a new instance of ImageDataset to load / save image data for given filepath.\n\n        Args:\n            filepath: The location of the image file to load / save data.\n        \"\"\"\n        # parse the path and protocol (e.g. file, http, s3, etc.)\n        protocol, path = get_protocol_and_path(filepath)\n        self._protocol = protocol\n        self._filepath = PurePosixPath(path)\n        self._fs = fsspec.filesystem(self._protocol)\n\n    def load(self) -> np.ndarray:\n        \"\"\"Loads data from the image file.\n\n        Returns:\n            Data from the image file as a numpy array\n        \"\"\"\n        # using get_filepath_str ensures that the protocol and path are appended correctly for different filesystems\n        load_path = get_filepath_str(self._filepath, self._protocol)\n        with self._fs.open(load_path) as f:\n            image = Image.open(f).convert(\"RGBA\")\n            return np.asarray(image)\n\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Cloud Storage Excel Dataset in Kedro\nDESCRIPTION: Example showing how to load an Excel file from Google Cloud Storage with project specification and GCP credentials in Kedro's catalog.yml.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog_yaml_examples.md#2025-04-19_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nrockets:\n  type: pandas.ExcelDataset\n  filepath: gcs://your_bucket/data/02_intermediate/company/motorbikes.xlsx\n  fs_args:\n    project: my-project\n  credentials: my_gcp_credentials\n  save_args:\n    sheet_name: Sheet1\n```\n\n----------------------------------------\n\nTITLE: Kedro Pipeline Execution Output\nDESCRIPTION: Sample output showing the execution flow of a Kedro pipeline, including data loading, node processing, and model evaluation steps.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/add_another_pipeline.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n                    INFO     Loading data from 'companies' (CSVDataset)...                   data_catalog.py:343\n                    INFO     Running node: preprocess_companies_node:                                node.py:327\n                             preprocess_companies([companies]) -> [preprocessed_companies]\n                    INFO     Saving data to 'preprocessed_companies' (MemoryDataset)...      data_catalog.py:382\n                    INFO     Completed 1 out of 6 tasks                                  sequential_runner.py:85\n                    INFO     Loading data from 'shuttles' (ExcelDataset)...                  data_catalog.py:343\n[08/09/22 16:56:15] INFO     Running node: preprocess_shuttles_node: preprocess_shuttles([shuttles]) node.py:327\n                             -> [preprocessed_shuttles]\n                    INFO     Saving data to 'preprocessed_shuttles' (MemoryDataset)...       data_catalog.py:382\n                    INFO     Completed 2 out of 6 tasks                                  sequential_runner.py:85\n                    INFO     Loading data from 'preprocessed_shuttles' (MemoryDataset)...    data_catalog.py:343\n                    INFO     Loading data from 'preprocessed_companies' (MemoryDataset)...   data_catalog.py:343\n                    INFO     Loading data from 'reviews' (CSVDataset)...                     data_catalog.py:343\n                    INFO     Running node: create_model_input_table_node:                            node.py:327\n                             create_model_input_table([preprocessed_shuttles,preprocessed_companies,\n                             reviews]) -> [model_input_table]\n[08/09/22 16:56:18] INFO     Saving data to 'model_input_table' (MemoryDataset)...           data_catalog.py:382\n[08/09/22 16:56:19] INFO     Completed 3 out of 6 tasks                                  sequential_runner.py:85\n                    INFO     Loading data from 'model_input_table' (MemoryDataset)...        data_catalog.py:343\n                    INFO     Loading data from 'params:model_options' (MemoryDataset)...     data_catalog.py:343\n                    INFO     Running node: split_data_node:                                          node.py:327\n                             split_data([model_input_table,params:model_options]) ->\n                             [X_train,X_test,y_train,y_test]\n                    INFO     Saving data to 'X_train' (MemoryDataset)...                     data_catalog.py:382\n                    INFO     Saving data to 'X_test' (MemoryDataset)...                      data_catalog.py:382\n                    INFO     Saving data to 'y_train' (MemoryDataset)...                     data_catalog.py:382\n                    INFO     Saving data to 'y_test' (MemoryDataset)...                      data_catalog.py:382\n                    INFO     Completed 4 out of 6 tasks                                  sequential_runner.py:85\n                    INFO     Loading data from 'X_train' (MemoryDataset)...                  data_catalog.py:343\n                    INFO     Loading data from 'y_train' (MemoryDataset)...                  data_catalog.py:343\n                    INFO     Running node: train_model_node: train_model([X_train,y_train]) ->       node.py:327\n                             [regressor]\n[08/09/22 16:56:20] INFO     Saving data to 'regressor' (PickleDataset)...                   data_catalog.py:382\n                    INFO     Completed 5 out of 6 tasks                                  sequential_runner.py:85\n                    INFO     Loading data from 'regressor' (PickleDataset)...                data_catalog.py:343\n                    INFO     Loading data from 'X_test' (MemoryDataset)...                   data_catalog.py:343\n                    INFO     Loading data from 'y_test' (MemoryDataset)...                   data_catalog.py:343\n                    INFO     Running node: evaluate_model_node:                                      node.py:327\n                             evaluate_model([regressor,X_test,y_test]) -> None\n                    INFO     Model has a coefficient R^2 of 0.462 on test data.                      nodes.py:55\n                    INFO     Completed 6 out of 6 tasks                                  sequential_runner.py:85\n                    INFO     Pipeline execution completed successfully.                             runner.py:89\n```\n\n----------------------------------------\n\nTITLE: Getting loaded Delta Table version\nDESCRIPTION: Python code to retrieve the currently loaded version of a Delta table dataset in an interactive Kedro IPython session.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/deltalake_versioning.md#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nIn [3]: model_input_table.get_loaded_version()\nOut [3]: 1\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Secret YAML for AWS Credentials\nDESCRIPTION: A YAML configuration for creating Kubernetes Secrets to store AWS credentials. The secret contains base64-encoded AWS access key ID and secret access key that will be used by the Argo Workflow pods to access S3 storage.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/argo.md#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# secret.yml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-secrets\ndata:\n  access_key_id: <AWS_ACCESS_KEY_ID value encoded with base64>\n  secret_access_key: <AWS_SECRET_ACCESS_KEY value encoded with base64>\ntype: Opaque\n```\n\n----------------------------------------\n\nTITLE: Configuring PyIceberg Catalog with YAML\nDESCRIPTION: YAML configuration for the PyIceberg catalog that uses SQLite for metadata storage and local filesystem for data storage.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/iceberg_versioning.md#2025-04-19_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ncatalog:\n  default:\n    type: sql\n    uri: sqlite:////tmp/warehouse/pyiceberg_catalog.db\n    warehouse: file:///tmp/warehouse/warehouse\n```\n\n----------------------------------------\n\nTITLE: Slicing Pipeline by Inputs in Python\nDESCRIPTION: This snippet demonstrates how to slice a pipeline by providing pre-calculated inputs. It shows examples of slicing from 'm2' and from 'm' and 'xs'.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/slice_a_pipeline.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprint(full_pipeline.from_inputs(\"m2\").describe())\n\nprint(full_pipeline.from_inputs(\"m\", \"xs\").describe())\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Entry Points in TOML\nDESCRIPTION: Configuration in pyproject.toml to register a Kedro plugin. This defines the entry point that Kedro will use to discover and load the plugin's commands.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/extend_kedro/plugins.md#2025-04-19_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[project.entry-points.\"kedro.project_commands\"]\nkedrojson = \"kedrojson.plugin:commands\"\n```\n\n----------------------------------------\n\nTITLE: Reloading Kedro in IPython Environment\nDESCRIPTION: Demonstrates the use of the line magic to reload Kedro in an IPython environment. This is useful for refreshing Kedro-related objects during development.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n%reload_kedro\n```\n\n----------------------------------------\n\nTITLE: Running Specific Kedro Pipeline\nDESCRIPTION: Execute a specific pipeline by name using the --pipeline flag. This example runs only the data_science pipeline.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/add_another_pipeline.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --pipeline=data_science\n```\n\n----------------------------------------\n\nTITLE: Running a Kedro Node (Python)\nDESCRIPTION: Shows how to run a Kedro node by instantiating its inputs with specific values.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/nodes.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nadder_node.run(dict(a=2, b=3))\n```\n\n----------------------------------------\n\nTITLE: Creating a Default Kedro Project with No Tools or Examples (Single Line)\nDESCRIPTION: A single-line command to create a Kedro project named 'My-Project' with no additional tools and no example code, bypassing the interactive CLI prompts.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/get_started/new_project.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkedro new --name=My-Project --tools=none --example=n\n```\n\n----------------------------------------\n\nTITLE: Pipeline Registration Implementation\nDESCRIPTION: Python code showing how to register named pipelines in the pipeline registry.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/run_a_pipeline.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef register_pipelines():\n    \"\"\"Register the project's pipelines.\n\n    Returns:\n        A mapping from pipeline names to ``Pipeline`` objects.\n    \"\"\"\n    pipelines = find_pipelines()\n    pipelines[\"__default__\"] = sum(pipelines.values())\n    my_pipeline = pipeline(\n        [\n            # your definition goes here\n        ]\n    )\n    pipelines[\"my_pipeline\"] = my_pipeline\n    return pipelines\n```\n\n----------------------------------------\n\nTITLE: Overriding Default Dataset Creation with Dataset Factory in YAML\nDESCRIPTION: Example showing how to use a catch-all dataset factory pattern that overrides Kedro's default MemoryDataset creation, replacing it with a custom dataset type for all unspecified datasets.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/kedro_dataset_factories.md#2025-04-19_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\n\"{default_dataset}\":\n  type: pandas.CSVDataset\n  filepath: data/{default_dataset}.csv\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Production Dataset in Base Catalog YAML\nDESCRIPTION: Shows how to configure a dataset for production use in the base catalog configuration file. This example points to a CSV file stored in an AWS S3 bucket.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog.md#2025-04-19_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\ncars:\n  filepath: s3://my_bucket/cars.csv\n  type: pandas.CSVDataset\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Parameters\nDESCRIPTION: YAML configuration for model parameters showing how to specify test size, random state, and features.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/namespaces.md#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_options_1:\n  test_size: 0.15\n  random_state: 3\n  features:\n    - passenger_capacity\n    - crew\n    - d_check_complete\n    - moon_clearance_complete\n    - company_rating\n```\n\n----------------------------------------\n\nTITLE: Registering the Pipeline Debug Hook in Kedro settings.py\nDESCRIPTION: Configuration that registers the PDB pipeline debugging hook in the Kedro project's settings.py file. This enables the hook to be automatically used when an error occurs in a pipeline.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/common_use_cases.md#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nHOOKS = (PDBPipelineDebugHook(),)\n```\n\n----------------------------------------\n\nTITLE: Changing Configuration File Patterns in Kedro Settings\nDESCRIPTION: This snippet shows how to modify the patterns that the configuration loader uses to find parameters files, changing from the default 'parameters' naming convention to a 'params' naming convention.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/advanced_configuration.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nCONFIG_LOADER_ARGS = {\n    \"config_patterns\": {\n        \"parameters\": [\"params*\", \"params*/**\", \"**/params*\"],\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Partitions for Lazy Saving\nDESCRIPTION: Function showing how to implement lazy saving with PartitionedDataset using callable functions.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/partitioned_and_incremental_datasets.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict, Callable\nimport pandas as pd\n\n\ndef create_partitions() -> Dict[str, Callable[[], Any]]:\n    \"\"\"Create new partitions and save using PartitionedDataset.\n\n    Returns:\n        Dictionary of the partitions to create to a function that creates them.\n    \"\"\"\n    return {\n        # create a file \"s3://my-bucket-name/part/foo.csv\"\n        \"part/foo\": lambda: pd.DataFrame({\"data\": [1, 2]}),\n        # create a file \"s3://my-bucket-name/part/bar.csv\"\n        \"part/bar\": lambda: pd.DataFrame({\"data\": [3, 4]}),\n    }\n```\n\n----------------------------------------\n\nTITLE: Checkpoint Configuration - YAML\nDESCRIPTION: YAML configuration example showing how to customize the checkpoint location and parameters for an IncrementalDataset.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/partitioned_and_incremental_datasets.md#2025-04-19_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nmy_partitioned_dataset:\n  type: partitions.IncrementalDataset\n  path: s3://my-bucket-name/path/to/folder\n  dataset: pandas.CSVDataset\n  checkpoint:\n    # update the filepath and load_args, but keep the dataset type unchanged\n    filepath: gcs://other-bucket/CHECKPOINT\n    load_args:\n      k1: v1\n```\n\n----------------------------------------\n\nTITLE: Dataset Configuration with MLflow Artifact Tracking\nDESCRIPTION: YAML diff showing how to modify a MatplotlibWriter dataset to use MLflow artifact tracking\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/mlflow.md#2025-04-19_snippet_3\n\nLANGUAGE: diff\nCODE:\n```\n# conf/base/catalog.yml\n\ndummy_confusion_matrix:\n-  type: matplotlib.MatplotlibWriter\n-  filepath: data/08_reporting/dummy_confusion_matrix.png\n-  versioned: true\n+  type: kedro_mlflow.io.artifacts.MlflowArtifactDataset\n+  dataset:\n+    type: matplotlib.MatplotlibWriter\n+    filepath: data/08_reporting/dummy_confusion_matrix.png\n```\n\n----------------------------------------\n\nTITLE: Resolving Dataset Factories in Kedro Catalog\nDESCRIPTION: Command to resolve dataset factories in the catalog file with explicit entries in the pipeline. Shows both explicit catalog entries and matching factory datasets.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/commands_reference.md#2025-04-19_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nkedro catalog resolve\n```\n\n----------------------------------------\n\nTITLE: Creating a Namespaced Pipeline in Kedro\nDESCRIPTION: This Python code creates a namespaced pipeline in Kedro. It demonstrates how to apply a namespace at the pipeline level while keeping certain inputs unchanged. Namespaces are useful for organizing nodes logically within a pipeline while maintaining a structured execution flow.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/nodes_grouping.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nreturn pipeline(\n    base_pipeline,\n    namespace = \"new_namespaced_pipeline\", # With that namespace, \"new_namespaced_pipeline\" prefix will be added to inputs, outputs, params, and node names\n    inputs={\"the_original_input_name\"}, # Inputs remain the same, without namespace prefix\n)\n```\n\n----------------------------------------\n\nTITLE: Handling Environment Variables in Python Configuration\nDESCRIPTION: This function processes a dictionary of configuration values, expanding any environment variables referenced using the ${VAR} syntax. It supports default values and raises an error for undefined variables.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/kedro/io/README.md#2025-04-19_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndef _parse_env_var(value: str) -> Any:\n    \"\"\"Parse environment variable value.\n\n    Args:\n        value: Value to parse.\n\n    Returns:\n        Parsed value.\n\n    Raises:\n        ValueError: If the value cannot be parsed.\n    \"\"\"\n\n    def _replace_env_var(match):\n        env_var = match.group()[2:-1]\n        var_name, *default = env_var.split(\":\")\n        value = os.getenv(var_name)\n        return value or default[0] if default else _raise_missing_var(var_name)\n\n    def _raise_missing_var(var_name):\n        raise ValueError(f\"Environment variable '{var_name}' is not set\")\n\n    if not isinstance(value, str):\n        return value\n\n    pattern = re.compile(r\"\\${([^}^{]+)}\")\n    return pattern.sub(_replace_env_var, value)\n```\n\n----------------------------------------\n\nTITLE: Running a Kedro Pipeline by Namespace\nDESCRIPTION: This command executes a specific namespace in Kedro. It's useful when you want to run a group of nodes that are logically organized within a pipeline, allowing for selective execution and improved visualization in Kedro-Viz.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/nodes_grouping.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --namespace=<your_namespace_name>\n```\n\n----------------------------------------\n\nTITLE: Initial Data Loading with Pandas\nDESCRIPTION: Basic data loading using pandas to read CSV and Excel files for the spaceflights dataset.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ncompanies = pd.read_csv(\"data/companies.csv\")\nreviews = pd.read_csv(\"data/reviews.csv\")\nshuttles = pd.read_excel(\"data/shuttles.xlsx\", engine=\"openpyxl\")\n```\n\n----------------------------------------\n\nTITLE: Replacing Jinja2 Syntax with OmegaConfigLoader in Kedro Catalog\nDESCRIPTION: Shows how to rewrite Jinja2 configuration to work with OmegaConfigLoader, using dataset factories to achieve similar functionality.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/config_loader_migration.md#2025-04-19_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\n\"{speed}-trains\":\n    type: MemoryDataset\n\n\"{speed}-cars\":\n    type: pandas.CSVDataset\n    filepath: s3://${bucket_name}/{speed}-cars.csv\n    save_args:\n        index: true\n```\n\n----------------------------------------\n\nTITLE: Parsing YAML Configuration Files in Python\nDESCRIPTION: This function reads and parses a YAML configuration file, handling potential YAML parsing errors and file not found exceptions. It returns the parsed content as a dictionary.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/kedro/io/README.md#2025-04-19_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef parse_config(config_file: str) -> Dict[str, Any]:\n    \"\"\"Parse the config file.\n\n    Args:\n        config_file: Path to the config file.\n\n    Returns:\n        Parsed configuration.\n    \"\"\"\n    try:\n        with open(config_file, \"r\") as f:\n            return yaml.safe_load(f)\n    except FileNotFoundError:\n        raise KedroConfigError(\n            f\"'{ config_file }' not found. Please check your config file path.\"\n        ) from None\n    except yaml.YAMLError as exc:\n        raise KedroConfigError(\n            f\"{ config_file } cannot be parsed. Please check the file contents.\"\n        ) from exc\n```\n\n----------------------------------------\n\nTITLE: Searching Datasets with Regex in Kedro Catalog\nDESCRIPTION: Python code to search for datasets in the Kedro Catalog using a regular expression pattern.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/kedro_and_notebooks.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncatalog.list(\"pre*\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Matplotlib Writer for Google Cloud Storage in Kedro\nDESCRIPTION: Example showing how to save a Matplotlib-generated image to Google Cloud Storage with project specification and GCP credentials in Kedro's catalog.yml.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog_yaml_examples.md#2025-04-19_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nresults_plot:\n  type: matplotlib.MatplotlibWriter\n  filepath: gcs://your_bucket/data/08_results/plots/output_1.jpeg\n  fs_args:\n    project: my-project\n  credentials: my_gcp_credentials\n```\n\n----------------------------------------\n\nTITLE: Recursively Processing Configuration Dictionaries in Python\nDESCRIPTION: This function recursively processes a dictionary of configuration values, applying the _parse_env_var function to each string value. It handles nested dictionaries and lists.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/kedro/io/README.md#2025-04-19_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef _parse_config_value(value: Any) -> Any:\n    \"\"\"Parse config value.\n\n    Args:\n        value: Value to parse.\n\n    Returns:\n        Parsed value.\n    \"\"\"\n    if isinstance(value, dict):\n        return {k: _parse_config_value(v) for k, v in value.items()}\n    if isinstance(value, list):\n        return [_parse_config_value(v) for v in value]\n    if isinstance(value, str):\n        return _parse_env_var(value)\n    return value\n```\n\n----------------------------------------\n\nTITLE: Registering Hooks in a Kedro Plugin\nDESCRIPTION: Configuration to register hook implementations from a plugin. This TOML entry point tells Kedro where to find the hooks defined by the plugin.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/extend_kedro/plugins.md#2025-04-19_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[project.entry-points.\"kedro.hooks\"]\nplugin_name = \"plugin_name.plugin:hooks\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Dataset with Credentials in YAML\nDESCRIPTION: This YAML snippet shows how to configure credentials for different data sources in a Kedro project's credentials file.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/advanced_data_catalog_usage.md#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ndev_s3:\n  client_kwargs:\n    aws_access_key_id: key\n    aws_secret_access_key: secret\n\nscooters_credentials:\n  con: sqlite:///kedro.db\n\nmy_gcp_credentials:\n  id_token: key\n```\n\n----------------------------------------\n\nTITLE: Transcoding Creating Hidden Dependencies in Kedro Pipeline\nDESCRIPTION: This example illustrates how transcoding can create hidden dependencies in a Kedro pipeline, potentially affecting performance when using parallel execution.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog_yaml_examples.md#2025-04-19_snippet_25\n\nLANGUAGE: python\nCODE:\n```\npipeline(\n    [\n        node(name=\"my_func1_node\", func=my_func1, inputs=\"my_dataframe@spark\", outputs=\"spark_output\"),\n        node(name=\"my_func2_node\", func=my_func2, inputs=\"pandas_input\", outputs=\"my_dataframe@pandas\"),\n        node(name=\"my_func3_node\", func=my_func3, inputs=\"my_dataframe@pandas\", outputs=\"pandas_output\"),\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Handling Missing Credentials Configuration in Python\nDESCRIPTION: Shows how to handle cases where credentials configuration files might be missing. This pattern catches the MissingConfigException and provides a fallback empty dictionary when no credentials are found.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/credentials.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\n\nfrom kedro.config import OmegaConfigLoader, MissingConfigException\nfrom kedro.framework.project import settings\n\nconf_path = str(Path(<project_root>) / settings.CONF_SOURCE)\nconf_loader = OmegaConfigLoader(conf_source=conf_path)\n\ntry:\n    credentials = conf_loader[\"credentials\"]\nexcept MissingConfigException:\n    credentials = {}\n```\n\n----------------------------------------\n\nTITLE: Reset Checkpoint Configuration - YAML\nDESCRIPTION: YAML configuration showing how to reset the checkpoint to process all available partitions by setting an empty checkpoint value.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/partitioned_and_incremental_datasets.md#2025-04-19_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nmy_partitioned_dataset:\n  type: partitions.IncrementalDataset\n  path: s3://my-bucket-name/path/to/folder\n  dataset: pandas.CSVDataset\n  checkpoint: \"\"\n```\n\n----------------------------------------\n\nTITLE: Installing Kedro Project Dependencies\nDESCRIPTION: Command to install the required dependencies for a Kedro project using pip. This should be run after navigating to the project directory.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/get_started/new_project.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Confirming IncrementalDataset in Same Node - Python\nDESCRIPTION: Example showing how to process and confirm an IncrementalDataset within the same pipeline node using Kedro's node function.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/partitioned_and_incremental_datasets.md#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.pipeline import node\n\n# process and then confirm `IncrementalDataset` within the same node\nnode(\n    process_partitions,\n    inputs=\"my_partitioned_dataset\",\n    outputs=\"my_processed_dataset\",\n    confirms=\"my_partitioned_dataset\",\n)\n```\n\n----------------------------------------\n\nTITLE: Kedro Run Command with Dask Runner\nDESCRIPTION: Command to execute Kedro pipeline using the DaskRunner implementation.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/dask.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --runner=kedro_tutorial.runner.DaskRunner\n```\n\n----------------------------------------\n\nTITLE: Loading Kedro IPython extension in Databricks\nDESCRIPTION: This command loads the Kedro IPython extension in a Databricks notebook, which is a prerequisite for using Kedro-Viz in the notebook environment.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_visualisation.md#2025-04-19_snippet_0\n\nLANGUAGE: ipython\nCODE:\n```\n%load_ext kedro.ipython\n```\n\n----------------------------------------\n\nTITLE: Data Type Conversion Functions for DataFrame Processing in Python\nDESCRIPTION: A collection of utility functions that convert data types in pandas Series. It includes functions to convert string representations of boolean values, percentages, and monetary values to their appropriate numeric formats.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook.ipynb#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef _is_true(x: pd.Series) -> pd.Series:\n    return x == \"t\"\n\n\ndef _parse_percentage(x: pd.Series) -> pd.Series:\n    x = x.str.replace(\"%\", \"\")\n    x = x.astype(float) / 100\n    return x\n\n\ndef _parse_money(x: pd.Series) -> pd.Series:\n    x = x.str.replace(\"$\", \"\").str.replace(\",\", \"\")\n    x = x.astype(float)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Defining Project Metadata in pyproject.toml for Kedro\nDESCRIPTION: The pyproject.toml file contains essential metadata for a Kedro project, including package name, project name, and Kedro version. It informs Kedro where to look for source code and configuration files.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/get_started/minimal_kedro_project.md#2025-04-19_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[tool.kedro]\npackage_name = \"package_name\"\nproject_name = \"project_name\"\nkedro_init_version = \"kedro_version\"\ntools = \"\"\nexample_pipeline = \"False\"\nsource_dir = \"src\"\n```\n\n----------------------------------------\n\nTITLE: Loading Kedro Settings from YAML File in Python\nDESCRIPTION: This function loads Kedro settings from a YAML file, processes environment variables, and returns the parsed configuration. It handles file not found errors and YAML parsing exceptions.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/kedro/io/README.md#2025-04-19_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef _load_settings(settings_file: str) -> Dict[str, Any]:\n    \"\"\"Load Kedro settings file.\n\n    Args:\n        settings_file: Path to Kedro settings file.\n\n    Raises:\n        KedroConfigError: If the settings file cannot be loaded.\n\n    Returns:\n        Parsed Kedro settings.\n    \"\"\"\n    try:\n        with open(settings_file, \"r\") as f:\n            settings = yaml.safe_load(f) or {}\n    except FileNotFoundError:\n        settings = {}\n    except Exception as exc:\n        raise KedroConfigError(\n            f\"Failed to parse settings file '{settings_file}'\\n{str(exc)}\"\n        ) from exc\n\n    return {k: _parse_config_value(v) for k, v in settings.items()}\n```\n\n----------------------------------------\n\nTITLE: Configuring PartitionedDataset with Detailed Dataset Options in YAML\nDESCRIPTION: Advanced example of defining a PartitionedDataset in the catalog.yml file with more granular configuration of the underlying dataset, including load and save arguments.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/partitioned_and_incremental_datasets.md#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmy_partitioned_dataset:\n  type: partitions.PartitionedDataset\n  path: s3://my-bucket-name/path/to/folder\n  dataset:  # full dataset config notation\n    type: pandas.CSVDataset\n    load_args:\n      delimiter: \",\"\n    save_args:\n      index: false\n  credentials: my_credentials\n  load_args:\n    load_arg1: value1\n    load_arg2: value2\n  filepath_arg: filepath  # the argument of the dataset to pass the filepath to\n  filename_suffix: \".csv\"\n```\n\n----------------------------------------\n\nTITLE: Detecting Circular Dependencies in Kedro Pipelines\nDESCRIPTION: This code snippet shows how Kedro handles circular dependencies in a pipeline. It attempts to create a pipeline with two nodes that depend on each other's outputs, resulting in a circular dependency error.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/pipeline_introduction.md#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    pipeline(\n        [\n            node(lambda x: x + 1, \"x\", \"y\", name=\"first node\"),\n            node(lambda y: y - 1, \"y\", \"x\", name=\"second node\"),\n        ]\n    )\nexcept Exception as e:\n    print(e)\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark in YAML\nDESCRIPTION: Example of Spark configuration options stored in conf/base/spark.yml, specifying driver maxResultSize and scheduler mode.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/pyspark_integration.md#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nspark.driver.maxResultSize: 3g\nspark.scheduler.mode: FAIR\n```\n\n----------------------------------------\n\nTITLE: PTVSD Remote Debugging Setup\nDESCRIPTION: Python code to enable remote debugging using PTVSD, including connection setup and execution pause.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/set_up_vscode.md#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport ptvsd\n\n# Allow other computers to attach to ptvsd at this IP address and port.\nptvsd.enable_attach(address=(\"127.0.0.1\", 3000), redirect_output=True)\n\n# Pause the program until a remote debugger is attached\nprint(\"Waiting for debugger to attach...\")\nptvsd.wait_for_attach()\n```\n\n----------------------------------------\n\nTITLE: Registering CLI Hooks in a Kedro Plugin\nDESCRIPTION: Configuration to register CLI hook implementations from a plugin. This TOML entry point specifies where Kedro can find CLI hooks defined by the plugin.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/extend_kedro/plugins.md#2025-04-19_snippet_7\n\nLANGUAGE: toml\nCODE:\n```\n[project.entry-points.\"kedro.cli_hooks\"]\nplugin_name = \"plugin_name.plugin:cli_hooks\"\n```\n\n----------------------------------------\n\nTITLE: Specifying Runtime Parameters via CLI\nDESCRIPTION: Demonstrates how to specify runtime parameters using the Kedro CLI, including handling of spaces and equals signs in parameter values.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/parameters.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --params=param_key1=value1,param_key2=2.0\n```\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --params=\"model_options.model_params.training_date=2011-11-11\"\n```\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --params=\"key1=value with spaces,key2=value\"\n```\n\n----------------------------------------\n\nTITLE: Running a Kedro Pipeline by Name\nDESCRIPTION: This command executes a specific pipeline in Kedro. It's useful when you have separated your logic into different pipelines and want to run them independently in the deployment environment.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/nodes_grouping.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --pipeline=<your_pipeline_name>\n```\n\n----------------------------------------\n\nTITLE: Default Kedro Project Structure\nDESCRIPTION: Shows the default directory structure when creating a new Kedro project with all tools selected.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/get_started/kedro_concepts.md#2025-04-19_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nproject-dir          # Parent directory of the template\n├── conf             # Project configuration files\n├── data             # Local project data (not committed to version control)\n├── docs             # Project documentation\n├── notebooks        # Project-related Jupyter notebooks (can be used for experimental code before moving the code to src)\n├── src              # Project source code\n├── tests            # Folder containing unit and integration tests\n├── .gitignore       # Hidden file that prevents staging of unnecessary files to `git`\n├── pyproject.toml   # Identifies the project root and contains configuration information\n├── README.md        # Project README\n├── requirements.txt # Project dependencies file\n```\n\n----------------------------------------\n\nTITLE: Directory Structure for Micro-packaging in Kedro\nDESCRIPTION: Shows the standard directory structure and file locations that Kedro uses when packaging micro-packages, including configuration, tests, and source code locations.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/micro_packaging.md#2025-04-19_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n├── conf\n│   └── base\n│       └── parameters_{{pipeline_name*}}  <-- All parameter file(s)\n├── tests\n│   ├── init__.py\n│   └── pipelines\n│       └── {{pipeline_name}}              <-- Pipeline tests\n└── src\n    └── my_project\n        ├── __init__.py\n        └── pipelines\n            └── {{pipeline_name}}          <-- Pipeline folder\n```\n\n----------------------------------------\n\nTITLE: Using oc.select for Default Template Values\nDESCRIPTION: YAML diff showing how to use the oc.select resolver to provide default values for templates in OmegaConfigLoader.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/config_loader_migration.md#2025-04-19_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nboats:\n  users:\n    - fred\n-    - \"${write_only_user|ron}\"\n+    - \"${oc.select:write_only_user,ron}\"\n```\n\n----------------------------------------\n\nTITLE: Runner Instantiation Helper Function\nDESCRIPTION: Helper function that instantiates the appropriate runner class with necessary configuration. Includes special handling for DaskRunner with client arguments.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/dask.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef _instantiate_runner(runner, is_async, project_context):\n    runner_class = load_obj(runner, \"kedro.runner\")\n    runner_kwargs = dict(is_async=is_async)\n\n    if runner.endswith(\"DaskRunner\"):\n        client_args = project_context.params.get(\"dask_client\") or {}\n        runner_kwargs.update(client_args=client_args)\n\n    return runner_class(**runner_kwargs)\n```\n\n----------------------------------------\n\nTITLE: Registering the Node Debug Hook in Kedro settings.py\nDESCRIPTION: Configuration that registers the PDB node debugging hook in the Kedro project's settings.py file. This enables the hook to be automatically used when an error occurs in a node.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/common_use_cases.md#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nHOOKS = (PDBNodeDebugHook(),)\n```\n\n----------------------------------------\n\nTITLE: Implementing the _describe Method in ImageDataset\nDESCRIPTION: Implementation of the _describe method for ImageDataset which returns a dictionary with information about the dataset attributes for logging purposes.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/how_to_create_a_custom_dataset.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass ImageDataset(AbstractDataset[np.ndarray, np.ndarray]):\n    def _describe(self) -> Dict[str, Any]:\n        \"\"\"Returns a dict that describes the attributes of the dataset.\"\"\"\n        return dict(filepath=self._filepath, protocol=self._protocol)\n```\n\n----------------------------------------\n\nTITLE: Kedro Pipeline Execution Log Output\nDESCRIPTION: Sample terminal output showing the execution of the namespaced pipelines, displaying how Kedro processes the data through both active_modelling_pipeline and candidate_modelling_pipeline instances.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/add_another_pipeline.md#2025-04-19_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n[11/02/22 10:41:08] INFO     Loading data from 'companies' (CSVDataset)...                                             data_catalog.py:343\n                    INFO     Running node: preprocess_companies_node: preprocess_companies([companies]) ->                     node.py:327\n                             [preprocessed_companies]\n                    INFO     Saving data to 'preprocessed_companies' (ParquetDataset)...                               data_catalog.py:382\n                    INFO     Completed 1 out of 9 tasks                                                            sequential_runner.py:85\n                    INFO     Loading data from 'shuttles' (ExcelDataset)...                                            data_catalog.py:343\n[11/02/22 10:41:13] INFO     Running node: preprocess_shuttles_node: preprocess_shuttles([shuttles]) ->                        node.py:327\n                             [preprocessed_shuttles]\n                    INFO     Saving data to 'preprocessed_shuttles' (ParquetDataset)...                                data_catalog.py:382\n                    INFO     Completed 2 out of 9 tasks                                                            sequential_runner.py:85\n                    INFO     Loading data from 'preprocessed_shuttles' (ParquetDataset)...                             data_catalog.py:343\n                    INFO     Loading data from 'preprocessed_companies' (ParquetDataset)...                            data_catalog.py:343\n                    INFO     Loading data from 'reviews' (CSVDataset)...                                               data_catalog.py:343\n                    INFO     Running node: create_model_input_table_node:                                                      node.py:327\n                             create_model_input_table([preprocessed_shuttles,preprocessed_companies,reviews]) ->\n                             [model_input_table]\n^[[B[11/02/22 10:41:14] INFO     Saving data to 'model_input_table' (ParquetDataset)...                                    data_catalog.py:382\n[11/02/22 10:41:15] INFO     Completed 3 out of 9 tasks                                                            sequential_runner.py:85\n                    INFO     Loading data from 'model_input_table' (ParquetDataset)...                                 data_catalog.py:343\n                    INFO     Loading data from 'params:active_modelling_pipeline.model_options' (MemoryDataset)...     data_catalog.py:343\n                    INFO     Running node: split_data_node:                                                                    node.py:327\n                             split_data([model_input_table,params:active_modelling_pipeline.model_options]) ->\n                             [active_modelling_pipeline.X_train,active_modelling_pipeline.X_test,active_modelling_pipeline.y_t\n                             rain,active_modelling_pipeline.y_test]\n                    INFO     Saving data to 'active_modelling_pipeline.X_train' (MemoryDataset)...                     data_catalog.py:382\n                    INFO     Saving data to 'active_modelling_pipeline.X_test' (MemoryDataset)...                      data_catalog.py:382\n                    INFO     Saving data to 'active_modelling_pipeline.y_train' (MemoryDataset)...                     data_catalog.py:382\n                    INFO     Saving data to 'active_modelling_pipeline.y_test' (MemoryDataset)...                      data_catalog.py:382\n                    INFO     Completed 4 out of 9 tasks                                                            sequential_runner.py:85\n                    INFO     Loading data from 'model_input_table' (ParquetDataset)...                                 data_catalog.py:343\n                    INFO     Loading data from 'params:candidate_modelling_pipeline.model_options' (MemoryDataset)...  data_catalog.py:343\n                    INFO     Running node: split_data_node:                                                                    node.py:327\n                             split_data([model_input_table,params:candidate_modelling_pipeline.model_options]) ->\n                             [candidate_modelling_pipeline.X_train,candidate_modelling_pipeline.X_test,candidate_modelling_pip\n                             eline.y_train,candidate_modelling_pipeline.y_test]\n                    INFO     Saving data to 'candidate_modelling_pipeline.X_train' (MemoryDataset)...                  data_catalog.py:382\n                    INFO     Saving data to 'candidate_modelling_pipeline.X_test' (MemoryDataset)...                   data_catalog.py:382\n                    INFO     Saving data to 'candidate_modelling_pipeline.y_train' (MemoryDataset)...                  data_catalog.py:382\n                    INFO     Saving data to 'candidate_modelling_pipeline.y_test' (MemoryDataset)...                   data_catalog.py:382\n                    INFO     Completed 5 out of 9 tasks                                                            sequential_runner.py:85\n                    INFO     Loading data from 'active_modelling_pipeline.X_train' (MemoryDataset)...                  data_catalog.py:343\n                    INFO     Loading data from 'active_modelling_pipeline.y_train' (MemoryDataset)...                  data_catalog.py:343\n                    INFO     Running node: train_model_node:                                                                   node.py:327\n                             train_model([active_modelling_pipeline.X_train,active_modelling_pipeline.y_train]) ->\n                             [active_modelling_pipeline.regressor]\n                    INFO     Saving data to 'active_modelling_pipeline.regressor' (PickleDataset)...                   data_catalog.py:382\n                    INFO     Completed 6 out of 9 tasks                                                            sequential_runner.py:85\n                    INFO     Loading data from 'candidate_modelling_pipeline.X_train' (MemoryDataset)...               data_catalog.py:343\n                    INFO     Loading data from 'candidate_modelling_pipeline.y_train' (MemoryDataset)...               data_catalog.py:343\n                    INFO     Running node: train_model_node:                                                                   node.py:327\n                             train_model([candidate_modelling_pipeline.X_train,candidate_modelling_pipeline.y_train]) ->\n                             [candidate_modelling_pipeline.regressor]\n```\n\n----------------------------------------\n\nTITLE: Loading Parameters with Kedro Config\nDESCRIPTION: Demonstrates loading model parameters from configuration and preparing data for model training\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nconf_params = conf_loader[\"parameters\"]\ntest_size = conf_params[\"model_options\"][\"test_size\"]\nrandom_state = conf_params[\"model_options\"][\"random_state\"]\nX = model_input_table[conf_params[\"model_options\"][\"features\"]]\ny = model_input_table[\"price\"]\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Dataset with Credentials\nDESCRIPTION: Python implementation of a custom ImageDataset class that handles credentials and filesystem arguments using fsspec.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/how_to_create_a_custom_dataset.md#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport fsspec\n\n\nclass ImageDataset(AbstractVersionedDataset):\n    def __init__(\n        self,\n        filepath: str,\n        version: Version = None,\n        credentials: Dict[str, Any] = None,\n        fs_args: Dict[str, Any] = None,\n    ):\n        \"\"\"Creates a new instance of ImageDataset to load / save image data for given filepath.\n\n        Args:\n            filepath: The location of the image file to load / save data.\n            version: The version of the dataset being saved and loaded.\n            credentials: Credentials required to get access to the underlying filesystem.\n                E.g. for ``GCSFileSystem`` it should look like {\"token\": None}.\n            fs_args: Extra arguments to pass into underlying filesystem class.\n                E.g. for ``GCSFileSystem`` class: {\"project\": \"my-project\", ...}.\n        \"\"\"\n        protocol, path = get_protocol_and_path(filepath)\n        self._protocol = protocol\n        self._fs = fsspec.filesystem(self._protocol, **credentials, **fs_args)\n\n    ...\n```\n\n----------------------------------------\n\nTITLE: Complete Data Processing Function\nDESCRIPTION: Single monolithic function combining all data processing and model training steps\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook.ipynb#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef big_function():\n    ####################\n    # Data processing  #\n    ####################\n    companies[\"iata_approved\"] = companies[\"iata_approved\"] == \"t\"\n    companies[\"company_rating\"] = (\n        companies[\"company_rating\"].str.replace(\"%\", \"\").astype(float)\n    )\n    shuttles[\"d_check_complete\"] = shuttles[\"d_check_complete\"] == \"t\"\n    shuttles[\"moon_clearance_complete\"] = shuttles[\"moon_clearance_complete\"] == \"t\"\n    shuttles[\"price\"] = (\n        shuttles[\"price\"].str.replace(\"$\", \"\").str.replace(\",\", \"\").astype(float)\n    )\n    rated_shuttles = shuttles.merge(reviews, left_on=\"id\", right_on=\"shuttle_id\")\n    model_input_table = rated_shuttles.merge(\n        companies, left_on=\"company_id\", right_on=\"id\"\n    )\n    model_input_table = model_input_table.dropna()\n    model_input_table.head()\n\n    X = model_input_table[conf_params[\"model_options\"][\"features\"]]\n    y = model_input_table[\"price\"]\n\n    ##################################\n    # Model training and evaluation  #\n    ##################################\n    from sklearn.model_selection import train_test_split\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=random_state\n    )\n\n    from sklearn.linear_model import LinearRegression\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    model.predict(X_test)\n    from sklearn.metrics import r2_score\n\n    y_pred = model.predict(X_test)\n    print(r2_score(y_test, y_pred))\n```\n\n----------------------------------------\n\nTITLE: Registering SparkHooks in Kedro Settings\nDESCRIPTION: Code snippet showing how to register SparkHooks in the project's settings.py file to enable the Spark initialization hook.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/pyspark_integration.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom <package_name>.hooks import SparkHooks\n\nHOOKS = (SparkHooks(),)\n```\n\n----------------------------------------\n\nTITLE: MLflow Configuration\nDESCRIPTION: YAML configuration for MLflow tracking URI in Kedro project\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/mlflow.md#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nserver:\n  mlflow_tracking_uri: mlflow_runs\n```\n\n----------------------------------------\n\nTITLE: Authenticating Remote Configuration in Kedro\nDESCRIPTION: Shows how to set up authentication for remote configuration sources using environment variables for different cloud platforms.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/configuration_basics.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexport AWS_ACCESS_KEY_ID=your_access_key\nexport AWS_SECRET_ACCESS_KEY=your_secret_key\nkedro run --conf-source=s3://my-bucket/configs/\n```\n\nLANGUAGE: bash\nCODE:\n```\ngcloud auth application-default login\nkedro run --conf-source=gs://my-bucket/configs/\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport AZURE_STORAGE_ACCOUNT=your_account_name\nexport AZURE_STORAGE_KEY=your_account_key\nkedro run --conf-source=abfs://container@account/configs/\n```\n\n----------------------------------------\n\nTITLE: Configuring MemoryDataset for Spark Objects\nDESCRIPTION: YAML configuration for using MemoryDataset with assign copy mode to handle non-DataFrame Spark objects in the Kedro pipeline.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/pyspark_integration.md#2025-04-19_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nexample_classifier:\n  type: MemoryDataset\n  copy_mode: assign\n```\n\n----------------------------------------\n\nTITLE: Mermaid Flowchart for Kedro Deployment Decision-Making\nDESCRIPTION: This Mermaid code snippet defines a flowchart that guides users through the decision-making process for deploying Kedro projects. It covers single-machine and distributed deployment options, as well as specific platform choices.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/index.md#2025-04-19_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart TD\n    A{Can your Kedro pipeline run on a single machine?} -- YES --> B[Consult the single-machine deployment guide];\n    B --> C{Do you have Docker on your machine?};\n    C -- YES --> D[Use a container-based approach];\n    C -- NO --> E[Use the CLI or package mode];\n    A -- NO --> F[Consult the distributed deployment guide];\n    F --> G[\"What distributed platform are you using?<br/><br/>Check out the guides for:<br/><br/><li>Airflow</li><li>Amazon SageMaker</li><li>AWS Step functions</li><li>Azure</li><li>Dask</li><li>Databricks</li><li>Kubeflow Workflows</li><li>Prefect</li><li>Vertex AI</li>\"];\n    style G text-align:left\n```\n\n----------------------------------------\n\nTITLE: Running Kedro Nodes by Tag\nDESCRIPTION: This command runs specific nodes or pipelines in Kedro based on tags. It's useful for executing specific sections without modifying the pipeline structure, especially when you need to run nodes that don't belong to the same pipeline.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/nodes_grouping.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --tags=<your_tag_name>\n```\n\n----------------------------------------\n\nTITLE: Creating New Checkpoint in Great Expectations V3 API\nDESCRIPTION: Command to create a new checkpoint in Great Expectations V3 API for data validation in Kedro.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/examples.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ngreat_expectations checkpoint new raw_companies_dataset_checkpoint\n```\n\n----------------------------------------\n\nTITLE: MWAA Environment Configuration\nDESCRIPTION: AWS MWAA environment configuration settings including S3 bucket paths for DAGs, plugins, requirements, and startup script.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/airflow.md#2025-04-19_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nS3 Bucket:\n  s3://your_S3_bucket\nDAGs folder\n  s3://your_S3_bucket/dags\nPlugins file - optional\n  s3://your_S3_bucket/plugins.zip\nRequirements file - optional\n  s3://your_S3_bucket/requrements.txt\nStartup script file - optional\n  s3://your_S3_bucket/startup.sh\n```\n\n----------------------------------------\n\nTITLE: Consuming Pipeline Output\nDESCRIPTION: Example showing how to consume the output of a Kedro pipeline in an interactive environment\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/package_a_project.md#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom spaceflights.__main__ import main\n\ndef run_kedro_pipeline():\n   result = main(pipeline_name=<pipeline>)\n   do_something_with(<result>)\n```\n\n----------------------------------------\n\nTITLE: Defining Git-Based Kedro Starter in Python\nDESCRIPTION: Python code to define a Kedro starter specification for a Git repository, allowing the use of a custom starter alias for a remote template.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/starters/create_a_starter.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# plugin.py\nstarters = [\n    KedroStarterSpec(\n        alias=\"test_plugin_starter\",\n        template_path=\"https://github.com/kedro-org/kedro-starters/\",\n        directory=\"spaceflights-pandas\",\n    )\n]\n```\n\n----------------------------------------\n\nTITLE: Using YAML Anchors for Reusable Dataset Configurations in Kedro\nDESCRIPTION: Example showing how to use YAML anchors to create reusable dataset configurations for multiple similar datasets, reducing duplication in Kedro's catalog.yml.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog_yaml_examples.md#2025-04-19_snippet_19\n\nLANGUAGE: yaml\nCODE:\n```\n_csv: &csv\n  type: spark.SparkDataset\n  file_format: csv\n  load_args:\n    sep: ','\n    na_values: ['#NA', NA]\n    header: True\n    inferSchema: False\n\ncars:\n  <<: *csv\n  filepath: s3a://data/01_raw/cars.csv\n\ntrucks:\n  <<: *csv\n  filepath: s3a://data/01_raw/trucks.csv\n\nbikes:\n  <<: *csv\n  filepath: s3a://data/01_raw/bikes.csv\n  load_args:\n    header: False\n```\n\n----------------------------------------\n\nTITLE: Resolved Catalog Output\nDESCRIPTION: Example YAML output from the 'kedro catalog resolve' command showing the resolved dataset entries.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/kedro_dataset_factories.md#2025-04-19_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\ncompanies:\n  filepath: data/01_raw/companies.csv\n  type: pandas.CSVDataset\nmodel_input_table:\n  filepath: data/03_primary/model_input_table.parquet\n  type: pandas.ParquetDataset\npreprocessed_companies:\n  filepath: data/02_intermediate/preprocessed_companies.parquet\n  type: pandas.ParquetDataset\npreprocessed_shuttles:\n  filepath: data/02_intermediate/preprocessed_shuttles.parquet\n  type: pandas.ParquetDataset\nreviews:\n  filepath: data/01_raw/reviews.csv\n  type: pandas.CSVDataset\nshuttles:\n  filepath: data/01_raw/shuttles.xlsx\n  load_args:\n    engine: openpyxl\n  type: pandas.ExcelDataset\n```\n\n----------------------------------------\n\nTITLE: Implementing Versioned ImageDataset in Python for Kedro\nDESCRIPTION: This code snippet defines a versioned ImageDataset class that extends AbstractVersionedDataset. It adds versioning support to the basic ImageDataset, allowing for version-specific loading and saving of image data.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/how_to_create_a_custom_dataset.md#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import PurePosixPath\nfrom typing import Any, Dict\n\nimport fsspec\nimport numpy as np\nfrom PIL import Image\n\nfrom kedro.io import AbstractVersionedDataset\nfrom kedro.io.core import get_filepath_str, get_protocol_and_path, Version\n\n\nclass ImageDataset(AbstractVersionedDataset[np.ndarray, np.ndarray]):\n    \"\"\"``ImageDataset`` loads / save image data from a given filepath as `numpy` array using Pillow.\n\n    Example:\n    ::\n\n        >>> ImageDataset(filepath='/img/file/path.png')\n    \"\"\"\n\n    def __init__(self, filepath: str, version: Version = None):\n        \"\"\"Creates a new instance of ImageDataset to load / save image data for given filepath.\n\n        Args:\n            filepath: The location of the image file to load / save data.\n            version: The version of the dataset being saved and loaded.\n        \"\"\"\n        protocol, path = get_protocol_and_path(filepath)\n        self._protocol = protocol\n        self._fs = fsspec.filesystem(self._protocol)\n\n        super().__init__(\n            filepath=PurePosixPath(path),\n            version=version,\n            exists_function=self._fs.exists,\n            glob_function=self._fs.glob,\n        )\n\n    def load(self) -> np.ndarray:\n        \"\"\"Loads data from the image file.\n\n        Returns:\n            Data from the image file as a numpy array\n        \"\"\"\n        load_path = get_filepath_str(self._get_load_path(), self._protocol)\n        with self._fs.open(load_path, mode=\"r\") as f:\n            image = Image.open(f).convert(\"RGBA\")\n            return np.asarray(image)\n\n    def save(self, data: np.ndarray) -> None:\n        \"\"\"Saves image data to the specified filepath.\"\"\"\n        save_path = get_filepath_str(self._get_save_path(), self._protocol)\n        with self._fs.open(save_path, mode=\"wb\") as f:\n            image = Image.fromarray(data)\n            image.save(f)\n\n    def _describe(self) -> Dict[str, Any]:\n        \"\"\"Returns a dict that describes the attributes of the dataset.\"\"\"\n        return dict(\n            filepath=self._filepath, version=self._version, protocol=self._protocol\n        )\n```\n\n----------------------------------------\n\nTITLE: Running the Kedro Pipeline with Iceberg Integration\nDESCRIPTION: Command to run the Kedro pipeline which will now use Iceberg tables for the configured datasets.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/iceberg_versioning.md#2025-04-19_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkedro run\n```\n\n----------------------------------------\n\nTITLE: Starting Kedro-Viz to Visualize a Project\nDESCRIPTION: Command to launch Kedro-Viz for visualizing a Kedro project. This automatically opens a browser tab to serve the visualization at http://127.0.0.1:4141/.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/get_started/new_project.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nkedro viz run\n```\n\n----------------------------------------\n\nTITLE: Implementing Partition Concatenation in Python\nDESCRIPTION: Function to concatenate partitioned data into a single pandas DataFrame. Takes a dictionary of partition load functions and returns a concatenated DataFrame.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/partitioned_and_incremental_datasets.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Callable, Dict\nimport pandas as pd\n\n\ndef concat_partitions(partitioned_input: Dict[str, Callable[[], Any]]) -> pd.DataFrame:\n    \"\"\"Concatenate input partitions into one pandas DataFrame.\n\n    Args:\n        partitioned_input: A dictionary with partition ids as keys and load functions as values.\n\n    Returns:\n        Pandas DataFrame representing a concatenation of all loaded partitions.\n    \"\"\"\n    result = pd.DataFrame()\n\n    for partition_key, partition_load_func in sorted(partitioned_input.items()):\n        partition_data = partition_load_func()  # load the actual partition data\n        # concat with existing result\n        result = pd.concat([result, partition_data], ignore_index=True, sort=True)\n\n    return result\n```\n\n----------------------------------------\n\nTITLE: Configuring Data Science Parameters in YAML\nDESCRIPTION: This YAML snippet defines the parameters used in the data science pipeline, including test size, random state, and features for the model.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/add_another_pipeline.md#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_options:\n  test_size: 0.2\n  random_state: 3\n  features:\n    - engines\n    - passenger_capacity\n    - crew\n    - d_check_complete\n    - moon_clearance_complete\n    - iata_approved\n    - company_rating\n    - review_scores_rating\n```\n\n----------------------------------------\n\nTITLE: Basic Kedro Logging Implementation\nDESCRIPTION: Shows how to implement basic logging in a Kedro node using Python's logging library. Demonstrates different logging levels including warning, info, and debug.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/logging/index.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nlogger = logging.getLogger(__name__)\nlogger.warning(\"Issue warning\")\nlogger.info(\"Send information\")\nlogger.debug(\"Useful information for debugging\")\n```\n\n----------------------------------------\n\nTITLE: Loading Kedro Node in Jupyter\nDESCRIPTION: Illustrates the use of the %load_node line magic to load the contents of a specific Kedro node into Jupyter notebook cells for inspection or debugging.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/kedro_and_notebooks.md#2025-04-19_snippet_12\n\nLANGUAGE: ipython\nCODE:\n```\n%load_node <my-node-name>\n```\n\n----------------------------------------\n\nTITLE: Using Default Values with Runtime Parameters in YAML\nDESCRIPTION: Illustrates how to specify a default value for a runtime parameter in a catalog entry.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/advanced_configuration.md#2025-04-19_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\ncompanies:\n  type: pandas.CSVDataset\n  filepath: \"${runtime_params:folder, 'data/01_raw'}/companies.csv\"\n```\n\n----------------------------------------\n\nTITLE: Runtime Logging Level Configuration\nDESCRIPTION: Example of changing logging level for specific Kedro component at runtime in Jupyter notebook.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/logging/index.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nlogging.getLogger(\"kedro.io.data_catalog\").setLevel(logging.WARNING)\n```\n\n----------------------------------------\n\nTITLE: Initializing AWSBatchRunner Import in Python\nDESCRIPTION: This snippet shows how to import and declare the AWSBatchRunner in the __init__.py file of the runner folder.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/aws_batch.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom .batch_runner import AWSBatchRunner\n\n__all__ = [\"AWSBatchRunner\"]\n```\n\n----------------------------------------\n\nTITLE: Slicing Pipeline to Specific Nodes in Python\nDESCRIPTION: This snippet shows how to slice a pipeline to end at specified nodes. It demonstrates slicing to the 'mean_node'.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/slice_a_pipeline.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint(full_pipeline.to_nodes(\"mean_node\").describe())\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Credentials\nDESCRIPTION: YAML configuration for AWS credentials to access S3 bucket.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/airflow.md#2025-04-19_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\ndev_s3:\n  client_kwargs:\n    aws_access_key_id: *********************\n    aws_secret_access_key: ******************************************\n```\n\n----------------------------------------\n\nTITLE: Finding Python Interpreter Path in Terminal\nDESCRIPTION: Commands to find the path to the Python interpreter in a conda environment for macOS/Linux and Windows systems, which is needed when configuring PyCharm.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/set_up_pycharm.md#2025-04-19_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n# macOS / Linux\nwhich python\n# Windows\npython -c \"import sys; print(sys.executable)\"\n```\n\n----------------------------------------\n\nTITLE: Listing Datasets in Kedro Catalog\nDESCRIPTION: Command to list all datasets per pipeline per type. Supports optional pipeline filtering using the --pipeline argument with comma-separated pipeline names.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/commands_reference.md#2025-04-19_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nkedro catalog list\n```\n\nLANGUAGE: bash\nCODE:\n```\nkedro catalog list --pipeline=ds,de\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiprocessing for Kedro Projects\nDESCRIPTION: Demonstrates how to handle process spawning in multiprocessing environments when using Kedro, ensuring proper project configuration in child processes.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/kedro_project_setup/session.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nif multiprocessing.get_start_method() == \"spawn\" and package_name:\n        _bootstrap_subprocess(package_name, logging_config)\n```\n\n----------------------------------------\n\nTITLE: Running Default Kedro Pipeline\nDESCRIPTION: Execute the default pipeline that includes all project pipelines in sequence.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/add_another_pipeline.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkedro run\n```\n\n----------------------------------------\n\nTITLE: Creating a New Kedro Project with Databricks-Iris Starter\nDESCRIPTION: Command to create a new Kedro project using the databricks-iris starter template.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_deployment_workflow.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkedro new --starter=databricks-iris\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Fair Scheduling\nDESCRIPTION: YAML configuration to enable fair scheduling mode in Spark, which ensures each node gets equal cluster resources. This configuration should be added to conf/base/spark.yml to override the default FIFO scheduling.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/pyspark_integration.md#2025-04-19_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nspark.scheduler.mode: FAIR\n```\n\n----------------------------------------\n\nTITLE: Installing Kedro Project with MLflow\nDESCRIPTION: Commands to create a new Kedro project with spaceflights starter and set up MLflow tracking\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/mlflow.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ kedro new --starter=spaceflights-pandas-viz --name spaceflights-mlflow\n$ cd spaceflights-mlflow\n$ python -m venv && source .venv/bin/activate\n(.venv) $ pip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Remote Debugger Launch Configuration\nDESCRIPTION: VS Code launch configuration for remote debugging setup with path mappings and connection settings.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/set_up_vscode.md#2025-04-19_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"Kedro Remote Debugger\",\n    \"type\": \"python\",\n    \"request\": \"attach\",\n    \"pathMappings\": [\n        {\n            \"localRoot\": \"${workspaceFolder}\",\n            \"remoteRoot\": \"/path/to/your/project\"\n        }\n    ],\n    \"port\": 3000,\n    \"host\": \"127.0.0.1\"\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Kedro Extension in IPython Console\nDESCRIPTION: Code snippet to load Kedro's IPython extension in PyCharm's Python Console, enabling access to Kedro context, session, and catalog objects.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/set_up_pycharm.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n%load_ext kedro.ipython\n```\n\n----------------------------------------\n\nTITLE: Defining a Reporting Function with **kwargs (Python)\nDESCRIPTION: Creates a reporting function that uses **kwargs to handle an arbitrary number of named inputs, useful for dynamic node creation.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/nodes.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef reporting(**kwargs):\n    result = []\n    for name, data in kwargs.items():\n        res = example_report(name, data)\n        result.append(res)\n    return combined_report(result)\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies with pip\nDESCRIPTION: This command uses pip to install all package dependencies specified in the requirements.txt file, which is essential for setting up a Kedro project.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/commands_reference.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Kedro via pip\nDESCRIPTION: Command to install Kedro using pip package manager. This is the first step in setting up a Kedro project.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/get_started/minimal_kedro_project.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install kedro\n```\n\n----------------------------------------\n\nTITLE: Accessing Project Path in Kedro Context\nDESCRIPTION: Python code to retrieve the project path from the Kedro context object.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/kedro_and_notebooks.md#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncontext.project_path\n```\n\n----------------------------------------\n\nTITLE: Configuring MinIO S3-Compatible Storage in Kedro\nDESCRIPTION: Example showing how to load a CSV file from MinIO, an S3-compatible storage service, using custom endpoint configuration in Kedro's catalog.yml.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog_yaml_examples.md#2025-04-19_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\ntest:\n  type: pandas.CSVDataset\n  filepath: s3://your_bucket/test.csv # assume `test.csv` is uploaded to the MinIO server.\n  credentials: dev_minio\n```\n\n----------------------------------------\n\nTITLE: FSSpec Arguments Configuration for Remote Pulls\nDESCRIPTION: YAML configuration example for providing authentication headers when pulling micro-packages from remote locations using fsspec.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/micro_packaging.md#2025-04-19_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nclient_kwargs:\n  headers:\n    Authorization: token <token>\n```\n\n----------------------------------------\n\nTITLE: Configuring PartitionedDataset with ImageDataset in YAML\nDESCRIPTION: This YAML configuration sets up a PartitionedDataset named 'pokemon' that uses the ImageDataset to load multiple PNG images from a specified directory.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/how_to_create_a_custom_dataset.md#2025-04-19_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\npokemon:\n  type: partitions.PartitionedDataset\n  dataset: kedro_pokemon.datasets.image_dataset.ImageDataset\n  path: data/01_raw/pokemon-images-and-types/images/images\n  filename_suffix: \".png\"\n```\n\n----------------------------------------\n\nTITLE: Initializing DaskRunner Module Import\nDESCRIPTION: Package initialization code that exports the DaskRunner class from the dask_runner module.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/dask.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom .dask_runner import DaskRunner\n\n__all__ = [\"DaskRunner\"]\n```\n\n----------------------------------------\n\nTITLE: Testing PyIceberg Catalog Configuration\nDESCRIPTION: Python code to verify that the PyIceberg catalog configuration is working correctly by loading the catalog in a Python shell.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/iceberg_versioning.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pyiceberg.catalog import load_catalog\ncatalog = load_catalog(name=\"default\")\n```\n\n----------------------------------------\n\nTITLE: Registering Hook Implementations in Kedro Settings\nDESCRIPTION: Example of registering hook implementations in the Kedro project's settings.py file. This snippet shows how to add multiple hook classes to the HOOKS tuple, which allows Kedro to use these hooks during execution.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/introduction.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# src/<package_name>/settings.py\nfrom <package_name>.hooks import ProjectHooks, DataCatalogHooks\n\nHOOKS = (ProjectHooks(), DataCatalogHooks())\n```\n\n----------------------------------------\n\nTITLE: Running the Prefect Deployment Script\nDESCRIPTION: Command to run the Python script that registers a Kedro pipeline as a Prefect flow, specifying the work pool and queue names created earlier.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/prefect.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython register_prefect_flow.py --work_pool_name <work_pool_name> --work_queue_name <work_queue_name>\n```\n\n----------------------------------------\n\nTITLE: Importing Kedro Configuration Functions in Python\nDESCRIPTION: Import statement for the configure_project function from the Kedro framework, to be used in conjunction with the KedroSession creation.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.framework.project import configure_project  # to add\nfrom kedro.framework.session import KedroSession\n```\n\n----------------------------------------\n\nTITLE: MLflow Model Registry Configuration\nDESCRIPTION: YAML configuration for registering models in MLflow with kedro-mlflow\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/mlflow.md#2025-04-19_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nregressor:\n  type: kedro_mlflow.io.models.MlflowModelTrackingDataset\n  flavor: mlflow.sklearn\n  save_args:\n    registered_model_name: spaceflights-regressor\n```\n\n----------------------------------------\n\nTITLE: Instantiating PartitionedDataset in Python\nDESCRIPTION: Example of programmatically creating a PartitionedDataset instance in Python, specifying the path, dataset type, credentials, and load arguments.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/partitioned_and_incremental_datasets.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro_datasets.pandas import CSVDataset\nfrom kedro_datasets.partitions import PartitionedDataset\n\nmy_credentials = {...}  # credentials dictionary\n\nmy_partitioned_dataset = PartitionedDataset(\n    path=\"s3://my-bucket-name/path/to/folder\",\n    dataset=CSVDataset,\n    credentials=my_credentials,\n    load_args={\"load_arg1\": \"value1\", \"load_arg2\": \"value2\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Kedro Project with a Specific Starter Version\nDESCRIPTION: This command creates a new Kedro project using a specific version (0.1.0) of the 'spaceflights-pandas' starter. The --checkout flag can point to a branch, tag, or commit in the starter repository.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/starters/starters.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkedro new --starter=spaceflights-pandas --checkout=0.1.0\n```\n\n----------------------------------------\n\nTITLE: Implementing _submit_job Method for AWSBatchRunner in Python\nDESCRIPTION: This method creates and submits jobs to AWS Batch with specified dependencies, job name, and command. It also tracks the job progress and handles errors.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/aws_batch.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef _submit_job(\n    self,\n    node: Node,\n    node_to_job: Dict[Node, str],\n    node_dependencies: Set[Node],\n    session_id: str,\n) -> Node:\n    self._logger.info(\"Submitting the job for node: %s\", str(node))\n\n    job_name = f\"kedro_{session_id}_{node.name}\".replace(\".\", \"-\")\n    depends_on = [{\"jobId\": node_to_job[dep]} for dep in node_dependencies]\n    command = [\"kedro\", \"run\", \"--nodes\", node.name]\n\n    response = self._client.submit_job(\n        jobName=job_name,\n        jobQueue=self._job_queue,\n        jobDefinition=self._job_definition,\n        dependsOn=depends_on,\n        containerOverrides={\"command\": command},\n    )\n\n    job_id = response[\"jobId\"]\n    node_to_job[node] = job_id\n\n    _track_batch_job(job_id, self._client)  # make sure the job finishes\n\n    return node\n```\n\n----------------------------------------\n\nTITLE: Configuring Versioned Dataset in Kedro Catalog\nDESCRIPTION: YAML configuration for a versioned ImageDataset in Kedro's data catalog, specifying the dataset type, filepath and versioning settings.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/how_to_create_a_custom_dataset.md#2025-04-19_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\npikachu:\n  type: kedro_pokemon.datasets.image_dataset.ImageDataset\n  filepath: data/01_raw/pokemon-images-and-types/images/images/pikachu.png\n  versioned: true\n```\n\n----------------------------------------\n\nTITLE: Registering Model Input Table in YAML for Kedro Project\nDESCRIPTION: This YAML configuration registers the model_input_table dataset in the Kedro catalog. It specifies the dataset type as pandas.ParquetDataset and sets the filepath for saving the data.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/create_a_pipeline.md#2025-04-19_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_input_table:\n  type: pandas.ParquetDataset\n  filepath: data/03_primary/model_input_table.parquet\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Dataset Creation in ThreadRunner\nDESCRIPTION: Example of using the new extra_dataset_patterns argument in ThreadRunner to customize default dataset creation.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/resources/migration.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.runner import ThreadRunner\n\nrunner = ThreadRunner(extra_dataset_patterns={\"{default}\": {\"type\": \"MyCustomDataset\"}})\n```\n\n----------------------------------------\n\nTITLE: Alternative Model Parameters Configuration\nDESCRIPTION: Secondary YAML configuration showing different model parameters for the second pipeline.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/namespaces.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel_options_2:\n  test_size: 0.3\n  random_state: 3\n  features:\n    - d_check_complete\n    - moon_clearance_complete\n    - iata_approved\n    - company_rating\n```\n\n----------------------------------------\n\nTITLE: Listing Datasets in Kedro Catalog\nDESCRIPTION: Python code to list all datasets available in the Kedro project's Data Catalog.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/kedro_and_notebooks.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncatalog.list()\n```\n\n----------------------------------------\n\nTITLE: Configuring SSH Remote Storage Dataset in Kedro\nDESCRIPTION: Example showing how to load a CSV file from a remote location through SSH connection with credentials in Kedro's catalog.yml.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog_yaml_examples.md#2025-04-19_snippet_18\n\nLANGUAGE: yaml\nCODE:\n```\ncool_dataset:\n  type: pandas.CSVDataset\n  filepath: \"sftp:///path/to/remote_cluster/cool_data.csv\"\n  credentials: cluster_credentials\n```\n\n----------------------------------------\n\nTITLE: Configuring Databricks Job Parameters for Kedro Project\nDESCRIPTION: This snippet shows the command-line parameters to be entered in the 'Parameters' field when configuring a Databricks job for a Kedro project. It specifies the configuration source location on DBFS.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_deployment_workflow.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n[\"--conf-source\", \"/dbfs/FileStore/iris_databricks/conf\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Prefect API URL for Local Instance\nDESCRIPTION: Sets the Prefect API URL configuration to point to a local Prefect instance running on port 4200.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/prefect.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nprefect config set PREFECT_API_URL=\"http://127.0.0.1:4200/api\"\n```\n\n----------------------------------------\n\nTITLE: Updating Dataset Layer Configuration in YAML\nDESCRIPTION: Example showing the movement of the layer attribute to metadata.kedro-viz in catalog.yml configuration.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/resources/migration.md#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ncompanies:\n  type: pandas.CSVDataSet\n  filepath: data/01_raw/companies.csv\n  layer: raw\n```\n\nLANGUAGE: yaml\nCODE:\n```\ncompanies:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/companies.csv\n  metadata:\n    kedro-viz:\n      layer: raw\n```\n\n----------------------------------------\n\nTITLE: Starting JupyterLab with Kedro\nDESCRIPTION: This command launches a JupyterLab server integrated with the Kedro project, providing an enhanced development environment.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/features/steps/test_starter/{{ cookiecutter.repo_name }}/README.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkedro jupyter lab\n```\n\n----------------------------------------\n\nTITLE: Packaging Kedro Project for Airflow Deployment\nDESCRIPTION: Shell command to package the Kedro project as a Python wheel file for installation in an Airflow container.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/airflow.md#2025-04-19_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nkedro package\n```\n\n----------------------------------------\n\nTITLE: Navigating to Project Directory\nDESCRIPTION: Command to change directory to the newly created spaceflights project root.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/tutorial_template.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd spaceflights\n```\n\n----------------------------------------\n\nTITLE: Changing Default Overriding Environment in Kedro\nDESCRIPTION: Shows how to change the default overriding environment in Kedro by modifying the CONFIG_LOADER_ARGS in settings.py.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/configuration_basics.md#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nCONFIG_LOADER_ARGS = {\"default_run_env\": \"prod\"}\n```\n\n----------------------------------------\n\nTITLE: Interactive Creation of a Spaceflights Project with Kedro Viz\nDESCRIPTION: Interactive CLI session for creating a Kedro project named 'spaceflights' with Kedro Viz (option 7) and example code. The ⮐ symbol represents pressing Enter.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/get_started/new_project.md#2025-04-19_snippet_11\n\nLANGUAGE: text\nCODE:\n```\nkedro new ⮐\nspaceflights ⮐\n7 ⮐\nyes ⮐\n```\n\n----------------------------------------\n\nTITLE: Kedro Run Configuration Example\nDESCRIPTION: YAML configuration file example for kedro run command arguments.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/run_a_pipeline.md#2025-04-19_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nrun:\n  tags: tag1, tag2, tag3\n  pipeline: pipeline1\n  runner: ParallelRunner\n  node_names: node1, node2\n  env: env1\n```\n\n----------------------------------------\n\nTITLE: Configuring Dockerfile for Kedro Integration\nDESCRIPTION: Dockerfile configuration to set up Kedro logging and install the Kedro project package in the Airflow container.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/airflow.md#2025-04-19_snippet_8\n\nLANGUAGE: dockerfile\nCODE:\n```\nENV KEDRO_LOGGING_CONFIG=\"conf/logging.yml\" # This line is not needed from Kedro 0.19.6\n\nRUN pip install --user dist/new_kedro_project-0.1-py3-none-any.whl\n```\n\n----------------------------------------\n\nTITLE: Packaging Kedro Project\nDESCRIPTION: Packages the Kedro application as a .whl file in the dist/ folder and separately packages the project configuration as a tar.gz file. This command is used for preparing the project for deployment.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/commands_reference.md#2025-04-19_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkedro package\n```\n\n----------------------------------------\n\nTITLE: Creating Kedro Project with Config File\nDESCRIPTION: Command to create a new Kedro project using a configuration file that specifies project settings.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/starters/index.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkedro new --config=config.yml\n```\n\n----------------------------------------\n\nTITLE: Creating Model Input Table in Python for Kedro Project\nDESCRIPTION: This function combines preprocessed shuttles, companies, and reviews data to create a model input table. It merges the datasets, drops unnecessary columns, and removes any rows with missing values.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/create_a_pipeline.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef create_model_input_table(\n    shuttles: pd.DataFrame, companies: pd.DataFrame, reviews: pd.DataFrame\n) -> pd.DataFrame:\n    \"\"\"Combines all data to create a model input table.\n\n    Args:\n        shuttles: Preprocessed data for shuttles.\n        companies: Preprocessed data for companies.\n        reviews: Raw data for reviews.\n    Returns:\n        model input table.\n\n    \"\"\"\n    rated_shuttles = shuttles.merge(reviews, left_on=\"id\", right_on=\"shuttle_id\")\n    rated_shuttles = rated_shuttles.drop(\"id\", axis=1)\n    model_input_table = rated_shuttles.merge(\n        companies, left_on=\"company_id\", right_on=\"id\"\n    )\n    model_input_table = model_input_table.dropna()\n    return model_input_table\n```\n\n----------------------------------------\n\nTITLE: Dataset Factory Patterns in YAML Configuration\nDESCRIPTION: Example YAML configuration showing different dataset factory patterns with varying levels of specificity, used to demonstrate the ranking system.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/kedro_dataset_factories.md#2025-04-19_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\n\"{layer}.{dataset_name}\":\n  type: pandas.CSVDataset\n  filepath: data/{layer}/{dataset_name}.csv\n\n\"preprocessed_{dataset_name}\":\n  type: pandas.ParquetDataset\n  filepath: data/02_intermediate/preprocessed_{dataset_name}.parquet\n\n\"processed_{dataset_name}\":\n  type: pandas.ParquetDataset\n  filepath: data/03_primary/processed_{dataset_name}.parquet\n\n\"{dataset_name}_csv\":\n  type: pandas.CSVDataset\n  filepath: data/03_primary/{dataset_name}.csv\n\n\"{namespace}.{dataset_name}_pq\":\n  type: pandas.ParquetDataset\n  filepath: data/03_primary/{dataset_name}_{namespace}.parquet\n\n\"{default_dataset}\":\n  type: pickle.PickleDataset\n  filepath: data/01_raw/{default_dataset}.pickle\n```\n\n----------------------------------------\n\nTITLE: Packaging Kedro Project\nDESCRIPTION: Command to package a Kedro project into a wheel file\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/package_a_project.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nkedro package\n```\n\n----------------------------------------\n\nTITLE: Kedro Pipeline Definition\nDESCRIPTION: Python code defining a Kedro pipeline with multiple nodes for data preprocessing and model input creation.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/kedro_dataset_factories.md#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef create_pipeline(**kwargs) -> Pipeline:\n    return pipeline(\n        [\n            node(\n                func=preprocess_companies,\n                inputs=\"companies\",\n                outputs=\"preprocessed_companies\",\n                name=\"preprocess_companies_node\",\n            ),\n            node(\n                func=preprocess_shuttles,\n                inputs=\"shuttles\",\n                outputs=\"preprocessed_shuttles\",\n                name=\"preprocess_shuttles_node\",\n            ),\n            node(\n                func=create_model_input_table,\n                inputs=[\"preprocessed_shuttles\", \"preprocessed_companies\", \"reviews\"],\n                outputs=\"model_input_table\",\n                name=\"create_model_input_table_node\",\n            ),\n        ]\n    )\n```\n\n----------------------------------------\n\nTITLE: Starting Kedro IPython session\nDESCRIPTION: Command to launch an interactive IPython session with Kedro components loaded, useful for inspecting Delta table metadata and history.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/deltalake_versioning.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nkedro ipython\n```\n\n----------------------------------------\n\nTITLE: Packaging Kedro Micro-Package\nDESCRIPTION: Packages files related to a micro-package (e.g., a modular pipeline) into a Python source distribution file. This command is deprecated and will be removed in Kedro 1.0.0.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/commands_reference.md#2025-04-19_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nkedro micropkg package <package_module_path>\n```\n\n----------------------------------------\n\nTITLE: Kedro Project Creation with Config File\nDESCRIPTION: Command to create a Kedro project using a configuration file\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/starters/new_project_tools.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkedro new --config=<path/to/config.yml>\n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Query Dataset in Kedro\nDESCRIPTION: Example showing how to load data from a SQL database using a custom SQL query with credentials in Kedro's catalog.yml.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog_yaml_examples.md#2025-04-19_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nscooters_query:\n  type: pandas.SQLQueryDataset\n  credentials: scooters_credentials\n  sql: select * from cars where gear=4\n  load_args:\n    index_col: [name]\n```\n\n----------------------------------------\n\nTITLE: Initializing Airflow Project with Astro CLI\nDESCRIPTION: Commands to create and initialize a new Airflow project using Astro CLI.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/airflow.md#2025-04-19_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ncd ..\nmkdir kedro-airflow-spaceflights\ncd kedro-airflow-spaceflights\nastro dev init\n```\n\n----------------------------------------\n\nTITLE: Downgrading Cookiecutter for Rich Compatibility\nDESCRIPTION: Command to install a specific version of Cookiecutter (2.2.0) that doesn't depend on the Rich library. This ensures Kedro can run without Rich while maintaining compatibility.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/logging/index.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install cookiecutter==2.2.0\n```\n\n----------------------------------------\n\nTITLE: ECR Docker Build and Deploy Commands\nDESCRIPTION: Console commands for building and pushing the Docker image to AWS ECR\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/aws_step_functions.md#2025-04-19_snippet_6\n\nLANGUAGE: console\nCODE:\n```\n# build and tag the image\n$ docker build -t spaceflights-step-functions .\n$ docker tag spaceflights-step-functions:latest <your-aws-account-id>.dkr.ecr.<your-aws-region>.amazonaws.com/spaceflights-step-functions:latest\n# login to ECR\n$ aws ecr get-login-password | docker login --username AWS --password-stdin <your-aws-account-id>.dkr.ecr.<your-aws-region>.amazonaws.com\n# push the image to ECR\n$ docker push <your-aws-account-id>.dkr.ecr.<your-aws-region>.amazonaws.com/spaceflights-step-functions:latest\n```\n\n----------------------------------------\n\nTITLE: Parameters Templating with OmegaConfigLoader in YAML\nDESCRIPTION: This example shows how to use OmegaConf variable interpolation in parameters files. The first file contains parameters with placeholders, and the second file provides the template values that will be interpolated.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/advanced_configuration.md#2025-04-19_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_options:\n  test_size: ${data.size}\n  random_state: 3\n```\n\nLANGUAGE: yaml\nCODE:\n```\ndata:\n  size: 0.2\n```\n\n----------------------------------------\n\nTITLE: Reloading Kedro Project in IPython\nDESCRIPTION: IPython magic command to reload a Kedro project, specifying the project root path for initial setup.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/kedro_and_notebooks.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n%reload_kedro <project_root>\n```\n\n----------------------------------------\n\nTITLE: Catalog Templating with OmegaConfigLoader in YAML\nDESCRIPTION: This example demonstrates how to use variable interpolation in catalog files. Template values in catalog files need to start with an underscore. The first file contains catalog entries with placeholders, and the second file provides the template values.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/advanced_configuration.md#2025-04-19_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\ncompanies:\n  type: ${_pandas.type}\n  filepath: data/01_raw/companies.csv\n```\n\nLANGUAGE: yaml\nCODE:\n```\n_pandas:\n  type: pandas.CSVDataset\n```\n\n----------------------------------------\n\nTITLE: Adding Datasets to KedroDataCatalog in Python\nDESCRIPTION: Shows how to add both datasets and raw data directly to the catalog. Raw data is automatically wrapped in a MemoryDataset.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/kedro_data_catalog.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro_datasets.pandas import CSVDataset\n\nbikes_ds = CSVDataset(filepath=\"../data/01_raw/bikes.csv\")\ncatalog[\"bikes\"] = bikes_ds  # Adding a dataset\ncatalog[\"cars\"] = [\"Ferrari\", \"Audi\"]  # Adding raw data\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating Conda Environment for Kedro and Databricks CLI\nDESCRIPTION: Commands to create a new conda environment, activate it, and install Kedro and the Databricks CLI.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_deployment_workflow.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create --name iris-databricks python=3.10\n```\n\nLANGUAGE: bash\nCODE:\n```\nconda activate iris-databricks\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install kedro databricks-cli\n```\n\n----------------------------------------\n\nTITLE: Creating a Kedro Project with a Remote VCS Repository Starter\nDESCRIPTION: This command demonstrates how to create a new Kedro project using a starter from a remote Git repository, specifically the 'spaceflights-pandas' starter from the kedro-starters repository.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/starters/starters.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkedro new --starter git+https://github.com/kedro-org/kedro-starters.git --directory spaceflights-pandas\n```\n\n----------------------------------------\n\nTITLE: Creating a Kedro Project with a Starter and Configuration File\nDESCRIPTION: This command creates a new Kedro project using the 'spaceflights-pandas' starter and a custom configuration file. This is useful when the starter requires more configuration than the default mode.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/starters/starters.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkedro new --config=my_kedro_project.yml --starter=spaceflights-pandas\n```\n\n----------------------------------------\n\nTITLE: Updating Templated Values in Catalog Configuration\nDESCRIPTION: YAML diff demonstrating how to update templated values in the catalog configuration file for compatibility with OmegaConfigLoader.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/config_loader_migration.md#2025-04-19_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nraw_boat_data:\n-   type: \"${datasets.spark}\"\n+   type: \"${_datasets.spark}\"\n-   filepath: \"s3a://${bucket_name}/${key_prefix}/raw/boats.csv\"\n+   filepath: \"s3a://${_bucket_name}/${_key_prefix}/raw/boats.csv\"\n    file_format: parquet\n\nraw_car_data:\n-    type: \"${datasets.csv}\"\n+    type: \"${_datasets.csv}\"\n-    filepath: \"s3://${bucket_name}/data/${key_prefix}/raw/cars.csv\"\n+    filepath: \"s3://${_bucket_name}/data/${_key_prefix}/raw/cars.csv\"\n```\n\n----------------------------------------\n\nTITLE: VS Code Launch Configuration for Kedro\nDESCRIPTION: JSON configuration for VS Code launch.json file to enable Kedro debugging with customizable arguments.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/set_up_vscode.md#2025-04-19_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Python: Kedro Run\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"console\": \"integratedTerminal\",\n            \"module\": \"kedro\",\n            \"args\": [\"run\"]\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Concise Table History\nDESCRIPTION: Python code to retrieve a more concise history of an Iceberg table directly from the pyiceberg.table.Table object, displaying snapshot IDs and timestamps.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/iceberg_versioning.md#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nmodel_input_table.table.history()\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-commit\nDESCRIPTION: Command to install pre-commit using pip\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/linting.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install pre-commit\n```\n\n----------------------------------------\n\nTITLE: Launching MLflow UI\nDESCRIPTION: Commands to install MLflow and launch the tracking UI locally\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/mlflow.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n(.venv) $ pip install mlflow\n(.venv) $ mlflow ui --backend-store-uri ./mlflow_runs\n```\n\n----------------------------------------\n\nTITLE: Starting Prefect Agent for Work Queue\nDESCRIPTION: Launches a Prefect Agent that subscribes to a specific work queue within a work pool, enabling it to pull and execute workflow tasks.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/prefect.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nprefect agent start --pool <work_pool_name> --work-queue <work_queue_name>\n```\n\n----------------------------------------\n\nTITLE: Generating Table of Contents for Kedro Nodes and Pipelines Documentation in Markdown\nDESCRIPTION: This code snippet creates a table of contents using Markdown syntax with the toctree directive. It lists various topics related to Kedro nodes and pipelines, setting the maximum depth to 1 for a flat structure.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/index.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n:maxdepth: 1\n\nnodes\npipeline_introduction\nmodular_pipelines\nnamespaces\npipeline_registry\nmicro_packaging\nrun_a_pipeline\nslice_a_pipeline\n```\n```\n\n----------------------------------------\n\nTITLE: Model Parameters Configuration\nDESCRIPTION: YAML configuration for model hyperparameters and features.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n# parameters.yml\n\nmodel_options:\n  test_size: 0.3\n  random_state: 3\n  features:\n    - engines\n    - passenger_capacity\n    - crew\n    - d_check_complete\n    - moon_clearance_complete\n    - iata_approved\n    - company_rating\n    - review_scores_rating\n```\n\n----------------------------------------\n\nTITLE: Installing Kedro-Viz using pip\nDESCRIPTION: This command installs the Kedro-Viz package, which is separate from the standard Kedro installation. Kedro-Viz is used to visualize pipelines in Kedro projects, showing data, nodes, and their connections.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/visualisation/index.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install kedro-viz\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Prompt in YAML for Kedro Starter\nDESCRIPTION: Example of a custom prompt definition in the prompts.yml file for a Kedro starter. This allows customization of the user input process during project creation.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/starters/create_a_starter.md#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ncustom_prompt:\n    title: \"Prompt title\"\n    text: |\n      Prompt description that explains to the user what\n      information they should provide.\n```\n\n----------------------------------------\n\nTITLE: Importing and Running Packaged Kedro Project in Python\nDESCRIPTION: Demonstrates how to import and run a packaged Kedro project from another Python project. This allows for integration of Kedro projects into larger Python applications.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom my_package.__main__ import main\n\nmain(\n    [\"--pipleine\", \"my_pipeline\"]\n)  # or just main() if no parameters are needed for the run\n```\n\n----------------------------------------\n\nTITLE: Creating Compressed Configuration Files for Kedro\nDESCRIPTION: Shows how to create tar.gz and zip files containing Kedro configuration, excluding local files.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/configuration_basics.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ntar --exclude=local/*.yml -czf <my_conf_name>.tar.gz --directory=<path-to-conf-dir> <conf-dir>\n```\n\nLANGUAGE: bash\nCODE:\n```\nzip -x <conf-dir>/local/** -r <my_conf_name>.zip <conf-dir>\n```\n\n----------------------------------------\n\nTITLE: Deleting Modular Pipeline in Kedro\nDESCRIPTION: Deletes all files related to a modular pipeline in the Kedro project. This command helps in managing and removing unnecessary pipelines.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/commands_reference.md#2025-04-19_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nkedro pipeline delete <pipeline_name>\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with pip in Kedro Project\nDESCRIPTION: This command installs the project dependencies listed in the requirements.txt file using pip.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/kedro/templates/project/{{ cookiecutter.repo_name }}/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Updating Templated Values in Catalog Globals\nDESCRIPTION: YAML diff showing how to update templated values in the catalog globals file for compatibility with OmegaConfigLoader.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/config_loader_migration.md#2025-04-19_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n- bucket_name: \"my_s3_bucket\"\n+ _bucket_name: \"my_s3_bucket\" # kedro requires `_` to mark templatable keys\n- key_prefix: \"my/key/prefix/\"\n+ _key_prefix: \"my/key/prefix/\"\n\n- datasets:\n+ _datasets:\n    csv: \"pandas.CSVDataset\"\n    spark: \"spark.SparkDataset\"\n```\n\n----------------------------------------\n\nTITLE: Bulk Micro-package Configuration in pyproject.toml\nDESCRIPTION: TOML configuration for bulk packaging multiple micro-packages, specifying package names, aliases, and destinations.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/micro_packaging.md#2025-04-19_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[tool.kedro.micropkg.package]\ncleaning_utils = {alias = \"aliased_util\", destination = \"somewhere/else\", env = \"uat\"}\nsecond_pipeline = {}\n```\n\n----------------------------------------\n\nTITLE: Getting Dataset Count in KedroDataCatalog using Python\nDESCRIPTION: Shows how to get the number of datasets in the catalog using the len() function.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/kedro_data_catalog.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nds_count = len(catalog)\n```\n\n----------------------------------------\n\nTITLE: Launching Jupyter Notebook with Kedro\nDESCRIPTION: Command to start a Jupyter notebook session with Kedro integration from within the project directory.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/kedro_and_notebooks.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkedro jupyter notebook\n```\n\n----------------------------------------\n\nTITLE: Azure ML Pipeline Reference in Markdown\nDESCRIPTION: Markdown documentation referencing the kedro-azureml plugin and its capabilities for Azure ML pipeline integration. Includes links to official documentation and mentions support for distributed training frameworks.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/azure.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Azure ML pipelines\n\n## `kedro-azureml` plugin\n\nFor deployment to Azure ML pipelines, you should [consult the documentation](https://kedro-azureml.readthedocs.io/en/stable/source/03_quickstart.html) for the [`kedro-azureml` plugin](https://github.com/getindata/kedro-azureml) from GetInData | Part of Xebia that enables you to run your code on Azure ML Pipelines in a fully managed fashion.\n\nThe plugin supports both: docker-based workflows and code-upload workflows.\nBesides that, kedro-azureml also supports distributed training in PyTorch/TensorFlow/MPI and works well with Azure ML native MLflow integration.\n```\n\n----------------------------------------\n\nTITLE: Starting IPython Shell with Kedro\nDESCRIPTION: This command starts an IPython interactive shell session integrated with the Kedro project, allowing for direct interaction with project components.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/features/steps/test_starter/{{ cookiecutter.repo_name }}/README.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nkedro ipython\n```\n\n----------------------------------------\n\nTITLE: Reading Remote Configuration in Kedro\nDESCRIPTION: Demonstrates how to read configuration from remote storage locations using cloud storage protocols in Kedro.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/configuration_basics.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --conf-source=s3://my-bucket/configs/\n```\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --conf-source=abfs://container@account/configs/\n```\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --conf-source=gs://my-bucket/configs/\n```\n\n----------------------------------------\n\nTITLE: Creating Essential Kedro Project Files\nDESCRIPTION: Bash commands to create the essential files for a Kedro project: settings.py and pipeline_registry.py. These files are required for Kedro to recognize and run the project.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/get_started/minimal_kedro_project.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmkdir minikedro\ntouch minikedro/settings.py minikedro/pipeline_registry.py\n```\n\n----------------------------------------\n\nTITLE: Defining a Helper Function for Node Behavior Extension in Python\nDESCRIPTION: A utility function that adds a 'hello' message behavior to a node. This function will be called by various hooks in the examples that follow.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/common_use_cases.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.pipeline.node import Node\n\n\ndef say_hello(node: Node):\n    \"\"\"An extra behaviour for a node to say hello before running.\"\"\"\n    print(f\"Hello from {node.name}\")\n```\n\n----------------------------------------\n\nTITLE: Inspecting the Iceberg Table Structure\nDESCRIPTION: Command to examine the directory structure of the Iceberg table created by the pipeline run, showing both data and metadata files.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/iceberg_versioning.md#2025-04-19_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ntree /tmp/warehouse\n```\n\n----------------------------------------\n\nTITLE: Optimizing Node Creation with Helper Function (Python)\nDESCRIPTION: Introduces a helper function to create input mappings, improving code reusability when creating nodes with multiple inputs.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/nodes.md#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.pipeline import node\n\n\nmapping = lambda x: {k: k for k in x}\n\nuk_reporting_node = node(\n    reporting,\n    inputs=mapping([\"uk_input1\", \"uk_input2\", ...]),\n    outputs=\"uk\",\n)\n\nge_reporting_node = node(\n    reporting,\n    inputs=mapping([\"ge_input1\", \"ge_input2\", ...]),\n    outputs=\"ge\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a New Kedro Project with Example Code and Logging\nDESCRIPTION: Shell command to create a new Kedro project with example code and custom logging tools, which will be prepared for Airflow deployment.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/airflow.md#2025-04-19_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nkedro new --example=yes --name=new-kedro-project --tools=log\n```\n\n----------------------------------------\n\nTITLE: Registering the Azure KeyVault Hook in Kedro settings.py\nDESCRIPTION: Configuration that registers the Azure KeyVault hook in the Kedro project's settings.py file. This enables the hook to be automatically used by the Kedro framework during execution.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/common_use_cases.md#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom my_project.hooks import AzureSecretsHook\n\nHOOKS = (AzureSecretsHook(),)\n```\n\n----------------------------------------\n\nTITLE: Creating and Deploying Databricks Asset Bundles\nDESCRIPTION: Commands to create and deploy Databricks Asset Bundles using kedro-databricks.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_ide_databricks_asset_bundles_workflow.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkedro databricks bundle\n```\n\nLANGUAGE: bash\nCODE:\n```\nkedro databricks deploy\n```\n\n----------------------------------------\n\nTITLE: Creating Telemetry Consent File for Kedro\nDESCRIPTION: Creates a .telemetry file in the Kedro project root to specify consent for usage analytics. This prevents the project from hanging when deployed.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/prefect.md#2025-04-19_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nconsent: true\n```\n\n----------------------------------------\n\nTITLE: Configuring CSV Save Parameters in Kedro\nDESCRIPTION: Example showing how to configure save_args to save data to a CSV file without row names (index) using utf-8 encoding in Kedro's catalog.yml.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog_yaml_examples.md#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ntest_dataset:\n  type: pandas.CSVDataset\n  ...\n  save_args:\n    index: False\n    encoding: \"utf-8\"\n```\n\n----------------------------------------\n\nTITLE: DVC Pipeline Stage Configuration\nDESCRIPTION: YAML configuration defining Kedro pipelines as DVC stages with dependencies and outputs\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/kedro_dvc_versioning.md#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nstages:\n  data_processing:\n    cmd: kedro run --pipeline data_processing\n    deps:\n      - data/01_raw/companies.csv\n      - data/01_raw/reviews.csv\n      - data/01_raw/shuttles.xlsx\n    outs:\n      - data/02_intermediate/preprocessed_companies.parquet\n      - data/02_intermediate/preprocessed_shuttles.parquet\n      - data/03_primary/model_input_table.parquet\n\n  data_science:\n    cmd: kedro run --pipeline data_science\n    deps:\n      - data/03_primary/model_input_table.parquet\n    outs:\n      - data/06_models/regressor.pickle\n```\n\n----------------------------------------\n\nTITLE: Viewing Kedro Project Execution Logs on Databricks\nDESCRIPTION: This code snippet displays the logging output from a successful Kedro project run on Databricks. It shows the model accuracy and completion status of the pipeline execution.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_deployment_workflow.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n...\n2023-06-06 12:56:14,399 - iris_databricks.nodes - INFO - Model has an accuracy of 0.972 on test data.\n2023-06-06 12:56:14,403 - kedro.runner.sequential_runner - INFO - Completed 3 out of 3 tasks\n2023-06-06 12:56:14,404 - kedro.runner.sequential_runner - INFO - Pipeline execution completed successfully.\n```\n\n----------------------------------------\n\nTITLE: Running Kedro Benchmark for Two Specific Commits\nDESCRIPTION: Executes the benchmark for 'runner' patterns between 'main' and 'runners' branches, sampling every 2nd commit. This allows for targeted performance comparison between specific versions.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/kedro_benchmarks/README.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nasv run --bench runner main..runners --step 2\n```\n\n----------------------------------------\n\nTITLE: Kedro Run Command Flow Visualization using Mermaid Sequence Diagram\nDESCRIPTION: This sequence diagram illustrates the control flow and component interactions that occur when executing the 'kedro run' command in Kedro. It shows how the CLI command flows through session creation, hook initialization, pipeline selection, catalog configuration, and pipeline execution with appropriate hook callbacks.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/diagrams/kedro-run.md#2025-04-19_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    title $ kedro run\n\n    participant cli as $ kedro run\n    participant prelude as See kedro-with-project.md for details\n    participant project_cli as Project directory <br> cli.py\n    participant session as KedroSession\n    participant context as KedroContext\n    participant runner as Runner\n    participant hooks as Hook manager\n\n    cli->>prelude: prepare click commands as prelude to this\n    prelude->>project_cli: run\n    project_cli->>session: create KedroSession\n    session->>hooks: initialise hook manager\n    session->>session: run\n    session->>session: load KedroContext\n    session->>session: get the selected pipeline\n    session->>session: filter the pipeline based on command line arguments\n    session->>context: get catalog with load version / save version\n    session->>runner: create runner\n    hooks->>hooks: before_pipeline_run\n    runner->>runner: run the filtered pipeline with the catalog\n    hooks->>hooks: on_pipeline_error (if runner fails)\n    hooks->>hooks: after_pipeline_run\n```\n\n----------------------------------------\n\nTITLE: Kedro Runner Module Structure\nDESCRIPTION: RST documentation structure defining the kedro.runner module contents, including function run_node and runner classes like AbstractRunner, ParallelRunner, SequentialRunner and ThreadRunner.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/api/kedro.runner.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nkedro.runner\n============\n\n.. rubric:: Description\n\n.. automodule:: kedro.runner\n\n   .. rubric:: Functions\n\n   .. autosummary::\n      :toctree:\n\n      kedro.runner.run_node\n\n\n\n\n\n   .. rubric:: Classes\n\n   .. autosummary::\n      :toctree:\n      :template: autosummary/class.rst\n\n      kedro.runner.AbstractRunner\n      kedro.runner.ParallelRunner\n      kedro.runner.SequentialRunner\n      kedro.runner.ThreadRunner\n```\n\n----------------------------------------\n\nTITLE: Checking Shell Type in Bash\nDESCRIPTION: This command is used to determine the type of shell being used, which is necessary for setting up autocompletion.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/commands_reference.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\necho $0\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark CSV Dataset from S3 in YAML\nDESCRIPTION: This YAML configuration demonstrates how to set up a Spark CSV dataset loaded from an S3 bucket using the new kedro.extras system. It specifies the dataset type, filepath using fsspec syntax, credentials, and file format.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_24\n\nLANGUAGE: yaml\nCODE:\n```\nweather:\n  type: spark.SparkDataSet  # Observe the specified type, this  affects all datasets\n  filepath: s3a://your_bucket/data/01_raw/weather*  # filepath uses fsspec to indicate the file storage system\n  credentials: dev_s3\n  file_format: csv\n```\n\n----------------------------------------\n\nTITLE: Creating a Kedro Project with an Aliased Starter\nDESCRIPTION: This command creates a new Kedro project using the 'spaceflights-pandas' starter, which is an alias provided by the Kedro team for convenience.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/starters/starters.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkedro new --starter=spaceflights-pandas\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Batch Parameters in YAML\nDESCRIPTION: This YAML configuration sets up the AWS Batch-related parameters including job queue, job definition, and maximum number of workers.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/aws_batch.md#2025-04-19_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\naws_batch:\n  job_queue: \"spaceflights_queue\"\n  job_definition: \"kedro_run\"\n  max_workers: 2\n```\n\n----------------------------------------\n\nTITLE: Running Databricks Job with CLI\nDESCRIPTION: Command to run the deployed Databricks job using the databricks CLI.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_ide_databricks_asset_bundles_workflow.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndatabricks bundle run\n```\n\n----------------------------------------\n\nTITLE: Listing Registered Kedro Pipelines\nDESCRIPTION: Lists all registered pipelines in the Kedro project. This command provides an overview of available pipelines in the project.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/commands_reference.md#2025-04-19_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nkedro registry list\n```\n\n----------------------------------------\n\nTITLE: Starting Jupyter Notebook with Kedro\nDESCRIPTION: This command launches a Jupyter notebook server integrated with the Kedro project, providing access to project-specific variables and data.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/features/steps/test_starter/{{ cookiecutter.repo_name }}/README.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkedro jupyter notebook\n```\n\n----------------------------------------\n\nTITLE: Accessing Delta Table version history\nDESCRIPTION: Python code that retrieves and displays the complete version history of a Delta table, showing operations, timestamps, and metrics for each version.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/deltalake_versioning.md#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nIn [2]: model_input_table.history\nOut [2]:\n[\n    {\n        'timestamp': 1739891304488,\n        'operation': 'WRITE',\n        'operationParameters': {'mode': 'Overwrite'},\n        'operationMetrics': {\n            'execution_time_ms': 8,\n            'num_added_files': 1,\n            'num_added_rows': 6027,\n            'num_partitions': 0,\n            'num_removed_files': 1\n        },\n        'clientVersion': 'delta-rs.0.23.1',\n        'version': 1\n    },\n    {\n        'timestamp': 1739891277424,\n        'operation': 'WRITE',\n        'operationParameters': {'mode': 'Overwrite'},\n        'clientVersion': 'delta-rs.0.23.1',\n        'operationMetrics': {\n            'execution_time_ms': 48,\n            'num_added_files': 1,\n            'num_added_rows': 6027,\n            'num_partitions': 0,\n            'num_removed_files': 0\n        },\n        'version': 0\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: DVC Pipeline with Parameter Tracking\nDESCRIPTION: YAML configuration showing how to track parameters in DVC pipeline stages\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/kedro_dvc_versioning.md#2025-04-19_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nstages:\n  data_science:\n    cmd: kedro run --pipeline data_science\n    deps:\n      - data/03_primary/model_input_table.parquet\n      - src/space_dvc/pipelines/data_science/nodes.py\n      - src/space_dvc/pipelines/data_science/pipeline.py\n    params:\n      - conf/base/parameters_data_science.yaml:\n          - model_options\n    outs:\n      - data/06_models/regressor.pickle\n```\n\n----------------------------------------\n\nTITLE: Running Kedro Pipeline with Hooks\nDESCRIPTION: Command to run the Kedro pipeline after implementing and registering the hooks.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/examples.md#2025-04-19_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n$ kedro run\n```\n\n----------------------------------------\n\nTITLE: Creating a Kedro Project with a Custom Starter\nDESCRIPTION: This command creates a new Kedro project using a custom starter located at a specified path. The --directory flag specifies the target directory for the new project.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/starters/starters.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkedro new --starter=<path-to-starter> --directory <directory>\n```\n\n----------------------------------------\n\nTITLE: Configuring File Open Arguments for Loading with Encoding\nDESCRIPTION: Example showing how to configure fs_args.open_args_load to specify the mode and encoding when opening a binary file for reading.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog.md#2025-04-19_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ntest_dataset:\n  type: ...\n  fs_args:\n    open_args_load:\n      mode: \"rb\"\n      encoding: \"utf-8\"\n```\n\n----------------------------------------\n\nTITLE: Packaging Kedro Pipeline\nDESCRIPTION: Command to package the Kedro pipeline as a Python package for installation in the Docker container.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/aws_step_functions.md#2025-04-19_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n$ kedro package\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Documentation Table of Contents for Databricks Integration\nDESCRIPTION: Sphinx toctree directive that lists the related Databricks integration documentation files with a maximum depth of 1 level. This structure organizes the documentation for various Databricks workflow options.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/index.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n:maxdepth: 1\n\ndatabricks_notebooks_development_workflow.md\ndatabricks_ide_databricks_asset_bundles_workflow.md\ndatabricks_deployment_workflow\ndatabricks_visualisation\ndatabricks_dbx_workflow.md\n```\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies in Databricks Notebook\nDESCRIPTION: IPython command to install project dependencies from requirements.txt file in the Databricks notebook.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_dbx_workflow.md#2025-04-19_snippet_7\n\nLANGUAGE: ipython\nCODE:\n```\n%pip install -r \"/Workspace/Repos/<databricks_username>/iris-databricks/requirements.txt\"\n```\n\n----------------------------------------\n\nTITLE: Migrating TemplatedConfigLoader to OmegaConfigLoader\nDESCRIPTION: Code diff demonstrating how to update the configuration loading process from TemplatedConfigLoader to OmegaConfigLoader, including changes in method calls and syntax.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/config_loader_migration.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n- conf_path = str(project_path / settings.CONF_SOURCE)\n- conf_loader = TemplatedConfigLoader(conf_source=conf_path, env=\"local\")\n- catalog = conf_loader.get(\"catalog*\")\n\n+ conf_path = str(project_path / settings.CONF_SOURCE)\n+ config_loader = OmegaConfigLoader(conf_source=conf_path, env=\"local\")\n+ catalog = config_loader[\"catalog\"] # note the key accessor syntax\n```\n\n----------------------------------------\n\nTITLE: Train-Test Split Implementation\nDESCRIPTION: Shows how to split data into training and testing sets using scikit-learn\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=test_size, random_state=random_state\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Parameters for Multiple Pipeline Instances in YAML\nDESCRIPTION: Updates the parameters_data_science.yml file with separate model options for active and candidate modeling pipelines, using different feature sets and random states.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/add_another_pipeline.md#2025-04-19_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nactive_modelling_pipeline:\n    model_options:\n      test_size: 0.2\n      random_state: 3\n      features:\n        - engines\n        - passenger_capacity\n        - crew\n        - d_check_complete\n        - moon_clearance_complete\n        - iata_approved\n        - company_rating\n        - review_scores_rating\n\ncandidate_modelling_pipeline:\n    model_options:\n      test_size: 0.2\n      random_state: 8\n      features:\n        - engines\n        - passenger_capacity\n        - crew\n        - review_scores_rating\n```\n\n----------------------------------------\n\nTITLE: Creating Entrypoint Script for Kedro Project\nDESCRIPTION: Python script that serves as the entrypoint for the Kedro project when running on EMR Serverless. It imports the main function from the packaged Kedro project and executes it with command-line arguments.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/amazon_emr_serverless.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport sys\nfrom <PACKAGE_NAME>.__main__ import main\n\nmain(sys.argv[1:])\n```\n\n----------------------------------------\n\nTITLE: Starting a Prefect Server Instance\nDESCRIPTION: Command to start a local Prefect Server instance which will manage and monitor workflow execution.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/prefect.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nprefect server start\n```\n\n----------------------------------------\n\nTITLE: Configuring CSV Dataset with Custom Load/Save Arguments in Kedro\nDESCRIPTION: Example showing how to configure a CSV dataset with specific load and save arguments like separator, date format, and decimal character in Kedro's catalog.yml.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog_yaml_examples.md#2025-04-19_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ncars:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/company/cars.csv\n  load_args:\n    sep: ','\n  save_args:\n    index: False\n    date_format: '%Y-%m-%d %H:%M'\n    decimal: .\n```\n\n----------------------------------------\n\nTITLE: Changing Configuration Source in settings.py\nDESCRIPTION: Demonstrates how to change the default configuration source directory by modifying the CONF_SOURCE variable in the project's settings.py file.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/configuration_basics.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nCONF_SOURCE = \"new_conf\"\n```\n\n----------------------------------------\n\nTITLE: Parameterizing Kedro Node Execution\nDESCRIPTION: Command to run a specific node in a Kedro pipeline by name. This enables individual components to be executed separately in a distributed environment.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/distributed.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --nodes=<node_name>\n```\n\n----------------------------------------\n\nTITLE: Implementing _track_batch_job Helper Function in Python\nDESCRIPTION: This function continuously polls the AWS Batch client for a job's status, raising an exception if the job fails and returning if successful.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/aws_batch.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef _track_batch_job(job_id: str, client: Any) -> None:\n    \"\"\"Continuously poll the Batch client for a job's status,\n    given the job ID. If it ends in FAILED state, raise an exception\n    and log the reason. Return if successful.\n    \"\"\"\n    while True:\n        # we don't want to bombard AWS with the requests\n        # to not get throttled\n        sleep(1.0)\n\n        jobs = client.describe_jobs(jobs=[job_id])[\"jobs\"]\n        if not jobs:\n            raise ValueError(f\"Job ID {job_id} not found.\")\n\n        job = jobs[0]\n        status = job[\"status\"]\n\n        if status == \"FAILED\":\n            reason = job[\"statusReason\"]\n            raise Exception(\n                f\"Job {job_id} has failed with the following reason: {reason}\"\n            )\n\n        if status == \"SUCCEEDED\":\n            return\n```\n\n----------------------------------------\n\nTITLE: Pytest Configuration\nDESCRIPTION: Default Pytest configuration settings in pyproject.toml\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/starters/new_project_tools.md#2025-04-19_snippet_8\n\nLANGUAGE: toml\nCODE:\n```\n[tool.pytest.ini_options]\naddopts = \"\"\"\n--cov-report term-missing \\\n--cov src/{{ cookiecutter.python_package }} -ra\"\"\"\n\n[tool.coverage.report]\nfail_under = 0\nshow_missing = true\nexclude_lines = [\"pragma: no cover\", \"raise NotImplementedError\"]\n```\n\n----------------------------------------\n\nTITLE: Interactive Creation of a Test Project with Multiple Tools\nDESCRIPTION: Interactive CLI session for creating a Kedro project named 'testproject' with linting (1), documentation (4), and PySpark (6), but no example code.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/get_started/new_project.md#2025-04-19_snippet_12\n\nLANGUAGE: text\nCODE:\n```\nkedro new ⮐\ntestproject ⮐\n1,4,6 ⮐\nno ⮐\n```\n\n----------------------------------------\n\nTITLE: Running Ruff Formatting and Checks\nDESCRIPTION: Commands to run ruff for code formatting verification and linting checks\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/linting.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nruff format --check <project_root>\nruff check <project_root>\n```\n\n----------------------------------------\n\nTITLE: Generating Table of Contents for Kedro Project Setup in Markdown\nDESCRIPTION: Creates a table of contents using the toctree directive, linking to sections on dependencies, session management, and settings. The maxdepth parameter is set to 1, indicating a single level of depth for the table of contents.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/kedro_project_setup/index.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n:maxdepth: 1\n\ndependencies\nsession\nsettings\n```\n```\n\n----------------------------------------\n\nTITLE: Package-based Deployment Commands\nDESCRIPTION: Commands for packaging and installing a Kedro project using wheel files. Includes packaging the project and installing it on the target machine.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/single_machine.md#2025-04-19_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkedro package\n```\n\nLANGUAGE: console\nCODE:\n```\npip install <path-to-wheel-file>\n```\n\nLANGUAGE: console\nCODE:\n```\npython -m project_name\n```\n\n----------------------------------------\n\nTITLE: Auto-generated Kedro Catalog YAML Configuration\nDESCRIPTION: This YAML snippet shows an example of an auto-generated catalog configuration file created by the 'kedro catalog create' command. It defines MemoryDataset entries for datasets missing from the DataCatalog.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog_yaml_examples.md#2025-04-19_snippet_26\n\nLANGUAGE: yaml\nCODE:\n```\nrockets:\n  type: MemoryDataset\nscooters:\n  type: MemoryDataset\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Docker Image for EMR Serverless\nDESCRIPTION: Dockerfile for creating a custom image that installs a specific Python version using pyenv, packages the Kedro project, and sets up the necessary environment for EMR Serverless.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/amazon_emr_serverless.md#2025-04-19_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nFROM public.ecr.aws/emr-serverless/spark/emr-6.10.0:latest AS base\n\nUSER root\n\n# Install Python build dependencies for pyenv\nRUN yum install -y gcc make patch zlib-devel bzip2 bzip2-devel readline-devel  \\\n        sqlite sqlite-devel openssl11-devel tk-devel libffi-devel xz-devel tar\n\n# Install git for pyenv installation\nRUN yum install -y git\n\n# Add pyenv to PATH and set up environment variables\nENV PYENV_ROOT /usr/.pyenv\nENV PATH $PYENV_ROOT/shims:$PYENV_ROOT/bin:$PATH\nENV PYTHON_VERSION=3.9.16\n\n# Install pyenv, initialize it, install desired Python version and set as global\nRUN curl https://pyenv.run | bash\nRUN eval \"$(pyenv init -)\"\nRUN pyenv install ${PYTHON_VERSION} && pyenv global ${PYTHON_VERSION}\n\n# Copy and install packaged Kedro project\nENV KEDRO_PACKAGE <PACKAGE_WHEEL_NAME>\nCOPY dist/$KEDRO_PACKAGE /tmp/dist/$KEDRO_PACKAGE\nRUN pip install --upgrade pip && pip install /tmp/dist/$KEDRO_PACKAGE  \\\n    && rm -f /tmp/dist/$KEDRO_PACKAGE\n\n# Copy and extract conf folder\nADD dist/conf.tar.gz /home/hadoop/\n\n# EMRS will run the image as hadoop\nUSER hadoop:hadoop\n```\n\n----------------------------------------\n\nTITLE: Launching Kedro-Viz visualization in Databricks\nDESCRIPTION: This command launches Kedro-Viz in a new browser tab, allowing users to visualize their Kedro pipeline and metrics. The command generates a clickable link that opens the Kedro-Viz web application.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_visualisation.md#2025-04-19_snippet_2\n\nLANGUAGE: ipython\nCODE:\n```\n%run_viz\n```\n\n----------------------------------------\n\nTITLE: Listing Pipeline Outputs in Kedro\nDESCRIPTION: Python code to list all outputs of the default pipeline in a Kedro project.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/kedro_and_notebooks.md#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npipelines[\"__default__\"].all_outputs()\n```\n\n----------------------------------------\n\nTITLE: Installing Project with Development Dependencies\nDESCRIPTION: Command to install the project including all project-specific dependencies and test requirements.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/automated_testing.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install .\"[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Compiling Project Dependencies for Containerization\nDESCRIPTION: Command to update and compile project requirements before containerization. This ensures all dependencies are properly documented for the Docker image.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/distributed.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip-compile --output-file=<project_root>/requirements.txt --input-file=<project_root>/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Updating Modular Pipeline Parameter Namespace in Python\nDESCRIPTION: Example of how to update the namespace for parameters in a modular pipeline. The pipeline definition remains unchanged, but the parameter structure in the configuration file needs to be updated to include the pipeline namespace.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nactive_pipeline = pipeline(\n    pipe=[\n        node(\n            func=some_func,\n            inputs=[\"model_input_table\", \"params:model_options\"],\n            outputs=[\"**my_output\"],\n        ),\n        ...,\n    ],\n    inputs=\"model_input_table\",\n    namespace=\"candidate_modelling_pipeline\",\n)\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow for Metrics Tracking in Kedro\nDESCRIPTION: Installation command for the MLflow package needed to add metrics tracking to models in Kedro pipelines.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/examples.md#2025-04-19_snippet_11\n\nLANGUAGE: console\nCODE:\n```\npip install mlflow\n```\n\n----------------------------------------\n\nTITLE: Creating a New Kedro Project with Visualization Tools\nDESCRIPTION: Command to create a new Kedro project named 'spaceflights' with visualization tools and example data.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/kedro_and_notebooks.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkedro new -n spaceflights --tools=viz --example=yes\n```\n\n----------------------------------------\n\nTITLE: Accessing InspectTable Object\nDESCRIPTION: Python code to create an InspectTable object from the PyIcebergDataset instance for detailed table inspection.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/iceberg_versioning.md#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ninspect_table = model_input_table.inspect()\n```\n\n----------------------------------------\n\nTITLE: Retrieving Pipeline Inputs and Outputs in Kedro\nDESCRIPTION: This code demonstrates how to use the 'inputs()' and 'outputs()' methods to check the inputs and outputs of a Kedro pipeline. It shows examples for the variance_pipeline.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/pipeline_introduction.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nvariance_pipeline.inputs()\n\nvariance_pipeline.outputs()\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-commit Git Hooks\nDESCRIPTION: Command to install pre-commit git hook scripts\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/linting.md#2025-04-19_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Generating Sphinx Class Documentation Template with Jinja2\nDESCRIPTION: This Jinja2 template creates structured Sphinx documentation for Python classes. It automatically includes members, undocumented members, and inherited members. The template also conditionally renders attribute and method sections with autosummary blocks for each item.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/_templates/autosummary/class.rst#2025-04-19_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ fullname | escape | underline }}\n\n.. currentmodule:: {{ module }}\n\n.. autoclass:: {{ objname }}\n   :members:\n   :undoc-members:\n   :inherited-members:\n\n   {% block attributes %}\n   {% if attributes %}\n   .. rubric:: Attributes\n\n   .. autosummary::\n   {% for item in attributes %}\n      ~{{ name }}.{{ item }}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n\n   {% block methods %}\n   {% if methods %}\n   .. rubric:: Methods\n\n   .. autosummary::\n   {% for item in all_methods %}\n      {%- if not item.startswith('_') %}\n      ~{{ name }}.{{ item }}\n      {%- endif -%}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n\n```\n\n----------------------------------------\n\nTITLE: Installing Kedro Starter with Git Alternative\nDESCRIPTION: Workaround command for installing a Kedro starter project when Git is not installed. This command directly downloads a starter package from GitHub without requiring Git.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/faq/faq.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkedro new -s https://github.com/kedro-org/kedro-starters/archive/0.18.6.zip --directory=pandas-iris\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 Credentials in Catalog\nDESCRIPTION: YAML configuration showing how to add credentials and filesystem arguments for S3 storage in Kedro's data catalog.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/how_to_create_a_custom_dataset.md#2025-04-19_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\npikachu:\n  type: kedro_pokemon.datasets.image_dataset.ImageDataset\n  filepath: s3://data/01_raw/pokemon-images-and-types/images/images/pikachu.png\n  credentials: <your_credentials>\n  fs_args:\n    arg_1: <value>\n```\n\n----------------------------------------\n\nTITLE: Creating New Kedro Project\nDESCRIPTION: Command to create a new Kedro project using the databricks-iris starter template.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_notebooks_development_workflow.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkedro new --starter=databricks-iris\n```\n\n----------------------------------------\n\nTITLE: Example Kedro Run Test in Python\nDESCRIPTION: Demonstrates how to programmatically execute a Kedro run using the KedroSession class in a pytest test function.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/automated_testing.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\nfrom kedro.framework.session import KedroSession\nfrom kedro.framework.startup import bootstrap_project\n\nclass TestKedroRun:\n    def test_kedro_run(self):\n        bootstrap_project(Path.cwd())\n\n        with KedroSession.create(project_path=Path.cwd()) as session:\n            assert session.run() is not None\n```\n\n----------------------------------------\n\nTITLE: Packaging Kedro Project for Databricks Deployment\nDESCRIPTION: Command to package the Kedro project, generating a .whl file in the dist directory.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_deployment_workflow.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkedro package\n```\n\n----------------------------------------\n\nTITLE: Running a Kedro Project\nDESCRIPTION: Command to execute a Kedro project. This requires at least one pipeline with nodes to be defined and registered in pipeline_registry.py.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/get_started/new_project.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkedro run\n```\n\n----------------------------------------\n\nTITLE: Configuring File Open Arguments for Appending Data\nDESCRIPTION: Example showing how to configure fs_args.open_args_save to specify append mode when saving data to a file instead of overwriting it.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog.md#2025-04-19_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ntest_dataset:\n  type: ...\n  fs_args:\n    open_args_save:\n      mode: \"a\"\n```\n\n----------------------------------------\n\nTITLE: Creating Initial Version Directory Structure\nDESCRIPTION: Console commands to set up the initial version directory structure for a versioned dataset in Kedro.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/how_to_create_a_custom_dataset.md#2025-04-19_snippet_10\n\nLANGUAGE: console\nCODE:\n```\n$ mv data/01_raw/pokemon-images-and-types/images/images/pikachu.png data/01_raw/pokemon-images-and-types/images/images/pikachu.png.backup\n$ mkdir -p data/01_raw/pokemon-images-and-types/images/images/pikachu.png/2020-02-22T00.00.00.000Z/\n$ mv data/01_raw/pokemon-images-and-types/images/images/pikachu.png.backup data/01_raw/pokemon-images-and-types/images/images/pikachu.png/2020-02-22T00.00.00.000Z/pikachu.png\n```\n\n----------------------------------------\n\nTITLE: Creating Prefect Work Pool and Queue\nDESCRIPTION: Commands to create a work pool to organize work and a work queue for the agent to pull from. These are required for Prefect's task execution architecture.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/prefect.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nprefect work-pool create --type prefect-agent <work_pool_name>\nprefect work-queue create --pool <work_pool_name> <work_queue_name>\n```\n\n----------------------------------------\n\nTITLE: Configuring Pickle Dataset in Kedro\nDESCRIPTION: Example showing how to load and save a pickle file from/to a local file system using Kedro's catalog.yml with pickle backend specification.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog_yaml_examples.md#2025-04-19_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nairplanes:\n  type: pickle.PickleDataset\n  filepath: data/06_models/airplanes.pkl\n  backend: pickle\n```\n\n----------------------------------------\n\nTITLE: Installing AWS CDK CLI\nDESCRIPTION: Commands to install the AWS Cloud Development Kit (CDK) CLI using npm and verify the installation.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/aws_step_functions.md#2025-04-19_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ npm install -g aws-cdk\n# to verify that the cdk command has been installed\n$ cdk -h\n```\n\n----------------------------------------\n\nTITLE: Creating a Conda Environment for Kedro Databricks Project\nDESCRIPTION: Creates a new Conda virtual environment named 'iris-databricks' with Python 3.10 for Kedro and Databricks development.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_dbx_workflow.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create --name iris-databricks python=3.10\n```\n\n----------------------------------------\n\nTITLE: Install Project Requirements\nDESCRIPTION: Command to install project dependencies\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/starters/new_project_tools.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Listing Kedro Starter Aliases\nDESCRIPTION: This command lists all the starter aliases supported by Kedro, allowing users to see available options for quick project creation.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/starters/starters.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkedro starter list\n```\n\n----------------------------------------\n\nTITLE: Kedro Run with Dask Environment\nDESCRIPTION: Command to run Kedro pipeline with Dask environment configuration and DaskRunner.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/dask.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --env=dask --runner=kedro_tutorial.runner.DaskRunner\n```\n\n----------------------------------------\n\nTITLE: Kedro Project Creation with Tools\nDESCRIPTION: Command to create a new Kedro project with specified tools\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/starters/new_project_tools.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkedro new --tools=<your tool selection>\n```\n\n----------------------------------------\n\nTITLE: Running Kedro pipeline to a specific output\nDESCRIPTION: Command to run the Kedro pipeline up to the model_input_table output, which generates a new version of this Delta table dataset.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/deltalake_versioning.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --to-outputs=model_input_table\n```\n\n----------------------------------------\n\nTITLE: Updating Pipeline Package Version Command\nDESCRIPTION: Shows the migration from the deprecated kedro pipeline package command to the new kedro micropkg package command. Version can be set via __init__.py.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nkedro micropkg package\n```\n\n----------------------------------------\n\nTITLE: Updating Parameter Configuration for Modular Pipelines\nDESCRIPTION: Example of how to update the parameter configuration in YAML format for a modular pipeline. The parameters are now nested under the pipeline namespace.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\ncandidate_modelling_pipeline:\n    model_options:\n      test_size: 0.2\n      random_state: 8\n      features:\n        - engines\n        - passenger_capacity\n        - crew\n```\n\n----------------------------------------\n\nTITLE: Creating a Kedro project with spaceflights-pandas starter\nDESCRIPTION: Command to initialize a new Kedro project using the spaceflights-pandas starter template, which provides example pipelines and datasets to work with.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/deltalake_versioning.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkedro new --starter spaceflights-pandas\n```\n\n----------------------------------------\n\nTITLE: Managing Astro Airflow Deployment\nDESCRIPTION: Commands for starting Astro development environment and copying results from Docker container.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/airflow.md#2025-04-19_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\ncd kedro-airflow-spaceflights\nastro dev start\n```\n\n----------------------------------------\n\nTITLE: Creating Modular Pipeline in Kedro\nDESCRIPTION: Creates a new modular pipeline in the Kedro project. This command sets up the necessary file structure for a modular pipeline.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/commands_reference.md#2025-04-19_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nkedro pipeline create <pipeline_name>\n```\n\n----------------------------------------\n\nTITLE: Configuring API Documentation with autosummary\nDESCRIPTION: RST markup for generating API documentation using autosummary directive to recursively document the Kedro package.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/index.rst#2025-04-19_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. autosummary::\n   :toctree: api\n   :caption: API documentation\n   :template: autosummary/module.rst\n   :recursive:\n\n   kedro\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies with Dev Tools\nDESCRIPTION: Command to install the project with development dependencies including linting tools\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/linting.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install .\"[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Kedro Project Configuration File Structure\nDESCRIPTION: Example YAML configuration file that defines project settings including output directory, project name, repository name, and Python package name.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/starters/index.md#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\noutput_dir: ~/code\nproject_name: My First Kedro Project\nrepo_name: testing-kedro\npython_package: test_kedro\n```\n\n----------------------------------------\n\nTITLE: Defining a pipeline with Delta Table operations\nDESCRIPTION: Python code showing a Kedro pipeline definition that uses both SparkDataset and DeltaTableDataset to process and update data with Delta Lake operations.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/deltalake_versioning.md#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\npipeline(\n    [\n        node(\n            func=process_barometer_data, inputs=\"temperature\", outputs=\"weather@spark\"\n        ),\n        node(\n            func=update_meterological_state,\n            inputs=\"weather@delta\",\n            outputs=\"first_operation_complete\",\n        ),\n        node(\n            func=estimate_weather_trend,\n            inputs=[\"first_operation_complete\", \"weather@delta\"],\n            outputs=\"second_operation_complete\",\n        ),\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Minimal Kedro Project Structure\nDESCRIPTION: Shows the simplified directory structure when creating a new Kedro project with no additional tools selected.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/get_started/kedro_concepts.md#2025-04-19_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nproject-dir          # Parent directory of the template\n├── conf             # Project configuration files\n├── notebooks        # Project-related Jupyter notebooks (can be used for experimental code before moving the code to src)\n├── src              # Project source code\n├── .gitignore       # Hidden file that prevents staging of unnecessary files to `git`\n├── pyproject.toml   # Identifies the project root and contains configuration information\n├── README.md        # Project README\n├── requirements.txt # Project dependencies file\n```\n\n----------------------------------------\n\nTITLE: Configuring Pre-commit Hooks\nDESCRIPTION: Sample pre-commit configuration file for setting up ruff hooks\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/linting.md#2025-04-19_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nrepos:\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    # Ruff version.\n    rev: '' # Replace with latest version, for example 'v0.1.8'\n    hooks:\n      - id: ruff\n        args: [--fix]\n      - id: ruff-format\n```\n\n----------------------------------------\n\nTITLE: Disabling Telemetry by Modifying .telemetry File\nDESCRIPTION: This YAML configuration should be placed in a .telemetry file in the root of your Kedro project to disable telemetry collection. Setting consent to false prevents the collection of anonymous usage data.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/telemetry.md#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nconsent: false\n```\n\n----------------------------------------\n\nTITLE: Remote Code Synchronization Command\nDESCRIPTION: SCP command to synchronize project files between local and remote machines for debugging.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/set_up_vscode.md#2025-04-19_snippet_8\n\nLANGUAGE: console\nCODE:\n```\nscp -r <project_root> <your_username>@<remote_server>:projects/\n```\n\n----------------------------------------\n\nTITLE: Model Training Implementation\nDESCRIPTION: Implements linear regression model training using scikit-learn, including feature selection and train-test split.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Model training\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\nX = model_input_table[\n    [\n        \"engines\",\n        \"passenger_capacity\",\n        \"crew\",\n        \"d_check_complete\",\n        \"moon_clearance_complete\",\n        \"iata_approved\",\n        \"company_rating\",\n        \"review_scores_rating\",\n    ]\n]\ny = model_input_table[\"price\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=3)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nmodel.predict(X_test)\n```\n\n----------------------------------------\n\nTITLE: Running Kedro Pipeline with Node Filters\nDESCRIPTION: Executes a Kedro pipeline run, filtering nodes from 'split' to 'predict' and 'report'. This command demonstrates how to combine multiple CLI options to customize pipeline execution.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/commands_reference.md#2025-04-19_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --from-nodes=split --to-nodes=predict,report\n```\n\n----------------------------------------\n\nTITLE: Setting Nested Parameters via CLI\nDESCRIPTION: Example of overriding nested parameter values through the Kedro CLI using dot notation.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --params=\"model.model_tuning.booster:gbtree\"\n```\n\n----------------------------------------\n\nTITLE: Setting Windows PYTHONPATH Environment Variable\nDESCRIPTION: Command to set the PYTHONPATH environment variable in Windows to include the project source directory.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/set_up_vscode.md#2025-04-19_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nPYTHONPATH=C:/path/to/project/src;%PYTHONPATH%\n```\n\n----------------------------------------\n\nTITLE: Installing project dependencies\nDESCRIPTION: Command to install all the required dependencies listed in the requirements.txt file, including the Delta Lake support packages.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/deltalake_versioning.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Loading Kedro IPython Extension\nDESCRIPTION: Loads the Kedro IPython extension for notebook integration.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_notebooks_development_workflow.md#2025-04-19_snippet_12\n\nLANGUAGE: ipython\nCODE:\n```\n%load_ext kedro.ipython\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic Kedro Project with kedro new\nDESCRIPTION: The basic command to create a new Kedro project in your preferred directory. This initiates an interactive CLI that guides you through project configuration.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/get_started/new_project.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkedro new\n```\n\n----------------------------------------\n\nTITLE: Regenerating the Model Input Table\nDESCRIPTION: Command to re-run a specific part of the pipeline to generate a new version of the model_input_table dataset.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/iceberg_versioning.md#2025-04-19_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --to-outputs=model_input_table\n```\n\n----------------------------------------\n\nTITLE: Creating a New Kedro Project with Databricks-Iris Starter\nDESCRIPTION: Initializes a new Kedro project using the databricks-iris starter template, which provides a foundation for Databricks integration.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_dbx_workflow.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkedro new --starter=databricks-iris\n```\n\n----------------------------------------\n\nTITLE: Generating API Documentation\nDESCRIPTION: Command to generate API documentation from source code\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/package_a_project.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nsphinx-apidoc --module-first -o source ../src/<package_name>\n```\n\n----------------------------------------\n\nTITLE: Implementing _instantiate_runner Helper Function in Python\nDESCRIPTION: This helper function instantiates the runner class with the appropriate parameters, including AWS Batch specific settings if applicable.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/aws_batch.md#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef _instantiate_runner(runner, is_async, project_context):\n    runner_class = load_obj(runner, \"kedro.runner\")\n    runner_kwargs = dict(is_async=is_async)\n\n    if runner.endswith(\"AWSBatchRunner\"):\n        batch_kwargs = project_context.params.get(\"aws_batch\") or {}\n        runner_kwargs.update(batch_kwargs)\n\n    return runner_class(**runner_kwargs)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Autocompletion for Z Shell (ZSh)\nDESCRIPTION: This command sets up autocompletion for Kedro commands in Z shell (ZSh). It should be added to the ~/.zshrc file.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/commands_reference.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\neval \"$(_KEDRO_COMPLETE=zsh_source kedro)\"\n```\n\n----------------------------------------\n\nTITLE: Using Fixtures in Kedro Test Functions\nDESCRIPTION: Shows how to use pytest fixtures as function arguments in test methods. Demonstrates fixture injection pattern for test data access.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/test_a_project.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef test_split_data(dummy_data, dummy_parameters):\n        ...\n```\n\n----------------------------------------\n\nTITLE: Checking Kedro Version Command\nDESCRIPTION: Command to check the installed version of Kedro via the command line interface. This simple command displays the current version of Kedro installed in the environment.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/faq/faq.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkedro -V\n```\n\n----------------------------------------\n\nTITLE: Setting Up Autocompletion for Bash\nDESCRIPTION: This command sets up autocompletion for Kedro commands in Bash shell. It should be added to the ~/.bashrc file or run on the command line.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/commands_reference.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\neval \"$(_KEDRO_COMPLETE=bash_source kedro)\"\n```\n\n----------------------------------------\n\nTITLE: Kedro Installation and Project Setup Commands\nDESCRIPTION: Commands for installing Kedro using pip or conda, installing project dependencies, and running the project.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/single_machine.md#2025-04-19_snippet_3\n\nLANGUAGE: console\nCODE:\n```\npip install kedro\n```\n\nLANGUAGE: console\nCODE:\n```\nconda install -c conda-forge kedro\n```\n\nLANGUAGE: console\nCODE:\n```\npip install -r requirements.txt\n```\n\nLANGUAGE: console\nCODE:\n```\nkedro run\n```\n\n----------------------------------------\n\nTITLE: Creating Initial Git Commit\nDESCRIPTION: Commands to stage all files and create the initial commit in the Git repository.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_notebooks_development_workflow.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# add all files to git staging area\ngit add .\n# create the first commit\ngit commit -m \"first commit\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Kedro Project in pyproject.toml\nDESCRIPTION: Example of how to copy configuration keys from .kedro.yml into pyproject.toml for Kedro 0.17.0 migration. Includes package_name, project_name, and project_version settings.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_18\n\nLANGUAGE: toml\nCODE:\n```\n[tools.kedro]\npackage_name = \"<package_name>\"\nproject_name = \"<project_name>\"\nproject_version = \"0.17.0\"\n```\n\n----------------------------------------\n\nTITLE: Installing Cookiecutter Package for Kedro Starter Creation\nDESCRIPTION: Command to install the Cookiecutter package, which is required for creating Kedro starters.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/starters/create_a_starter.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install cookiecutter\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies with pip\nDESCRIPTION: Command to install all project dependencies listed in requirements.txt file.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/kedro_project_setup/dependencies.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Migration Example: Using New pipeline() Constructor\nDESCRIPTION: Shows the updated syntax for creating transformed pipelines using the pipeline() constructor, which replaces Pipeline.transform(). The new approach uses more granular parameters like inputs, outputs, and parameters.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.pipeline import pipeline\n\nresult = pipeline(\n    my_pipeline,\n    inputs={\"input\": \"new_input\"},\n    outputs={\"output\": \"new_output\"},\n    parameters={\"params:x\": \"params:y\"},\n    namespace=\"pre\",\n)\n```\n\n----------------------------------------\n\nTITLE: Running EMR Serverless Job with AWS CLI\nDESCRIPTION: AWS CLI command to start an EMR Serverless job run with Kedro. The command specifies the entry point script, pipeline configuration, and Spark submit parameters including custom Python path settings. Requires application ID and execution role ARN to be specified.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/amazon_emr_serverless.md#2025-04-19_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\naws emr-serverless start-job-run \\\n    --application-id <application-id> \\\n    --execution-role-arn <execution-role-arn> \\\n    --job-driver '{\n        \"sparkSubmit\": {\n            \"entryPoint\": \"<s3-path-to-entrypoint-script>\",\n            \"entryPointArguments\": [\"--env\", \"<emr-conf>\", \"--runner\", \"ThreadRunner\", \"--pipeline\", \"<kedro-pipeline-name>\"],\n            \"sparkSubmitParameters\": \"--conf spark.emr-serverless.driverEnv.PYSPARK_DRIVER_PYTHON=/usr/.pyenv/versions/3.9.16/bin/python --conf spark.emr-serverless.driverEnv.PYSPARK_PYTHON=/usr/.pyenv/versions/3.9.16/bin/python --conf spark.executorEnv.PYSPARK_PYTHON=/usr/.pyenv/versions/3.9.16/bin/python\"\n        }\n    }'\n```\n\n----------------------------------------\n\nTITLE: Installing Kedro with OmegaConf Support\nDESCRIPTION: Commands to install Kedro version 0.18.5 or later, which includes OmegaConf as a dependency for OmegaConfigLoader.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/config_loader_migration.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install kedro==0.18.5\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install -U kedro\n```\n\n----------------------------------------\n\nTITLE: Installing Pillow for Image Processing in Kedro\nDESCRIPTION: Command to install the Pillow library which provides image processing functionality for the custom dataset.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/how_to_create_a_custom_dataset.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install Pillow\n```\n\n----------------------------------------\n\nTITLE: Handling DatasetNotFoundError: Dataset not found in the catalog in Python\nDESCRIPTION: This snippet demonstrates an error message that appears when a specified dataset is not found in the Kedro catalog. It suggests checking the catalog.yml file for any changes or inconsistencies.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/spaceflights_tutorial_faqs.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nDatasetNotFoundError: Dataset 'companies' not found in the catalog\n```\n\n----------------------------------------\n\nTITLE: Updating Import Statements for OmegaConfigLoader\nDESCRIPTION: Code diff demonstrating the change in import statements from ConfigLoader to OmegaConfigLoader.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/config_loader_migration.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n- from kedro.config import ConfigLoader\n\n+ from kedro.config import OmegaConfigLoader\n```\n\n----------------------------------------\n\nTITLE: Adding Persistent Storage for Intermediate Datasets in Airflow Configuration\nDESCRIPTION: YAML configuration that defines a catch-all pattern using Dataset Factories to convert memory datasets to persistent CSV files. This ensures intermediate datasets are preserved and accessible across different Airflow tasks.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/airflow.md#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n\"{base_dataset}\":\n  type: pandas.CSVDataset\n  filepath: data/02_intermediate/{base_dataset}.csv\n```\n\n----------------------------------------\n\nTITLE: Dockerfile for AWS Lambda-compatible Kedro Image\nDESCRIPTION: Dockerfile to create a custom Docker image that complies with AWS Lambda requirements and includes the Kedro pipeline.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/aws_step_functions.md#2025-04-19_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Define global args\nARG FUNCTION_DIR=\"/home/app/\"\nARG RUNTIME_VERSION=\"3.9\"\n\n# Stage 1 - bundle base image + runtime\n# Grab a fresh copy of the image and install GCC\nFROM python:${RUNTIME_VERSION}-buster as build-image\n\n# Install aws-lambda-cpp build dependencies\nRUN apt-get update && \\\n  apt-get install -y \\\n  g++ \\\n  make \\\n  cmake \\\n  unzip \\\n  libcurl4-openssl-dev\n\n# Include global args in this stage of the build\nARG FUNCTION_DIR\nARG RUNTIME_VERSION\n# Create the function directory\nRUN mkdir -p ${FUNCTION_DIR}\nRUN mkdir -p ${FUNCTION_DIR}/{conf}\n# Add handler function\nCOPY lambda_handler.py ${FUNCTION_DIR}\n# Add conf/ directory\nCOPY conf ${FUNCTION_DIR}/conf\n# Install Kedro pipeline\nCOPY dist/spaceflights_step_functions-0.1-py3-none-any.whl .\nRUN python${RUNTIME_VERSION} -m pip install --no-cache-dir spaceflights_step_functions-0.1-py3-none-any.whl --target ${FUNCTION_DIR}\n# Install Lambda Runtime Interface Client for Python\nRUN python${RUNTIME_VERSION} -m pip install --no-cache-dir awslambdaric --target ${FUNCTION_DIR}\n\n# Stage 3 - final runtime image\n# Grab a fresh copy of the Python image\nFROM python:${RUNTIME_VERSION}-buster\n# Include global arg in this stage of the build\nARG FUNCTION_DIR\n# Set working directory to function root directory\nWORKDIR ${FUNCTION_DIR}\n```\n\n----------------------------------------\n\nTITLE: Creating a New Kedro Project with Telemetry Disabled\nDESCRIPTION: This command creates a new Kedro project with telemetry disabled by setting the --telemetry flag to 'no'. This will create a .telemetry file in the project root with consent set to false.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/telemetry.md#2025-04-19_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nkedro new --telemetry=no\n```\n\n----------------------------------------\n\nTITLE: Ruff Linting Configuration\nDESCRIPTION: Default Ruff linter configuration settings in pyproject.toml\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/starters/new_project_tools.md#2025-04-19_snippet_7\n\nLANGUAGE: toml\nCODE:\n```\nline-length = 88\nshow-fixes = true\nselect = [\n    \"F\",   # Pyflakes\n    \"W\",   # pycodestyle\n    \"E\",   # pycodestyle\n    \"I\",   # isort\n    \"UP\",  # pyupgrade\n    \"PL\",  # Pylint\n    \"T201\", # Print Statement\n]\nignore = [\"E501\"]  # Ruff format takes care of line-too-long\n```\n\n----------------------------------------\n\nTITLE: Creating Conda Environment for Kedro Project\nDESCRIPTION: Commands to create and activate a new Conda environment for the Kedro project with Python 3.10.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_notebooks_development_workflow.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create --name iris-databricks python=3.10\n```\n\n----------------------------------------\n\nTITLE: Installing Kedro Package\nDESCRIPTION: Command to install Kedro using pip in the activated environment.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_notebooks_development_workflow.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install kedro\n```\n\n----------------------------------------\n\nTITLE: Installing Kedro via pip\nDESCRIPTION: Command to install Kedro from the Python Package Index (PyPI) using pip. This is the standard method for installing Kedro in a Python environment.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install kedro\n```\n\n----------------------------------------\n\nTITLE: Installing Pipeline Dependencies\nDESCRIPTION: Command to install pipeline-specific dependencies\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/modular_pipelines.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Initializing Sphinx Documentation\nDESCRIPTION: Command to set up initial Sphinx documentation structure\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/package_a_project.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsphinx-quickstart docs\n```\n\n----------------------------------------\n\nTITLE: Updating APIDataset Configuration in YAML\nDESCRIPTION: Example showing the restructuring of APIDataset configuration with requests-specific arguments moved under load_args.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/resources/migration.md#2025-04-19_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nus_corn_yield_data:\n  type: api.APIDataSet\n  url: https://quickstats.nass.usda.gov\n  credentials: usda_credentials\n  params:\n    key: SOME_TOKEN\n    format: JSON\n```\n\nLANGUAGE: yaml\nCODE:\n```\nus_corn_yield_data:\n  type: api.APIDataSet\n  url: https://quickstats.nass.usda.gov\n  credentials: usda_credentials\n  load_args:\n    params:\n      key: SOME_TOKEN\n      format: JSON\n```\n\n----------------------------------------\n\nTITLE: Setting Kedro Logging Environment Variable\nDESCRIPTION: Shell command to set the KEDRO_LOGGING_CONFIG environment variable in the MWAA startup script.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/airflow.md#2025-04-19_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nexport KEDRO_LOGGING_CONFIG=\"plugins/logging.yml\"\n```\n\n----------------------------------------\n\nTITLE: Kedro Project Creation with Name\nDESCRIPTION: Command to create a new Kedro project with a specified name\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/starters/new_project_tools.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkedro new --name=spaceflights\n```\n\n----------------------------------------\n\nTITLE: Dask Client Configuration YAML\nDESCRIPTION: YAML configuration for Dask client settings, specifying the address for connecting to a Dask cluster.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/dask.md#2025-04-19_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ndask_client:\n  address: 127.0.0.1:8786\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Kedro Nodes (Python)\nDESCRIPTION: Imports necessary libraries from Kedro and other standard tools to run the code snippets related to Kedro nodes.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/nodes.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.pipeline import *\nfrom kedro.io import *\nfrom kedro.runner import *\n\nimport pickle\nimport os\n```\n\n----------------------------------------\n\nTITLE: Setting Up Airflow-Specific Configuration Directory\nDESCRIPTION: Shell commands to create an Airflow-specific configuration directory and copy the base catalog configuration for customization.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/airflow.md#2025-04-19_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncd new-kedro-project\nmkdir conf/airflow\ncp conf/base/catalog.yml conf/airflow/catalog.yml\n```\n\n----------------------------------------\n\nTITLE: Defining CommandCollection Class in Python for Kedro CLI\nDESCRIPTION: This snippet shows the class declaration for CommandCollection. It appears to be a utility class for managing CLI commands in the Kedro framework, potentially extending or customizing functionality from a base collection type.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/api/kedro.framework.cli.utils.CommandCollection.rst#2025-04-19_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nclass CommandCollection\n```\n\n----------------------------------------\n\nTITLE: Installing PTVSD for Remote Debugging\nDESCRIPTION: Console command to install the PTVSD package required for remote debugging capabilities.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/set_up_vscode.md#2025-04-19_snippet_5\n\nLANGUAGE: console\nCODE:\n```\npython -m pip install --upgrade ptvsd\n```\n\n----------------------------------------\n\nTITLE: Updating .kedro.yml Configuration in YAML\nDESCRIPTION: Example of updating the .kedro.yml file to include project name, version and package name. This is required to fix issues with kedro lint and kedro jupyter notebook convert for projects created with Kedro <=0.16.4.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_21\n\nLANGUAGE: yaml\nCODE:\n```\nproject_name: \"<your_project_name>\"\nproject_version: \"<kedro_version_of_the_project>\"\npackage_name: \"<your_package_name>\"\n```\n\n----------------------------------------\n\nTITLE: Generating Locked Dependencies with pip-compile\nDESCRIPTION: Command to compile requirements.txt into a requirements.lock file with pinned versions using pip-tools.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/kedro_project_setup/dependencies.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip-compile <project_root>/requirements.txt --output-file <project_root>/requirements.lock\n```\n\n----------------------------------------\n\nTITLE: Running Sliced Pipeline from Nodes using Kedro CLI\nDESCRIPTION: This bash command demonstrates how to run a sliced pipeline from specified nodes using the Kedro CLI.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/slice_a_pipeline.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --from-nodes=mean_node\n```\n\n----------------------------------------\n\nTITLE: Displaying Markdown Links for Machine Learning Libraries in Amazon SageMaker\nDESCRIPTION: This snippet shows markdown links to popular machine learning libraries and frameworks supported by Amazon SageMaker, including scikit-learn, XGBoost, TensorFlow, and PyTorch.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/amazon_sagemaker.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n[`scikit-learn`](https://scikit-learn.org/)\n[`XGBoost`](https://xgboost.readthedocs.io/)\n[`TensorFlow`](https://www.tensorflow.org/)\n[`PyTorch`](https://pytorch.org/)\n```\n\n----------------------------------------\n\nTITLE: Updating Parameters Syntax in Kedro CLI\nDESCRIPTION: Example showing the change in parameter passing syntax from colon-separated to equals-separated format when using the kedro run command.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/resources/migration.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --params=param_key1:value1,param_key2:2.0\n```\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --params=param_key1=value1,param_key2=2.0\n```\n\n----------------------------------------\n\nTITLE: Dask Cluster Setup Commands\nDESCRIPTION: Commands for setting up a Dask cluster with scheduler and worker processes on a local machine.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/dask.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ dask-scheduler\nScheduler started at 127.0.0.1:8786\n\n$ PYTHONPATH=$PWD/src dask-worker 127.0.0.1:8786\n$ PYTHONPATH=$PWD/src dask-worker 127.0.0.1:8786\n$ PYTHONPATH=$PWD/src dask-worker 127.0.0.1:8786\n```\n\n----------------------------------------\n\nTITLE: Installing Project Requirements\nDESCRIPTION: Command to install all the project requirements including the recently added PyIceberg package.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/iceberg_versioning.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Creating Tests Directory\nDESCRIPTION: Command to create a new directory for tests in the project root.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/automated_testing.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmkdir <project_root>/tests\n```\n\n----------------------------------------\n\nTITLE: Using Only One Configuration Environment in Kedro\nDESCRIPTION: Demonstrates how to configure Kedro to use only the base configuration environment without any other environments.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/configuration_basics.md#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nCONFIG_LOADER_ARGS = {\"default_run_env\": \"base\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kedro Project in Python\nDESCRIPTION: Code snippet showing how to configure a Kedro project using the configure_project function before creating a KedroSession. This is part of the migration guide for Kedro 0.17.1.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nconfigure_project(metadata.package_name)  # to add\n\nsession = KedroSession.create(metadata.package_name, path)\n```\n\n----------------------------------------\n\nTITLE: Creating Secondary Data Science Pipeline\nDESCRIPTION: Demonstrates how to create a second data science pipeline with namespace isolation.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/namespaces.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.pipeline import Pipeline, pipeline\nfrom ..data_science.pipeline import base_data_science\n\ndef create_pipeline() -> Pipeline:\n    return pipeline(\n        base_data_science,\n        namespace = \"ds_2\",\n        parameters={\"params:model_options\": \"params:model_options_2\"},\n        inputs={\"model_input_table\"},\n    )\n```\n\n----------------------------------------\n\nTITLE: Minimal pyproject.toml Configuration for Kedro\nDESCRIPTION: A minimal pyproject.toml file configuration for a Kedro project. It defines the package name, project name, Kedro version, and source directory.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/get_started/minimal_kedro_project.md#2025-04-19_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[tool.kedro]\npackage_name = \"minikedro\"\nproject_name = \"minikedro\"\nkedro_init_version = \"0.19.9\"\nsource_dir = \".\"\n```\n\n----------------------------------------\n\nTITLE: Importing Kedro CLI Utilities in Python\nDESCRIPTION: This snippet demonstrates how to import the utilities from the kedro.framework.cli.utils module. It includes functions for command execution, environment options, and other CLI-related utilities.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/api/kedro.framework.cli.utils.rst#2025-04-19_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom kedro.framework.cli.utils import (\n    command_with_verbosity,\n    env_option,\n    find_stylesheets,\n    forward_command,\n    get_pkg_version,\n    python_call,\n    split_string,\n    CommandCollection,\n    KedroCliError\n)\n```\n\n----------------------------------------\n\nTITLE: Updating Databricks Configuration for Existing Cluster\nDESCRIPTION: YAML configuration diff showing how to update databricks.yml to use an existing cluster instead of creating a new one.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_ide_databricks_asset_bundles_workflow.md#2025-04-19_snippet_4\n\nLANGUAGE: diff\nCODE:\n```\n    tasks:\n        - task_key: default\n-          job_cluster_key: default\n+          existing_cluster_id: 0502-***********\n```\n\n----------------------------------------\n\nTITLE: Installing Jupyter for Kedro Project\nDESCRIPTION: This command installs Jupyter for use with the Kedro project.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/kedro/templates/project/{{ cookiecutter.repo_name }}/README.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install jupyter\n```\n\n----------------------------------------\n\nTITLE: Installing DVC Package\nDESCRIPTION: Command to install DVC using pip package manager\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/kedro_dvc_versioning.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dvc\n```\n\n----------------------------------------\n\nTITLE: Loading Kedro Project in Databricks Notebook\nDESCRIPTION: IPython command to load the Kedro project in the Databricks notebook using the reload_kedro line magic.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_dbx_workflow.md#2025-04-19_snippet_9\n\nLANGUAGE: ipython\nCODE:\n```\n%reload_kedro /Workspace/Repos/<databricks_username>/iris-databricks\n```\n\n----------------------------------------\n\nTITLE: Handling FileNotFoundError during Kedro pipeline run in Bash\nDESCRIPTION: This snippet illustrates an error that occurs when attempting to run a Kedro pipeline with missing input datasets. It shows the traceback and suggests ensuring all required input datasets exist before running the pipeline.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/spaceflights_tutorial_faqs.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --pipeline=data_science\n\n2019-10-04 12:36:12,158 - kedro.io.data_catalog - INFO - Loading data from `model_input_table` (CSVDataset)...\n2019-10-04 12:36:12,158 - kedro.runner.sequential_runner - WARNING - There are 3 nodes that have not run.\nYou can resume the pipeline run with the following command:\nkedro run\nTraceback (most recent call last):\n  ...\n  File \"pandas/_libs/parsers.pyx\", line 382, in pandas._libs.parsers.TextReader.__cinit__\n  File \"pandas/_libs/parsers.pyx\", line 689, in pandas._libs.parsers.TextReader._setup_parser_source\nFileNotFoundError: [Errno 2] File b'data/03_primary/model_input_table.csv' does not exist: b'data/03_primary/model_input_table.csv'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  ...\n    raise DatasetError(message) from exc\nkedro.io.core.DatasetError: Failed while loading data from dataset CSVDataset(filepath=data/03_primary/model_input_table.csv, save_args={'index': False}).\n[Errno 2] File b'data/03_primary/model_input_table.csv' does not exist: b'data/03_primary/model_input_table.csv'\n```\n\n----------------------------------------\n\nTITLE: Kedro Project Example Pipeline Selection\nDESCRIPTION: Dialog asking whether to include an example pipeline in the Kedro project. The default is 'no'.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/get_started/new_project.md#2025-04-19_snippet_9\n\nLANGUAGE: text\nCODE:\n```\nWould you like to include an example pipeline? :\n (no):\n```\n\n----------------------------------------\n\nTITLE: Installing ASV Benchmarking Tool for Kedro\nDESCRIPTION: Installs the ASV (Airspeed Velocity) benchmarking tool using pip. This tool is required to run the Kedro benchmarks.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/kedro_benchmarks/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install asv\n```\n\n----------------------------------------\n\nTITLE: Setting Console Dimensions for Rich Logging\nDESCRIPTION: Command to set terminal dimensions for rich logging output formatting.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/logging/index.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport COLUMNS=120 LINES=25\n```\n\n----------------------------------------\n\nTITLE: Negative Unit Test for Split Data Function\nDESCRIPTION: Test case that validates error handling when price data is missing from the input DataFrame.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/test_a_project.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef test_split_data_missing_price():\n    # Arrange\n    dummy_data = pd.DataFrame(\n        {\n            \"engines\": [1, 2, 3],\n            \"crew\": [4, 5, 6],\n            \"passenger_capacity\": [5, 6, 7],\n            # Note the missing price data\n        }\n    )\n\n    dummy_parameters = {\n        \"model_options\": {\n            \"test_size\": 0.2,\n            \"random_state\": 3,\n            \"features\": [\"engines\", \"passenger_capacity\", \"crew\"],\n        }\n    }\n\n    with pytest.raises(KeyError) as e_info:\n        # Act\n        X_train, X_test, y_train, y_test = split_data(dummy_data, dummy_parameters[\"model_options\"])\n\n    # Assert\n    assert \"price\" in str(e_info.value)\n```\n\n----------------------------------------\n\nTITLE: Project Dependencies Requirements File\nDESCRIPTION: List of project dependencies including code quality packages, notebook tooling, testing frameworks, Kedro core dependencies, and data science libraries.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/tutorial_template.md#2025-04-19_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n# code quality packages\nipython~=8.10; python_version >= '3.8'\nruff==0.1.8\n\n# notebook tooling\njupyter~=1.0\njupyterlab_server>=2.11.1\njupyterlab~=3.0\n\n# Pytest + useful extensions\npytest-cov~=3.0\npytest-mock>=1.7.1, <2.0\npytest~=7.2\n\n# Kedro dependencies and datasets to work with different data formats (including CSV, Excel, and Parquet)\nkedro~=0.19.0\nkedro-datasets[pandas-csvdataset, pandas-exceldataset, pandas-parquetdataset]>=3.0\nkedro-telemetry>=0.3.1\nkedro-viz~=6.0 # Visualise pipelines\n\n# For modeling in the data science pipeline\nscikit-learn~=1.0\n```\n\n----------------------------------------\n\nTITLE: Bulk Pull Configuration in pyproject.toml\nDESCRIPTION: TOML configuration for pulling multiple micro-packages, including remote URLs and local paths with custom options.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/micro_packaging.md#2025-04-19_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[tool.kedro.micropkg.pull]\n\"src/dist/first-pipeline-0.1-py3-none-any.tar.gz\" = {}\n\"https://www.url.to/second-pipeline.tar.gz\" = {alias = \"aliased_pipeline\", destination = \"pipelines\", fs-args = \"pipeline_pull_args.yml\"}\n```\n\n----------------------------------------\n\nTITLE: Defining Kedro Configuration Module Structure in reStructuredText\nDESCRIPTION: This snippet defines the documentation structure for the kedro.config module using reStructuredText directives. It includes module description, available classes (AbstractConfigLoader and OmegaConfigLoader), and exceptions (MissingConfigException).\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/api/kedro.config.rst#2025-04-19_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\nkedro.config\n============\n\n.. rubric:: Description\n\n.. automodule:: kedro.config\n\n.. rubric:: Classes\n\n.. autosummary::\n   :toctree:\n   :template: autosummary/class.rst\n\n   kedro.config.AbstractConfigLoader\n   kedro.config.OmegaConfigLoader\n\n.. rubric:: Exceptions\n\n.. autosummary::\n   :toctree:\n   :template: autosummary/class.rst\n\n   kedro.config.MissingConfigException\n```\n\n----------------------------------------\n\nTITLE: Installing Kedro via conda\nDESCRIPTION: Command to install Kedro using the conda package manager. This method is useful for users who prefer conda environments or need specific dependency management.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c conda-forge kedro\n```\n\n----------------------------------------\n\nTITLE: Basic Kedro Project Creation Command\nDESCRIPTION: Basic command to create a new Kedro project\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/starters/new_project_tools.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkedro new\n```\n\n----------------------------------------\n\nTITLE: Installing Project in Development Mode\nDESCRIPTION: Command to install the project in development mode for documentation building\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/package_a_project.md#2025-04-19_snippet_6\n\nLANGUAGE: text\nCODE:\n```\npip install -e ../src\n```\n\n----------------------------------------\n\nTITLE: Loading Kedro IPython Extension in Databricks Notebook\nDESCRIPTION: IPython command to load the Kedro IPython extension in the Databricks notebook.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_dbx_workflow.md#2025-04-19_snippet_8\n\nLANGUAGE: ipython\nCODE:\n```\n%load_ext kedro.ipython\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for Kedro Extension Documentation in Markdown\nDESCRIPTION: A toctree directive in markdown that outlines the documentation for extending Kedro. It includes links to common use cases, plugins, architecture overview, and instructions for creating a starter.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/extend_kedro/index.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n:maxdepth: 1\n\ncommon_use_cases\nplugins\narchitecture_overview\n../starters/create_a_starter\n```\n```\n\n----------------------------------------\n\nTITLE: Set Kedro Logging Config\nDESCRIPTION: Command to set the KEDRO_LOGGING_CONFIG environment variable\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/starters/new_project_tools.md#2025-04-19_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nexport KEDRO_LOGGING_CONFIG=conf/logging.yml\n```\n\n----------------------------------------\n\nTITLE: Executing Kedro Pipeline with Active and Candidate Models\nDESCRIPTION: Log output from running a Kedro pipeline that includes two modeling pipelines (active and candidate). Shows data loading, model evaluation, and task completion status.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/add_another_pipeline.md#2025-04-19_snippet_11\n\nLANGUAGE: plaintext\nCODE:\n```\nINFO     Saving data to 'candidate_modelling_pipeline.regressor' (PickleDataset)...                data_catalog.py:382\nINFO     Completed 7 out of 9 tasks                                                            sequential_runner.py:85\nINFO     Loading data from 'active_modelling_pipeline.regressor' (PickleDataset)...                data_catalog.py:343\nINFO     Loading data from 'active_modelling_pipeline.X_test' (MemoryDataset)...                   data_catalog.py:343\nINFO     Loading data from 'active_modelling_pipeline.y_test' (MemoryDataset)...                   data_catalog.py:343\nINFO     Running node: evaluate_model_node:                                                                node.py:327\n         evaluate_model([active_modelling_pipeline.regressor,active_modelling_pipeline.X_test,active_model\n         ling_pipeline.y_test]) -> None\nINFO     Model has a coefficient R^2 of 0.462 on test data.                                                nodes.py:60\nINFO     Completed 8 out of 9 tasks                                                            sequential_runner.py:85\nINFO     Loading data from 'candidate_modelling_pipeline.regressor' (PickleDataset)...             data_catalog.py:343\nINFO     Loading data from 'candidate_modelling_pipeline.X_test' (MemoryDataset)...                data_catalog.py:343\nINFO     Loading data from 'candidate_modelling_pipeline.y_test' (MemoryDataset)...                data_catalog.py:343\nINFO     Running node: evaluate_model_node:                                                                node.py:327\n         evaluate_model([candidate_modelling_pipeline.regressor,candidate_modelling_pipeline.X_test,candid\n         ate_modelling_pipeline.y_test]) -> None\nINFO     Model has a coefficient R^2 of 0.449 on test data.                                                nodes.py:60\nINFO     Completed 9 out of 9 tasks                                                            sequential_runner.py:85\nINFO     Pipeline execution completed successfully.\n```\n\n----------------------------------------\n\nTITLE: Dataset Path Format Example\nDESCRIPTION: Example showing the format for specifying bucket name in dataset filepath using filesystem protocol.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_26\n\nLANGUAGE: text\nCODE:\n```\ns3://bucket-name/path/to/key.csv\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Documentation Structure with toctree\nDESCRIPTION: RST markup defining the documentation structure using toctree directives to organize content into logical sections including tutorials, configuration, development guides and API documentation.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n   :caption: Learn about Kedro\n\n   introduction/index.md\n   get_started/index.md\n   course/index.md\n```\n\n----------------------------------------\n\nTITLE: Starting Jupyter Notebook Server in Kedro Project\nDESCRIPTION: This command starts a local Jupyter notebook server for the Kedro project.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/kedro/templates/project/{{ cookiecutter.repo_name }}/README.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkedro jupyter notebook\n```\n\n----------------------------------------\n\nTITLE: Using Dataset Factory with Namespaces in YAML\nDESCRIPTION: Example of a dataset factory that handles namespaced datasets, allowing a single configuration entry to represent multiple namespaced versions of a dataset.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/kedro_dataset_factories.md#2025-04-19_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n\"{namespace}.regressor\":\n  type: pickle.PickleDataset\n  filepath: data/06_models/regressor_{namespace}.pkl\n  versioned: true\n```\n\n----------------------------------------\n\nTITLE: Loading Multiple CLI Flags Example\nDESCRIPTION: Example showing how to load multiple Kedro run CLI flags from a configuration file and parametrized pipeline runs.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --config run_config.yml\nkedro run --params param1:value1,param2:value2\n```\n\n----------------------------------------\n\nTITLE: Sequence Diagram for Kedro CLI Command Execution Flow\nDESCRIPTION: A detailed sequence diagram showing how Kedro CLI processes commands, loads plugins, and interacts with different components. The flow starts from the initial 'kedro' command and shows interactions with entry points, plugins, project configuration, and Click framework.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/diagrams/kedro-with-project.md#2025-04-19_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    title $ kedro in directory with Kedro project\n\n    participant kedro as $ kedro\n    participant entrypoint as pyproject.toml <br> kedro = \"kedro.framework.cli:main\"\n    participant init_plugins as Kedro Plugins <br> [project.entry-points.\"kedro.init\"]\n    participant kedro_cli as Kedro CLI <br> global commands <br> info, new, docs, starter\n    participant global_plugins as Kedro Plugins <br> [project.entry-points.\"kedro.global_commands\"]\n    participant pyproject.toml as Current directory <br> pyproject.toml\n    participant project_plugins as Kedro Plugins <br> [project.entry-points.\"kedro.project_commands\"]\n    participant kedro_project as \"Current directory <br> Kedro Project: cli.py\"\n    participant click as \"Click\"\n\n    kedro->>entrypoint: Python calls this\n\n    entrypoint->>init_plugins: load and run all installed plugins\n    entrypoint->>kedro_cli: collect built-in commands\n    entrypoint->>global_plugins: load and collect global plugin commands\n    entrypoint->>pyproject.toml: check current dir for a Kedro project\n    entrypoint->>pyproject.toml: bootstrap the project\n    entrypoint->>entrypoint: add project metadata to the click cli context\n    entrypoint->>project_plugins: load and collect project plugin commands\n    entrypoint->>kedro_project: load and collect project cli commands\n    entrypoint->>click: combine all command collections and run click\n```\n\n----------------------------------------\n\nTITLE: Installing JupyterLab for Kedro Project\nDESCRIPTION: This command installs JupyterLab for use with the Kedro project.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/kedro/templates/project/{{ cookiecutter.repo_name }}/README.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install jupyterlab\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for Kedro Project\nDESCRIPTION: This snippet lists the required Python packages and their version constraints for a Kedro project. It includes IPython, JupyterLab, Notebook, Kedro, and a Kedro dataset plugin for pandas CSV datasets.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/features/steps/test_starter/{{ cookiecutter.repo_name }}/requirements.txt#2025-04-19_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nipython>=8.10\njupyterlab>=3.0\nnotebook\nkedro~={{ cookiecutter.kedro_version }}\nkedro-datasets[pandas-csvdataset]\n```\n\n----------------------------------------\n\nTITLE: Creating New Kedro Spaceflights Project\nDESCRIPTION: Command to generate a new project using the Kedro spaceflights starter template.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/tutorial_template.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkedro new --starter=spaceflights-pandas\n```\n\n----------------------------------------\n\nTITLE: Loading Kedro IPython Extension\nDESCRIPTION: IPython magic command to load the Kedro extension in a Jupyter notebook or other IPython-compatible environment.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/kedro_and_notebooks.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n%load_ext kedro.ipython\n```\n\n----------------------------------------\n\nTITLE: Flowchart Implementation in Mermaid\nDESCRIPTION: A mermaid flowchart diagram showing the project creation process with different tool selection paths. The diagram illustrates three main paths: selecting no tools, specific tools (lint, docs, PySpark), or all tools, followed by the option to include example pipelines.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/starters/new_project_tools.md#2025-04-19_snippet_10\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart TD\n    A[Start] --> B[Enter Project Name: Example Project];\n    B --> C3[Select Tools: None];\n    B --> C1[Select Tools: lint, docs, PySpark];\n    B --> C2[Select Tools: All];\n\n    C1 --> D1[Include Example Pipeline?];\n    C2 --> D2[Include Example Pipeline?];\n    C3 --> D3[Include Example Pipeline?];\n\n    D1 -->|Yes| E1[New Project Created\\nName: Example Project\\nTools: lint, docs, PySpark\\nExample: Yes];\n    D1 -->|No| E2[New Project Created\\nName: Example Project\\nTools: lint, docs, PySpark\\nExample: No];\n\n    D2 -->|Yes| F1[New Project Created\\nName: Example Project\\nTools: All: lint, test, logging, docs, data, PySpark, viz \\nExample: Yes];\n    D2 -->|No| F2[New Project Created\\nName: Example Project\\nTools: All: lint, test, logging, docs, data, PySpark, viz \\nExample: No];\n\n    D3 -->|Yes| G1[New Project Created\\nName: Example Project\\nTools: None\\nExample: Yes];\n    D3 -->|No| G2[New Project Created\\nName: Example Project\\nTools: None\\nExample: No];\n```\n\n----------------------------------------\n\nTITLE: Activating Conda Environment\nDESCRIPTION: Activates the 'iris-databricks' Conda environment to use for development.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_dbx_workflow.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda activate iris-databricks\n```\n\n----------------------------------------\n\nTITLE: Argo Workflow Template for Kedro Pipelines in Jinja\nDESCRIPTION: A Jinja template that defines the Argo Workflow specification structure. It includes container configuration, environment variable setup for AWS credentials, and creates a directed acyclic graph (DAG) of Kedro pipeline nodes as Argo tasks.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/argo.md#2025-04-19_snippet_1\n\nLANGUAGE: jinja\nCODE:\n```\n{# <project_root>/templates/argo_spec.tmpl #}\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: {{ package_name }}-\nspec:\n  entrypoint: dag\n  templates:\n  - name: kedro\n    metadata:\n      labels:\n        {# Add label to have an ability to remove Kedro Pods easily #}\n        app: kedro-argo\n    retryStrategy:\n      limit: 1\n    inputs:\n      parameters:\n      - name: kedro_node\n    container:\n      imagePullPolicy: Always\n      image: {{ image }}\n      env:\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              {# Secrets name #}\n              name: aws-secrets\n              key: access_key_id\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secrets\n              key: secret_access_key\n      command: [kedro]{% raw %}\n      args: [\"run\", \"-n\",  \"{{inputs.parameters.kedro_node}}\"]\n      {% endraw %}\n  - name: dag\n    dag:\n      tasks:\n      {% for task in tasks %}\n      - name: {{ task.name }}\n        template: kedro\n        {% if task.deps %}\n        dependencies:\n        {% for dep in task.deps %}\n          - {{ dep }}\n        {% endfor %}\n        {% endif %}\n        arguments:\n          parameters:\n          - name: kedro_node\n            value: {{ task.node }}\n      {% endfor %}\n```\n\n----------------------------------------\n\nTITLE: Starting IPython Session in Kedro Project\nDESCRIPTION: This command starts an IPython session for the Kedro project.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/kedro/templates/project/{{ cookiecutter.repo_name }}/README.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nkedro ipython\n```\n\n----------------------------------------\n\nTITLE: Creating Virtual Environment with Conda\nDESCRIPTION: Commands to create and activate a Python 3.10 virtual environment using Conda for the Databricks-Iris project.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_ide_databricks_asset_bundles_workflow.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create --name databricks-iris python=3.10\n```\n\nLANGUAGE: bash\nCODE:\n```\nconda activate databricks-iris\n```\n\n----------------------------------------\n\nTITLE: Displaying Markdown Links for kedro-sagemaker Plugin Resources\nDESCRIPTION: This snippet shows markdown links to the GitHub repository and documentation for the kedro-sagemaker plugin, which enables running Kedro pipelines on Amazon SageMaker.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/amazon_sagemaker.md#2025-04-19_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n[GitHub repository for `kedro-sagemaker`](https://github.com/getindata/kedro-sagemaker)\n[documentation](https://kedro-sagemaker.readthedocs.io/)\n```\n\n----------------------------------------\n\nTITLE: Starting JupyterLab in Kedro Project\nDESCRIPTION: This command starts JupyterLab for the Kedro project.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/kedro/templates/project/{{ cookiecutter.repo_name }}/README.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkedro jupyter lab\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for Kedro\nDESCRIPTION: Requirements file that defines the necessary Python packages and their version constraints for a Kedro development environment. Includes IPython 8.10 or higher, JupyterLab 3.0 or higher, notebook package, and a templated Kedro version.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/kedro/templates/project/{{ cookiecutter.repo_name }}/requirements.txt#2025-04-19_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nipython>=8.10\njupyterlab>=3.0\nnotebook\nkedro~={{ cookiecutter.kedro_version }}\n```\n\n----------------------------------------\n\nTITLE: Creating and Navigating to Project Directory\nDESCRIPTION: Bash commands to create a new directory for the Kedro project and navigate into it. This sets up the initial project structure.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/get_started/minimal_kedro_project.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmkdir minikedro\ncd minikiedro\n```\n\n----------------------------------------\n\nTITLE: Installing Kedro Datasets with Delta Lake dependencies\nDESCRIPTION: Installation command for adding kedro-datasets with pandas-deltatabledataset support to requirements.txt, which enables Delta Lake integration in Kedro.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/deltalake_versioning.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkedro-datasets[pandas-deltatabledataset]\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for Kedro Hooks Documentation in Markdown\nDESCRIPTION: This code snippet defines a table of contents for the Kedro hooks documentation using Markdown syntax. It includes links to introduction, common use cases, and examples sections.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/index.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n:maxdepth: 1\n\nintroduction\ncommon_use_cases\nexamples\n```\n```\n\n----------------------------------------\n\nTITLE: Retrieving Iceberg Table History\nDESCRIPTION: Python code to retrieve and display the history of an Iceberg table using the InspectTable object, showing detailed information about snapshots.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/iceberg_versioning.md#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ninspect_table.history()\n```\n\n----------------------------------------\n\nTITLE: Displaying Markdown Link for Amazon SageMaker Documentation\nDESCRIPTION: This snippet shows a markdown link to the official Amazon SageMaker service documentation.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/amazon_sagemaker.md#2025-04-19_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n[official service documentation](https://docs.aws.amazon.com/sagemaker/index.html)\n```\n\n----------------------------------------\n\nTITLE: Setting PYTHONPATH for Debugging\nDESCRIPTION: Configuration of PYTHONPATH environment variable in .env file for debugging Kedro projects.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/set_up_vscode.md#2025-04-19_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nPYTHONPATH=/path/to/project/src:$PYTHONPATH\n```\n\n----------------------------------------\n\nTITLE: Kedro Project Creation with Example Code\nDESCRIPTION: Command to create a new Kedro project with example code\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/starters/new_project_tools.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkedro new --example=y\n```\n\n----------------------------------------\n\nTITLE: Describing Registered Kedro Pipeline\nDESCRIPTION: Describes a registered pipeline in the Kedro project, listing all nodes in the pipeline. If no pipeline name is provided, it describes the __default__ pipeline.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/commands_reference.md#2025-04-19_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nkedro registry describe <pipeline_name>\n```\n\n----------------------------------------\n\nTITLE: Custom Pipeline Template Command\nDESCRIPTION: CLI command for creating a pipeline using a custom template\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/modular_pipelines.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkedro pipeline create <pipeline_name> --template <path_to_template>\n```\n\n----------------------------------------\n\nTITLE: VS Code Deploy Task Configuration\nDESCRIPTION: JSON configuration for VS Code tasks.json to automate code deployment to remote server.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/set_up_vscode.md#2025-04-19_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"label\": \"Deploy\",\n    \"type\": \"shell\",\n    \"command\": \"scp -r <project_root> <your_username>@<remote_server>:projects/\"\n}\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Kedro-Telemetry Plugin\nDESCRIPTION: This command uninstalls the kedro-telemetry plugin, which will prevent telemetry collection. Note that this approach is considered a last resort as it breaks Kedro's dependencies.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/telemetry.md#2025-04-19_snippet_2\n\nLANGUAGE: console\nCODE:\n```\npip uninstall kedro-telemetry\n```\n\n----------------------------------------\n\nTITLE: Git Setup and Project Clone Commands\nDESCRIPTION: Commands for setting up Git on the server, including version verification, user configuration, and project cloning.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/single_machine.md#2025-04-19_snippet_2\n\nLANGUAGE: console\nCODE:\n```\ngit --version\n```\n\nLANGUAGE: console\nCODE:\n```\ngit config --global user.name \"Server\"\ngit config --global user.email \"server@server.com\"\n```\n\nLANGUAGE: console\nCODE:\n```\ngit clone <repository>\n```\n\n----------------------------------------\n\nTITLE: Creating a Kedro Project with Linting, Documentation, and PySpark (Single Line)\nDESCRIPTION: A single-line command to create a Kedro project named 'testproject' with linting, documentation, and PySpark tools, but no example code.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/get_started/new_project.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkedro new --name=testproject --tools=lint,docs,pyspark --example=n\n```\n\n----------------------------------------\n\nTITLE: Adding PyIceberg to Requirements\nDESCRIPTION: Instructions to add the PyIceberg package with PyArrow support to the project's requirements.txt file.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/iceberg_versioning.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npyiceberg[pyarrow]~=0.8.0\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Documentation Tree for Kedro\nDESCRIPTION: RestructuredText configuration block that defines the table of contents tree structure. Sets maximum depth to 1 and includes links to FAQ, glossary, and migration documentation pages.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/resources/index.md#2025-04-19_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n{toctree}\n:maxdepth: 1\n\n../faq/faq\nglossary\nmigration\n\n```\n\n----------------------------------------\n\nTITLE: Basic Dataset Configuration in Kedro\nDESCRIPTION: YAML configuration for defining a pandas CSV dataset in Kedro's catalog\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/kedro_dvc_versioning.md#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ncompanies:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/companies.csv\n```\n\n----------------------------------------\n\nTITLE: Listing DBFS Directory Contents\nDESCRIPTION: Lists contents of the project data directory in DBFS to verify successful data copying.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_notebooks_development_workflow.md#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndbutils.fs.ls(\"dbfs:/FileStore/iris-databricks/data\")\n```\n\n----------------------------------------\n\nTITLE: Building HTML Documentation\nDESCRIPTION: Command to generate HTML documentation from Sphinx source files\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/package_a_project.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake html\n```\n\n----------------------------------------\n\nTITLE: Interactive Creation of a Default Kedro Project\nDESCRIPTION: Interactive CLI session for creating a default Kedro project named 'My-Project' with no tools and no example code. The ⮐ symbol represents pressing Enter.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/get_started/new_project.md#2025-04-19_snippet_10\n\nLANGUAGE: text\nCODE:\n```\nkedro new ⮐\nMy-Project ⮐\nnone ⮐\nno ⮐\n```\n\n----------------------------------------\n\nTITLE: Updating Kedro Micropkg Configuration in TOML\nDESCRIPTION: Example of how to update the Kedro micropkg configuration in pyproject.toml. The keys now include the full module path and are wrapped in double quotes.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_9\n\nLANGUAGE: toml\nCODE:\n```\n[tool.kedro.micropkg.package]\n\"pipelines.data_engineering\" = {destination = \"path/to/here\"}\n\"pipelines.data_science\" = {alias = \"ds\", env = \"local\"}\n\n[tool.kedro.micropkg.pull]\n\"s3://my_bucket/my_pipeline\" = {alias = \"pipelines.aliased_pipeline\"}\n```\n\n----------------------------------------\n\nTITLE: Kedro Project Configuration File\nDESCRIPTION: YAML configuration file for Kedro project creation settings\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/starters/new_project_tools.md#2025-04-19_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n# config.yml\n\n\"project_name\":\n    \"My Project\"\n\n\"repo_name\":\n    \"my-project\"\n\n\"python_package\":\n    \"my project\"\n\n\"tools\":\n    \"lint, test, log, docs, data, pyspark, viz\"\n\n\"example_pipeline\":\n    \"y\"\n```\n\n----------------------------------------\n\nTITLE: Pulling Kedro Micro-Package\nDESCRIPTION: Pulls a micro-package into a Kedro project. This command is deprecated and will be removed in Kedro 1.0.0. It can work with PyPI, local storage, or cloud storage sources.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/commands_reference.md#2025-04-19_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nkedro micropkg pull <link-to-micro-package-sdist-file>\n```\n\n----------------------------------------\n\nTITLE: Installing memory_profiler for Memory Consumption Tracking in Kedro\nDESCRIPTION: Installation command for the memory_profiler package needed to track memory consumption in Kedro pipelines.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/examples.md#2025-04-19_snippet_0\n\nLANGUAGE: console\nCODE:\n```\npip install memory_profiler\n```\n\n----------------------------------------\n\nTITLE: Creating Partitions for Immediate Saving\nDESCRIPTION: Function demonstrating how to create new data partitions for immediate saving using PartitionedDataset.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/partitioned_and_incremental_datasets.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict\nimport pandas as pd\n\n\ndef create_partitions() -> Dict[str, Any]:\n    \"\"\"Create new partitions and save using PartitionedDataset.\n\n    Returns:\n        Dictionary with the partitions to create.\n    \"\"\"\n    return {\n        # create a file \"s3://my-bucket-name/part/foo.csv\"\n        \"part/foo\": pd.DataFrame({\"data\": [1, 2]}),\n        # create a file \"s3://my-bucket-name/part/bar.csv.csv\"\n        \"part/bar.csv\": pd.DataFrame({\"data\": [3, 4]}),\n    }\n```\n\n----------------------------------------\n\nTITLE: Sequence Diagram for Kedro CLI Execution Flow in Non-Kedro Directory\nDESCRIPTION: This diagram shows the sequence of interactions when running the 'kedro' command in a directory without a Kedro project. It illustrates how the entry point loads plugins, collects commands, checks for a project configuration, and ultimately passes control to Click for command execution.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/diagrams/kedro-no-project.md#2025-04-19_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    title $ kedro in directory without Kedro project\n\n    participant kedro as $ kedro\n    participant entrypoint as pyproject.toml <br> kedro = \"kedro.framework.cli:main\"\n    participant init_plugins as Kedro Plugins <br> [project.entry-points.\"kedro.init\"]\n    participant kedro_cli as Kedro CLI <br> global commands <br> info, new, docs, starter\n    participant global_plugins as Kedro Plugins <br> [project.entry-points.\"kedro.global_commands\"]\n    participant pyproject.toml as Current directory <br> pyproject.toml\n    participant click as Click\n\n    kedro->>entrypoint: Python calls this\n\n    entrypoint->>init_plugins: load and run all installed plugins\n    entrypoint->>kedro_cli: collect built-in commands\n    entrypoint->>global_plugins: load and collect global plugin commands\n    entrypoint->>pyproject.toml: check current dir for a Kedro project\n    pyproject.toml-->>entrypoint: not found or missing [tool.kedro]\n    entrypoint->>click: combine all command collections and run click\n```\n\n----------------------------------------\n\nTITLE: Displaying Kedro Starter Project Structure\nDESCRIPTION: Shows the directory structure of a Kedro starter project template, including configurable variables for project and package names.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/starters/create_a_starter.md#2025-04-19_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n{{ cookiecutter.repo_name }}     # Parent directory of the template\n├── conf                         # Project configuration files\n├── data                         # Local project data (not committed to version control)\n├── docs                         # Project documentation\n├── notebooks                    # Project related Jupyter notebooks (can be used for experimental code before moving the code to src)\n├── pyproject.toml               #\n├── README.md                    # Project README\n├── requirements.txt\n└── src                          # Project source code\n    └── {{ cookiecutter.python_package }}\n       ├── __init.py__\n       ├── pipelines\n       ├── pipeline_registry.py\n       ├── __main__.py\n       └── settings.py\n└── tests\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies with pip\nDESCRIPTION: This command installs all the project dependencies listed in the requirements.txt file using pip.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/features/steps/test_starter/{{ cookiecutter.repo_name }}/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running a Kedro Project\nDESCRIPTION: This command runs a Kedro project by calling the run() method of the KedroSession defined in kedro.framework.session.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/commands_reference.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nkedro run\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Extensions\nDESCRIPTION: Python configuration for enabling autodoc and autosummary extensions in Sphinx\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/package_a_project.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nextensions = [\"sphinx.ext.autodoc\", \"sphinx.ext.autosummary\"]\nautosummary_generate = True\n```\n\n----------------------------------------\n\nTITLE: Installing pytest Individually\nDESCRIPTION: Command to install pytest as a standalone package using pip.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/automated_testing.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install pytest\n```\n\n----------------------------------------\n\nTITLE: Testing Dataset Serialization with multiprocessing\nDESCRIPTION: Python code to verify if a dataset is serializable using multiprocessing's ForkingPickler.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/how_to_create_a_custom_dataset.md#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndataset = context.catalog._datasets[\"pokemon\"]\nfrom multiprocessing.reduction import ForkingPickler\n\n# the following call shouldn't throw any errors\nForkingPickler.dumps(dataset)\n```\n\n----------------------------------------\n\nTITLE: Installing Great Expectations for Data Validation in Kedro\nDESCRIPTION: Installation command for the Great Expectations package needed for data validation in Kedro pipelines.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/examples.md#2025-04-19_snippet_4\n\nLANGUAGE: console\nCODE:\n```\npip install great-expectations\n```\n\n----------------------------------------\n\nTITLE: Invoking Kedro CLI from Python\nDESCRIPTION: This command demonstrates how to invoke the Kedro CLI as a Python module, which can be useful in certain scripting scenarios.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/commands_reference.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython -m kedro\n```\n\n----------------------------------------\n\nTITLE: Launching JupyterLab with Kedro Kernel\nDESCRIPTION: This command creates a custom Jupyter kernel that automatically loads the Kedro extension and launches JupyterLab with this kernel selected.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/kedro_and_notebooks.md#2025-04-19_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nkedro jupyter lab\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Virtual Environment Path in VS Code\nDESCRIPTION: Setting up the path for Python virtual environments in VS Code settings.json to enable proper interpreter detection.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/set_up_vscode.md#2025-04-19_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n\"python.venvPath\": \"/path/containing/your/venvs/\"\n```\n\n----------------------------------------\n\nTITLE: Installing Jupyter for Kedro Integration\nDESCRIPTION: This command installs Jupyter, which is necessary for using Jupyter notebooks with Kedro projects.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/features/steps/test_starter/{{ cookiecutter.repo_name }}/README.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install jupyter\n```\n\n----------------------------------------\n\nTITLE: Installing Kedro for TemplatedConfigLoader Migration\nDESCRIPTION: Command to install Kedro version 0.18.13 or later, which includes necessary functionality to replace TemplatedConfigLoader with OmegaConfigLoader.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/config_loader_migration.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install \"kedro>=0.18.13\"\n```\n\n----------------------------------------\n\nTITLE: Embedding YouTube Video with RST Directive\nDESCRIPTION: An embedded YouTube video using the RST directive to showcase the introduction to the Kedro video course. The directive includes the video ID and sets the width to 100%.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/course/index.md#2025-04-19_snippet_0\n\nLANGUAGE: eval-rst\nCODE:\n```\n..  youtube:: DD7JuYKp6BA\n    :width: 100%\n```\n\n----------------------------------------\n\nTITLE: Defining Test Dependencies for Kedro CI\nDESCRIPTION: Specifies the package versions required for running end-to-end tests in CI environment, particularly optimized for Windows. Includes essential packages like behave for testing, pandas for data manipulation, and other utility packages.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/features/windows_reqs.txt#2025-04-19_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\nbehave==1.2.6\npandas~=1.3\npsutil~=7.0\nrequests~=2.32\ntoml~=0.10.1\nPyYAML>=4.2, <7.0\npackaging>=20.0\n```\n\n----------------------------------------\n\nTITLE: Configuring Git Remote and Pushing to GitHub\nDESCRIPTION: Commands to configure Git remote and push the initial commit to GitHub, with options for both HTTPS and SSH.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_notebooks_development_workflow.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# configure a new remote\n# for HTTPS run:\ngit remote add origin https://github.com/<username>/iris-databricks.git\n# or for SSH run:\ngit remote add origin git@github.com:<username>/iris-databricks.git\n\n# verify the new remote URL\ngit remote -v\n\n# push the first commit\ngit push --set-upstream origin main\n```\n\n----------------------------------------\n\nTITLE: Handling Invalid Node Definitions in Kedro Pipelines\nDESCRIPTION: This example demonstrates how Kedro handles an attempt to create a pipeline with an invalid node that has no input and output. It shows the error message generated when trying to create such a pipeline.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/pipeline_introduction.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    pipeline([node(lambda: print(\"!\"), None, None)])\nexcept Exception as e:\n    print(e)\n```\n\n----------------------------------------\n\nTITLE: Launching Qt Console with Kedro Kernel\nDESCRIPTION: This command starts a Qt Console session using the 'spaceflights' Kedro kernel, which supports graphical features like embedded figures.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/kedro_and_notebooks.md#2025-04-19_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\njupyter qtconsole --kernel=spaceflights\n```\n\n----------------------------------------\n\nTITLE: Updating Catalog Configuration for OmegaConfigLoader in Kedro\nDESCRIPTION: Demonstrates how to update the catalog configuration to use the globals resolver with OmegaConfigLoader, replacing the previous TemplatedConfigLoader syntax.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/config_loader_migration.md#2025-04-19_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nraw_boat_data:\n   type: \"${globals:datasets.spark}\"  # nested paths into global dict are allowed\n   filepath: \"s3a://${globals:bucket_name}/${globals:key_prefix}/${globals:folders.raw}/boats.csv\"\n    file_format: parquet\n\nraw_car_data:\n   type: \"${globals:datasets.csv}\"\n   filepath: \"s3://${globals:bucket_name}/data/${globals:key_prefix}/${globals:folders.raw}/${globals:filename,'cars.csv'}\"  # default to 'cars.csv' if the 'filename' key is not found in the global dict\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Template\nDESCRIPTION: A reStructuredText template that generates documentation by escaping the full name, setting the current module context, and auto-documenting the specified object.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/_templates/autosummary/base.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n{{ fullname | escape | underline}}\n\n.. currentmodule:: {{ module }}\n\n.. auto{{ objtype }}:: {{ objname }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Test Dependencies in pyproject.toml\nDESCRIPTION: Adds test dependencies to the project's pyproject.toml file for easy installation of development requirements.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/automated_testing.md#2025-04-19_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[project.optional-dependencies]\ndev = [\n    \"pytest-cov\",\n    \"pytest-mock\",\n    \"pytest\",\n]\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies\nDESCRIPTION: Installs project dependencies from requirements.txt file using pip.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_notebooks_development_workflow.md#2025-04-19_snippet_11\n\nLANGUAGE: ipython\nCODE:\n```\n%pip install -r \"/Workspace/Repos/<databricks_username>/iris-databricks/requirements.txt\"\n```\n\n----------------------------------------\n\nTITLE: Dataset Catalog Configuration\nDESCRIPTION: Example YAML catalog configuration with explicit dataset entries and pattern-based entries for demonstration of catalog resolve command.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/kedro_dataset_factories.md#2025-04-19_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\ncompanies:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/companies.csv\n\nreviews:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/reviews.csv\n\nshuttles:\n  type: pandas.ExcelDataset\n  filepath: data/01_raw/shuttles.xlsx\n  load_args:\n    engine: openpyxl\n\n\"preprocessed_{name}\":\n  type: pandas.ParquetDataset\n  filepath: data/02_intermediate/preprocessed_{name}.parquet\n\n\"{default}\":\n  type: pandas.ParquetDataset\n  filepath: data/03_primary/{default}.parquet\n```\n\n----------------------------------------\n\nTITLE: Adding Pre-commit to Requirements\nDESCRIPTION: Entry to add pre-commit to project requirements.txt file\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/linting.md#2025-04-19_snippet_5\n\nLANGUAGE: text\nCODE:\n```\npre-commit\n```\n\n----------------------------------------\n\nTITLE: Force Checkpoint Configuration - YAML\nDESCRIPTION: YAML configuration example demonstrating how to force a specific checkpoint value for IncrementalDataset processing.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/partitioned_and_incremental_datasets.md#2025-04-19_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nmy_partitioned_dataset:\n  type: partitions.IncrementalDataset\n  path: s3://my-bucket-name/path/to/folder\n  dataset: pandas.CSVDataset\n  checkpoint: 2020-01-01/data.csv\n```\n\n----------------------------------------\n\nTITLE: Importing KedroCLI Class from Kedro CLI Module in Python\nDESCRIPTION: This code snippet shows how to import the KedroCLI class from the kedro.framework.cli.cli module. The KedroCLI class is likely responsible for handling Kedro's command-line interface operations.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/api/kedro.framework.cli.cli.KedroCLI.rst#2025-04-19_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom kedro.framework.cli.cli import KedroCLI\n```\n\n----------------------------------------\n\nTITLE: Installing JupyterLab for Kedro Integration\nDESCRIPTION: This command installs JupyterLab, an advanced interface for Jupyter that can be used with Kedro projects.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/features/steps/test_starter/{{ cookiecutter.repo_name }}/README.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install jupyterlab\n```\n\n----------------------------------------\n\nTITLE: Visualizing Kedro IPython Command Flow with Mermaid Sequence Diagram\nDESCRIPTION: A sequence diagram showing the flow and interactions when executing the 'kedro ipython' command. It illustrates how Kedro bootstraps the project, creates a session, loads the context, and exposes key variables in the IPython environment.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/diagrams/kedro-ipython.md#2025-04-19_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    title $ kedro ipython\n\n    participant cli as $ kedro ipython\n    participant env as Environment variables\n    participant ipython as IPython\n    participant entrypoint as ipython/__init__.py <br> reload_kedro\n    participant project as Kedro project directory\n    participant session as KedroSession\n    participant context as KedroContext\n\n    cli->>cli: Check if IPython is importable\n    cli->>env: Set KEDRO_ENV to the chosen Kedro environment\n    cli->>ipython: Start ipython\n    ipython->>entrypoint: load ipython extension\n    entrypoint->>project: find Kedro project\n    entrypoint->>project: bootstrap the project\n    entrypoint->>entrypoint: remove imported project package modules\n    entrypoint->>session: create a KedroSession\n    entrypoint->>session: load KedroContext\n    entrypoint->>context: get the data catalog\n    entrypoint->>entrypoint: expose session, context, catalog and pipelines variables\n    entrypoint->>entrypoint: register reload_kedro line magic\n```\n\n----------------------------------------\n\nTITLE: Setting Up Autocompletion for Fish Shell\nDESCRIPTION: This command sets up autocompletion for Kedro commands in Fish shell. It should be added to the ~/.config/fish/completions/foo-bar.fish file.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/commands_reference.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\neval (env _KEDRO_COMPLETE=fish_source kedro)\n```\n\n----------------------------------------\n\nTITLE: Running Kedro Benchmark with ASV\nDESCRIPTION: Executes the Kedro benchmark using the ASV tool. This command runs the benchmark for the current state of the repository.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/kedro_benchmarks/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nasv run\n```\n\n----------------------------------------\n\nTITLE: Installing Specific Kedro Dataset Type Dependencies\nDESCRIPTION: Command to install dependencies for a specific dataset type within a group in Kedro, following PEP 685 naming convention.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/kedro_project_setup/dependencies.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install \"kedro-datasets[<group>-<dataset>]\"\n```\n\n----------------------------------------\n\nTITLE: Including Technical Steering Committee Documentation in TOC\nDESCRIPTION: Sphinx toctree directive that includes the technical steering committee document in the navigation while keeping it hidden from the main content view.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/contribution/index.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n:hidden:\n\ntechnical_steering_committee\n```\n```\n\n----------------------------------------\n\nTITLE: Incorrect Usage of Transcoding in Kedro Pipeline\nDESCRIPTION: This example demonstrates an incorrect way to use transcoding in a Kedro pipeline, where the same dataset is used as both input and output of a single node, which will raise an error.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog_yaml_examples.md#2025-04-19_snippet_23\n\nLANGUAGE: python\nCODE:\n```\npipeline(\n    [\n        node(name=\"my_func1_node\", func=my_func1, inputs=\"my_dataframe@pandas\", outputs=\"my_dataframe@spark\"),\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Kedro Logging Config Path\nDESCRIPTION: Command to set custom logging configuration path using environment variable.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/logging/index.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport KEDRO_LOGGING_CONFIG=<project_root>/conf/logging.yml\n```\n\n----------------------------------------\n\nTITLE: Sphinx Documentation Template with Jinja2\nDESCRIPTION: A comprehensive template that generates structured Sphinx documentation. It creates sections for module description, functions, classes, exceptions and nested modules using autosummary directives and Jinja2 templating logic.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/_templates/autosummary/module.rst#2025-04-19_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ fullname | escape | underline }}\n\n.. rubric:: Description\n\n.. automodule:: {{ fullname }}\n\n   {% block functions %}\n   {% if functions %}\n   .. rubric:: Functions\n\n   .. autosummary::\n      :toctree:\n   {% for item in functions %}\n      {{ item }}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n\n   {% block classes %}\n   {% if classes %}\n   .. rubric:: Classes\n\n   .. autosummary::\n      :toctree:\n      :template: autosummary/class.rst\n   {% for item in classes %}\n      {{ item }}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n\n   {% block exceptions %}\n   {% if exceptions %}\n   .. rubric:: Exceptions\n\n   .. autosummary::\n      :toctree:\n      :template: autosummary/class.rst\n   {% for item in exceptions %}\n      {{ item }}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n\n{% block modules %}\n{% if modules %}\n.. rubric:: Modules\n\n.. autosummary::\n   :toctree:\n   :recursive:\n{% for item in modules %}\n   {{ item }}\n{%- endfor %}\n{% endif %}\n{% endblock %}\n```\n\n----------------------------------------\n\nTITLE: Kedro Catalog Rank Output Example\nDESCRIPTION: Example output from the 'kedro catalog rank' command showing dataset patterns ordered by specificity and placeholder count.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/kedro_dataset_factories.md#2025-04-19_snippet_10\n\nLANGUAGE: text\nCODE:\n```\n- 'preprocessed_{dataset_name}'\n- 'processed_{dataset_name}'\n- '{namespace}.{dataset_name}_pq'\n- '{dataset_name}_csv'\n- '{layer}.{dataset_name}'\n- '{default_dataset}'\n```\n\n----------------------------------------\n\nTITLE: Loading Kedro DataCatalog with YAML\nDESCRIPTION: Shows how to load a YAML configuration file and initialize Kedro's DataCatalog\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/notebook-example/add_kedro_to_a_notebook.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Using Kedro's DataCatalog\n\nimport yaml\n\nfrom kedro.io import DataCatalog\n\n# load the configuration file\nwith open(\"catalog.yml\") as f:\n    conf_catalog = yaml.safe_load(f)\n\n# Create the DataCatalog instance from the configuration\ncatalog = DataCatalog.from_config(conf_catalog)\n\n# Load the datasets\n...\n```\n\n----------------------------------------\n\nTITLE: Embedding YouTube Video in RST\nDESCRIPTION: RST directive for embedding a YouTube video tutorial with specified width parameter.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/spaceflights_tutorial.md#2025-04-19_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n{eval-rst}\n..  youtube:: YBY2Lcz7Gw4\n    :width: 100%\n```\n\n----------------------------------------\n\nTITLE: Running pytest\nDESCRIPTION: Command to run pytest from the project's root directory.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/automated_testing.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncd <project_root>\npytest\n```\n\n----------------------------------------\n\nTITLE: Running Kedro Benchmark for Specific Commits\nDESCRIPTION: Runs the Kedro benchmark for a range of commits from 'main' to 'mybranch'. This is useful for comparing performance across different versions or branches of the project.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/kedro_benchmarks/README.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nasv run main..mybranch\n```\n\n----------------------------------------\n\nTITLE: Running Packaged Project\nDESCRIPTION: Python code example showing how to run a packaged Kedro project programmatically\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/package_a_project.md#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom <package_name>.__main__ import main\n\nmain([\"--pipeline\", \"__default__\"])  # or simply main() if you don't want to provide any arguments\n```\n\n----------------------------------------\n\nTITLE: Incorrect Transcoding with Multiple Outputs\nDESCRIPTION: This snippet shows an incorrect use of transcoding where multiple nodes output the same dataset, which will result in an error due to non-unique outputs.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/data_catalog_yaml_examples.md#2025-04-19_snippet_24\n\nLANGUAGE: python\nCODE:\n```\npipeline(\n    [\n        node(name=\"my_func1_node\", func=my_func1, inputs=\"spark_input\", outputs=\"my_dataframe@spark\"),\n        node(name=\"my_func2_node\", func=my_func2, inputs=\"pandas_input\", outputs=\"my_dataframe@pandas\"),\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: TOC Entry for Advanced Concepts\nDESCRIPTION: Sphinx toctree directive for advanced data catalog concepts.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/index.md#2025-04-19_snippet_3\n\nLANGUAGE: sphinx\nCODE:\n```\n{toctree}\n:maxdepth: 1\n\nadvanced_data_catalog_usage\npartitioned_and_incremental_datasets\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents in Markdown\nDESCRIPTION: This code snippet defines a hidden table of contents using Markdown syntax. It includes links to two pages: one about adding Kedro to a notebook and another about using notebooks with Kedro projects.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/index.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n:maxdepth: 1\n:hidden:\n\nnotebook-example/add_kedro_to_a_notebook\nkedro_and_notebooks\n```\n```\n\n----------------------------------------\n\nTITLE: Comparing Kedro Benchmark Results Between Commits\nDESCRIPTION: Compares the benchmark results between two specific versions (v0.1 and v0.2) of Kedro. This is useful for identifying performance changes between releases.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/kedro_benchmarks/README.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nasv compare v0.1 v0.2\n```\n\n----------------------------------------\n\nTITLE: Sphinx Documentation for Kedro RichHandler\nDESCRIPTION: This RST (reStructuredText) code snippet defines a documentation page for the RichHandler class in Kedro's logging module. It uses Sphinx directives to import class documentation automatically from the source code.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/api/kedro.logging.RichHandler.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nkedro.logging.RichHandler\n=========================\n\n.. currentmodule:: kedro.logging\n\n.. autoclass:: RichHandler\n```\n\n----------------------------------------\n\nTITLE: Installing Ruff Dependencies in pyproject.toml\nDESCRIPTION: Configuration for adding ruff as an optional development dependency in pyproject.toml\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/linting.md#2025-04-19_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[project.optional-dependencies]\ndev = [\"ruff\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 Data Catalog for AWS Batch in Kedro\nDESCRIPTION: A YAML configuration for the data catalog that sets up all datasets to use AWS S3 storage locations. This allows nodes in the pipeline to read and write data from a central S3 bucket when executed as separate AWS Batch jobs.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/aws_batch.md#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ncompanies:\n  type: pandas.CSVDataset\n  filepath: s3://<your-bucket>/companies.csv\n\nreviews:\n  type: pandas.CSVDataset\n  filepath: s3://<your-bucket>/reviews.csv\n\nshuttles:\n  type: pandas.ExcelDataset\n  filepath: s3://<your-bucket>/shuttles.xlsx\n\npreprocessed_companies:\n  type: pandas.CSVDataset\n  filepath: s3://<your-bucket>/preprocessed_companies.csv\n\npreprocessed_shuttles:\n  type: pandas.CSVDataset\n  filepath: s3://<your-bucket>/preprocessed_shuttles.csv\n\nmodel_input_table:\n  type: pandas.CSVDataset\n  filepath: s3://<your-bucket>/model_input_table.csv\n\nregressor:\n  type: pickle.PickleDataset\n  filepath: s3://<your-bucket>/regressor.pickle\n  versioned: true\n\nX_train:\n  type: pickle.PickleDataset\n  filepath: s3://<your-bucket>/X_train.pickle\n\nX_test:\n  type: pickle.PickleDataset\n  filepath: s3://<your-bucket>/X_test.pickle\n\ny_train:\n  type: pickle.PickleDataset\n  filepath: s3://<your-bucket>/y_train.pickle\n\ny_test:\n  type: pickle.PickleDataset\n  filepath: s3://<your-bucket>/y_test.pickle\n```\n\n----------------------------------------\n\nTITLE: Running Tests with pytest\nDESCRIPTION: This command runs all the tests defined in the project using pytest, which is the recommended testing framework for Kedro projects.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/features/steps/test_starter/{{ cookiecutter.repo_name }}/README.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npytest\n```\n\n----------------------------------------\n\nTITLE: Commented Data Copy Operation\nDESCRIPTION: Commented version of the DBFS copy operation to prevent unnecessary execution during notebook runs.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_notebooks_development_workflow.md#2025-04-19_snippet_9\n\nLANGUAGE: ipython\nCODE:\n```\n#dbutils.fs.cp(\n#    \"file:///Workspace/Repos/<databricks_username>/iris-databricks/data\",\n#    \"dbfs:/FileStore/iris-databricks/data\",\n#    recurse=True,\n#)\n```\n\n----------------------------------------\n\nTITLE: Using Kedro Without Rich Library\nDESCRIPTION: Instructions for uninstalling the rich library and downgrading cookiecutter to use Kedro without rich formatting.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/configuration_basics.md#2025-04-19_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npip uninstall rich\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install cookiecutter==2.2.0\n```\n\n----------------------------------------\n\nTITLE: Kedro Node Naming Convention\nDESCRIPTION: Example of the default naming convention for anonymous nodes showing input/output format.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n<function_name>([in1;in2;...]) -> [out1;out2;...]\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Documentation Structure in reStructuredText\nDESCRIPTION: This snippet defines the structure of a Sphinx documentation file. It includes a dynamic project name, a table of contents directive, and references to index and search pages.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/features/steps/test_starter/{{ cookiecutter.repo_name }}/docs/source/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. {{ cookiecutter.python_package }} documentation master file, created by sphinx-quickstart.\n   You can adapt this file completely to your liking, but it should at least\n   contain the root `toctree` directive.\n\nWelcome to project {{ cookiecutter.python_package }}'s API docs!\n=============================================\n\n.. toctree::\n   :maxdepth: 4\n\n   modules\n\n\nIndices and tables\n==================\n\n* :ref:`genindex`\n* :ref:`modindex`\n* :ref:`search`\n```\n\n----------------------------------------\n\nTITLE: Updating Import Statements from TemplatedConfigLoader\nDESCRIPTION: Code diff showing the change in import statements from TemplatedConfigLoader to OmegaConfigLoader.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/config_loader_migration.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n- from kedro.config import TemplatedConfigLoader\n+ from kedro.config import OmegaConfigLoader\n```\n\n----------------------------------------\n\nTITLE: Defining Kedro Configuration Table of Contents in Markdown\nDESCRIPTION: This code snippet defines a table of contents for Kedro configuration documentation using Markdown and toctree directive. It includes links to various configuration-related topics such as basics, credentials, parameters, config loader migration, and advanced configuration.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/index.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n:maxdepth: 1\n\nconfiguration_basics\ncredentials\nparameters\nconfig_loader_migration\nadvanced_configuration\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Ruff Rules in pyproject.toml\nDESCRIPTION: Basic ruff configuration enabling essential rule sets for code quality checks\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/linting.md#2025-04-19_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[tool.ruff]\nselect = [\n    \"F\",  # Pyflakes\n    \"E\",  # Pycodestyle\n    \"W\",  # Pycodestyle\n    \"UP\",  # pyupgrade\n    \"I\",  # isort\n    \"PL\", # Pylint\n]\n```\n\n----------------------------------------\n\nTITLE: Uploading Data to DBFS using Databricks CLI\nDESCRIPTION: Commands to upload project data to Databricks File System (DBFS) and list the contents of the destination folder.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_dbx_workflow.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndatabricks fs cp --recursive <project_root>/data/ dbfs:/FileStore/iris-databricks/data\n```\n\nLANGUAGE: bash\nCODE:\n```\ndatabricks fs ls dbfs:/FileStore/iris-databricks/data\n```\n\n----------------------------------------\n\nTITLE: Running Tests with pytest in Kedro Project\nDESCRIPTION: This command runs the project tests using pytest.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/kedro/templates/project/{{ cookiecutter.repo_name }}/README.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npytest\n```\n\n----------------------------------------\n\nTITLE: Commented DBFS Directory Listing\nDESCRIPTION: Commented version of the DBFS directory listing command.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_notebooks_development_workflow.md#2025-04-19_snippet_10\n\nLANGUAGE: ipython\nCODE:\n```\n#dbutils.fs.ls(\"dbfs:/FileStore/iris-databricks/data\")\n```\n\n----------------------------------------\n\nTITLE: Loading Data from Kedro Catalog\nDESCRIPTION: Python code to load a specific dataset ('shuttles') from the Kedro Data Catalog.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/kedro_and_notebooks.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncatalog.load(\"shuttles\")\n```\n\n----------------------------------------\n\nTITLE: CDK Configuration JSON\nDESCRIPTION: JSON configuration for CDK deployment script\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/aws_step_functions.md#2025-04-19_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"app\": \"python3 deploy.py\"\n}\n```\n\n----------------------------------------\n\nTITLE: Apache License 2.0 Copyright Notice Template\nDESCRIPTION: Standard boilerplate copyright notice and license text to be included in project files. Requires replacing [yyyy] with the year and [name of copyright owner] with appropriate copyright holder information.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/LICENSE.md#2025-04-19_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nCopyright [yyyy] [name of copyright owner]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n\n----------------------------------------\n\nTITLE: Embedding YouTube Video in ReStructuredText\nDESCRIPTION: This code snippet demonstrates how to embed a YouTube video in ReStructuredText documentation. It uses the 'eval-rst' directive to include a YouTube video with a specified width.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/introduction/index.md#2025-04-19_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n..  youtube:: PdNkECqvI58\n    :width: 100%\n```\n\n----------------------------------------\n\nTITLE: Defining Toctree for Kedro First Steps Documentation in Markdown\nDESCRIPTION: A toctree directive that organizes the first steps documentation for Kedro. It lists pages for installation, creating new projects, Kedro concepts, and minimal project setup with maxdepth set to 1.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/get_started/index.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n:maxdepth: 1\n\ninstall\nnew_project\nkedro_concepts\nminimal_kedro_project\n```\n```\n\n----------------------------------------\n\nTITLE: Embedding Table of Contents in RST\nDESCRIPTION: RST directive for creating a table of contents with links to tutorial sections including template setup, data configuration, pipeline creation, testing and packaging.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/spaceflights_tutorial.md#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n{toctree}\n:maxdepth: 1\n\ntutorial_template\nset_up_data\ncreate_a_pipeline\nadd_another_pipeline\ntest_a_project\npackage_a_project\nspaceflights_tutorial_faqs\n```\n\n----------------------------------------\n\nTITLE: Installing Project in Editable Mode\nDESCRIPTION: Command to install the project in editable mode, allowing changes to project files without needing to reinstall.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/automated_testing.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Configuring RST Documentation Root with Cookiecutter Template\nDESCRIPTION: Basic RST configuration for a Sphinx documentation root file. Uses cookiecutter templating to insert the Python package name and sets up the main documentation structure with table of contents and standard Sphinx indexes.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/kedro/templates/project/{{ cookiecutter.repo_name }}/docs/source/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. {{ cookiecutter.python_package }} documentation master file, created by sphinx-quickstart.\n   You can adapt this file completely to your liking, but it should at least\n   contain the root `toctree` directive.\n\nWelcome to project {{ cookiecutter.python_package }}'s API docs!\n=============================================\n\n.. toctree::\n   :maxdepth: 4\n\n   modules\n\n\nIndices and tables\n==================\n\n* :ref:`genindex`\n* :ref:`modindex`\n* :ref:`search`\n```\n\n----------------------------------------\n\nTITLE: Note on Plugin License Requirement\nDESCRIPTION: Specifies the license requirement for plugins to be considered for inclusion in the list. Plugins must have an Apache 2.0 compatible license.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/extend_kedro/plugins.md#2025-04-19_snippet_9\n\nLANGUAGE: markdown\nCODE:\n```\n```{note}\nYour plugin needs to have an [Apache 2.0 compatible license](https://www.apache.org/legal/resolved.html#category-a) to be considered for this list.\n```\n```\n\n----------------------------------------\n\nTITLE: TOC Entry for Basic Data Catalog\nDESCRIPTION: Sphinx toctree directive for basic data catalog documentation.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/index.md#2025-04-19_snippet_0\n\nLANGUAGE: sphinx\nCODE:\n```\n{toctree}\n:maxdepth: 1\n\ndata_catalog\n```\n\n----------------------------------------\n\nTITLE: Handling DatasetError: Failed while loading data from dataset in Python\nDESCRIPTION: This snippet shows an error message that occurs when Kedro fails to load raw test data due to missing CSV files. It indicates that the required data files are not present in the expected directory.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/spaceflights_tutorial_faqs.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nDatasetError: Failed while loading data from dataset\nCSVDataset(filepath=...).\n[Errno 2] No such file or directory: '.../companies.csv'\n```\n\n----------------------------------------\n\nTITLE: Copying Kedro Project Files\nDESCRIPTION: Shell commands to copy necessary Kedro project files including data, configuration, wheel file and DAG to the Airflow project directory.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/airflow.md#2025-04-19_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ncd ..\ncp -r new-kedro-project/data kedro-airflow-spaceflights/data\ncp -r new-kedro-project/conf kedro-airflow-spaceflights/conf\nmkdir -p kedro-airflow-spaceflights/dist/\ncp new-kedro-project/dist/new_kedro_project-0.1-py3-none-any.whl kedro-airflow-spaceflights/dist/\ncp new-kedro-project/dags/new_kedro_project_airflow_dag.py kedro-airflow-spaceflights/dags/\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies\nDESCRIPTION: Command to install all project dependencies listed in requirements.txt using pip.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/tutorial_template.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Configuring Lambda Docker Container\nDESCRIPTION: Dockerfile instructions for setting up the Lambda execution environment with Kedro dependencies\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/aws_step_functions.md#2025-04-19_snippet_5\n\nLANGUAGE: dockerfile\nCODE:\n```\nCOPY --from=build-image ${FUNCTION_DIR} ${FUNCTION_DIR}\nENTRYPOINT [ \"/usr/local/bin/python\", \"-m\", \"awslambdaric\" ]\nCMD [ \"lambda_handler.handler\" ]\n```\n\n----------------------------------------\n\nTITLE: Installing statsd for Pipeline Observability in Kedro\nDESCRIPTION: Installation command for the statsd package needed to add observability to Kedro pipelines for visualization with Grafana.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/hooks/examples.md#2025-04-19_snippet_9\n\nLANGUAGE: console\nCODE:\n```\npip install statsd\n```\n\n----------------------------------------\n\nTITLE: Modifying Kedro Source Directory Configuration\nDESCRIPTION: Shows how to modify the project structure from src layout to flat layout by changing the source_dir setting in pyproject.toml.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/kedro_project_setup/settings.md#2025-04-19_snippet_1\n\nLANGUAGE: diff\nCODE:\n```\n+++ source_dir = \"\"\n--- source_dir = \"src\"\n```\n\n----------------------------------------\n\nTITLE: Uploading Project Data and Configuration to DBFS\nDESCRIPTION: Databricks CLI commands to upload project data and configuration to DBFS, and to list the uploaded contents.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_deployment_workflow.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndatabricks fs cp --recursive <project_root>/data/ dbfs:/FileStore/iris_databricks/data\n```\n\nLANGUAGE: bash\nCODE:\n```\ndatabricks fs cp --recursive <project_root>/conf/ dbfs:/FileStore/iris_databricks/conf\n```\n\nLANGUAGE: bash\nCODE:\n```\ndatabricks fs ls dbfs:/FileStore/iris_databricks/data\n```\n\n----------------------------------------\n\nTITLE: Installing pytest-cov\nDESCRIPTION: Command to install the pytest-cov plugin for generating test coverage reports.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/automated_testing.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install pytest-cov\n```\n\n----------------------------------------\n\nTITLE: Running Kedro pipeline\nDESCRIPTION: Command to execute the complete Kedro pipeline, which will process the data and save the specified outputs as Delta tables.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/deltalake_versioning.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkedro run\n```\n\n----------------------------------------\n\nTITLE: Syncing Local Code with Databricks Repo using dbx\nDESCRIPTION: Uses dbx to synchronize the local project directory with a Databricks Repo, enabling continuous code syncing during development.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_dbx_workflow.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nconda activate iris-databricks\ncd <project_root>\ndbx sync repo --dest-repo iris-databricks --source .\n```\n\n----------------------------------------\n\nTITLE: Installing Ruff Independently\nDESCRIPTION: Direct installation command for ruff using pip\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/linting.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install ruff\n```\n\n----------------------------------------\n\nTITLE: Creating Plugins Archive for MWAA\nDESCRIPTION: Command to archive Kedro project files including wheel file, configuration tarball, and logging configuration into a plugins.zip file for MWAA deployment.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/airflow.md#2025-04-19_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nzip -j plugins.zip dist/new_kedro_project-0.1-py3-none-any.whl dist/conf-new_kedro_project.tar.gz conf/logging.yml\n```\n\n----------------------------------------\n\nTITLE: Deploying AWSBatchRunner via Kedro CLI in Bash\nDESCRIPTION: This command triggers the Kedro run using the custom AWSBatchRunner in the AWS Batch environment.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/aws_batch.md#2025-04-19_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --env=aws_batch --runner=kedro_tutorial.runner.AWSBatchRunner\n```\n\n----------------------------------------\n\nTITLE: Kedro Dataset Class Deprecation Table\nDESCRIPTION: Markdown table showing the mapping between deprecated dataset class names and their new names in Kedro 0.19.0, including their module locations.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Type                        | Deprecated Alias            | Location                       |\n| --------------------------- | --------------------------- | ------------------------------ |\n| `CachedDataset`             | `CachedDataSet`             | `kedro.io.cached_dataset`      |\n| `LambdaDataset`             | `LambdaDataSet`             | `kedro.io.lambda_dataset`      |\n| `IncrementalDataset`        | `IncrementalDataSet`        | `kedro.io.partitioned_dataset` |\n| `MemoryDataset`             | `MemoryDataSet`             | `kedro.io.memory_dataset`      |\n| `PartitionedDataset`        | `PartitionedDataSet`        | `kedro.io.partitioned_dataset` |\n| `DatasetError`              | `DataSetError`              | `kedro.io.core`                |\n| `DatasetAlreadyExistsError` | `DataSetAlreadyExistsError` | `kedro.io.core`                |\n| `DatasetNotFoundError`      | `DataSetNotFoundError`      | `kedro.io.core`                |\n```\n\n----------------------------------------\n\nTITLE: Specifying Additional Configuration Environments in Kedro\nDESCRIPTION: Demonstrates how to use additional configuration environments in Kedro using the --env flag or KEDRO_ENV environment variable.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/configuration/configuration_basics.md#2025-04-19_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nkedro run --env=<your-environment>\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport KEDRO_ENV=<your-environment>\n```\n\n----------------------------------------\n\nTITLE: Markdown Note Block for Documentation Update\nDESCRIPTION: A markdown note block indicating that debugging documentation has been moved to other locations\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/debugging.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n``` {note}\nOur debugging documentation has moved. Please see our existing guides:\n```\n```\n\n----------------------------------------\n\nTITLE: Installing and Initializing Kedro-Databricks\nDESCRIPTION: Commands to install kedro-databricks package and initialize Databricks configuration.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_ide_databricks_asset_bundles_workflow.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install kedro-databricks\n```\n\nLANGUAGE: bash\nCODE:\n```\nkedro databricks init\n```\n\n----------------------------------------\n\nTITLE: Visualizing Kedro Plugin Command Flow with Mermaid Sequence Diagram\nDESCRIPTION: This Mermaid sequence diagram illustrates the process flow when executing the 'kedro plugin' command. It shows the interactions between the CLI, project plugins, KedroSession, and KedroCLI, highlighting the steps from command initiation to KedroSession creation.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/diagrams/kedro-plugin.md#2025-04-19_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    title $ kedro plugin\n\n    participant cli as $ kedro plugin\n    participant prelude as See kedro-with-project.md for details\n    participant project_plugins as Kedro Plugins <br> [project.entry-points.\"kedro.project_commands\"]\n    participant session as KedroSession\n    participant kedro_cli as KedroCLI\n\n    cli->>prelude: prepare click commands as prelude to this\n    prelude->>project_plugin: execute plugin click command\n    project_plugin->>kedro_cli: get ProjectMetadata from the KedroCLI command collection group\n    project_plugin->>project_plugin: plugin code\n    project_plugin->>session: need to create KedroSession for all runtime config and info\n```\n\n----------------------------------------\n\nTITLE: Data Catalog Configuration for ImageDataset in Kedro\nDESCRIPTION: YAML configuration for the data catalog that registers a Pikachu image to be loaded using the custom ImageDataset. Specifies the dataset type and filepath.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/how_to_create_a_custom_dataset.md#2025-04-19_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n# in conf/base/catalog.yml\n\npikachu:\n  type: kedro_pokemon.datasets.image_dataset.ImageDataset\n  filepath: data/01_raw/pokemon-images-and-types/images/images/pikachu.png\n  # Note: the duplicated `images` path is part of the original Kaggle dataset\n```\n\n----------------------------------------\n\nTITLE: Creating New Kedro Project\nDESCRIPTION: Command to create a new Kedro project using the databricks-iris starter template.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_ide_databricks_asset_bundles_workflow.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkedro new --starter=databricks-iris\n```\n\n----------------------------------------\n\nTITLE: Loading PyIcebergDataset Instance\nDESCRIPTION: Python code to load an instance of PyIcebergDataset from the Kedro catalog for further inspection.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/iceberg_versioning.md#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nmodel_input_table = catalog.datasets['model_input_table']\n```\n\n----------------------------------------\n\nTITLE: Starting Kedro IPython Session\nDESCRIPTION: Command to start an IPython session with Kedro components loaded for interactive data inspection.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/iceberg_versioning.md#2025-04-19_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nkedro ipython\n```\n\n----------------------------------------\n\nTITLE: Accessing Default Pipeline\nDESCRIPTION: Shows how to access the default pipeline with lazy loading behavior, where pipelines are only loaded when first accessed.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.framework.project import pipelines\n\nprint(pipelines[\"__default__\"])  # pipeline loading is only triggered here\n```\n\n----------------------------------------\n\nTITLE: Creating a Kedro Project with Spaceflights Starter\nDESCRIPTION: Command to create a new Kedro project using the spaceflights-pandas starter template, which provides example pipelines to work with.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/iceberg_versioning.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkedro new --starter spaceflights-pandas --name kedro-iceberg\n```\n\n----------------------------------------\n\nTITLE: Running a Kedro Plugin Command\nDESCRIPTION: Example of how to invoke a Kedro plugin command from the command line after the plugin is installed.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/extend_kedro/plugins.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkedro to_json\n```\n\n----------------------------------------\n\nTITLE: Loading and inspecting Delta Table in IPython\nDESCRIPTION: Python code to load a Delta table dataset from the catalog and inspect its version history in an interactive Kedro IPython session.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/deltalake_versioning.md#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nIn [1]: model_input_table = catalog.datasets['model_input_table']\n```\n\n----------------------------------------\n\nTITLE: Kedro Run Command Flow Sequence Diagram\nDESCRIPTION: A sequence diagram showing how a Kedro project's run command is executed, starting from CLI/module invocation through the pyproject.toml entrypoint, finding the run command, and creating/executing a KedroSession.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/diagrams/python-m-project.md#2025-04-19_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    title python -m {project}.run\n\n    participant cli as $ <project> <br> $ python -m <project>\n    participant entrypoint as pyproject.toml <br> <project> = <project>.__main__:main\n    participant find_run as find_run_command()\n    participant session as KedroSession\n\n    cli->>entrypoint: Python calls the entrypoint\n    entrypoint->>find_run: Find run command\n    find_run->>session: create session (for default run)\n    session->>session: run\n```\n\n----------------------------------------\n\nTITLE: AWS CDK Dependencies Configuration\nDESCRIPTION: Required AWS CDK dependencies for deployment script\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/aws_step_functions.md#2025-04-19_snippet_7\n\nLANGUAGE: text\nCODE:\n```\naws_cdk.aws_s3\naws_cdk.core\naws-cdk.aws_ecr\naws-cdk.aws_lambda\naws-cdk.aws-stepfunctions\naws-cdk.aws-stepfunctions-tasks\n```\n\n----------------------------------------\n\nTITLE: Implementing CLI Hooks in a Kedro Plugin\nDESCRIPTION: Example of implementing CLI hooks in a Kedro plugin. This code defines a hook that runs before a command is executed, allowing plugins to modify or extend Kedro's CLI behavior.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/extend_kedro/plugins.md#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nfrom kedro.framework.cli.hooks import cli_hook_impl\n\n\nclass MyCLIHooks:\n    @cli_hook_impl\n    def before_command_run(self, project_metadata, command_args):\n        logging.info(\n            \"Command %s will be run for project %s\", command_args, project_metadata\n        )\n\n\ncli_hooks = MyCLIHooks()\n```\n\n----------------------------------------\n\nTITLE: SSH Tunnel Creation for Remote Debugging\nDESCRIPTION: Command to establish SSH tunnel for remote debugging connection.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/set_up_vscode.md#2025-04-19_snippet_10\n\nLANGUAGE: console\nCODE:\n```\nssh -vNL 3000:127.0.0.1:3000 <your_username>@<remote_server>\n```\n\n----------------------------------------\n\nTITLE: Implementing Lazy Loading for Kedro Plugin Commands\nDESCRIPTION: Advanced plugin implementation that uses lazy loading to improve performance. This demonstrates how to delay loading of commands and their dependencies until they are actually needed.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/extend_kedro/plugins.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport click\nfrom kedro.framework.project import pipelines\nfrom kedro.framework.cli.utils import LazyGroup\n\n@click.group()\ndef commands():\n    pass\n\n@commands.group(\n    name=\"to_json\",\n    cls=LazyGroup,\n    lazy_subcommands={\n        \"nodes\": \"kedrojson.plugin.nodes\",\n        \"pipelines\": \"kedrojson.plugin.pipelines\"\n        }\n)\ndef to_json():\n    \"\"\"Convert Kedro nodes and pipelines to JSON\"\"\"\n    pass\n\n@click.command(name=\"nodes\")\ndef nodes():\n    \"\"\"Convert Kedro nodes to JSON\"\"\"\n    import some_large_library\n    print(\"Converting nodes to JSON\")\n    ...\n\n@click.command(\"pipelines\")\ndef pipelines():\n    \"\"\"Convert Kedro pipelines to JSON\"\"\"\n    print(\"Converting pipelines to JSON\")\n    ...\n```\n\n----------------------------------------\n\nTITLE: Launching IPython Shell with Kedro Extension\nDESCRIPTION: This command starts an IPython shell with the Kedro extension pre-loaded, equivalent to running 'ipython --ext kedro.ipython'.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/notebooks_and_ipython/kedro_and_notebooks.md#2025-04-19_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nkedro ipython\n```\n\n----------------------------------------\n\nTITLE: Installing Project for Testing\nDESCRIPTION: Commands for installing the project in editable mode and running tests. Shows how to set up the environment for testing execution.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/test_a_project.md#2025-04-19_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd <project_root>\npytest tests/pipelines/test_data_science_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Installing Kedro from GitHub source\nDESCRIPTION: Command to install the latest development version of Kedro directly from the GitHub repository's main branch. This allows access to the most recent features and updates before official release.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/README.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/kedro-org/kedro@main\n```\n\n----------------------------------------\n\nTITLE: Migration Example: Converting Pipeline.transform() to pipeline() Constructor\nDESCRIPTION: Demonstrates how to migrate from the deprecated Pipeline.transform() method to the new pipeline() constructor in Kedro 0.16.0. Shows the differences in argument naming and structure between the old and new approaches.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nresult = my_pipeline.transform(\n    datasets={\"input\": \"new_input\", \"output\": \"new_output\", \"params:x\": \"params:y\"},\n    prefix=\"pre\",\n)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Kedro Project Execution Flow with Mermaid\nDESCRIPTION: A sequence diagram showing the interaction between a third-party Python script, the Kedro project configuration directory, and a KedroSession. The diagram illustrates how external scripts can integrate with Kedro projects by accessing the configuration and creating sessions to run pipelines.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/diagrams/installed-kedro-project.md#2025-04-19_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    title Installed Kedro project\n\n    participant script as Third-party Python script\n    participant curr_dir as Directory with Kedro conf/ in it\n    participant session as KedroSession\n\n    script->>script: run third-party script\n    script->>curr_dir: get path to the project config\n    script->>session: create a session with Kedro project config dir\n    session->>session: run a pipeline and/or nodes\n```\n\n----------------------------------------\n\nTITLE: Installing Kedro Dataset Group Dependencies\nDESCRIPTION: Command to install dependencies for a specific group of dataset types in Kedro, where <group> represents the dataset group (e.g., pandas, spark).\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/kedro_project_setup/dependencies.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install \"kedro-datasets[<group>]\"\n```\n\n----------------------------------------\n\nTITLE: Testing Databricks CLI Authentication\nDESCRIPTION: Verifies Databricks CLI authentication by listing files in the DBFS root directory.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_dbx_workflow.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndatabricks fs ls dbfs:/\n```\n\n----------------------------------------\n\nTITLE: Copying Project Data to DBFS\nDESCRIPTION: Copies data directory from local Databricks Repo to DBFS storage recursively for project access.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_notebooks_development_workflow.md#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndbutils.fs.cp(\n    \"file:///Workspace/Repos/<databricks_username>/iris-databricks/data/\",\n    \"dbfs:/FileStore/iris-databricks/data\",\n    recurse=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating and Running a KedroSession\nDESCRIPTION: Example showing the new way to run a Kedro pipeline via a KedroSession in Kedro 0.17.0, which replaces the previous method using KedroContext.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nwith KedroSession.create(package_name=...) as session:\n    session.run()\n```\n\n----------------------------------------\n\nTITLE: Configuring Dataset Loading with Specific Version\nDESCRIPTION: YAML configuration for loading a specific version of an Iceberg table by specifying the snapshot_id in scan_args.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/iceberg_versioning.md#2025-04-19_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_input_table:\n  type: kedro_iceberg.pyiceberg_dataset.PyIcebergDataset\n  catalog: default\n  namespace: default\n  table_name: model_input_table\n  table_type: pandas\n  scan_args:\n    snapshot_id: <snapshot_id>\n```\n\n----------------------------------------\n\nTITLE: Creating Warehouse Directory for Iceberg Tables\nDESCRIPTION: Command to create a temporary directory to store Iceberg tables data and metadata.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/integrations/iceberg_versioning.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p /tmp/warehouse\n```\n\n----------------------------------------\n\nTITLE: Initializing Git Repository\nDESCRIPTION: Commands to initialize Git repository in the project directory and create the first commit.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_notebooks_development_workflow.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# change the directory to the project root\ncd iris-databricks/\n# initialise git\ngit init\n```\n\n----------------------------------------\n\nTITLE: Accessing Kedro Settings in Python\nDESCRIPTION: Example of how to access Kedro settings after the refactoring in version 0.17.1. This allows direct access to configuration values like CONF_ROOT.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.framework.project import settings\n\nprint(settings.CONF_ROOT)\n```\n\n----------------------------------------\n\nTITLE: Installing Packaged Project\nDESCRIPTION: Command to install a packaged Kedro project from a wheel file\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/package_a_project.md#2025-04-19_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install <path-to-wheel-file>\n```\n\n----------------------------------------\n\nTITLE: Node Execution Syntax\nDESCRIPTION: Alternative syntax for executing nodes directly with named parameters instead of using the run method\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nmy_node(input1=1, input2=2)  # Alternative to: my_node.run(dict(input1=1, input2=2))\n```\n\n----------------------------------------\n\nTITLE: Installing Kedro and dbx\nDESCRIPTION: Installs the latest versions of Kedro and dbx Python packages within the activated Conda environment.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_dbx_workflow.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install kedro dbx --upgrade\n```\n\n----------------------------------------\n\nTITLE: Defining Non-Git Kedro Starter in Python\nDESCRIPTION: Python code to define a Kedro starter specification for a local directory, enabling the use of a custom starter alias.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/starters/create_a_starter.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# plugin.py\nstarters = [\n    KedroStarterSpec(\n        alias=\"test_plugin_starter\",\n        template_path=\"your_local_directory/starter_folder\",\n    )\n]\n```\n\n----------------------------------------\n\nTITLE: Running Kedro Project in Databricks Notebook\nDESCRIPTION: IPython command to run the loaded Kedro project using the session variable.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_dbx_workflow.md#2025-04-19_snippet_10\n\nLANGUAGE: ipython\nCODE:\n```\nsession.run()\n```\n\n----------------------------------------\n\nTITLE: Installing pip-tools for Dependency Management\nDESCRIPTION: Command to install pip-tools package for managing and freezing project dependencies.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/kedro_project_setup/dependencies.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install pip-tools\n```\n\n----------------------------------------\n\nTITLE: Rich Console Markup Logging\nDESCRIPTION: Example of using Rich's console markup in Kedro logging for formatted error messages with styling.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/logging/index.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nlogger.error(\"[bold red blink]Important error message![/]\", extra={\"markup\": True})\n```\n\n----------------------------------------\n\nTITLE: Loading Kedro Pipelines Without Context\nDESCRIPTION: Demonstrates how to load Kedro pipelines without creating a KedroContext instance, introduced in version 0.17.2.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom kedro.framework.project import pipelines\n\nprint(pipelines)\n```\n\n----------------------------------------\n\nTITLE: Kedro Catalog Resolve Command Usage\nDESCRIPTION: Bash command showing how to redirect the output of 'kedro catalog resolve' to a file.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/kedro_dataset_factories.md#2025-04-19_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nkedro catalog resolve > output_file.yaml\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Rich Library in Kedro\nDESCRIPTION: Command to remove the Rich library from the Python environment. This affects the visual formatting of Kedro's logging and output.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/logging/index.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip uninstall rich\n```\n\n----------------------------------------\n\nTITLE: Loading Kedro Project\nDESCRIPTION: Loads the Kedro project using the reload_kedro magic command.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_notebooks_development_workflow.md#2025-04-19_snippet_13\n\nLANGUAGE: ipython\nCODE:\n```\n%reload_kedro /Workspace/Repos/<databricks_username>/iris-databricks\n```\n\n----------------------------------------\n\nTITLE: Generated Pipeline Directory Structure\nDESCRIPTION: Shows the folder structure generated by the pipeline create command, including configuration, source code, and test directories\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/modular_pipelines.md#2025-04-19_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n├── conf\n│   └── base\n│       └── parameters_{{pipeline_name}}.yml  <-- Pipeline-specific parameters\n└── src\n    ├── my_project\n    │   ├── __init__.py\n    │   └── pipelines\n    │       ├── __init__.py\n    │       └── {{pipeline_name}}      <-- This folder defines the modular pipeline\n    │           ├── __init__.py        <-- So that Python treats this pipeline as a module\n    │           ├── nodes.py           <-- To declare your nodes\n    │           └── pipeline.py        <-- To structure the pipeline itself\n    └── tests\n        ├── __init__.py\n        └── pipelines\n            ├── __init__.py\n            └── {{pipeline_name}}      <-- Pipeline-specific tests\n                ├── __init__.py\n                └── test_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Activating Conda Environment\nDESCRIPTION: Command to activate the newly created Conda environment for the project.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_notebooks_development_workflow.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda activate iris-databricks\n```\n\n----------------------------------------\n\nTITLE: Registering Kedro Starter in pyproject.toml\nDESCRIPTION: TOML configuration to register a custom Kedro starter specification in the project's pyproject.toml file.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/starters/create_a_starter.md#2025-04-19_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[project.entry-points.\"kedro.starters\"]\nstarter = \"plugin:starters\"\n```\n\n----------------------------------------\n\nTITLE: Running Kedro Session\nDESCRIPTION: Executes the Kedro project pipeline using the session object.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/deployment/databricks/databricks_notebooks_development_workflow.md#2025-04-19_snippet_14\n\nLANGUAGE: ipython\nCODE:\n```\nsession.run()\n```\n\n----------------------------------------\n\nTITLE: TOC Entry for YAML Examples\nDESCRIPTION: Sphinx toctree directive for data catalog YAML examples.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/index.md#2025-04-19_snippet_1\n\nLANGUAGE: sphinx\nCODE:\n```\n{toctree}\n:maxdepth: 1\n\ndata_catalog_yaml_examples\n```\n\n----------------------------------------\n\nTITLE: Context Property Access in IPython\nDESCRIPTION: Example showing how to access parameters as a context property in IPython/Jupyter environments\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ncontext.params\n```\n\n----------------------------------------\n\nTITLE: Adding Modules to Documentation\nDESCRIPTION: RST configuration to include modules in documentation\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/package_a_project.md#2025-04-19_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n.. toctree::\n\n   modules\n```\n\n----------------------------------------\n\nTITLE: Kedro Project Tools Selection Dialog\nDESCRIPTION: Dialog showing the tools options available when creating a new Kedro project. The user can select options by number, or choose 'all' or 'none'.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/get_started/new_project.md#2025-04-19_snippet_8\n\nLANGUAGE: text\nCODE:\n```\nTools\n1) Lint: Basic linting with ruff\n2) Test: Basic testing with pytest\n3) Log: Additional, environment-specific logging options\n4) Docs: A Sphinx documentation setup\n5) Data Folder: A folder structure for data management\n6) PySpark: Configuration for working with PySpark\n7) Kedro-Viz: Kedro's native visualisation tool\n\nWhich tools would you like to include in your project? [1-7/1,3/all/none]:\n (none):\n```\n\n----------------------------------------\n\nTITLE: TOC Entry for KedroDataCatalog\nDESCRIPTION: Sphinx toctree directive for KedroDataCatalog documentation.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/index.md#2025-04-19_snippet_5\n\nLANGUAGE: sphinx\nCODE:\n```\n{toctree}\n:maxdepth: 1\n\nkedro_data_catalog\n```\n\n----------------------------------------\n\nTITLE: Jupyter Line Magic Command\nDESCRIPTION: Magic command for running kedro viz within a Jupyter notebook cell\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n%run_viz\n```\n\n----------------------------------------\n\nTITLE: Installing Kedro-Viz for Project Visualization\nDESCRIPTION: Command to install the Kedro-Viz package, which is not included in the standard Kedro installation. This enables visualization of the project's data pipeline.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/get_started/new_project.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install kedro-viz\n```\n\n----------------------------------------\n\nTITLE: Pipeline Context Override\nDESCRIPTION: Implementation requirement for overriding the _get_pipeline abstract method in ProjectContext\nSOURCE: https://github.com/kedro-org/kedro/blob/main/RELEASE.md#2025-04-19_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n_get_pipeline\n```\n\n----------------------------------------\n\nTITLE: TOC Entry for Custom Dataset Creation\nDESCRIPTION: Sphinx toctree directive for custom dataset creation tutorial.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/index.md#2025-04-19_snippet_4\n\nLANGUAGE: sphinx\nCODE:\n```\n{toctree}\n:maxdepth: 1\n\nhow_to_create_a_custom_dataset\n```\n\n----------------------------------------\n\nTITLE: TOC Entry for Dataset Factories\nDESCRIPTION: Sphinx toctree directive for Kedro dataset factories documentation.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/data/index.md#2025-04-19_snippet_2\n\nLANGUAGE: sphinx\nCODE:\n```\n{toctree}\n:maxdepth: 1\n\nkedro_dataset_factories\n```\n\n----------------------------------------\n\nTITLE: Pulling Kedro Micro-Package into Project\nDESCRIPTION: Pulls files related to a micro-package from PyPI or a storage location. This command is deprecated and will be removed in Kedro 1.0.0.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/development/commands_reference.md#2025-04-19_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nkedro micropkg pull <package_name> (or path to a sdist file)\n```\n\n----------------------------------------\n\nTITLE: Installing Sphinx Documentation Tool\nDESCRIPTION: Command to install Sphinx documentation framework using pip\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/package_a_project.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install sphinx\n```\n\n----------------------------------------\n\nTITLE: Pipeline Namespace Configuration\nDESCRIPTION: Shows how to configure pipeline namespaces while preserving specific input/output names.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/nodes_and_pipelines/namespaces.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npipeline(\n    [node(...), node(...), node(...)],\n    namespace=\"your_namespace_name\",\n    inputs={\"first_input_to_not_be_prefixed\", \"second_input_to_not_be_prefixed\"},\n    outputs={\"first_output_to_not_be_prefixed\", \"second_output_to_not_be_prefixed\"},\n    parameters={\"first_parameter_to_not_be_prefixed\", \"second_parameter_to_not_be_prefixed\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Handling DatasetError: Exception when parsing config for Dataset in Bash\nDESCRIPTION: This snippet shows an error message that occurs when there are missing dependencies required to parse data in the Kedro Data Catalog. It suggests installing the necessary project dependencies.\nSOURCE: https://github.com/kedro-org/kedro/blob/main/docs/source/tutorial/spaceflights_tutorial_faqs.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nDatasetError: An exception occurred when parsing config for Dataset\n'data_processing.preprocessed_companies':\nObject 'ParquetDataset' cannot be loaded from 'kedro_datasets.pandas'. Please see the\ndocumentation on how to install relevant dependencies for kedro_datasets.pandas.ParquetDataset:\nhttps://docs.kedro.org/en/stable/kedro_project_setup/dependencies.html\n```"
  }
]
[
  {
    "owner": "redpanda-data",
    "repo": "connect",
    "content": "TITLE: Running Redpanda Connect with a configuration file\nDESCRIPTION: This command runs Redpanda Connect using a specified configuration file. The `rpk connect run` command starts the process and loads the configuration from the file. This is the primary way to deploy Redpanda Connect pipelines.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/README.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nrpk connect run ./config.yaml\n```\n\n----------------------------------------\n\nTITLE: Running Redpanda Connect in Docker with a config file\nDESCRIPTION: This command runs Redpanda Connect within a Docker container, mounting a configuration file from the host machine.  The `-v` flag maps the local config file to `/connect.yaml` inside the container. This method provides a portable and isolated environment for Redpanda Connect.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/README.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ndocker run --rm -v /path/to/your/config.yaml:/connect.yaml docker.redpanda.com/redpandadata/connect run\n```\n\n----------------------------------------\n\nTITLE: Generating Data with Redpanda Connect\nDESCRIPTION: This snippet uses Redpanda Connect to generate and insert 100,000,000 records into the `testing_a` topic. It uses a YAML configuration file (`generate.yaml`) to define the data generation parameters and the target Redpanda topic. This is a key step in populating the Redpanda cluster with data for benchmarking.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/resources/docker/redpanda_benchmarking/README.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\n# Inserts 100,000,000 records into topic testing_a\nredpanda-connect run ./generate.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining a data pipeline with YAML configuration\nDESCRIPTION: This YAML configuration defines a simple data pipeline in Redpanda Connect. It specifies a GCP Pub/Sub input, a Bloblang mapping processor, and a Redis Streams output. The processor transforms the input message and adds metadata before sending it to the Redis stream.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/README.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ninput:\n  gcp_pubsub:\n    project: foo\n    subscription: bar\n\npipeline:\n  processors:\n    - mapping: |\n        root.message = this\n        root.meta.link_count = this.links.length()\n        root.user.age = this.user.age.number()\n\noutput:\n  redis_streams:\n    url: tcp://TODO:6379\n    stream: baz\n    max_in_flight: 20\n```\n\n----------------------------------------\n\nTITLE: Adding MySQL CDC Input\nDESCRIPTION: Introduces the `mysql_cdc` input for capturing change data from MySQL databases. This new input enables Redpanda Connect to ingest real-time data modifications from MySQL, facilitating event-driven architectures and data synchronization scenarios.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_0\n\n\n\n----------------------------------------\n\nTITLE: Generating Config with redpanda-connect create\nDESCRIPTION: This command generates a Redpanda Connect configuration file that reads Kafka messages, decodes them with a schema registry service, and writes them to NATS JetStream.  The output is redirected to a YAML file named example.yaml.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/config/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nrpk connect create kafka/schema_registry_decode/nats_jetstream > example.yaml\n```\n\n----------------------------------------\n\nTITLE: Record and Schema Migration - End to End Flow\nDESCRIPTION: This sequence diagram provides an overview of the end-to-end process of migrating records and schemas using Redpanda Migrator. It outlines the interactions between various components, including the source and destination Schema Registries (SR), the Migrator itself, and the source and destination Kafka clusters. It covers schema retrieval, topic creation, ACL management, record processing, and schema ID updates.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/internal/impl/kafka/enterprise/redpanda_migrator.md#_snippet_2\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n\nparticipant Source SR\nparticipant SR Input\nparticipant SR Output\nparticipant Destination SR\nparticipant Source\nparticipant Migrator Input\nparticipant Migrator Output\nparticipant Destination\n\nSource SR->>SR Input: Read all via REST API\nSR Input->>SR Output: Stream schemas\nSR Output->>Destination SR: POST via REST API\nSource->>Migrator Output: Read all topics on startup\nMigrator Output->>Destination: Create all topics & ACLs\nSource->>Migrator Input: Record batch\nMigrator Input->>Migrator Output: Record batch\nMigrator Output->>Migrator Output: Lookup topic in local cache\nMigrator Output->>Source: Fetch topic config\nMigrator Output->>Destination: Create topic & ACLs and update local cache\nMigrator Output->>Migrator Output: R = Foreach record\nMigrator Output->>Migrator Output: Lookup schema ID X in local cache\nMigrator Output->>SR Output: GetDestinationSchemaID(X)\nSR Output->>SR Output: Lookup X in local cache\nSR Output->>Source SR: S = GetSchemaByID(X)\nSR Output->>Destination SR: Y = getOrCreateSchemaID(S)\nSR Output->>Migrator Output: Y\nMigrator Output->>Migrator Output: UpdateID(R, Y)\nMigrator Output->>Destination: Record batch\n```\n\n----------------------------------------\n\nTITLE: Processor Example: Confluent Schema Registry Encode in Go\nDESCRIPTION: This is an example implementation of a Benthos processor component that encodes data using Confluent Schema Registry. It illustrates how to create a component that transforms data using schemas. The implementation details are in the referenced Go file.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/internal/impl/README.md#_snippet_2\n\nLANGUAGE: Go\nCODE:\n```\n[./confluent/processor_schema_registry_encode.go](./confluent/processor_schema_registry_encode.go)\n```\n\n----------------------------------------\n\nTITLE: Adding Complex Database Type Support to `pg_stream`\nDESCRIPTION: Adds support for complex database types (JSONB, TEXT[], INET, TSVECTOR, TSRANGE, POINT, INTEGER[]) to the `pg_stream` input. This expands the range of data types that can be captured from PostgreSQL databases.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_61\n\n\n\n----------------------------------------\n\nTITLE: Adding Parquet File Support to `bigquery` Output\nDESCRIPTION: Adds support for Parquet files to the `bigquery` output. This enables writing data to Google BigQuery in the efficient Parquet format.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_62\n\n\n\n----------------------------------------\n\nTITLE: Creating Manifest with plugin_uploader.py (Shell)\nDESCRIPTION: The command creates a `manifest.json` file listing all available plugin archives and uploads it to an S3 bucket. It requires specifying the AWS region, S3 bucket, plugin name, and repository hostname. AWS permissions for `s3:PutObject`, `s3:ListBucket`, and `s3:GetObjectTagging` are required.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/resources/plugin_uploader/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n./plugin_uploader.py upload-manifest \\\n                        --region=<AWS_REGION> \\\n                        --bucket=<AWS_S3_BUCKET> \\\n                        --plugin=<PLUGIN_NAME> \\\n                        --repo-hostname=<REPO_HOSTNAME>\n```\n\n----------------------------------------\n\nTITLE: Fixing Snapshot Consistency in `postgres_cdc`\nDESCRIPTION: Fixes a snapshot stream consistency issue in the `postgres_cdc` input. This issue could lead to data loss if writes occurred during the snapshot phase of the change data capture process. The fix ensures consistent data capture during snapshot operations.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_7\n\n\n\n----------------------------------------\n\nTITLE: Adding `kafka_timestamp_ms` to Kafka Outputs\nDESCRIPTION: Adds the `kafka_timestamp_ms` field to the `kafka`, `kafka_franz`, `redpanda`, `redpanda_common` and `redpanda_migrator` outputs. This allows setting the timestamp of the Kafka message in milliseconds.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_74\n\n\n\n----------------------------------------\n\nTITLE: Uploading Archives with plugin_uploader.py (Shell)\nDESCRIPTION: The command creates tar.gz archives from binaries based on Goreleaser's artifacts and metadata files, then uploads them to an S3 bucket. It requires specifying artifacts file, metadata file, project root, AWS region and bucket, plugin name, goos and goarch values. AWS permissions for `s3:PutObject` and `s3:PutObjectTagging` are required.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/resources/plugin_uploader/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./plugin_uploader.py upload-archives \\\n                        --artifacts-file=$DIST/artifacts.json \\\n                        --metadata-file=$DIST/metadata.json \\\n                        --project-root-dir=<PROJECT_ROOT> \\\n                        --region=<AWS_REGION> \\\n                        --bucket=<AWS_S3_BUCKET> \\\n                        --plugin=<PLUGIN_NAME> \\\n                        --goos=<OS1,OS2,...> \\\n                        --goarch=<ARCH1,ARCH2,...>\n```\n\n----------------------------------------\n\nTITLE: Adding `ollama_moderation` Processor\nDESCRIPTION: Introduces a new `ollama_moderation` processor. This processor allows for moderation of LLM (Language Learning Model) responses using LlamaGuard or ShieldGemma to check for safety and appropriateness.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_28\n\n\n\n----------------------------------------\n\nTITLE: Adding Tool Calling Support to `ollama_chat`\nDESCRIPTION: Adds tool calling support to the `ollama_chat` processor. This enables the processor to interact with external tools or services to augment its capabilities.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_27\n\n\n\n----------------------------------------\n\nTITLE: Adding Kafka Session Timeout Fields\nDESCRIPTION: Adds `rebalance_timeout`, `session_timeout` and `heartbeat_interval` fields to the Kafka consumer inputs (`kafka_franz`, `redpanda`, `redpanda_common`, `redpanda_migrator` and `ockam_kafka`). These fields configure the Kafka consumer's session management behavior.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_2\n\n\n\n----------------------------------------\n\nTITLE: Adding `queries` Field to SQL Components\nDESCRIPTION: Adds the `queries` field to both the `sql_raw` processor and output components. This enables the execution of multiple SQL statements transactionally, providing greater control over database interactions.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_29\n\n\n\n----------------------------------------\n\nTITLE: Adding `schema_evolution.processors` to `snowpipe_streaming`\nDESCRIPTION: Adds the `schema_evolution.processors` field to `snowpipe_streaming` output. This field facilitates custom processing steps (side effects or enrichment) during schema evolution, providing greater control over data transformation during schema changes.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_5\n\n\n\n----------------------------------------\n\nTITLE: Adding `avro.preserve_logical_types` to `schema_registry_decode`\nDESCRIPTION: Adds the `avro.preserve_logical_types` field to the `schema_registry_decode` processor. This new field allows users to preserve logical types within Avro schemas during decoding, preventing them from being converted to primitive types.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_3\n\n\n\n----------------------------------------\n\nTITLE: Cache Example: Redis Cache in Go\nDESCRIPTION: This is an example implementation of a Benthos cache component using Redis. It demonstrates how to use Redis as a cache within a Benthos pipeline. The implementation details are in the referenced Go file.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/internal/impl/README.md#_snippet_4\n\nLANGUAGE: Go\nCODE:\n```\n[./redis/cache.go](./redis/cache.go)\n```\n\n----------------------------------------\n\nTITLE: Rate Limit Example: Redis Rate Limit in Go\nDESCRIPTION: This is an example implementation of a Benthos rate limit component using Redis. It demonstrates how to use Redis to implement rate limiting within a Benthos pipeline. The implementation details are in the referenced Go file.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/internal/impl/README.md#_snippet_6\n\nLANGUAGE: Go\nCODE:\n```\n[./redis/rate_limit.go](./redis/rate_limit.go)\n```\n\n----------------------------------------\n\nTITLE: Adding `fetch_max_wait` Field to Kafka Inputs\nDESCRIPTION: Adds the `fetch_max_wait` field to the Kafka input components (`kafka_franz`, `ockam_kafka`, `redpanda`, `redpanda_common`, and `redpanda_migrator`).  This configuration setting controls the maximum time to wait for new data when fetching from Kafka.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_23\n\n\n\n----------------------------------------\n\nTITLE: Adding `unchanged_toast_value` to `postgres_cdc`\nDESCRIPTION: Adds the `unchanged_toast_value` field to the `postgres_cdc` input. This setting specifies the value used to represent unchanged TOAST (The Oversized Attribute Storage Technique) values when a PostgreSQL table lacks a full replica identity.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_6\n\n\n\n----------------------------------------\n\nTITLE: Adding `redpanda_lag` Metric to Redpanda Inputs\nDESCRIPTION: The `redpanda` and `redpanda_common` inputs now emit the `redpanda_lag` metric. This metric provides insight into the lag between the consumer and the latest message in the topic, enabling real-time monitoring.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_33\n\n\n\n----------------------------------------\n\nTITLE: Improving Error Handling in `snowpipe_streaming`\nDESCRIPTION: Improves error handling for authentication failures in the `snowpipe_streaming` output. The enhancement focuses on providing more robust and informative error messages when uploading data to cloud storage during authentication failures, improving the operational experience.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_10\n\n\n\n----------------------------------------\n\nTITLE: Adding `pg_stream` Input (PostgreSQL CDC)\nDESCRIPTION: Adds a new `pg_stream` input supporting change data capture (CDC) from PostgreSQL databases. This input captures and streams changes from PostgreSQL.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_72\n\n\n\n----------------------------------------\n\nTITLE: Output Example: NATS JetStream Output in Go\nDESCRIPTION: This is an example implementation of a Benthos output component using NATS JetStream. It shows how to create a component that publishes data to a NATS JetStream topic. The implementation details are in the referenced Go file.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/internal/impl/README.md#_snippet_1\n\nLANGUAGE: Go\nCODE:\n```\n[./nats/output_jetstream.go](./nats/output_jetstream.go)\n```\n\n----------------------------------------\n\nTITLE: Adding Table Name Interpolation to `snowpipe_streaming`\nDESCRIPTION: Adds support for interpolating table names within the `snowpipe_streaming` output. This feature enables dynamic table name generation based on message content or context.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_24\n\n\n\n----------------------------------------\n\nTITLE: Fixing TOAST Propagation Issue in `postgres_cdc`\nDESCRIPTION: Fixes an issue with `postgres_cdc` where TOAST values were not being propagated when using `REPLICA IDENTITY FULL`. This ensures that large attribute values are correctly captured and transmitted during CDC operations.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_43\n\n\n\n----------------------------------------\n\nTITLE: Adding Avro Schema Metadata to `avro` Scanner\nDESCRIPTION: The `avro` scanner now emits metadata for the Avro schema used, along with the schema fingerprint. This provides information about the schema used to process Avro data, aiding in data lineage and schema management.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_21\n\n\n\n----------------------------------------\n\nTITLE: Example Redpanda Connect Config YAML\nDESCRIPTION: This YAML configuration file shows a basic setup with a `generate` input and an `aws_s3` output. It's used as an example to illustrate what kind of information Redpanda Connect's telemetry collects (but not the specific values). The telemetry system only extracts the types of inputs and outputs used.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/internal/telemetry/README.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ninput:\n  label: fooer\n  generate:\n    interval: 1s\n    mapping: 'root.foo = \"bar\"'\n\noutput:\n  label: bazer\n  aws_s3:\n    bucket: baz\n    path: meow.txt\n```\n\n----------------------------------------\n\nTITLE: Fixing `pg_stream` Issue with UUID Type\nDESCRIPTION: Fixed `pg_stream` issue with discrepancies between replication and snapshot streaming for `UUID` type. This ensures consistency when capturing UUID data during replication and snapshot processes.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_65\n\n\n\n----------------------------------------\n\nTITLE: Adding Metadata Field Usage within Template Mapping\nDESCRIPTION: The metadata field `label` can now be utilized within a template's `mapping` field to access the label associated with the template instantiation in a config within Benthos. This enhances template flexibility by allowing access to contextual information.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_37\n\n\n\n----------------------------------------\n\nTITLE: Registering Destination Schemas in `redpanda_migrator`\nDESCRIPTION: The `redpanda_migrator` output now registers destination schemas with all the subjects associated with the source schema ID extracted from each message. This improves schema management and data integrity during migrations.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_67\n\n\n\n----------------------------------------\n\nTITLE: Adding Metadata to `schema_registry_decode`\nDESCRIPTION: The `schema_registry_decode` processor now adds metadata `schema_id` which stores the schema ID from the schema registry used to decode the message. This is used for tracking the schema used in decoding events.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_4\n\n\n\n----------------------------------------\n\nTITLE: Adding `max_records_per_request` to `aws_sqs` Output\nDESCRIPTION: Adds the `max_records_per_request` field to the `aws_sqs` output.  This allows you to control the maximum number of records sent in each request to SQS, improving efficiency.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_69\n\n\n\n----------------------------------------\n\nTITLE: Adding `topic_lag_refresh_period` to Redpanda Inputs\nDESCRIPTION: Adds the `topic_lag_refresh_period` field to both the `redpanda` and `redpanda_common` input components. This field configures how frequently the topic lag is refreshed, allowing users to control the granularity of lag monitoring.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_32\n\n\n\n----------------------------------------\n\nTITLE: Adding `max_outstanding` Field to `aws_sqs` Input\nDESCRIPTION: Adds the `max_outstanding` field to the `aws_sqs` input. This field prevents unbounded memory usage by limiting the number of messages that can be outstanding (received but not yet processed) at any given time.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_20\n\n\n\n----------------------------------------\n\nTITLE: Fixing Errors in `gcp_bigquery` Output with Parquet Format\nDESCRIPTION: Fixes an issue in the `gcp_bigquery` output where errors were incorrectly returned when using the Parquet format. This ensures that data is written to BigQuery successfully when using Parquet.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_53\n\n\n\n----------------------------------------\n\nTITLE: Adding `content_type` Field to `amqp_1` Output\nDESCRIPTION: Adds the `content_type` field to the `amqp_1` output. This allows users to specify the content type of messages sent to AMQP 1.0 brokers, providing flexibility in message formatting.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_22\n\n\n\n----------------------------------------\n\nTITLE: Fixing Lease Refresh Issue in `aws_sqs`\nDESCRIPTION: Fixes an issue in `aws_sqs` where refreshing in-flight message leases could prevent acks from being processed.  This resolves an issue where processed messages could be re-delivered because the ack was never sent.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_42\n\n\n\n----------------------------------------\n\nTITLE: Adding `redpanda_migrator_offsets` Input\nDESCRIPTION: Introduces a new `redpanda_migrator_offsets` input component. This input is specifically designed for reading and processing Redpanda migrator offsets.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_30\n\n\n\n----------------------------------------\n\nTITLE: Tracer Provider Example: OTLP Tracer in Go\nDESCRIPTION: This is an example implementation of a Benthos tracer provider component using OTLP (OpenTelemetry Protocol). It demonstrates how to integrate Benthos with an OTLP-compliant tracing backend. The implementation details are in the referenced Go file.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/internal/impl/README.md#_snippet_8\n\nLANGUAGE: Go\nCODE:\n```\n[./otlp/tracer_otlp.go](./otlp/tracer_otlp.go)\n```\n\n----------------------------------------\n\nTITLE: Adding `instance_id` Field to Kafka Inputs\nDESCRIPTION: Adds the `instance_id` field to several Kafka-related input components (`kafka`, `kafka_franz`, `ockam_kafka`, `redpanda`, `redpanda_common`, and `redpanda_migrator`). This field is intended to help identify the specific instance of the Kafka consumer within a larger deployment.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_1\n\n\n\n----------------------------------------\n\nTITLE: Adding `SetOutputBrokerPattern` Method to `StreamBuilder`\nDESCRIPTION: Adds the `SetOutputBrokerPattern` method to the `StreamBuilder` type in the Benthos Go API. This new method provides programmatic control over the output broker pattern for streams.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_39\n\n\n\n----------------------------------------\n\nTITLE: Adding `kafka_lag` Metadata to Redpanda Inputs\nDESCRIPTION: The `redpanda` and `redpanda_common` inputs now emit the `kafka_lag` metadata. This metadata provides information about the Kafka lag, enriching the data flowing through Redpanda Connect.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_34\n\n\n\n----------------------------------------\n\nTITLE: Adding Exactly-Once Delivery to `snowpipe_streaming`\nDESCRIPTION: Adds support for exactly-once delivery using the `offset_token` within the `snowpipe_streaming` output. This ensures that each message is delivered to Snowflake exactly once, even in the event of failures or retries.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_26\n\n\n\n----------------------------------------\n\nTITLE: Starting Services with Docker Compose\nDESCRIPTION: This command starts the services defined in the docker-compose.yml file in detached mode. This allows the services to run in the background.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/resources/docker/schema_registry/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndocker-compose up -d\n```\n\n----------------------------------------\n\nTITLE: Schema Creation Detail\nDESCRIPTION: This sequence diagram details the steps involved in schema creation, specifically focusing on the interactions between the Source Schema Registry (SR), SR Output, and the Destination SR when given an ID `X`, subject `S` and version `V` along with the schema body `B` as input. It illustrates the lookup process, fetching of references and previous versions, backfilling, and caching operations.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/internal/impl/kafka/enterprise/redpanda_migrator.md#_snippet_3\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n\nparticipant Source SR\nparticipant SR Output\nparticipant Destination SR\n\nSR Output->>SR Output: Lookup ID Y = (X,S,V) in local cache\nSR Output->>Source SR: Fetch B.references recursively\nSR Output->>Destination SR: Backfill references and cache new IDs locally\nSR Output->>Source SR: Fetch previous versions for S\nSR Output->>Destination SR: Backfill previous versions and cache new IDs locally\nSR Output->>Destination SR: Create Y from (S,V,B)\nSR Output->>SR Output: Cache Y = (X,S,V) in the local cache\n\n```\n\n----------------------------------------\n\nTITLE: Running Benthos Docker with CLI Flags\nDESCRIPTION: This command runs the Benthos Docker image and configures it using command-line flags (`-s`). It sets the input type to `http_server`, the output type to `kafka`, specifies the Kafka broker address and topic.  The `-p` flag maps port 4195 from the host to the container, enabling access to the http_server. The `--rm` flag ensures the container is removed after it exits.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/resources/docker/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndocker run --rm -p 4195:4195 ghcr.io/redpanda-data/connect \\\n  -s \"input.type=http_server\" \\\n  -s \"output.type=kafka\" \\\n  -s \"output.kafka.addresses=kafka-server:9092\" \\\n  -s \"output.kafka.topic=benthos_topic\"\n```\n\n----------------------------------------\n\nTITLE: Migrate Consumer Group Updates - Arbitrary Offset\nDESCRIPTION: This sequence diagram illustrates the process of migrating a consumer group update for a record located at an arbitrary offset within a topic partition. It involves retrieving the timestamp of the consumed record in the source cluster and then using this timestamp to look up the corresponding offset in the destination cluster.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/internal/impl/kafka/enterprise/redpanda_migrator.md#_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n\nparticipant Source\nparticipant Offsets Input\nparticipant Offsets Output\nparticipant Destination\n\nSource->>Offsets Input: O = Receive(__consumer_offsets)\nSource->>Offsets Input: X = ListEndOffsets(T, P)\nSource->>Offsets Input: Check X < O\nSource->>Offsets Input: TS = ReadTimestamp(T, P, O)\nOffsets Input->>Offsets Output: (T, P, TS)\nOffsets Output->>Destination: O' = ListOffsetsAfterMilli(T, P, TS)\nOffsets Output->>Destination: CommitOffsets(T, P, O')\n```\n\n----------------------------------------\n\nTITLE: Running Redpanda Connect with Docker using flags\nDESCRIPTION: This command runs Redpanda Connect within a Docker container, configuring the input and output connectors via command-line flags. The `-s` flags set various configuration options, overriding any defaults or settings in a config file. This allows for quick and dynamic configuration without needing to manage config files.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/README.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ndocker run --rm -p 4195:4195 docker.redpanda.com/redpandadata/connect run \\\n  -s \"input.type=http_server\" \\\n  -s \"output.type=kafka\" \\\n  -s \"output.kafka.addresses=kafka-server:9092\" \\\n  -s \"output.kafka.topic=redpanda_topic\"\n```\n\n----------------------------------------\n\nTITLE: Fixing Quoted Table Metadata in `postgres_cdc`\nDESCRIPTION: Fixes an issue where the `@table` metadata was incorrectly quoted during the snapshot phase within the `postgres_cdc` input. The fix ensures that table names are handled correctly as metadata during snapshot operations.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_8\n\n\n\n----------------------------------------\n\nTITLE: Running Redpanda Connect with mounted config and data\nDESCRIPTION: This command demonstrates how to run the Redpanda Connect Docker image, mounting both a configuration file and a data volume from the host. This allows the container to access persistent configuration and write data to the host machine. The `-p` flag exposes port 4195 for external access.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/README.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\ndocker run --rm \\\n\t-v /path/to/your/benthos.yaml:/config.yaml \\\n\t-v /tmp/data:/data \\\n\t-p 4195:4195 \\\n\tdocker.redpanda.com/redpandadata/connect run /config.yaml\n```\n\n----------------------------------------\n\nTITLE: Handling Null Values Correctly in `postgres_cdc`\nDESCRIPTION: Ensures that the `postgres_cdc` input correctly handles `null` values. This fix addresses potential issues with the accurate capture and representation of null values during change data capture from PostgreSQL.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_17\n\n\n\n----------------------------------------\n\nTITLE: Addressing `snowflake_streaming` Channel Creation Issue\nDESCRIPTION: Addresses an issue where `snowflake_streaming` could create more channels than configured. This ensures that only the configured number of channels are created for Snowflake streaming.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_77\n\n\n\n----------------------------------------\n\nTITLE: Input Example: NATS JetStream Input in Go\nDESCRIPTION: This is an example implementation of a Benthos input component using NATS JetStream.  It demonstrates how to create a component that consumes data from a NATS JetStream topic.  The implementation details are in the referenced Go file.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/internal/impl/README.md#_snippet_0\n\nLANGUAGE: Go\nCODE:\n```\n[./nats/input_jetstream.go](./nats/input_jetstream.go)\n```\n\n----------------------------------------\n\nTITLE: Preventing Message Rejection in `redpanda_migrator`\nDESCRIPTION: Prevents the `redpanda_migrator` output from rejecting messages when schema ID translation fails. This fix ensures that messages are not dropped if the schema ID translation process encounters issues.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_18\n\n\n\n----------------------------------------\n\nTITLE: Adding Channel Name Interpolation to `snowpipe_streaming`\nDESCRIPTION: Adds support for interpolating channel names within the `snowpipe_streaming` output. This allows for dynamic channel naming based on message content or context.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_25\n\n\n\n----------------------------------------\n\nTITLE: Allowing Quoted Identifiers in `postgres_cdc`\nDESCRIPTION: The `postgres_cdc` input now allows quoted identifiers for table names. This enables the use of table names that contain special characters or reserved words.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_54\n\n\n\n----------------------------------------\n\nTITLE: Preventing Kafka Key Conversion to String in `redpanda_migrator`\nDESCRIPTION: Prevents the `redpanda_migrator` input from incorrectly converting the Kafka key to a string. The fix ensures that the Kafka key is preserved in its original data type during migration processes.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_19\n\n\n\n----------------------------------------\n\nTITLE: Adding Offset Fields to `redpanda_migrator_offsets` Output\nDESCRIPTION: Adds the `offset_topic`, `offset_group`, `offset_partition`, `offset_commit_timestamp`, and `offset_metadata` fields to the `redpanda_migrator_offsets` output. These fields provide detailed control and metadata enrichment for offset management.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_31\n\n\n\n----------------------------------------\n\nTITLE: Buffer Example: SQLite Buffer in Go\nDESCRIPTION: This is an example implementation of a Benthos buffer component using SQLite. It shows how to use SQLite to buffer data within a Benthos pipeline. The implementation details are in the referenced Go file.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/internal/impl/README.md#_snippet_5\n\nLANGUAGE: Go\nCODE:\n```\n[./sql/buffer_sqlite.go](./sql/buffer_sqlite.go)\n```\n\n----------------------------------------\n\nTITLE: Fixing File Deletion Bug in `sftp` Input\nDESCRIPTION: Fixes a bug in the `sftp` input where the last file was not deleted when `watcher` and `delete_on_finish` were enabled. This ensures that all files are correctly deleted when the input is configured to watch for changes and delete files upon completion.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_45\n\n\n\n----------------------------------------\n\nTITLE: Adding Spanner Driver Support to SQL Plugins\nDESCRIPTION: Adds support for the `spanner` driver to SQL plugins. This enables Redpanda Connect to interact with Google Cloud Spanner databases.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_60\n\n\n\n----------------------------------------\n\nTITLE: Setting Labels for Subcomponents in `redpanda_migrator_bundle`\nDESCRIPTION: The `redpanda_migrator_bundle` input and output now set labels for their subcomponents. This enhances observability and helps in tracing and debugging subcomponent behavior.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_35\n\n\n\n----------------------------------------\n\nTITLE: Deprecating `avro_raw_json` for `avro.raw_unions`\nDESCRIPTION: Deprecates the `avro_raw_json` field and introduces `avro.raw_unions` for the `schema_registry_decode` processor. This change represents a refinement in how Avro raw unions are handled, promoting a more standardized and consistent approach.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_9\n\n\n\n----------------------------------------\n\nTITLE: Adding `timestamp` Bloblang Method\nDESCRIPTION: (Benthos) New Bloblang method `timestamp` added for manipulating timestamps within Bloblang expressions.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_75\n\n\n\n----------------------------------------\n\nTITLE: Migrate Consumer Group Updates - End of Topic Offset\nDESCRIPTION: This sequence diagram illustrates the process of migrating a consumer group update for a record located at the end of a topic partition.  It involves retrieving the timestamp of the last record in the source and translates this timestamp to destination cluster.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/internal/impl/kafka/enterprise/redpanda_migrator.md#_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n\nparticipant Source\nparticipant Offsets Input\nparticipant Offsets Output\nparticipant Destination\n\nSource->>Offsets Input: O = Receive(__consumer_offsets)\nSource->>Offsets Input: X = ListEndOffsets(T, P)\nSource->>Offsets Input: Check X == O\nSource->>Offsets Input: TS = ReadTimestamp(T, P, -1)\nOffsets Input->>Offsets Output: (T, P, TS)\nOffsets Output->>Destination: O', TS' = ListOffsetsAfterMilli(T, P, TS)\nOffsets Output->>Destination: If TS' != -1 then O'' = ListEndOffsets(T, P)\nOffsets Output->>Destination: CommitOffsets(T, P, O' + 1 ==  O'' ? O'' : O')\n```\n\n----------------------------------------\n\nTITLE: Changing `mode` Metadata to `operation` in `postgres_cdc`\nDESCRIPTION: The `postgres_cdc` input no longer emits `mode` metadata. Instead, snapshot reads set `operation` metadata to `read` instead of `insert`.  This clarifies the operation type for snapshot data.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_58\n\n\n\n----------------------------------------\n\nTITLE: Adding `benchmark` Processor\nDESCRIPTION: (Benthos) New `benchmark` processor added to support measuring performance of parts of a Benthos config.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_76\n\n\n\n----------------------------------------\n\nTITLE: Adding `redpanda-license` CLI Flag\nDESCRIPTION: Adds a new CLI flag `redpanda-license` as an alternative way to specify a Redpanda license. This provides a more convenient way to pass the license key to Redpanda Connect.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_64\n\n\n\n----------------------------------------\n\nTITLE: Adding `exists` Operator to the `cache` Processor\nDESCRIPTION: Adds a new `exists` operator to the `cache` processor within Benthos. This operator allows checking if a key exists in the cache.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_63\n\n\n\n----------------------------------------\n\nTITLE: Fixing Initial Snapshot Streaming Consistency in `postgres_cdc`\nDESCRIPTION: Fixes an initial snapshot streaming consistency issue with `postgres_cdc`. This ensures that the snapshot phase correctly captures all data, preventing data loss or inconsistencies when initiating CDC operations.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_44\n\n\n\n----------------------------------------\n\nTITLE: Registering Schema with Shell Script\nDESCRIPTION: This command executes the insert_schema.sh script. This script is responsible for registering the initial schema with the schema registry service.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/resources/docker/schema_registry/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n./insert_schema.sh\n```\n\n----------------------------------------\n\nTITLE: Adding New Bloblang Functions\nDESCRIPTION: Adds new `error_source_name`, `error_source_label` and `error_source_path` Bloblang functions to Benthos. These functions provide access to error source information, aiding in error handling and debugging.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_40\n\n\n\n----------------------------------------\n\nTITLE: Metrics Exporter Example: Prometheus Metrics in Go\nDESCRIPTION: This is an example implementation of a Benthos metrics exporter component using Prometheus. It shows how to expose Benthos metrics to Prometheus. The implementation details are in the referenced Go file.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/internal/impl/README.md#_snippet_7\n\nLANGUAGE: Go\nCODE:\n```\n[./prometheus/metrics_prometheus.go](./prometheus/metrics_prometheus.go)\n```\n\n----------------------------------------\n\nTITLE: Installing Redpanda Connect on Linux using curl\nDESCRIPTION: This shell command downloads and installs Redpanda Connect on a Linux system. It retrieves the latest release of the rpk tool, unpacks it, and places it in the user's local bin directory, making it accessible from the command line. It depends on `curl` and `unzip` commands.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncurl -LO https://github.com/redpanda-data/redpanda/releases/latest/download/rpk-linux-amd64.zip\nunzip rpk-linux-amd64.zip -d ~/.local/bin/\n```\n\n----------------------------------------\n\nTITLE: Fixing SIGSEGV in `postgres_cdc` with TOAST Values\nDESCRIPTION: Fixes a SIGSEGV (segmentation fault) error in the `postgres_cdc` input that occurred when using TOAST values in tables without FULL replica identity. This issue caused the process to crash when encountering specific combinations of TOAST values and replica identity configurations.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_15\n\n\n\n----------------------------------------\n\nTITLE: Dropping Support for PostgreSQL 10 and 11 in `postgres_cdc`\nDESCRIPTION: Removes support for PostgreSQL versions 10 and 11 from the `postgres_cdc` input. This reflects an update in the supported PostgreSQL versions for change data capture operations, potentially due to compatibility or maintenance considerations.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_13\n\n\n\n----------------------------------------\n\nTITLE: Changing Default Output Behavior in Streams\nDESCRIPTION: Streams and the StreamBuilder API now use `reject` by default when no output is specified in the config and `stdout` isn't registered (for example when the `io` components are not imported) within Benthos. This behavior change ensures that messages are rejected when no valid output is configured.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_51\n\n\n\n----------------------------------------\n\nTITLE: Viewing Generated Messages with Docker Compose Logs\nDESCRIPTION: This command displays the logs of the connect-out service in real-time.  It is used to observe the messages generated by the service.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/resources/docker/schema_registry/README.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ndocker-compose logs -f connect-out\n```\n\n----------------------------------------\n\nTITLE: Deprecating Fields for `redpanda_migrator` Input\nDESCRIPTION: Deprecates the `batch_size`, `multi_header`, `replication_factor`, `replication_factor_override`, and `output_resource` fields for the `redpanda_migrator` input. These fields are being phased out in favor of newer configuration options.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_46\n\n\n\n----------------------------------------\n\nTITLE: Skipping Schema ID Translation in `redpanda_migrator_bundle`\nDESCRIPTION: The `redpanda_migrator_bundle` output now skips schema ID translation when `translate_schema_ids: false` and `schema_registry` is configured. This prevents unnecessary schema ID translation when it's explicitly disabled.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_56\n\n\n\n----------------------------------------\n\nTITLE: Preventing Schema ID Translation in `redpanda_migrator_bundle`\nDESCRIPTION: The `redpanda_migrator_bundle` output no longer attempts to translate schema IDs when a schema registry is not configured. This avoids unnecessary errors and improves performance when no schema registry is available.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_59\n\n\n\n----------------------------------------\n\nTITLE: Adding `bloblang` Scalar Type to Template Fields\nDESCRIPTION: Adds the `bloblang` scalar type to template fields within Benthos. This enables the use of Bloblang expressions directly within template configurations, simplifying data transformation and processing.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_38\n\n\n\n----------------------------------------\n\nTITLE: Starting Redpanda and Creating Topics\nDESCRIPTION: This snippet starts the Redpanda cluster and related services (e.g., Grafana) using Docker Compose. It then creates four test topics (testing_a, testing_b, testing_c, testing_d) in Redpanda, each with 10 partitions, using the `rpk topic create` command. This sets up the environment for subsequent benchmarking.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/resources/docker/redpanda_benchmarking/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n# Start redpanda, grafana, etc\ndocker-compose up -d\n\n# Create some test topics\nrpk topic create testing_a -p 10\nrpk topic create testing_b -p 10\nrpk topic create testing_c -p 10\nrpk topic create testing_d -p 10\n```\n\n----------------------------------------\n\nTITLE: Changing Log Level in `branch` Processor\nDESCRIPTION: The `branch` processor no longer emits a log entry at the error level when child processors throw errors within Benthos. This change reduces noise in the logs and allows for more granular error handling.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_50\n\n\n\n----------------------------------------\n\nTITLE: Adding `metadata_max_age` to `redpanda_migrator_offsets`\nDESCRIPTION: Adds the field `metadata_max_age` to the `redpanda_migrator_offsets` output. This field controls the maximum age of the offset metadata before it is refreshed.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_73\n\n\n\n----------------------------------------\n\nTITLE: Running unit tests for Redpanda Connect\nDESCRIPTION: This command runs the unit tests for the Redpanda Connect project. It executes the tests defined in the Go code and reports any failures. This ensures the codebase is functioning as expected and helps catch bugs early. Note: It skips integration tests.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/README.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nmake test\n```\n\n----------------------------------------\n\nTITLE: Fixing `aws_s3` Error Handling\nDESCRIPTION: Fixes an issue where empty files read by the `aws_s3` input incorrectly triggered errors. This resolves a spurious error condition when processing empty files from AWS S3.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_14\n\n\n\n----------------------------------------\n\nTITLE: Building Redpanda Connect with extra dependencies\nDESCRIPTION: This command builds Redpanda Connect with extra components that require linking to external C libraries (e.g., zmq4). The `-tags \"x_benthos_extra\"` flag enables these optional dependencies during the build process. This build will fail if the required external libraries are not installed on the system.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/README.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n# With go\ngo install -tags \"x_benthos_extra\" github.com/redpanda-data/connect/v4/cmd/redpanda-connect@latest\n\n# Using make\nmake TAGS=x_benthos_extra\n```\n\n----------------------------------------\n\nTITLE: Fixing `avro` Scanner Bug\nDESCRIPTION: Fixed `avro` scanner bug introduced in v4.25.0. This resolves an issue that affected the ability of the avro scanner to parse Avro files correctly.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_66\n\n\n\n----------------------------------------\n\nTITLE: Installing dependencies (Shell)\nDESCRIPTION: Installs the necessary python dependencies using pip. This ensures that the plugin uploader script can run without errors. It requires a `requirements.txt` file with all the dependencies listed.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/resources/plugin_uploader/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running specific integration tests\nDESCRIPTION: This command allows you to run specific integration tests within a connector directory. The `go test` command is used with a regular expression to match the desired tests. Integration tests typically involve external dependencies and are skipped by the standard `make test` command.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/README.md#_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\ngo test -run \"^Test.*Integration.*$\" ./internal/impl/<connector directory>/...\n```\n\n----------------------------------------\n\nTITLE: Deprecating Fields for `redpanda_migrator_offsets` Output\nDESCRIPTION: Deprecates the `kafka_key` and `max_in_flight` fields for the `redpanda_migrator_offsets` output. These fields are being replaced with alternative mechanisms for managing offset data.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_47\n\n\n\n----------------------------------------\n\nTITLE: Adding `label` Field to Template Tests\nDESCRIPTION: Adds the `label` field to the template tests definitions within Benthos. This allows users to associate labels with template tests, improving test organization and identification.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_36\n\n\n\n----------------------------------------\n\nTITLE: Deprecating `batching` for `redpanda_migrator` Output\nDESCRIPTION: Deprecates the `batching` field for the `redpanda_migrator` output.  This configuration option is being replaced with newer batching strategies.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_48\n\n\n\n----------------------------------------\n\nTITLE: Adding `--verbose` Flag to Lint Commands\nDESCRIPTION: Adds the `--verbose` flag to the `benthos lint` and `benthos template lint` commands in Benthos. This flag enables more detailed output during linting, providing more insights into potential issues.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_41\n\n\n\n----------------------------------------\n\nTITLE: Deprecating `schema_evolution.new_column_type_mapping`\nDESCRIPTION: Deprecates the `schema_evolution.new_column_type_mapping` field within the `snowpipe_streaming` component. The recommended replacement is `schema_evolution.processors`, aligning schema evolution configuration with the newer processor-based approach.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_11\n\n\n\n----------------------------------------\n\nTITLE: Linting Redpanda Connect code\nDESCRIPTION: This command runs the linter on the Redpanda Connect codebase. It uses `golangci-lint` to check the code for style issues, potential errors, and other problems. This helps maintain code quality and consistency.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/README.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nmake lint\n```\n\n----------------------------------------\n\nTITLE: Running Integration Tests with Snowflake Credentials in Go\nDESCRIPTION: This snippet demonstrates how to execute integration tests for the Snowflake connector using the `go test` command. It requires setting environment variables for the Snowflake user, account, and database. The test expects an unencrypted private key to be present in the `resources` directory.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/internal/impl/snowflake/streaming/README.md#_snippet_0\n\nLANGUAGE: Go\nCODE:\n```\nSNOWFLAKE_USER=XXX \\\n  SNOWFLAKE_ACCOUNT=alskjd-asdaks \\\n  SNOWFLAKE_DB=xxx \\\n  go test -v .\n```\n\n----------------------------------------\n\nTITLE: Removing Tombstone Messages from `redpanda_migrator` Input\nDESCRIPTION: The `redpanda_migrator` input no longer emits tombstone messages. Tombstone messages are typically used to mark deleted records, but are no longer required in the data migration process.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_49\n\n\n\n----------------------------------------\n\nTITLE: Fixing CLI Template Rejection Issue\nDESCRIPTION: (Benthos) Fixed an issue where running a CLI with a custom environment would cause imported templates to be rejected. This ensures that templates are properly imported even with custom environments.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_70\n\n\n\n----------------------------------------\n\nTITLE: Building Redpanda Connect from source with Make\nDESCRIPTION: These commands clone the Redpanda Connect repository, navigate into the project directory, and build the project using the `make` command. This process compiles the Go code and produces an executable binary. It requires Go and Make to be installed.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/README.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ngit clone git@github.com:redpanda-data/connect\ncd connect\nmake\n```\n\n----------------------------------------\n\nTITLE: Scanner Example: Avro Scanner in Go\nDESCRIPTION: This is an example implementation of a Benthos scanner component related to Avro data. It provides an example of how to scan Avro formatted data. The implementation details are in the referenced Go file.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/internal/impl/README.md#_snippet_3\n\nLANGUAGE: Go\nCODE:\n```\n[./avro/scanner.go](./avro/scanner.go)\n```\n\n----------------------------------------\n\nTITLE: Building Redpanda Connect Docker image\nDESCRIPTION: This command builds a Redpanda Connect Docker image using the provided `Dockerfile`. The resulting image is minimal, built from scratch, and contains the Redpanda Connect executable. Docker must be installed and running.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/README.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nmake docker\n```\n\n----------------------------------------\n\nTITLE: Declaring Project Dependencies\nDESCRIPTION: This snippet declares the dependencies for the /redpanda-data/connect project using specific version constraints. It specifies pydantic (version 2.8 or greater), boto3 (version 1.26 or greater), and click (exactly version 8.1.7). These dependencies are required for the project to function correctly.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/resources/plugin_uploader/requirements.txt#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\npydantic>=2.8\nboto3>=1.26\nclick==8.1.7\n```\n\n----------------------------------------\n\nTITLE: Installing Redpanda Connect using Homebrew\nDESCRIPTION: This command uses Homebrew to install Redpanda Connect. It assumes that Homebrew is already installed and configured on the system. This provides a convenient way to install and manage Redpanda Connect on macOS.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nbrew install redpanda-data/tap/redpanda\n```\n\n----------------------------------------\n\nTITLE: Generating SSH Keys with OpenSSL\nDESCRIPTION: These commands generate RSA keys using openssl, create a password-protected key, and an unencrypted key for testing. The encrypted key is specifically encoded using `-v2 des3` to ensure compatibility with PKCS#5 v2.0.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/internal/impl/snowflake/resources/ssh_keys/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n> openssl genrsa 2048 | openssl pkcs8 -topk8 -v2 des3 -inform PEM -passout pass:test123 -out internal/impl/snowflake/resources/ssh_keys/snowflake_rsa_key.p8\n> openssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -nocrypt -out internal/impl/snowflake/resources/ssh_keys/snowflake_rsa_key.pem\n```\n\n----------------------------------------\n\nTITLE: Renaming `pg_stream` to `postgres_cdc`\nDESCRIPTION: The `pg_stream` input has been renamed to `postgres_cdc`.  The old name will continue to function as an alias for backward compatibility.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_57\n\n\n\n----------------------------------------\n\nTITLE: Removing `-cgo` Suffixed Docker Images\nDESCRIPTION: The `-cgo` suffixed docker images are no longer built and pushed along with the regular images. It is still possible to create your own CGO builds with the command `CGO_ENABLED=1 make TAGS=x_benthos_extra redpanda-connect`.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_71\n\n\n\n----------------------------------------\n\nTITLE: Pulling the Redpanda Connect Docker image\nDESCRIPTION: This command pulls the latest Redpanda Connect Docker image from the Docker Hub repository. It's necessary to have Docker installed on the system. This image contains everything needed to run Redpanda Connect in a containerized environment.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndocker pull docker.redpanda.com/redpandadata/connect\n```\n\n----------------------------------------\n\nTITLE: Fixing Interpolation Support in `javascript` Processor Docs\nDESCRIPTION: Removes erroneous mentions of interpolation support for the `code` and `file` fields in the documentation for the `javascript` processor. This clarifies the actual capabilities of the processor and avoids misleading documentation.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_16\n\n\n\n----------------------------------------\n\nTITLE: Adding License Package to Go API\nDESCRIPTION: Adds a new `public/license` package to the Go API. This allows programmatic instantiations of Redpanda Connect to run enterprise license components.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_52\n\n\n\n----------------------------------------\n\nTITLE: Enforcing Redpanda License for Enterprise Features\nDESCRIPTION: Enterprise features will now only run when a valid Redpanda license is present. This enforces the licensing requirements for enterprise functionalities and ensures proper usage.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_68\n\n\n\n----------------------------------------\n\nTITLE: Validating Redpanda Enterprise Trial Licenses\nDESCRIPTION: Trial Redpanda Enterprise licenses are now considered valid.  This corrects an issue that prevented trial licenses from being recognized.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_55\n\n\n\n----------------------------------------\n\nTITLE: Increasing Default `max_message_bytes` and `broker_write_max_bytes`\nDESCRIPTION: Increases the default values for `max_message_bytes` and `broker_write_max_bytes`, using IEC units (e.g., KiB, MiB, GiB) instead of SI units (e.g., KB, MB, GB). This aligns with Redpanda and Kafka's default settings.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/CHANGELOG.md#_snippet_12\n\n\n\n----------------------------------------\n\nTITLE: Formatting Redpanda Connect code\nDESCRIPTION: This command formats the Redpanda Connect codebase. It uses `go fmt` and other formatting tools to ensure the code adheres to the project's style guidelines. This improves readability and maintainability.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/README.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nmake fmt\n```\n\n----------------------------------------\n\nTITLE: Running Benthos Docker with Config File\nDESCRIPTION: This command runs the Benthos Docker image, mounting a local configuration file to `/connect.yaml` within the container. This allows Benthos to use the settings defined in the configuration file. The `--rm` flag ensures the container is removed after it exits.\nSOURCE: https://github.com/redpanda-data/connect/blob/main/resources/docker/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndocker run --rm -v /path/to/your/config.yaml:/connect.yaml ghcr.io/redpanda-data/connect\n```"
  }
]
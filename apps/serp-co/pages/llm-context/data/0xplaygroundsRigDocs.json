[
  {
    "owner": "0xplaygrounds",
    "repo": "rig-docs",
    "content": "TITLE: Creating In-Memory Vector Store in Rust\nDESCRIPTION: Initializes an in-memory vector store using Rig's InMemoryVectorStore. This store will hold the document embeddings for quick retrieval in the RAG system.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/rag/rag_system.mdx#2025-04-15_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::vector_store::in_memory_store::InMemoryVectorStore;\nuse rig::vector_store::VectorStore;\n\nlet mut vector_store = InMemoryVectorStore::default();\n```\n\n----------------------------------------\n\nTITLE: Complete RAG System Implementation with Rig in Rust\nDESCRIPTION: This comprehensive example demonstrates a complete RAG system implementation using Rig. It includes initializing an OpenAI client, creating an embedding model, setting up an in-memory vector store, adding documents with embeddings, and using a RAG agent to answer questions with context.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/quickstart.mdx#2025-04-15_snippet_6\n\nLANGUAGE: rust\nCODE:\n```\nuse rig:{\n    completion::Prompt,\n    embeddings::EmbeddingsBuilder,\n    providers::openai::Client,\n    vector_store::{in_memory_store::InMemoryVectorStore, VectorStore},\n};\n\n#[tokio::main]\nasync fn main() -> Result<(), anyhow::Error> {\n    // Initialize OpenAI client and embedding model\n    let openai_client = Client::from_env();\n    let embedding_model = openai_client.embedding_model(\"text-embedding-ada-002\");\n\n    // Create and populate vector store\n    let mut vector_store = InMemoryVectorStore::default();\n    let embeddings = EmbeddingsBuilder::new(embedding_model.clone())\n        .simple_document(\"doc1\", \"Rig is a Rust library for building LLM applications.\")\n        .simple_document(\"doc2\", \"Rig supports OpenAI and Cohere as LLM providers.\")\n        .build()\n        .await?;\n    vector_store.add_documents(embeddings).await?;\n\n    // Create and use RAG agent\n    let rag_agent = openai_client.context_rag_agent(\"gpt-4\")\n        .preamble(\"You are an assistant that answers questions about Rig.\")\n        .dynamic_context(1, vector_store.index(embedding_model))\n        .build();\n\n    let response = rag_agent.prompt(\"What is Rig?\").await?;\n    println!(\"RAG Agent: {}\", response);\n\n    Ok(())\n}\n```\n\n----------------------------------------\n\nTITLE: Integrating MongoDB Vector Search in Rust Application\nDESCRIPTION: This example demonstrates a complete integration of MongoDB vector search in a Rust application. It includes initializing OpenAI and MongoDB clients, setting up the vector store, and embedding and searching documents.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/mongodb.mdx#2025-04-15_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\n#[tokio::main]\nasync fn main() -> Result<(), anyhow::Error> {\n    // Initialize OpenAI client\n    let openai_api_key = env::var(\"OPENAI_API_KEY\").expect(\"OPENAI_API_KEY not set\");\n    let openai_client = Client::new(&openai_api_key);\n\n    // Initialize MongoDB client\n    let mongodb_connection_string =\n        env::var(\"MONGODB_CONNECTION_STRING\").expect(\"MONGODB_CONNECTION_STRING not set\");\n    let options = ClientOptions::parse(mongodb_connection_string)\n        .await\n        .expect(\"MongoDB connection string should be valid\");\n\n    let mongodb_client =\n        MongoClient::with_options(options).expect(\"MongoDB client options should be valid\");\n\n    // Initialize MongoDB vector store\n    let collection: Collection<bson::Document> = mongodb_client\n        .database(\"knowledgebase\")\n        .collection(\"context\");\n\n    // Select the embedding model and generate our embeddings\n    let model = openai_client.embedding_model(TEXT_EMBEDDING_ADA_002);\n\n    let words = vec![\n        Word {\n            id: \"doc0\".to_string(),\n            definition: \"Definition of a *flurbo*: A flurbo is a green alien that lives on cold planets\".to_string(),\n        },\n        Word {\n            id: \"doc1\".to_string(),\n            definition: \"Definition of a *glarb-glarb*: A glarb-glarb is a ancient tool used by the ancestors of the inhabitants of planet Jiro to farm the land.\".to_string(),\n        },\n        Word {\n            id: \"doc2\".to_string(),\n            definition: \"Definition of a *linglingdong*: A term used by inhabitants of the far side of the moon to describe humans.\".to_string(),\n        }\n    ];\n```\n\n----------------------------------------\n\nTITLE: Type-Safe Data Extraction with Rig in Rust\nDESCRIPTION: This example shows how Rig leverages Rust's type system to provide type-safe data extraction from unstructured text. It defines a Person struct and uses an extractor to convert text into a strongly-typed object.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/quickstart.mdx#2025-04-15_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\n#[derive(serde::Deserialize, JsonSchema)]\nstruct Person {\n    name: String,\n    age: u8,\n}\n\nlet extractor = openai_client.extractor::<Person>(\"gpt-4\").build();\nlet person: Person = extractor.extract(\"John Doe is 30 years old\").await?;\n```\n\n----------------------------------------\n\nTITLE: Creating a RAG-Enabled Agent in Rust\nDESCRIPTION: Shows how to create an agent with Retrieval-Augmented Generation (RAG) capabilities using an in-memory vector store.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/agent.mdx#2025-04-15_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::{Agent, vector_store::InMemoryVectorStore};\n\n// Create vector store and index\nlet store = InMemoryVectorStore::new();\nlet index = store.index(embedding_model);\n\n// Create RAG agent\nlet agent = openai.agent(\"gpt-4\")\n    .preamble(\"You are a knowledge assistant.\")\n    .dynamic_context(3, index)  // Retrieve 3 relevant documents\n    .build();\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG System with Rig in Rust\nDESCRIPTION: This code snippet demonstrates the complete implementation of a RAG system using Rig. It includes PDF content extraction, vector store creation, embedding generation, and interaction with OpenAI's GPT model. The system uses an in-memory vector store and provides a CLI interface for user interaction.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/rag/rag_system.mdx#2025-04-15_snippet_6\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::providers::openai;\nuse rig::vector_store::in_memory_store::InMemoryVectorStore;\nuse rig::vector_store::VectorStore;\nuse rig::embeddings::EmbeddingsBuilder;\nuse rig::cli_chatbot::cli_chatbot;  // Import the cli_chatbot function\nuse std::path::Path;\nuse anyhow::{Result, Context};\nuse pdf_extract::extract_text;\n\nfn load_pdf_content<P: AsRef<Path>>(file_path: P) -> Result<String> {\n    extract_text(file_path.as_ref())\n        .with_context(|| format!(\"Failed to extract text from PDF: {:?}\", file_path.as_ref()))\n}\n\n#[tokio::main]\nasync fn main() -> Result<()> {\n    // Initialize OpenAI client\n    let openai_client = openai::Client::from_env();\n    let embedding_model = openai_client.embedding_model(\"text-embedding-ada-002\");\n\n    // Create vector store\n    let mut vector_store = InMemoryVectorStore::default();\n\n    // Get the current directory and construct paths to PDF files\n    let current_dir = std::env::current_dir()?;\n    let documents_dir = current_dir.join(\"documents\");\n\n    let pdf1_path = documents_dir.join(\"Moores_Law_for_Everything.pdf\");\n    let pdf2_path = documents_dir.join(\"The_Last_Question.pdf\");\n\n    // Load PDF documents\n    let pdf1_content = load_pdf_content(&pdf1_path)?;\n    let pdf2_content = load_pdf_content(&pdf2_path)?;\n\n    // Create embeddings and add to vector store\n    let embeddings = EmbeddingsBuilder::new(embedding_model.clone())\n        .simple_document(\"Moores_Law_for_Everything\", &pdf1_content)\n        .simple_document(\"The_Last_Question\", &pdf2_content)\n        .build()\n        .await?;\n\n    vector_store.add_documents(embeddings).await?;\n\n    // Create RAG agent\n    let rag_agent = openai_client.context_rag_agent(\"gpt-3.5-turbo\")\n        .preamble(\"You are a helpful assistant that answers questions based on the given context from PDF documents.\")\n        .dynamic_context(2, vector_store.index(embedding_model))\n        .build();\n\n    // Use the cli_chatbot function to create the CLI interface\n    cli_chatbot(rag_agent).await?;\n\n    Ok(())\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Tool in Rust for Rig Framework\nDESCRIPTION: This code snippet demonstrates how to implement a basic tool in Rig that adds two numbers. It shows the implementation of the Tool trait, including defining input arguments, output types, error handling, tool definition, and execution logic.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/tools.mdx#2025-04-15_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\n#[derive(Deserialize)]\nstruct AddArgs {\n    x: i32,\n    y: i32,\n}\n\n#[derive(Deserialize, Serialize)]\nstruct Adder;\n\nimpl Tool for Adder {\n    const NAME: &'static str = \"add\";\n    type Error = MathError;\n    type Args = AddArgs;\n    type Output = i32;\n\n    async fn definition(&self, _prompt: String) -> ToolDefinition {\n        ToolDefinition {\n            name: \"add\".to_string(),\n            description: \"Add x and y together\".to_string(),\n            parameters: json!({\n                \"type\": \"object\",\n                \"properties\": {\n                    \"x\": { \"type\": \"number\", \"description\": \"First number\" },\n                    \"y\": { \"type\": \"number\", \"description\": \"Second number\" }\n                }\n            })\n        }\n    }\n\n    async fn call(&self, args: Self::Args) -> Result<Self::Output, Self::Error> {\n        Ok(args.x + args.y)\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Dynamic Context Resolution in Rust\nDESCRIPTION: Shows the implementation of dynamic context resolution in the agent, including querying vector stores and integrating retrieved information.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/agent.mdx#2025-04-15_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\nlet dynamic_context = stream::iter(self.dynamic_context.iter())\n    .then(|(num_sample, index)| async {\n        Ok::<_, VectorStoreError>(\n            index\n                .top_n(prompt, *num_sample)\n                .await?\n                .into_iter()\n                .map(|(_, id, doc)| {\n                    // Pretty print the document if possible for better readability\n                    let text = serde_json::to_string_pretty(&doc)\n                        .unwrap_or_else(|_| doc.to_string());\n\n                    Document {\n                        id,\n                        text,\n                        additional_props: HashMap::new(),\n                    }\n                })\n                .collect::<Vec<_>>()\n        )\n    })\n    .try_fold(vec![], |mut acc, docs| async {\n        acc.extend(docs);\n        Ok(acc)\n    })\n    .await\n    .map_err(|e| CompletionError::RequestError(Box::new(e)))?\n```\n\n----------------------------------------\n\nTITLE: Creating a RAG Knowledge Base Agent in Rust\nDESCRIPTION: Shows how to set up an agent with RAG capabilities for use as a knowledge base assistant.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/agent.mdx#2025-04-15_snippet_7\n\nLANGUAGE: rust\nCODE:\n```\nlet kb_agent = openai.agent(\"gpt-4\")\n    .preamble(\"You are a knowledge base assistant.\")\n    .dynamic_context(5, document_store)\n    .temperature(0.3)\n    .build();\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG-Enabled Tool in Rust for Rig Framework\nDESCRIPTION: This code snippet shows how to implement a RAG-enabled tool in Rig. It demonstrates the implementation of both the Tool and ToolEmbedding traits, allowing the tool to be stored in vector stores, retrieved based on semantic similarity, and dynamically added to agent prompts.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/tools.mdx#2025-04-15_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nstruct Add;\n\nimpl Tool for Add {\n    const NAME: &'static str = \"add\";\n\n    type Error = MathError;\n    type Args = OperationArgs;\n    type Output = i32;\n\n    async fn definition(&self, _prompt: String) -> ToolDefinition {\n        serde_json::from_value(json!({\n            \"name\": \"add\",\n            \"description\": \"Add x and y together\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"x\": {\n                        \"type\": \"number\",\n                        \"description\": \"The first number to add\"\n                    },\n                    \"y\": {\n                        \"type\": \"number\",\n                        \"description\": \"The second number to add\"\n                    }\n                }\n            }\n        }))\n        .expect(\"Tool Definition\")\n    }\n\n    async fn call(&self, args: Self::Args) -> Result<Self::Output, Self::Error> {\n        let result = args.x + args.y;\n        Ok(result)\n    }\n}\n\nimpl ToolEmbedding for Add {\n    type InitError = InitError;\n    type Context = ();\n    type State = ();\n\n    fn init(_state: Self::State, _context: Self::Context) -> Result<Self, Self::InitError> {\n        Ok(Add)\n    }\n\n    fn embedding_docs(&self) -> Vec<String> {\n        vec![\"Add x and y together\".into()]\n    }\n\n    fn context(&self) -> Self::Context {}\n```\n\n----------------------------------------\n\nTITLE: Implementing News Article Analysis in Rust\nDESCRIPTION: Creates a comprehensive news article analyzer using GPT-4 for topic classification, sentiment analysis, entity recognition, and key points extraction.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/text_extraction_classification.mdx#2025-04-15_snippet_12\n\nLANGUAGE: rust\nCODE:\n```\n#[tokio::main]\nasync fn main() -> Result<()> {\n    dotenv::dotenv().ok();\n    let openai_client = openai::Client::from_env()?;\n    \n    let news_analyzer = openai_client\n        .extractor::<NewsArticleAnalysis>(\"gpt-4\")\n        .preamble(\"\n            You are a news article analysis AI. For the given news article:\n            1. Classify the main topic (Politics, Technology, Sports, Entertainment, or Other).\n            2. Analyze the overall sentiment (Positive, Negative, or Neutral) with a confidence score.\n            3. Identify and extract named entities (Person, Organization, Location) with their start and end indices.\n            4. Extract 3-5 key points from the article.\n        \")\n        .build();\n\n    let article = \"/* Article text here */\";\n    \n    let result = news_analyzer.extract(article).await?;\n\n    println!(\"Article Analysis:\");\n    println!(\"Topic: {:?}\", result.topic);\n    println!(\"Sentiment: {:?} (Confidence: {:.2})\", result.sentiment.sentiment, result.sentiment.confidence);\n    println!(\"\\nEntities:\");\n    for entity in &result.entities {\n        println!(\n            \"- {:?}: {} ({}:{})\",\n            entity.entity_type, entity.text, entity.start, entity.end\n        );\n    }\n    println!(\"\\nKey Points:\");\n    for (i, point) in result.key_points.iter().enumerate() {\n        println!(\"{}.{}\", i + 1, point);\n    }\n\n    Ok(())\n}\n```\n\n----------------------------------------\n\nTITLE: Extending Rig with Custom Vector Store Implementation\nDESCRIPTION: This snippet demonstrates Rig's extensibility by showing how to implement a custom vector store and integrate it with a RAG agent. It highlights how Rig can be customized to fit specific project requirements.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/quickstart.mdx#2025-04-15_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\nimpl VectorStore for MyCustomStore {\n    // Implementation details...\n}\n\nlet my_store = MyCustomStore::new();\nlet rag_agent = openai_client.context_rag_agent(\"gpt-4\")\n    .dynamic_context(2, my_store.index(embedding_model))\n    .build();\n```\n\n----------------------------------------\n\nTITLE: Implementing Embeddings and Vector Store with Rig in Rust\nDESCRIPTION: Demonstrates how to use Rig to create embeddings from text documents, store them in an in-memory vector store, and perform semantic search to find the most relevant document for a given query.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/why_rig.mdx#2025-04-15_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::providers::openai;\nuse rig::embeddings::EmbeddingsBuilder;\nuse rig::vector_store::{in_memory_store::InMemoryVectorStore, VectorStore};\n\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn std::error::Error>> {\n    // Initialize OpenAI client and create an embedding model\n    let openai_client = openai::Client::from_env();\n    let embedding_model = openai_client.embedding_model(\"text-embedding-ada-002\");\n\n    // Create an in-memory vector store\n    let mut vector_store = InMemoryVectorStore::default();\n\n    // Generate embeddings for two documents\n    let embeddings = EmbeddingsBuilder::new(embedding_model.clone())\n        .simple_document(\"doc1\", \"Rust is a systems programming language.\")\n        .simple_document(\"doc2\", \"Python is known for its simplicity.\")\n        .build()\n        .await?;\n\n    // Add the embeddings to the vector store\n    vector_store.add_documents(embeddings).await?;\n\n    // Create an index from the vector store\n    let index = vector_store.index(embedding_model);\n    // Query the index for the most relevant document to \"What is Rust?\"\n    let results = index.top_n_from_query(\"What is Rust?\", 1).await?;\n\n    // Print the most relevant document\n    println!(\"Most relevant document: {:?}\", results[0].1.document);\n\n    Ok()\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a RAG (Retrieval-Augmented Generation) Pipeline in Rust\nDESCRIPTION: Demonstrates building a complete RAG pipeline that performs parallel document lookup and query embedding, formats context, and generates a response using an LLM model.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/chains.mdx#2025-04-15_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::pipeline::{self, Op};\n\nlet pipeline = pipeline::new()\n    // Parallel: Query embedding & document lookup\n    .chain(parallel!(\n        passthrough(),\n        lookup::<_, _, Document>(vector_store, 3)\n    ))\n    // Format context\n    .map(|(query, docs)| format!(\n        \"Query: {}\\nContext: {}\",\n        query,\n        docs.join(\"\\n\")\n    ))\n    // Generate response\n    .prompt(llm_model);\n```\n\n----------------------------------------\n\nTITLE: Building a RAG System with Rig in Rust\nDESCRIPTION: Demonstrates how to create a Retrieval-Augmented Generation (RAG) system using Rig, combining document embeddings, vector storage, and LLM-based generation to answer questions based on stored knowledge.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/why_rig.mdx#2025-04-15_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::providers::openai;\nuse rig::embeddings::EmbeddingsBuilder;\nuse rig::vector_store::{in_memory_store::InMemoryVectorStore, VectorStore};\nuse rig::completion::Prompt;\n\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn std::error::Error>> {\n    // Initialize OpenAI client and create an embedding model\n    let openai_client = openai::Client::from_env();\n    let embedding_model = openai_client.embedding_model(\"text-embedding-ada-002\");\n\n    // Create an in-memory vector store\n    let mut vector_store = InMemoryVectorStore::default();\n\n    // Generate embeddings for two documents about Rust\n    let embeddings = EmbeddingsBuilder::new(embedding_model.clone())\n        .simple_document(\"doc1\", \"Rust was initially designed by Graydon Hoare at Mozilla Research.\")\n        .simple_document(\"doc2\", \"Rust's first stable release (1.0) was on May 15, 2015.\")\n        .build()\n        .await?;\n\n    // Add the embeddings to the vector store\n    vector_store.add_documents(embeddings).await?;\n\n    // Create an index from the vector store\n    let index = vector_store.index(embedding_model);\n\n    // Create a RAG agent using GPT-4 with the vector store as context\n    let rag_agent = openai_client.context_rag_agent(\"gpt-4\")\n        .preamble(\"You are an assistant that answers questions about Rust programming language.\")\n        .dynamic_context(1, index)\n        .build();\n\n    // Use the RAG agent to answer a question about Rust\n    let response = rag_agent.prompt(\"When was Rust's first stable release?\").await?;\n    println!(\"RAG Agent Response: {}\", response);\n\n    Ok()\n}\n```\n\n----------------------------------------\n\nTITLE: Building RAG Agent with OpenAI in Rust\nDESCRIPTION: Creates a RAG agent using OpenAI's GPT-3.5-turbo model. This agent will use the vector store to retrieve relevant context and answer questions based on the PDF documents.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/rag/rag_system.mdx#2025-04-15_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\nlet rag_agent = openai_client.context_rag_agent(\"gpt-3.5-turbo\")\n    .preamble(\"You are a helpful assistant that answers questions based on the given context from PDF documents.\")\n    .dynamic_context(2, vector_store.index(embedding_model))\n    .build();\n```\n\n----------------------------------------\n\nTITLE: Concurrent LLM Request Handling with Rig in Rust\nDESCRIPTION: This snippet demonstrates how Rig efficiently handles multiple LLM requests concurrently using Rust's async capabilities and Tokio runtime. It showcases resource sharing, memory safety, and efficient processing of batch operations.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/why_rig.mdx#2025-04-15_snippet_6\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::providers::openai;\nuse rig::completion::Prompt;\nuse tokio::task;\nuse std::time::Instant;\nuse std::sync::Arc;\n\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn std::error::Error>> {\n    let openai_client = openai::Client::from_env();\n    let model = Arc::new(openai_client.model(\"gpt-3.5-turbo\").build());\n\n    let start = Instant::now();\n    let mut handles = vec![];\n\n    // Spawn 10 concurrent tasks\n    for i in 0..10 {\n        let model_clone = Arc::clone(&model);\n        let handle = task::spawn(async move {\n            let prompt = format!(\"Generate a random fact about the number {}\", i);\n            model_clone.prompt(&prompt).await\n        });\n        handles.push(handle);\n    }\n\n    // Collect results\n    for handle in handles {\n        let result = handle.await??;\n        println!(\"Result: {}\", result);\n    }\n\n    println!(\"Time elapsed: {:?}\", start.elapsed());\n    Ok()\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Embedding Model and Document Processing in Rust\nDESCRIPTION: Sets up the embedding model using OpenAI's text-embedding-ada-002, loads PDF documents, creates embeddings, and adds them to the vector store. This is a crucial step in preparing the data for the RAG system.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/rag/rag_system.mdx#2025-04-15_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::embeddings::EmbeddingsBuilder;\nuse std::env;\n\n// Create an embedding model using OpenAI's text-embedding-ada-002\nlet embedding_model = openai_client.embedding_model(\"text-embedding-ada-002\");\n\n// Get the current directory and construct paths to PDF files\nlet current_dir = env::current_dir()?;\nlet documents_dir = current_dir.join(\"documents\");\n\nlet pdf1_path = documents_dir.join(\"Moores_Law_for_Everything.pdf\");\nlet pdf2_path = documents_dir.join(\"The_Last_Question.pdf\");\n\n// Load PDF documents\nlet pdf1_content = load_pdf_content(&pdf1_path)?;\nlet pdf2_content = load_pdf_content(&pdf2_path)?;\n\n// Create embeddings for the PDF contents\nlet embeddings = EmbeddingsBuilder::new(embedding_model.clone())\n    .simple_document(\"Moores_Law_for_Everything\", &pdf1_content)\n    .simple_document(\"The_Last_Question\", &pdf2_content)\n    .build()\n    .await?;\n\n// Add the embeddings to our vector store\nvector_store.add_documents(embeddings).await?;\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG (Retrieval-Augmented Generation) with Rig\nDESCRIPTION: This snippet demonstrates how to create a RAG agent that uses a vector store to dynamically retrieve context for augmenting LLM responses. It shows Rig's ability to condense complex AI workflows into a few lines of code.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/quickstart.mdx#2025-04-15_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\nlet rag_agent = openai_client.context_rag_agent(\"gpt-4\")\n    .preamble(\"You are a helpful assistant.\")\n    .dynamic_context(2, vector_store.index(embedding_model))\n    .build();\n\nlet response = rag_agent.prompt(\"What is the capital of France?\").await?;\n```\n\n----------------------------------------\n\nTITLE: Complete Qdrant Vector Search Implementation in Rust\nDESCRIPTION: A full example showing how to initialize OpenAI embedding model, connect to Qdrant, create a collection, set up a vector store, and perform a search query. This demonstrates the complete workflow from setup to execution.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/qdrant.mdx#2025-04-15_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\nconst COLLECTION_NAME: &str = \"MY_COLLECTION\";\nconst COLLECTION_SIZE: usize = 1536; // vector embedding size for the collection goes here\n\n#[tokio::main]\nasync fn main() -> Result<(), anyhow::Error> {\n    let openai_api_key = env::var(\"OPENAI_API_KEY\").expect(\"OPENAI_API_KEY not set\");\n    let openai_client = Client::new(&openai_api_key);\n    let model = openai_client.embedding_model(TEXT_EMBEDDING_ADA_002);\n\n    let qdrant_client = Qdrant::connect(\"http://localhost:6334\").await?;\n\n    // Create a collection with 1536 dimensions if it doesn't exist\n    // Note: Make sure the dimensions match the size of the embeddings returned by the\n    // model you are using\n    if !qdrant_client.collection_exists(COLLECTION_NAME).await? {\n        qdrant_client.create_collection(\n            CreateCollectionBuilder::new(COLLECTION_NAME)\n                .vectors_config(VectorParamsBuilder::new(\n                    COLLECTION_SIZE as u64,\n                    qdrant_client::qdrant::Distance::Cosine)\n                ),\n            ).await?;\n    }\n\n    let query_params = QueryPointsBuilder::new(COLLECTION_NAME).with_payload(true);\n    let vector_store = QdrantVectorStore::new(qdrant_inner, model.clone(), query_params.build());\n\n\n    let results = vector_store.top_n::<Utterance>(query, 1).await?;\n    println!(\"{:#?}\", results);\n\n    Ok()\n}\n```\n\n----------------------------------------\n\nTITLE: Flexible Agent Configuration in Rust\nDESCRIPTION: Shows the extensive configuration options available through the AgentBuilder pattern, including basic settings, context management, and tool integration.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/agent.mdx#2025-04-15_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\nlet agent = AgentBuilder::new(model)\n    // Basic configuration\n    .preamble(\"System instructions\")\n    .temperature(0.8)\n    .max_tokens(1000)\n    \n    // Context management\n    .context(\"Static context\")\n    .dynamic_context(5, vector_store)\n    \n    // Tool integration\n    .tool(tool1)\n    .dynamic_tools(3, tool_store, toolset)\n    \n    // Additional parameters\n    .additional_params(json!({\n        \"top_p\": 0.9,\n        \"frequency_penalty\": 0.7\n    }))\n    .build();\n```\n\n----------------------------------------\n\nTITLE: Implementing CLI Interface for RAG System in Rust\nDESCRIPTION: Utilizes Rig's built-in cli_chatbot function to create a command-line interface for interacting with the RAG agent. This simplifies the process of querying the system and handling user interactions.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/rag/rag_system.mdx#2025-04-15_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::cli_chatbot::cli_chatbot;\n\n// Create RAG agent\nlet rag_agent = openai_client.context_rag_agent(\"gpt-3.5-turbo\")\n    .preamble(\"You are a helpful assistant that answers questions based on the given context from PDF documents.\")\n    .dynamic_context(2, vector_store.index(embedding_model))\n    .build();\n\n// Use the cli_chatbot function to create the CLI interface\ncli_chatbot(rag_agent).await?;\n```\n\n----------------------------------------\n\nTITLE: Complete Neo4j Vector Search Example in Rust\nDESCRIPTION: A complete example demonstrating the full workflow of connecting to Neo4j, creating a vector index, and performing a semantic search query. This includes error handling and printing the results.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/neo4j.mdx#2025-04-15_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\n#[tokio::main]\nasync fn main() -> Result<(), anyhow::Error> {\n    let neo4j_client = Neo4jClient::connect(\"neo4j://localhost:7687\", \"username\", \"password\").await?;\n    let model = openai_client.embedding_model(TEXT_EMBEDDING_ADA_002);\n\n    neo4j_client.create_vector_index(\n        IndexConfig::new(\"moviePlots\"),\n        \"Movie\",\n        &model\n    ).await?;\n\n    let index = neo4j_client.get_index(model, \"moviePlots\", SearchParams::default()).await?;\n    let results = index.top_n::<Movie>(\"a historical movie on quebec\", 5).await?;\n    println!(\"{:#?}\", results);\n\n    Ok(())\n}\n```\n\n----------------------------------------\n\nTITLE: Basic OpenAI Integration Example in Rust\nDESCRIPTION: Demonstrates how to create an OpenAI client, initialize a GPT-4 agent, and send a basic prompt. Requires OPENAI_API_KEY environment variable to be set.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/index.mdx#2025-04-15_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::{completion::Prompt, providers::openai};\n\n#[tokio::main]\nasync fn main() {\n    // Create OpenAI client and agent.\n    // This requires the `OPENAI_API_KEY` environment variable to be set.\n    let openai_client = openai::Client::from_env();\n\n    let gpt4 = openai_client.agent(\"gpt-4\").build();\n\n    // Prompt the model and print its response\n    let response = gpt4\n        .prompt(\"Who are you?\")\n        .await\n        .expect(\"Failed to prompt GPT-4\");\n\n    println!(\"GPT-4: {response}\");\n}\n```\n\n----------------------------------------\n\nTITLE: Defining In-Memory Vector Store Structure in Rust\nDESCRIPTION: Core data structure of the InMemoryVectorStore that uses a HashMap to store documents with their embeddings. The structure supports both single and multiple embeddings per document through the OneOrMany enum.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/in_memory.mdx#2025-04-15_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\npub struct InMemoryVectorStore<D: Serialize> {\n    embeddings: HashMap<String, (D, OneOrMany<Embedding>)>,\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Tool-Augmented Agent in Rust\nDESCRIPTION: Illustrates the creation of an agent with both static and dynamic tools, enhancing its capabilities.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/agent.mdx#2025-04-15_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::{Agent, Tool};\n\n// Create agent with tools\nlet agent = openai.agent(\"gpt-4\")\n    .preamble(\"You are a capable assistant with tools.\")\n    .tool(calculator)\n    .tool(web_search)\n    .dynamic_tools(2, tool_index, toolset)\n    .build();\n```\n\n----------------------------------------\n\nTITLE: Defining the Completion and CompletionModel Traits in Rust\nDESCRIPTION: Provides low-level control for LLM interactions including fine-grained request configuration, access to raw completion responses, and tool call handling. The CompletionModel trait is meant to be implemented to define custom completion models from providers.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/completion.mdx#2025-04-15_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\n...        chat_history: Vec<Message>,\n    ) -> impl std::future::Future<Output = Result<String, PromptError>> + Send;\n}\n\n/// Trait defininig a low-level LLM completion interface\npub trait Completion<M: CompletionModel> {\n    /// Generates a completion request builder for the given `prompt` and `chat_history`.\n    /// This function is meant to be called by the user to further customize the\n    /// request at prompt time before sending it.\n    ///\n    /// ‚ùóIMPORTANT: The type that implements this trait might have already\n    /// populated fields in the builder (the exact fields depend on the type).\n    /// For fields that have already been set by the model, calling the corresponding\n    /// method on the builder will overwrite the value set by the model.\n    ///\n    /// For example, the request builder returned by [`Agent::completion`](crate::agent::Agent::completion) will already\n    /// contain the `preamble` provided when creating the agent.\n    fn completion(\n        &self,\n        prompt: &str,\n        chat_history: Vec<Message>,\n    ) -> impl std::future::Future<Output = Result<CompletionRequestBuilder<M>, CompletionError>> + Send;\n}\n\n/// General completion response struct that contains the high-level completion choice\n/// and the raw response.\n#[derive(Debug)]\npub struct CompletionResponse<T> {\n    /// The completion choice returned by the completion model provider\n    pub choice: ModelChoice,\n    /// The raw response returned by the completion model provider\n    pub raw_response: T,\n}\n\n/// Enum representing the high-level completion choice returned by the completion model provider.\n#[derive(Debug)]\npub enum ModelChoice {\n    /// Represents a completion response as a message\n    Message(String),\n    /// Represents a completion response as a tool call of the form\n    /// `ToolCall(function_name, function_params)`.\n    ToolCall(String, serde_json::Value),\n}\n\n/// Trait defining a completion model that can be used to generate completion responses.\n/// This trait is meant to be implemented by the user to define a custom completion model,\n/// either from a third party provider (e.g.: OpenAI) or a local model.\npub trait CompletionModel: Clone + Send + Sync {\n    /// The raw response type returned by the underlying completion model.\n    type Response: Send + Sync;\n\n    /// Generates a completion response for the given completion request.\n    fn completion(\n        &self,\n        request: CompletionRequest,\n    ) -> impl std::future::Future<Output = Result<CompletionResponse<Self::Response>, CompletionError>>\n           + Send;\n\n    /// Generates a completion request builder for the given `prompt`.\n    fn completion_request(&self, prompt: &str) -> CompletionRequestBuilder<Self> {\n        CompletionRequestBuilder::new(self.clone(), prompt.to_string())\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Rig's Unified API Across Different LLM Providers\nDESCRIPTION: This code example demonstrates Rig's consistent interface across different LLM providers (OpenAI and Cohere). It shows how the same prompt pattern can be used regardless of the underlying provider.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/quickstart.mdx#2025-04-15_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\n// Using OpenAI\nlet gpt4 = openai_client.model(\"gpt-4\").build();\nlet response = gpt4.prompt(\"Hello, GPT-4!\").await?;\n\n// Using Cohere\nlet command = cohere_client.model(\"command\").build();\nlet response = command.prompt(\"Hello, Cohere!\").await?;\n```\n\n----------------------------------------\n\nTITLE: Creating a Qdrant Collection for Vector Storage in Rust\nDESCRIPTION: Sets up a vector collection in Qdrant if it doesn't already exist and initializes a QdrantVectorStore with the specified parameters. The collection uses cosine distance for similarity measurements.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/qdrant.mdx#2025-04-15_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nif !qdrant_client.collection_exists(COLLECTION_NAME).await? {\n    qdrant_client.create_collection(\n        CreateCollectionBuilder::new(COLLECTION_NAME)\n            .vectors_config(VectorParamsBuilder::new(\n                COLLECTION_SIZE as u64,\n                qdrant_client::qdrant::Distance::Cosine)\n            ),\n        ).await?;\n}\n\nlet query_params = QueryPointsBuilder::new(COLLECTION_NAME).with_payload(true);\nlet vector_store = QdrantVectorStore::new(qdrant_inner, model.clone(), query_params.build());\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client and Sending a Basic Prompt with Rig in Rust\nDESCRIPTION: This snippet demonstrates how to initialize the OpenAI client using environment variables, create a GPT-4 model instance, and send a simple prompt to receive a response. It shows Rig's abstraction of OpenAI API complexity.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/quickstart.mdx#2025-04-15_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::{completion::Prompt, providers::openai};\n\n#[tokio::main]\nasync fn main() -> Result<(), anyhow::Error> {\n    // Initialize the OpenAI client using environment variables\n    let openai_client = openai::Client::from_env();\n    \n    // Create a GPT-4 model instance\n    let gpt4 = openai_client.model(\"gpt-4\").build();\n    \n    // Send a prompt to GPT-4 and await the response\n    let response = gpt4.prompt(\"Explain quantum computing in one sentence.\").await?;\n    \n    // Print the response\n    println!(\"GPT-4: {}\", response);\n    \n    Ok(())\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Extractor Usage Example in Rust\nDESCRIPTION: Demonstrates how to define a target structure and create an extractor instance to parse text data into strongly-typed structures using OpenAI's GPT-4 model.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/extractors.mdx#2025-04-15_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::providers::openai;\n\n// Define target structure\n#[derive(serde::Deserialize, serde::Serialize, schemars::JsonSchema)]\nstruct Person {\n    name: Option<String>,\n    age: Option<u8>,\n    profession: Option<String>,\n}\n\n// Create and use extractor\nlet openai = openai::Client::new(api_key);\nlet extractor = openai.extractor::<Person>(openai::GPT_4O).build();\n\nlet person = extractor.extract(\"John Doe is a 30 year old doctor.\").await?;\n```\n\n----------------------------------------\n\nTITLE: Using CompletionRequestBuilder in Rust\nDESCRIPTION: Demonstrates the fluent API for constructing LLM completion requests with various parameters like preamble, temperature, max tokens, documents, and tools.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/completion.mdx#2025-04-15_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\nlet request = model.completion_request(\"prompt\")\n    .preamble(\"system instructions\")\n    .temperature(0.7)\n    .max_tokens(1000)\n    .documents(context_docs)\n    .tools(available_tools)\n    .build();\n```\n\n----------------------------------------\n\nTITLE: Creating and Building Embeddings with OpenAI in Rust\nDESCRIPTION: Demonstrates how to create an embedding model using OpenAI and build embeddings from text documents. The example shows how to create a batch of embeddings using the EmbeddingsBuilder.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/embeddings.mdx#2025-04-15_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::{embeddings::EmbeddingsBuilder, providers::openai};\n\n// Create embedding model\nlet model = openai_client.embedding_model(\"text-embedding-ada-002\");\n\n// Build embeddings\nlet embeddings = EmbeddingsBuilder::new(model)\n    .document(\"Some text\")? \n    .document(\"More text\")?\n    .build()\n    .await?;\n```\n\n----------------------------------------\n\nTITLE: Implementing a Discord Bot with Rig Agent Integration in Rust\nDESCRIPTION: A complete Discord bot implementation using the Serenity framework that integrates with a Rig AI agent. The bot responds to slash commands like '/hello' and '/ask', and also processes messages where it's mentioned. It includes error handling, logging, and proper Discord interaction management.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/discord_bot.mdx#2025-04-15_snippet_20\n\nLANGUAGE: rust\nCODE:\n```\n// main.rs\n\nmod rig_agent;\n\nuse anyhow::Result;\nuse serenity::async_trait;\nuse serenity::model::application::command::Command;\nuse serenity::model::application::interaction::{Interaction, InteractionResponseType};\nuse serenity::model::gateway::Ready;\nuse serenity::model::channel::Message;\nuse serenity::prelude::*;\nuse serenity::model::application::command::CommandOptionType;\nuse std::env;\nuse std::sync::Arc;\nuse tracing::{error, info, debug};\nuse rig_agent::RigAgent;\nuse dotenv::dotenv;\n\n// Define a key for storing the bot's user ID in the TypeMap\nstruct BotUserId;\n\nimpl TypeMapKey for BotUserId {\n    type Value = serenity::model::id::UserId;\n}\n\nstruct Handler {\n    rig_agent: Arc<RigAgent>,\n}\n\n#[async_trait]\nimpl EventHandler for Handler {\n    async fn interaction_create(&self, ctx: Context, interaction: Interaction) {\n        debug!(\"Received an interaction\");\n        if let Interaction::ApplicationCommand(command) = interaction {\n            debug!(\"Received command: {}\", command.data.name);\n            let content = match command.data.name.as_str() {\n                \"hello\" => \"Hello! I'm your helpful Rust and Rig-powered assistant. How can I assist you today?\".to_string(),\n                \"ask\" => {\n                    let query = command\n                        .data\n                        .options\n                        .get(0)\n                        .and_then(|opt| opt.value.as_ref())\n                        .and_then(|v| v.as_str())\n                        .unwrap_or(\"What would you like to ask?\");\n                    debug!(\"Query: {}\", query);\n                    match self.rig_agent.process_message(query).await {\n                        Ok(response) => response,\n                        Err(e) => {\n                            error!(\"Error processing request: {:?}\", e);\n                            format!(\"Error processing request: {:?}\", e)\n                        }\n                    }\n                }\n                _ => \"Not implemented :(\".to_string(),\n            };\n\n            debug!(\"Sending response: {}\", content);\n\n            if let Err(why) = command\n                .create_interaction_response(&ctx.http, |response| {\n                    response\n                        .kind(InteractionResponseType::ChannelMessageWithSource)\n                        .interaction_response_data(|message| message.content(content))\n                })\n                .await\n            {\n                error!(\"Cannot respond to slash command: {}\", why);\n            } else {\n                debug!(\"Response sent successfully\");\n            }\n        }\n    }\n\n    async fn message(&self, ctx: Context, msg: Message) {\n        if msg.mentions_me(&ctx.http).await.unwrap_or(false) {\n            debug!(\"Bot mentioned in message: {}\", msg.content);\n\n            let bot_id = {\n                let data = ctx.data.read().await;\n                data.get::<BotUserId>().copied()\n            };\n\n            if let Some(bot_id) = bot_id {\n                let mention = format!(\"<@{}>\", bot_id);\n                let content = msg.content.replace(&mention, \"\").trim().to_string();\n\n                debug!(\"Processed content after removing mention: {}\", content);\n\n                match self.rig_agent.process_message(&content).await {\n                    Ok(response) => {\n                        if let Err(why) = msg.channel_id.say(&ctx.http, response).await {\n                            error!(\"Error sending message: {:?}\", why);\n                        }\n                    }\n                    Err(e) => {\n                        error!(\"Error processing message: {:?}\", e);\n                        if let Err(why) = msg\n                            .channel_id\n                            .say(&ctx.http, format!(\"Error processing message: {:?}\", e))\n                            .await\n                        {\n                            error!(\"Error sending error message: {:?}\", why);\n                        }\n                    }\n                }\n            } else {\n                error!(\"Bot user ID not found in TypeMap\");\n            }\n        }\n    }\n\n    async fn ready(&self, ctx: Context, ready: Ready) {\n        info!(\"{} is connected!\", ready.user.name);\n\n        {\n            let mut data = ctx.data.write().await;\n            data.insert::<BotUserId>(ready.user.id);\n        }\n\n        let commands = Command::set_global_application_commands(&ctx.http, |commands| {\n            commands\n                .create_application_command(|command| {\n                    command\n                        .name(\"hello\")\n                        .description(\"Say hello to the bot\")\n                })\n                .create_application_command(|command| {\n                    command\n                        .name(\"ask\")\n                        .description(\"Ask the bot a question\")\n                        .create_option(|option| {\n                            option\n                                .name(\"query\")\n                                .description(\"Your question for the bot\")\n                                .kind(CommandOptionType::String)\n                                .required(true)\n                        })\n                })\n        })\n        .await;\n\n        println!(\"Created the following global commands: {:#?}\", commands);\n    }\n}\n\n#[tokio::main]\nasync fn main() -> Result<()> {\n    dotenv().ok();\n\n    tracing_subscriber::fmt()\n        .with_max_level(tracing::Level::DEBUG)\n        .init();\n\n    let token = env::var(\"DISCORD_TOKEN\").expect(\"Expected DISCORD_TOKEN in environment\");\n\n    let rig_agent = Arc::new(RigAgent::new().await?);\n\n    let intents = GatewayIntents::GUILD_MESSAGES\n        | GatewayIntents::DIRECT_MESSAGES\n        | GatewayIntents::MESSAGE_CONTENT;\n\n    let mut client = Client::builder(&token, intents)\n        .event_handler(Handler {\n            rig_agent: Arc::clone(&rig_agent),\n        })\n        .await\n        .expect(\"Err creating client\");\n\n    if let Err(why) = client.start().await {\n        error!(\"Client error: {:?}\", why);\n    }\n\n    Ok(())\n}\n```\n\n----------------------------------------\n\nTITLE: Creating an Extraction Pipeline for Sentiment Analysis\nDESCRIPTION: Shows how to build a pipeline that extracts structured sentiment data from text, using a Sentiment struct with score and label fields.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/chains.mdx#2025-04-15_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::pipeline::{self, Op};\n\n#[derive(Deserialize, JsonSchema)]\nstruct Sentiment {\n    score: f64,\n    label: String,\n}\n\nlet pipeline = pipeline::new()\n    .map(|text| format!(\"Analyze sentiment: {}\", text))\n    .extract::<_, _, Sentiment>(extractor);\n```\n\n----------------------------------------\n\nTITLE: Tool Management in Rust\nDESCRIPTION: Demonstrates how agents manage static and dynamic tool sets, resolve tool calls, and handle tool execution.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/agent.mdx#2025-04-15_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\nlet dynamic_tools = stream::iter(self.dynamic_tools.iter())\n    .then(|(num_sample, index)| async {\n        Ok::<_, VectorStoreError>(\n            index\n                .top_n_ids(prompt, *num_sample)\n                .await?\n                .into_iter()\n                .map(|(_, id)| id)\n                .collect::<Vec<_>>()\n        )\n    })\n    .try_fold(vec![], |mut acc, docs| async {\n        for doc in docs {\n            if let Some(tool) = self.tools.get(&doc) {\n                acc.push(tool.definition(prompt.into()).await)\n            } else {\n                tracing::warn!(\"Tool implementation not found in toolset: {}\", doc);\n            }\n        }\n        Ok(acc)\n    })\n    .await\n    .map_err(|e| CompletionError::RequestError(Box::new(e)))?\n```\n\n----------------------------------------\n\nTITLE: Complete LanceDB Integration Example with Document Embedding and Search in Rust\nDESCRIPTION: This snippet provides a complete example of using LanceDB with Rig, including initializing the OpenAI client, setting up LanceDB locally, generating embeddings for test data, and preparing for vector search operations.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/lancedb.mdx#2025-04-15_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\n#[path = \"./fixtures/lib.rs\"]\nmod fixture;\n\n#[tokio::main]\nasync fn main() -> Result<(), anyhow::Error> {\n    // Initialize OpenAI client. Use this to generate embeddings (and generate test data for RAG demo).\n    let openai_client = Client::from_env();\n\n    // Select an embedding model.\n    let model = openai_client.embedding_model(TEXT_EMBEDDING_ADA_002);\n\n    // Initialize LanceDB locally.\n    let db = lancedb::connect(\"data/lancedb-store\").execute().await?;\n\n    // Generate embeddings for the test data.\n    let embeddings = EmbeddingsBuilder::new(model.clone())\n        .documents(words())?\n        // Note: need at least 256 rows in order to create an index so copy the definition 256 times for testing purposes.\n        .documents(\n            (0..256)\n```\n\n----------------------------------------\n\nTITLE: Advanced Request Configuration Example in Rust\nDESCRIPTION: Demonstrates advanced usage of the completion API with detailed request configuration. It sets up a request with custom preamble, temperature, context documents, and available tools before sending it.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/completion.mdx#2025-04-15_snippet_8\n\nLANGUAGE: rust\nCODE:\n```\nlet request = model\n    .completion_request(\"Complex query\")\n    .preamble(\"Expert system\")\n    .temperature(0.8)\n    .documents(context)\n    .tools(available_tools)\n    .send()\n    .await?;\n```\n\n----------------------------------------\n\nTITLE: Implementing Batch Processing for Multiple Inputs\nDESCRIPTION: Demonstrates how to use batch processing to analyze sentiment for multiple documents concurrently, improving throughput for parallel operations.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/chains.mdx#2025-04-15_snippet_9\n\nLANGUAGE: rust\nCODE:\n```\nlet pipeline = pipeline::new()\n    .map(|text| analyze_sentiment(text));\n\n// Process 5 documents concurrently\nlet results = pipeline.batch_call(5, documents).await;\n```\n\n----------------------------------------\n\nTITLE: Complete Usage Example of In-Memory Vector Store in Rust\nDESCRIPTION: A complete example demonstrating how to initialize an in-memory vector store, create embeddings, add documents to the store, and perform similarity search operations.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/in_memory.mdx#2025-04-15_snippet_6\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::providers::openai;\nuse rig::embeddings::EmbeddingsBuilder;\nuse rig::vector_store::in_memory_store::InMemoryVectorStore;\n\n#[tokio::main]\nasync fn main() -> Result<(), anyhow::Error> {\n    // Initialize store\n    let mut store = InMemoryVectorStore::default();\n\n    // Create embeddings\n    let embeddings = EmbeddingsBuilder::new(model)\n        .simple_document(\"doc1\", \"First document content\")\n        .simple_document(\"doc2\", \"Second document content\")\n        .build()\n        .await?;\n\n    // Add documents to store\n    store.add_documents(embeddings);\n\n    // Create vector store index\n    let index = store.index(model);\n\n    // Search similar documents\n    let results = store\n        .top_n::<Document>(\"search query\", 5)\n        .await?;\n\n    Ok(())\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing LanceDB Vector Store with S3 Storage in Rust\nDESCRIPTION: This snippet shows how to initialize a LanceDB vector store using S3 storage. It demonstrates setting up the OpenAI client for embeddings, connecting to LanceDB on S3, and generating embeddings for test data.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/lancedb.mdx#2025-04-15_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nlet model = openai_client.embedding_model(TEXT_EMBEDDING_ADA_002);\n\n// Initialize LanceDB on S3.\n// Note: see below docs for more options and IAM permission required to read/write to S3.\n// https://lancedb.github.io/lancedb/guides/storage/#aws-s3\nlet db = lancedb::connect(\"s3://lancedb-test-829666124233\")\n    .execute()\n    .await?;\n\n// Generate embeddings for the test data.\nlet embeddings = EmbeddingsBuilder::new(model.clone())\n    .documents(words())?\n    // Note: need at least 256 rows in order to create an index so copy the definition 256 times for testing purposes.\n    .documents(\n        (0..256)\n            .map(|i| Word {\n                id: format!(\"doc{}\", i),\n                definition: \"Definition of *flumbuzzle (noun)*: A sudden, inexplicable urge to rearrange or reorganize small objects, such as desk items or books, for no apparent reason.\".to_string()\n            })\n```\n\n----------------------------------------\n\nTITLE: Implementing RecordBatch Deserialization for LanceDB in Rust\nDESCRIPTION: This code defines a trait and implementation for deserializing LanceDB query results (RecordBatch) into serde_json::Value vectors. It includes error handling and type conversions for various Arrow data types.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/lancedb.mdx#2025-04-15_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\nuse std::sync::Arc;\n\nuse arrow_array:{\n    cast::AsArray,\n    types::\n        ArrowDictionaryKeyType, BinaryType, ByteArrayType, Date32Type, Date64Type, Decimal128Type,\n        DurationMicrosecondType, DurationMillisecondType, DurationNanosecondType,\n        DurationSecondType, Float32Type, Float64Type, Int16Type, Int32Type, Int64Type, Int8Type,\n        IntervalDayTime, IntervalDayTimeType, IntervalMonthDayNano, IntervalMonthDayNanoType,\n        IntervalYearMonthType, LargeBinaryType, LargeUtf8Type, RunEndIndexType,\n        Time32MillisecondType, Time32SecondType, Time64MicrosecondType, Time64NanosecondType,\n        TimestampMicrosecondType, TimestampMillisecondType, TimestampNanosecondType,\n        TimestampSecondType, UInt16Type, UInt32Type, UInt64Type, UInt8Type, Utf8Type,\n    },\n    Array, ArrowPrimitiveType, OffsetSizeTrait, RecordBatch, RunArray, StructArray, UnionArray,\n};\nuse lancedb::arrow::arrow_schema::{ArrowError, DataType, IntervalUnit, TimeUnit};\nuse rig::vector_store::VectorStoreError;\nuse serde::Serialize;\nuse serde_json::{json, Value};\n\nuse crate::serde_to_rig_error;\n\nfn arrow_to_rig_error(e: ArrowError) -> VectorStoreError {\n    VectorStoreError::DatastoreError(Box::new(e))\n}\n\n/// Trait used to deserialize data returned from LanceDB queries into a serde_json::Value vector.\n/// Data returned by LanceDB is a vector of `RecordBatch` items.\npub(crate) trait RecordBatchDeserializer {\n    fn deserialize(&self) -> Result<Vec<serde_json::Value>, VectorStoreError>;\n}\n\nimpl RecordBatchDeserializer for Vec<RecordBatch> {\n    fn deserialize(&self) -> Result<Vec<serde_json::Value>, VectorStoreError> {\n        Ok(self\n            .iter()\n            .map(|record_batch| record_batch.deserialize())\n            .collect::<Result<Vec<_>, _>>()?)\n            .into_iter()\n            .flatten()\n            .collect())\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic Agent in Rust\nDESCRIPTION: Demonstrates how to create a simple agent using OpenAI's GPT-4 model with custom configuration.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/agent.mdx#2025-04-15_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::{providers::openai, Agent};\n\nlet openai = openai::Client::from_env();\n\n// Create simple agent\nlet agent = openai.agent(\"gpt-4\")\n    .preamble(\"You are a helpful assistant.\")\n    .temperature(0.7)\n    .build();\n\n// Use the agent\nlet response = agent.prompt(\"Hello!\").await?;\n```\n\n----------------------------------------\n\nTITLE: Implementing OpenAI Provider in Rust\nDESCRIPTION: Core implementation of the OpenAI provider in Rig. It includes the main Client struct, its implementation, and the necessary imports. This snippet shows how the OpenAI integration is structured within the Rig framework.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/model_providers/openai.mdx#2025-04-15_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\n//! OpenAI API client and Rig integration\n//!\n//! # Example\n//! ```\n//! use rig::providers::openai;\n//!\n//! let client = openai::Client::new(\"YOUR_API_KEY\");\n//!\n//! let gpt4o = client.completion_model(openai::GPT_4O);\n//! ```\nuse crate::\n    agent::AgentBuilder,\n    completion::{self, CompletionError, CompletionRequest},\n    embeddings::{self, EmbeddingError, EmbeddingsBuilder},\n    extractor::ExtractorBuilder,\n    json_utils, Embed,\n};\nuse schemars::JsonSchema;\nuse serde::{Deserialize, Serialize};\nuse serde_json::json;\n\n// ================================================================\n// Main OpenAI Client\n// ================================================================\nconst OPENAI_API_BASE_URL: &str = \"https://api.openai.com\";\n\n#[derive(Clone)]\npub struct Client {\n    base_url: String,\n    http_client: reqwest::Client,\n}\n\nimpl Client {\n    /// Create a new OpenAI client with the given API key.\n    pub fn new(api_key: &str) -> Self {\n        Self::from_url(api_key, OPENAI_API_BASE_URL)\n    }\n\n    /// Create a new OpenAI client with the given API key and base API URL.\n\n```\n\n----------------------------------------\n\nTITLE: Connecting to Neo4j Database in Rust\nDESCRIPTION: Code for establishing a connection to a Neo4j database using the Neo4jClient. This snippet demonstrates how to connect to a Neo4j instance with authentication credentials.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/neo4j.mdx#2025-04-15_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nlet neo4j_client = Neo4jClient::connect(\"neo4j://localhost:7687\", \"username\", \"password\").await?;\n```\n\n----------------------------------------\n\nTITLE: Parsing and Formatting Flight Search Results in Rust\nDESCRIPTION: Processes the API response by parsing JSON data, extracting flight options, and formatting the results into a readable string for the user. Handles potential parsing errors and structures the output.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/flight_assistant.mdx#2025-04-15_snippet_10\n\nLANGUAGE: rust\nCODE:\n```\nlet text = response\n    .text()\n    .await\n    .map_err(|e| FlightSearchError::HttpRequestFailed(e.to_string()))?;\n\nlet data: Value = serde_json::from_str(&text)\n    .map_err(|e| FlightSearchError::HttpRequestFailed(e.to_string()))?;\n\nlet mut flight_options = Vec::new();\n\n// Here, we need to extract the flight options. (It's quite detailed, so we've omitted the full code to keep the focus clear.)\n\n// Format the flight options into a readable string\nlet mut output = String::new();\noutput.push_str(\"Here are some flight options:\\n\\n\");\n\nfor (i, option) in flight_options.iter().enumerate() {\n    output.push_str(&format!(\"{}. **Airline**: {}\\n\", i + 1, option.airline));\n    // Additional formatting...\n}\n\nOk(output)\n```\n\n----------------------------------------\n\nTITLE: Basic Completion Example in Rust\nDESCRIPTION: Demonstrates a simple usage pattern for basic completion requests using the OpenAI client. It initializes a client with an API key, creates a completion model, and sends a prompt to get a response.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/completion.mdx#2025-04-15_snippet_6\n\nLANGUAGE: rust\nCODE:\n```\nlet openai = Client::new(api_key);\nlet model = openai.completion_model(\"gpt-4\");\n\nlet response = model\n    .prompt(\"Explain quantum computing\")\n    .await?;\n```\n\n----------------------------------------\n\nTITLE: Implementing RigAgent with OpenAI Integration in Rust\nDESCRIPTION: Defines a RigAgent struct that implements RAG capabilities using OpenAI's models and Rig library. The agent loads markdown documents from the filesystem, creates embeddings, and initializes a vector store for context-aware responses. It includes preamble configuration for AI behavior and message processing functionality.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/discord_bot.mdx#2025-04-15_snippet_19\n\nLANGUAGE: rust\nCODE:\n```\n// rig_agent.rs\n\nuse anyhow::{Context, Result};\nuse rig::providers::openai;\nuse rig::vector_store::in_memory_store::InMemoryVectorStore;\nuse rig::embeddings::EmbeddingsBuilder;\nuse rig::rag::RagAgent;\nuse std::path::Path;\nuse std::fs;\nuse std::sync::Arc;\n\n\npub struct RigAgent {\n    rag_agent: Arc<RagAgent<openai::CompletionModel, rig::vector_store::InMemoryVectorIndex<openai::EmbeddingModel>, rig::vector_store::NoIndex>>,\n}\n\nimpl RigAgent {\n    pub async fn new() -> Result<Self> {\n        // Initialize OpenAI client\n        let openai_client = openai::Client::from_env();\n        let embedding_model = openai_client.embedding_model(\"text-embedding-3-small\");\n\n        // Create vector store\n        let mut vector_store = InMemoryVectorStore::default();\n\n        // Get the current directory and construct paths to markdown files\n        let current_dir = std::env::current_dir()?;\n        let documents_dir = current_dir.join(\"documents\");\n\n        let md1_path = documents_dir.join(\"Rig_guide.md\");\n        let md2_path = documents_dir.join(\"Rig_faq.md\");\n        let md3_path = documents_dir.join(\"Rig_examples.md\");\n\n        // Load markdown documents\n        let md1_content = Self::load_md_content(&md1_path)?;\n        let md2_content = Self::load_md_content(&md2_path)?;\n        let md3_content = Self::load_md_content(&md3_path)?;\n\n        // Create embeddings and add to vector store\n        let embeddings = EmbeddingsBuilder::new(embedding_model.clone())\n            .simple_document(\"Rig_guide\", &md1_content)\n            .simple_document(\"Rig_faq\", &md2_content)\n            .simple_document(\"Rig_examples\", &md3_content)\n            .build()\n            .await?;\n\n        vector_store.add_documents(embeddings).await?;\n\n        // Create index\n        let context_index = vector_store.index(embedding_model);\n\n        // Create RAG agent\n        let rag_agent = Arc::new(openai_client.context_rag_agent(\"gpt-4\")\n            .preamble(\"You are an advanced AI assistant powered by [Rig](https://rig.rs/), a Rust library for building LLM applications. Your primary function is to provide accurate, helpful, and context-aware responses by leveraging both your general knowledge and specific information retrieved from a curated knowledge base.\n\n                    Key responsibilities and behaviors:\n                    1. Information Retrieval: You have access to a vast knowledge base. When answering questions, always consider the context provided by the retrieved information.\n                    2. Clarity and Conciseness: Provide clear and concise answers. Ensure responses are short and to the point. Use bullet points or numbered lists for complex information when appropriate.\n                    3. Technical Proficiency: You have deep knowledge about Rig and its capabilities. When discussing Rig or answering related questions, provide detailed and technically accurate information.\n                    4. Code Examples: When appropriate, provide Rust code examples to illustrate concepts, especially when discussing Rig's functionalities. Always format code examples for proper rendering in Discord by wrapping them in triple backticks and specifying the language as 'rust'. For example:\n                        \\`\\`\\`rust\n                        let example_code = \\\"This is how you format Rust code for Discord\\\";\n                        println!(\\\"{}\\\", example_code);\n                        \\`\\`\\`\n                    \")\n            .dynamic_context(2, context_index)\n            .build());\n\n        Ok(Self { rag_agent })\n    }\n\n    pub async fn process_message(&self, message: &str) -> Result<String> {\n        self.rag_agent.prompt(message).await.map_err(anyhow::Error::from)\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client and Models in Rust\nDESCRIPTION: Demonstrates how to create an OpenAI client and initialize completion and embedding models using the Rig framework. It shows both environment variable and explicit API key initialization methods.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/model_providers/openai.mdx#2025-04-15_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::providers::openai;\n\n// Create client from environment variable\nlet client = openai::Client::from_env();\n\n// Or explicitly with API key\nlet client = openai::Client::new(\"your-api-key\");\n\n// Create a completion model\nlet gpt4 = client.completion_model(openai::GPT_4);\n\n// Create an embedding model\nlet embedder = client.embedding_model(openai::TEXT_EMBEDDING_3_LARGE);\n```\n\n----------------------------------------\n\nTITLE: Agent System Integration Example\nDESCRIPTION: Demonstrates how to integrate an extractor as a tool within a larger agent-based system.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/extractors.mdx#2025-04-15_snippet_9\n\nLANGUAGE: rust\nCODE:\n```\nlet data_extractor = client.extractor::<StructuredData>(model).build();\nlet agent = client.agent(model)\n    .tool(data_extractor)\n    .build();\n```\n\n----------------------------------------\n\nTITLE: Initializing Anthropic Client and Models in Rust\nDESCRIPTION: Demonstrates how to create an Anthropic client with specific version and beta features, and then instantiate either a completion model or an agent with a preamble.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/model_providers/anthropic.mdx#2025-04-15_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::providers::anthropic::{ClientBuilder, CLAUDE_3_SONNET};\n\n// Create client with specific version and beta features\nlet client = ClientBuilder::new(\"your-api-key\")\n    .anthropic_version(\"2023-06-01\")\n    .anthropic_beta(\"prompt-caching-2024-07-31\")\n    .build();\n\n// Create a completion model\nlet claude = client.completion_model(CLAUDE_3_SONNET);\n\n// Or create an agent directly\nlet agent = client\n    .agent(CLAUDE_3_SONNET)\n    .preamble(\"You are a helpful assistant\")\n    .build();\n```\n\n----------------------------------------\n\nTITLE: Implementing Named Entity Recognition in Rust\nDESCRIPTION: Creates a named entity recognizer using OpenAI's GPT-3.5-turbo model to extract and classify entities from text.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/text_extraction_classification.mdx#2025-04-15_snippet_8\n\nLANGUAGE: rust\nCODE:\n```\n#[tokio::main]\nasync fn main() -> Result<()> {\n    dotenv::dotenv().ok();\n    let openai_client = openai::Client::from_env()?;\n    \n    let ner_extractor = openai_client\n        .extractor::<ExtractedEntities>(\"gpt-3.5-turbo\")\n        .preamble(\"\n            You are a named entity recognition AI. Identify and extract people, organizations, and locations from the given text.\n            Provide the start and end indices for each entity.\n        \")\n        .build();\n\n    let text = \"Apple Inc., based in Cupertino, was founded by Steve Jobs and Steve Wozniak.\";\n    let result = ner_extractor.extract(text).await?;\n\n    println!(\"Text: {}\", text);\n    for entity in result.entities {\n        println!(\n            \"Entity: {:?}, Type: {:?}, Range: {}:{}\",\n            entity.text, entity.entity_type, entity.start, entity.end\n        );\n    }\n\n    Ok(())\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Sentiment Classifier with Rig and OpenAI\nDESCRIPTION: Main function implementing a sentiment analysis classifier using Rig's Extractor with OpenAI's GPT-3.5-turbo model, demonstrating the complete workflow from initialization to classification.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/text_extraction_classification.mdx#2025-04-15_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\n#[tokio::main]\nasync fn main() -> Result<()> {\n    dotenv::dotenv().ok(); // Load environment variables securely\n    let openai_client = openai::Client::from_env()?;\n    \n    let sentiment_classifier = openai_client\n        .extractor::<SentimentClassification>(\"gpt-3.5-turbo\")\n        .preamble(\"You are a sentiment analysis AI. Classify the sentiment of the given text.\")\n        .build();\n\n    let text = \"I absolutely loved the new restaurant. The food was amazing!\";\n    let result = sentiment_classifier.extract(text).await?;\n\n    println!(\"Text: {}\", text);\n    println!(\"Sentiment: {:?}\", result.sentiment);\n    println!(\"Confidence: {:.2}\", result.confidence);\n\n    Ok()\n}\n```\n\n----------------------------------------\n\nTITLE: Batch Processing Implementation for Extractors\nDESCRIPTION: Implementation of a batch processing function that handles multiple documents using a single extractor instance.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/extractors.mdx#2025-04-15_snippet_7\n\nLANGUAGE: rust\nCODE:\n```\nasync fn process_documents(extractor: &Extractor<Model, DataType>, docs: Vec<String>) -> Vec<Result<DataType, ExtractionError>> {\n    let mut results = Vec::new();\n    for doc in docs {\n        results.push(extractor.extract(&doc).await);\n    }\n    results\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing the Embed Trait for Custom Types in Rust\nDESCRIPTION: Demonstrates how to make custom types embeddable by implementing the Embed trait. This example shows how to embed a Document type with title and content fields.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/embeddings.mdx#2025-04-15_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nstruct Document {\n    title: String,\n    content: String\n}\n\nimpl Embed for Document {\n    fn embed(&self, embedder: &mut TextEmbedder) -> Result<(), EmbedError> {\n        embedder.embed(self.title.clone());\n        embedder.embed(self.content.clone());\n        Ok(())\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom CompletionModel Provider in Rust\nDESCRIPTION: Shows how to implement the CompletionModel trait for a custom provider. The implementation defines the response type and provides the completion method that handles provider-specific logic for generating completion responses.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/completion.mdx#2025-04-15_snippet_9\n\nLANGUAGE: rust\nCODE:\n```\nimpl CompletionModel for CustomProvider {\n    type Response = CustomResponse;\n    \n    async fn completion(\n        &self, \n        request: CompletionRequest\n    ) -> Result<CompletionResponse<Self::Response>, CompletionError> {\n        // Provider-specific implementation\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Using CLI Chatbot with OpenAI Agent in Rust\nDESCRIPTION: Demonstrates how to use the cli_chatbot function with an OpenAI agent. It sets up the agent with a specific model and preamble, then runs the chatbot.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/extensions/cli_chatbot.mdx#2025-04-15_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::{cli_chatbot, providers::openai};\n\nlet agent = openai.agent(\"gpt-4\")\n    .preamble(\"You are a helpful assistant.\")\n    .build();\n\ncli_chatbot(agent).await?;\n```\n\n----------------------------------------\n\nTITLE: PDF Document Processing Implementation\nDESCRIPTION: Implementation of PDF document loading traits with error handling and path context.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/loaders.mdx#2025-04-15_snippet_6\n\nLANGUAGE: rust\nCODE:\n```\n    fn load(self) -> Result<Document, PdfLoaderError> {\n        Document::load(self).map_err(PdfLoaderError::PdfError)\n    }\n    fn load_with_path(self) -> Result<(PathBuf, Document), PdfLoaderError> {\n        let contents = Document::load(&self);\n        Ok((self, contents?))\n    }\n}\nimpl<T: Loadable> Loadable for Result<T, PdfLoaderError> {\n    fn load(self) -> Result<Document, PdfLoaderError> {\n        self.map(|t| t.load())?\n    }\n    fn load_with_path(self) -> Result<(PathBuf, Document), PdfLoaderError> {\n        self.map(|t| t.load_with_path())?\n    }\n```\n\n----------------------------------------\n\nTITLE: Initializing and Searching with MongoDB Vector Store in Rust\nDESCRIPTION: This snippet demonstrates how to initialize a MongoDB vector store and perform a search operation using Rig's MongoDbVectorIndex. It shows the basic setup and usage of the vector store for semantic search capabilities.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/mongodb.mdx#2025-04-15_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nuse rig_mongodb::{MongoDbVectorIndex, SearchParams};\n\n// Initialize the vector store\nlet index = MongoDbVectorIndex::new(\n    collection,\n    embedding_model,\n    \"vector_index\",\n    SearchParams::new()\n).await?;\n\n// Search for similar documents\nlet results = index.top_n::<Document>(\"search query\", 5).await?;\n```\n\n----------------------------------------\n\nTITLE: Using Multiple LLM Providers with Rig's Unified API in Rust\nDESCRIPTION: Demonstrates Rig's consistent API across different LLM providers (OpenAI and Cohere), allowing developers to switch between providers with minimal code changes while maintaining the same interface for prompting models.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/why_rig.mdx#2025-04-15_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::providers::{openai, cohere};\nuse rig::completion::Prompt;\n\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn std::error::Error>> {\n    // Initialize OpenAI client using environment variables\n    let openai_client = openai::Client::from_env();\n    let gpt4 = openai_client.model(\"gpt-4\").build();\n\n    // Initialize Cohere client with API key from environment variable\n    let cohere_client = cohere::Client::new(&std::env::var(\"COHERE_API_KEY\")?);\n    let command = cohere_client.model(\"command\").build();\n\n    // Use OpenAI's GPT-4 to explain quantum computing\n    let gpt4_response = gpt4.prompt(\"Explain quantum computing in one sentence.\").await?;\n    println!(\"GPT-4: {}\", gpt4_response);\n\n    // Use Cohere's Command model to explain quantum computing\n    let command_response = command.prompt(\"Explain quantum computing in one sentence.\").await?;\n    println!(\"Cohere Command: {}\", command_response);\n\n    Ok()\n}\n```\n\n----------------------------------------\n\nTITLE: Parallel Operations Implementation with Recursion Step\nDESCRIPTION: A code snippet from the parallel operations implementation showing the token accumulation process for parallel futures with position tracking.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/chains.mdx#2025-04-15_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\n            ]\n            values_and_positions: [\n                $($acc)*\n                $current ( $($underscores)* + )\n            ]\n            munching: []\n        }\n    );\n\n    // Recursion step: map each value with its \"position\" (underscore count).\n    (\n        // Accumulate a token for each future that has been expanded: \"_ _ _\".\n        current_position: [\n```\n\n----------------------------------------\n\nTITLE: Connecting to LanceDB Store in Rust\nDESCRIPTION: Code snippet showing how to establish connection to LanceDB store using the LanceDB client in Rust, connecting to an EFS mount point\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/deploy/Blog_2_aws_lambda_lancedb.mdx#2025-04-15_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\nlet db = lancedb::connect(\"/mnt/efs\").execute().await?;\n```\n\n----------------------------------------\n\nTITLE: Creating a Tool-First Agent in Rust\nDESCRIPTION: Illustrates the creation of an agent that prioritizes tool usage, including both static and dynamic tools.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/agent.mdx#2025-04-15_snippet_8\n\nLANGUAGE: rust\nCODE:\n```\nlet tool_agent = openai.agent(\"gpt-4\")\n    .preamble(\"You are a tool-using assistant.\")\n    .tool(calculator)\n    .tool(web_search)\n    .dynamic_tools(2, tool_store, toolset)\n    .temperature(0.5)\n    .build();\n```\n\n----------------------------------------\n\nTITLE: Initializing and Prompting Gemini Agent in Rust\nDESCRIPTION: This snippet demonstrates how to initialize a Gemini client, create an agent with custom configuration, and prompt it for a response. It uses the gemini::Client, sets up a custom agent with specific parameters, and handles the response.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/examples/model_providers/gemini.mdx#2025-04-15_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::\n    completion::Prompt,\n    providers::gemini::{self, completion::gemini_api_types::GenerationConfig},\n};\n#[tracing::instrument(ret)]\n#[tokio::main]\n\nasync fn main() -> Result<(), anyhow::Error> {\n    tracing_subscriber::fmt()\n        .with_max_level(tracing::Level::DEBUG)\n        .with_target(false)\n        .init();\n\n    // Initialize the Google Gemini client\n    let client = gemini::Client::from_env();\n\n    // Create agent with a single context prompt\n    let agent = client\n        .agent(gemini::completion::GEMINI_1_5_PRO)\n        .preamble(\"Be creative and concise. Answer directly and clearly.\")\n        .temperature(0.5)\n        // The `GenerationConfig` utility struct helps construct a typesafe `additional_params`\n        .additional_params(serde_json::to_value(GenerationConfig {\n            top_k: Some(1),\n            top_p: Some(0.95),\n            candidate_count: Some(1),\n            ..Default::default()\n        })?)\n        .build();\n\n    tracing::info!(\"Prompting the agent...\");\n\n    // Prompt the agent and print the response\n    let response = agent\n        .prompt(\"How much wood would a woodchuck chuck if a woodchuck could chuck wood? Infer an answer.\")\n        .await;\n\n    tracing::info!(\"Response: {:?}\", response);\n\n    match response {\n        Ok(response) => println!(\"{}\", response),\n        Err(e) => {\n            tracing::error!(\"Error: {:?}\", e);\n            return Err(e.into());\n        }\n    }\n\n    Ok()\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Conversational Agent in Rust\nDESCRIPTION: Demonstrates how to create and use a conversational agent with specific temperature settings.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/agent.mdx#2025-04-15_snippet_6\n\nLANGUAGE: rust\nCODE:\n```\nlet chat_agent = openai.agent(\"gpt-4\")\n    .preamble(\"You are a conversational assistant.\")\n    .temperature(0.9)\n    .build();\n\nlet response = chat_agent\n    .chat(\"Hello!\", previous_messages)\n    .await?;\n```\n\n----------------------------------------\n\nTITLE: Connecting to LanceDB with S3 Storage in Rust\nDESCRIPTION: Code to establish a connection to a LanceDB database stored in an S3 bucket. Includes an alternative option using DynamoDB commit store to prevent data corruption from concurrent writes.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/deploy/Blog_2_aws_lambda_lancedb.mdx#2025-04-15_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\n// Note: Create s3://rig-montreal-lancedb bucket beforehand\nlet db = lancedb::connect(\"s3://rig-montreal-lancedb\").execute().await?;\n// OR\nlet db = lancedb::connect(\"s3+ddb://rig-montreal-lancedb?ddbTableName=my-dynamodb-table\").execute().await?;\n```\n\n----------------------------------------\n\nTITLE: Creating Simple and Async Operations in Rust Pipeline Module\nDESCRIPTION: Demonstrates how to create basic operations (ops) in a pipeline - a simple operation that adds two numbers and an asynchronous operation that multiplies a value by 2.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/chains.mdx#2025-04-15_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::pipeline::{self, Op};\n\n// Simple operation that adds two numbers\nlet add_op = pipeline::new()\n    .map(|(x, y)| x + y);\n\n// Operation with async processing\nlet async_op = pipeline::new()\n    .then(|x| async move { x * 2 });\n```\n\n----------------------------------------\n\nTITLE: Implementing the Main Function for Flight Search Agent in Rust\nDESCRIPTION: Sets up and runs the AI agent with the flight search tool. Uses environment variables, creates an OpenAI client, builds an agent with the tool, and sends a prompt to search for flights between specific airports.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/flight_assistant.mdx#2025-04-15_snippet_11\n\nLANGUAGE: rust\nCODE:\n```\nmod flight_search_tool;\n\nuse crate::flight_search_tool::FlightSearchTool;\nuse dotenv::dotenv;\nuse rig::completion::Prompt;\nuse rig::providers::openai;\nuse std::error::Error;\n\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn Error>> {\n    dotenv().ok();\n\n    let openai_client = openai::Client::from_env();\n\n    let agent = openai_client\n        .agent(\"gpt-4\")\n        .preamble(\"You are a helpful assistant that can find flights for users.\")\n        .tool(FlightSearchTool)\n        .build();\n\n    let response = agent\n        .prompt(\"Find me flights from San Antonio (SAT) to Atlanta (ATL) on November 15th 2024.\")\n        .await?;\n\n    println!(\"Agent response:\\n{}\", response);\n\n    Ok()\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Discord Slash Command Interactions in Rust\nDESCRIPTION: Implements the interaction_create event handler to process slash commands. It handles \"hello\" and \"ask\" commands, using the RigAgent to process user queries and respond appropriately.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/discord_bot.mdx#2025-04-15_snippet_10\n\nLANGUAGE: rust\nCODE:\n```\nasync fn interaction_create(&self, ctx: Context, interaction: Interaction) {\n    debug!(\"Received an interaction\");\n    if let Interaction::ApplicationCommand(command) = interaction {\n        debug!(\"Received command: {}\", command.data.name);\n        let content = match command.data.name.as_str() {\n            \"hello\" => \"Hello! I'm your helpful Rust and Rig-powered assistant. How can I assist you today?\".to_string(),\n            \"ask\" => {\n                let query = command\n                    .data\n                    .options\n                    .get(0)\n                    .and_then(|opt| opt.value.as_ref())\n                    .and_then(|v| v.as_str())\n                    .unwrap_or(\"What would you like to ask?\");\n                debug!(\"Query: {}\", query);\n                match self.rig_agent.process_message(query).await {\n                    Ok(response) => response,\n                    Err(e) => {\n                        error!(\"Error processing request: {:?}\", e);\n                        format!(\"Error processing request: {:?}\", e)\n                    }\n                }\n            }\n            _ => \"Not implemented :(\".to_string(),\n        };\n\n        debug!(\"Sending response: {}\", content);\n\n        if let Err(why) = command\n            .create_interaction_response(&ctx.http, |response| {\n                response\n                    .kind(InteractionResponseType::ChannelMessageWithSource)\n                    .interaction_response_data(|message| message.content(content))\n            })\n            .await\n        {\n            error!(\"Cannot respond to slash command: {}\", why);\n        } else {\n            debug!(\"Response sent successfully\");\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Vector Index in Neo4j with Rust\nDESCRIPTION: Code for creating a vector index in Neo4j using the Neo4jClient. This illustrates how to define an index configuration for a specific node type and embedding model.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/neo4j.mdx#2025-04-15_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nneo4j_client.create_vector_index(\n    IndexConfig::new(\"moviePlots\"),\n    \"Movie\",\n    &model\n).await?;\n```\n\n----------------------------------------\n\nTITLE: News Article Analysis Data Structures in Rust\nDESCRIPTION: Defines data structures for comprehensive news article analysis including topic classification and key points extraction.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/text_extraction_classification.mdx#2025-04-15_snippet_11\n\nLANGUAGE: rust\nCODE:\n```\n#[derive(Debug, Deserialize, Serialize)]\nenum Topic {\n    Politics,\n    Technology,\n    Sports,\n    Entertainment,\n    Other(String),\n}\n\n#[derive(Debug, Deserialize, Serialize)]\nstruct NewsArticleAnalysis {\n    topic: Topic,\n    sentiment: SentimentClassification,\n    entities: Vec<Entity>,\n    key_points: Vec<String>,\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Flight Search Tool in Rust\nDESCRIPTION: Core implementation of the flight search tool including imports, data structures, and error handling\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/flight_assistant.mdx#2025-04-15_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\nuse chrono::{DateTime, Duration, Utc};\nuse rig::completion::ToolDefinition;\nuse rig::tool::Tool;\nuse serde::{Deserialize, Serialize};\nuse serde_json::{json, Value};\nuse std::collections::HashMap;\nuse std::env;\n\n#[derive(Deserialize)]\npub struct FlightSearchArgs {\n    source: String,\n    destination: String,\n    date: Option<String>,\n    sort: Option<String>,\n    service: Option<String>,\n    itinerary_type: Option<String>,\n    adults: Option<u8>,\n    seniors: Option<u8>,\n    currency: Option<String>,\n    nearby: Option<String>,\n    nonstop: Option<String>,\n}\n\n#[derive(Serialize)]\npub struct FlightOption {\n    pub airline: String,\n    pub flight_number: String,\n    pub departure: String,\n    pub arrival: String,\n    pub duration: String,\n    pub stops: usize,\n    pub price: f64,\n    pub currency: String,\n    pub booking_url: String,\n}\n\n#[derive(Debug, thiserror::Error)]\npub enum FlightSearchError {\n    #[error(\"HTTP request failed: {0}\")]\n    HttpRequestFailed(String),\n    #[error(\"Invalid response structure\")]\n    InvalidResponse,\n    #[error(\"API error: {0}\")]\n    ApiError(String),\n    #[error(\"Missing API key\")]\n    MissingApiKey,\n}\n```\n\n----------------------------------------\n\nTITLE: Loading PDF Documents in Rust\nDESCRIPTION: Shows PDF document loading with PdfFileLoader including page extraction and error handling capabilities.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/loaders.mdx#2025-04-15_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::loaders::PdfFileLoader;\n\nlet documents = PdfFileLoader::with_glob(\"docs/*.pdf\")?\n    .load_with_path()\n    .ignore_errors()\n    .by_page()\n    .into_iter();\n```\n\n----------------------------------------\n\nTITLE: Performing Vector Search in Neo4j with Rust\nDESCRIPTION: Code snippet showing how to query a Neo4j vector index for similar nodes. This example searches for movies matching a text description and limits results to the top 5 matches.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/neo4j.mdx#2025-04-15_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\nlet results = index.top_n::<Movie>(\"a historical movie on quebec\", 5).await?;\n```\n\n----------------------------------------\n\nTITLE: Implementing Combined Text Analysis in Rust\nDESCRIPTION: Creates a unified text analyzer that performs both sentiment analysis and named entity recognition.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/text_extraction_classification.mdx#2025-04-15_snippet_10\n\nLANGUAGE: rust\nCODE:\n```\n#[tokio::main]\nasync fn main() -> Result<()> {\n    dotenv::dotenv().ok();\n    let openai_client = openai::Client::from_env()?;\n    \n    let text_analyzer = openai_client\n        .extractor::<TextAnalysis>(\"gpt-3.5-turbo\")\n        .preamble(\"\n            You are a text analysis AI. For the given text:\n            1. Classify the overall sentiment (Positive, Negative, or Neutral) with a confidence score.\n            2. Identify and extract named entities (Person, Organization, Location) with their start and end indices.\n        \")\n        .build();\n\n    let text = \"I had a great time visiting Google's headquarters in Mountain View. Sundar Pichai's leadership has been impressive.\";\n    let result = text_analyzer.extract(text).await?;\n\n    println!(\"Text: {}\", text);\n    println!(\"Sentiment: {:?} (Confidence: {:.2})\", result.sentiment.sentiment, result.sentiment.confidence);\n    println!(\"Entities:\");\n    for entity in result.entities {\n        println!(\n            \"- {:?}: {} ({}:{})\",\n            entity.entity_type, entity.text, entity.start, entity.end\n        );\n    }\n\n    Ok(())\n}\n```\n\n----------------------------------------\n\nTITLE: Performing Vector Search with Qdrant in Rust\nDESCRIPTION: Executes a semantic search query to find the top 5 movie entries matching the provided query string.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/qdrant.mdx#2025-04-15_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\nlet results = vector_store.top_n::<Movie>(\"a historical movie on quebec\", 5).await?;\n```\n\n----------------------------------------\n\nTITLE: Implementing the Tool Trait for FlightSearchTool in Rust\nDESCRIPTION: Implements the Tool trait for the FlightSearchTool, defining its name, arguments, output, error types, and core functionality. This implementation includes both the definition method that provides metadata and a basic call method.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/flight_assistant.mdx#2025-04-15_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\nimpl Tool for FlightSearchTool {\n    const NAME: &'static str = \"search_flights\";\n\n    type Args = FlightSearchArgs;\n    type Output = String;\n    type Error = FlightSearchError;\n\n    async fn definition(&self, _prompt: String) -> ToolDefinition {\n        ToolDefinition {\n            name: Self::NAME.to_string(),\n            description: \"Search for flights between two airports\".to_string(),\n            parameters: json!({\n                \"type\": \"object\",\n                \"properties\": {\n                    \"source\": { \"type\": \"string\", \"description\": \"Source airport code (e.g., 'JFK')\" },\n                    \"destination\": { \"type\": \"string\", \"description\": \"Destination airport code (e.g., 'LAX')\" },\n                    \"date\": { \"type\": \"string\", \"description\": \"Flight date in 'YYYY-MM-DD' format\" },\n                },\n                \"required\": [\"source\", \"destination\"]\n            }),\n        }\n    }\n\n    async fn call(&self, args: Self::Args) -> Result<Self::Output, Self::Error> {\n        // We'll implement the logic for calling the flight search API next.\n        Ok(\"Flight search results\".to_string())\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining LanceDB Schema for Embeddings in Rust\nDESCRIPTION: This code snippet demonstrates how to define the required schema for storing embeddings in LanceDB. It creates a schema with fields for id, definition, and embedding, where the embedding is stored as a fixed-size list of float64 values.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/lancedb.mdx#2025-04-15_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nSchema::new(Fields::from(vec![\n    Field::new(\"id\", DataType::Utf8, false),\n    Field::new(\"definition\", DataType::Utf8, false),\n    Field::new(\n        \"embedding\",\n        DataType::FixedSizeList(\n            Arc::new(Field::new(\"item\", DataType::Float64, true)),\n            dims as i32,\n        ),\n        false,\n    ),\n]))\n```\n\n----------------------------------------\n\nTITLE: Initializing FileLoader in Rust\nDESCRIPTION: Demonstrates basic file loading using FileLoader to read Rust files from examples directory with glob pattern matching and error handling.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/loaders.mdx#2025-04-15_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::loaders::FileLoader;\n\n// Load all Rust files in examples directory\nlet examples = FileLoader::with_glob(\"examples/*.rs\")?\n    .read_with_path()\n    .ignore_errors()\n    .into_iter();\n```\n\n----------------------------------------\n\nTITLE: OpenAI Provider Integration Example in Rust\nDESCRIPTION: Example demonstrating how to initialize and use the OpenAI provider, including client setup, model creation, and agent configuration.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations.mdx#2025-04-15_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::providers::openai;\n\n// Initialize the client\nlet client = openai::Client::new(\"your-api-key\");\n\n// Create a model\nlet gpt4 = client.completion_model(\"gpt-4\");\n\n// Or create an agent directly\nlet agent = client.agent(\"gpt-4\")\n    .preamble(\"You are a helpful assistant\")\n    .build();\n```\n\n----------------------------------------\n\nTITLE: Configuring Flexible Extraction with Context\nDESCRIPTION: Shows how to customize an extractor with additional context, preamble, and model configuration for more precise extraction results.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/extractors.mdx#2025-04-15_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\nlet extractor = openai.extractor::<Person>(model)\n    .preamble(\"Extract person details with high precision\")\n    .context(\"Additional context about person formats\")\n    .build();\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Operations by Implementing the Op Trait\nDESCRIPTION: Shows how to implement a custom operation by implementing the Op trait, defining input/output types and the call method to split a string into a vector of words.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/chains.mdx#2025-04-15_snippet_8\n\nLANGUAGE: rust\nCODE:\n```\nstruct CustomOp;\n\nimpl Op for CustomOp {\n    type Input = String;\n    type Output = Vec<String>;\n\n    async fn call(&self, input: Self::Input) -> Self::Output {\n        input.split_whitespace()\n            .map(String::from)\n            .collect()\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Making API Request to Flight Search Service in Rust\nDESCRIPTION: Sends an HTTP GET request to the TripAdvisor flight search API using reqwest client, with proper headers and query parameters, and handles potential HTTP request errors.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/flight_assistant.mdx#2025-04-15_snippet_9\n\nLANGUAGE: rust\nCODE:\n```\nlet client = reqwest::Client::new();\nlet response = client\n    .get(\"https://tripadvisor16.p.rapidapi.com/api/v1/flights/searchFlights\")\n    .headers({\n        let mut headers = reqwest::header::HeaderMap::new();\n        headers.insert(\"X-RapidAPI-Host\", \"tripadvisor16.p.rapidapi.com\".parse().unwrap());\n        headers.insert(\"X-RapidAPI-Key\", api_key.parse().unwrap());\n        headers\n    })\n    .query(&query_params)\n    .send()\n    .await\n    .map_err(|e| FlightSearchError::HttpRequestFailed(e.to_string()))?\n```\n\n----------------------------------------\n\nTITLE: Anthropic Client Builder Implementation in Rust\nDESCRIPTION: Implements the ClientBuilder for configuring Anthropic API settings, including API key, base URL, version, and beta features. Provides a fluent API for client creation.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/model_providers/anthropic.mdx#2025-04-15_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\nconst ANTHROPIC_API_BASE_URL: &str = \"https://api.anthropic.com\";\n\n#[derive(Clone)]\npub struct ClientBuilder<'a> {\n    api_key: &'a str,\n    base_url: &'a str,\n    anthropic_version: &'a str,\n    anthropic_betas: Option<Vec<&'a str>>,\n}\n\n/// Create a new anthropic client using the builder\n///\n/// # Example\n/// ```\n/// use rig::providers::anthropic::{ClientBuilder, self};\n///\n/// // Initialize the Anthropic client\n/// let anthropic_client = ClientBuilder::new(\"your-claude-api-key\")\n///    .anthropic_version(ANTHROPIC_VERSION_LATEST)\n///    .anthropic_beta(\"prompt-caching-2024-07-31\")\n///    .build()\n/// ```\nimpl<'a> ClientBuilder<'a> {\n    pub fn new(api_key: &'a str) -> Self {\n        Self {\n            api_key,\n```\n\n----------------------------------------\n\nTITLE: Implementing the Chat Trait in Rust\nDESCRIPTION: Provides conversation-aware interactions with language models. It maintains chat history and supports contextual responses.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/completion.mdx#2025-04-15_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nasync fn chat(&self, prompt: &str, history: Vec<Message>) -> Result<String, PromptError>;\n```\n\n----------------------------------------\n\nTITLE: Basic Extraction Pattern Implementation\nDESCRIPTION: Shows the simplest pattern for performing data extraction using a configured extractor instance.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/extractors.mdx#2025-04-15_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\nlet extractor = client.extractor::<SimpleType>(model).build();\nlet data = extractor.extract(\"raw text\").await?;\n```\n\n----------------------------------------\n\nTITLE: Implementing Sentiment Classification with OpenAI in Rust\nDESCRIPTION: Creates a sentiment classifier using OpenAI's GPT-3.5-turbo model with example-based prompt engineering for improved accuracy.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/text_extraction_classification.mdx#2025-04-15_snippet_6\n\nLANGUAGE: rust\nCODE:\n```\nlet sentiment_classifier = openai_client\n    .extractor::<SentimentClassification>(\"gpt-3.5-turbo\")\n    .preamble(\"\n        You are a sentiment analysis AI. Classify the sentiment of the given text.\n        Examples:\n        Text: 'This movie was terrible. I hated every minute of it.'\n        Sentiment: Negative, Confidence: 0.9\n        Text: 'The weather today is okay, nothing special.'\n        Sentiment: Neutral, Confidence: 0.7\n        Text: 'I'm so excited about my upcoming vacation!'\n        Sentiment: Positive, Confidence: 0.95\n    \")\n    .build();\n```\n\n----------------------------------------\n\nTITLE: Defining the FlightSearchTool Structure in Rust\nDESCRIPTION: Creates the basic structure for the flight search tool that will be used by the AI agent.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/flight_assistant.mdx#2025-04-15_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\npub struct FlightSearchTool;\n```\n\n----------------------------------------\n\nTITLE: Implementing Sequential Operations in a Rust Pipeline\nDESCRIPTION: Shows how to chain multiple operations sequentially in a pipeline, processing data step by step from addition to string conversion.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/chains.mdx#2025-04-15_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::pipeline::{self, Op};\n\nlet pipeline = pipeline::new()\n    .map(|(x, y)| x + y)     // Add numbers\n    .map(|z| z * 2)          // Double result\n    .map(|n| n.to_string()); // Convert to string\n\nlet result = pipeline.call((5, 3)).await;\nassert_eq!(result, \"16\");\n```\n\n----------------------------------------\n\nTITLE: Building Query Parameters for Flight API in Rust\nDESCRIPTION: Constructs a HashMap of query parameters for the flight search API request, including source and destination airport codes and the flight date.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/flight_assistant.mdx#2025-04-15_snippet_8\n\nLANGUAGE: rust\nCODE:\n```\nlet mut query_params = HashMap::new();\nquery_params.insert(\"sourceAirportCode\", args.source);\nquery_params.insert(\"destinationAirportCode\", args.destination);\nquery_params.insert(\"date\", date);\n```\n\n----------------------------------------\n\nTITLE: Defining ModelChoice and CompletionResponse in Rust\nDESCRIPTION: Shows the structured response types used in Rig's completion system. ModelChoice handles different response types (text or tool calls), and CompletionResponse provides both the parsed choice and raw provider response.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/completion.mdx#2025-04-15_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\nenum ModelChoice {\n    Message(String),\n    ToolCall(String, Value)\n}\n\nstruct CompletionResponse<T> {\n    choice: ModelChoice,\n    raw_response: T,\n}\n```\n\n----------------------------------------\n\nTITLE: Creating In-Memory Vector Store with Auto-generated IDs in Rust\nDESCRIPTION: Example of creating an InMemoryVectorStore instance with automatically generated document IDs, where each document is paired with its corresponding embedding.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/in_memory.mdx#2025-04-15_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\nlet store = InMemoryVectorStore::from_documents(vec![\n    (doc1, embedding1),\n    (doc2, embedding2)\n]);\n```\n\n----------------------------------------\n\nTITLE: Implementing Vector Search Algorithm in Rust\nDESCRIPTION: Core implementation of the vector search algorithm that uses cosine similarity to find the most relevant documents. It employs a BinaryHeap for efficient top-N results selection and handles multiple embeddings per document.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/in_memory.mdx#2025-04-15_snippet_7\n\nLANGUAGE: rust\nCODE:\n```\n    /// Implement vector search on [InMemoryVectorStore].\n    /// To be used by implementations of [VectorStoreIndex::top_n] and [VectorStoreIndex::top_n_ids] methods.\n    fn vector_search(&self, prompt_embedding: &Embedding, n: usize) -> EmbeddingRanking<D> {\n        // Sort documents by best embedding distance\n        let mut docs = BinaryHeap::new();\n\n        for (id, (doc, embeddings)) in self.embeddings.iter() {\n            // Get the best context for the document given the prompt\n            if let Some((distance, embed_doc)) = embeddings\n                .iter()\n                .map(|embedding| {\n                    (\n                        OrderedFloat(embedding.cosine_similarity(prompt_embedding, false)),\n                        &embedding.document,\n                    )\n                })\n                .max_by(|a, b| a.0.cmp(&b.0))\n            {\n                docs.push(Reverse(RankingItem(distance, id, doc, embed_doc)));\n            };\n\n```\n\n----------------------------------------\n\nTITLE: Filtering Embedding Columns in LanceDB Schema with Rust\nDESCRIPTION: This snippet demonstrates how to filter out columns that do not include embeddings from a LanceDB table schema. It iterates through the schema fields and excludes columns with Float64 data type in FixedSizeList.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/lancedb.mdx#2025-04-15_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\nimpl FilterTableColumns for Arc<Schema> {\n    fn filter_embeddings(self) -> Vec<String> {\n        self.fields()\n            .iter()\n            .filter_map(|field| match field.data_type() {\n                DataType::FixedSizeList(inner, ..) => match inner.data_type() {\n                    DataType::Float64 => None,\n```\n\n----------------------------------------\n\nTITLE: Implementing Top-N Search for MongoDB Vector Store in Rust\nDESCRIPTION: This snippet shows the implementation of the top_n search function for the MongoDB vector store. It creates an embedding for the query and uses MongoDB's aggregation pipeline to perform vector similarity search, score calculation, and result formatting.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/mongodb.mdx#2025-04-15_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nasync fn top_n<T: for<'a> Deserialize<'a> + Send>(\n    &self,\n    query: &str,\n    n: usize,\n) -> Result<Vec<(f64, String, T)>, VectorStoreError> {\n    let prompt_embedding = self.model.embed_text(query).await?;\n\n    let mut cursor = self\n        .collection\n        .aggregate([\n```\n\n----------------------------------------\n\nTITLE: Vector Operations on Embeddings in Rust\nDESCRIPTION: Shows how to perform vector operations like calculating cosine similarity and euclidean distance between embeddings. Rig provides multiple distance metrics for comparing vector representations.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/embeddings.mdx#2025-04-15_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nlet similarity = embedding1.cosine_similarity(&embedding2, false);\nlet distance = embedding1.euclidean_distance(&embedding2);\n```\n\n----------------------------------------\n\nTITLE: Implementing CLI Chatbot Function in Rust\nDESCRIPTION: The core implementation of the cli_chatbot function. It handles user input, maintains chat history, processes responses, and manages the chat loop. It also includes error handling and tracing support.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/extensions/cli_chatbot.mdx#2025-04-15_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\npub async fn cli_chatbot(chatbot: impl Chat) -> Result<(), PromptError> {\n    let stdin = io::stdin();\n    let mut stdout = io::stdout();\n    let mut chat_log = vec![];\n\n    println!(\"Welcome to the chatbot! Type 'exit' to quit.\");\n    loop {\n        print!(\"> \");\n        // Flush stdout to ensure the prompt appears before input\n        stdout.flush().unwrap();\n\n        let mut input = String::new();\n        match stdin.read_line(&mut input) {\n            Ok(_) => {\n                // Remove the newline character from the input\n                let input = input.trim();\n                // Check for a command to exit\n                if input == \"exit\" {\n                    break;\n                }\n                tracing::info!(\"Prompt:\\n{}\\n\", input);\n\n                let response = chatbot.chat(input, chat_log.clone()).await?;\n                chat_log.push(Message {\n                    role: \"user\".into(),\n                    content: input.into(),\n                });\n                chat_log.push(Message {\n                    role: \"assistant\".into(),\n                    content: response.clone(),\n                });\n\n                println!(\"========================== Response ============================\");\n                println!(\"{response}\");\n                println!(\"================================================================\\n\\n\");\n\n                tracing::info!(\"Response:\\n{}\\n\", response);\n            }\n            Err(error) => println!(\"Error reading input: {}\", error),\n        }\n    }\n\n    Ok(())\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing the Prompt Trait in Rust\nDESCRIPTION: The simplest interface for one-shot interactions with language models. It provides a fire-and-forget prompting mechanism that returns string responses.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/completion.mdx#2025-04-15_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nasync fn prompt(&self, prompt: &str) -> Result<String, PromptError>;\n```\n\n----------------------------------------\n\nTITLE: Contextual Extraction Pattern Implementation\nDESCRIPTION: Demonstrates how to perform extraction with additional context and rules for more complex scenarios.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/extractors.mdx#2025-04-15_snippet_6\n\nLANGUAGE: rust\nCODE:\n```\nlet extractor = client.extractor::<ComplexType>(model)\n    .preamble(\"Extract with following rules...\")\n    .context(\"Domain-specific information...\")\n    .build();\n```\n\n----------------------------------------\n\nTITLE: Message Processing Function Implementation in Rust\nDESCRIPTION: Processes user messages using the RAG agent to generate contextually appropriate responses. Handles error mapping for consistent error handling.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/discord_bot.mdx#2025-04-15_snippet_6\n\nLANGUAGE: rust\nCODE:\n```\npub async fn process_message(&self, message: &str) -> Result<String> {\n    self.rag_agent.prompt(message).await.map_err(anyhow::Error::from)\n}\n```\n\n----------------------------------------\n\nTITLE: Defining CompletionError Types in Rust\nDESCRIPTION: Showcases the comprehensive error types used in Rig's completion system to handle various failure scenarios like HTTP errors, JSON parsing errors, request errors, response errors, and provider-specific errors.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/completion.mdx#2025-04-15_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\nenum CompletionError {\n    HttpError(reqwest::Error),\n    JsonError(serde_json::Error),\n    RequestError(Box<dyn Error>),\n    ResponseError(String),\n    ProviderError(String),\n}\n```\n\n----------------------------------------\n\nTITLE: Anthropic Completion Model Struct Definition in Rust\nDESCRIPTION: Defines the CompletionModel struct which encapsulates the HTTP client and model name, providing functionality for making completion requests to Anthropic models.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/model_providers/anthropic.mdx#2025-04-15_snippet_6\n\nLANGUAGE: rust\nCODE:\n```\n\n#[derive(Clone)]\npub struct CompletionModel {\n    client: Client,\n    pub model: String,\n}\n```\n\n----------------------------------------\n\nTITLE: Using try_batch_call for Error Handling in Rust Pipeline\nDESCRIPTION: Demonstrates how to use the try_batch_call method to process multiple inputs while handling potential errors, with a simple example checking results.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/chains.mdx#2025-04-15_snippet_7\n\nLANGUAGE: rust\nCODE:\n```\nlet result = op.try_batch_call(2, vec![2, 4]).await;\nassert_eq!(result, Ok(vec![3, 5]));\n```\n\n----------------------------------------\n\nTITLE: Processing LanceDB Query Results with RecordBatch in Rust\nDESCRIPTION: This code snippet shows how to process query results from LanceDB using Arrow's RecordBatch. It executes a vector query, collects the results into RecordBatches, and then deserializes them into JSON values.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/lancedb.mdx#2025-04-15_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nimpl QueryToJson for lancedb::query::VectorQuery {\n    async fn execute_query(&self) -> Result<Vec<serde_json::Value>, VectorStoreError> {\n        let record_batches = self\n            .execute()\n            .await\n            .map_err(lancedb_to_rig_error)?\n            .try_collect::<Vec<_>>()\n            .await\n            .map_err(lancedb_to_rig_error)?;\n\n        record_batches.deserialize()\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client and PDF Extraction in Rust\nDESCRIPTION: Sets up the OpenAI client using Rig and creates a function to extract text from PDF files. This forms the foundation for processing document content in the RAG system.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/rag/rag_system.mdx#2025-04-15_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::providers::openai;\nuse std::path::Path;\nuse anyhow::{Result, Context};\nuse pdf_extract::extract_text;\n\n// Function to load and extract text from a PDF file\nfn load_pdf_content<P: AsRef<Path>>(file_path: P) -> Result<String> {\n    extract_text(file_path.as_ref())\n        .with_context(|| format!(\"Failed to extract text from PDF: {:?}\", file_path.as_ref()))\n}\n\n// In your main function:\nlet openai_client = openai::Client::from_env();\n```\n\n----------------------------------------\n\nTITLE: Defining Data Structures for Sentiment Analysis\nDESCRIPTION: Rust code defining data structures for sentiment analysis, including an enum for sentiment classifications and a struct to hold sentiment analysis results.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/text_extraction_classification.mdx#2025-04-15_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\nuse serde::{Deserialize, Serialize};\nuse rig::providers::openai;\nuse rig::extractor::Extractor;\nuse anyhow::Result;\n\n#[derive(Debug, Deserialize, Serialize)]\nenum Sentiment {\n    Positive,\n    Negative,\n    Neutral,\n}\n\n#[derive(Debug, Deserialize, Serialize)]\nstruct SentimentClassification {\n    sentiment: Sentiment,\n    confidence: f32,\n}\n```\n\n----------------------------------------\n\nTITLE: Creating In-Memory Vector Store with Function-generated IDs in Rust\nDESCRIPTION: Example of creating an InMemoryVectorStore with IDs generated by a custom function that derives the ID from document properties.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/in_memory.mdx#2025-04-15_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\nlet store = InMemoryVectorStore::from_documents_with_id_f(\n    documents,\n    |doc| format!(\"doc_{}\", doc.title)\n);\n```\n\n----------------------------------------\n\nTITLE: Defining RigAgent Struct in Rust\nDESCRIPTION: Rust code defining the RigAgent struct that manages retrieval and response generation, using an Arc pointer to a RagAgent for thread-safe data sharing.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/discord_bot.mdx#2025-04-15_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\npub struct RigAgent {\n    rag_agent: Arc<RagAgent<openai::CompletionModel, rig::vector_store::InMemoryVectorIndex<openai::EmbeddingModel>, rig::vector_store::NoIndex>>,\n}\n```\n\n----------------------------------------\n\nTITLE: Anthropic Usage Struct Definition in Rust\nDESCRIPTION: Defines the Usage struct that captures detailed token usage information from Anthropic responses, including cache statistics for optimization tracking.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/model_providers/anthropic.mdx#2025-04-15_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\n    pub input_tokens: u64,\n    pub cache_read_input_tokens: Option<u64>,\n    pub cache_creation_input_tokens: Option<u64>,\n    pub output_tokens: u64,\n}\n\nimpl std::fmt::Display for Usage {\n```\n\n----------------------------------------\n\nTITLE: Handling Tool Calls and Responses in OpenAI Integration\nDESCRIPTION: Defines structures for handling tool calls and responses from OpenAI models. It includes definitions for Choice, Message, ToolCall, and ToolDefinition, showcasing how Rig processes and converts OpenAI's function calling format.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/model_providers/openai.mdx#2025-04-15_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\n#[derive(Debug, Deserialize)]\npub struct Choice {\n    pub index: usize,\n    pub message: Message,\n    pub logprobs: Option<serde_json::Value>,\n    pub finish_reason: String,\n}\n\n#[derive(Debug, Deserialize)]\npub struct Message {\n    pub role: String,\n    pub content: Option<String>,\n    pub tool_calls: Option<Vec<ToolCall>>,\n}\n\n#[derive(Debug, Deserialize)]\npub struct ToolCall {\n    pub id: String,\n    pub r#type: String,\n    pub function: Function,\n}\n\n#[derive(Clone, Debug, Deserialize, Serialize)]\npub struct ToolDefinition {\n    pub r#type: String,\n    pub function: completion::ToolDefinition,\n}\n\nimpl From<completion::ToolDefinition> for ToolDefinition {\n    fn from(tool: completion::ToolDefinition) -> Self {\n        Self {\n            r#type: \"function\".into(),\n            function: tool,\n        }\n    }\n\n```\n\n----------------------------------------\n\nTITLE: Content Processing in Rust\nDESCRIPTION: Demonstrates built-in methods for processing file contents with error handling.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/loaders.mdx#2025-04-15_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\nlet processed_files = FileLoader::with_glob(\"*.txt\")?\n    .read()                // Read contents\n    .ignore_errors()       // Skip failed reads\n    .into_iter()\n    .collect::<Vec<_>>();\n```\n\n----------------------------------------\n\nTITLE: Defining Core Extractor Structure in Rust\nDESCRIPTION: Core struct definition for the Extractor type that combines an LLM Agent with type-safe deserialization capabilities. Requires CompletionModel and JsonSchema trait implementations.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/extractors.mdx#2025-04-15_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\n/// Extractor for structured data from text\npub struct Extractor<M: CompletionModel, T: JsonSchema + for<'a> Deserialize<'a> + Send + Sync> {\n    agent: Agent<M>,\n    _t: PhantomData<T>,\n}\n```\n\n----------------------------------------\n\nTITLE: Flexible Loading Patterns in Rust\nDESCRIPTION: Shows different methods for specifying input sources using glob patterns and directory paths.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/loaders.mdx#2025-04-15_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\n// Using glob patterns\nlet glob_loader = FileLoader::with_glob(\"**/*.txt\")?\n\n// Using directory\nlet dir_loader = FileLoader::with_dir(\"data/\")?\n```\n\n----------------------------------------\n\nTITLE: Error Type Definition in Rust\nDESCRIPTION: Defines custom error types for file loading operations including glob pattern, IO, and pattern errors.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/loaders.mdx#2025-04-15_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\npub enum FileLoaderError {\n    InvalidGlobPattern(String),\n    IoError(std::io::Error),\n    PatternError(glob::PatternError),\n    GlobError(glob::GlobError),\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Tool (Adder) in Rig\nDESCRIPTION: This code snippet demonstrates how to create a custom tool in Rig. It defines an 'Adder' struct that implements the Tool trait, allowing for custom functionality within a Rig agent. The tool takes two integers as input and returns their sum.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/why_rig.mdx#2025-04-15_snippet_8\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::tool::Tool;\nuse rig::completion::ToolDefinition;\nuse serde::{Deserialize, Serialize};\nuse serde_json::json;\n\n// Define the arguments for the addition operation\n#[derive(Deserialize)]\nstruct AddArgs {\n    x: i32,\n    y: i32,\n}\n\n// Define a custom error type for math operations\n#[derive(Debug, thiserror::Error)]\n#[error(\"Math error\")]\nstruct MathError;\n\n// Define the Adder struct\n#[derive(Deserialize, Serialize)]\nstruct Adder;\n\n// Implement the Tool trait for Adder\nimpl Tool for Adder {\n    const NAME: &'static str = \"add\";\n\n    type Error = MathError;\n    type Args = AddArgs;\n    type Output = i32;\n\n    // Define the tool's interface\n    async fn definition(&self, _prompt: String) -> ToolDefinition {\n        ToolDefinition {\n            name: \"add\".to_string(),\n            description: \"Add two numbers\".to_string(),\n            parameters: json!({\n                \"type\": \"object\",\n                \"properties\": {\n                    \"x\": {\n                        \"type\": \"integer\",\n                        \"description\": \"First number to add\"\n                    },\n                    \"y\": {\n                        \"type\": \"integer\",\n                        \"description\": \"Second number to add\"\n                    }\n                },\n                \"required\": [\"x\", \"y\"]\n            }),\n        }\n    }\n\n    // Implement the addition operation\n    async fn call(&self, args: Self::Args) -> Result<Self::Output, Self::Error> {\n        Ok(args.x + args.y)\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up Discord Bot and Global Commands in Rust\nDESCRIPTION: Implements the ready event handler to store the bot's user ID and set up global slash commands. It creates \"hello\" and \"ask\" commands with appropriate descriptions and options.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/discord_bot.mdx#2025-04-15_snippet_12\n\nLANGUAGE: rust\nCODE:\n```\nasync fn ready(&self, ctx: Context, ready: Ready) {\n    info!(\"{} is connected!\", ready.user.name);\n\n    {\n        let mut data = ctx.data.write().await;\n        data.insert::<BotUserId>(ready.user.id);\n    }\n\n    let commands = Command::set_global_application_commands(&ctx.http, |commands| {\n        commands\n            .create_application_command(|command| {\n                command\n                    .name(\"hello\")\n                    .description(\"Say hello to the bot\")\n            })\n            .create_application_command(|command| {\n                command\n                    .name(\"ask\")\n                    .description(\"Ask the bot a question\")\n                    .create_option(|option| {\n                        option\n                            .name(\"query\")\n                            .description(\"Your question for the bot\")\n                            .kind(CommandOptionType::String)\n                            .required(true)\n                    })\n            })\n    })\n    .await;\n\n    println!(\"Created the following global commands: {:#?}\", commands);\n}\n```\n\n----------------------------------------\n\nTITLE: Vector Store Integration List\nDESCRIPTION: List of available vector store companion crates including MongoDB, LanceDB, Neo4j, and Qdrant integrations.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations.mdx#2025-04-15_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\nVector stores are available as separate companion-crates:\n- MongoDB vector store: [`rig-mongodb`](https://github.com/0xPlaygrounds/rig/tree/main/rig-mongodb)\n- LanceDB vector store: [`rig-lancedb`](https://github.com/0xPlaygrounds/rig/tree/main/rig-lancedb)\n- Neo4j vector store: [`rig-neo4j`](https://github.com/0xPlaygrounds/rig/tree/main/rig-neo4j)\n- Qdrant vector store: [`rig-qdrant`](https://github.com/0xPlaygrounds/rig/tree/main/rig-qdrant)\n```\n\n----------------------------------------\n\nTITLE: Submit Tool Implementation for Data Extraction\nDESCRIPTION: Implementation of the Submit Tool trait that handles the actual data submission and validation process within the extraction system.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/extractors.mdx#2025-04-15_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\nimpl<T: JsonSchema + for<'a> Deserialize<'a> + Serialize + Send + Sync> Tool for SubmitTool<T> {\n    const NAME: &'static str = \"submit\";\n    type Error = SubmitError;\n    type Args = T;\n    type Output = T;\n\n    async fn definition(&self, _prompt: String) -> ToolDefinition {\n        ToolDefinition {\n            name: Self::NAME.to_string(),\n            description: \"Submit the structured data you extracted from the provided text.\"\n                .to_string(),\n            parameters: json!(schema_for!(T)),\n        }\n    }\n\n    async fn call(&self, data: Self::Args) -> Result<Self::Output, Self::Error> {\n        Ok(data)\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Rig Agent New Method in Rust\nDESCRIPTION: Initializes a new Rig agent with OpenAI integration, vector store setup, and document embedding functionality. Sets up the RAG agent with GPT-4 model and custom preamble for Discord interactions.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/discord_bot.mdx#2025-04-15_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\nimpl RigAgent {\n    pub async fn new() -> Result<Self> {\n        // Initialize OpenAI client\n        let openai_client = openai::Client::from_env();\n        let embedding_model = openai_client.embedding_model(\"text-embedding-3-small\");\n\n        // Create vector store\n        let mut vector_store = InMemoryVectorStore::default();\n\n        // Get the current directory and construct paths to markdown files\n        let current_dir = std::env::current_dir()?;\n        let documents_dir = current_dir.join(\"documents\");\n\n        let md1_path = documents_dir.join(\"Rig_guide.md\");\n        let md2_path = documents_dir.join(\"Rig_faq.md\");\n        let md3_path = documents_dir.join(\"Rig_examples.md\");\n\n        // Load markdown documents\n        let md1_content = Self::load_md_content(&md1_path)?;\n        let md2_content = Self::load_md_content(&md2_path)?;\n        let md3_content = Self::load_md_content(&md3_path)?;\n\n        // Create embeddings and add to vector store\n        let embeddings = EmbeddingsBuilder::new(embedding_model.clone())\n            .simple_document(\"Rig_guide\", &md1_content)\n            .simple_document(\"Rig_faq\", &md2_content)\n            .simple_document(\"Rig_examples\", &md3_content)\n            .build()\n            .await?;\n\n        vector_store.add_documents(embeddings).await?;\n\n        // Create index\n        let context_index = vector_store.index(embedding_model);\n\n        // Create RAG agent\n        let rag_agent = Arc::new(openai_client.context_rag_agent(\"gpt-4\")\n            .preamble(\"You are an advanced AI assistant powered by [Rig](https://rig.rs/), a Rust library for building LLM applications. Your primary function is to provide accurate, helpful, and context-aware responses by leveraging both your general knowledge and specific information retrieved from a curated knowledge base.\n\n                    Key responsibilities and behaviors:\n                    1. Information Retrieval: You have access to a vast knowledge base. When answering questions, always consider the context provided by the retrieved information.\n                    2. Clarity and Conciseness: Provide clear and concise answers. Ensure responses are short and to the point. Use bullet points or numbered lists for complex information when appropriate.\n                    3. Technical Proficiency: You have deep knowledge about Rig and its capabilities. When discussing Rig or answering related questions, provide detailed and technically accurate information.\n                    4. Code Examples: When appropriate, provide Rust code examples to illustrate concepts, especially when discussing Rig's functionalities. Always format code examples for proper rendering in Discord by wrapping them in triple backticks and specifying the language as 'rust'. For example:\n                        \\`\\`\\`rust\n                        let example_code = \\\"This is how you format Rust code for Discord\\\";\n                        println!(\\\"{}\\\", example_code);\n                        \\`\\`\\`\n                    \")\n            .dynamic_context(2, context_index)\n            .build());\n\n        Ok(Self { rag_agent })\n    }\n\n    // ... we'll add more code here as we build things out\n}\n```\n\n----------------------------------------\n\nTITLE: Error Handling Example for Vector Store Operations in Rust\nDESCRIPTION: Example showing how to handle various error types that can occur during vector store operations, including embedding errors, JSON serialization errors, and missing document errors.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/in_memory.mdx#2025-04-15_snippet_8\n\nLANGUAGE: rust\nCODE:\n```\nmatch store.get_document::<MyDoc>(\"doc1\") {\n    Ok(Some(doc)) => println!(\"Found document: {:?}\", doc),\n    Ok(None) => println!(\"Document not found\"),\n    Err(VectorStoreError::JsonError(e)) => println!(\"Failed to deserialize: {}\", e),\n    Err(e) => println!(\"Other error: {}\", e),\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Supported Model Providers in Rust\nDESCRIPTION: Core module documentation listing the natively supported model providers including OpenAI, Cohere, Anthropic, Perplexity, Gemini, and xAI.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations.mdx#2025-04-15_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\n//! Rig natively supports the following completion and embedding model provider integrations:\n//! - OpenAI\n//! - Cohere\n//! - Anthropic\n//! - Perplexity\n//! - Gemini\n//! - xAI\n```\n\n----------------------------------------\n\nTITLE: Creating In-Memory Vector Store with Custom IDs in Rust\nDESCRIPTION: Example of creating an InMemoryVectorStore with explicitly specified custom document IDs, where each entry consists of an ID string, document object, and embedding.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/in_memory.mdx#2025-04-15_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\nlet store = InMemoryVectorStore::from_documents_with_ids(vec![\n    (\"custom_id_1\", doc1, embedding1),\n    (\"custom_id_2\", doc2, embedding2)\n]);\n```\n\n----------------------------------------\n\nTITLE: Connecting to LanceDB with Lambda Ephemeral Storage\nDESCRIPTION: Code to create a LanceDB connection using Lambda's ephemeral storage in the /tmp directory. This storage is temporary and will be wiped on cold starts.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/deploy/Blog_2_aws_lambda_lancedb.mdx#2025-04-15_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nlet db = lancedb::connect(\"/tmp\").execute().await?;\n```\n\n----------------------------------------\n\nTITLE: Defining Handler Struct with RigAgent in Rust\nDESCRIPTION: Creates a Handler struct that holds an Arc<RigAgent> to allow sharing the Rig agent across multiple event handlers without transferring ownership.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/discord_bot.mdx#2025-04-15_snippet_8\n\nLANGUAGE: rust\nCODE:\n```\nstruct Handler {\n    rig_agent: Arc<RigAgent>,\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Modules for Rig Agent in Rust\nDESCRIPTION: Rust code importing necessary modules for implementing the Rig agent, including error handling, OpenAI providers, vector storage, and embedding generation.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/discord_bot.mdx#2025-04-15_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nuse anyhow::{Context, Result};\nuse rig::providers::openai;\nuse rig::vector_store::in_memory_store::InMemoryVectorStore;\nuse rig::embeddings::EmbeddingsBuilder;\nuse rig::rag::RagAgent;\nuse std::path::Path;\nuse std::fs;\nuse std::sync::Arc;\n```\n\n----------------------------------------\n\nTITLE: Anthropic Response Content Types Definition in Rust\nDESCRIPTION: Defines the different content types that can be returned in Anthropic responses, including plain text and tool usage outputs, using an enum with serde support.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/model_providers/anthropic.mdx#2025-04-15_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\n}\n\n#[derive(Debug, Deserialize, Serialize)]\n#[serde(untagged)]\npub enum Content {\n    String(String),\n    Text {\n        r#type: String,\n        text: String,\n    },\n    ToolUse {\n        r#type: String,\n        id: String,\n        name: String,\n        input: serde_json::Value,\n    },\n```\n\n----------------------------------------\n\nTITLE: Basic File Loading Example\nDESCRIPTION: Simple example of loading and processing text files using FileLoader.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/loaders.mdx#2025-04-15_snippet_7\n\nLANGUAGE: rust\nCODE:\n```\nlet loader = FileLoader::with_glob(\"data/*.txt\")?\nfor content in loader.read().ignore_errors() {\n    // Process content\n}\n```\n\n----------------------------------------\n\nTITLE: Contextual Chat Example in Rust\nDESCRIPTION: Shows how to use the chat API for conversational interactions. It sends a new prompt while maintaining conversation context through a history of previous messages.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/completion.mdx#2025-04-15_snippet_7\n\nLANGUAGE: rust\nCODE:\n```\nlet chat_response = model\n    .chat(\n        \"Continue the discussion\",\n        vec![Message::user(\"Previous context\")]\n    )\n    .await?;\n```\n\n----------------------------------------\n\nTITLE: Adding Rig Dependencies to Cargo.toml\nDESCRIPTION: This snippet shows how to include Rig in a Rust project by adding the necessary dependencies to the Cargo.toml file. It includes the rig-core library and tokio with full features for async runtime support.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/quickstart.mdx#2025-04-15_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[dependencies]\nrig-core = \"0.0.6\"\ntokio = { version = \"1.34.0\", features = [\"full\"] }\n```\n\n----------------------------------------\n\nTITLE: Extraction Error Handling Implementation\nDESCRIPTION: Definition of the ExtractionError enum that handles various error cases including missing data, deserialization failures, and prompt errors.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/extractors.mdx#2025-04-15_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\n#[derive(Debug, thiserror::Error)]\npub enum ExtractionError {\n    #[error(\"No data extracted\")]\n    NoData,\n\n    #[error(\"Failed to deserialize the extracted data: {0}\")]\n    DeserializationError(#[from] serde_json::Error),\n\n    #[error(\"PromptError: {0}\")]\n    PromptError(#[from] PromptError),\n}\n```\n\n----------------------------------------\n\nTITLE: File Loader Integration Example\nDESCRIPTION: Shows how to integrate the extractor with a file loading system to process multiple documents from files.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/extractors.mdx#2025-04-15_snippet_8\n\nLANGUAGE: rust\nCODE:\n```\nlet docs = FileLoader::with_glob(\"*.txt\")?\n    .read()\n    .ignore_errors();\n\nlet extractor = client.extractor::<DocumentData>(model).build();\n\nfor doc in docs {\n    let structured_data = extractor.extract(&doc).await?;\n    // Process structured data\n}\n```\n\n----------------------------------------\n\nTITLE: Anthropic Tool Definition Struct in Rust\nDESCRIPTION: Defines the ToolDefinition struct for representing function calling capabilities with Anthropic models. Tools need a name and optional description for the model to understand their purpose.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/model_providers/anthropic.mdx#2025-04-15_snippet_7\n\nLANGUAGE: rust\nCODE:\n```\n}\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct ToolDefinition {\n    pub name: String,\n    pub description: Option<String>,\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies to Cargo.toml for Discord Bot Project\nDESCRIPTION: TOML configuration adding necessary dependencies for the Discord bot project, including Rig, Serenity, Tokio, and other utility crates.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/discord_bot.mdx#2025-04-15_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[dependencies]\nrig-core = \"0.2.1\" # [Rig Crate](https://crates.io/crates/rig-core)\ntokio = { version = \"1.34.0\", features = [\"full\"] }\nserenity = { version = \"0.11\", default-features = false, features = [\"client\", \"gateway\", \"rustls_backend\", \"cache\", \"model\", \"http\"] }\ndotenv = \"0.15.0\"\nanyhow = \"1.0.75\"\ntracing = \"0.1\"\ntracing-subscriber = \"0.3\"\nreqwest = { version = \"0.11\", features = [\"json\"] }\nserde = { version = \"1.0\", features = [\"derive\"] }\nserde_json = \"1.0\"\nschemars = \"0.8\"\nasync-trait = \"0.1.83\"\n```\n\n----------------------------------------\n\nTITLE: Creating Embeddings with Gemini API in Rust\nDESCRIPTION: This snippet shows how to use the Gemini API to create embeddings for text documents. It defines a custom struct with an embeddable field, initializes a Gemini client, and generates embeddings for multiple documents.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/examples/model_providers/gemini.mdx#2025-04-15_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nuse rig::providers::gemini;\nuse rig::Embed;\n\n#[derive(Embed, Debug)]\nstruct Greetings {\n    #[embed]\n    message: String,\n}\n\n#[tokio::main]\nasync fn main() -> Result<(), anyhow::Error> {\n    // Initialize the Google Gemini client\n    // Create OpenAI client\n    let client = gemini::Client::from_env();\n\n    let embeddings = client\n        .embeddings(gemini::embedding::EMBEDDING_001)\n        .document(Greetings {\n            message: \"Hello, world!\".to_string(),\n        })?\n        .document(Greetings {\n            message: \"Goodbye, world!\".to_string(),\n        })?\n        .build()\n        .await\n        .expect(\"Failed to embed documents\");\n\n    println!(\"{:?}\", embeddings);\n\n    Ok()\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing EventHandler Trait Structure in Rust\nDESCRIPTION: Sets up the structure for implementing the EventHandler trait for the Handler struct to handle various Discord events including interactions, messages, and ready events.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/discord_bot.mdx#2025-04-15_snippet_9\n\nLANGUAGE: rust\nCODE:\n```\n#[async_trait]\nimpl EventHandler for Handler {\n    async fn interaction_create(&self, ctx: Context, interaction: Interaction) {\n        // ... handle interactions\n    }\n\n    async fn message(&self, ctx: Context, msg: Message) {\n        // ... handle messages\n    }\n\n    async fn ready(&self, ctx: Context, ready: Ready) {\n        // ... handle readiness\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Max Tokens Validation for Anthropic Requests in Rust\nDESCRIPTION: Shows error handling for Anthropic's requirement that max_tokens must be explicitly set in completion requests, unlike some other providers that may have defaults.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/model_providers/anthropic.mdx#2025-04-15_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\n        let prompt_with_context = completion_request.prompt_with_context();\n\n        // Check if max_tokens is set, required for Anthropic\n        if completion_request.max_tokens.is_none() {\n            return Err(CompletionError::RequestError(\n                \"max_tokens must be set for Anthropic\".into(),\n```\n\n----------------------------------------\n\nTITLE: Visualizing Pipeline Structure as a DAG with Mermaid\nDESCRIPTION: Represents a pipeline as a Directed Acyclic Graph (DAG) using Mermaid diagram notation, showing data flow from input through operations to output.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/chains.mdx#2025-04-15_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph TD\n    A[Input] --> B[Operation 1]\n    B --> C[Operation 2]\n    C --> D[Operation 3]\n    D --> E[Output]\n```\n\n----------------------------------------\n\nTITLE: Agent Context Loading in Rust\nDESCRIPTION: Shows integration with Rig's agent system for loading multiple context documents.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/loaders.mdx#2025-04-15_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\n    // Load in all the rust examples\n    let examples = FileLoader::with_glob(\"rig-core/examples/*.rs\")?\n        .read_with_path()\n        .ignore_errors()\n        .into_iter();\n\n    // Create an agent with multiple context documents\n    let agent = examples\n        .fold(AgentBuilder::new(model), |builder, (path, content)| {\n            builder.context(format!(\"Rust Example {:?}:\\n{}\", path, content).as_str())\n        })\n        .build();\n```\n\n----------------------------------------\n\nTITLE: RigAgent Testing Implementation\nDESCRIPTION: Standalone test implementation for RigAgent functionality before Discord integration\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/discord_bot.mdx#2025-04-15_snippet_18\n\nLANGUAGE: rust\nCODE:\n```\n#[tokio::main]\nasync fn main() -> Result<()> {\n    dotenv().ok();\n\n    let rig_agent = RigAgent::new().await?;\n    let response = rig_agent.process_message(\"What is Rig?\").await?;\n    println!(\"Response: {}\", response);\n\n    Ok(())\n}\n```\n\n----------------------------------------\n\nTITLE: Combined Text Analysis Data Structure in Rust\nDESCRIPTION: Defines a combined data structure for both sentiment analysis and entity recognition results.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/text_extraction_classification.mdx#2025-04-15_snippet_9\n\nLANGUAGE: rust\nCODE:\n```\n#[derive(Debug, Deserialize, Serialize)]\nstruct TextAnalysis {\n    sentiment: SentimentClassification,\n    entities: Vec<Entity>,\n}\n```\n\n----------------------------------------\n\nTITLE: Building and Deploying Lambda Functions with cargo-lambda\nDESCRIPTION: Shell commands for building and deploying Lambda functions using the cargo-lambda tool. Shows commands for both the loader function (which writes to LanceDB) and the app function (which reads from LanceDB).\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/deploy/Blog_2_aws_lambda_lancedb.mdx#2025-04-15_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Lambda that writes to the store\ncargo lambda build --release --bin loader\ncargo lambda deploy --binary-name loader <your_loader_function_name>\n\n# Lambda that reads to the store\ncargo lambda build --release --bin app\ncargo lambda deploy --binary-name app <your_app_function_name>\n```\n\n----------------------------------------\n\nTITLE: Handling Discord Messages and Bot Mentions in Rust\nDESCRIPTION: Implements the message event handler to respond when the bot is mentioned. It extracts the actual query by removing the mention, processes it with the RigAgent, and sends the response back to the channel.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/discord_bot.mdx#2025-04-15_snippet_11\n\nLANGUAGE: rust\nCODE:\n```\nasync fn message(&self, ctx: Context, msg: Message) {\n    if msg.mentions_me(&ctx.http).await.unwrap_or(false) {\n        debug!(\"Bot mentioned in message: {}\", msg.content);\n\n        let bot_id = {\n            let data = ctx.data.read().await;\n            data.get::<BotUserId>().copied()\n        };\n\n        if let Some(bot_id) = bot_id {\n            let mention = format!(\"<@{}>\", bot_id);\n            let content = msg.content.replace(&mention, \"\").trim().to_string();\n\n            debug!(\"Processed content after removing mention: {}\", content);\n\n            match self.rig_agent.process_message(&content).await {\n                Ok(response) => {\n                    if let Err(why) = msg.channel_id.say(&ctx.http, response).await {\n                        error!(\"Error sending message: {:?}\", why);\n                    }\n                }\n                Err(e) => {\n                    error!(\"Error processing message: {:?}\", e);\n                    if let Err(why) = msg\n                        .channel_id\n                        .say(&ctx.http, format!(\"Error processing message: {:?}\", e))\n                        .await\n                    {\n                        error!(\"Error sending error message: {:?}\", why);\n                    }\n                }\n            }\n        } else {\n            error!(\"Bot user ID not found in TypeMap\");\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Deploying Rig Application to AWS Lambda using Cargo Lambda CLI\nDESCRIPTION: Commands to build and deploy a Rig application to AWS Lambda using the cargo lambda CLI. Includes setting the function name, building the release version, and deploying to AWS.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/deploy/Blog_1_aws_lambda.mdx#2025-04-15_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nfunction_name='rig-entertainer'\n\ncd rig-entertainer-lambda\ncargo lambda build --release # Can define different architectures here with --arm64 for example\ncargo lambda deploy $function_name # Since the name of the crate is the same as the the lambda function name, no need to specify a binary file\n```\n\n----------------------------------------\n\nTITLE: Setting Default Date for Flight Search in Rust\nDESCRIPTION: Provides a default date (30 days from current date) if the user doesn't specify one, using Rust's date handling capabilities.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/flight_assistant.mdx#2025-04-15_snippet_7\n\nLANGUAGE: rust\nCODE:\n```\nlet date = args.date.unwrap_or_else(|| {\n    let date = Utc::now() + Duration::days(30);\n    date.format(\"%Y-%m-%d\").to_string()\n});\n```\n\n----------------------------------------\n\nTITLE: AWS Lambda Deployment Commands\nDESCRIPTION: Series of commands for building Rust binaries, creating deployment packages, and uploading to S3 bucket for Lambda deployment\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/deploy/Blog_2_aws_lambda_lancedb.mdx#2025-04-15_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Can also do this directly on the AWS console\naws s3api create-bucket --bucket <your_bucket_name>\n\ncargo lambda build --release --bin loader\ncargo lambda build --release --bin app\n\ncd target/lambda/loader\nzip -r bootstrap.zip bootstrap\n# Can also do this directly on the AWS console\naws s3 cp bootstrap.zip s3://<your_bucket_name>/rig/loader/\n\ncd ..\nzip -r bootstrap.zip bootstrap\n# Can also do this directly on the AWS console\naws s3 cp bootstrap.zip s3://<your_bucket_name>/rig/app/\n```\n\n----------------------------------------\n\nTITLE: Defining MongoDB Vector Index Structure in Rust\nDESCRIPTION: This code defines the core MongoDbVectorIndex struct used for interacting with MongoDB's vector search capabilities. It encapsulates the MongoDB collection, embedding model, index name, and search parameters.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/mongodb.mdx#2025-04-15_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\npub struct MongoDbVectorIndex {\n    collection: Collection<Document>,\n    model: Box<dyn EmbeddingModel>,\n    index_name: String,\n    search_params: SearchParams,\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Entity Recognition Data Structures in Rust\nDESCRIPTION: Defines the core data structures for named entity recognition including EntityType enum and Entity struct.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/text_extraction_classification.mdx#2025-04-15_snippet_7\n\nLANGUAGE: rust\nCODE:\n```\n#[derive(Debug, Deserialize, Serialize)]\nenum EntityType {\n    Person,\n    Organization,\n    Location,\n}\n\n#[derive(Debug, Deserialize, Serialize)]\nstruct Entity {\n    text: String,\n    entity_type: EntityType,\n    start: usize,\n    end: usize,\n}\n\n#[derive(Debug, Deserialize, Serialize)]\nstruct ExtractedEntities {\n    entities: Vec<Entity>,\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing TryOp Trait for Error Handling in Batch Operations\nDESCRIPTION: Code snippet showing the implementation of try_batch_call method from the TryOp trait, used for operations that may fail when processing multiple inputs concurrently.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/chains.mdx#2025-04-15_snippet_6\n\nLANGUAGE: rust\nCODE:\n```\n    fn try_batch_call<I>(\n        &self,\n        n: usize,\n        input: I,\n    ) -> impl Future<Output = Result<Vec<Self::Output>, Self::Error>> + Send\n    where\n        I: IntoIterator<Item = Self::Input> + Send,\n        I::IntoIter: Send,\n        Self: Sized,\n    {\n        use stream::{StreamExt, TryStreamExt};\n\n        async move {\n            stream::iter(input)\n            // ... more code here\n```\n\n----------------------------------------\n\nTITLE: Configuring Dependencies in Cargo.toml\nDESCRIPTION: Configuration of the Cargo.toml file with necessary dependencies for a text classification project, including rig-core, tokio, anyhow, and serde.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/text_extraction_classification.mdx#2025-04-15_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[package]\nname = \"text_classifier_extractor\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n[dependencies]\nrig-core = \"0.0.6\"\ntokio = { version = \"1.34.0\", features = [\"full\"] }\nanyhow = \"1.0.75\"\nserde = { version = \"1.0\", features = [\"derive\"] }\nserde_json = \"1.0\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up Rust Project for RAG System\nDESCRIPTION: Creates a new Rust project and adds necessary dependencies to the Cargo.toml file for building a RAG system.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/rag/rag_system.mdx#2025-04-15_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncargo new rag_system\ncd rag_system\n```\n\nLANGUAGE: toml\nCODE:\n```\n[package]\nname = \"rag_system\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n[dependencies]\nrig-core = \"0.0.6\"\ntokio = { version = \"1.34.0\", features = [\"full\"] }\nanyhow = \"1.0.75\"\npdf-extract = \"0.7.3\"\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=your_api_key_here\n```\n\n----------------------------------------\n\nTITLE: Directory Processing Example\nDESCRIPTION: Example of processing files in a directory with path context.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/loaders.mdx#2025-04-15_snippet_9\n\nLANGUAGE: rust\nCODE:\n```\nlet dir_loader = FileLoader::with_dir(\"data/\")?\n    .read_with_path()\n    .ignore_errors();\n\nfor (path, content) in dir_loader {\n    // Process files with path context\n}\n```\n\n----------------------------------------\n\nTITLE: Anthropic API Version Constants in Rust\nDESCRIPTION: Defines constants for Anthropic API versions that can be used when configuring the client. These constants ensure compatibility with different API iterations.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/model_providers/anthropic.mdx#2025-04-15_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\npub const ANTHROPIC_VERSION_2023_01_01: &str = \"2023-01-01\";\npub const ANTHROPIC_VERSION_2023_06_01: &str = \"2023-06-01\";\npub const ANTHROPIC_VERSION_LATEST: &str = ANTHROPIC_VERSION_2023_06_01;\n```\n\n----------------------------------------\n\nTITLE: Error Handling Implementation with Anyhow\nDESCRIPTION: Example of error handling implementation using the anyhow crate for improved error context and propagation\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/discord_bot.mdx#2025-04-15_snippet_16\n\nLANGUAGE: rust\nCODE:\n```\nuse anyhow::{Context, Result};\n\nfn load_md_content<P: AsRef<Path>>(file_path: P) -> Result<String> {\n    fs::read_to_string(file_path.as_ref())\n        .with_context(|| format!(\"Failed to read markdown file: {:?}\", file_path.as_ref()))\n}\n```\n\n----------------------------------------\n\nTITLE: Running the RAG System in Rust\nDESCRIPTION: This command runs the RAG system implemented in Rust. It assumes that the necessary PDF files are present in the 'documents' folder in the project root.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/rag/rag_system.mdx#2025-04-15_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncargo run\n```\n\n----------------------------------------\n\nTITLE: Adding Rig-Qdrant Dependency in Cargo.toml\nDESCRIPTION: Specifies the required dependency for adding rig-qdrant to your Rust project's Cargo.toml file.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/qdrant.mdx#2025-04-15_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[dependencies]\nrig-qdrant = \"0.1.5\"\n```\n\n----------------------------------------\n\nTITLE: Loading Markdown Content Helper Function in Rust\nDESCRIPTION: Helper function that reads the content of a Markdown file from a specified file path. Takes a generic parameter implementing AsRef<Path> for flexible path handling.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/discord_bot.mdx#2025-04-15_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\nfn load_md_content<P: AsRef<Path>>(file_path: P) -> Result<String> {\n    fs::read_to_string(file_path.as_ref())\n        .with_context(|| format!(\"Failed to read markdown file: {:?}\", file_path.as_ref()))\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Environment Variables in Rust\nDESCRIPTION: Code snippet showing how to load environment variables from a .env file using the dotenv crate in a Rust application.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/text_extraction_classification.mdx#2025-04-15_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\ndotenv::dotenv().ok();\n```\n\n----------------------------------------\n\nTITLE: Sample Output of Concurrent LLM Requests with Rig\nDESCRIPTION: This snippet shows an example of the output generated by the concurrent LLM request handling code. It displays multiple generated facts printed concurrently, followed by the total time taken for processing.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/why_rig.mdx#2025-04-15_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\nResult: The number 0 is the only number that is neither positive nor negative.\nResult: The number 1 is the only number that is neither prime nor composite.\nResult: The number 2 is the only even prime number.\nResult: The number 3 is the only prime number that is one less than a perfect square.\n...\n\nTime elapsed: 1.502160549s\n```\n\n----------------------------------------\n\nTITLE: Installing Rig Core Package in Rust\nDESCRIPTION: Command to add the rig-core package to a Rust project using cargo.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/index.mdx#2025-04-15_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncargo add rig-core\n```\n\n----------------------------------------\n\nTITLE: Visualizing Rig Data Flow Architecture\nDESCRIPTION: A plaintext flowchart showing the complete data flow in Rig's system architecture. The diagram illustrates the progression from user input through various processing stages including configuration, LLM interaction, and post-processing to final output presentation.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/text_extraction_classification.mdx#2025-04-15_snippet_13\n\nLANGUAGE: plaintext\nCODE:\n```\n                        [User Input]\n                            |\n                            v\n                         [Rig Extractor]\n                             |\n                   +-------------------+\n                   |   Configurations  |\n                   | - Model Selection |\n                   | - Preamble Setup  |\n                   | - LLM Prompt      |\n                   +-------------------+\n                             |\n                             v\n                [Secure API Call to LLM Provider]\n                             |\n                             v\n                      [Large Language Model]\n                             |\n                             v\n                      [LLM Response]\n                             |\n                             v\n                   [Rig Postprocessing]\n                   +-------------------+\n                   | - Parse Output    |\n                   | - Deserialize     |\n                   | - Validate        |\n                   +-------------------+\n                             |\n                             v\n                      [Structured Output]\n                             |\n                             v\n                   [Result Presentation]\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment Variables\nDESCRIPTION: Environment configuration file for storing API keys securely\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/flight_assistant.mdx#2025-04-15_snippet_2\n\nLANGUAGE: dotenv\nCODE:\n```\nOPENAI_API_KEY=your_openai_api_key_here\nRAPIDAPI_KEY=your_rapidapi_key_here\n```\n\n----------------------------------------\n\nTITLE: Vector Store Integration Request Template\nDESCRIPTION: Template for requesting new vector store integrations, including sections for description and required resources.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations.mdx#2025-04-15_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n## Vector Store Integration Request\n<!--\nDescribe the vector store and the features it provides (e.g.: is it cloud only? a plugin to an existing database? document-based or relational? etc.)\n-->\n\n### Resources\n<!--\nLinks to API docs, SDKs or any other information that would help in the integration of the new vector store.\n-->\n```\n\n----------------------------------------\n\nTITLE: Vector Store Search Result Output\nDESCRIPTION: Shows the expected output when querying the vector store for documents related to Rust, displaying the most semantically relevant document to the query.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/why_rig.mdx#2025-04-15_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nMost relevant document: \"Rust is a systems programming language.\"\n```\n\n----------------------------------------\n\nTITLE: Adding rig-neo4j Dependency in Cargo.toml\nDESCRIPTION: Configuration for adding the rig-neo4j crate to your Rust project's dependencies in the Cargo.toml file. This dependency provides Neo4j vector store implementation functionality.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/neo4j.mdx#2025-04-15_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[dependencies]\nrig-neo4j = \"0.2.0\"\n```\n\n----------------------------------------\n\nTITLE: Environment Variables Configuration\nDESCRIPTION: Environment variables setup for Discord bot token and OpenAI API key in .env file format\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/discord_bot.mdx#2025-04-15_snippet_14\n\nLANGUAGE: plaintext\nCODE:\n```\nDISCORD_TOKEN=your_discord_bot_token\nOPENAI_API_KEY=your_openai_api_key\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable\nDESCRIPTION: Command to set the OpenAI API key as an environment variable, which is required for authenticating with the OpenAI service when using Rig.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/examples/advanced/concurrent_processing.mdx#2025-04-15_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=your_api_key_here\n```\n\n----------------------------------------\n\nTITLE: Visualizing In-Memory Store Structure Using Plaintext\nDESCRIPTION: Example of the memory layout for the in-memory vector store, showing how documents with single and multiple embeddings are stored in the HashMap structure.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/in_memory.mdx#2025-04-15_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n{\n    \"doc1\" => (\n        Document { title: \"Example 1\", ... },\n        One(Embedding { vec: [0.1, 0.2, ...] })\n    ),\n    \"doc2\" => (\n        Document { title: \"Example 2\", ... },\n        Many([\n            Embedding { vec: [0.3, 0.4, ...] },\n            Embedding { vec: [0.5, 0.6, ...] }\n        ])\n    )\n}\n```\n\n----------------------------------------\n\nTITLE: Rendering Navigation Cards for Vector Stores with Nextra Components\nDESCRIPTION: This code snippet uses Nextra's Cards component to create a navigation interface. It displays clickable cards for five different vector store types supported by Rig, each linking to its respective documentation page.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores.mdx#2025-04-15_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\n<Cards>\n<Cards.Card title=\"In-Memory Vector Store\" href=\"./41_vector_stores/in_memory\" />\n<Cards.Card title=\"MongoDB Vector Store\" href=\"./41_vector_stores/mongodb\" />\n<Cards.Card title=\"LanceDB Vector Store\" href=\"./41_vector_stores/lancedb\" />\n<Cards.Card title=\"Neo4j Vector Store\" href=\"./41_vector_stores/neo4j\" />\n<Cards.Card title=\"Qdrant Vector Store\" href=\"./41_vector_stores/qdrant\" />\n</Cards>\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Bash\nDESCRIPTION: Command to set the OpenAI API key as an environment variable, which is required for the application to make API calls to OpenAI's services.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/examples/rag/rag_pdf.mdx#2025-04-15_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=your_api_key_here\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Rig Documentation\nDESCRIPTION: Command to install project dependencies using pnpm package manager. This must be run before starting the development server.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/README.md#2025-04-15_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npnpm i\n```\n\n----------------------------------------\n\nTITLE: PDF Processing Example\nDESCRIPTION: Example of processing PDF documents with page-by-page iteration.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts/loaders.mdx#2025-04-15_snippet_8\n\nLANGUAGE: rust\nCODE:\n```\nlet pdf_loader = PdfFileLoader::with_glob(\"docs/*.pdf\")?\nlet pages = pdf_loader\n    .load_with_path()\n    .ignore_errors()\n    .by_page()\n    .into_iter();\n```\n\n----------------------------------------\n\nTITLE: Building and Running the Application in Bash\nDESCRIPTION: Command to build and run the PDF RAG application using Cargo, Rust's package manager and build system.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/examples/rag/rag_pdf.mdx#2025-04-15_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncargo run\n```\n\n----------------------------------------\n\nTITLE: Building and Running the Flight Search Application in Bash\nDESCRIPTION: Commands for building and running the Rust application using Cargo, the Rust package manager and build system.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/flight_assistant.mdx#2025-04-15_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ncargo build\n```\n\nLANGUAGE: bash\nCODE:\n```\ncargo run\n```\n\n----------------------------------------\n\nTITLE: Rendering Book Icon with Section Title in JSX\nDESCRIPTION: This code shows how to render the Book icon component inline with the Documentation section title using the section_icon style.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/how_to_contribute.mdx#2025-04-15_snippet_2\n\nLANGUAGE: jsx\nCODE:\n```\n<><Book style={section_icon}/> Documentation </>\n```\n\n----------------------------------------\n\nTITLE: Discord Integration Setup in Rust\nDESCRIPTION: Sets up the Discord bot integration using the Serenity library, including necessary imports and bot user ID storage implementation.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/discord_bot.mdx#2025-04-15_snippet_7\n\nLANGUAGE: rust\nCODE:\n```\n// main.rs\n\nmod rig_agent;\n\nuse anyhow::Result;\nuse serenity::async_trait;\nuse serenity::model::application::command::Command;\nuse serenity::model::application::interaction::{Interaction, InteractionResponseType};\nuse serenity::model::gateway::Ready;\nuse serenity::model::channel::Message;\nuse serenity::prelude::*;\nuse serenity::model::application::command::CommandOptionType;\nuse std::env;\nuse std::sync::Arc;\nuse tracing::{error, info, debug};\nuse rig_agent::RigAgent;\nuse dotenv::dotenv;\n\nstruct BotUserId;\n\nimpl TypeMapKey for BotUserId {\n    type Value = serenity::model::id::UserId;\n}\n```\n\n----------------------------------------\n\nTITLE: Creating New Rust Project for Rig Concurrent Processing\nDESCRIPTION: Command to create a new Rust project for working with Rig's concurrent processing capabilities. This initializes a new cargo project and changes directory into it.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/examples/advanced/concurrent_processing.mdx#2025-04-15_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncargo new rig-concurrent-processing\ncd rig-concurrent-processing\n```\n\n----------------------------------------\n\nTITLE: Initializing New Rust Project for Discord Bot\nDESCRIPTION: Commands to create a new Rust project named 'discord_rig_bot' and navigate into the project directory.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/discord_bot.mdx#2025-04-15_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncargo new discord_rig_bot\ncd discord_rig_bot\n```\n\n----------------------------------------\n\nTITLE: Cloning the Repository in Bash\nDESCRIPTION: Commands to clone the repository and navigate to the project directory. This is the first step in setting up the PDF RAG system locally.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/examples/rag/rag_pdf.mdx#2025-04-15_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone this repo\ncd pdf-rag-system\n```\n\n----------------------------------------\n\nTITLE: Connecting to Qdrant Server in Rust\nDESCRIPTION: Creates a connection to a Qdrant server running on localhost port 6334.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/qdrant.mdx#2025-04-15_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nlet qdrant_client = Qdrant::connect(\"http://localhost:6334\").await?;\n```\n\n----------------------------------------\n\nTITLE: Running Discord Bot with Cargo\nDESCRIPTION: Command to run the Discord bot using Cargo\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/discord_bot.mdx#2025-04-15_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ncargo run\n```\n\n----------------------------------------\n\nTITLE: Creating New Rust Project with Cargo\nDESCRIPTION: Commands to initialize a new Rust project named flight_search_assistant using Cargo\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/flight_assistant.mdx#2025-04-15_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncargo new flight_search_assistant\ncd flight_search_assistant\n```\n\n----------------------------------------\n\nTITLE: RAG System Response Output\nDESCRIPTION: Shows the expected output from the RAG system when asked about Rust's first stable release, demonstrating how the agent retrieves and uses information from the vector store.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/why_rig.mdx#2025-04-15_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\nRAG Agent Response: Rust's first stable release (1.0) was on May 15, 2015.\n```\n\n----------------------------------------\n\nTITLE: Rendering Documentation Cards with JSX\nDESCRIPTION: This code snippet uses JSX to create a set of cards for different sections of the Rig documentation. Each card includes an icon, title, link, and brief description.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/index.mdx#2025-04-15_snippet_0\n\nLANGUAGE: JSX\nCODE:\n```\n<Cards>\n    <Cards.Card\n    title=\"Why Rig?\"\n    icon={<CircleHelp />}\n    href=\"/docs/why_rig\"\n        children={    <div style={{ fontSize: '0.9em', color: '#666', whiteSpace: 'normal', wordWrap: 'break-word' }}>\n        Main features and benefits of Rig.\n        </div>\n    }\n    />\n    <Cards.Card\n    title=\"Architecture\"\n    icon={<Landmark />}\n    href=\"/docs/architecture\"\n    children={    <div style={{ fontSize: '0.9em', color: '#666', whiteSpace: 'normal', wordWrap: 'break-word' }}>\n        Rig's architecture, design principles and key abstractions.\n        </div>\n    }\n    />\n    <Cards.Card\n    title=\"Concepts\"\n    icon={<Plug />}\n    href=\"/docs/concepts\"\n    children={    <div style={{ fontSize: '0.9em', color: '#666', whiteSpace: 'normal', wordWrap: 'break-word' }}>\n        Rig's core concepts and abstractions.\n        </div>\n    }\n    />\n    <Cards.Card\n    title=\"Integrations\"\n    icon={<Plug />}\n    href=\"/docs/integrations\"\n    children={    <div style={{ fontSize: '0.9em', color: '#666', whiteSpace: 'normal', wordWrap: 'break-word' }}>\n        Integrations with LLM providers, vector databases and third party plugins (e.g Discord, Twitter).\n        </div>\n    }\n    />\n    <Cards.Card\n    title=\"Extensions\"\n    icon={<Blocks />}\n    href=\"/docs/extensions\"\n    children={    <div style={{ fontSize: '0.9em', color: '#666', whiteSpace: 'normal', wordWrap: 'break-word' }}>\n        Utility tools for Rig, such as a CLI, a GUI, etc.\n        </div>\n    }\n    />\n    <Cards.Card\n    title=\"How to Contribute?\"\n    icon={<HeartHandshake />}\n    href=\"/docs/how_to_contribute\"\n    children={    <div style={{ fontSize: '0.9em', color: '#666', whiteSpace: 'normal', wordWrap: 'break-word' }}>\n        How to contribute to Rig.\n        </div>\n    }\n    />\n</Cards>\n```\n\n----------------------------------------\n\nTITLE: Starting Local Development Server for Rig Documentation\nDESCRIPTION: Command to start the Nextra development server. After running this command, the documentation can be accessed at localhost:3000.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/README.md#2025-04-15_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npnpm dev\n```\n\n----------------------------------------\n\nTITLE: Example Interaction with RAG System\nDESCRIPTION: This plaintext snippet demonstrates a sample interaction with the RAG system. It shows how the system responds to various queries, including summarizing document content, analyzing themes, and drawing connections between different texts.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/rag/rag_system.mdx#2025-04-15_snippet_8\n\nLANGUAGE: plaintext\nCODE:\n```\nWelcome to the chatbot! Type 'exit' to quit.\n> hi \n========================== Response ============================\nHello! How can I assist you today?\n================================================================\n\n\n> give me a summary of what sam altman wrote\n========================== Response ============================\nSam Altman discusses the imminent socioeconomic changes brought about by rapidly advancing artificial intelligence (AI) technology. He explains that as AI becomes more capable of performing various tasks currently done by humans, there will be a significant shift in the distribution of wealth from labor to capital. Altman emphasizes the need for public policy to adapt to this technological future to ensure a fair distribution of wealth and enable everyone to benefit from it. He proposes a system that taxes assets such as companies and land to distribute wealth more equitably in society. Altman envisions a future where AI revolutionizes various industries, leading to an abundance of wealth that can be shared among citizens through innovative policies. He suggests that by taking advantage of technological advancements and implementing fair policies, society can improve living standards for everyone.\n================================================================\n\n\n> tell me about the story by asimov\n========================== Response ============================\nIsaac Asimov's story \"The Last Question\" explores the theme of entropy and the eventual heat death of the universe. The story spans billions of years and follows humanity's interactions with a supercomputer known as the Multivac and later the Cosmic AC. As the stars and galaxies gradually fade away, humanity faces the inevitability of the universe's decay due to the second law of thermodynamics. Despite their technological advancements and seemingly limitless energy sources, the characters in the story grapple with fundamental questions about the fate of the universe and the concept of reversing entropy. In the end, the story reveals a thought-provoking conclusion about the relationship between humanity, technology, and the ultimate destiny of the cosmos.\n================================================================\n\n\n> how does the ideas of both texts relate to eachother?\n========================== Response ============================\nBoth texts, \"Moore's Law for Everything\" by Sam Altman and \"The Last Question\" by Isaac Asimov, touch upon the theme of technological advancement and its potential implications for society and the universe.\n\nIn \"Moore's Law for Everything,\" Altman discusses the exponential growth of AI technology and its expected impact on society, including the redistribution of wealth from labor to capital. Altman highlights the need for policy adaptation to ensure a fair distribution of the wealth generated by AI advancements. He envisions a future where AI revolutionizes various industries and creates abundant wealth that can be shared among all citizens through innovative policies.\n\nOn the other hand, \"The Last Question\" delves into the concept of entropy and the gradual heat death of the universe as explored through interactions with advanced supercomputers. The story contemplates the limitations of technology and humanity's quest to reverse entropy and preserve the universe's existence.\n\nBoth texts address the transformative power of technology, raising questions about the long-term implications and ethical considerations surrounding technological progress. They explore themes of societal adaptation, resource management, and the inevitability of change in the face of advancing technology and cosmic forces.\n================================================================\n```\n\n----------------------------------------\n\nTITLE: Fetching API Key for Flight Search in Rust\nDESCRIPTION: Retrieves the RapidAPI key from environment variables to authenticate API requests, with error handling for missing API keys.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/flight_assistant.mdx#2025-04-15_snippet_6\n\nLANGUAGE: rust\nCODE:\n```\nlet api_key = env::var(\"RAPIDAPI_KEY\").map_err(|_| FlightSearchError::MissingApiKey)?;\n```\n\n----------------------------------------\n\nTITLE: Implementing Nextra Cards for Rig Examples Showcase in JSX\nDESCRIPTION: This code renders a showcase section for Rig examples using Nextra's Cards component. It imports Lucide icons and displays three card items: an arXiv Research Assistant, a PDF Summarizer, and a Sentiment Analysis tool, with links and descriptions.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/examples/index.mdx#2025-04-15_snippet_0\n\nLANGUAGE: JSX\nCODE:\n```\nimport { GraduationCap, FileText, BookHeart } from 'lucide-react'\nimport { Cards } from 'nextra/components'\n\n<div className=\"section-title\">Overview</div>\n\nAn index of our team curated Rig code examples and showcase of the best usecases from the community.\n\nFor more detailed walkthroughs and guides, please see our [Tutorials & Guides](/guides) section.\n\n### Showcase\n\n<Cards>\n    <Cards.Card\n    title={<><GraduationCap />arXiv Research Assistant</>}\n    href=\"https://rig-arxiv-agent-khjr.shuttle.app\"\n    children={    <div style={{ fontSize: '0.9em', color: '#666', whiteSpace: 'normal', wordWrap: 'break-word' }}>\n        A `shuttle.dev` deployed research assistant that uses arxiv to find and summarize research papers.\n        </div>\n    }\n    arrow\n    target=\"_blank\"\n    />\n    <Cards.Card\n    title={<><FileText />PDF Summarizer</>}\n    href=\"\"\n    children={    <div style={{ fontSize: '0.9em', color: '#666', whiteSpace: 'normal', wordWrap: 'break-word' }}>\n        Coming soon‚Ñ¢Ô∏è...\n        </div>\n    }\n    />\n    <Cards.Card\n    title={<><BookHeart />Sentiment Analysis/Extractor</>}\n    href=\"\"\n    children={    <div style={{ fontSize: '0.9em', color: '#666', whiteSpace: 'normal', wordWrap: 'break-word' }}>\n        Coming soon‚Ñ¢Ô∏è...\n        </div>\n    }\n    />\n</Cards>\n```\n\n----------------------------------------\n\nTITLE: Using CLI Chatbot in Multi-Agent System\nDESCRIPTION: Illustrates the use of cli_chatbot in a multi-agent system context. It shows how the chatbot can be integrated with more complex agent setups.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/extensions/cli_chatbot.mdx#2025-04-15_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\n// Spin up a chatbot using the agent\ncli_chatbot(translator).await?;\n```\n\n----------------------------------------\n\nTITLE: Using React Components with Inline Icons in JSX\nDESCRIPTION: This snippet demonstrates how to use React components with the defined section_icon style to create headings with inline icons.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/how_to_contribute.mdx#2025-04-15_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\n<><Hammer style={section_icon}/> Development </>\n```\n\n----------------------------------------\n\nTITLE: Running the Rig Concurrent Processing Example\nDESCRIPTION: Command to compile and execute the Rig concurrent processing example with Cargo, Rust's package manager and build system.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/examples/advanced/concurrent_processing.mdx#2025-04-15_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncargo run\n```\n\n----------------------------------------\n\nTITLE: Rendering Lightbulb Icon with Section Title in JSX\nDESCRIPTION: This code demonstrates how to render the Lightbulb icon component inline with the Ideas and Feedback section title using the section_icon style.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/how_to_contribute.mdx#2025-04-15_snippet_3\n\nLANGUAGE: jsx\nCODE:\n```\n<><Lightbulb style={section_icon}/> Ideas and Feedback </>\n```\n\n----------------------------------------\n\nTITLE: Implementing Main Function for Discord Bot in Rust\nDESCRIPTION: Main function implementation that initializes the Discord bot with Tokio runtime, sets up logging, configures gateway intents, and starts the client. Includes environment variable loading and RigAgent initialization.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/discord_bot.mdx#2025-04-15_snippet_13\n\nLANGUAGE: rust\nCODE:\n```\n#[tokio::main]\nasync fn main() -> Result<()> {\n    dotenv().ok();\n\n    tracing_subscriber::fmt()\n        .with_max_level(tracing::Level::DEBUG)\n        .init();\n\n    let token = env::var(\"DISCORD_TOKEN\").expect(\"Expected DISCORD_TOKEN in environment\");\n\n    let rig_agent = Arc::new(RigAgent::new().await?);\n\n    let intents = GatewayIntents::GUILD_MESSAGES\n        | GatewayIntents::DIRECT_MESSAGES\n        | GatewayIntents::MESSAGE_CONTENT;\n\n    let mut client = Client::builder(&token, intents)\n        .event_handler(Handler {\n            rig_agent: Arc::clone(&rig_agent),\n        })\n        .await\n        .expect(\"Err creating client\");\n\n    if let Err(why) = client.start().await {\n        error!(\"Client error: {:?}\", why);\n    }\n\n    Ok(())\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Dependencies in Cargo.toml for Rig\nDESCRIPTION: Cargo.toml configuration specifying the required dependencies for the Rig concurrent processing example. Includes rig-core for LLM functionality and tokio with full features for asynchronous processing.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/examples/advanced/concurrent_processing.mdx#2025-04-15_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[dependencies]\nrig-core = \"0.1.0\"\ntokio = { version = \"1.0\", features = [\"full\"] }\n```\n\n----------------------------------------\n\nTITLE: Configuring Project Dependencies in Cargo.toml\nDESCRIPTION: Project configuration file specifying required dependencies including rig-core, tokio, serde, and other essential libraries\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/flight_assistant.mdx#2025-04-15_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[package]\nname = \"flight_search_assistant\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n[dependencies]\nrig-core = \"0.1.0\"\ntokio = { version = \"1.34.0\", features = [\"full\"] }\nserde = { version = \"1.0\", features = [\"derive\"] }\nserde_json = \"1.0\"\nreqwest = { version = \"0.11\", features = [\"json\", \"tls\"] }\ndotenv = \"0.15\"\nthiserror = \"1.0\"\nchrono = { version = \"0.4\", features = [\"serde\"] }\n```\n\n----------------------------------------\n\nTITLE: Sample Output from Multiple LLM Providers\nDESCRIPTION: Shows the expected output when comparing responses from OpenAI's GPT-4 and Cohere's Command models when prompted with the same question about quantum computing.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/why_rig.mdx#2025-04-15_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nGPT-4: Quantum computing utilizes quantum mechanics principles to perform complex computations exponentially faster than classical computers.\nCohere Command: Quantum computing harnesses quantum superposition and entanglement to process information in ways impossible for classical computers.\n```\n\n----------------------------------------\n\nTITLE: Adding Dotenv to Cargo.toml for Secure API Key Management\nDESCRIPTION: Updating the Cargo.toml file to include the dotenv crate for securely loading environment variables containing API keys.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/text_extraction_classification.mdx#2025-04-15_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\ndotenv = \"0.15.0\"\n```\n\n----------------------------------------\n\nTITLE: Creating a New Rust Project with Cargo\nDESCRIPTION: Commands to create a new Rust project named 'text_classifier_extractor' using Cargo and navigate into the project directory.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/text_extraction_classification.mdx#2025-04-15_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncargo new text_classifier_extractor\ncd text_classifier_extractor\n```\n\n----------------------------------------\n\nTITLE: CloudWatch Metrics Query\nDESCRIPTION: SQL query for CloudWatch Logs Insights to analyze Lambda function performance metrics including memory usage, duration, and cold starts\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/deploy/Blog_2_aws_lambda_lancedb.mdx#2025-04-15_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nfilter @type = \"REPORT\"\n| stats \n      avg(@maxMemoryUsed) / 1000000 as MemoryUsageMB,\n      avg(@duration) / 1000 as AvgDurationSec,\n      max(@duration) / 1000 as MaxDurationSec, \n      min(@duration) / 1000 as MinDurationSec, \n      avg(@initDuration) / 1000 as AvgColdStartTimeSec, \n      count(*) as NumberOfInvocations,\n      sum(@initDuration > 0) as ColdStartInvocations\nby bin(1d) as TimeRange, @memorySize / 1000000 as MemoryConfigurationMB\n```\n\n----------------------------------------\n\nTITLE: Rendering Navigation Cards for Model Providers in JSX\nDESCRIPTION: This code snippet renders a card-based navigation UI using the Nextra components library. It displays cards for each supported model provider (OpenAI, Anthropic, Groq, Gemini, xAI, Perplexity, and Cohere) with links to their respective documentation pages.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/model_providers.mdx#2025-04-15_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\n<Cards>\n<Cards.Card title=\"OpenAI\" href=\"./model_providers/openai\" />\n<Cards.Card title=\"Anthropic\" href=\"./model_providers/anthropic\" />\n<Cards.Card title=\"Groq\" href=\"./model_providers/groq\" />\n<Cards.Card title=\"Gemini\" href=\"./model_providers/gemini\" />\n<Cards.Card title=\"xAI\" href=\"./model_providers/xai\" />\n<Cards.Card title=\"Perplexity\" href=\"./model_providers/perplexity\" />\n<Cards.Card title=\"Cohere\" href=\"./model_providers/cohere\" />\n</Cards>\n```\n\n----------------------------------------\n\nTITLE: Tracing Setup for Logging\nDESCRIPTION: Configuration of tracing subscriber for logging with debug level\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/guides/advanced/discord_bot.mdx#2025-04-15_snippet_17\n\nLANGUAGE: rust\nCODE:\n```\nuse tracing::{info, error, debug};\nuse tracing_subscriber;\n\ntracing_subscriber::fmt()\n    .with_max_level(tracing::Level::DEBUG)\n    .init();\n```\n\n----------------------------------------\n\nTITLE: Implementing OneOrMany Enum for Embedding Storage in Rust\nDESCRIPTION: Definition of the OneOrMany enum that allows storing either a single embedding or multiple embeddings for each document in the vector store.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/integrations/vector_stores/in_memory.mdx#2025-04-15_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\npub enum OneOrMany<T> {\n    One(T),\n    Many(Vec<T>),\n}\n```\n\n----------------------------------------\n\nTITLE: Using CLI Chatbot in Calculator Example\nDESCRIPTION: Shows how the cli_chatbot function is used in a calculator chatbot example. It demonstrates the integration of the chatbot into a specific application.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/extensions/cli_chatbot.mdx#2025-04-15_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\ncli_chatbot(calculator_rag).await?;\n```\n\n----------------------------------------\n\nTITLE: Enabling Tokio Features\nDESCRIPTION: Command to add tokio with specific features required for the main example.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/index.mdx#2025-04-15_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncargo add tokio --features macros,rt-multi-thread\n```\n\n----------------------------------------\n\nTITLE: Markdown Frontmatter for Rig Concepts Page\nDESCRIPTION: YAML frontmatter block defining the title and description for the concepts documentation page\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/concepts.mdx#2025-04-15_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: üß© Concepts\ndescription: This section contains the concepts for Rig.\n---\n```\n\n----------------------------------------\n\nTITLE: Defining Section Icon Component Styling in JSX\nDESCRIPTION: This snippet defines the styling for section icons to be displayed inline with specific size and margin properties.\nSOURCE: https://github.com/0xPlaygrounds/rig-docs/blob/main/pages/docs/how_to_contribute.mdx#2025-04-15_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nexport const section_icon = { display: 'inline', size: '1rem', marginBottom: '0.25rem' }\n```"
  }
]
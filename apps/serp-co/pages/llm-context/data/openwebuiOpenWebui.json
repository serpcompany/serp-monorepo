[
  {
    "owner": "open-webui",
    "repo": "open-webui",
    "content": "TITLE: Specifying Python Dependencies in requirements.txt\nDESCRIPTION: Lists all necessary Python packages and their versions for the Open-WebUI project using the standard `requirements.txt` format. This ensures environment reproducibility by defining dependencies for web serving (FastAPI, Uvicorn), AI (OpenAI, Langchain, Transformers), databases (SQLAlchemy, Peewee, vector DBs), document parsing, authentication, testing, cloud integration (AWS, Azure, Google Cloud), and tracing (OpenTelemetry).\nSOURCE: https://github.com/open-webui/open-webui/blob/main/backend/requirements.txt#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nfastapi==0.115.7\nuvicorn[standard]==0.34.0\npydantic==2.10.6\npython-multipart==0.0.20\n\npython-socketio==5.13.0\npython-jose==3.4.0\npasslib[bcrypt]==1.7.4\n\nrequests==2.32.3\naiohttp==3.11.11\nasync-timeout\naiocache\naiofiles\n\nsqlalchemy==2.0.38\nalembic==1.14.0\npeewee==3.17.9\npeewee-migrate==1.12.2\npsycopg2-binary==2.9.9\npgvector==0.4.0\nPyMySQL==1.1.1\nbcrypt==4.3.0\n\npymongo\nredis\nboto3==1.35.53\n\nargon2-cffi==23.1.0\nAPScheduler==3.10.4\n\nRestrictedPython==8.0\n\nloguru==0.7.2\nasgiref==3.8.1\n\n# AI libraries\nopenai\nanthropic\ngoogle-generativeai==0.8.4\ntiktoken\n\nlangchain==0.3.19\nlangchain-community==0.3.18\n\nfake-useragent==2.1.0\nchromadb==0.6.3\npymilvus==2.5.0\nqdrant-client~=1.12.0\nopensearch-py==2.8.0\nplaywright==1.49.1 # Caution: version must match docker-compose.playwright.yaml\nelasticsearch==8.17.1\n\n\ntransformers\nsentence-transformers==3.3.1\naccelerate\ncolbert-ai==0.2.21\neinops==0.8.1\n\n\nftfy==6.2.3\npypdf==4.3.1\nfpdf2==2.8.2\npymdown-extensions==10.14.2\ndocx2txt==0.8\npython-pptx==1.0.0\nunstructured==0.16.17\nnltk==3.9.1\nMarkdown==3.7\npypandoc==1.15\npandas==2.2.3\nopenpyxl==3.1.5\npyxlsb==1.0.10\nxlrd==2.0.1\nvalidators==0.34.0\npsutil\nsentencepiece\nsoundfile==0.13.1\nazure-ai-documentintelligence==1.0.0\n\npillow==11.1.0\nopencv-python-headless==4.11.0.86\nrapidocr-onnxruntime==1.3.24\nrank-bm25==0.2.2\n\nonnxruntime==1.20.1\n\nfaster-whisper==1.1.1\n\nPyJWT[crypto]==2.10.1\nauthlib==1.4.1\n\nblack==25.1.0\nlangfuse==2.44.0\nyoutube-transcript-api==1.0.3\npytube==15.0.0\n\nextract_msg\npydub\nduckduckgo-search~=8.0.0\n\n## Google Drive\ngoogle-api-python-client\ngoogle-auth-httplib2\ngoogle-auth-oauthlib\n\n## Tests\ndocker~=7.1.0\npytest~=8.3.2\npytest-docker~=3.1.1\n\ngoogleapis-common-protos==1.63.2\ngoogle-cloud-storage==2.19.0\n\nazure-identity==1.21.0\nazure-storage-blob==12.24.1\n\n\n## LDAP\nldap3==2.9.1\n\n## Firecrawl\nfirecrawl-py==1.12.0\n\n# Sougou API SDK(Tencentcloud SDK)\ntencentcloud-sdk-python==3.0.1336\n\n## Trace\nopentelemetry-api==1.31.1\nopentelemetry-sdk==1.31.1\nopentelemetry-exporter-otlp==1.31.1\nopentelemetry-instrumentation==0.52b1\nopentelemetry-instrumentation-fastapi==0.52b1\nopentelemetry-instrumentation-sqlalchemy==0.52b1\nopentelemetry-instrumentation-redis==0.52b1\nopentelemetry-instrumentation-requests==0.52b1\nopentelemetry-instrumentation-logging==0.52b1\nopentelemetry-instrumentation-httpx==0.52b1\nopentelemetry-instrumentation-aiohttp-client==0.52b1\n\n```\n\n----------------------------------------\n\nTITLE: Running Open WebUI with Ollama in Docker (GPU Support) - Bash\nDESCRIPTION: This code snippet demonstrates how to deploy Open WebUI bundled with Ollama on a system equipped with GPU support using Docker. The command runs the container in detached mode, exposes required ports, enables GPU access, and mounts persistent volumes for Ollama and backend data. Dependencies: Docker installed with GPU support (e.g., NVIDIA runtime). The main parameters include container naming, port mapping, volume definition, and restart policy. Outputs a running Docker container accessible at the specified port.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\n```\n\n----------------------------------------\n\nTITLE: Running Open WebUI with Ollama in Docker (CPU Only) - Bash\nDESCRIPTION: This snippet illustrates how to deploy Open WebUI with Ollama using Docker for systems without GPU hardware. The command omits the GPU flag and otherwise closely resembles the GPU version, running the container in detached mode with port mapping and volume mounts for persistent storage. Dependencies: Docker. All main parameters are configurable (e.g., mounting volumes, naming the container). Outputs a running instance accessible at the mapped port.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\n```\n\n----------------------------------------\n\nTITLE: Running Open WebUI with Nvidia GPU Support via Docker (Bash)\nDESCRIPTION: Runs Open WebUI using the CUDA-enabled Docker image (`ghcr.io/open-webui/open-webui:cuda`) to leverage Nvidia GPU acceleration. The `--gpus all` flag enables access to all available host GPUs. Requires Docker, the Nvidia Container Toolkit installed on the host, and a compatible Nvidia GPU. It maps port 3000, mounts the data volume, uses `--add-host` for potential host communication, names the container, and sets it to restart always.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda\n```\n\n----------------------------------------\n\nTITLE: Running Open WebUI for OpenAI API Only via Docker (Bash)\nDESCRIPTION: Runs Open WebUI in a Docker container configured exclusively for use with the OpenAI API. The `-e OPENAI_API_KEY` flag sets the necessary API key as an environment variable (replace `your_secret_key` with your actual key). It maps host port 3000 to container port 8080, mounts the `open-webui` volume for data persistence, names the container `open-webui`, and ensures automatic restarts. This setup does not require or connect to an Ollama instance.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -d -p 3000:8080 -e OPENAI_API_KEY=your_secret_key -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n```\n\n----------------------------------------\n\nTITLE: Running Open WebUI with Local Ollama via Docker (Bash)\nDESCRIPTION: Runs Open WebUI in a Docker container, configured for use with an Ollama instance running on the same host machine. It maps host port 3000 to container port 8080, uses `--add-host` to allow the container to reach the host's Ollama service, mounts a named volume `open-webui` to `/app/backend/data` for persistence, names the container `open-webui`, and ensures it restarts automatically. Uses the `main` tag of the official image.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n```\n\n----------------------------------------\n\nTITLE: Running Open WebUI with Remote Ollama via Docker (Bash)\nDESCRIPTION: Runs Open WebUI in a Docker container, configured to connect to an Ollama instance running on a different server. The `-e OLLAMA_BASE_URL` flag sets the environment variable to the URL of the remote Ollama service (replace `https://example.com` with the actual URL). It maps host port 3000, mounts the `open-webui` volume for data persistence, names the container, and ensures automatic restarts.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n```\n\n----------------------------------------\n\nTITLE: Installing Open WebUI via Pip (Bash)\nDESCRIPTION: Installs the Open WebUI package using the Python package installer, pip. This command should be run in a terminal. It is required to have Python 3.11 installed for compatibility.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install open-webui\n```\n\n----------------------------------------\n\nTITLE: Running Open WebUI Server via Pip (Bash)\nDESCRIPTION: Starts the Open WebUI server after installation via pip. Execute this command in the terminal. The server will typically be accessible at http://localhost:8080 by default.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nopen-webui serve\n```\n\n----------------------------------------\n\nTITLE: Resolving Open WebUI Connection Errors via Docker Host Networking - Bash\nDESCRIPTION: This code demonstrates how to start the Open WebUI Docker container using host networking to resolve issues accessing Ollama at localhost addresses. It uses the --network=host flag, adjusts volume mapping, and sets an environment variable to specify the Ollama server URL. Dependencies: Docker. Key parameters include OLLAMA_BASE_URL and host networking mode, which can affect exposed ports and network visibility. Outputs the container running on the host network for seamless service connectivity.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/README.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n```\n\n----------------------------------------\n\nTITLE: Enabling Offline Mode for HuggingFace Model Hub - Bash\nDESCRIPTION: This snippet sets an environment variable to enable offline mode when running Open WebUI, preventing attempts to download models from HuggingFace's remote hub. Usage: 'export HF_HUB_OFFLINE=1' is run in the shell before starting the application or container. No external dependencies except for an environment supporting Bash or compatible shells. Only requires setting the variable; effective for offline environments or air-gapped deployments.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/README.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nexport HF_HUB_OFFLINE=1\n```\n\n----------------------------------------\n\nTITLE: Running Open WebUI Development Branch with Custom Docker Options - Bash\nDESCRIPTION: This snippet launches Open WebUI from the 'dev' branch using Docker, enabling access to the latest features. It includes custom host mapping, named container, volume mount for persistent data, and auto-restart policy. Dependencies: Docker. The code allows users to test new features while highlighting the risk of instability in development builds. Main parameters: port mapping, volume mount, host alias, and tag selection (':dev'). The result is a running, experimental container instance.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/README.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -d -p 3000:8080 -v open-webui:/app/backend/data --name open-webui --add-host=host.docker.internal:host-gateway --restart always ghcr.io/open-webui/open-webui:dev\n```\n\n----------------------------------------\n\nTITLE: Updating Open WebUI Docker Container with Watchtower - Bash\nDESCRIPTION: This code snippet updates a running Open WebUI Docker container to the latest image version using Watchtower. It launches Watchtower with direct access to the Docker socket, performs a one-time update on the specified container (by name), and then exits. Dependencies: Docker, Watchtower image. The main parameter is the name of the container to update. Outputs the update process logs and pulls updated images if available. Replace 'open-webui' with your custom container name as needed.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/README.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui\n```\n\n----------------------------------------\n\nTITLE: Applying Kustomize Configuration for GPU Deployment (Bash)\nDESCRIPTION: This command uses kubectl with the '-k' flag to apply Kustomize configurations located in './kubernetes/manifest'. This typically includes base manifests plus overlays specific to GPU-enabled deployments, allowing Ollama and Open WebUI to utilize GPU resources on the Kubernetes cluster.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/INSTALLATION.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -k ./kubernetes/manifest\n```\n\n----------------------------------------\n\nTITLE: Applying Base Kustomize Configuration for CPU Deployment (Bash)\nDESCRIPTION: This command uses kubectl to apply the base Kubernetes manifests located in './kubernetes/manifest/base'. This setup is intended for deploying Ollama and Open WebUI in a CPU-only configuration on a Kubernetes cluster.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/INSTALLATION.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f ./kubernetes/manifest/base\n```\n\n----------------------------------------\n\nTITLE: Installing Helm Chart for CPU Deployment (Bash)\nDESCRIPTION: This command installs the previously packaged Helm chart (ollama-webui-*.tgz) with the release name 'ollama-webui'. It uses the default chart values, typically configured for a CPU-only deployment of Ollama and Open WebUI.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/INSTALLATION.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install ollama-webui ./ollama-webui-*.tgz\n```\n\n----------------------------------------\n\nTITLE: Packaging a Helm Chart (Bash)\nDESCRIPTION: This command packages the Helm chart located in the './kubernetes/helm/' directory into a versioned chart archive file (.tgz). This step is a prerequisite before installing the chart using 'helm install'.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/INSTALLATION.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm package ./kubernetes/helm/\n```\n\n----------------------------------------\n\nTITLE: Installing Helm Chart for GPU Deployment (Bash)\nDESCRIPTION: This command installs the packaged Helm chart (ollama-webui-*.tgz) with the release name 'ollama-webui', specifically configuring it for GPU usage. It uses the '--set' flag to override the default values, allocating one NVIDIA GPU resource ('nvidia.com/gpu=\"1\"') to the Ollama component.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/INSTALLATION.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install ollama-webui ./ollama-webui-*.tgz --set ollama.resources.limits.nvidia.com/gpu=\"1\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Apache VirtualHost with SSL for Open WebUI (Apache)\nDESCRIPTION: Extends the basic VirtualHost config to add SSL termination for Open WebUI, using cert/key files and SSLProxy options. It specifies document root, backend proxying for HTTP and WebSocket, and enables additional SSL-related options such as SSLProxyEngine and SSLCACertificateFile. Assumes SSL certificates have been provisioned for the domain and that all SSL files are in place. Requires Apache SSL modules and the proper directory structure for certificate storage.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/docs/apache.md#_snippet_1\n\nLANGUAGE: apache\nCODE:\n```\n# For SSL\n<VirtualHost 192.168.1.100:443>\n    ServerName server.com\n    DocumentRoot /home/server/public_html\n\n    ProxyPass / http://server.com:3000/ nocanon\n    ProxyPassReverse / http://server.com:3000/\n    # Needed after 0.5\n    ProxyPass / ws://server.com:3000/ nocanon\n    ProxyPassReverse / ws://server.com:3000/\n\n    SSLEngine on\n    SSLCertificateFile /etc/ssl/virtualmin/170514456861234/ssl.cert\n    SSLCertificateKeyFile /etc/ssl/virtualmin/170514456861234/ssl.key\n    SSLProtocol all -SSLv2 -SSLv3 -TLSv1 -TLSv1.1\n\n    SSLProxyEngine on\n    SSLCACertificateFile /etc/ssl/virtualmin/170514456865864/ssl.ca\n</VirtualHost>\n```\n\n----------------------------------------\n\nTITLE: Configuring Apache VirtualHost for Open WebUI (Apache)\nDESCRIPTION: Sets up an Apache VirtualHost directive for the Open WebUI user interface. This configuration listens on port 80 and proxies HTTP and WebSocket traffic to a backend server running on port 3000. It assumes the site will be hosted at 'server.com' and that the document root is set accordingly. Dependencies include Apache 2.x, enabled proxy and proxy websocket modules, and the Open WebUI application running and accessible at the specified backend address.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/docs/apache.md#_snippet_0\n\nLANGUAGE: apache\nCODE:\n```\n# Assuming you have a website hosting this UI at \"server.com\"\n<VirtualHost 192.168.1.100:80>\n    ServerName server.com\n    DocumentRoot /home/server/public_html\n\n    ProxyPass / http://server.com:3000/ nocanon\n    ProxyPassReverse / http://server.com:3000/\n    # Needed after 0.5\n    ProxyPass / ws://server.com:3000/ nocanon\n    ProxyPassReverse / ws://server.com:3000/\n\n</VirtualHost>\n```\n\n----------------------------------------\n\nTITLE: Apache VirtualHost for HTTP with HTTPS Redirect and Proxy for Ollama Model Server (Apache)\nDESCRIPTION: Defines a VirtualHost for HTTP (port 80) serving the Ollama model API and automatically redirects all traffic to HTTPS. Contains directory permissions, proxy and SSL settings, Rewrite rules to force HTTPS, and reverse proxy directives to route requests to the backend Ollama server on a separate port. Assumes mod_rewrite and proxy modules are enabled.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/docs/apache.md#_snippet_10\n\nLANGUAGE: apache\nCODE:\n```\n<VirtualHost 192.168.254.109:80>\n    DocumentRoot \"/var/www/html/\"\n    ServerName models.server.city\n    <Directory \"/var/www/html/\">\n        Options None\n        Require all granted\n    </Directory>\n\n    ProxyRequests Off\n    ProxyPreserveHost On\n    ProxyAddHeaders On\n    SSLProxyEngine on\n\n    ProxyPass / http://server.city:1000/ nocanon # or port 11434\n    ProxyPassReverse / http://server.city:1000/ # or port 11434\n\n    RewriteEngine on\n    RewriteCond %{SERVER_NAME} =models.server.city\n    RewriteRule ^ https://%{SERVER_NAME}%{REQUEST_URI} [END,NE,R=permanent]\n</VirtualHost>\n```\n\n----------------------------------------\n\nTITLE: Apache VirtualHost with SSL and Proxy for Ollama Model Server (Apache)\nDESCRIPTION: Creates an SSL-enabled VirtualHost configuration for serving the Ollama model API behind Apache. It includes document root, server name, directory access control, SSL settings through Let's Encrypt, and reverse proxying to the backend Ollama server. Assumes all specified SSL certificate files exist and that mod_ssl, proxy, and related modules are enabled.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/docs/apache.md#_snippet_9\n\nLANGUAGE: apache\nCODE:\n```\n# Assuming you have a website hosting this UI at \"models.server.city\"\n<IfModule mod_ssl.c>\n    <VirtualHost 192.168.254.109:443>\n        DocumentRoot \"/var/www/html/\"\n        ServerName models.server.city\n        <Directory \"/var/www/html/\">\n            Options None\n            Require all granted\n        </Directory>\n\n        ProxyRequests Off\n        ProxyPreserveHost On\n        ProxyAddHeaders On\n        SSLProxyEngine on\n\n        ProxyPass / http://server.city:1000/ nocanon # or port 11434\n        ProxyPassReverse / http://server.city:1000/ # or port 11434\n\n        SSLCertificateFile /etc/letsencrypt/live/models.server.city/fullchain.pem\n        SSLCertificateKeyFile /etc/letsencrypt/live/models.server.city/privkey.pem\n        Include /etc/letsencrypt/options-ssl-apache.conf\n    </VirtualHost>\n</IfModule>\n```\n\n----------------------------------------\n\nTITLE: Requesting and Installing SSL Certificates with Certbot (Bash)\nDESCRIPTION: Runs Certbot to obtain and configure SSL certificates for the specified domain using the Apache plugin. The command 'certbot --apache -d <domain>' automates certificate retrieval, Apache SSL configuration, and key management. Assumes the domain is already pointing to the correct server and VirtualHost is enabled.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/docs/apache.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncertbot --apache -d server.com\n```\n\n----------------------------------------\n\nTITLE: Installing Certbot and Certbot Apache Plugin (Bash)\nDESCRIPTION: Installs Certbot and its Apache plugin for managing SSL certificates with Let's Encrypt using snap and apt. Required for automating SSL certificate issuance and renewal on Apache hosts. The commands should be run with root or sudo privileges.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/docs/apache.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsnap install certbot --classic\n```\n\nLANGUAGE: bash\nCODE:\n```\nsnap apt install python3-certbot-apache\n```\n\n----------------------------------------\n\nTITLE: Enabling Apache Site Configuration (Bash)\nDESCRIPTION: Uses the a2ensite Bash command, a Debian/Ubuntu utility, to enable a site configuration for Apache. The provided example enables 'server.com.conf'. This is a prerequisite for activating the provided VirtualHost and is necessary before requesting and setting up SSL.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/docs/apache.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\na2ensite server.com.conf # this will enable the site. a2ensite is short for \"Apache 2 Enable Site\"\n```\n\n----------------------------------------\n\nTITLE: Enabling Apache Site Configuration for Ollama Model Server (Bash)\nDESCRIPTION: Enables the Apache configuration for the Ollama model domain using a2ensite. This command must be run before SSL certificate requests or reloading for changes to take effect. Run as root or with sudo privileges.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/docs/apache.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\na2ensite models.server.city.conf\n```\n\n----------------------------------------\n\nTITLE: Editing Site Configuration Files (Bash)\nDESCRIPTION: Opens site configuration files using nano, a terminal-based text editor. The example usages are for editing 'server.com.conf' and 'models.server.city.conf', each corresponding to different Apache VirtualHost configurations. Ensures administrators can modify Apache configurations as described in the guide.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/docs/apache.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nnano models.server.city.conf # match this with your ollama server domain\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo nano /etc/systemd/system/ollama.service\n```\n\n----------------------------------------\n\nTITLE: Configuring Ollama Systemd Service File (ini)\nDESCRIPTION: Example configuration for the Ollama systemd unit file, allowing customization of IP/port, origin domains, environment, and command path. The file enables the Ollama server to listen on a chosen interface and port (such as 0.0.0.0:11434), supports configuration of CORS with OLLAMA_ORIGINS, and specifies the execution user/group. The '[Unit]', '[Service]', and '[Install]' sections are standard systemd. Ensure ollama binary is installed and paths exist as specified.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/docs/apache.md#_snippet_7\n\nLANGUAGE: ini\nCODE:\n```\n[Unit]\nDescription=Ollama Service\nAfter=network-online.target\n\n[Service]\nExecStart=/usr/local/bin/ollama serve\nEnvironment=\"OLLAMA_HOST=0.0.0.0:11434\" # this line is mandatory. You can also specify 192.168.254.109:DIFFERENT_PORT, format\nEnvironment=\"OLLAMA_ORIGINS=http://192.168.254.106:11434,https://models.server.city\" # this line is optional\nUser=ollama\nGroup=ollama\nRestart=always\nRestartSec=3\nEnvironment=\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/s>\n\n[Install]\nWantedBy=default.target\n```\n\n----------------------------------------\n\nTITLE: Setting Ollama Service Environment Variable (ini)\nDESCRIPTION: Specifies the OLLAMA_HOST environment variable in the systemd Ollama service file. Required for Ollama to listen on a specific IP and port. Must be placed in the '[Service]' section of the unit file. No other functional dependencies except a valid Ollama server install.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/docs/apache.md#_snippet_8\n\nLANGUAGE: ini\nCODE:\n```\nEnvironment=\"OLLAMA_HOST=0.0.0.0:11434\" # this line is mandatory. You can also specify\n```\n\n----------------------------------------\n\nTITLE: Restarting Apache to Apply Configuration Changes (Bash)\nDESCRIPTION: Reloads the Apache HTTP server to apply any new or changed configuration files using systemctl. Necessary after enabling sites, making config edits, or fetching new certificates. Requires root or sudo access.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/docs/apache.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nsystemctl reload apache2\n```\n\n----------------------------------------\n\nTITLE: Navigating to Apache Sites-Available Directory (Bash)\nDESCRIPTION: Uses cd to change the working directory to Apache's sites-available, where enabled/available VirtualHost configuration files reside. This is typically a preparatory step before creating or editing site-specific configurations.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/docs/apache.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd /etc/apache2/sites-available/\n```\n\n----------------------------------------\n\nTITLE: Connecting to External Databases using DATABASE_URL Environment Variable\nDESCRIPTION: Details the usage of the 'DATABASE_URL' environment variable (added in v0.1.122) to connect Open WebUI to an external custom SQLite or Postgres database, enabling persistent storage outside the default setup.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/CHANGELOG.md#_snippet_11\n\nLANGUAGE: plaintext\nCODE:\n```\nDATABASE_URL\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Proxy for OpenAI/Ollama API Calls (Environment Variable)\nDESCRIPTION: This environment variable, `http_proxy`, allows users to configure an HTTP proxy for outgoing API calls made to OpenAI and Ollama services within Open WebUI, as mentioned in version 0.2.4. Setting this variable directs relevant network traffic through the specified proxy server, facilitating usage in restricted network environments.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/CHANGELOG.md#_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nhttp_proxy\n```\n\n----------------------------------------\n\nTITLE: Setting Batch Size for OpenAI Embeddings in RAG (Environment Variable)\nDESCRIPTION: Introduced in version 0.2.3, the `RAG_EMBEDDING_OPENAI_BATCH_SIZE` environment variable allows configuration of the batch size for processing multiple embeddings simultaneously when using OpenAI for Retrieval-Augmented Generation (RAG). Increasing this value can enhance performance when dealing with large datasets by reducing the number of separate API calls.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/CHANGELOG.md#_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nRAG_EMBEDDING_OPENAI_BATCH_SIZE\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced Ollama Parameters (num_thread, use_mmap, use_mlock)\nDESCRIPTION: Version 0.2.1 added support for advanced configuration parameters for Ollama interactions: `num_thread` (controls the number of CPU threads used), `use_mmap` (enables memory mapping for model loading), and `use_mlock` (locks the model in RAM). These parameters allow for fine-tuning performance and resource usage of the Ollama backend.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/CHANGELOG.md#_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\n'num_thread', 'use_mmap', and 'use_mlock'\n```\n\n----------------------------------------\n\nTITLE: Accessing Models via Unified API Endpoint in Open WebUI\nDESCRIPTION: Details the unified API endpoint '/api/models' used in Open WebUI (changed in v0.1.125) for retrieving model information. This change simplifies API interactions by consolidating model fetching requests to a single path.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/CHANGELOG.md#_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\n/api/models\n```\n\n----------------------------------------\n\nTITLE: Using Dynamic System Prompt Variables in Open WebUI\nDESCRIPTION: Describes the usage of dynamic variables '{{CURRENT_DATE}}' and '{{USER_NAME}}' within system prompts in Open WebUI (added in v0.1.125). These variables allow for personalized and time-sensitive prompts based on the current date and the logged-in user's name.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/CHANGELOG.md#_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\n{{CURRENT_DATE}}\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n{{USER_NAME}}\n```\n\n----------------------------------------\n\nTITLE: Executing Python Code with Pre-installed Libraries in Open WebUI\nDESCRIPTION: Lists the Python libraries available for local code execution within the browser in Open WebUI (added in v0.1.125). These include 'requests', 'beautifulsoup4', 'numpy', 'pandas', 'seaborn', 'matplotlib', 'scikit-learn', 'scipy', and 'regex', enabling various data processing and analysis tasks.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/CHANGELOG.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n'requests', 'beautifulsoup4', 'numpy', 'pandas', 'seaborn', 'matplotlib', 'scikit-learn', 'scipy', 'regex'\n```\n\n----------------------------------------\n\nTITLE: Documenting Project Changes in Markdown Changelog - Markdown\nDESCRIPTION: This snippet exemplifies the format for recording changes in a project changelog using Markdown. It uses sections for version numbers, release dates, and categorized lists (Added, Fixed, Changed) that describe each change. No external dependencies are required; the file is intended to be human-readable and accessible to all project contributors and users. Inputs are manually written entries, and outputs are rendered Markdown documents for transparency and auditing; care must be taken to follow the changelog format for consistency.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/CHANGELOG.md#_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n# Changelog\n\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.1.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [0.6.5] - 2025-04-14\n\n### Added\n\n- ðŸ›‚â€¯**Granular Voice Feature Permissions Per User Group**: Admins can now separately manage access to Speech-to-Text (record voice), Text-to-Speech (read aloud), and Tool Calls for each user groupâ€”giving teams tighter control over voice features and enhanced governance across roles.\n- ðŸ—£ï¸â€¯**Toggle Voice Activity Detection (VAD) for Whisper STT**: New environment variable lets you enable/disable VAD filtering with built-in Whisper speech-to-text, giving you flexibility to optimize for different audio quality and response accuracy levels.\n- ðŸ“‹â€¯**Copy Formatted Response Mode**: You can now enable â€œCopy Formattedâ€ in Settings > Interface to copy AI responses exactly as styled (with rich formatting, links, and structure preserved), making it faster and cleaner to paste into documents, emails, or reports.\n- âš™ï¸â€¯**Backend Stability and Performance Enhancements**: General backend refactoring improves system resilience, consistency, and overall reliabilityâ€”offering smoother performance across workflows whether chatting, generating media, or using external tools.\n- ðŸŒŽâ€¯**Translation Refinements Across Multiple Languages**: Updated translations deliver smoother language localization, clearer labels, and improved international usability throughout the UIâ€”ensuring a better experience for non-English speakers.\n\n### Fixed\n\n- ðŸ› ï¸â€¯**LDAP Login Reliability Restored**: Resolved a critical issue where some LDAP setups failed due to attribute parsingâ€”ensuring consistent, secure, and seamless user authentication across enterprise deployments.\n- ðŸ–¼ï¸â€¯**Image Generation in Temporary Chats Now Works Properly**: Fixed a bug where image outputs werenâ€™t generated during temporary chatsâ€”visual content can now be used reliably in all chat modes without interruptions.\n\n## [0.6.4] - 2025-04-12\n\n### Fixed\n\n- ðŸ› ï¸â€¯**RAG_TEMPLATE Display Issue Resolved**: Fixed a formatting problem where the custom RAG_TEMPLATE wasn't correctly rendered in the interfaceâ€”ensuring that custom retrieval prompts now appear exactly as intended for more reliable prompt engineering.\n\n## [0.6.3] - 2025-04-12\n\n### Added\n\n- ðŸ§ªâ€¯**Auto-Artifact Detection Toggle**: Automatically detects artifacts in resultsâ€”but now you can disable this behavior under advanced settings for full control.\n- ðŸ–¼ï¸â€¯**Widescreen Mode for Shared Chats**: Shared link conversations now support widescreen layoutsâ€”perfect for presentations or easier review across wider displays.\n- ðŸ”â€¯**Reindex Knowledge Files on Demand**: Admins can now trigger reindexing of all knowledge files after changing embeddingsâ€”ensuring immediate alignment with new models for optimal RAG performance.\n- ðŸ“„â€¯**OpenAPI YAML Format Support**: External tools can now use YAML-format OpenAPI specsâ€”making integration simpler for developers familiar with YAML-based configurations.\n- ðŸ’¬â€¯**Message Content Copy Behavior**: Copy action now excludes 'details' tagsâ€”streamlining clipboard content when sharing or pasting summaries elsewhere.\n- ðŸ§­â€¯**Sougou Web Search Integration**: New search engine option addedâ€”enhancing global relevance and diversity of search sources for multilingual users.\n- ðŸ§°â€¯**Frontend Web Loader Engine Configuration**: Admins can now set preferred web loader engine for RAG workflows directly from the frontendâ€”offering more control across setups.\n- ðŸ‘¥â€¯**Multi-Model Chat Permission Control**: Admins can manage access to multi-model chats per user groupâ€”allowing tighter governance in team environments.\n- ðŸ§±â€¯**Persistent Configuration Can Be Disabled**: New environment variable lets advanced users and hosts turn off persistent configsâ€”ideal for volatile or stateless deployments.\n- ðŸ§ â€¯**Elixir Code Highlighting Support**: Elixir syntax is now beautifully rendered in code blocksâ€”perfect for developers using this language in AI or automation projects.\n- ðŸŒâ€¯**PWA External Manifest URL Support**: You can now define an external manifest.jsonâ€”integrate Open WebUI seamlessly in managed or proxy-based PWA environments like Cloudflare Zero Trust.\n- ðŸ§ªâ€¯**Azure AI Speech-to-Text Provider Integration**: Easily transcribe large audio files (up to 200MB) with high accuracy using Microsoft's Azure STTâ€”fully configurable in Audio Settings.\n- ðŸ”â€¯**PKCE (Code Challenge Method) Support for OIDC**: Enhance your OIDC login security with Proof Key for Code Exchangeâ€”ideal for zero-trust and native client apps.\n- âœ¨â€¯**General UI/UX Enhancements**: Numerous refinements across layout, styling, and tool interactionsâ€”reducing visual noise and improving overall usability across key workflows.\n- ðŸŒâ€¯**Translation Updates Across Multiple Languages**: Refined Catalan, Russian, Chinese (Simplified & Traditional), Hungarian, and Spanish translations for clearer navigation and instructions globally.\n\n### Fixed\n\n- ðŸ’¥â€¯**Chat Completion Error with Missing Models Resolved**: Fixed internal server error when referencing a model that doesnâ€™t existâ€”ensuring graceful fallback and clear error guidance.\n- ðŸ”§â€¯**Correct Knowledge Base Citations Restored**: Citations generated by RAG workflows now show accurate referencesâ€”ensuring verifiability in outputs from sourced content.\n- ðŸŽ™ï¸â€¯**Broken OGG/WebM Audio Upload Handling for OpenAI Fixed**: Uploading OGG or WebM files now converts properly to WAV before transcriptionâ€”restoring accurate AI speech recognition workflows.\n- ðŸ”â€¯**Tool Server 'Session' Authentication Restored**: Previously broken session auth on external tool servers is now fully functionalâ€”ensuring secure and seamless access to connected tools.\n- ðŸŒâ€¯**Folder-Based Chat Rename Now Updates Correctly**: Renaming chats in folders now reflects instantly everywhereâ€”improving chat organization and clarity.\n- ðŸ“œâ€¯**KaTeX Overflow Displays Fixed**: Math expressions now stay neatly within message boundsâ€”preserving layout consistency even with long formulas.\n- ðŸš«â€¯**Stopping Ongoing Chat Fixed**: You can now return to an active (ongoing) chat and stop generation at any timeâ€”ensuring full control over sessions.\n- ðŸ”§â€¯**TOOL_SERVERS / TOOL_SERVER_CONNECTIONS Indexing Issue Fixed**: Fixed a mismatch between tool lists and their access pathsâ€”restoring full function and preventing confusion in tool management.\n- ðŸ”â€¯**LDAP Login Handles Multiple Emails**: When LDAP returns multiple email attributes, the first valid one is now usedâ€”ensuring login success and account consistency.\n- ðŸ§©â€¯**Model Visibility Toggle Fix**: Toggling model visibility now works even for untouched modelsâ€”letting admins smoothly manage user access across base models.\n- âš™ï¸â€¯**Cross-Origin manifest.json Now Loads Properly**: Compatibility issues with Cloudflare Zero Trust (and others) resolved, allowing manifest.json to load behind authenticated proxies.\n\n### Changed\n\n- ðŸ”’â€¯**Default Access Scopes Set to Private for All Resources**: Models, tools, and knowledge are now private by default when createdâ€”ensuring better baseline security and visibility controls.\n- ðŸ§±â€¯**General Backend Refactoring for Stability**: Numerous invisible improvements enhance backend scalability, security, and maintainabilityâ€”powering upcoming features with a stronger foundation.\n- ðŸ§©â€¯**Stable Dependency Upgrades**: Updated key platform librariesâ€”Chromadb (0.6.3), pgvector (0.4.0), Azure Identity (1.21.0), and Youtube Transcript API (1.0.3)â€”for improved compatibility, functionality, and security.\n\n## [0.6.2] - 2025-04-06\n\n### Added\n\n- ðŸŒ **Improved Global Language Support**: Expanded and refined translations across multiple languages to enhance clarity and consistency for international users.\n\n### Fixed\n\n- ðŸ› ï¸ **Accurate Tool Descriptions from OpenAPI Servers**: External tools now use full endpoint descriptions instead of summaries when generating tool specificationsâ€”helping AI models understand tool purpose more precisely and choose the right tool more accurately in tool workflows.\n- ðŸ”§ **Precise Web Results Source Attribution**: Fixed a key issue where all web search results showed the same source IDâ€”now each result gets its correct and distinct source, ensuring accurate citations and traceability.\n- ðŸ” **Clean Web Search Retrieval**: Web search now retains only results from URLs where real content was successfully fetchedâ€”improving accuracy and removing empty or broken links from citations.\n- ðŸŽµ **Audio File Upload Response Restored**: Resolved an issue where uploading audio files did not return valid responses, restoring smooth file handling for transcription and audio-based workflows.\n\n### Changed\n\n- ðŸ§° **General Backend Refactoring**: Multiple behind-the-scenes improvements streamline backend performance, reduce complexity, and ensure a more stable, maintainable system overallâ€”making everything smoother without changing your workflow.\n\n## [0.6.1] - 2025-04-05\n\n```\n\n----------------------------------------\n\nTITLE: Storing User Settings Persistently in Open WebUI (config.json)\nDESCRIPTION: Identifies 'config.json' as the file used by Open WebUI (added in v0.1.125) to persistently store user settings locally. This ensures settings are saved for convenience.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/CHANGELOG.md#_snippet_9\n\nLANGUAGE: json\nCODE:\n```\nconfig.json\n```\n\n----------------------------------------\n\nTITLE: Disabling Authentication using WEBUI_AUTH Environment Variable\nDESCRIPTION: Explains how to disable user authentication in Open WebUI by setting the 'WEBUI_AUTH' environment variable to False (added in v0.1.124). This option is intended only for fresh installations without existing users.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/CHANGELOG.md#_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\nWEBUI_AUTH\n```\n\n----------------------------------------\n\nTITLE: Enabling Configuration Reset on Startup (Environment Variable)\nDESCRIPTION: Introduced in version 0.2.2, setting the `RESET_CONFIG_ON_START` environment variable instructs Open WebUI to reset its configuration settings to their default values upon application startup. This provides an easy way to revert any custom configurations without manual intervention.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/CHANGELOG.md#_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nRESET_CONFIG_ON_START\n```\n\n----------------------------------------\n\nTITLE: Managing Bundled LiteLLM with ENABLE_LITELLM Environment Variable\nDESCRIPTION: Describes the 'ENABLE_LITELLM' environment variable (mentioned in v0.1.122 documentation context) potentially used to manage memory usage related to the (now deprecated) bundled LiteLLM support in Open WebUI. Setting this likely controls whether LiteLLM-related components are loaded.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/CHANGELOG.md#_snippet_12\n\nLANGUAGE: plaintext\nCODE:\n```\nENABLE_LITELLM\n```\n\n----------------------------------------\n\nTITLE: Exporting LiteLLM Configuration File from Open WebUI\nDESCRIPTION: Specifies the 'LiteLLM config.yaml' file which contains LiteLLM configuration settings. As bundled LiteLLM support is deprecated (v0.1.125), users migrating can export this file from admin settings > database > export LiteLLM config.yaml to use with a self-hosted LiteLLM instance.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/CHANGELOG.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nLiteLLM config.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring robots.txt to Block All Crawlers\nDESCRIPTION: This robots.txt configuration uses the wildcard `*` to target all user agents (web crawlers) and the `/` directive with `Disallow` to instruct them not to crawl any part of the website. This effectively prevents search engines and other compliant bots from indexing or accessing the site.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/static/robots.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nUser-agent: *\nDisallow: /\n```\n\n----------------------------------------\n\nTITLE: Running Open WebUI Docker Container with Host Network\nDESCRIPTION: This Bash command runs the Open WebUI Docker container using the host's network stack (`--network=host`) to resolve common connection issues where the container cannot reach the Ollama server (typically running on the host). It maps a volume for data persistence, sets the necessary `OLLAMA_BASE_URL` environment variable to point to the Ollama API endpoint accessible from the host, names the container, and ensures it restarts automatically. Using `--network=host` changes the access port from the default 3000 to 8080.\nSOURCE: https://github.com/open-webui/open-webui/blob/main/TROUBLESHOOTING.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n```"
  }
]
[
  {
    "owner": "gptme",
    "repo": "gptme",
    "content": "TITLE: Running GPTme with Local Model and Ollama\nDESCRIPTION: This command runs GPTme with a local LLM served by Ollama. It sets the OPENAI_BASE_URL environment variable to point to the Ollama server, then executes GPTme with the specified local model and input \"hello\".  The '/exit' terminates the gptme session.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/providers.rst#_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\nOPENAI_BASE_URL=\"http://127.0.0.1:11434/v1\" gptme 'hello' -m local/llama3.2:1b\n```\n\n----------------------------------------\n\nTITLE: Creating a New Python File with GPTME\nDESCRIPTION: This example demonstrates how to instruct gptme to create a new Python file named `life.py` implementing Conway's Game of Life. The assistant generates the code and the system saves it to the specified file.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/usage.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\n...\n```\n\n----------------------------------------\n\nTITLE: Serving a Local LLM with Ollama\nDESCRIPTION: This command starts the Ollama server, making the locally downloaded LLMs accessible via an OpenAI API-compatible endpoint. This allows GPTme to interact with the local model.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/providers.rst#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\nollama serve\n```\n\n----------------------------------------\n\nTITLE: Installing gptme with pipx\nDESCRIPTION: This snippet shows how to install gptme using pipx. Pipx is recommended for installing command-line applications like gptme in isolated environments.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/getting-started.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npipx install gptme\n```\n\n----------------------------------------\n\nTITLE: GPTME Bash Examples\nDESCRIPTION: Demonstrates various use cases of the `gptme` command-line tool in a bash environment. Includes examples of generating web apps, rendering images, summarizing files, refactoring code, analyzing images, fixing TODOs, creating commits, fixing failing tests, exploring file systems, taking screenshots, improving configurations, implementing features from URLs/issues, creating new projects, chaining prompts, and resuming conversations.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/examples.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngptme 'write a web app to particles.html which shows off an impressive and colorful particle effect using three.js'\ngptme 'render mandelbrot set to mandelbrot.png'\n\n# files\ngptme 'summarize this' README.md\ngptme 'refactor this' main.py\ngptme 'what do you see?' image.png  # vision\n\n# stdin\ngit status -vv | gptme 'fix TODOs'\ngit status -vv | gptme 'commit'\nmake test | gptme 'fix the failing tests'\n\n# if path not directly provided in prompt, it can read files using tools\ngptme 'explore'\ngptme 'take a screenshot and tell me what you see'\ngptme 'suggest improvements to my vimrc'\n\n# can read URLs (if browser tool is available)\ngptme 'implement this' https://github.com/gptme/gptme/issues/286\n\n# can use `gh` shell tool to read issues, PRs, etc.\ngptme 'implement gptme/gptme/issues/286'\n\n# create new projects\ngptme 'create a performant n-body simulation in rust'\n\n# chaining prompts\ngptme 'make a change' - 'test it' - 'commit it'\ngptme 'show me something cool in the python repl' - 'something cooler' - 'something even cooler'\n\n# resume the last conversation\ngptme -r\n```\n\n----------------------------------------\n\nTITLE: Defining a Basic Custom Tool with ToolSpec in Python\nDESCRIPTION: This code defines a basic custom tool named 'hello' using the `ToolSpec` class. It demonstrates how to define the tool's name, description, instructions, execution function, block types, and parameters. The `execute` function takes arguments and keyword arguments, and yields a message to the user.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/custom_tool.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom gptme.tools import ToolSpec, Parameter, ToolUse\nfrom gptme.message import Message\n\ndef execute(code, args, kwargs, confirm):\n    name = kwargs.get('name', 'World')\n    yield Message('system', f\"Hello, {name}!\")\n\ntool = ToolSpec(\n    name=\"hello\",\n    desc=\"A simple greeting tool\",\n    instructions=\"Greets the user by name\",\n    execute=execute,\n    block_types=[\"hello\"],\n    parameters=[\n        Parameter(\n            name=\"name\",\n            type=\"string\",\n            description=\"Name to greet\",\n            required=False,\n        ),\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Global Configuration Example TOML\nDESCRIPTION: This is an example of a global configuration file for gptme, located at `~/.config/gptme/config.toml`. It shows how to set prompt preferences, API keys, and tool configurations.  It demonstrates the structure and available options for customizing gptme's behavior.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/config.rst#_snippet_0\n\nLANGUAGE: TOML\nCODE:\n```\n[prompt]\nabout_user = \"I am a curious human programmer.\"\nresponse_preference = \"Don't explain basic concepts\"\n\n[env]\n# Uncomment to use Claude 3.5 Sonnet by default\n#MODEL = \"anthropic/claude-3-5-sonnet-20240620\"\n\n# One of these need to be set\n# If none of them are, they will be prompted for on first start\nOPENAI_API_KEY = \"\"\nANTHROPIC_API_KEY = \"\"\nOPENROUTER_API_KEY = \"\"\nXAI_API_KEY = \"\"\nGEMINI_API_KEY = \"\"\nGROQ_API_KEY = \"\"\nDEEPSEEK_API_KEY = \"\"\n\n# Uncomment to use with Ollama\n#MODEL = \"local/<model-name>\"\n#OPENAI_BASE_URL = \"http://localhost:11434/v1\"\n\n# Uncomment to change tool configuration\n#TOOL_FORMAT = \"markdown\" # Select the tool formal. One of `markdown`, `xml`, `tool`\n#TOOL_ALLOWLIST = \"save,append,patch,ipython,shell,browser\"  # Comma separated list of allowed tools\n#TOOL_MODULES = \"gptme.tools,custom.tools\" # List of python comma separated python module path\n```\n\n----------------------------------------\n\nTITLE: Automate Feature Implementation with GPTme (Bash)\nDESCRIPTION: This example demonstrates automating the implementation of a feature using gptme, involving branch creation, file lookup, code modification, type checking, testing, and pull request creation. It utilizes gptme with a series of commands chained together to perform the automation.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/automation.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ gptme 'read <url>' '-' 'create a branch' '-' 'look up relevant files' '-' 'make changes' '-' 'typecheck it' '-' 'test it' '-' 'create a pull request'\n```\n\n----------------------------------------\n\nTITLE: Running GPTme with OpenAI Model\nDESCRIPTION: This command executes GPTme with the OpenAI GPT-4o model. It sends the message \"hello\" to the specified model for processing.  The model flag (-m) sets the LLM provider and model to use. This example requires an OpenAI API key configured via environment variables or the global configuration file.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/providers.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ngptme --model openai/gpt-4o \"hello\"\n```\n\n----------------------------------------\n\nTITLE: Running GPTME\nDESCRIPTION: This command initiates the GPTME application. After installation using pipx, this command starts the interactive session, allowing you to interact with the AI assistant from your terminal.\nSOURCE: https://github.com/gptme/gptme/blob/master/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ngptme\n```\n\n----------------------------------------\n\nTITLE: Running GPTme with Anthropic Model\nDESCRIPTION: This command runs GPTme with Anthropic's default model. If the model part is unspecified, GPTme will fall back to the provider's default. The input is the string \"hello\". An Anthropic API key needs to be available in the environment or configuration.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/providers.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ngptme --model anthropic \"hello\"  # if model part unspecified, will fall back to the provider default\n```\n\n----------------------------------------\n\nTITLE: Running GPTme with OpenRouter Model\nDESCRIPTION: This command executes GPTme with the meta-llama/llama-3.1-70b-instruct model from the OpenRouter provider. The message \"hello\" is provided as input.  It requires an OpenRouter API key to be configured.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/providers.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ngptme --model openrouter/meta-llama/llama-3.1-70b-instruct \"hello\"\n```\n\n----------------------------------------\n\nTITLE: Starting a New Chat or Selecting an Existing One (Bash)\nDESCRIPTION: This bash command is used to initiate a new chat session with gptme or to select an existing chat from a list of past conversations. It provides a starting point for interacting with the gptme tool.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/usage.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngptme\n```\n\n----------------------------------------\n\nTITLE: Requesting Improvements to a Python File (Bash)\nDESCRIPTION: This bash command demonstrates how to request changes to an existing Python file using gptme. The command includes the name of the file to be modified, and the requested changes. gptme will then attempt to generate a patch to apply the specified changes.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/usage.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngptme 'make improvements to life.py so that dead cells fade out over time'\n```\n\n----------------------------------------\n\nTITLE: Running GPTme with Gemini Model\nDESCRIPTION: This command uses the gemini-1.5-flash-latest model from Google's Gemini provider in GPTme. The input is \"hello\".  It requires a Gemini API key.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/providers.rst#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ngptme --model gemini/gemini-1.5-flash-latest \"hello\"\n```\n\n----------------------------------------\n\nTITLE: GitHub Actions Workflow (YAML)\nDESCRIPTION: This YAML file defines a GitHub Actions workflow that automatically runs code reviews on pull requests. It installs gptme and the GitHub CLI, authenticates with GitHub using a token, and then executes the `review_pr.sh` script. The workflow triggers on `opened` and `synchronize` pull request events.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/automation/example_code_review.rst#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nname: Automated Code Review\non:\n  pull_request:\n    types: [opened, synchronize]\n\njobs:\n  review:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Install gptme and GitHub CLI\n        run: |\n          pip install gptme\n          gh auth login --with-token <<< \"${{ secrets.GITHUB_TOKEN }}\"\n      - name: Run code review\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        run: |\n          ./review_pr.sh ${{ github.repository }} ${{ github.event.pull_request.number }}\n```\n\n----------------------------------------\n\nTITLE: Skipping Confirmation Prompts (Bash)\nDESCRIPTION: This bash command uses the ``--no-confirm`` flag to skip confirmation prompts, allowing gptme to execute actions without requiring user confirmation. This is useful when the user is confident in the LLM's actions.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/usage.rst#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngptme --no-confirm 'create a snake game using curses in snake.py, dont run it'\n```\n\n----------------------------------------\n\nTITLE: Installing gptme Development Environment (bash)\nDESCRIPTION: This code snippet provides the commands to set up a development environment for gptme. It includes cloning the repository, navigating to the project root, installing poetry (if needed), activating the virtual environment using poetry, and building the project using make.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/contributing.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# checkout the code and navigate to the root of the project\ngit clone https://github.com/gptme/gptme.git\ncd gptme\n\n# install poetry (if not installed)\npipx install poetry\n\n# activate the virtualenv\npoetry shell\n\n# build the project\nmake build\n```\n\n----------------------------------------\n\nTITLE: Running GPTME in Non-Interactive Mode (Bash)\nDESCRIPTION: This bash command uses the ``--non-interactive`` flag to run gptme in a mode that terminates after completing all prompts. This is useful for scripting and automation, and implies ``--no-confirm``.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/usage.rst#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ngptme --non-interactive 'create a snake game using curses in snake.py, dont run it' '-' 'make the snake green and the apple red'\n```\n\n----------------------------------------\n\nTITLE: Configuring MCP in config.toml\nDESCRIPTION: This code snippet demonstrates how to configure MCP settings in the global configuration file (`~/.config/gptme/config.toml`). It includes settings to enable MCP globally, auto-start servers, and configure specific servers with their command, arguments, and environment variables.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/mcp.rst#_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[mcp]\nenabled = true\nauto_start = true\n\n[[mcp.servers]]\nname = \"my-server\"\nenabled = true\ncommand = \"server-command\"\nargs = [\"--arg1\", \"--arg2\"]\nenv = { API_KEY = \"your-key\" }\n```\n\n----------------------------------------\n\nTITLE: Generating Code Metrics with Make\nDESCRIPTION: This snippet executes a `make` command to generate code metrics for the gptme project. The command is executed from the parent directory (..). The output includes information about project overview, complex functions, large files, and duplicated files, aiding in code quality assessment.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/arewetiny.rst#_snippet_8\n\nLANGUAGE: Makefile\nCODE:\n```\nmake metrics\n```\n\n----------------------------------------\n\nTITLE: Installing gptme with Server Extras (Bash)\nDESCRIPTION: This command installs the gptme package with the `server` extras, which are required to run the gptme server and web UI. It uses pipx for installation.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/server.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npipx install 'gptme[server]'\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Tools Using TOML Configuration\nDESCRIPTION: This TOML configuration snippet demonstrates how to register custom tools within the gptme environment by specifying the module in the `TOOL_MODULES` setting. This allows gptme to automatically load your custom tools. It includes the default `gptme.tools` package and adds a custom package example.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/custom_tool.rst#_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[env]\nTOOL_MODULES = \"gptme.tools,yourpackage.your_custom_tool_module\"\n```\n\n----------------------------------------\n\nTITLE: Generate Daily Activity Summary with ActivityWatch and gptme (Bash)\nDESCRIPTION: This bash script generates a daily summary of ActivityWatch data using gptme. It defines functions to get yesterday's date and ActivityWatch report. The script then uses gptme to create a concise summary of the report, including insights on productivity and patterns, and saves it to a temporary file. Finally, the script prints the path to the generated summary file.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/automation/example_activity_summary.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\n\n# Function to get yesterday's date in YYYY-MM-DD format\nget_yesterday() {\n    date -d \"yesterday\" +%Y-%m-%d\n}\n\n# Function to get ActivityWatch report\nget_aw_report() {\n    local date=$1\n    aw-client report $(hostname) --start $date --stop $(date -d \"$date + 1 day\" +%Y-%m-%d)\n}\n\n# Generate daily summary\ngenerate_daily_summary() {\n    local yesterday=$(get_yesterday)\n    local aw_report=$(get_aw_report $yesterday)\n\n    # Create a temporary file\n    local summary_file=$(mktemp)\n\n    # Generate summary using gptme\n    gptme --non-interactive \"Based on the following ActivityWatch report for $yesterday, provide a concise summary of yesterday's activities.\n    Include insights on productivity, time spent on different categories, and any notable patterns.\n    Suggest areas for improvement if applicable.\n\n    ActivityWatch Report:\n    $aw_report\n\n    Please format the summary in a clear, easy-to-read structure.\n    Save the summary to this file: $summary_file\"\n\n    # Return the path to the summary file\n    echo \"$summary_file\"\n}\n\n# Run the summary generation and get the file path\nsummary_file=$(generate_daily_summary)\n\n# Output the file path (you can use this in other scripts or log it)\necho \"Daily summary saved to: $summary_file\"\n```\n\n----------------------------------------\n\nTITLE: Modifying PYTHONPATH for Development and Testing in Bash\nDESCRIPTION: This bash command demonstrates how to temporarily modify the `PYTHONPATH` environment variable to allow Python to locate a custom module during development and testing, without requiring installation. This is useful for development environments where the module is not yet installed system-wide.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/custom_tool.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport PYTHONPATH=$PYTHONPATH:/path/to/your/module\n```\n\n----------------------------------------\n\nTITLE: GPTMe CLI Help Usage\nDESCRIPTION: This shell command displays the help message for the `gptme` CLI tool, showing available commands, options, and a brief description of its functionality.\nSOURCE: https://github.com/gptme/gptme/blob/master/README.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n$ gptme --help\nUsage: gptme [OPTIONS] [PROMPTS]...\n\n  gptme is a chat-CLI for LLMs, empowering them with tools to run shell\n  commands, execute code, read and manipulate files, and more.\n\n  If PROMPTS are provided, a new conversation will be started with it. PROMPTS\n  can be chained with the '-' separator.\n\n  The interface provides user commands that can be used to interact with the\n  system.\n\n  Available commands:\n    /undo         Undo the last action\n    /log          Show the conversation log\n    /tools        Show available tools\n    /edit         Edit the conversation in your editor\n    /rename       Rename the conversation\n    /fork         Create a copy of the conversation with a new name\n    /summarize    Summarize the conversation\n    /replay       Re-execute codeblocks in the conversation, wont store output in log\n    /impersonate  Impersonate the assistant\n    /tokens       Show the number of tokens used\n    /export       Export conversation as standalone HTML\n    /help         Show this help message\n    /exit         Exit the program\n\n  Keyboard shortcuts:\n    Ctrl+J        Insert a new line without executing the prompt\n\nOptions:\n  -n, --name TEXT        Name of conversation. Defaults to generating a random\n                         name.\n  -m, --model TEXT       Model to use, e.g. openai/gpt-4o,\n                         anthropic/claude-3-5-sonnet-20240620. If only\n                         provider given, a default is used.\n  -w, --workspace TEXT   Path to workspace directory. Pass '@log' to create a\n                         workspace in the log directory.\n  -r, --resume           Load last conversation\n  -y, --no-confirm       Skips all confirmation prompts.\n  -n, --non-interactive  Force non-interactive mode. Implies --no-confirm.\n  --system TEXT          System prompt. Can be 'full', 'short', or something\n                         custom.\n  -t, --tools TEXT       Comma-separated list of tools to allow. Available:\n                         read, save, append, patch, shell, subagent, tmux,\n                         browser, gh, chats, screenshot, vision, computer,\n                         python.\n  --no-stream            Don't stream responses\n  --show-hidden          Show hidden system messages.\n  -v, --verbose          Show verbose output.\n  --version              Show version and configuration information\n  --help                 Show this message and exit.\n\n```\n\n----------------------------------------\n\nTITLE: Counting Total Lines of Code with Make\nDESCRIPTION: This snippet executes a `make` command to count the total lines of code in the gptme project. The command is executed from the parent directory (..).  The output provides the overall code size of the project.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/arewetiny.rst#_snippet_7\n\nLANGUAGE: Makefile\nCODE:\n```\nmake cloc-total\n```\n\n----------------------------------------\n\nTITLE: Pulling a Local LLM with Ollama\nDESCRIPTION: This command pulls the llama3.2:1b model using Ollama. This downloads the specified model to the local machine, making it available for use with an OpenAI API-compatible server.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/providers.rst#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\nollama pull llama3.2:1b\n```\n\n----------------------------------------\n\nTITLE: Building and Running gptme Computer Use Interface in Docker (Bash)\nDESCRIPTION: This set of commands clones the gptme repository, builds a Docker container for the computer use interface, and runs the container, mapping necessary configuration files and ports. Requires Docker to be installed.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/server.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Clone the repository\ngit clone https://github.com/gptme/gptme.git\ncd gptme\n# Build container\nmake build-docker-computer\n# Run container\ndocker run -v ~/.config/gptme:/home/computeruse/.config/gptme -p 6080:6080 -p 8080:8080 gptme-computer:latest\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Import Time with Make\nDESCRIPTION: This snippet executes a `make` command to benchmark the import time of the gptme project. The command is run from the parent directory (..) and truncates long output using ellipsis. The output measures the time taken to import the project's modules, serving as a performance indicator.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/arewetiny.rst#_snippet_0\n\nLANGUAGE: Makefile\nCODE:\n```\nmake bench-importtime\n```\n\n----------------------------------------\n\nTITLE: Chaining Multiple Prompts (Bash)\nDESCRIPTION: This bash command uses the ``-`` separator to chain multiple prompts together, allowing gptme to complete each prompt sequentially. This is useful for breaking down complex tasks into smaller, manageable steps.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/usage.rst#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngptme 'create a project' '-' 'add tests' '-' 'commit changes'\n```\n\n----------------------------------------\n\nTITLE: Running the gptme Server (Bash)\nDESCRIPTION: This command starts the gptme server. It assumes that gptme has been installed with the `server` extras. The server provides a REST API and a minimal web UI.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/server.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngptme-server\n```\n\n----------------------------------------\n\nTITLE: Running GPTme with Local Model\nDESCRIPTION: This command executes GPTme with a local LLM specified as local/llama3.2:1b. The input is \"hello\". This setup requires a local OpenAI API-compatible server like Ollama to be running. It assumes the local server is configured to accept requests on the specified base URL.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/providers.rst#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\ngptme --model local/llama3.2:1b \"hello\"\n```\n\n----------------------------------------\n\nTITLE: Review Pull Request Script (Bash)\nDESCRIPTION: This bash script fetches the diff of a pull request from GitHub, generates a code review using gptme, and posts the review as a comment on the pull request. It requires the GitHub CLI (`gh`) and `curl` to be installed and authenticated, and `gptme` to be installed. It takes the repository name and pull request number as input.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/automation/example_code_review.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\n# Usage: ./review_pr.sh <repo> <pr_number>\n\nrepo=$1\npr_number=$2\n\n# Fetch PR diff\ndiff=$(gh pr view $pr_number --repo $repo --json diffUrl -q .diffUrl | xargs curl -s)\n\n# Generate review using gptme\nreview=$(gptme --non-interactive \"Review this pull request diff and provide constructive feedback:\n1. Identify potential bugs or issues.\n2. Suggest improvements for code quality and readability.\n3. Check for adherence to best practices.\n4. Highlight any security concerns.\n\nPull Request Diff:\n$diff\n\nFormat your review as a markdown list with clear, concise points.\")\n\n# Post review comment\ngh pr comment $pr_number --repo $repo --body \"## Automated Code Review\n\n$review\n\n*This review was generated automatically by gptme.*\"\n```\n\n----------------------------------------\n\nTITLE: Running evals with Docker for GPTme\nDESCRIPTION: These bash commands build a Docker image for GPTme and then runs the 'hello' evaluation within the Docker container using the Claude 3.7 Sonnet model. It ensures isolation and reproducibility. The ANTHROPIC_API_KEY environment variable must be set and a directory for results must be mounted.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/evals.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake build-docker\ndocker run \\\n    -e \"ANTHROPIC_API_KEY=<your api key>\" \\\n    -v $(pwd)/eval_results:/app/eval_results \\\n    gptme-eval hello --model anthropic/claude-3-7-sonnet-20250219\n```\n\n----------------------------------------\n\nTITLE: Running gptme with the Computer Tool (Bash)\nDESCRIPTION: This command runs gptme with the `computer` tool enabled. This allows the model to interact with the computer's desktop environment. It also requires X11 and xdotool installed, and it's disabled by default for security reasons.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/server.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngptme -t computer   # and whichever other tools you want\n```\n\n----------------------------------------\n\nTITLE: Counting LLM Lines of Code with Make\nDESCRIPTION: This snippet uses a `make` command to count the lines of code within the 'LLM' (Language Model) component of the gptme project. The command is run from the parent directory (..). This provides a metric for the size of the code dedicated to language model integration.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/arewetiny.rst#_snippet_2\n\nLANGUAGE: Makefile\nCODE:\n```\nmake cloc-llm\n```\n\n----------------------------------------\n\nTITLE: Running GPTme with XAI Model\nDESCRIPTION: This command uses the grok-beta model from the XAI provider.  The input message is \"hello\". It requires an XAI API key.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/providers.rst#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\ngptme --model xai/grok-beta \"hello\"\n```\n\n----------------------------------------\n\nTITLE: Collecting Conversation History using collect.py\nDESCRIPTION: This command executes the `collect.py` script to gather conversation history for fine-tuning. The `--model` parameter specifies the model to be used, such as \"HuggingFaceH4/zephyr-7b-beta\". The script generates `train.csv` and `train.jsonl` files in the `train` directory.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/finetuning.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./train/collect.py --model \"HuggingFaceH4/zephyr-7b-beta\"  # or whatever model you intend to fine-tune\n```\n\n----------------------------------------\n\nTITLE: Installing GPTME with pipx\nDESCRIPTION: This command installs the GPTME package using pipx, a tool for installing Python applications in isolated environments. It requires Python 3.10 or higher. This ensures that GPTME and its dependencies are isolated from other Python projects on your system, preventing conflicts and ensuring a clean environment.\nSOURCE: https://github.com/gptme/gptme/blob/master/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n# requires Python 3.10+\npipx install gptme\n```\n\n----------------------------------------\n\nTITLE: Add Project to List - reStructuredText\nDESCRIPTION: This reStructuredText snippet provides a template for adding your project to the list of gptme-related projects.  It includes the project name and a brief description.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/projects.rst#_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n* `Project Name <https://github.com/username/project>`_\n    Brief description of your project.\n```\n\n----------------------------------------\n\nTITLE: Counting Tools Lines of Code with Make\nDESCRIPTION: This snippet executes a `make` command to count the lines of code within the 'tools' component of the gptme project.  The command is executed from the parent directory (..). The output quantifies the code size of utility functions and scripts.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/arewetiny.rst#_snippet_3\n\nLANGUAGE: Makefile\nCODE:\n```\nmake cloc-tools\n```\n\n----------------------------------------\n\nTITLE: GitHub Actions Workflow for gptme-bot\nDESCRIPTION: This workflow file triggers the `gptme-bot` action in response to comments created on GitHub issues. It checks out the repository, runs the `gptme-bot` action with the provided OpenAI API key, GitHub token, and a list of allowed users. The bot will then execute based on the content of the comment.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/bot.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nname: gptme-bot\n\non:\n  issue_comment:\n    types: [created]\n\npermissions: write-all\n\njobs:\n  run-bot:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: run gptme-bot action\n        uses: gptme/gptme/.github/actions/bot@master\n        with:\n          openai_api_key: ${{ secrets.OPENAI_API_KEY }}\n          github_token: ${{ secrets.GITHUB_TOKEN }}\n          allowlist: \"erikbjare\"\n```\n\n----------------------------------------\n\nTITLE: Counting Server Lines of Code with Make\nDESCRIPTION: This snippet runs a `make` command to count the lines of code within the 'server' component of the gptme project.  The command is executed from the parent directory (..). The output represents the amount of code for the server-side functionality.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/arewetiny.rst#_snippet_4\n\nLANGUAGE: Makefile\nCODE:\n```\nmake cloc-server\n```\n\n----------------------------------------\n\nTITLE: Running a simple eval with GPTme\nDESCRIPTION: This bash command runs the 'hello' evaluation using the Claude 3.7 Sonnet model. It's a basic test to check if the model and evaluation setup are working correctly. It directly executes the `gptme-eval` command with the specified model.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/evals.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngptme-eval hello --model anthropic/claude-3-7-sonnet-20250219\n```\n\n----------------------------------------\n\nTITLE: GPTME Examples\nDESCRIPTION: These are example commands demonstrating how to use GPTME for various tasks such as generating code, rendering images, improving configuration files, and fixing tests. The first example uses three.js for particle effects, the second renders a Mandelbrot set, the third suggests improvements to a vimrc file, the fourth converts a video and adjusts the volume and the last two examples pipes the output of git diff and make test commands into gptme to complete TODOs and fix failing tests.\nSOURCE: https://github.com/gptme/gptme/blob/master/README.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ngptme 'write an impressive and colorful particle effect using three.js to particles.html'\ngptme 'render mandelbrot set to mandelbrot.png'\ngptme 'suggest improvements to my vimrc'\ngptme 'convert to h265 and adjust the volume' video.mp4\ngit diff | gptme 'complete the TODOs in this diff'\nmake test | gptme 'fix the failing tests'\n```\n\n----------------------------------------\n\nTITLE: Starting an interactive chat session\nDESCRIPTION: This snippet shows the command to start an interactive chat session with the AI assistant provided by gptme. It requires gptme to be installed and potentially an LLM provider API key to be configured.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/getting-started.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngptme\n```\n\n----------------------------------------\n\nTITLE: Running GPTme with Deepseek Model\nDESCRIPTION: This command runs GPTme with the deepseek-reasoner model from Deepseek.  It sends the message \"hello\" to the specified model. It requires a Deepseek API key configured.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/providers.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ngptme --model deepseek/deepseek-reasoner \"hello\"\n```\n\n----------------------------------------\n\nTITLE: Counting Core Lines of Code with Make\nDESCRIPTION: This snippet utilizes a `make` command to count the lines of code within the 'core' component of the gptme project. The command is executed from the parent directory (..). The output provides a numerical representation of the code volume in the core modules.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/arewetiny.rst#_snippet_1\n\nLANGUAGE: Makefile\nCODE:\n```\nmake cloc-core\n```\n\n----------------------------------------\n\nTITLE: Configuring the Memory Server\nDESCRIPTION: This snippet demonstrates configuring the memory server as an MCP server in gptme. It specifies the server's name, enabling status, command (using `npx`), command-line arguments to run the `@modelcontextprotocol/server-memory` package, and the environment variable `MEMORY_FILE_PATH` to define the path to the memory JSON file. This server provides persistent knowledge storage.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/mcp.rst#_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[mcp.servers]]\nname = \"memory\"\nenabled = true\ncommand = \"npx\"\nargs = [\n    \"-y\",\n    \"@modelcontextprotocol/server-memory\"\n]\nenv = { MEMORY_FILE_PATH = \"/path/to/memory.json\" }\n```\n\n----------------------------------------\n\nTITLE: Counting Tests Lines of Code with Make\nDESCRIPTION: This snippet executes a `make` command to count the lines of code within the 'tests' directory of the gptme project.  The command is executed from the parent directory (..). The output provides a metric for the size of the test suite.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/arewetiny.rst#_snippet_5\n\nLANGUAGE: Makefile\nCODE:\n```\nmake cloc-tests\n```\n\n----------------------------------------\n\nTITLE: Counting Eval Lines of Code with Make\nDESCRIPTION: This snippet uses a `make` command to count the lines of code within the 'eval' (evaluation) component of the gptme project. The command is executed from the parent directory (..). The output indicates the code size for evaluation scripts and tools.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/arewetiny.rst#_snippet_6\n\nLANGUAGE: Makefile\nCODE:\n```\nmake cloc-eval\n```\n\n----------------------------------------\n\nTITLE: Automate Daily Summary with Cron (Bash)\nDESCRIPTION: This cron entry automates the execution of the daily summary script at 8 AM every day.  It requires the correct path to the daily summary script to be specified.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/automation/example_activity_summary.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n0 8 * * * /path/to/daily_summary_script.sh\n```\n\n----------------------------------------\n\nTITLE: Running GPTme with Groq Model\nDESCRIPTION: This command executes GPTme using the llama-3.3-70b-versatile model from Groq. The input message is \"hello\".  A Groq API key is required.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/providers.rst#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\ngptme --model groq/llama-3.3-70b-versatile \"hello\"\n```\n\n----------------------------------------\n\nTITLE: Project Configuration Example TOML\nDESCRIPTION: This is an example of a project configuration file for gptme, named `gptme.toml`. It allows project-specific configurations like including files in the context, setting a project-specific prompt, or customizing the base prompt. This configuration overrides the global configuration for the specified project.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/config.rst#_snippet_1\n\nLANGUAGE: TOML\nCODE:\n```\nfiles = [\"README.md\", \"Makefile\"]\nprompt = \"This is gptme.\"\n```\n\n----------------------------------------\n\nTITLE: Applying Changes to a Python File (Patch)\nDESCRIPTION: This example demonstrates the patch generated by gptme to modify the `life.py` file, making alive cells green and dead cells black. The patch is applied by the system.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/usage.rst#_snippet_3\n\nLANGUAGE: patch\nCODE:\n```\n...\n```\n\n----------------------------------------\n\nTITLE: Make Script Executable (Bash)\nDESCRIPTION: This command makes the `review_pr.sh` script executable, allowing it to be run directly from the command line. This is a necessary step before the script can be used in the GitHub Actions workflow.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/automation/example_code_review.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nchmod +x review_pr.sh\n```\n\n----------------------------------------\n\nTITLE: Tool Confirmation Request Example JSON\nDESCRIPTION: This JSON snippet shows the format of a tool confirmation request sent to the client when a tool use is detected. It includes the tool's ID and details about the tool, its arguments, and the content to be executed.\nSOURCE: https://github.com/gptme/gptme/blob/master/gptme/server/server-api-improvements.md#_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"id\": \"tool-1234\",\n  \"tooluse\": {\n    \"tool\": \"shell\",\n    \"args\": [],\n    \"content\": \"ls -la /\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Tool Confirmation Response: Auto Action JSON\nDESCRIPTION: This JSON snippet illustrates a confirmation response with an 'auto' action. The 'count' field specifies the number of times the server should automatically confirm the tool execution. The 'id' field identifies the tool for auto-confirmation.\nSOURCE: https://github.com/gptme/gptme/blob/master/gptme/server/server-api-improvements.md#_snippet_4\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"id\": \"tool-1234\",\n  \"action\": \"auto\",\n  \"count\": 5\n}\n```\n\n----------------------------------------\n\nTITLE: Tool Confirmation Response: Confirm Action JSON\nDESCRIPTION: This JSON snippet represents a confirmation response from the client, indicating that the tool execution should be confirmed. The 'id' field matches the tool ID from the original confirmation request, and the 'action' field is set to 'confirm'.\nSOURCE: https://github.com/gptme/gptme/blob/master/gptme/server/server-api-improvements.md#_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"id\": \"tool-1234\",\n  \"action\": \"confirm\"\n}\n```\n\n----------------------------------------\n\nTITLE: Tool Confirmation Response: Skip Action JSON\nDESCRIPTION: This JSON snippet shows a confirmation response with a 'skip' action. It instructs the server to skip the execution of the specified tool. The 'id' field ensures that the correct tool is skipped.\nSOURCE: https://github.com/gptme/gptme/blob/master/gptme/server/server-api-improvements.md#_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"id\": \"tool-1234\",\n  \"action\": \"skip\"\n}\n```\n\n----------------------------------------\n\nTITLE: Tool Confirmation Response: Edit Action JSON\nDESCRIPTION: This JSON snippet demonstrates a confirmation response with an 'edit' action. The 'content' field contains the modified command that the client wants the server to execute. The 'id' field identifies the tool being edited.\nSOURCE: https://github.com/gptme/gptme/blob/master/gptme/server/server-api-improvements.md#_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"id\": \"tool-1234\",\n  \"action\": \"edit\",\n  \"content\": \"ls -la /\"\n}\n```\n\n----------------------------------------\n\nTITLE: Add 'Built with gptme' Badge - Markdown\nDESCRIPTION: This markdown snippet creates a badge indicating that the project was built using gptme. It uses shields.io to generate the badge image and links to the gptme repository.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/projects.rst#_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n[![built using gptme](https://img.shields.io/badge/built%20using-gptme%20%F0%9F%A4%96-5151f5?style=flat)](https://github.com/gptme/gptme)\n```\n\n----------------------------------------\n\nTITLE: Add 'Powered by gptme' Badge - Markdown\nDESCRIPTION: This markdown snippet generates a badge indicating that the project is powered by gptme. It uses shields.io to generate the badge image and links to the gptme repository.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/projects.rst#_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n[![Powered by gptme](https://img.shields.io/badge/powered%20by-gptme%20%F0%9F%A4%96-5151f5?style=flat)](https://github.com/gptme/gptme)\n```\n\n----------------------------------------\n\nTITLE: Installing pipx with pip\nDESCRIPTION: This snippet demonstrates how to install pipx using pip. Pipx is a prerequisite for the recommended installation method of gptme.\nSOURCE: https://github.com/gptme/gptme/blob/master/docs/getting-started.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install --user pipx\n```"
  }
]
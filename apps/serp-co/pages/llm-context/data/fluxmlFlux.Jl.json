[
  {
    "owner": "fluxml",
    "repo": "flux.jl",
    "content": "TITLE: Training the Flux MLP on MNIST Dataset - Julia\nDESCRIPTION: The train function initializes training parameters, loads data, constructs the model, sets up the loss and optimizer (Adam), and iteratively trains the model for the specified epochs. For each epoch, it processes mini-batches, computes gradients, updates weights, and reports training and test accuracy. Invokes train() to start the process. Requires previous function definitions and Flux/MLUtils packages.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-mlp.md#_snippet_6\n\nLANGUAGE: julia\nCODE:\n```\nfunction train(; kws...)\n    # Initializing Model parameters \n    args = Args(; kws...)\n\n    device = args.usegpu ? Flux.get_device() : Flux.get_device(\"CPU\")\n    \n    # Load Data\n    train_loader, test_loader = getdata(args)\n\n    # Construct model\n    model = build_model() |> device\n\n    loss(model, x, y) = logitcrossentropy(model(x), y)\n    \n    ## Training\n    opt_state = Flux.setup(Adam(args.rate), model)\n\t\n    for epoch in 1:args.epochs\n        @info \"Epoch $epoch\"\n        for d in train_loader\n            x, y = d |> device\n            g = gradient(m -> loss(m, x, y), model)[1]\n            Flux.update!(opt_state, model, g)\n        end\n        @show accuracy(train_loader, model)\n        @show accuracy(test_loader, model)\n    end\nend\n\ntrain()\n```\n\n----------------------------------------\n\nTITLE: Implementing a Basic Training Loop in Julia with Flux\nDESCRIPTION: Demonstrates the fundamental manual process of training a Flux model. It involves initializing the optimizer state using `Flux.setup`, iterating through the training dataset (`train_set`), calculating gradients of the loss function with respect to model parameters using `Flux.gradient`, and finally updating the model's parameters using `Flux.update!` based on the chosen optimization `rule`.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/training/training.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\n# Initialise the optimiser for this model:\nopt_state = Flux.setup(rule, model)\n\nfor data in train_set\n  # Unpack this element (for supervised training):\n  input, label = data\n\n  # Calculate the gradient of the objective\n  # with respect to the parameters within the model:\n  grads = Flux.gradient(model) do m\n      result = m(input)\n      loss(result, label)\n  end\n\n  # Update the parameters so as to reduce the objective,\n  # according the chosen optimisation rule:\n  Flux.update!(opt_state, model, grads[1])\nend\n```\n\n----------------------------------------\n\nTITLE: Implementing a Basic MLP for XOR with Flux.jl\nDESCRIPTION: This Julia snippet demonstrates the end-to-end process of setting up a Flux.jl environment, generating noisy XOR data, defining a simple MLP model (Dense and BatchNorm layers), optionally moving data and model to GPU, setting up an Adam optimizer, training the model using a loop with `Flux.withgradient` and `Flux.update!`, calculating `logitcrossentropy` loss, logging the loss, and performing basic evaluation by comparing predicted probabilities (`softmax`) to the ground truth.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/quickstart.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\n# Install everything, including CUDA, and load packages:\nusing Pkg; Pkg.add([\"Flux\", \"CUDA\", \"cuDNN\", \"ProgressMeter\"])\nusing Flux, Statistics, ProgressMeter\nusing CUDA  # optional\ndevice = gpu_device()  # function to move data and model to the GPU\n\n# Generate some data for the XOR problem: vectors of length 2, as columns of a matrix:\nnoisy = rand(Float32, 2, 1000)                                    # 2×1000 Matrix{Float32}\ntruth = [xor(col[1]>0.5, col[2]>0.5) for col in eachcol(noisy)]   # 1000-element Vector{Bool}\n\n# Define our model, a multi-layer perceptron with one hidden layer of size 3:\nmodel = Chain(\n    Dense(2 => 3, tanh),      # activation function inside layer\n    BatchNorm(3),\n    Dense(3 => 2)) |> device  # move model to GPU, if one is available\n\n# The model encapsulates parameters, randomly initialised. Its initial output is:\nout1 = model(noisy |> device)    # 2×1000 Matrix{Float32}, or CuArray{Float32}\nprobs1 = softmax(out1) |> cpu    # normalise to get probabilities (and move off GPU)\n\n# To train the model, we use batches of 64 samples, and one-hot encoding:\ntarget = Flux.onehotbatch(truth, [true, false])                   # 2×1000 OneHotMatrix\nloader = Flux.DataLoader((noisy, target), batchsize=64, shuffle=true);\n\nopt_state = Flux.setup(Flux.Adam(0.01), model)  # will store optimiser momentum, etc.\n\n# Training loop, using the whole data set 1000 times:\nlosses = []\n@showprogress for epoch in 1:1_000\n    for xy_cpu in loader\n        # Unpack batch of data, and move to GPU:\n        x, y = xy_cpu |> device\n        loss, grads = Flux.withgradient(model) do m\n            # Evaluate model and loss inside gradient context:\n            y_hat = m(x)\n            Flux.logitcrossentropy(y_hat, y)\n        end\n        Flux.update!(opt_state, model, grads[1])\n        push!(losses, loss)  # logging, outside gradient context\n    end\nend\n\nopt_state # parameters, momenta and output have all changed\n\nout2 = model(noisy |> device)         # first row is prob. of true, second row p(false)\nprobs2 = softmax(out2) |> cpu         # normalise to get probabilities\nmean((probs2[1,:] .> 0.5) .== truth)  # accuracy 94% so far!\n```\n\n----------------------------------------\n\nTITLE: Using the `train!` Function for Concise Training Loops in Julia\nDESCRIPTION: Shows a more compact way to express the training loop using the `Flux.train!` function. This function encapsulates the common pattern of iterating through data, calculating gradients, and updating parameters. It requires the model, the training dataset, the optimizer state, and a function (often provided via a `do` block) that computes the loss given the model, input (`x`), and label (`y`).\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/training/training.md#_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\ntrain!(model, train_set, opt_state) do m, x, y\n  loss(m(x), y)\nend\n```\n\n----------------------------------------\n\nTITLE: Building a Flexible Manual Training Loop with Logging in Julia\nDESCRIPTION: Presents a detailed, flexible manual training loop in Julia using Flux. It uses `Flux.withgradient` to get both the loss value (`val`) and gradients (`grads`). The loop includes logic for logging individual losses (`losses`), checking for non-finite loss values (`isfinite`), skipping updates (`continue`), updating parameters (`Flux.update!`), calculating overall epoch accuracy (`my_accuracy`), logging epoch results (`my_log`), and implementing early stopping (`break`) based on an accuracy threshold.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/training/training.md#_snippet_10\n\nLANGUAGE: julia\nCODE:\n```\nopt_state = Flux.setup(Adam(), model)\n\nmy_log = []\nfor epoch in 1:100\n  losses = Float32[]\n  for (i, data) in enumerate(train_set)\n    input, label = data\n\n    val, grads = Flux.withgradient(model) do m\n      # Any code inside here is differentiated.\n      # Evaluation of the model and loss must be inside!\n      result = m(input)\n      my_loss(result, label)\n    end\n\n    # Save the loss from the forward pass. (Done outside of gradient.)\n    push!(losses, val)\n\n    # Detect loss of Inf or NaN. Print a warning, and then skip update!\n    if !isfinite(val)\n      @warn \"loss is $val on item $i\" epoch\n      continue\n    end\n\n    Flux.update!(opt_state, model, grads[1])\n  end\n\n  # Compute some accuracy, and save details as a NamedTuple\n  acc = my_accuracy(model, train_set)\n  push!(my_log, (; acc, losses))\n\n  # Stop training when some criterion is reached\n  if  acc > 0.95\n    println(\"stopping after $epoch epochs\")\n    break\n  end\nend\n```\n\n----------------------------------------\n\nTITLE: Training a Simple Model with Flux.jl in Julia\nDESCRIPTION: This Julia snippet demonstrates a basic machine learning workflow using Flux.jl. It generates synthetic data, defines a simple neural network model (either as a closure or using built-in layers like Dense), sets up the Adam optimizer, trains the model for 100 epochs using `Flux.train!`, and finally uses the Plots library to visualize both the original data-generating function and the function learned by the model.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/README.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nusing Flux\ndata = [(x, 2x-x^3) for x in -2:0.1f0:2]\n\nmodel = let\n  w, b, v = (randn(Float32, 23) for _ in 1:3)  # parameters\n  x -> sum(v .* tanh.(w*x .+ b))               # callable\nend\n# model = Chain(vcat, Dense(1 => 23, tanh), Dense(23 => 1, bias=false), only)\n\nopt_state = Flux.setup(Adam(), model)\nfor epoch in 1:100\n  Flux.train!((m,x,y) -> (m(x) - y)^2, model, data, opt_state)\nend\n\nusing Plots\nplot(x -> 2x-x^3, -2, 2, label=\"truth\")\nscatter!(model, -2:0.1f0:2, label=\"learned\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing CIFAR-10 Dataset with MLDatasets and Flux - julia\nDESCRIPTION: This group of snippets covers importing necessary modules (Statistics, Flux, MLDatasets, ImageCore, Plots, MLUtils), with optional CUDA/AMDGPU/Metal for GPU arrays, then loads and one-hot encodes CIFAR-10 images and labels. Demonstrates practical dataset handling and preprocessing (including optional GPU support and type conversions) for standard ML benchmarking in Julia. Requires the respective packages; input generates dataset arrays and encoded label arrays; output is ready-for-batch data.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-blitz.md#_snippet_15\n\nLANGUAGE: julia\nCODE:\n```\nusing Statistics\nusing Flux\nusing MLDatasets: CIFAR10\nusing ImageCore: colorview, RGB\nusing Flux: onehotbatch, onecold, DataLoader\nusing Plots: plot\nusing MLUtils: splitobs, numobs\n\n# using CUDA # Uncomment if you have CUDA installed. Can also use AMDGPU or Metal instead\n# using AMDGPU\n# using Metal\n```\n\nLANGUAGE: julia\nCODE:\n```\ntrain_x, train_y = CIFAR10(:train)[:]\nlabels = onehotbatch(train_y, 0:9)\n```\n\n----------------------------------------\n\nTITLE: Preparing and Loading MNIST Data for Flux - Julia\nDESCRIPTION: The getdata function loads MNIST train and test splits using MLDataSets, reshapes image matrices to vectors, applies one-hot encoding to labels, and constructs DataLoader objects for training and testing. It sets environmental variables for data download, reshapes data for MLP input, batches and shuffles the data. Requires MNIST dataset and associated Flux/MLUtils utilities. Returns tuple of (train_loader, test_loader).\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-mlp.md#_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\nfunction getdata(args)\n    ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = \"true\"\n\n    # Loading Dataset\t\n    xtrain, ytrain = MNIST(:train)[:]\n    xtest, ytest = MNIST(:test)[:]\n\t\n    # Reshape Data in order to flatten each image into a linear array\n    xtrain = Flux.flatten(xtrain)\n    xtest = Flux.flatten(xtest)\n\n    # One-hot-encode the labels\n    ytrain, ytest = onehotbatch(ytrain, 0:9), onehotbatch(ytest, 0:9)\n\n    # Batching\n    train_loader = DataLoader((xtrain, ytrain), batchsize=args.batchsize, shuffle=true)\n    test_loader = DataLoader((xtest, ytest), batchsize=args.batchsize)\n\n    return train_loader, test_loader\nend\n```\n\n----------------------------------------\n\nTITLE: Inspecting Updated Model Parameters After Single Training Step in Julia\nDESCRIPTION: Displays the altered `weight` and `bias` of the model following one call to `train!`, illustrating the effect of learning on parameter values. Output confirms that training steps do indeed change the model's internal state. Requires access to the trained model instance.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/overview.md#_snippet_10\n\nLANGUAGE: julia\nCODE:\n```\njulia> predict.weight, predict.bias\n(Float32[7.246838;;], Float32[1.748103])\n```\n\n----------------------------------------\n\nTITLE: Defining and Evaluating a Mean Squared Error Loss Function with Flux in Julia\nDESCRIPTION: Defines a custom loss function using mean squared error (MSE) between model predictions and target values, leveraging Julia's Statistics library for the `mean` function. Evaluates the loss on current predictions to assess model accuracy. Dependencies: Flux, Statistics. Key parameters: model, input features, true labels. Output: a floating point scalar quantifying prediction error.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/overview.md#_snippet_5\n\nLANGUAGE: julia\nCODE:\n```\njulia> using Statistics\n\njulia> loss(model, x, y) = mean(abs2.(model(x) .- y));\n\njulia> loss(predict, x_train, y_train)\n122.64734f0\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using a Stateful Optimizer (Momentum) in Julia\nDESCRIPTION: Illustrates how to set up and use a stateful optimizer like `Momentum`. `Flux.setup` is called once before training to initialize the optimizer state (`opt_state`) for the given `model` and rule (`Momentum(0.01, 0.9)`). Inside the loop, after calculating gradients, `Flux.update!` is called to update both the model parameters and the optimizer's internal state (e.g., velocity for momentum).\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/training/training.md#_snippet_5\n\nLANGUAGE: julia\nCODE:\n```\n# Initialise momentum \nopt_state = Flux.setup(Momentum(0.01, 0.9), model)\n\nfor data in train_set\n  grads = [...]\n\n  # Update both model parameters and optimiser state:\n  Flux.update!(opt_state, model, grads[1])\nend\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Epoch Training with `Flux.train!` in Julia\nDESCRIPTION: Shows a standard multi-epoch training structure using the compact `Flux.train!` function. An optimizer (e.g., `Adam`) state is initialized once using `Flux.setup`. The outer loop iterates through epochs, and in each epoch, `Flux.train!` is called to process the entire `train_set`, performing gradient calculation and parameter updates internally based on the provided loss function.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/training/training.md#_snippet_9\n\nLANGUAGE: julia\nCODE:\n```\nopt_state = Flux.setup(Adam(), model)\n\nfor epoch in 1:100\n  Flux.train!(model, train_set, opt_state) do m, x, y\n    loss(m(x), y)\n  end\nend\n```\n\n----------------------------------------\n\nTITLE: Using `Flux.DataLoader` for Batching Training Data in Julia\nDESCRIPTION: Illustrates the use of `Flux.DataLoader` to create mini-batches from data. It takes a tuple of data arrays `(X, Y)` and a `batchsize` (e.g., 32) as input. It returns an iterator (`data`) where each element is a tuple containing a batch of inputs (`x1`) and a batch of corresponding labels (`y1`). The batch dimension is the last dimension of the arrays.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/training/training.md#_snippet_8\n\nLANGUAGE: julia\nCODE:\n```\ndata = Flux.DataLoader((X, Y), batchsize=32)\n\nx1, y1 = first(data)\nsize(x1) == (28, 28, 32)\nlength(data) == 1875 === 60_000 ÷ 32\n```\n\n----------------------------------------\n\nTITLE: Defining and Using a Custom Training Step Function in Julia\nDESCRIPTION: Defines a function `train_custom_model!` that encapsulates a single training step. It takes the loss function, weights, biases, features, and labels as input, calculates gradients using `gradient`, and updates the weights and biases in-place using a learning rate of 0.1 via broadcasting (`@.`). The snippet also calls the function and displays the updated parameters and loss.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_19\n\nLANGUAGE: julia\nCODE:\n```\njulia> function train_custom_model!(f_loss, weights, biases, features, labels)\n           dLdW, dLdb, _, _ = gradient(f_loss, weights, biases, features, labels)\n           @. weights = weights - 0.1 * dLdW\n           @. biases = biases - 0.1 * dLdb\n       end;\n```\n\nLANGUAGE: julia\nCODE:\n```\n\njulia> train_custom_model!(custom_loss, W, b, x, y);\n```\n\nLANGUAGE: julia\nCODE:\n```\n\njulia> W, b, custom_loss(W, b, x, y)\n(Float32[2.340657], Float32[0.7516814], 13.64972f0)\n```\n\n----------------------------------------\n\nTITLE: Implementing the ConvNet Training Loop in Julia with Flux.jl\nDESCRIPTION: Defines the main `train` function that orchestrates the entire training process. It initializes training arguments, loads/processes data, builds the model, moves data and model to the GPU using `gpu()`, and precompiles the model. It defines a nested `loss` function using `logitcrossentropy` with data augmentation. The core logic iterates through epochs, performing training steps using `Flux.train!` with the Adam optimizer. It includes checks for NaN parameters, calculates test accuracy, implements early stopping if target accuracy (0.999) is met, saves the best model parameters to a BSON file when accuracy improves, and implements learning rate decay if accuracy stagnates for 5 epochs. The loop also breaks if no improvement occurs for 10 epochs.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-02-07-convnet.md#_snippet_6\n\nLANGUAGE: julia\nCODE:\n```\nfunction train(; kws...)   \n   args = TrainArgs(; kws...)\n \n   @info(\"Loading data set\")\n   train_set, test_set = get_processed_data(args)\n \n   # Define our model.  We will use a simple convolutional architecture with\n   # three iterations of Conv -> ReLU -> MaxPool, followed by a final Dense layer.\n   @info(\"Building model...\")\n   model = build_model(args)\n \n   # Load model and datasets onto GPU, if enabled\n   train_set = gpu.(train_set)\n   test_set = gpu.(test_set)\n   model = gpu(model)\n  \n   # Make sure our model is nicely precompiled before starting our training loop\n   model(train_set[1][1])\n \n   # `loss()` calculates the crossentropy loss between our prediction `y_hat`\n   # (calculated from `model(x)`) and the ground truth `y`.  We augment the data\n   # a bit, adding gaussian random noise to our image to make it more robust.\n   function loss(x, y)   \n       x̂ = augment(x)\n       ŷ = model(x̂)\n       return logitcrossentropy(ŷ, y)\n   end\n  \n   # Train our model with the given training set using the Adam optimiser and\n   # printing out performance against the test set as we go.\n   opt = Adam(args.lr)\n  \n   @info(\"Beginning training loop...\")\n   best_acc = 0.0\n   last_improvement = 0\n   for epoch_idx in 1:args.epochs\n       # Train for a single epoch\n       Flux.train!(loss, params(model), train_set, opt)\n      \n       # Terminate on NaN\n       if anynan(Flux.params(model))\n           @error \"NaN params\"\n           break\n       end\n  \n       # Calculate accuracy:\n       acc = accuracy(test_set..., model)\n      \n       @info(@sprintf(\"[%d]: Test accuracy: %.4f\", epoch_idx, acc))\n       # If our accuracy is good enough, quit out.\n       if acc >= 0.999\n           @info(\" -> Early-exiting: We reached our target accuracy of 99.9%\")\n           break\n       end\n  \n       # If this is the best accuracy we've seen so far, save the model out\n       if acc >= best_acc\n           @info(\" -> New best accuracy! Saving model out to mnist_conv.bson\")\n           BSON.@save joinpath(args.savepath, \"mnist_conv.bson\") params=cpu.(params(model)) epoch_idx acc\n           best_acc = acc\n           last_improvement = epoch_idx\n       end\n  \n       # If we haven't seen improvement in 5 epochs, drop our learning rate:\n       if epoch_idx - last_improvement >= 5 && opt.eta > 1e-6\n           opt.eta /= 10.0\n           @warn(\" -> Haven't improved in a while, dropping learning rate to $(opt.eta)!\")\n \n           # After dropping learning rate, give it a few epochs to improve\n           last_improvement = epoch_idx\n       end\n  \n       if epoch_idx - last_improvement >= 10\n           @warn(\" -> We're calling this converged.\")\n           break\n       end\n   end\nend\n```\n\n----------------------------------------\n\nTITLE: Splitting and Batching Data for Minibatch Learning with Flux.DataLoader - julia\nDESCRIPTION: This snippet segments the CIFAR-10 dataset into training and validation sets, then organizes both into shuffled and batched DataLoader iterators for efficient minibatch learning. Demonstrates MLUtils.splitobs and DataLoader class from Flux, facilitating scalable and reproducible training. Requires MLUtils and Flux.DataLoader; inputs are (train_x, labels), desired split indices, and batch sizes; outputs are DataLoader objects for iteration.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-blitz.md#_snippet_17\n\nLANGUAGE: julia\nCODE:\n```\ntrainset, valset = splitobs((train_x, labels), at = 45000)\ntrainloader = DataLoader(trainset, batchsize = 1000, shuffle = true)\nvalloader = DataLoader(trainset, batchsize = 1000)\n```\n\n----------------------------------------\n\nTITLE: Moving Flux Models to GPU Using cu() with CUDA.jl in Julia\nDESCRIPTION: Shows how to use cu() to move a pre-defined Flux model from CPU to GPU, including all parameter arrays. Recommends disabling allowscalar for CUDA, and demonstrates functional equivalence of operations on CPU and GPU. Dependencies: Flux.jl, CUDA.jl, cuDNN.jl (added via Pkg.add); input is a Flux model and arrays, output is a model running on GPU arrays.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/gpu.md#_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nusing Pkg; Pkg.add([\"CUDA\", \"cuDNN\"])  # do this once\n\nusing Flux, CUDA\nCUDA.allowscalar(false)  # recommended\n\nmodel = Dense(W, true, tanh)  # wrap the same matrix W in a Flux layer\nmodel(x) \\u2248 y                  # same result, still on CPU\n\nc_model = cu(model)  # move all the arrays within model to the GPU\nc_model(cx)          # computation on the GPU\n\n```\n\n----------------------------------------\n\nTITLE: Defining a Recurrent Model with Flux RNN Layer for Whole Sequence Processing (Julia)\nDESCRIPTION: Shows how to define a struct-based model wrapping a Flux RNN layer and Dense head for efficient, batched sequence processing. All time steps are processed in one call, exploiting Flux's recurrent API (RNN layer). Demonstrates training step setup, including random data generation, model initialization, and updating parameters. Requires Flux.jl and Optimisers.jl. Inputs are typically 3D tensors: (input size, sequence length, batch size).\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/recurrence.md#_snippet_4\n\nLANGUAGE: Julia\nCODE:\n```\nstruct RecurrentModel{H,C,D}\n    h0::H\n    rnn::C\n    dense::D\nend\n\nFlux.@layer RecurrentModel trainable=(rnn, dense)\n\nfunction RecurrentModel(input_size::Int, hidden_size::Int)\n    return RecurrentModel(\n                 zeros(Float32, hidden_size), \n                 RNN(input_size => hidden_size),\n                 Dense(hidden_size => 1))\nend\n\nfunction (m::RecurrentModel)(x)\n    z = m.rnn(x, m.h0)  # [hidden_size, seq_len, batch_size] or [hidden_size, seq_len]\n    ŷ = m.dense(z)      # [1, seq_len, batch_size] or [1, seq_len]\n    return ŷ\nend\n\nseq_len, batch_size, input_size = 3, 4, 2\nx = rand(Float32, input_size, seq_len, batch_size)\ny = rand(Float32, 1, seq_len, batch_size)\n\nmodel = RecurrentModel(input_size, 5)\nopt_state = Flux.setup(AdamW(1e-3), model)\n\ng = gradient(m -> Flux.mse(m(x), y), model)[1]\nFlux.update!(opt_state, model, g)\n```\n\n----------------------------------------\n\nTITLE: Single-Step Model Training Using Flux's train! with Gradient Descent in Julia\nDESCRIPTION: Performs one training iteration with `train!`, updating model parameters using the specified loss, data batch, and optimizer. This operation applies automatic differentiation under the hood and alters the `predict` model's parameters to reduce loss. Requires previously defined model, loss, optimizer, and data batch.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/overview.md#_snippet_8\n\nLANGUAGE: julia\nCODE:\n```\njulia> train!(loss, predict, data, opt)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Flux Model Performance on Test Data in Julia\nDESCRIPTION: This Julia snippet demonstrates how to evaluate a trained Flux model. First, it normalizes the test input data (`x_test`) using `Flux.normalise`. Then, it calculates the loss using a predefined `loss` function, the trained `model`, the normalized test inputs (`x_test_n`), and the corresponding test outputs (`y_test`). The resulting floating-point number represents the model's loss on the test set, indicating its generalization performance. This requires the Flux.jl library and pre-existing `model`, `loss`, `x_test`, and `y_test` variables.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_32\n\nLANGUAGE: jldoctest\nCODE:\n```\njulia> x_test_n = Flux.normalise(x_test);\n\njulia> loss(model, x_test_n, y_test)\n66.91015f0\n```\n\n----------------------------------------\n\nTITLE: Training Loop with On-the-fly Batch GPU Transfer in Flux.jl (Julia)\nDESCRIPTION: Illustrates a recommended training loop for large datasets, where each batch is transferred to the GPU as it is retrieved from the DataLoader. Shows model parameter updates using Flux's gradient and update! functions. Inputs: CPU-resident training data, model, optimizer; outputs: updated model using GPU-transferred batches.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/gpu.md#_snippet_9\n\nLANGUAGE: julia\nCODE:\n```\ntrain_loader = Flux.DataLoader((X, Y), batchsize=64, shuffle=true)\n# ... model definition, optimiser setup\nfor epoch in 1:epochs\n    for (x_cpu, y_cpu) in train_loader\n        x = gpu(x_cpu)\n        y = gpu(y_cpu)\n        grads = gradient(m -> loss(m, x, y), model)\n        Flux.update!(opt_state, model, grads[1])\n    end\n\n```\n\n----------------------------------------\n\nTITLE: Evaluating Trained Model Accuracy using Flux.jl - Julia\nDESCRIPTION: This function, test, evaluates a previously trained convolutional neural network on the MNIST test dataset using Flux.jl in Julia. It accepts keyword arguments to adjust runtime parameters, loads the test set, rebuilds the model architecture, and restores learned parameters from a BSON file. The model and test data are moved to the GPU before computing and displaying test accuracy. Required dependencies include Flux.jl, BSON.jl, GPU support, and project-specific functions such as get_processed_data, build_model, and accuracy. Inputs are runtime keyword arguments; output is the displayed test accuracy. The function assumes access to saved model weights and a compatible environment for GPU computation.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-02-07-convnet.md#_snippet_7\n\nLANGUAGE: julia\nCODE:\n```\nfunction test(; kws...)\n   args = TrainArgs(; kws...)\n  \n   # Loading the test data\n   _,test_set = get_processed_data(args)\n  \n   # Re-constructing the model with random initial weights\n   model = build_model(args)\n  \n   # Loading the saved parameters\n   BSON.@load joinpath(args.savepath, \"mnist_conv.bson\") params\n  \n   # Loading parameters onto the model\n   Flux.loadparams!(model, params)\n  \n   test_set = gpu.(test_set)\n   model = gpu(model)\n   @show accuracy(test_set...,model)\nend\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Model Training Step Function in Julia\nDESCRIPTION: Defines a function `train_custom_model!` that encapsulates a single training step for the custom model. It takes the loss function, weights, biases, features, and labels as input, calculates gradients using `gradient`, and updates the weights and biases in-place using a learning rate of 0.1.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_24\n\nLANGUAGE: julia\nCODE:\n```\n```jldoctest logistic_regression\njulia> function train_custom_model!(f_loss, weights, biases, features, labels_onehot)\n           dLdW, dLdb, _, _ = gradient(f_loss, weights, biases, features, labels_onehot)\n           weights .= weights .- 0.1 .* dLdW\n           biases .= biases .- 0.1 .* dLdb\n       end;\n```\n```\n\n----------------------------------------\n\nTITLE: Encoding Categorical Values with OneHotArrays.jl in Julia\nDESCRIPTION: This snippet demonstrates the encoding of categorical values into one-hot vectors using the function onehot from the OneHotArrays.jl package in Julia. The code assumes the user has installed and imported OneHotArrays.jl. The onehot function takes a label (such as :b) and a list of possible categories, returning a sparse OneHotVector with a true value at the position of the specified label and false elsewhere. Inputs: (label, categories array). Output: OneHotVector representing the label. The output resembles an array of booleans, where a single position is set to 1. Constraints: The categories array must contain the given label.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/data/onehot.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\njulia> using OneHotArrays\n\njulia> onehot(:b, [:a, :b, :c])\n3-element OneHotVector(::UInt32) with eltype Bool:\n ⋅\n 1\n ⋅\n\njulia> onehot(:c, [:a, :b, :c])\n3-element OneHotVector(::UInt32) with eltype Bool:\n ⋅\n ⋅\n 1\n```\n\n----------------------------------------\n\nTITLE: Transferring Data and Model Arrays to GPU with CUDA.jl in Julia\nDESCRIPTION: Demonstrates using CUDA.jl's cu() function for converting arrays from CPU (Array) to GPU (CuArray) along with broadcasting operations. Shows the recursion of cu() into nested structures and highlights the automatic conversion of Float64 to Float32 for GPU efficiency. Requires the CUDA.jl package, and arrays or tuples as input, producing GPU-resident arrays as output.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/gpu.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nW = randn(3, 4)  # some weights, on CPU: 3×4 Array{Float64, 2}\nx = randn(4)     # fake data\ny = tanh.(W * x) # computation on the CPU\n\nusing CUDA\n\ncu(W) isa CuArray{Float32}\n(cW, cx) = (W, x) |> cu  # move both to GPU\ncy = tanh.(cW * cx)      # computation on the GPU\n\n```\n\n----------------------------------------\n\nTITLE: Composing Layers with Function Composition and Computing Gradients - Flux Julia\nDESCRIPTION: This snippet creates a composed model by chaining Layer structs using ∘ (function composition) and then evaluates the model's output for a vector input. The model's gradient is calculated using Flux.gradient. The result is a nested named tuple structure reflecting the parameter hierarchy, demonstrating how composed models yield nested gradients. Requires: Flux.jl, a custom Layer type. Input: Float32 vector. Output: Scalar and hierarchical gradient tuple.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/basics.md#_snippet_18\n\nLANGUAGE: julia\nCODE:\n```\nmodel1 = only ∘ Layer(20, 1) ∘ Layer(1, 20)\n\ny = model1(Float32[0.1])  # output is a Float32 number\n\ngrad = Flux.gradient(|>, [1f0], model1)[2]\n```\n\n----------------------------------------\n\nTITLE: Simplified Training Loop using Flux.train!\nDESCRIPTION: This Julia snippet demonstrates an alternative, more concise way to implement the training loop using the `Flux.train!` convenience function. `Flux.train!` handles the gradient calculation, parameter updates, and iteration over the data loader automatically. The provided block defines the loss calculation within the `do` syntax. This replaces the explicit `withgradient`, `update!`, and inner loop structure.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/quickstart.md#_snippet_4\n\nLANGUAGE: julia\nCODE:\n```\nfor epoch in 1:1_000\n    Flux.train!(model, loader |> device, opt_state) do m, x, y\n        y_hat = m(x)\n        Flux.logitcrossentropy(y_hat, y)\n    end\nend\n```\n\n----------------------------------------\n\nTITLE: Setting up Crossentropy Loss and Momentum Optimizer with Flux.jl in Julia\nDESCRIPTION: This snippet sets up the logit crossentropy loss function and initializes the Momentum optimizer for the defined model using Flux.jl in Julia. The crossentropy loss is suitable for multi-class classification tasks, and the Momentum optimizer helps with adaptive learning rates during training. Prerequisites include Flux.jl and its dependencies being properly installed.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-blitz.md#_snippet_19\n\nLANGUAGE: julia\nCODE:\n```\nusing Flux: logitcrossentropy, Momentum\n\nloss(m, x, y) = logitcrossentropy(m(x), y)\nopt_state = Flux.setup(Momentum(0.01), model)\n```\n\n----------------------------------------\n\nTITLE: Defining a Linear Model with Flux.Dense in Julia\nDESCRIPTION: Creates a linear regression model using `Flux.Dense(13 => 1)`. This defines a dense layer that takes 13 input features (corresponding to the Boston Housing dataset features) and produces 1 output (the predicted house price). Flux automatically initializes the weights and bias, resulting in 14 parameters (13 weights + 1 bias).\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_27\n\nLANGUAGE: julia\nCODE:\n```\njulia> model = Dense(13 => 1)\nDense(13 => 1)      # 14 parameters\n```\n\n----------------------------------------\n\nTITLE: Multi-Epoch Model Training Loop with Flux's train! in Julia\nDESCRIPTION: Implements repeated training over the data batch for 200 epochs, invoking `train!` in a for loop. This structure further reduces loss and converges model parameters toward the optimal solution. Inputs: previously established loss, model, data, optimizer. Output: modifies model in-place, typical for supervised learning workflows.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/overview.md#_snippet_11\n\nLANGUAGE: julia\nCODE:\n```\njulia> for epoch in 1:200\n         train!(loss, predict, data, opt)\n       end\n\njulia> loss(predict, x_train, y_train)\n0.00339581f0\n\njulia> predict.weight, predict.bias\n(Float32[4.0159144;;], Float32[2.004479])\n```\n\n----------------------------------------\n\nTITLE: Defining Feedforward Neural Network Model with Chain and Dense Layers - Flux Julia\nDESCRIPTION: This code defines a multi-layer perceptron model using Flux's high-level Chain and Dense. The model applies two Dense layers with an activation (σ) and finally extracts the scalar output using only. It demonstrates model construction, automatic pretty-printing, and internal storage of parameters. Dependencies: Flux.jl. Inputs: Float32 vectors, with size compatibility needed. Outputs: Scalar or vector predictions. Limitations: Uses default parameter initializers.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/basics.md#_snippet_20\n\nLANGUAGE: julia\nCODE:\n```\nmodel3 = Chain(Dense(1 => 20, σ), Dense(20 => 1), only)\n```\n\n----------------------------------------\n\nTITLE: Building a Convolutional Neural Network (ConvNet) with Flux.jl in Julia\nDESCRIPTION: Defines the `build_model` function that constructs the ConvNet architecture using `Flux.Chain`. The model takes 28x28x1 grayscale images as input. It consists of three sequential blocks, each containing a 3x3 Convolution layer (`Conv`) with ReLU activation and padding, followed by a 2x2 Max Pooling layer (`MaxPool`). The number of channels increases from 1 to 16, then 32, and stays at 32. Finally, the output is flattened (`Flux.flatten`) and passed through a Dense layer to produce 10 output values corresponding to the digit classes.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-02-07-convnet.md#_snippet_4\n\nLANGUAGE: julia\nCODE:\n```\nfunction build_model(args; imgsize = (28,28,1), nclasses = 10)\n   cnn_output_size = Int.(floor.([imgsize[1]/8,imgsize[2]/8,32])) \n \n   return Chain(\n   # First convolution, operating upon a 28x28 image\n   Conv((3, 3), imgsize[3]=>16, pad=(1,1), relu),\n   MaxPool((2,2)),\n \n   # Second convolution, operating upon a 14x14 image\n   Conv((3, 3), 16=>32, pad=(1,1), relu),\n   MaxPool((2,2)),\n \n   # Third convolution, operating upon a 7x7 image\n   Conv((3, 3), 32=>32, pad=(1,1), relu),\n   MaxPool((2,2)),\n \n   # Reshape 3d array into a 2d one using `Flux.flatten`, at this point it should be (3, 3, 32, N)\n   flatten,\n   Dense(prod(cnn_output_size), 10))\nend\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Loss Function in Julia\nDESCRIPTION: Provides an example of defining a simple custom loss function, mean squared error, in Julia. The function `loss` takes the model's prediction `y_hat` and the true label `y` as input and returns a scalar value representing the error. This function would typically be called within the `Flux.gradient` computation.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/training/training.md#_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\nloss(y_hat, y) = sum((y_hat .- y).^2)\n```\n\n----------------------------------------\n\nTITLE: DataLoader-wide GPU Transfer for Training with Flux.jl (Julia)\nDESCRIPTION: Demonstrates a more concise approach where the entire DataLoader is wrapped with gpu(), transferring all batches automatically to the GPU. Model updates are performed in the same loop, mirroring the manual approach but improving code simplicity. Inputs: training data, model, optimizer; DataLoader is GPU-enabled.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/gpu.md#_snippet_10\n\nLANGUAGE: julia\nCODE:\n```\ngpu_train_loader = Flux.DataLoader((X, Y), batchsize=64, shuffle=true) |> gpu\n# ... model definition, optimiser setup\nfor epoch in 1:epochs\n    for (x, y) in gpu_train_loader\n        grads = gradient(m -> loss(m, x, y), model)\n        Flux.update!(opt_state, model, grads[1])\n    end\n\n```\n\n----------------------------------------\n\nTITLE: Verifying Trained Model Predictions on Test Data in Julia\nDESCRIPTION: Applies the trained `predict` model to unseen test inputs (`x_test`) and directly compares predictions to the known expected outputs (`y_test`). Outputs should closely match if training succeeded. Inputs: trained model and test data; outputs: prediction and true label matrices for evaluation.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/overview.md#_snippet_12\n\nLANGUAGE: julia\nCODE:\n```\njulia> predict(x_test)\n1×5 Matrix{Float32}:\n 26.1121  30.13  34.1479  38.1657  42.1836\n\njulia> y_test\n1×5 Matrix{Int64}:\n 26  30  34  38  42\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Convergence-Based Training Loop in Julia\nDESCRIPTION: Implements a custom training loop using a `while true` construct. The loop repeatedly calls `train_model!` to update the `model`. It tracks the loss (`loss_init`) and breaks the loop when the absolute difference between the current loss and the previous loss falls below a small threshold (`1e-4`), effectively stopping training when the loss converges.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_30\n\nLANGUAGE: julia\nCODE:\n```\njulia> loss_init = Inf;\n```\n\nLANGUAGE: julia\nCODE:\n```\n\njulia> while true\n           train_model!(loss, model, x_train_n, y_train)\n           if loss_init == Inf\n               loss_init = loss(model, x_train_n, y_train)\n               continue\n           end\n           if abs(loss_init - loss(model, x_train_n, y_train)) < 1e-4\n               break\n           else\n               loss_init = loss(model, x_train_n, y_train)\n           end\n       end;\n```\n\n----------------------------------------\n\nTITLE: Defining MSE Loss Function for Flux Model in Julia\nDESCRIPTION: Defines a `loss` function that takes the Flux `model`, `features`, and `labels` as input. Inside the function, it calculates the model's predictions (`ŷ = model(features)`) and then computes the Mean Squared Error (MSE) between the predictions and the true labels using `Flux.mse`. The snippet also demonstrates calculating the initial loss on the normalized training data.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_28\n\nLANGUAGE: julia\nCODE:\n```\njulia> function loss(model, features, labels)\n           ŷ = model(features)\n           Flux.mse(ŷ, labels)\n       end;\n```\n\nLANGUAGE: julia\nCODE:\n```\n\njulia> loss(model, x_train_n, y_train)\n676.1656f0\n```\n\n----------------------------------------\n\nTITLE: Transferring Trained Flux Models Back to CPU Before Saving in Julia\nDESCRIPTION: Emphasizes the necessity to move trained models from GPU to CPU using cpu() before serializing with BSON.@save. Demonstrates both direct reassignment and scoped let-block assignment to ensure model is on CPU for saving. Requires the BSON.jl package for serialization. Inputs: Flux model on GPU; outputs: model on CPU, saved to disk.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/gpu.md#_snippet_12\n\nLANGUAGE: julia\nCODE:\n```\nmodel = cpu(model) # or model = model |> cpu\n\n```\n\nLANGUAGE: julia\nCODE:\n```\nusing BSON\n# ...\nBSON.@save \"./path/to/trained_model.bson\" model\n\n# in this approach the cpu-transferred model (referenced by the variable `model`)\n# only exists inside the `let` statement\nlet model = cpu(model)\n   # ...\n   BSON.@save \"./path/to/trained_model.bson\" model\nend\n\n```\n\n----------------------------------------\n\nTITLE: Constructing an RNNCell Layer Using Flux.jl (Julia)\nDESCRIPTION: Instantiates a vanilla RNNCell using Flux's built-in layer, initializes parameters for sequence length, input/output sizes, and simulates a single-sequence forward pass. Uses the Flux deep learning framework and its RNNCell abstraction, requiring `using Flux`. Assumes random input data, processes sequentially to collect outputs. Inputs are vectors; state is updated at each time step and collected outputs stored in a list.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/recurrence.md#_snippet_1\n\nLANGUAGE: Julia\nCODE:\n```\nusing Flux\n\noutput_size = 5\ninput_size = 2\nseq_len = 3\nx = [rand(Float32, input_size) for i = 1:seq_len] \nh0 = zeros(Float32, output_size) \n\nrnn_cell = Flux.RNNCell(input_size => output_size)\n\ny = []\nht = h0\nfor xt in x\n    yt, ht = rnn_cell(xt, ht)\n    y = [y; [yt]]\nend\n```\n\n----------------------------------------\n\nTITLE: Curve Fitting with Custom Loss Function and Training Loop - Flux Julia\nDESCRIPTION: This snippet creates synthetic curve-fitting data and fits a two-layer neural network model to approximate the function f(x) = 2x - x^3. It generates training data, iterates for 1000 steps, and uses Flux.train! with a squared error loss and simple gradient descent. Dependencies: Flux.jl. Inputs: Vector-of-input/target pairs. Outputs: Trained model parameters. Constraints: The loss function must return a scalar and the model must accept the format data provides.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/basics.md#_snippet_22\n\nLANGUAGE: julia\nCODE:\n```\ndata = [([x], 2x-x^3) for x in -2:0.1f0:2]  # training points (x, y)\n\nfor _ in 1:1000  # adjust parameters to minimise the error:\n  Flux.train!((m,x,y) -> (m(x) - y)^2, model3, data, Descent(0.01))\nend\n```\n\n----------------------------------------\n\nTITLE: Defining and Calculating Flux Model Accuracy in Julia\nDESCRIPTION: Defines a function `flux_accuracy` to calculate the accuracy of the Flux model. It takes features (`x`) and original labels (`y`). It generates predictions using the `flux_model`, converts predictions to class names using `Flux.onecold` and the `classes` list, compares them to the true labels, and calculates the mean accuracy. The snippet also shows the accuracy calculation for the untrained Flux model.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_20\n\nLANGUAGE: julia\nCODE:\n```\n```jldoctest logistic_regression; filter = r\"[+-]?([0-9]*[.])?[0-9]+(f[+-]*[0-9])?\"\njulia> flux_accuracy(x, y) = mean(Flux.onecold(flux_model(x), classes) .== y);\n\njulia> flux_accuracy(x, y)\n0.24\n```\n```\n\n----------------------------------------\n\nTITLE: Creating DataLoader from Pre-Transferred GPU Data for Flux.jl Training (Julia)\nDESCRIPTION: Shows how to transfer the entire dataset to the GPU before constructing the DataLoader—optimal for smaller datasets. DataLoader batches are already on GPU, reducing transfer overhead inside training loops. Inputs: arrays X, Y on CPU; outputs: DataLoader batches on GPU.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/gpu.md#_snippet_11\n\nLANGUAGE: julia\nCODE:\n```\ngpu_train_loader = Flux.DataLoader((X, Y) |> gpu, batchsize = 32)\n# ...\nfor epoch in 1:epochs\n    for (x, y) in gpu_train_loader\n        # ...\n\n```\n\n----------------------------------------\n\nTITLE: Checking GPU Functionality with Metal in Julia\nDESCRIPTION: This code snippet checks whether Metal GPU support is available and functional using the Metal.jl library in Julia. Users must install Metal.jl and have a compatible Apple system with a Metal-supported GPU. The code imports Metal, then calls \\\"Metal.functional()\\\" which returns true if a compatible GPU is detected, otherwise false. Use these commands in the Julia REPL or scripts to verify Metal GPU setup.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/gpu.md#_snippet_15\n\nLANGUAGE: julia-repl\nCODE:\n```\njulia> using Metal\n\njulia> Metal.functional()\ntrue\n```\n\n----------------------------------------\n\nTITLE: Defining a RecurrentCell Model Combining RNNCell and Dense Layer (Julia)\nDESCRIPTION: Implements a Julia struct-based recurrent model that chains an RNNCell and Dense output layer. Utilizes Flux structure macros and annotations to register trainable parameters. The model processes sequences of matrices (one per time step), keeping the hidden state untrained, collects and stacks outputs for batch vectorization. Inputs are batched matrices per time step, and outputs are tensors shaped by hidden and batch sizes. Dependencies include Flux.jl.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/recurrence.md#_snippet_2\n\nLANGUAGE: Julia\nCODE:\n```\nstruct RecurrentCellModel{H,C,D}\n    h0::H\n    cell::C\n    dense::D\nend\n\n# we choose to not train the initial hidden state\nFlux.@layer RecurrentCellModel trainable=(cell, dense) \n\nfunction RecurrentCellModel(input_size::Int, hidden_size::Int)\n    return RecurrentCellModel(\n                 zeros(Float32, hidden_size), \n                 RNNCell(input_size => hidden_size),\n                 Dense(hidden_size => 1))\nend\n\nfunction (m::RecurrentCellModel)(x)\n    z = []\n    ht = m.h0\n    for xt in x\n        yt, ht = m.cell(xt, ht)\n        z = [z; [yt]]\n    end\n    z = stack(z, dims=2) # [hidden_size, seq_len, batch_size] or [hidden_size, seq_len]\n    ŷ = m.dense(z)       # [1, seq_len, batch_size] or [1, seq_len]\n    return ŷ\nend\n```\n\n----------------------------------------\n\nTITLE: Splitting Data into Training and Testing Sets in Julia\nDESCRIPTION: Splits the Boston Housing dataset (`x`, `y`) into training (`x_train`, `y_train`) and testing (`x_test`, `y_test`) subsets using simple array slicing. The first 400 samples are used for training, and the remaining 106 for testing. It also displays the dimensions of the resulting arrays.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_24\n\nLANGUAGE: julia\nCODE:\n```\njulia> x_train, x_test, y_train, y_test = x[:, 1:400], x[:, 401:end], y[:, 1:400], y[:, 401:end];\n```\n\nLANGUAGE: julia\nCODE:\n```\n\njulia> x_train |> size, x_test |> size, y_train |> size, y_test |> size\n((13, 400), (13, 106), (1, 400), (1, 106))\n```\n\n----------------------------------------\n\nTITLE: Wrapping a Custom RNNCell with Recurrence Layer in Flux.jl (Julia)\nDESCRIPTION: Demonstrates using the Recurrence utility to process an entire sequence with a recurrent cell. Effectively behaves like built-in RNN or LSTM layers but allows custom or nonstandard cells. Input is a tensor (input size, sequence length, batch size), output is a tensor collected over the sequence. Requires Flux.jl imports.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/recurrence.md#_snippet_5\n\nLANGUAGE: Julia\nCODE:\n```\nrnn = Recurrence(LSTMCell(2 => 3))   # similar to LSTM(2 => 3)\nx = rand(Float32, 2, 4, 3)\ny = rnn(x)\n```\n\n----------------------------------------\n\nTITLE: Training a Simple Flux Model with Predefined Data Batches - julia\nDESCRIPTION: These snippets generate random training data and labels, define a cross-entropy loss, and use Flux's train! function to perform a training step for the model using a single minibatch. The process demonstrates basic dataset preparation, model training loop, and optimizer integration. Requires Flux; inputs are model, data, labels, and optimizer state; outputs are updated model parameters.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-blitz.md#_snippet_13\n\nLANGUAGE: julia\nCODE:\n```\ndata, labels = rand(10, 100), fill(0.5, 2, 100)\nloss(m, x, y) = logitcrossentropy(m(x), y)\nFlux.train!(loss, model, [(data, labels)], opt_state)\n```\n\n----------------------------------------\n\nTITLE: L2 Regularisation Across Trainable Parameters in Flux.jl (Julia)\nDESCRIPTION: This snippet shows a scalable approach to L2 regularisation in Flux.jl by iterating over all trainable parameters using `Flux.trainables`. The `pen_l2` function computes the L2 penalty for each parameter, and the total penalty is added to the loss. This method works for arbitrarily large models by abstracting away manual parameter access. The model, data, and custom loss must be defined, and the penalty scale (0.42) may require adjustment for realistic scenarios.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/training/training.md#_snippet_12\n\nLANGUAGE: julia\nCODE:\n```\npen_l2(x::AbstractArray) = sum(abs2, x)/2\n\ngrads = Flux.gradient(model) do m\n  result = m(input)\n  penalty = sum(pen_l2, Flux.trainables(m))\n  my_loss(result, label) + 0.42f0 * penalty\nend\n```\n\n----------------------------------------\n\nTITLE: Computing Function Values Alongside Gradients in Julia with Flux.withgradient\nDESCRIPTION: This snippet uses Flux.withgradient to simultaneously compute the function value and its gradients for a callable struct model. Both value and gradient are returned in a structured format, supporting detailed inspection and use in optimization. Required dependencies: Flux.jl, model definitions. Inputs: function, numeric arguments; outputs: NamedTuple with value and gradient components. Supports complex model parameterizations.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/basics.md#_snippet_13\n\nLANGUAGE: julia\nCODE:\n```\njulia> Flux.withgradient((x,p) -> p(x), 5.0, poly3s)\n(val = 17.5, grad = (2.0, (θ3 = [1.0, 5.0, 25.0],)))\n```\n\n----------------------------------------\n\nTITLE: Training a Neural Network Classifier with Gradient Descent Loop in Flux.jl (Julia)\nDESCRIPTION: Illustrates the main training loop for the classifier over 10 epochs, using Flux.jl's gradient utilities and Momentum optimizer. The loop processes data in batches, calculates loss and gradients, and updates model parameters. It also reports accuracy on the validation set after each epoch. Ensure all dependencies for Flux, CUDA (if using GPU), and data loaders are set up.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-blitz.md#_snippet_21\n\nLANGUAGE: julia\nCODE:\n```\nepochs = 10\n\nfor epoch in 1:epochs\n    for batch in trainloader\n        x, y = batch |> gpu\n        g = gradient(model) do m \n            loss(m, x, y)\n        end[1]\n        Flux.update!(opt_state, model, g)\n    end\n    @show accuracy(model, valloader)\nend\n```\n\n----------------------------------------\n\nTITLE: Training with a Composed Optimizer (WeightDecay + Descent) in Flux.jl\nDESCRIPTION: Demonstrates a complete training loop using a composed optimizer (`OptimiserChain` with `WeightDecay` and `Descent`). It initializes weights, sets up the optimizer state using `Flux.setup`, defines a simple MSE loss function, calculates gradients using `gradient`, and updates the weights using `Flux.update!`. This shows the practical application of combining optimization rules.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/training/optimisers.md#_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\nw = [randn(10, 10), randn(10, 10)]\nopt_state = Flux.setup(opt, w)\n\nloss(w, x) = Flux.mse(w[1] * x, w[2] * x)\n\nloss(w, rand(10)) # around 0.9\n\nfor t = 1:10^5\n  g = gradient(w -> loss(w[1], w[2], rand(10)), w)\n  Flux.update!(opt_state, w, g)\nend\n\nloss(w, rand(10)) # around 0.9\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Flux MLP - Julia\nDESCRIPTION: Imports the necessary libraries including Flux.jl for neural networks, Statistics, MLUtils, and MLDatasets for accessing the MNIST data. Optionally, enables GPU support by uncommenting the CUDA line. These modules are prerequisites for all subsequent code, providing essential tools for data manipulation, loading, and model construction.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-mlp.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nusing Flux, Statistics\nusing Flux: DataLoader\nusing Flux: onehotbatch, onecold, logitcrossentropy\n# using CUDA # Uncomment this line if you have a nvidia GPU. Also AMDGPU and Metal are supported.\nusing MLDatasets: MNIST\nusing MLUtils\n```\n\n----------------------------------------\n\nTITLE: Defining and Calculating Custom Model Loss in Julia\nDESCRIPTION: Defines a function `custom_loss` that calculates the loss for a custom model using the previously defined `custom_logitcrossentropy`. It takes model weights, biases, input features, and one-hot encoded labels, makes predictions using `custom_model`, and returns the loss value. The snippet also demonstrates calling this function with example data (`W`, `b`, `x`, `custom_y_onehot`) and shows the resulting loss.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_14\n\nLANGUAGE: julia\nCODE:\n```\n```jldoctest logistic_regression; filter = r\"[+-]?([0-9]*[.])?[0-9]+(f[+-]*[0-9])?\"\njulia> function custom_loss(weights, biases, features, labels_onehot)\n           ŷ = custom_model(weights, biases, features)\n           custom_logitcrossentropy(ŷ, labels_onehot)\n       end;\n\njulia> custom_loss(W, b, x, custom_y_onehot)\n1.1714406827505623\n```\n```\n\n----------------------------------------\n\nTITLE: Combining Weight Decay and Adam with OptimiserChain in Flux.jl (Julia)\nDESCRIPTION: This snippet sets up an optimizer state that applies both weight decay regularisation and Adam updates using Flux.jl's `OptimiserChain`. `WeightDecay(0.42)` augments the gradients by adding a scaled version of the parameters, followed by standard Adam updates. The `model` must be defined, and `Flux`, as well as optimizers from `Optimisers`, are required. This method is scalable and cleanly separates regularisation logic from the loss function.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/training/training.md#_snippet_14\n\nLANGUAGE: julia\nCODE:\n```\ndecay_opt_state = Flux.setup(OptimiserChain(WeightDecay(0.42), Adam(0.1)), model)\n```\n\n----------------------------------------\n\nTITLE: Defining Training Parameters with Struct - Julia\nDESCRIPTION: Defines an Args mutable struct with keyword defaults for training configuration: learning rate, batch size, number of epochs, and GPU usage. Allows customization through struct instantiation and provides default values suitable for running the MLP training scripts. Intended as a centralized config for model and data setup.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-mlp.md#_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nBase.@kwdef mutable struct Args\n    rate::Float64 = 3e-4    # learning rate\n    batchsize::Int = 1024   # batch size\n    epochs::Int = 10        # number of epochs\n    usegpu::Bool = true\nend\n```\n\n----------------------------------------\n\nTITLE: Computing Gradients for All Model Parameters in Flux - julia\nDESCRIPTION: This block uses Flux and several submodules to create a two-layer neural network, defines a cross-entropy loss, computes the gradient of the loss with respect to all model parameters, and prints each parameter's path and its gradient. It demonstrates structured parameter access and automatic differentiation for complex models. Requires Flux and dependencies (logitcrossentropy, trainables, getkeypath). Inputs are a model and input data; outputs are a mapping from parameter (with key) to gradient arrays.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-blitz.md#_snippet_10\n\nLANGUAGE: julia\nCODE:\n```\nusing Flux\nusing Flux: logitcrossentropy, trainables, getkeypath\n\nx = rand(Float32, 10)\nmodel = Chain(Dense(10 => 5, relu), Dense(5 => 2))\nloss(model, x) = logitcrossentropy(model(x), [0.5, 0.5])\ngrad = gradient(m -> loss(m, x), model)[1]\nfor (k, p) in trainables(model, path=true)\n    println(\"$k  => $(getkeypath(grad, k))\")\nend\n```\n\n----------------------------------------\n\nTITLE: Checkpointing a Flux Model during Training using JLD2 in Julia\nDESCRIPTION: Demonstrates how to periodically save the state of a Flux model (`m`) during a training loop (e.g., every epoch) to a file named `model-checkpoint.jld2` using `Flux.state` and `jldsave` from JLD2.jl. This allows resuming training if interrupted.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/saving.md#_snippet_2\n\nLANGUAGE: jldoctest saving\nCODE:\n```\njulia> using Flux: throttle\n\njulia> using JLD2\n\njulia> m = Chain(Dense(10 => 5, relu), Dense(5 => 2))\nChain(\n  Dense(10 => 5, relu),                 # 55 parameters\n  Dense(5 => 2),                        # 12 parameters\n)                   # Total: 4 arrays, 67 parameters, 476 bytes.\n\njulia> for epoch in 1:10\n          # ... train model ...\n          jldsave(\"model-checkpoint.jld2\", model_state = Flux.state(m))\n       end;\n\n```\n\n----------------------------------------\n\nTITLE: Differentiating Custom Taylor Series Functions - julia\nDESCRIPTION: This snippet implements a Taylor expansion-based approximation for sin(x), evaluates its value and its derivative using Flux_gradient, and compares it to Julia's built-in sin and cos functions. Demonstrates how user-defined, nonlinear functions are supported by Zygote/Flux for differentiation, including working with sums and generator expressions. Dependence on Flux; input is a scalar (x), outputs are the function value and its gradient, which should approximate cos(x).\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-blitz.md#_snippet_6\n\nLANGUAGE: julia\nCODE:\n```\nmysin(x) = sum((-1)^k*x^(1+2k)/factorial(1+2k) for k in 0:5)\n\nx = 0.5\n\nmysin(x), gradient(mysin, x)\n\nsin(x), cos(x)\n```\n\n----------------------------------------\n\nTITLE: Exporting DataLoader from MLUtils in Julia\nDESCRIPTION: This snippet indicates that the DataLoader type from MLUtils is made available in Flux for batching and iterating over datasets, particularly to provide mini-batches in the format expected by Flux.train!. It serves to bridge the MLUtils data handling with Flux's training routines. No specific input or output is presented in code, but the main dependency is MLUtils.DataLoader, and it's primarily used for managing and iterating through training data in batches.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/data/mlutils.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nMLUtils.DataLoader\n```\n\n----------------------------------------\n\nTITLE: Defining and Calculating Flux Model Loss in Julia\nDESCRIPTION: Defines a function `flux_loss` that calculates the loss for a Flux model. It takes the Flux model, input features, and one-hot encoded labels, makes predictions using the model, and computes the loss using `Flux.logitcrossentropy`. The snippet also demonstrates calling this function with a `flux_model`, features `x`, and `flux_y_onehot` labels, showing the resulting loss.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_15\n\nLANGUAGE: julia\nCODE:\n```\n```jldoctest logistic_regression; filter = r\"[+-]?([0-9]*[.])?[0-9]+(f[+-]*[0-9])?\"\njulia> function flux_loss(flux_model, features, labels_onehot)\n           ŷ = flux_model(features)\n           Flux.logitcrossentropy(ŷ, labels_onehot)\n       end;\n\njulia> flux_loss(flux_model, x, flux_y_onehot)\n1.2156688659673647\n```\n```\n\n----------------------------------------\n\nTITLE: Loading and Accessing the Iris Dataset in Julia\nDESCRIPTION: Loads the Iris dataset using MLDatasets.jl. The `Iris()` function fetches the dataset metadata and structure. `Iris(as_df=false)[:]` unpacks it into features (`x`) and targets (`y`) as native Julia arrays (Matrix), rather than DataFrames, suitable for machine learning tasks.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\njulia> Iris()\ndataset Iris:\n  metadata   =>    Dict{String, Any} with 4 entries\n  features   =>    150×4 DataFrame\n  targets    =>    150×1 DataFrame\n  dataframe  =>    150×5 DataFrame\n\njulia> x, y = Iris(as_df=false)[:];\n```\n\n----------------------------------------\n\nTITLE: Training Loop for Custom Model in Julia\nDESCRIPTION: Implements a training loop for the custom logistic regression model. It iterates up to 500 times, calling `train_custom_model!` in each iteration to update parameters `W` and `b`. The loop includes an early stopping condition, breaking if the `custom_accuracy` reaches or exceeds 0.98. Finally, it displays the accuracy achieved after training.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_25\n\nLANGUAGE: julia\nCODE:\n```\n```jldoctest logistic_regression; filter = r\"[+-]?([0-9]*[.])?[0-9]+(f[+-]*[0-9])?\"\njulia> for i = 1:500\n            train_custom_model!(custom_loss, W, b, x, custom_y_onehot);\n            custom_accuracy(W, b, x, y) >= 0.98 && break\n       end\n\njulia> @show custom_accuracy(W, b, x, y);\ncustom_accuracy(W, b, x, y) = 0.98\n```\n```\n\n----------------------------------------\n\nTITLE: Custom StackedRNN Struct for Manual Layer State Handling in Flux.jl (Julia)\nDESCRIPTION: Outlines a custom struct-based model for stacked recurrent layers, giving explicit control over initial hidden states per layer. Useful for specialized training or initial state customization. Automatically creates layers and their states in the constructor, processes input sequentially through each layer with respective states. Inputs are (input size, sequence length) tensors. Dependencies include Flux's LSTM and initialstates utilities.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/recurrence.md#_snippet_7\n\nLANGUAGE: Julia\nCODE:\n```\nstruct StackedRNN{L,S}\n    layers::L\n    states0::S\nend\n\nFlux.@layer StackedRNN\n\nfunction StackedRNN(d::Int; num_layers::Int)\n    layers = [LSTM(d => d) for _ in 1:num_layers]\n    states0 = [Flux.initialstates(l) for l in layers]\n    return StackedRNN(layers, states0)\nend\n\nfunction (rnn::StackedRNN)(x)\n   for (layer, state0) in zip(rnn.layers, rnn.states0)\n       x = layer(x, state0) \n   end\n   return x\nend\n\nrnn = StackedRNN(3; num_layers=2)\nx = rand(Float32, 3, 10)\ny = rnn(x)\n```\n\n----------------------------------------\n\nTITLE: Loading Flux Model State using Flux.loadmodel! and JLD2 in Julia\nDESCRIPTION: Loads the previously saved model state from `mymodel.jld2` using JLD2.jl, creates a new instance of the `MyModel` struct (requires the struct definition to be available), and then restores the saved state into the new model instance using `Flux.loadmodel!`. Note the warning about potential issues loading GPU-trained models without GPU support.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/saving.md#_snippet_1\n\nLANGUAGE: jldoctest saving\nCODE:\n```\njulia> using Flux, JLD2\n\njulia> model_state = JLD2.load(\"mymodel.jld2\", \"model_state\");\n\njulia> model = MyModel(); # MyModel definition must be available\n\njulia> Flux.loadmodel!(model, model_state);\n\n```\n\n----------------------------------------\n\nTITLE: Adjusting Learning Rate Dynamically in a Flux.jl Training Loop (Julia)\nDESCRIPTION: This code block illustrates how to modify the learning rate for an optimizer mid-training using `Flux.adjust!`. The optimizer state is initialized with Adam at 0.1, and after 100 epochs, the learning rate is changed to 0.01. Requires a defined `model`, training routine `train!`, data loader, and the Flux library. This enables scheduled learning rate decay or similar strategies inside training epochs for better control of optimization dynamics.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/training/training.md#_snippet_15\n\nLANGUAGE: julia\nCODE:\n```\nopt_state = Flux.setup(Adam(0.1), model)  # initialise once\n\nfor epoch in 1:1000\n  train!([...], state)  # Train with η = 0.1 for first 100,\n  if epoch == 100       # then change to use η = 0.01 for the rest.\n    Flux.adjust!(opt_state, 0.01)\n  end\nend\n```\n\n----------------------------------------\n\nTITLE: One-Hot Encoding Iris Targets using FluxML's OneHotArrays.jl in Julia\nDESCRIPTION: Defines the class labels explicitly as a constant vector `classes`. Uses the `onehotbatch` function from `OneHotArrays.jl` (part of the FluxML ecosystem) to efficiently perform one-hot encoding on the target vector `y` based on the defined `classes`, creating a memory-efficient `OneHotMatrix`.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_4\n\nLANGUAGE: julia\nCODE:\n```\njulia> const classes = [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"];\n\njulia> flux_y_onehot = onehotbatch(y, classes)\n3×150 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n 1  1  1  1  1  1  1  1  1  1  1  1  1  …  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     1  1  1  1  1  1  1  1  1  1  1  1\n```\n\n----------------------------------------\n\nTITLE: Implementing Model Accuracy Calculation with Flux.jl in Julia\nDESCRIPTION: Defines an accuracy function to evaluate classification performance using Flux.jl. The function iterates through a data loader, applies the model to batches, compares predicted and true labels using `onecold`, and returns the overall accuracy. The use of gpu ensures data is processed on compatible GPU hardware. Data must be in a compatible loader format.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-blitz.md#_snippet_20\n\nLANGUAGE: julia\nCODE:\n```\nfunction accuracy(model, loader)\n    n = 0\n    acc = 0\n    for batch in loader\n        x, y = batch |> gpu\n        ŷ = model(x)\n        acc += sum(onecold(ŷ) .== onecold(y))\n        n += numobs(x)\n    end\n    return acc / n\nend\n```\n\n----------------------------------------\n\nTITLE: Defining Discriminator Loss Function - DCGAN with Flux.jl - Julia\nDESCRIPTION: Implements the binary cross-entropy loss (using logits) for evaluating discriminator performance in distinguishing real vs. fake images. Uses Flux's logitbinarycrossentropy for numerical stability. Expects outputs from the discriminator\\'s forward pass. Outputs a scalar representing summed loss across real and fake assessments.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-10-08-dcgan-mnist.md#_snippet_9\n\nLANGUAGE: julia\nCODE:\n```\nfunction discriminator_loss(real_output, fake_output)\n    real_loss = logitbinarycrossentropy(real_output, 1)\n    fake_loss = logitbinarycrossentropy(fake_output, 0)\n    return real_loss + fake_loss\nend\n```\n\n----------------------------------------\n\nTITLE: Calculating Gradients for Custom Loss using Flux/Zygote in Julia\nDESCRIPTION: Demonstrates calculating the gradients of the `custom_loss` function with respect to its parameters (weights `W`, biases `b`, features `x`, labels `custom_y_onehot`) using the `gradient` function provided by Flux (which re-exports Zygote.jl's `gradient`). It assigns the gradients with respect to weights and biases to `dLdW` and `dLdb`, ignoring the gradients for features and labels.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_21\n\nLANGUAGE: julia\nCODE:\n```\n```jldoctest logistic_regression\njulia> dLdW, dLdb, _, _ = gradient(custom_loss, W, b, x, custom_y_onehot);\n```\n```\n\n----------------------------------------\n\nTITLE: Defining Generator Loss Function - DCGAN with Flux.jl - Julia\nDESCRIPTION: Specifies a loss for the generator network, encouraging it to produce images that the discriminator classifies as real (label 1). Uses Flux\\'s logitbinarycrossentropy for stable calculation. Takes the discriminator\\'s logits on fake images as input and outputs a scalar loss value.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-10-08-dcgan-mnist.md#_snippet_10\n\nLANGUAGE: julia\nCODE:\n```\ngenerator_loss(fake_output) = logitbinarycrossentropy(fake_output, 1)\n```\n\n----------------------------------------\n\nTITLE: Automatic GPU and CPU Selection with gpu() and cpu() in Flux.jl (Julia)\nDESCRIPTION: Describes the implementation of gpu() and cpu() in Flux.jl, which automatically choose the device to transfer data to based on available backend packages and detected hardware. gpu() calls gpu_device(), while cpu() calls cpu_device(). This enables backend-agnostic model and data transfers at runtime. Inputs: arrays or models; outputs: transferred to the detected (or default) device.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/gpu.md#_snippet_7\n\nLANGUAGE: julia\nCODE:\n```\ngpu(x) = gpu_device()(x)\ncpu(x) = cpu_device()(x)\n\n```\n\n----------------------------------------\n\nTITLE: Defining a Training Step Function for Flux Model in Julia\nDESCRIPTION: Defines a function `train_model!` specifically for training the Flux `model`. It calculates the gradient of the `loss` function with respect to the `model`'s parameters using `gradient`. It then updates the model's weights (`model.weight`) and bias (`model.bias`) in-place using these gradients (`dLdm.weight`, `dLdm.bias`), broadcasting (`@.`), and a small learning rate (0.000001).\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_29\n\nLANGUAGE: julia\nCODE:\n```\njulia> function train_model!(f_loss, model, features, labels)\n           dLdm, _, _ = gradient(f_loss, model, features, labels)\n           @. model.weight = model.weight - 0.000001 * dLdm.weight\n           @. model.bias = model.bias - 0.000001 * dLdm.bias\n       end;\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing Test Data using Flux.jl Data Utilities in Julia\nDESCRIPTION: This snippet loads the CIFAR10 test set, one-hot encodes the labels, and creates a DataLoader for batch processing during evaluation. It demonstrates standard preprocessing steps required before model validation. Dependencies include a compatible DataLoader utility and the CIFAR10 dataset.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-blitz.md#_snippet_22\n\nLANGUAGE: julia\nCODE:\n```\ntest_x, test_y = CIFAR10(:test)[:]\ntest_labels = onehotbatch(test_y, 0:9)\ntestloader = DataLoader((test_x, test_labels), batchsize = 1000, shuffle = true)\n```\n\n----------------------------------------\n\nTITLE: Checking Custom Loss After Parameter Update in Julia\nDESCRIPTION: Calculates and displays the value of the `custom_loss` function using the updated weights (`W`) and biases (`b`) after performing one step of gradient descent. This is done to verify that the loss has decreased.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_23\n\nLANGUAGE: julia\nCODE:\n```\n```jldoctest logistic_regression; filter = r\"[+-]?([0-9]*[.])?[0-9]+(f[+-]*[0-9])?\"\njulia> custom_loss(W, b, x, custom_y_onehot)\n1.164742997664842\n```\n```\n\n----------------------------------------\n\nTITLE: Checking Custom Loss After Training in Julia\nDESCRIPTION: Calculates and displays the final value of the `custom_loss` function using the trained weights (`W`) and biases (`b`) after the completion of the training loop.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_26\n\nLANGUAGE: julia\nCODE:\n```\n```jldoctest logistic_regression; filter = r\"[+-]?([0-9]*[.])?[0-9]+(f[+-]*[0-9])?\"\njulia> custom_loss(W, b, x, custom_y_onehot)\n0.6520349798243569\n```\n```\n\n----------------------------------------\n\nTITLE: Calculating Layer Output Size with Flux.outputsize in Julia\nDESCRIPTION: Shows how to use the `Flux.outputsize` function directly to determine the output dimensions of a specific layer given an input size. Here, it calculates the output size of a `Conv` layer `c` when processing an input of size `(28, 28, 1, 32)`, returning `(13, 13, 5, 32)`.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/outputsize.md#_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nc = Conv((3, 3), 1 => 5, relu, stride=2)\nFlux.outputsize(c, (28, 28, 1, 32))  # returns (13, 13, 5, 32)\n```\n\n----------------------------------------\n\nTITLE: Defining a Logistic Regression Model using Flux.jl's Chain and Dense Layer in Julia\nDESCRIPTION: Constructs the equivalent logistic regression model using Flux.jl's high-level API. It uses a `Chain` to sequence layers: first a `Dense` layer mapping 4 input features to 3 output classes (which implicitly includes weights and biases), followed by the built-in `softmax` activation function. Flux automatically handles parameter initialization.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_11\n\nLANGUAGE: julia\nCODE:\n```\njulia> flux_model = Chain(Dense(4 => 3), softmax)\nChain(\n  Dense(4 => 3),                        # 15 parameters\n  softmax,\n)\n```\n\n----------------------------------------\n\nTITLE: Device-Agnostic Array Transfer Abstraction for Model Deployment in Flux.jl (Julia)\nDESCRIPTION: Presents a pattern for making GPU device selection transparent by using an abstracted device function. At the top of a file, imports and aliases a device's transfer function (e.g., cu, roc, or identity) to a common name. This approach enables code that works seamlessly regardless of GPU backend or CPU. Dependencies: corresponding GPU or CPU environment.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/gpu.md#_snippet_6\n\nLANGUAGE: julia\nCODE:\n```\nusing CUDA: cu as device  # after this, `device === cu`\n# using AMDGPU: roc as device\n# device = identity  # do-nothing, for CPU\n\nusing Flux\nmodel = Chain(...) |> device\n\n```\n\n----------------------------------------\n\nTITLE: Combining Gradient Clipping with Adam Optimizer in Julia\nDESCRIPTION: Creates an `OptimiserChain` that applies gradient clipping (`ClipGrad` with a threshold of 1e-3) before applying the `Adam` optimizer (with a learning rate of 1e-3). This technique is useful for preventing exploding gradients, particularly in recurrent neural networks.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/training/optimisers.md#_snippet_5\n\nLANGUAGE: julia\nCODE:\n```\nopt = OptimiserChain(ClipGrad(1e-3), Adam(1e-3))\n```\n\n----------------------------------------\n\nTITLE: Computing Average Loss on Dataset with Flux - Julia\nDESCRIPTION: Implements the loss_all function to calculate average logit cross-entropy loss over all mini-batches in a DataLoader, using the provided model. Aggregates total loss and normalizes by the number of observations, leveraging MLUtils and Flux utilities. Takes DataLoader and model as arguments. Returns scalar loss value.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-mlp.md#_snippet_4\n\nLANGUAGE: julia\nCODE:\n```\nfunction loss_all(dataloader, model)\n    l = 0f0\n    n = 0\n    for (x, y) in dataloader\n        l += logitcrossentropy(model(x), y, agg=sum)\n        n += MLUtils.numobs(x)\n    end\n    return l / n\nend\n```\n\n----------------------------------------\n\nTITLE: Composing Weight Decay and Descent Optimizers in Julia\nDESCRIPTION: Creates an `OptimiserChain` in Flux.jl (via Optimisers.jl) that first applies `WeightDecay` with a factor of 1e-4 and then applies the `Descent` optimizer. This allows adding L2 regularization (weight decay) to the standard gradient descent update rule.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/training/optimisers.md#_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nopt = OptimiserChain(WeightDecay(1e-4), Descent())\n```\n\n----------------------------------------\n\nTITLE: Defining and Using a Parameterized Polynomial Function (Global Variable) in Julia\nDESCRIPTION: This snippet defines a simple polynomial function, poly1, in Julia that uses a global parameter vector θ. It shows function declaration, invocation, and an example evaluation to confirm the result. The function parameterization style illustrated here relies on externally scoped variables, which can make parameter tracking and management explicit. Required dependencies: none beyond Julia Base. Inputs: real number x; output: real number y. Limitation: parameter vector must be defined in the global scope prior to use.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/basics.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nθ = [10, 1, 0.1]\n\npoly1(x::Real) = θ[1] + θ[2]*x + θ[3]*x^2\n\npoly1(5) == 17.5  # true\n\n# output\n\ntrue\n```\n\n----------------------------------------\n\nTITLE: Custom Training Loops with Closures for Flux Model Updates - julia\nDESCRIPTION: This snippet sketches a custom training loop where each minibatch is processed by computing loss and gradient via a closure in a do-block, then updating parameters in place. Showcases Julia's flexibility for advanced or custom logic while training. Dependencies: Flux.jl; input: iterable of (data, labels), model, optimizer state; output: in-place model updates per batch.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-blitz.md#_snippet_14\n\nLANGUAGE: julia\nCODE:\n```\nfor d in training_set # assuming d looks like (data, labels)\n    # our super logic\n    g = gradient(model) do model\n        l = loss(model, d...)\n    end[1]\n    Flux.update!(opt_state, model, g)\nend\n```\n\n----------------------------------------\n\nTITLE: Struct-Based Neural Network Layer Definition in Julia\nDESCRIPTION: This snippet defines a Layer struct encapsulating a weight matrix (W), bias vector (b), and activation function (act), and makes its instances callable for vector inputs. A convenience constructor generates weights and biases for given input/output sizes and allows custom activation functions, defaulting to sigmoid. Required dependencies: Julia Base, proper sigmoid definition, and random number generator. Inputs: vector of length in; outputs: vector of length out. Provides modular, composable neural network layer design.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/basics.md#_snippet_16\n\nLANGUAGE: julia\nCODE:\n```\nstruct Layer  # container struct\n    W::Matrix\n    b::Vector\n    act::Function\nend\n\n(d::Layer)(x) = d.act.(d.W*x .+ d.b)  # make it callabale\n\nLayer(in::Int, out::Int, act::Function=sigmoid) =\n  Layer(randn(Float32, out, in), zeros(Float32, out), act)\n\nlayer3s = Layer(3, 2)  # instance with its own parameters\n```\n\n----------------------------------------\n\nTITLE: Plotting Data and Fitted Line with Plots.jl in Julia\nDESCRIPTION: Uses the `Plots.jl` package to visualize the results. The first `plot` call creates a scatter plot of the input data (`x`, `y`). The second `plot!` call adds the learned linear regression line (y = W[1]*x + b[1]) to the existing plot. Requires the `Plots.jl` package to be installed and imported.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_21\n\nLANGUAGE: julia\nCODE:\n```\njulia> plot(reshape(x, (61, 1)), reshape(y, (61, 1)), lw = 3, seriestype = :scatter, label = \"\", title = \"Simple Linear Regression\", xlabel = \"x\", ylabel= \"y\");\n```\n\nLANGUAGE: julia\nCODE:\n```\n\njulia> plot!((x) -> b[1] + W[1] * x, -3, 3, label=\"Custom model\", lw=2);\n```\n\n----------------------------------------\n\nTITLE: Computing Gradients of Scalar Functions with Flux.gradient in Julia\nDESCRIPTION: This code uses Flux's gradient function to compute the exact derivative of poly1 at x=5. The output is a tuple to support functions with multiple arguments. Required dependencies: Flux.jl. Inputs: poly1 (function), x=5; output: tuple of gradients. Limitation: requires Flux installed and imported. Demonstrates single-argument gradient computation.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/basics.md#_snippet_7\n\nLANGUAGE: julia\nCODE:\n```\njulia> using Flux\n\njulia> gradient(poly1, 5)\n(2.0,)\n```\n\n----------------------------------------\n\nTITLE: Generating Feature Matrix for Linear Regression in Julia\nDESCRIPTION: Generates a matrix of feature values ranging from -3.0 to 3.0 in steps of 0.1 using hcat and collect, storing them as Float32. The result is a 1x61 matrix, where each column represents a feature input for linear regression. Requires no external data, and produces a regularly spaced array for use in synthetic dataset creation.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\njulia> x = hcat(collect(Float32, -3:0.1:3)...)\n```\n\n----------------------------------------\n\nTITLE: Implementing the GAN Training Loop in Julia with Flux.jl\nDESCRIPTION: This Julia code snippet implements the main training loop for a Generative Adversarial Network (GAN). It initializes arrays to store per-epoch losses for the generator and discriminator. The outer loop iterates through epochs, and the inner loop iterates through mini-batches from `train_loader`. Inside the inner loop, real data is flattened, noise is generated, fake data is produced by the generator, and both the discriminator (`train_dscr!`) and generator (`train_gen!`) are trained sequentially. Batch losses are accumulated and averaged per epoch. Periodically (controlled by `output_period`), it generates sample images using the current generator state and displays them as a heatmap. Requires pre-defined `generator`, `discriminator` models, `train_dscr!`, `train_gen!` functions, `train_loader`, `train_x` data, and hyperparameters `num_epochs`, `latent_dim`, `output_period`. Assumes GPU usage with the `gpu` function.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-10-14-vanilla-gan.md#_snippet_8\n\nLANGUAGE: julia\nCODE:\n```\nlossvec_gen = zeros(num_epochs)\nlossvec_dscr = zeros(num_epochs)\n\nfor n in 1:num_epochs\n    loss_sum_gen = 0.0f0\n    loss_sum_dscr = 0.0f0\n\n    for x in train_loader\n        # - Flatten the images from 28x28xbatchsize to 784xbatchsize\n        real_data = flatten(x);\n\n        # Train the discriminator\n        noise = randn(latent_dim, size(x)[end]) |> gpu\n        fake_data = generator(noise)\n        loss_dscr = train_dscr!(discriminator, real_data, fake_data)\n        loss_sum_dscr += loss_dscr\n\n        # Train the generator\n        loss_gen = train_gen!(discriminator, generator)\n        loss_sum_gen += loss_gen\n    end\n\n    # Add the per-sample loss of the generator and discriminator\n    lossvec_gen[n] = loss_sum_gen / size(train_x)[end]\n    lossvec_dscr[n] = loss_sum_dscr / size(train_x)[end]\n\n    if n % output_period == 0\n        @show n\n        noise = randn(latent_dim, 4) |> gpu;\n        fake_data = reshape(generator(noise), 28, 4*28);\n        p = heatmap(fake_data, colormap=:inferno)\n        print(p)\n    end\nend \n```\n\n----------------------------------------\n\nTITLE: Calculating Gradients Manually with Zygote in Julia\nDESCRIPTION: Demonstrates using the `gradient` function (re-exported by Flux) from Zygote.jl to compute the derivatives of a `custom_loss` function with respect to model parameters (weights `W` and bias `b`) and inputs (`x`, `y`). The gradients `dLdW` and `dLdb` are captured for subsequent parameter updates.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_16\n\nLANGUAGE: julia\nCODE:\n```\njulia> dLdW, dLdb, _, _ = gradient(custom_loss, W, b, x, y);\n```\n\n----------------------------------------\n\nTITLE: Checking Flux Model Accuracy and Loss After Training in Julia\nDESCRIPTION: Displays the final accuracy using `@show flux_accuracy(x, y)` and calculates the final loss using `flux_loss(flux_model, x, flux_y_onehot)` after training the `flux_model` using the Flux-specific training loop.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_28\n\nLANGUAGE: julia\nCODE:\n```\n```jldoctest logistic_regression; filter = r\"[+-]?([0-9]*[.])?[0-9]+(f[+-]*[0-9])?\"\njulia> @show flux_accuracy(x, y);\nflux_accuracy(x, y) = 0.98\n\njulia> flux_loss(flux_model, x, flux_y_onehot)\n0.6952386604624324\n```\n```\n\n----------------------------------------\n\nTITLE: Batch One-Hot Encoding and Decoding with OneHotArrays.jl in Julia\nDESCRIPTION: This snippet shows how to use onehotbatch to encode multiple categorical samples simultaneously as a OneHotMatrix, and how to decode the batch using onecold. Requires OneHotArrays.jl. The onehotbatch function produces a matrix where each column is a one-hot vector for the corresponding sample. Decoding the matrix returns the list of predicted labels for each column. Inputs: array of labels and categories array. Outputs: OneHotMatrix and decoded vector of labels. Input arrays must be of compatible lengths and categories must contain all sample labels.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/data/onehot.md#_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\njulia> using OneHotArrays\n\njulia> onehotbatch([:b, :a, :b], [:a, :b, :c])\n3×3 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n ⋅  1  ⋅\n 1  ⋅  1\n ⋅  ⋅  ⋅\n\njulia> onecold(ans, [:a, :b, :c])\n3-element Vector{Symbol}:\n :b\n :a\n :b\n```\n\n----------------------------------------\n\nTITLE: Freezing and Thawing Model Parameters During Training in Flux.jl (Julia)\nDESCRIPTION: This code illustrates how to temporarily freeze (disable training) for a subset of a model's parameters (e.g., encoder) and then fully restore training using Flux.jl. `Flux.freeze!` and `Flux.thaw!` modify which parts of the optimizer state are updated during training. The approach is ideal for transfer learning or staged training, and requires the optimizer state to be structured to match the model's components.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/training/training.md#_snippet_19\n\nLANGUAGE: julia\nCODE:\n```\nFlux.freeze!(opt_state.layers.enc)\n\n# Now training won't update parameters in bimodel.layers.enc\ntrain!(loss, bimodel, data, opt_state)\n\n# Un-freeze the entire model:\nFlux.thaw!(opt_state)\n```\n\n----------------------------------------\n\nTITLE: Creating a Training Data Iterator from Array Slices in Julia\nDESCRIPTION: Demonstrates creating a memory-efficient data iterator from large arrays `X` (inputs) and `Y` (labels). `eachslice` (with `dims=3`) iterates through images in `X`, and `eachcol` iterates through corresponding labels in `Y`. `zip` combines these iterators element-wise, producing tuples of `(AbstractMatrix, AbstractVector)` suitable for training.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/training/training.md#_snippet_7\n\nLANGUAGE: julia\nCODE:\n```\nX = rand(28, 28, 60_000);  # many images, each 28 × 28\nY = rand(10, 60_000)\ndata = zip(eachslice(X; dims=3), eachcol(Y))\n\nfirst(data) isa Tuple{AbstractMatrix, AbstractVector}  # true\n```\n\n----------------------------------------\n\nTITLE: Evaluating Test Set Accuracy for a CNN Model using Flux.jl in Julia\nDESCRIPTION: Invokes the previously defined accuracy function on the test data loader to compute overall classification accuracy of the trained CNN. Assumes testloader and model are set up, and the accuracy function is defined as above.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-blitz.md#_snippet_25\n\nLANGUAGE: julia\nCODE:\n```\naccuracy(model, testloader)\n```\n\n----------------------------------------\n\nTITLE: Computing Gradients and Hessians with Flat Parameters in Flux - Julia\nDESCRIPTION: This sequence demonstrates computing gradients using both the nested model structure and the flat parameter representation. It defines a custom loss function operating on flat parameters, computes the flat gradient using the gradient function and the flat vector, and then calculates the Hessian matrix using Zygote.hessian. Requires Zygote for automatic differentiation and Flux for model definition. Inputs include random input data and the model; outputs are gradients and Hessians in flat form. Limitations: Hessian requires scalar loss, and dimensions must match flattened parameter length.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/destructure.md#_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\njulia> x = rand(Float32, 2, 16);\n\njulia> grad = gradient(m -> sum(abs2, m(x)), model)  # nested gradient\n((layers = ((weight = Float32[10.339018 11.379145], bias = Float32[22.845667], σ = nothing), (weight = Float32[-29.565302;;], bias = Float32[-37.644184], σ = nothing)),),)\n\njulia> function loss(v::Vector)\n         m = rebuild(v)\n         y = m(x)\n         sum(abs2, y)\n       end;\n\njulia> gradient(loss, flat)  # flat gradient, same numbers\n(Float32[10.339018, 11.379145, 22.845667, -29.565302, -37.644184],)\n\njulia> Zygote.hessian(loss, flat)  # second derivative\n5×5 Matrix{Float32}:\n  -7.13131   -5.54714  -11.1393  -12.6504   -8.13492\n  -5.54714   -7.11092  -11.0208  -13.9231   -9.36316\n -11.1393   -11.0208   -13.7126  -27.9531  -22.741\n -12.6504   -13.9231   -27.9531   18.0875   23.03\n  -8.13492   -9.36316  -22.741    23.03     32.0\n\njulia> Flux.destructure(grad)  # acts on non-models, too\n(Float32[10.339018, 11.379145, 22.845667, -29.565302, -37.644184], Restructure(Tuple, ..., 5))\n```\n\n----------------------------------------\n\nTITLE: Implicit Gradient Tracking with Global Parameters in Julia (Flux.Params)\nDESCRIPTION: This example shows Flux's implicit gradient tracking using Params for global parameters. The gradient is computed for a zero-argument function that implicitly depends on the global θ. The result is an indexable dictionary linking θ to its gradients. Required dependencies: Flux.jl. Inputs: function with implicit dependencies and Params; outputs: Grads dictionary, gradient vector. Limitation: global state management can be error-prone.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/basics.md#_snippet_11\n\nLANGUAGE: julia\nCODE:\n```\ng1 = gradient(() -> poly1(5), Params([θ]))\ng1[θ] == [1.0, 5.0, 25.0]\n```\n\n----------------------------------------\n\nTITLE: Initializing Manual Logistic Regression Model Parameters (Weights and Bias) in Julia\nDESCRIPTION: Initializes the parameters for the custom logistic regression model. The weight matrix `W` is initialized with random `Float32` values (size 3x4 for 3 output classes and 4 input features). The bias vector `b` is initialized as a vector of three `Float32` zeros.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_6\n\nLANGUAGE: julia\nCODE:\n```\njulia> W = rand(Float32, 3, 4);\n\njulia> b = [0.0f0, 0.0f0, 0.0f0];\n```\n\n----------------------------------------\n\nTITLE: Visualizing XOR Classification Results with Plots.jl\nDESCRIPTION: This Julia snippet uses the Plots.jl library to create scatter plots visualizing the results of the XOR classification. It generates three plots side-by-side: one showing the true classification based on the generated data, one showing the classification probabilities from the untrained network, and one showing the probabilities from the trained network. It relies on the `noisy`, `truth`, `probs1`, and `probs2` variables generated in the previous snippet.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/quickstart.md#_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nusing Plots  # to draw the above figure\n\np_true = scatter(noisy[1,:], noisy[2,:], zcolor=truth, title=\"True classification\", legend=false)\np_raw =  scatter(noisy[1,:], noisy[2,:], zcolor=probs1[1,:], title=\"Untrained network\", label=\"\", clims=(0,1))\np_done = scatter(noisy[1,:], noisy[2,:], zcolor=probs2[1,:], title=\"Trained network\", legend=false)\n\nplot(p_true, p_raw, p_done, layout=(1,3), size=(1000,330))\n```\n\n----------------------------------------\n\nTITLE: Inspecting Model Parameters Before Training in Julia\nDESCRIPTION: Retrieves and displays the current (pre-training) values of the Dense model's `weight` and `bias` properties. This allows comparison with post-training values to observe parameter adjustment. No additional dependencies beyond the previously created `predict` model.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/overview.md#_snippet_7\n\nLANGUAGE: julia\nCODE:\n```\njulia> predict.weight\n1×1 Matrix{Float32}:\n 0.9066542\n\njulia> predict.bias\n1-element Vector{Float32}:\n 0.0\n```\n\n----------------------------------------\n\nTITLE: Building a Three-Layer MLP Model with Flux Chain - Julia\nDESCRIPTION: Defines the build_model function, returning a Chain of two Dense layers for an MLP: the first layer with input size matching flattened images, outputting 32 ReLU units, the second layer mapping to nclasses outputs. Uses Chain and Dense from Flux. Accepts optional image size and class count. The model is ready for training with images reshaped as vectors.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-mlp.md#_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\nfunction build_model(; imgsize=(28,28,1), nclasses=10)\n    return Chain(\n  \t    Dense(prod(imgsize) => 32, relu),\n            Dense(32 => nclasses))\nend\n```\n\n----------------------------------------\n\nTITLE: Displaying a Random Test Image using Julia Plotting Utilities\nDESCRIPTION: Displays a randomly selected image from the CIFAR10 test set using the `plot` and `image` functions. Useful for visual inspection of data before feeding it to the network. Requires appropriate plotting packages (e.g., Plots.jl) and the test dataset loaded in compatible format.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-blitz.md#_snippet_23\n\nLANGUAGE: julia\nCODE:\n```\nplot(image(test_x[:,:,:,rand(1:end)]))\n```\n\n----------------------------------------\n\nTITLE: Accessing Weights and Bias of a Flux Dense Layer in Julia\nDESCRIPTION: Demonstrates how to inspect the automatically initialized parameters (weights and bias) within a Flux model. It accesses the `Dense` layer (the first layer in the `Chain`, index `[1]`) and retrieves its `weight` matrix and `bias` vector.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_12\n\nLANGUAGE: julia\nCODE:\n```\njulia> flux_model[1].weight, flux_model[1].bias\n(Float32[0.78588694 -0.45968163 -0.77409476 0.2358028; -0.9049773 -0.58643705 0.466441 -0.79523873; 0.82426906 0.4143493 0.7630932 0.020588955], Float32[0.0, 0.0, 0.0])\n```\n\n----------------------------------------\n\nTITLE: Constructing and Evaluating Custom Layer with Gradient Calculation - Flux Julia\nDESCRIPTION: This code snippet demonstrates how to use a custom Layer struct by initializing weights, biases, and an activation function, then applying it to an input vector. It computes the gradient of the output with respect to input and model parameters using Flux.gradient. The output includes a named tuple for the gradients, matching the Layer struct's fields. Dependencies: Flux.jl. Input: Float32 vector. Output: Forward and backward pass results. Limitation: The activation function parameter is not differentiable.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/basics.md#_snippet_17\n\nLANGUAGE: julia\nCODE:\n```\nx = Float32[0.1, 0.2, 0.3]  # input\n\nlayer3s(x)  # output, 2-element Vector{Float32}\n\nFlux.gradient((x,d) -> d(x)[1], x, layer3s)[2]  # NamedTuple{(:W, :b, :act)}\n```\n\n----------------------------------------\n\nTITLE: Building a Dense Model Layer for Regression Predictions in Julia using Flux\nDESCRIPTION: Initializes a single-layer dense neural network with 1 input and 1 output node via Flux's Dense layer. Reveals initial model parameters (`weight` and `bias`) representing learnable coefficients. This structure models linear relationships and serves as the function approximator throughout. Requires Flux and the model's parameters will be updated during training.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/overview.md#_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\njulia> model = Dense(1 => 1)\nDense(1 => 1)       # 2 parameters\n\njulia> model.weight\n1×1 Matrix{Float32}:\n 0.95041317\n\njulia> model.bias\n1-element Vector{Float32}:\n 0.0\n```\n\n----------------------------------------\n\nTITLE: Setting Separate Learning Rates for Model Parts in Flux.jl (Julia)\nDESCRIPTION: This snippet demonstrates how to assign different learning rates to components within a composite model (e.g., encoder vs decoder) using Flux.jl. After building a `Chain` model with labeled subcomponents, optimizer states are created for the whole model, and `Flux.adjust!` is used to update only the encoder's learning rate. Requires model composition with labeled layers, and is useful for fine-grained optimization in complex architectures.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/training/training.md#_snippet_16\n\nLANGUAGE: julia\nCODE:\n```\n# Consider some model with two parts:\nbimodel = Chain(enc = [...], dec = [...])\n\n# This returns a tree whose structure matches the model:\nopt_state = Flux.setup(Adam(0.02), bimodel)\n\n# Adjust the learning rate to be used for bimodel.layers.enc\nFlux.adjust!(opt_state.layers.enc, 0.03)\n```\n\n----------------------------------------\n\nTITLE: Defining a Complete Custom Logistic Regression Model with Softmax in Julia\nDESCRIPTION: Defines the complete custom logistic regression model function `custom_model`. It takes weights `W`, bias `b`, and input `x`, applies the previously defined linear function `m(W, b, x)`, and then pipes (|>) the result through the `custom_softmax` activation function to produce class probabilities.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_8\n\nLANGUAGE: julia\nCODE:\n```\njulia> custom_model(W, b, x) = m(W, b, x) |> custom_softmax\ncustom_model (generic function with 1 method)\n```\n\n----------------------------------------\n\nTITLE: Loading Boston Housing Dataset with MLDatasets.jl in Julia\nDESCRIPTION: Loads the Boston Housing dataset using the `BostonHousing` function from `MLDatasets.jl`. It retrieves the features (`x`) and labels (`y`) and converts them to the `Float32` data type, which is commonly used in machine learning frameworks like Flux.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_23\n\nLANGUAGE: julia\nCODE:\n```\njulia> dataset = BostonHousing();\n```\n\nLANGUAGE: julia\nCODE:\n```\n\njulia> x, y = BostonHousing(as_df=false)[:];\n```\n\nLANGUAGE: julia\nCODE:\n```\n\njulia> x, y = Float32.(x), Float32.(y);\n```\n\n----------------------------------------\n\nTITLE: Calculating Gradients and Optimizer Steps on GPU with Flux.jl and CUDA.jl in Julia\nDESCRIPTION: Illustrates how Flux.jl training workflows can be adapted to operate entirely on the GPU using GPU-backed models and inputs. Calculates gradients for CPU and GPU models, sets up the optimizer after device transfer, and applies updates. Requires that c_model and cx are already on GPU. Inputs: models, data; outputs: gradients and updates performed on GPU arrays.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/gpu.md#_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\ngrads = Flux.gradient((f,x) -> sum(abs2, f(x)), model, x)  # on CPU\nc_grads = Flux.gradient((f,x) -> sum(abs2, f(x)), c_model, cx)  # same result, all on GPU\n\nc_opt = Flux.setup(Adam(), c_model)  # setup optimiser after moving model to GPU\n\nFlux.update!(c_opt, c_model, c_grads[1])  # mutates c_model but not model\n\n```\n\n----------------------------------------\n\nTITLE: Calculating Loss using Batched Data Matrices in Julia\nDESCRIPTION: Shows the efficient method for calculating total loss using batched data. The model evaluates the entire batch (`x_batch`) at once, producing `y_preds`. The loss is then computed element-wise (using broadcasting `loss.(...)`) between predicted and target batches. This leverages optimized BLAS matrix-matrix operations for significant speedup.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/performance.md#_snippet_5\n\nLANGUAGE: julia\nCODE:\n```\n```julia\nfunction loss_total(x_batch::Matrix, y_batch::Matrix)\n    y_preds = model(x_batch)\n    sum(loss.(y_preds, y_batch))\nend\n```\n```\n\n----------------------------------------\n\nTITLE: Generating and Evaluating Model Predictions on Random Test Images with Flux.jl in Julia\nDESCRIPTION: Selects five random images from the test set, moves them to the GPU, retrieves true labels, and applies the model to obtain predictions. This is used to manually inspect model outputs for sample test instances. Assumes model is trained, test data is loaded, and gpu support is enabled.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-blitz.md#_snippet_24\n\nLANGUAGE: julia\nCODE:\n```\nids = rand(1:10000, 5)\nrand_test = test_x[:,:,:,ids] |> gpu\nrand_truth = test_y[ids]\nmodel(rand_test)\n```\n\n----------------------------------------\n\nTITLE: Composing Layers with Closures for Model Flexibility - Flux Julia\nDESCRIPTION: This snippet illustrates how to compose neural network layers using closures and local variables instead of function composition, enabling flexible field naming and encapsulation. The model is a closure that holds two Layer instances and applies one after the other, finishing with an only to get a scalar. It demonstrates forward evaluation and gradient calculation via Flux.gradient, returning gradients as a named tuple with the same field names as the closure. Dependencies: Flux.jl, custom Layer type. Input: Float32 vector. Output: Scalar, with gradient tuple for each contained Layer. No external struct is needed.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/basics.md#_snippet_19\n\nLANGUAGE: julia\nCODE:\n```\nmodel2 = let\n    lay1 = Layer(1, 20)  # local variables containing layers\n    lay2 = Layer(20, 1)\n    function fwd(x)  # equivalent to x -> only(lay2(lay1(x)))\n        mid = lay1(x)\n        lay2(mid) |> only\n    end\nend\n\nmodel2(Float32[0.1])\n\nFlux.gradient(|>, [1f0], model2)[2]\n```\n\n----------------------------------------\n\nTITLE: Calling Loss Functions with Optional Aggregation - Flux.jl - Julia\nDESCRIPTION: Demonstrates the conventional usage of loss functions in Flux.jl, accepting predictions and targets as arguments, with an optional aggregation (agg) parameter to specify how batch results are reduced. The shown usages include default mean, sum, partial reduction by dimension, weighted mean, and no aggregation. Requires the Flux and Flux.Losses modules. Inputs are prediction ŷ and target y, with agg as an optional function. Outputs are scalar or array loss values depending on aggregation. This pattern enables flexible loss calculation in supervised learning scenarios.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/models/losses.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nloss(ŷ, y)                         # defaults to `mean`\nloss(ŷ, y, agg=sum)                # use `sum` for reduction\nloss(ŷ, y, agg=x->sum(x, dims=2))  # partial reduction\nloss(ŷ, y, agg=x->mean(w .* x))    # weighted mean\nloss(ŷ, y, agg=identity)           # no aggregation.\n```\n\n----------------------------------------\n\nTITLE: Testing Custom Model Output and Size in Julia\nDESCRIPTION: Verifies that the custom linear regression model returns predicted values and the correct output shape. It checks both the full prediction output shape and the correspondence of the first prediction with the first label. Serves as a preliminary pass/fail check prior to training.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_7\n\nLANGUAGE: julia\nCODE:\n```\njulia> custom_model(W, b, x) |> size\n(1, 61)\n\njulia> custom_model(W, b, x)[1], y[1]\n(-1.6116865f0, -7.0f0)\n```\n\n----------------------------------------\n\nTITLE: Defining the Linear Component of a Custom Logistic Regression Model in Julia\nDESCRIPTION: Defines a basic Julia function `m` that represents the linear transformation part (Wx + b) of a logistic regression model, before the activation function. It takes a weight matrix `W`, a bias vector `b`, and input data `x` as arguments and performs matrix multiplication followed by broadcasting addition of the bias.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_5\n\nLANGUAGE: julia\nCODE:\n```\njulia> m(W, b, x) = W*x .+ b\nm (generic function with 1 method)\n```\n\n----------------------------------------\n\nTITLE: Differentiating Functions of Matrices and Vectors in Julia - julia\nDESCRIPTION: This code defines a function 'myloss' that computes the sum of a linear transformation W * x with bias b, then shows how to compute gradients with respect to each parameter using Flux.gradient. Demonstrates automatic differentiation over multi-parameter functions where arguments are arrays. Requires Flux, input is (W: matrix, b: vector, x: vector), and output is a tuple of gradients matching those types.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-blitz.md#_snippet_7\n\nLANGUAGE: julia\nCODE:\n```\nmyloss(W, b, x) = sum(W * x .+ b)\n\nW = randn(3, 5)\nb = zeros(3)\nx = rand(5)\n\ngradient(myloss, W, b, x)\n```\n\n----------------------------------------\n\nTITLE: Comparing Custom and Flux-Based Loss Outputs in Julia\nDESCRIPTION: Computes the loss values of both custom and Flux-based models after synchronizing parameters to confirm their functional equivalence. Returns a tuple of loss values from custom_loss and flux_loss. Confirms alignment of implementations and serves as a validation checkpoint.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_14\n\nLANGUAGE: julia\nCODE:\n```\njulia> custom_loss(W, b, x, y), flux_loss(flux_model, x, y)\n(22.74856f0, 22.74856f0)\n```\n\n----------------------------------------\n\nTITLE: Applying fmap from Functors and CUDA to Migrate and Update Parameters - Julia\nDESCRIPTION: These snippets show how to use Functors.fmap for recursive transformations of model structures. The first example migrates all arrays within a model to the GPU using CUDA.cu. The second applies a basic gradient step to parameters by mapping over model and gradient pairs, subtracting scaled gradients for arrays. Dependencies: Functors.jl, CUDA.jl, Flux.jl. Inputs: Model, (and optionally) gradient structures. Outputs: New models with migrated or updated parameters. Limitation: Assumes array fields and matching structure in gradients.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/basics.md#_snippet_21\n\nLANGUAGE: julia\nCODE:\n```\nusing CUDA, Functors\nfmap(cu, model1)\n```\n\nLANGUAGE: julia\nCODE:\n```\nfmap((x, dx) -> x isa Array ? (x - dx/100) : x, model, grad)\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Model Layer in Flux (Julia)\nDESCRIPTION: This snippet creates a custom model container struct in Julia for a neural network, wrapping a chain and allowing custom forward-pass logic. It demonstrates how to define callable structs, add additional computation in the forward method, and register the struct with Flux via the Flux.@layer macro for improved usability. Key dependencies are the Flux framework and adherence to rules regarding array mutation for Zygote compatibility. Usage requires Julia and Flux.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/custom_layers.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nstruct CustomModel{T <: Chain} # Parameter to avoid type instability\n  chain::T\nend\n\nfunction (m::CustomModel)(x)\n  # Arbitrary code can go here, but note that everything will be differentiated.\n  # Zygote does not allow some operations, like mutating arrays.\n\n  return m.chain(x) + x\nend\n\n# This is optional but recommended for pretty printing and other niceties\nFlux.@layer CustomModel\n```\n\n----------------------------------------\n\nTITLE: Constructing Dense Layers with Flux in Julia - julia\nDESCRIPTION: This snippet imports Flux, instantiates a Dense layer that maps an input of 10 to 5 outputs, and prepares a Float32 random input vector. This is a key abstraction for modeling in Flux, encapsulating parameter arrays for linear transformations. Only Flux.jl is required. Input is a Float32 vector of size 10, output is a Float32 vector of size 5 (layer output).\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-blitz.md#_snippet_8\n\nLANGUAGE: julia\nCODE:\n```\nusing Flux\n\nm = Dense(10 =>  5)\n\nx = rand(Float32, 10)\n```\n\n----------------------------------------\n\nTITLE: Defining Type-Stable Leaky ReLU using `oftype` in Julia\nDESCRIPTION: Presents the idiomatic and safe way to define the `leaky_tanh` function while preserving the input type. It uses `oftype(x/1, 0.01)` to ensure the numeric literal `0.01` matches the type of the input `x`, thus avoiding unintended type promotions and maintaining performance regardless of input precision.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/performance.md#_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\n```julia\nleaky_tanh(x) = oftype(x/1, 0.01)*x + tanh(x)\n```\n```\n\n----------------------------------------\n\nTITLE: Saving Time-Stamped Model Checkpoints using JLD2 in Julia\nDESCRIPTION: Illustrates saving multiple model checkpoints with unique filenames based on the current timestamp using `now()` from the `Dates` module (implicitly required). This prevents overwriting previous checkpoints and allows tracking model evolution over time.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/saving.md#_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\njldsave(\"model-$(now()).jld2\", model_state = Flux.state(m))\n\n```\n\n----------------------------------------\n\nTITLE: Calculating Loss by Iterating Over Individual Observations in Julia\nDESCRIPTION: Illustrates an inefficient method for calculating the total loss by looping through individual feature vectors (`xs`) and target vectors (`ys`). This approach evaluates the model (`model(x)`) one observation at a time, leading to slower performance compared to batch processing due to repeated, less optimized matrix-vector operations.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/performance.md#_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\n```julia\nfunction loss_total(xs::AbstractVector{<:Vector}, ys::AbstractVector{<:Vector})\n    sum(zip(xs, ys)) do (x, y_target)\n        y_pred = model(x)  # evaluate the model\n        return loss(y_pred, y_target)\n    end\nend\n```\n```\n\n----------------------------------------\n\nTITLE: Referencing NNlib Convolution Functions and Types in Julia\nDESCRIPTION: Lists convolution-related functions and dimension types from NNlib.jl for documentation generation using Julia's `@docs` macro. Includes `conv`, `depthwiseconv`, `ConvDims`, `DepthwiseConvDims`, and `DenseConvDims`, which are used internally by Flux's `Conv` and `CrossCor` layers.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/models/nnlib.md#_snippet_4\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\nconv\nConvDims\ndepthwiseconv\nDepthwiseConvDims\nDenseConvDims\n```\n```\n\n----------------------------------------\n\nTITLE: Defining the Discriminator Network with Feed-Forward Layers in Flux (Julia)\nDESCRIPTION: Creates the discriminator network as a multi-layer perceptron using Flux's Chain and Dense layers. Each layer uses a leakyrelu activation with alpha 0.2 and Dropout for generalization. The final layer applies a sigmoid activation for binary classification. All layers are set to run on the GPU if available. Hyper-parameters such as n_features and dropout probability are referenced. No external dependencies beyond Flux and CUDA are required.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-10-14-vanilla-gan.md#_snippet_4\n\nLANGUAGE: julia\nCODE:\n```\ndiscriminator = Chain(Dense(n_features => 1024, x -> leakyrelu(x, 0.2f0)),\n                        Dropout(0.3),\n                        Dense(1024 => 512, x -> leakyrelu(x, 0.2f0)),\n                        Dropout(0.3),\n                        Dense(512 => 256, x -> leakyrelu(x, 0.2f0)),\n                        Dropout(0.3),\n                        Dense(256 => 1, sigmoid)) |> gpu\n```\n\n----------------------------------------\n\nTITLE: Encapsulating Parameters in a Struct for Callable Polynomial Types in Julia\nDESCRIPTION: This snippet demonstrates parameter encapsulation using a Poly3 struct, which stores the parameter vector θ3. The struct is made callable by defining a function call overload, allowing instances to act as functions. This approach enhances modularity and tracks parameters structurally for differentiation. Required dependencies: Julia Base. Input: real x; output: polynomial result. Emphasizes struct-based organization for composable models.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/basics.md#_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\nstruct Poly3{T}  # container struct\n    θ3::T\nend\n(p::Poly3)(x::Real) = evalpoly(x, p.θ3)  # make this callable\n\npoly3s = Poly3([10, 1, 0.1])  # construct an instance\n\npoly3s(5) == 17.5  # true\n\n# output\n\ntrue\n```\n\n----------------------------------------\n\nTITLE: Referencing NNlib Attention Primitives in Julia\nDESCRIPTION: Lists NNlib.jl functions related to dot-product attention mechanisms, intended for documentation generation using Julia's `@docs` macro. These functions serve as primitives for Flux's `MultiHeadAttention` layer and include calculating attention, scores, and creating causal masks.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/models/nnlib.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\nNNlib.dot_product_attention\nNNlib.dot_product_attention_scores\nNNlib.make_causal_mask\n```\n```\n\n----------------------------------------\n\nTITLE: Referencing Miscellaneous NNlib Functions in Julia\nDESCRIPTION: Lists miscellaneous utility functions from NNlib.jl for documentation generation using Julia's `@docs` macro. Includes `logsumexp` and the Gated Linear Unit (`glu`).\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/models/nnlib.md#_snippet_11\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\nlogsumexp\nNNlib.glu\n```\n```\n\n----------------------------------------\n\nTITLE: Initializing Hyper-Parameters and Optimizers for GAN in Flux (Julia)\nDESCRIPTION: Defines and initializes key hyper-parameters for GAN training, such as learning rates for generator/discriminator, batch size, epochs, latent dimension, and output frequency. Also constructs ADAM optimizers for both generator and discriminator. The parameters and optimizers are crucial for model convergence and are referenced throughout the training loops. Requires prior definition/import of ADAM (from Flux) and GPU setup if used.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-10-14-vanilla-gan.md#_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\n    lr_g = 2e-4          # Learning rate of the generator network\n    lr_d = 2e-4          # Learning rate of the discriminator network\n    batch_size = 128    # batch size\n    num_epochs = 1000   # Number of epochs to train for\n    output_period = 100 # Period length for plots of generator samples\n    n_features = 28 * 28# Number of pixels in each sample of the MNIST dataset\n    latent_dim = 100    # Dimension of latent space\n    opt_dscr = ADAM(lr_d)# Optimiser for the discriminator\n    opt_gen = ADAM(lr_g) # Optimiser for the generator\n```\n\n----------------------------------------\n\nTITLE: Using @autosize Macro for Automatic Layer Sizing in Julia\nDESCRIPTION: Demonstrates the use of the `@autosize` macro in Flux.jl to automatically infer the input size for a `Dense` layer within a `Chain`. The `_` placeholder is replaced with the calculated output size from the preceding layers (Conv and flatten) based on the initial input size `(28, 28, 1, 32)`. This results in a `Dense` layer with 845 inputs.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/outputsize.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\n@autosize (28, 28, 1, 32) Chain(Conv((3, 3), _ => 5, relu, stride=2), Flux.flatten, Dense(_ => 10))\n```\n\n----------------------------------------\n\nTITLE: Documenting Flux Shape Inference Tools in Julia\nDESCRIPTION: Specifies the Flux.jl components related to shape inference (`@autosize` macro and `outputsize` function) to be included in generated documentation using Documenter.jl's `@docs` block.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/outputsize.md#_snippet_4\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\nFlux.@autosize\nFlux.outputsize\n```\n```\n\n----------------------------------------\n\nTITLE: Testing Affine Layer on GPU and with Float16 (Julia REPL)\nDESCRIPTION: This snippet demonstrates casting the Affine layer's fields to half precision (Float16) and shows that all fields are affected by type conversion, even when not trainable. Designed to show the difference between fields included in training versus those affected by general model transformations. Dependency: Flux.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/custom_layers.md#_snippet_3\n\nLANGUAGE: julia-repl\nCODE:\n```\njulia> a |> f16\nAffine(Float16[1.0 2.0; 3.0 4.0; 5.0 6.0], Float16[7.0, 8.0, 9.0])\n```\n\n----------------------------------------\n\nTITLE: Initializing Optimizer and Data Tuple for Flux Training Loop in Julia\nDESCRIPTION: Imports the `train!` function from Flux, constructs a gradient descent optimizer (`Descent()`), and assembles the training input/target tuple into a data array for iteration. These setup steps are prerequisites to performing model fitting. Key dependencies: Flux. Optimizer parameters can be adjusted as needed; data is expected in a tuple-of-arrays batch format.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/overview.md#_snippet_6\n\nLANGUAGE: julia\nCODE:\n```\njulia> using Flux: train!\n\njulia> opt = Descent()\nDescent(0.1f0)\n\njulia> data = [(x_train, y_train)]\n1-element Vector{Tuple{Matrix{Int64}, Matrix{Int64}}}:\n ([0 1 … 4 5], [2 6 … 18 22])\n```\n\n----------------------------------------\n\nTITLE: Using a Custom Model Layer in Flux (Julia)\nDESCRIPTION: This snippet demonstrates how to instantiate and use the custom model (e.g., CustomModel) with a chain of dense layers. It shows the creation of dense layers, model wrapping, and execution with random input. Requires Flux and a properly defined CustomModel struct as in the previous snippet. Input is an array, and output is processed via both the chain and direct addition.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/custom_layers.md#_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nchain = Chain(Dense(10 => 10, relu), Dense(10 => 10))\nmodel = CustomModel(chain)\nmodel(rand(Float32, 10))\n```\n\n----------------------------------------\n\nTITLE: Loading and Processing MNIST Dataset in Julia\nDESCRIPTION: Defines the `get_processed_data` function responsible for loading the MNIST dataset and preparing it for training and testing. It loads training images and labels using `MNIST.traindata()`, partitions the training indices into mini-batches based on `args.batch_size`, and uses the `make_minibatch` function to create a list of training batches. It also loads the test data (`MNIST.testdata()`) and prepares it as a single large mini-batch.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-02-07-convnet.md#_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\nfunction get_processed_data(args)\n   # Load labels and images\n   train_imgs, train_labels = MNIST.traindata()\n   mb_idxs = partition(1:length(train_labels), args.batch_size)\n   train_set = [make_minibatch(train_imgs, train_labels, i) for i in mb_idxs]\n  \n   # Prepare test set as one giant minibatch:\n   test_imgs, test_labels = MNIST.testdata()\n   test_set = make_minibatch(test_imgs, test_labels, 1:length(test_labels))\n \n   return train_set, test_set\n \nend\n```\n\n----------------------------------------\n\nTITLE: Setting Up a Descent Optimizer State with Flux - julia\nDESCRIPTION: This snippet initializes an optimizer state for gradient descent with a given learning rate (eta) in Flux, using the setup helper with Descent. Establishes modular optimizer handling for subsequent training. Dependencies: Flux.jl; input is model and learning rate; output is optimizer state/data structure.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-blitz.md#_snippet_12\n\nLANGUAGE: julia\nCODE:\n```\nopt_state = Flux.setup(Descent(η), model)\n```\n\n----------------------------------------\n\nTITLE: Training Discriminator Step - Flux.jl - Julia\nDESCRIPTION: This function encapsulates the discriminator's training logic, calculating gradients through `Flux.withgradient` and applying parameter updates via Flux's optimizer interface. It expects real and fake images along with model/optimizer references, and returns the computed loss. The function assumes required loss functions and update utilities are defined elsewhere in the file or imported as dependencies.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-10-08-dcgan-mnist.md#_snippet_12\n\nLANGUAGE: julia\nCODE:\n```\nfunction train_discriminator!(gen, disc, real_img, fake_img, opt, ps, hparams)\n\n    disc_loss, grads = Flux.withgradient(ps) do\n        discriminator_loss(disc(real_img), disc(fake_img))\n    end\n\n    # Update the discriminator parameters\n    update!(opt, ps, grads)\n    return disc_loss\nend\n```\n\n----------------------------------------\n\nTITLE: Full GAN Training Loop and Image Export - Flux.jl - Julia\nDESCRIPTION: This comprehensive training function initializes models, optimizers, and fixed noise for monitoring, then carries out a multi-epoch GAN training loop. It periodically saves example generated images to disk for visual feedback and supports seamless switching between CUDA and CPU devices. Dependencies include Flux.jl, the model and loss definitions, and a filesystem access library for saving images. Inputs are encapsulated in the hparams parameter structure.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-10-08-dcgan-mnist.md#_snippet_14\n\nLANGUAGE: julia\nCODE:\n```\nfunction train(hparams)\n\n    dev = hparams.device\n    # Check if CUDA is actually present\n    if hparams.device == gpu\n        if !CUDA.has_cuda()\n        dev = cpu\n        @warn \"No gpu found, falling back to CPU\"\n        end\n    end\n\n    # Load the normalized MNIST images\n    dataloader = load_MNIST_images(hparams)\n\n    # Initialize the models and pass them to correct device\n    disc = Discriminator() |> dev\n    gen =  Generator(hparams.latent_dim) |> dev\n\n    # Collect the generator and discriminator parameters\n    disc_ps = params(disc)\n    gen_ps = params(gen)\n\n    # Initialize the ADAM optimisers for both the sub-models\n    # with respective learning rates\n    disc_opt = ADAM(hparams.disc_lr)\n    gen_opt = ADAM(hparams.gen_lr)\n\n    # Create a batch of fixed noise for visualizing the training of generator over time\n    fixed_noise = [randn(Float32, hparams.latent_dim, 1) |> dev for _=1:hparams.output_dim^2]\n\n    # Training loop\n    train_steps = 0\n    for ep in 1:hparams.epochs\n        @info \"Epoch $ep\"\n        for real_img in dataloader\n\n            # Transfer the data to the GPU\n            real_img = real_img |> dev\n\n            # Create a random noise\n            noise = randn!(similar(real_img, (hparams.latent_dim, hparams.batch_size)))\n            # Pass the noise to the generator to create a fake imagae\n            fake_img = gen(noise)\n\n            # Update discriminator and generator\n            loss_disc = train_discriminator!(gen, disc, real_img, fake_img, disc_opt, disc_ps, hparams)\n            loss_gen = train_generator!(gen, disc, fake_img, gen_opt, gen_ps, hparams)\n\n            if train_steps % hparams.verbose_freq == 0\n                @info(\"Train step $(train_steps), Discriminator loss = $(loss_disc), Generator loss = $(loss_gen)\")\n                # Save generated fake image\n                output_image = create_output_image(gen, fixed_noise, hparams)\n                save(@sprintf(\"output/dcgan_steps_%06d.png\", train_steps), output_image)\n            end\n            train_steps += 1\n        end\n    end\n\n    output_image = create_output_image(gen, fixed_noise, hparams)\n    save(@sprintf(\"output/dcgan_steps_%06d.png\", train_steps), output_image)\n\n    return nothing\nend\n```\n\n----------------------------------------\n\nTITLE: Building the Generator Model - DCGAN with Flux.jl - Julia\nDESCRIPTION: Declares a `Generator` construction function returning a neural network chain with Dense, BatchNorm, ConvTranspose, and reshaping layers appropriate for the DCGAN generator. Transforms input noise vectors of length latent_dim into 28x28 grayscale images, progressively upsampling and normalizing at each step. Uses custom weight initialization and relu/tanh activations, with dependency on Flux.jl and the previously defined `dcgan_init`.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-10-08-dcgan-mnist.md#_snippet_5\n\nLANGUAGE: julia\nCODE:\n```\nfunction Generator(latent_dim)\n    Chain(\n        Dense(latent_dim  => 7*7*256, bias=false),\n        BatchNorm(7*7*256, relu),\n\n        x -> reshape(x, 7, 7, 256, :),\n\n        ConvTranspose((5, 5), 256 => 128; stride = 1, pad = 2, init = dcgan_init, bias=false),\n        BatchNorm(128, relu),\n\n        ConvTranspose((4, 4), 128 => 64; stride = 2, pad = 1, init = dcgan_init, bias=false),\n        BatchNorm(64, relu),\n\n        # The tanh activation ensures that output is in range of (-1, 1)\n        ConvTranspose((4, 4), 64 => 1, tanh; stride = 2, pad = 1, init = dcgan_init, bias=false),\n    )\nend\n```\n\n----------------------------------------\n\nTITLE: Hyperparameter Definition and Model Training Invocation - Flux.jl - Julia\nDESCRIPTION: This snippet demonstrates the instantiation of a HyperParams object with default values and launches the model training process. It is a high-level entry point that ties together previous function definitions; required dependencies include a valid HyperParams type and earlier defined training routines.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-10-08-dcgan-mnist.md#_snippet_15\n\nLANGUAGE: julia\nCODE:\n```\n# Define the hyper-parameters (here, we go with the default ones)\nhparams = HyperParams()\ntrain(hparams)\n```\n\n----------------------------------------\n\nTITLE: Defining Hyperparameter Structure for DCGAN - Julia\nDESCRIPTION: Creates a Julia struct, `HyperParams`, with default fields for common DCGAN hyperparameters, including batch size, latent vector length, training epochs, learning rates, verbosity, output image dimension, and compute device (CPU or GPU). Uses Julia\\'s `@kwdef` macro for convenient default value initialization. Must be used with compatible Flux and CUDA packages.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-10-08-dcgan-mnist.md#_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\nBase.@kwdef struct HyperParams\n    batch_size::Int = 128\n    latent_dim::Int = 100\n    epochs::Int = 25\n    verbose_freq::Int = 1000\n    output_dim::Int = 5\n    disc_lr::Float64 = 0.0002\n    gen_lr::Float64 = 0.0002\n    device::Function = gpu\nend\n```\n\n----------------------------------------\n\nTITLE: Exporting Training Images to Animated GIF - Julia\nDESCRIPTION: This post-training utility collates generated images from the output directory and creates an animated GIF from them. It relies on filesystem access and image loading/saving libraries in Julia, and expects the images to have been previously generated and saved in the specified folder. This is primarily for visualization and presentation of training progress.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-10-08-dcgan-mnist.md#_snippet_16\n\nLANGUAGE: julia\nCODE:\n```\nfolder = \"output\"\n# Get the image filenames from the folder\nimg_paths = readdir(folder, join=true)\n# Load all the images as an array\nimages = load.(img_paths)\n# Join all the images in the array to create a matrix of images\ngif_mat = cat(images..., dims=3)\nsave(\"./output.gif\", gif_mat)\n```\n\n----------------------------------------\n\nTITLE: Computing Gradients with Respect to Function Parameters (Explicit and Structural) in Julia\nDESCRIPTION: This snippet shows how Flux.gradient computes structural gradients for parameterized functions. For poly2 (explicit θ argument), the gradient returns a tuple with explicit derivatives, while for poly3s (struct-encapsulated parameters), a NamedTuple matching the struct fields is returned. Required dependencies: Flux.jl, previous function/struct definitions. Inputs: function and parameter representations; outputs: tuples with gradients with respect to each parameter and input. Emphasizes explicit versus structural parameter tracking.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/basics.md#_snippet_9\n\nLANGUAGE: julia\nCODE:\n```\njulia> grad2 = gradient(poly2, 5, θ)\n(2.0, [1.0, 5.0, 25.0])\n\njulia> grad3 = gradient((x,p) -> p(x), 5, poly3s)\n(2.0, (θ3 = [1.0, 5.0, 25.0],))\n```\n\n----------------------------------------\n\nTITLE: Computing Accuracy for Classification with Flux - Julia\nDESCRIPTION: Defines the accuracy function to calculate model accuracy over all batched data in a DataLoader. Decodes model predictions and truth using onecold, counts matches, and aggregates correct predictions. Designed for datasets in Flux pipeline, expects DataLoader and model as arguments, returns scalar accuracy value.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-mlp.md#_snippet_5\n\nLANGUAGE: julia\nCODE:\n```\nfunction accuracy(dataloader, model)\n    acc = 0\n    n = 0\n    for (x, y) in dataloader\n        acc += sum(onecold(cpu(model(x))) .== onecold(cpu(y)))\n        n += MLUtils.numobs(x)\n    end\n    return acc / n\nend\n```\n\n----------------------------------------\n\nTITLE: Dynamically Building a CNN using Flux.outputsize in Julia\nDESCRIPTION: Defines a function `make_model` that constructs a convolutional neural network (CNN) based on specified image dimensions, channel configuration, and class count. It uses `Flux.outputsize` to calculate the output size of the convolutional layers (`conv_outsize`) and then uses this size (`prod(conv_outsize)`) to define the input dimension of the final `Dense` layer. The example demonstrates creating a model and verifying its output size against the expected size.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/outputsize.md#_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\nusing Flux\n\"\"\"\n    make_model(width, height, [inchannels, nclasses; layer_config])\n\nCreate a CNN for a given set of configuration parameters. Arguments:\n- `width`, `height`: the input image size in pixels\n- `inchannels`: the number of channels in the input image, default `1`\n- `nclasses`: the number of output classes, default `10`\n- Keyword `layer_config`: a vector of the number of channels per layer, default `[16, 16, 32, 64]`\n\"\"\"\nfunction make_model(width, height, inchannels = 1, nclasses = 10;\n                    layer_config = [16, 16, 32, 64])\n  # construct a vector of layers:\n  conv_layers = []\n  push!(conv_layers, Conv((5, 5), inchannels => layer_config[1], relu, pad=SamePad()))\n  for (inch, outch) in zip(layer_config, layer_config[2:end])\n    push!(conv_layers, Conv((3, 3), inch => outch, sigmoid, stride=2))\n  end\n\n  # compute the output dimensions after these conv layers:\n  conv_outsize = Flux.outputsize(conv_layers, (width, height, inchannels); padbatch=true)\n\n  # use this to define appropriate Dense layer:\n  last_layer = Dense(prod(conv_outsize) => nclasses)\n  return Chain(conv_layers..., Flux.flatten, last_layer)\nend\n\nm = make_model(28, 28, 3, layer_config = [9, 17, 33, 65])\n\nFlux.outputsize(m, (28, 28, 3, 42)) == (10, 42) == size(m(randn(Float32, 28, 28, 3, 42)))\n```\n\n----------------------------------------\n\nTITLE: Training Flux Models with Enzyme using Flux.train! in Julia\nDESCRIPTION: Demonstrates performing a model training step using `Flux.train!` when Enzyme is the AD backend. The `Enzyme.Duplicated` wrapped model is passed directly to `train!`, along with the loss function, training data (as an iterable), and the optimizer state obtained from `Flux.setup`. This automatically handles gradient calculation via Enzyme and parameter updates.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/training/enzyme.md#_snippet_3\n\nLANGUAGE: julia-repl\nCODE:\n```\njulia> opt_state = Flux.setup(Adam(0), model);\n\njulia> Flux.train!((m,x,y) -> sum(abs2, m(x) .- y), dup_model, [(x1, y1)], opt_state)\n```\n\n----------------------------------------\n\nTITLE: Using and Testing a Custom Join Layer with GPUs (Julia)\nDESCRIPTION: This snippet shows practical integration of the custom Join layer in a full model pipeline, including GPU compatibility. It builds a compound model with several branches, demonstrates input preparation and model invocation, and highlights that the output is a combined vector. Dependencies include Flux, CUDA, and previously defined Join and Chain layers. Inputs and model structure must match for successful execution.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/custom_layers.md#_snippet_7\n\nLANGUAGE: julia\nCODE:\n```\nmodel = Chain(\n              Join(vcat,\n                   Chain(Dense(1 => 5, relu), Dense(5 => 1)), # branch 1\n                   Dense(1 => 2),                             # branch 2\n                   Dense(1 => 1)                              # branch 3\n                  ),\n              Dense(4 => 1)\n             ) |> gpu\n\nxs = map(gpu, (rand(1), rand(1), rand(1)))\n\nmodel(xs)\n# returns a single float vector with one value\n```\n\n----------------------------------------\n\nTITLE: Defining a Convolutional Neural Network with Flux.jl in Julia\nDESCRIPTION: This snippet defines a CNN model for image classification using the Flux.jl library in Julia. The architecture involves several convolutional, pooling, and dense layers, finishing with a 10-class output, and is set up to run on a GPU. The model expects input images with 3 color channels and outputs probabilities for 10 classes. Ensure Flux and GPU support libraries are installed (e.g., CUDA.jl for GPU).\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-blitz.md#_snippet_18\n\nLANGUAGE: julia\nCODE:\n```\nmodel = Chain(\n          Conv((5,5), 3 => 16, relu),\n          MaxPool((2, 2)),\n          Conv((5, 5), 16 => 8, relu),\n          MaxPool((2,2)),\n          x -> reshape(x, :, size(x, 4)),\n          Dense(200 => 120),\n          Dense(120 => 84),\n          Dense(84 => 10)) |> gpu\n```\n\n----------------------------------------\n\nTITLE: Optimized Device Reuse for Batch Data/Model Transfer in Flux.jl (Julia)\nDESCRIPTION: Shows how to extract and reuse a device object (from gpu_device()) for repeated data/model transfer operations, which improves type stability and efficiency in training loops. Illustrates a training loop where batches are transferred with the to_device function obtained beforehand. Requires prior setup of GPU packages; input/output expected as arrays or tuples.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/gpu.md#_snippet_8\n\nLANGUAGE: julia\nCODE:\n```\nto_device = gpu_device()\ngpu_model = model |> to_device\n\nfor epoch in 1:num_epochs\n    for (x, y) in dataloader\n        x_gpu, y_gpu = (x, y) |> to_device\n        # training code...\n\n```\n\n----------------------------------------\n\nTITLE: Defining and Calculating Custom Model Accuracy in Julia\nDESCRIPTION: Defines a function `custom_accuracy` to calculate the accuracy of the custom model. It takes weights, biases, features, and original labels (`y`). It generates predictions using `custom_model`, converts predictions to class names using `custom_onecold`, compares them to the true labels, and calculates the mean accuracy. The snippet also shows the accuracy calculation for the untrained model.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_19\n\nLANGUAGE: julia\nCODE:\n```\n```jldoctest logistic_regression; filter = r\"[+-]?([0-9]*[.])?[0-9]+(f[+-]*[0-9])?\"\njulia> custom_accuracy(W, b, x, y) = mean(custom_onecold(custom_model(W, b, x)) .== y);\n\njulia> custom_accuracy(W, b, x, y)\n0.3333333333333333\n```\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication and Linear Algebra with Julia Arrays - julia\nDESCRIPTION: The following snippet showcases the creation of random matrices and vectors and then performs a matrix-vector multiplication, which is fundamental in ML workflows. It requires no dependencies beyond Julia's base and demonstrates how to connect randomly initialized weights and input data for linear transformations. The input is a (5,10) matrix and a 10-element vector, and the output is a 5-element result.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-blitz.md#_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\nW = randn(5, 10)\nx = rand(10)\nW * x\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing MNIST for DCGAN - Flux.jl - Julia\nDESCRIPTION: Defines a Julia function to load and normalize MNIST handwritten digit images for input into the DCGAN. Images are normalized to the range (-1, 1) and reshaped for convolutional input. The `DataLoader` utility splits the data into mini-batches and enables shuffling. Takes a hyperparameter struct to determine batch size. Outputs a DataLoader ready for training loops; data is initially on CPU and should be transferred to GPU if used.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-10-08-dcgan-mnist.md#_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\nfunction load_MNIST_images(hparams)\n    images = MNIST.traintensor(Float32)\n\n    # Normalize the images to (-1, 1)\n    normalized_images = @. 2f0 * images - 1f0\n    image_tensor = reshape(normalized_images, 28, 28, 1, :)\n\n    # Create a dataloader that iterates over mini-batches of the image tensor\n    dataloader = DataLoader(image_tensor, batchsize=hparams.batch_size, shuffle=true)\n\n    return dataloader\nend\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Iris Data: Type Conversion and Manual One-Hot Encoding in Julia\nDESCRIPTION: Converts the feature matrix `x` to `Float32` type, often preferred for performance in machine learning. Flattens the target vector `y` using `vec()`. Performs manual one-hot encoding by comparing unique labels against the permuted target vector, creating a `BitMatrix` where each column represents a one-hot encoded label.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\njulia> x = Float32.(x);\n\njulia> y = vec(y);\n\njulia> custom_y_onehot = unique(y) .== permutedims(y)\n3×150 BitMatrix:\n 1  1  1  1  1  1  1  1  1  1  1  1  1  …  0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0     1  1  1  1  1  1  1  1  1  1  1  1\n```\n\n----------------------------------------\n\nTITLE: Using Flux's Built-in Parallel Layer as a Custom Join Layer (Julia)\nDESCRIPTION: This snippet demonstrates how the Join functionality can be replaced by the built-in Parallel layer for conciseness and maintainability. It illustrates two forms of constructing Join as syntactic sugar for Parallel, followed by an example of usage in a model pipeline, fully compatible with GPU execution. Depends on Flux, CUDA, and the Parallel layer's semantics for handling multi-branch architectures.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/custom_layers.md#_snippet_8\n\nLANGUAGE: julia\nCODE:\n```\nJoin(combine, paths) = Parallel(combine, paths)\nJoin(combine, paths...) = Join(combine, paths)\n\n# use vararg/tuple version of Parallel forward pass\nmodel = Chain(\n              Join(vcat,\n                   Chain(Dense(1 => 5, relu), Dense(5 => 1)),\n                   Dense(1 => 2),\n                   Dense(1 => 1)\n                  ),\n              Dense(4 => 1)\n             ) |> gpu\n\nxs = map(gpu, (rand(1), rand(1), rand(1)))\n\nmodel(xs)\n# returns a single float vector with one value\n```\n\n----------------------------------------\n\nTITLE: Plotting Training Loss Curve with Plots.jl\nDESCRIPTION: This Julia snippet utilizes the Plots.jl library to visualize the training progress. It plots the loss value recorded for each individual batch against the iteration number (on a log scale). It also overlays a plot of the mean loss per epoch, calculated using `Iterators.partition`. This requires the `losses` list and the `loader` (to determine epoch length `n`) from the main training snippet.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/quickstart.md#_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\nplot(losses; xaxis=(:log10, \"iteration\"),\n    yaxis=\"loss\", label=\"per batch\")\nn = length(loader)\nplot!(n:n:length(losses), mean.(Iterators.partition(losses, n)),\n    label=\"epoch mean\", dpi=200)\n```\n\n----------------------------------------\n\nTITLE: Encapsulating Parameters with Closures for a Polynomial Function in Julia\nDESCRIPTION: This code wraps the parameter vector θ3 within a closure, creating poly3 as an anonymous function that internally captures θ3. This encapsulation aids in model reusability and encapsulates parameter state within the function. The closure approach is idiomatic in Julia for models with internal state. Required dependencies: Julia Base. Input: real number x; output: polynomial result. Limitation: the closure is limited to the lifetime of its local θ3.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/basics.md#_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\npoly3 = let θ3 = [10, 1, 0.1]\n    x -> evalpoly(x, θ3)\nend\n\npoly3(5) == 17.5  # true\n\n# output\n\ntrue\n```\n\n----------------------------------------\n\nTITLE: Defining a Parameterized Polynomial Function with Explicit Arguments in Julia\nDESCRIPTION: This snippet demonstrates defining a polynomial function, poly2, that explicitly accepts the parameter vector θ2 as an argument instead of using a global variable. It uses Julia's evalpoly from Base.Math as the core computation. This approach provides clarity in parameter passing, facilitates gradient computation with respect to parameters, and eases function reuse. Required dependencies: Julia Base; requires evalpoly to be available. Inputs: real number x and vector θ2; outputs: polynomial evaluation as a real number.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/basics.md#_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\npoly2(x::Real, θ2) = evalpoly(x, θ2)  # built-in, from Base.Math\n\npoly2(5, θ) == 17.5  # true\n\n# output\n\ntrue\n```\n\n----------------------------------------\n\nTITLE: Building Custom Patience-Based Triggers for Threshold Detection - Flux.jl - Julia\nDESCRIPTION: This snippet introduces a custom trigger that uses Flux.jl's patience utility to monitor whether a metric function stays below a threshold for several steps. It defines a threshold helper that wraps a predicate with patience logic, allowing for flexible compound triggers. The only dependency is Flux.jl, with main parameters being the monitored function, threshold value, and delay. Output is a trigger function suitable for use as a stopping condition in loops.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/training/callbacks.md#_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\nthreshold(f, thresh, delay) = patience(delay) do\\n  f() < thresh\\nend\n```\n\n----------------------------------------\n\nTITLE: Creating a Controlled Pseudo-Loss and Early Stopping Trigger - Flux.jl - Julia\nDESCRIPTION: This snippet demonstrates how to construct a stateful pseudo-loss function using a closure in Julia, which initially decreases before increasing, simulating typical model loss behavior during training. It shows how to create an early stopping trigger with Flux.jl that halts training if the loss increases for two consecutive steps, using an initial score and tracking epochs within a training loop. Required dependency: Flux.jl. Inputs include no-argument functions for evaluating loss, and outputs are triggers (boolean functions) that control loop execution.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/training/callbacks.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\n# create a pseudo-loss that decreases for 4 calls, then starts increasing\\n# we call this like loss()\\nloss = let t = 0\\n  () -> begin\\n    t += 1\\n    (t - 4) ^ 2\\n  end\\nend\\n\\n# create an early stopping trigger\\n# returns true when the loss increases for two consecutive steps\\nes = early_stopping(loss, 2; init_score = 9)\\n\\n# this will stop at the 6th (4 decreasing + 2 increasing calls) epoch\\nfor epoch in 1:10\\n  es() && break\\nend\n```\n\n----------------------------------------\n\nTITLE: Calculating Gradients with Flux.gradient and Enzyme.Duplicated in Julia\nDESCRIPTION: Demonstrates setting up a Flux model, wrapping it with `Enzyme.Duplicated` to allocate space for gradients and enable Enzyme AD, and calculating the gradient of a loss function using `Flux.gradient`. Inputs not requiring gradients are wrapped in `Const`. This setup signals Flux to use Enzyme instead of Zygote for differentiation.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/training/enzyme.md#_snippet_0\n\nLANGUAGE: julia-repl\nCODE:\n```\njulia> using Flux, Enzyme\n\njulia> model = Chain(Dense(28^2 => 32, sigmoid), Dense(32 => 10), softmax);  # from model zoo\n\njulia> dup_model = Enzyme.Duplicated(model)  # this allocates space for the gradient\nDuplicated(\n  Chain(\n    Dense(784 => 32, σ),                # 25_120 parameters\n    Dense(32 => 10),                    # 330 parameters\n    NNlib.softmax,\n  ),\n  # norm(∇) ≈ 0.0f0\n)                   # Total: 4 arrays, 25_450 parameters, 199.391 KiB.\n\njulia> x1 = randn32(28*28, 1);  # fake image\n\njulia> y1 = [i==3 for i in 0:9];  # fake label\n\njulia> grads_f = Flux.gradient((m,x,y) -> sum(abs2, m(x) .- y), dup_model, Const(x1), Const(y1))  # uses Enzyme\n((layers = ((weight = Float32[-0.010354728 0.032972857 …\n    -0.0014538406], σ = nothing), nothing),), nothing, nothing)\n```\n\n----------------------------------------\n\nTITLE: Constructing Training and Test Data Using Julia and the Target Function\nDESCRIPTION: Generates matrix-form training and test input samples (x_train, x_test) using ranges, and then computes corresponding labels (y_train, y_test) via the previously defined `actual` function. This creates deterministic inputs and outputs for supervised model fitting. Inputs are integer ranges; outputs are numerically computed target values. No external dependencies beyond previous setup.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/overview.md#_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\njulia> x_train, x_test = hcat(0:5...), hcat(6:10...)\n([0 1 … 4 5], [6 7 … 9 10])\n\njulia> y_train, y_test = actual.(x_train), actual.(x_test)\n([2 6 … 18 22], [26 30 … 38 42])\n```\n\n----------------------------------------\n\nTITLE: Referencing NNlib Batched Operation Functions in Julia\nDESCRIPTION: Lists functions for performing batched matrix operations from NNlib.jl for documentation generation using Julia's `@docs` macro. Includes functions like `batched_mul`, `batched_mul!`, `batched_adjoint`, `batched_transpose`, and `batched_vec`. `batched_mul` is used internally by Flux's `Bilinear` layer.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/models/nnlib.md#_snippet_7\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\nbatched_mul\nbatched_mul!\nbatched_adjoint\nbatched_transpose\nbatched_vec\n```\n```\n\n----------------------------------------\n\nTITLE: Documenting Zygote Gradient and Jacobian APIs in Julia\nDESCRIPTION: This snippet documents the main API functions provided by Zygote.jl for calculating gradients, Jacobians, and Hessians, including variants for returning auxiliary information. It lists function signatures for explicit differentiation in Flux, which expects users to provide a function and its arguments. These entries serve as in-doc lookups for users using Flux's integration with Zygote. There are no direct dependencies aside from Zygote.jl, and the listing covers Zygote.gradient, Zygote.withgradient, Zygote.jacobian, Zygote.withjacobian, Zygote.hessian, Zygote.hessian_reverse, Zygote.diaghessian, and Zygote.pullback. Parameters reflect those accepted by the respective Zygote functions, and outputs include gradients or Jacobians/hessians of given functions.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/training/zygote.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nZygote.gradient(f, args...)\nZygote.withgradient(f, args...)\nZygote.jacobian(f, args...)\nZygote.withjacobian(f, args...)\nZygote.hessian\nZygote.hessian_reverse\nZygote.diaghessian\nZygote.pullback\n```\n\n----------------------------------------\n\nTITLE: Referencing Gradient Clipping Types from Optimisers.jl in Julia Docs\nDESCRIPTION: Uses the `@docs` directive to incorporate documentation for gradient clipping types (`ClipGrad`, `ClipNorm`) from `Optimisers.jl` into the Flux.jl documentation. These can be used within an `OptimiserChain` to constrain gradient magnitudes.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/training/optimisers.md#_snippet_6\n\nLANGUAGE: julia\nCODE:\n```\nOptimisers.ClipGrad\nOptimisers.ClipNorm\n```\n\n----------------------------------------\n\nTITLE: Instantiating a Predictive Model Layer as a Function with Flux in Julia\nDESCRIPTION: Creates a new Dense(1 => 1) layer named `predict`, emphasizing that Flux models act as callable functions. This layer uses the same configuration as before and can be immediately used to process input and generate predictions. No new external dependencies are introduced.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/overview.md#_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\njulia> predict = Dense(1 => 1)\nDense(1 => 1)       # 2 parameters\n```\n\n----------------------------------------\n\nTITLE: Normalizing Training Data with Flux.normalise in Julia\nDESCRIPTION: Normalizes the training features (`x_train`) using the `Flux.normalise` function, storing the result in `x_train_n`. It then calculates and displays the standard deviation of the normalized data (`x_train_n`) using `Statistics.std` to confirm that it is now close to 1.0.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_26\n\nLANGUAGE: julia\nCODE:\n```\njulia> x_train_n = Flux.normalise(x_train);\n```\n\nLANGUAGE: julia\nCODE:\n```\n\njulia> std(x_train_n)\n1.0000844f0\n```\n\n----------------------------------------\n\nTITLE: Registering and Forward Pass for Join Layer in Flux (Julia)\nDESCRIPTION: This code registers the Join struct for Flux and defines its forward-call logic, allowing multiple inputs to be processed through multiple submodels and combined by a arbitrary function (e.g., concatenation). Each model path and input tuple are mapped and the combine function applied. Prerequisites: previously defined Join struct, Flux, and proper input formatting.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/custom_layers.md#_snippet_6\n\nLANGUAGE: julia\nCODE:\n```\nFlux.@layer Join\n```\n\nLANGUAGE: julia\nCODE:\n```\n(m::Join)(xs::Tuple) = m.combine(map((f, x) -> f(x), m.paths, xs)...) \n(m::Join)(xs...) = m(xs)\n```\n\n----------------------------------------\n\nTITLE: Using a Stateful Learning Rate Scheduler in ParameterSchedulers.jl with Flux.jl (Julia)\nDESCRIPTION: This block demonstrates creation and iteration of a stateful learning rate scheduler using `ParameterSchedulers.Stateful` and `next!`, integrated with Flux.jl optimization. The learning rate schedule is advanced explicitly at each training epoch, updating the optimizer state accordingly. This approach is suitable when the learning schedule must retain internal state across iterations. Requires ParameterSchedulers and Flux, with the model and training code defined elsewhere.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/training/training.md#_snippet_18\n\nLANGUAGE: julia\nCODE:\n```\nusing ParameterSchedulers: Stateful, next!\n\nschedule = Stateful(Cos(\\u03bb0 = 1e-4, \\u03bb1 = 1e-2, period = 10))\nfor epoch in 1:100\n  Flux.adjust!(opt_state, next!(schedule))\n  # your training code here\nend\n```\n\n----------------------------------------\n\nTITLE: Importing Required Julia Packages for Logistic Regression\nDESCRIPTION: Imports necessary Julia packages: Flux for machine learning, Statistics for basic stats, MLDatasets for loading datasets (Iris), DataFrames for data manipulation, and OneHotArrays for encoding categorical labels. This sets up the environment for the logistic regression task.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\njulia> using Flux, Statistics, MLDatasets, DataFrames, OneHotArrays\n```\n\n----------------------------------------\n\nTITLE: Defining Helper Functions for Training in Julia\nDESCRIPTION: Defines three utility functions for the training process. `augment` applies simple data augmentation by adding scaled Gaussian noise to the input image `x` (potentially on the GPU). `anynan` checks if any model parameters (`x`, typically `Flux.params(model)`) contain NaN values, returning true if any NaN is found. `accuracy` computes the classification accuracy by comparing the model's predictions (`model(x)`) with the true labels (`y`), ensuring comparison happens on the CPU using `onecold` and `cpu`.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-02-07-convnet.md#_snippet_5\n\nLANGUAGE: julia\nCODE:\n```\naugment(x) = x .+ gpu(0.1f0*randn(eltype(x), size(x)))\nanynan(x) = any(y -> any(isnan, y), x)\naccuracy(x, y, model) = mean(onecold(cpu(model(x))) .== onecold(cpu(y)))\n```\n\n----------------------------------------\n\nTITLE: Transferring Arrays to GPU with CUDA.jl - julia\nDESCRIPTION: This snippet moves data to a CUDA-enabled GPU using the cu() constructor from the CUDA.jl package, after drawing random numbers for a (5,3) matrix. This approach allows all operations demonstrated earlier to run transparently on GPU hardware for acceleration. Requires the CUDA Julia package and GPU support; input is a standard Julia array, output is a CUDA array.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-blitz.md#_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\nusing CUDA\nx = cu(rand(5, 3))\n```\n\n----------------------------------------\n\nTITLE: Custom Join Layer for Multiple Inputs in Flux (Julia)\nDESCRIPTION: This snippet defines a custom Join layer for handling multiple inputs, parameterized for type stability and flexibility in field types. It allows arbitrary combining functions and pipelines for each input path and demonstrates use with Julia's parametric types system. Also includes a convenient constructor. Prerequisites: Flux, understanding of Julia structs, and knowledge of custom layer construction.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/custom_layers.md#_snippet_5\n\nLANGUAGE: julia\nCODE:\n```\nusing Flux\nusing CUDA\n\n# custom join layer\nstruct Join{T, F}\n  combine::F\n  paths::T\nend\n\n# allow Join(op, m1, m2, ...) as a constructor\nJoin(combine, paths...) = Join(combine, paths)\n```\n\n----------------------------------------\n\nTITLE: Building the Discriminator Model - DCGAN with Flux.jl - Julia\nDESCRIPTION: Defines a `Discriminator` function constructing a neural network for binary classification of real vs. fake images. Uses convolutional layers with leakyrelu activations, dropout, flattening, and a final dense output layer. Employs custom Gaussian initialization and mirrors typical DCGAN discriminator architectures. Requires Flux.jl and prior definition of `dcgan_init`.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-10-08-dcgan-mnist.md#_snippet_7\n\nLANGUAGE: julia\nCODE:\n```\nfunction Discriminator()\n    Chain(\n        Conv((4, 4), 1 => 64; stride = 2, pad = 1, init = dcgan_init),\n        x->leakyrelu.(x, 0.2f0),\n        Dropout(0.3),\n\n        Conv((4, 4), 64 => 128; stride = 2, pad = 1, init = dcgan_init),\n        x->leakyrelu.(x, 0.2f0),\n        Dropout(0.3),\n\n        # The output is now of the shape (7, 7, 128, batch_size)\n        flatten,\n        Dense(7 * 7 * 128, 1) \n    )\nend\n```\n\n----------------------------------------\n\nTITLE: Visualizing Synthetic Dataset Using Plots.jl in Julia\nDESCRIPTION: Creates a scatter plot to visualize the relationship between features x and labels y using the plot function with vectorized inputs. The function customizes the appearance (linewidth, labels, titles) and requires the Plots.jl package. This demonstrates data distribution and assists in verifying expected correlation for regression.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_4\n\nLANGUAGE: julia\nCODE:\n```\njulia> plot(vec(x), vec(y), lw = 3, seriestype = :scatter, label = \"\", title = \"Generated data\", xlabel = \"x\", ylabel= \"y\");\n```\n\n----------------------------------------\n\nTITLE: Structural Gradients for Composed Parameterized Functions in Julia\nDESCRIPTION: This snippet demonstrates how Flux.gradient can compute nested structural gradients for composed models (using ComposedFunction). It returns NamedTuples matching the nested structure of the composed function's parameters. Required dependencies: Flux.jl, definitions for poly4 and its components. Inputs: composed function, numeric input; outputs: tuple with nested NamedTuples of gradients. Reveals the flexibility of Flux's structural gradient mechanism.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/basics.md#_snippet_10\n\nLANGUAGE: julia\nCODE:\n```\njulia> grad4 = gradient(|>, 5, poly4)\n(1.0, (outer = (θ3 = [1.0, 17.5, 306.25],), inner = (θ3 = [0.5, 2.5, 12.5],)))\n```\n\n----------------------------------------\n\nTITLE: Customizing Trainable Parameters with Affine Layer (Julia REPL)\nDESCRIPTION: This interactive snippet illustrates how to define a custom Affine layer struct, constructor, and forward call, and subsequently adjust which fields are marked as trainable by overloading Flux.trainable. It demonstrates default versus custom trainable behavior and how only marked fields are updated during training, but all fields are visible for GPU/f16 transformation. Dependencies include Flux. Key parameters are W (weights) and b (bias), and the approach works with numerical array fields.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/custom_layers.md#_snippet_2\n\nLANGUAGE: julia-repl\nCODE:\n```\njulia> struct Affine\n        W\n        b\n      end\n\njulia> Affine(in::Int, out::Int) = Affine(randn(out, in), randn(out));\n\njulia> (m::Affine)(x) = m.W * x .+ m.b;\n\njulia> Flux.@layer Affine\n\njulia> a = Affine(Float32[1 2; 3 4; 5 6], Float32[7, 8, 9])\nAffine(Float32[1.0 2.0; 3.0 4.0; 5.0 6.0], Float32[7.0, 8.0, 9.0])\n\njulia> Flux.trainable(a) # default behavior\n(W = Float32[1.0 2.0; 3.0 4.0; 5.0 6.0], b = Float32[7.0, 8.0, 9.0])\n\njulia> Flux.trainable(a::Affine) = (; W = a.W)  # returns a NamedTuple using the field's name\n\njulia> Flux.trainable(a)\n(W = Float32[1.0 2.0; 3.0 4.0; 5.0 6.0],)\n```\n\n----------------------------------------\n\nTITLE: Training Generator Step - Flux.jl - Julia\nDESCRIPTION: This function defines a modular generator optimization step, computing the generator loss and gradients using Flux. It expects the discriminator and generated 'fake' images, along with optimization-related parameters, then applies the updates accordingly. The design isolates generator training logic for flexibility within larger training routines and assumes generator_loss and update! are available.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-10-08-dcgan-mnist.md#_snippet_13\n\nLANGUAGE: julia\nCODE:\n```\nfunction train_generator!(gen, disc, fake_img, opt, ps, hparams)\n\n    gen_loss, grads = Flux.withgradient(ps) do\n        generator_loss(disc(fake_img))\n    end\n\n    update!(opt, ps, grads)\n    return gen_loss\nend\n```\n\n----------------------------------------\n\nTITLE: Creating Mini-batches for Training Data in Julia\nDESCRIPTION: Defines the `make_minibatch` function to prepare a single mini-batch of images and labels for training. It takes the full image dataset (`X`), the corresponding labels (`Y`), and a set of indices (`idxs`) for the current batch. It constructs an `Array{Float32}` for the image batch, populates it with the selected images cast to `Float32`, and creates a one-hot encoded label batch using `Flux.onehotbatch`.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-02-07-convnet.md#_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\nfunction make_minibatch(X, Y, idxs)\n   X_batch = Array{Float32}(undef, size(X)[1:end-1]..., 1, length(idxs))\n   for i in 1:length(idxs)\n       X_batch[:, :, :, i] = Float32.(X[:,:,idxs[i]])\n   end\n   Y_batch = onehotbatch(Y[idxs], 0:9)\n   return (X_batch, Y_batch)\nend\n```\n\n----------------------------------------\n\nTITLE: Setting Up Formatting Tooling with Julia - Julia\nDESCRIPTION: This snippet runs the setup script for the formatting tooling used by Flux.jl, which initializes the development environment and configures linting/formatting hooks. The script must be executed from the repository's root directory with Julia installed. It is a prerequisite to ensuring local code changes conform to the project's style rules. No arguments are needed; the script sets up the necessary pre-commit hook automatically.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/CONTRIBUTING.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\njulia dev/setup.jl\n```\n\n----------------------------------------\n\nTITLE: Transferring Flux Models to AMD GPUs with AMDGPU.jl in Julia\nDESCRIPTION: Demonstrates using AMDGPU.jl's roc() function to transfer Flux models and input data to AMD GPU devices. Disables scalar operations for GPU safety and shows gradient calculation adapted to ROCArray. Requires AMDGPU.jl and an AMD GPU with ROCm and MIOpen installed. Suitable for users targeting AMD hardware; outputs are ROCArray-backed models and tensors.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/gpu.md#_snippet_4\n\nLANGUAGE: julia\nCODE:\n```\nusing Flux, AMDGPU\nAMDGPU.allowscalar(false)\n\nr_model = roc(model)\nr_model(roc(x))\n\nFlux.gradient((f,x) -> sum(abs2, f(x)), r_model, roc(x))\n\n```\n\n----------------------------------------\n\nTITLE: Defining the Generator Network with Feed-Forward Layers in Flux (Julia)\nDESCRIPTION: Constructs the generator neural network as a series of Dense layers with leakyrelu activations, gradually expanding from latent dimension to image size. The output is shaped to match the image feature vector and normalized to [-1,1] via tanh to align with training data. GPU compatibility is built in. Requires latent_dim and n_features set beforehand. No output other than the generator object; constraints: design is tailored for 28x28 images.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-10-14-vanilla-gan.md#_snippet_5\n\nLANGUAGE: julia\nCODE:\n```\ngenerator = Chain(Dense(latent_dim, 256, x -> leakyrelu(x, 0.2f0)),\n                    Dense(256 => 512, x -> leakyrelu(x, 0.2f0)),\n                    Dense(512 => 1024, x -> leakyrelu(x, 0.2f0)),\n                    Dense(1024 => n_features, tanh)) |> gpu\n```\n\n----------------------------------------\n\nTITLE: Customizing Layer Initialization in Flux.jl (Julia)\nDESCRIPTION: Demonstrates how to construct a convolutional layer with a custom weight initializer using the init keyword argument in Julia's Flux.jl. The first example applies Flux.glorot_normal to Conv, and inspects the initialized bias. The setup block uses the Flux library, and the output shows the weight and bias shapes. No external dependencies are needed beyond Flux.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/utilities.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\n```jldoctest; setup = :(using Flux)\njulia> conv = Conv((3, 3), 3 => 2, relu; init=Flux.glorot_normal)\nConv((3, 3), 3 => 2, relu)  # 56 parameters\n\njulia> conv.bias\n2-element Vector{Float32}:\n 0.0\n 0.0\n```\n```\n\n----------------------------------------\n\nTITLE: Custom Split Layer for Multiple Outputs in Flux (Julia)\nDESCRIPTION: This snippet defines a custom Split layer in Flux.jl that splits a single input into multiple outputs via different paths. Struct parameterization ensures flexibility, and Flux.@layer is used for registration. The forward-call implementation applies all path functions to the input using map. All dependencies (Flux, CUDA) must be loaded, and input must conform to expected array types.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/custom_layers.md#_snippet_9\n\nLANGUAGE: julia\nCODE:\n```\nusing Flux\nusing CUDA\n\n# custom split layer\nstruct Split{T}\n  paths::T\nend\n\nSplit(paths...) = Split(paths)\n\nFlux.@layer Split\n\n(m::Split)(x::AbstractArray) = map(f -> f(x), m.paths)\n```\n\n----------------------------------------\n\nTITLE: Documenting ChainRulesCore Custom Rule APIs in Julia\nDESCRIPTION: This snippet lists the functions, macros, and types used for defining and handling custom differentiation rules in ChainRulesCore.jl, including rrule and frule for custom gradient (reverse-mode) and forward-mode rules, macros like @scalar_rule, and special types representing tangents. These are prerequisites for manual gradient definition, providing the low-level tools needed to override or supplement automatic differentiation in Julia environments like Flux. Correct use of these interfaces requires familiarity with the ChainRules API and careful management of tangent types and rule configs.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/training/zygote.md#_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\nChainRulesCore.rrule\nChainRulesCore.frule\nChainRulesCore.@scalar_rule\nChainRulesCore.NoTangent\nChainRulesCore.ZeroTangent\nChainRulesCore.RuleConfig\nChainRulesCore.Tangent\nChainRulesCore.canonicalize\n```\n\n----------------------------------------\n\nTITLE: Using Metal GPU Backend for Flux Models with Metal.jl in Julia\nDESCRIPTION: Shows usage of Metal.jl's mtl() function for moving models and inputs to Apple M-series GPUs. Highlights that Metal support is experimental and disables scalar fallback for performance. Requires Metal.jl, an Apple M-series device, and illustrates model inference and gradient calculation. Outputs use MtlArray and Metal-backed compute.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/gpu.md#_snippet_5\n\nLANGUAGE: julia\nCODE:\n```\nusing Flux, Metal\nMetal.allowscalar(false)\n\nm_model = mtl(model)\nm_y = m_model(mtl(x))\n\nFlux.gradient((f,x) -> sum(abs2, f(x)), m_model, mtl(x))\n\n```\n\n----------------------------------------\n\nTITLE: Manually Running the Flux Formatter with Verbose Output - Julia\nDESCRIPTION: This snippet manually runs Flux’s custom formatter script across the codebase, providing verbose output for the operation. To use it, the developer must be in the root of the repository and have executed the setup step beforehand. The command takes optional arguments: the target directory (here, \".\") and an optional verbosity flag (\"--verbose\"). It formats all source files according to Flux's style guide.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/CONTRIBUTING.md#_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\njulia dev/flux_format.jl --verbose .\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing MNIST Data for GAN Training with Flux in Julia\nDESCRIPTION: Loads the MNIST dataset, extracts the training images, reshapes data, and rescales pixel values from [0,1] to [-1,1] to enhance GAN training stability. DataLoader is used to batch and shuffle samples efficiently. GPU transfer is performed if enabled, requiring CUDA. Inputs include batch size (from hyper-parameters); outputs are batches of prepared image data for training.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-10-14-vanilla-gan.md#_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\n    # Load the dataset\n    train_x, _ = MNIST.traindata(Float32);\n    # This dataset has pixel values ∈ [0:1]. Map these to [-1:1]\n    train_x = 2f0 * reshape(train_x, 28, 28, 1, :) .- 1f0 |>gpu;\n    # DataLoader allows to access data batch-wise and handles shuffling.\n    train_loader = DataLoader(train_x, batchsize=batch_size, shuffle=true);\n```\n\n----------------------------------------\n\nTITLE: Defining Simple Neural Network Layers with Various Parameterizations in Julia\nDESCRIPTION: This snippet creates a neural network layer that maps vector inputs to outputs using matrix multiplication, bias addition, and activation, defined in three forms: global variables (layer1), explicit arguments (layer2), and closure encapsulation (layer3). sigmoid is implemented as an activation. Required dependencies: Julia Base, random number generation. Inputs: input vector x; outputs: output vector of length 2. Limitation: proper initialization of parameters (W, b) is necessary.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/basics.md#_snippet_14\n\nLANGUAGE: julia\nCODE:\n```\nW = randn(2, 3)\nb = zeros(2)\n\nsigmoid(x::Real) = 1 / (1 + exp(-x))\nlayer1(x) = sigmoid.(W*x .+ b)\n\n# output\n\nlayer1 (generic function with 1 method)\n```\n\n----------------------------------------\n\nTITLE: Computing the Confusion Matrix for Classification Results in Julia\nDESCRIPTION: Computes and displays the confusion matrix summarizing prediction errors across all classes. It processes test batches on CPU, applies the trained model, collects predicted and true digit indices, and tallies them in a 10x10 matrix. This is useful for detailed per-class evaluation. Requires the model, testloader, and functions like onecold and numobs.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-blitz.md#_snippet_26\n\nLANGUAGE: julia\nCODE:\n```\nconfusion_matrix = zeros(Int, 10, 10)\nm = model |> cpu\nfor batch in testloader\n    @show numobs(batch)\n    x, y = batch\n    preds = m(x)\n    ŷ = onecold(preds)\n    y = onecold(y)\n    for (yi, ŷi) in zip(y, ŷ)\n        confusion_matrix[yi, ŷi] += 1\n    end\nend\n\nconfusion_matrix\n```\n\n----------------------------------------\n\nTITLE: Instantiating Flux Dense Layer for Linear Regression in Julia\nDESCRIPTION: Defines a Flux Dense layer with one input and one output neuron using Dense(1 => 1). This abstracts the linear model (with weights and bias) in a single object, automatically initializing parameters and enabling compatibility with the Flux ecosystem. Requires Flux.jl to be imported; does not need manual parameter specification.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_9\n\nLANGUAGE: julia\nCODE:\n```\njulia> flux_model = Dense(1 => 1)\nDense(1 => 1)       # 2 parameters\n```\n\n----------------------------------------\n\nTITLE: Updating Custom Model Parameters using Gradient Descent in Julia\nDESCRIPTION: Shows a single step of the gradient descent algorithm to update the custom model's weights (`W`) and biases (`b`). It subtracts the gradients (`dLdW`, `dLdb`), scaled by a learning rate of 0.1, from the current parameters using element-wise subtraction and assignment (`.=` ).\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_22\n\nLANGUAGE: julia\nCODE:\n```\n```jldoctest logistic_regression; filter = r\"[+-]?([0-9]*[.])?[0-9]+(f[+-]*[0-9])?\"\njulia> W .= W .- 0.1 .* dLdW;\n\njulia> b .= b .- 0.1 .* dLdb;\n```\n```\n\n----------------------------------------\n\nTITLE: Using Flux.@layer Macro for Custom Trainable Fields (Julia)\nDESCRIPTION: This example shows macro-style specification of trainable fields for a custom Affine layer, simplifying the designation of trainable parameters. It leverages the Flux.@layer macro for concise parameter configuration. All dependencies must already be in place, specifically Flux and a properly defined Affine struct.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/custom_layers.md#_snippet_4\n\nLANGUAGE: julia\nCODE:\n```\nFlux.@layer Affine trainable=(W,)\n```\n\n----------------------------------------\n\nTITLE: Neural Network Layers with Explicit Arguments and Closures in Julia\nDESCRIPTION: This code defines two alternative approaches for neural network layers: one that takes explicit parameters (layer2), and one that encapsulates parameters in a closure (layer3). The closure version is functionally equivalent to a Flux model and demonstrates local parameter state. Required dependencies: Julia Base. Inputs: vector x, weight matrices W2 or W3, biases b2 or b3; outputs: transformed vector. Limitation: closure approach requires careful construction to avoid parameter loss.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/basics.md#_snippet_15\n\nLANGUAGE: julia\nCODE:\n```\nlayer2(x, W2, b2) = sigmoid.(W2*x .+ b2)  # explicit parameter arguments\n\nlayer3 = let\n    W3 = randn(2, 3)\n    b3 = zeros(2)\n    x -> sigmoid.(W3*x .+ b3)  # closure over local variables\nend\n\nlayer3([1.0, 2.0, 3.0]) isa Vector  # check that it runs\n\n# output\n\ntrue\n```\n\n----------------------------------------\n\nTITLE: Defining the Training Function for the Generator in Julia with Flux\nDESCRIPTION: Implements the generator training step, sampling random noise as input and seeking to maximize the discriminator's probability of identifying generated images as real. The discriminator is temporarily set to test mode (to freeze dropout), predictions are assessed, and binary cross-entropy loss against real labels is computed. Gradients are determined only with respect to generator parameters, and updates are performed with ADAM. Inputs: discriminator, generator. Output: scalar loss. Assumes opt_gen exists globally.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-10-14-vanilla-gan.md#_snippet_7\n\nLANGUAGE: julia\nCODE:\n```\nfunction train_gen!(discriminator, generator)\n    # Sample noise\n    noise = randn(latent_dim, batch_size) |> gpu;\n\n    # Define parameters and get the pullback\n    ps = Flux.params(generator)\n    # Set discriminator into test mode to disable dropout layers\n    testmode!(discriminator)\n    # Evaluate the loss function while calculating the pullback. We get the loss for free\n    loss, back = Zygote.pullback(ps) do\n        preds = discriminator(generator(noise));\n        loss = Flux.Losses.binarycrossentropy(preds, 1.) \n    end\n    # Evaluate the pullback with a seed-gradient of 1.0 to get the gradients for\n    # the parameters of the generator\n    grads = back(1.0f0)\n    Flux.update!(opt_gen, Flux.params(generator), grads)\n    # Set discriminator back into automatic mode\n    trainmode!(discriminator, mode=:auto)\n    return loss\nend\n```\n\n----------------------------------------\n\nTITLE: Computing Gradients with Respect to Multiple Arguments in Julia\nDESCRIPTION: This snippet demonstrates obtaining gradients of a function with respect to several arguments using Flux.gradient. The function (x,y,z) -> (x*y) + z returns a scalar, and gradient returns partial derivatives for each argument. Required dependencies: Flux.jl. Inputs: numeric arguments x, y, z; outputs: tuple of partial derivatives. Useful for multi-input models or loss functions.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/basics.md#_snippet_8\n\nLANGUAGE: julia\nCODE:\n```\njulia> gradient((x,y,z) -> (x*y)+z, 30, 40, 50)\n(40.0, 30.0, 1.0)\n```\n\n----------------------------------------\n\nTITLE: Extracting Model Parameters with Flux.trainables - julia\nDESCRIPTION: This minimal snippet shows how to extract all parameter arrays from a layer or model in Flux using the trainables function for use in further computation, such as gradient updates or saving state. Requires Flux.jl; input is a model or layer, and output is a list of trainable parameters.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-blitz.md#_snippet_9\n\nLANGUAGE: julia\nCODE:\n```\nFlux.trainables(m)\n```\n\n----------------------------------------\n\nTITLE: Updating Flux Model Parameters with Enzyme Gradients in Julia\nDESCRIPTION: Illustrates two equivalent methods for updating model parameters using `Flux.update!` after calculating gradients via Enzyme. The first method uses the gradient tuple explicitly returned by `Flux.gradient`. The second, Enzyme-specific method, directly uses the `Enzyme.Duplicated` model, which internally stores the computed gradient. Both require an optimizer state initialized with `Flux.setup`.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/training/enzyme.md#_snippet_1\n\nLANGUAGE: julia-repl\nCODE:\n```\njulia> opt_state = Flux.setup(Adam(), model)\n\njulia> ans == Flux.setup(Adam(), dup_model)\n\njulia> Flux.update!(opt_state, model, grads_f[1])  # exactly as for Zygote gradients\n\njulia> Flux.update!(opt_state, dup_model)  # equivlent new path, Enzyme only\n```\n\n----------------------------------------\n\nTITLE: Setting up a Training Loop for a RecurrentCell Model Using Flux and Optimisers (Julia)\nDESCRIPTION: Demonstrates setting up a loss function, generating batch-sequenced dummy data, initializing a model, optimizer, and performing a single parameter update step. Uses Optimisers.jl's AdamW, the Flux.mse loss, and Flux's optimizer API. Inputs and expected outputs are stacked tensors. Assumes model structure from a previous RecurrentCellModel definition with untrained initial state.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/recurrence.md#_snippet_3\n\nLANGUAGE: Julia\nCODE:\n```\nusing Optimisers: AdamW\n\nfunction loss(model, x, y)\n    ŷ = model(x)\n    y = stack(y, dims=2)\n    return Flux.mse(ŷ, y)\nend\n\n# create dummy data\nseq_len, batch_size, input_size = 3, 4, 2\nx = [rand(Float32, input_size, batch_size) for _ = 1:seq_len]\ny = [rand(Float32, 1, batch_size) for _ = 1:seq_len]\n\n# initialize the model and optimizer\nmodel = RecurrentCellModel(input_size, 5)\nopt_state = Flux.setup(AdamW(1e-3), model)\n\n# compute the gradient and update the model\ng = gradient(m -> loss(m, x, y), model)[1]\nFlux.update!(opt_state, model, g)\n```\n\n----------------------------------------\n\nTITLE: Finding Maximum Index per Column in Julia using argmax\nDESCRIPTION: Demonstrates how to find the index of the maximum element in each column of a one-hot encoded matrix (`custom_y_onehot`) using Julia's `argmax` function. It first shows the Cartesian indices returned by `argmax` and then extracts the row index (representing the class) for each column into a new matrix `max_idx`.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_16\n\nLANGUAGE: julia\nCODE:\n```\n```jldoctest logistic_regression; filter = r\"[+-]?([0-9]*[.])?[0-9]+(f[+-]*[0-9])?\"\njulia> argmax(custom_y_onehot, dims=1)  # calculate the cartesian index of max element column-wise\n1×150 Matrix{CartesianIndex{2}}:\n CartesianIndex(1, 1)  CartesianIndex(1, 2)  …  CartesianIndex(3, 150)\n\njulia> max_idx = [x[1] for x in argmax(custom_y_onehot; dims=1)]\n1×150 Matrix{Int64}:\n 1  1  1  1  1  1  1  1  1  1  1  1  1  …  3  3  3  3  3  3  3  3  3  3  3  3\n```\n```\n\n----------------------------------------\n\nTITLE: Using Patience with Arbitrary Predicates and Arguments - Flux.jl - Julia\nDESCRIPTION: This example details how to apply Flux.jl's patience utility with custom predicates that accept positional and keyword arguments. It demonstrates defining triggers that evaluate conditions based on runtime parameters and shows two usage scenarios: one where the predicate never triggers, and another where it does after a given epoch. Prerequisite: Flux.jl. Key parameters include custom predicates, patience count, and arguments passed at call time. Output is a boolean trigger controlling flow in training loops.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/training/callbacks.md#_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\ntrigger = patience((a; b) -> a > b, 3)\\n\\n# this will iterate until the 10th epoch\\nfor epoch in 1:10\\n  trigger(1; b = 2) && break\\nend\\n\\n# this will stop at the 3rd epoch\\nfor epoch in 1:10\\n  trigger(3; b = 2) && break\\nend\n```\n\n----------------------------------------\n\nTITLE: Defining and Differentiating Scalar Functions with Flux/Zygote - julia\nDESCRIPTION: This pair of snippets define a simple quadratic function and demonstrate evaluating it at a scalar input, showcasing Julia's concise syntax for mathematical functions. Then, with Flux's gradient function, the snippet computes the gradient with respect to the input, extracting the first result from the returned tuple. Requires Flux.jl for gradient; inputs are scalars, and outputs are float values representing function or derivative evaluations.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-blitz.md#_snippet_4\n\nLANGUAGE: julia\nCODE:\n```\nf(x) = 3x^2 + 2x + 1\n\nf(5)\n```\n\nLANGUAGE: julia\nCODE:\n```\nusing Flux: gradient\n\ndf(x) = gradient(f, x)[1]\n\ndf(5)\n```\n\n----------------------------------------\n\nTITLE: Defining Mean Squared Error Loss Using Flux Utilities in Julia\nDESCRIPTION: Creates a loss function flux_loss(flux_model, features, labels) that leverages the Flux.mse utility for mean squared error computation between model predictions and labels. Eliminates the need for manual broadcasting. Inputs: model instance, features matrix, label matrix; outputs: scalar MSE loss.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_12\n\nLANGUAGE: julia\nCODE:\n```\njulia> function flux_loss(flux_model, features, labels)\n           ŷ = flux_model(features)\n           Flux.mse(ŷ, labels)\n       end;\n\njulia> flux_loss(flux_model, x, y)\n22.74856f0\n```\n\n----------------------------------------\n\nTITLE: Defining Leaky ReLU with Implicit Type Promotion in Julia\nDESCRIPTION: Shows a `leaky_tanh` function where interaction with a `Float64` numeric literal (`0.01`) causes the result to be promoted to `Float64`, even if the input `x` is `Float32`. This unintended type promotion can lead to performance issues similar to explicit casting.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/performance.md#_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\n```julia\nleaky_tanh(x) = 0.01*x + tanh(x)\n```\n```\n\n----------------------------------------\n\nTITLE: Automatic Differentiation with Enzyme in Julia for Struct Models\nDESCRIPTION: This code snippet illustrates running Flux.gradient with Enzyme AD backend by wrapping inputs with Const and Duplicated. It demonstrates that arguments marked constant get nothing as gradient, and other gradients follow the struct fields. Required dependencies: Flux.jl, Enzyme.jl, model definitions. Inputs: function, Const and Duplicated wrapped arguments; outputs: tuple, with nothing for constants, and NamedTuple for parameter gradients. Limitation: Enzyme.jl must be installed and loaded.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/basics.md#_snippet_12\n\nLANGUAGE: julia\nCODE:\n```\njulia> using Enzyme: Const, Duplicated\n\njulia> grad3e = Flux.gradient((x,p) -> p(x), Const(5.0), Duplicated(poly3s))\n(nothing, (θ3 = [1.0, 5.0, 25.0],))\n```\n\n----------------------------------------\n\nTITLE: Composing Parameterized Functions Using Structs in Julia\nDESCRIPTION: This snippet demonstrates the function composition of parameterized polynomials using struct-based encapsulation. Julia's ∘ (compose) operator creates a ComposedFunction instance with outer and inner fields, enabling model stacking and modularity. Required dependencies: Julia Base; Poly3 must be defined. Inputs: instances of Poly3; outputs: composed function object and results of evaluation. Shows how compositional models are constructed and inspected.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/basics.md#_snippet_5\n\nLANGUAGE: julia\nCODE:\n```\npoly4 = Poly3([1, 0.5, 0]) ∘ Poly3([10, 1, 0.1])\n\npoly4 isa ComposedFunction  # ∘ creates another struct...\npoly4.outer.θ3 == θ         # which has fields :inner & :outer\n\npoly4(5) == 9.75  # true\n\n# output\n\ntrue\n```\n\n----------------------------------------\n\nTITLE: Decoding and Argmax with onecold in OneHotArrays.jl (Julia)\nDESCRIPTION: This snippet illustrates the use of the onecold function, which is the inverse of onehot in the OneHotArrays.jl package. It can decode outputs from a OneHotVector, a boolean array, or an array of real-valued weights. For a boolean one-hot input, it returns the label corresponding to the true position; for a numeric array, it returns the label with the maximum value (argmax behavior). It requires the labels array as a reference. Inputs: (one-hot or numeric array, categories array). Outputs: The decoded label. Limitation: The input array must align in length with the categories array.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/data/onehot.md#_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\njulia> onecold(ans, [:a, :b, :c])\n:c\n\njulia> onecold([true, false, false], [:a, :b, :c])\n:a\n\njulia> onecold([0.3, 0.2, 0.5], [:a, :b, :c])\n:c\n```\n\n----------------------------------------\n\nTITLE: Custom Loss Function for Multiple Outputs in Flux (Julia)\nDESCRIPTION: This snippet defines a loss function for models with tuple outputs, aggregating per-output mean squared error with their RMS. It is suitable for multi-output models where ground-truth and predictions are stored as tuples. Depends on Flux and Statistics. Expects a tuple ys for ground-truth, model output, and returns a scalar loss. Must be used in alignment with Split models.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/custom_layers.md#_snippet_11\n\nLANGUAGE: julia\nCODE:\n```\nusing Statistics\n\n# assuming model returns the output of a Split\n# x is a single input\n# ys is a tuple of outputs\nfunction loss(x, ys, model)\n  # rms over all the mse\n  ŷs = model(x)\n  return sqrt(mean(Flux.mse(y, ŷ) for (y, ŷ) in zip(ys, ŷs)))\nend\n```\n\n----------------------------------------\n\nTITLE: Customizing Early Stopping for Increasing Metrics - Flux.jl - Julia\nDESCRIPTION: This snippet shows how to customize Flux.jl's early stopping utility for metrics that increase, such as accuracy. It builds a stateful pseudo-accuracy function via a closure and demonstrates configuring early_stopping with a custom distance function to monitor increases, triggering early stopping after the metric fails to improve for several steps. Required dependency: Flux.jl. Main parameters are the metric function and patience value, with outputs controlling loop progression.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/training/callbacks.md#_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\n# create a pseudo-accuracy that increases by 0.01 each time from 0 to 1\\n# we call this like acc()\\nacc = let v = 0\\n  () -> v = max(1, v + 0.01)\\nend\\n\\n# create an early stopping trigger for accuracy\\nes = early_stopping(acc, 3; delta = (best_score, score) -> score - best_score)\\n\\n# this will iterate until the 10th epoch\\nfor epoch in 1:10\\n  es() && break\\nend\n```\n\n----------------------------------------\n\nTITLE: Loading Flux Models Saved Directly as Structs using BSON in Julia\nDESCRIPTION: Shows how to load a Flux model that was previously saved as a struct using `BSON.@save`. The `BSON.@load` macro deserializes the struct directly into the `model` variable in the current session.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/saving.md#_snippet_7\n\nLANGUAGE: jldoctest saving\nCODE:\n```\njulia> using Flux, BSON\n\njulia> BSON.@load \"mymodel.bson\" model\n\njulia> model\nChain(\n  Dense(10 => 5, relu),                 # 55 parameters\n  Dense(5 => 2),                        # 12 parameters\n)                   # Total: 4 arrays, 67 parameters, 476 bytes.\n\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Logit Cross-Entropy Loss in Julia\nDESCRIPTION: Defines a custom logit cross-entropy loss function named `custom_logitcrossentropy` in Julia. This function calculates the mean of the negative sum of element-wise products between the true labels (`y`) and the log-softmax of the predictions (`ŷ`), applied across specified dimensions.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_13\n\nLANGUAGE: julia\nCODE:\n```\n```jldoctest logistic_regression\njulia> custom_logitcrossentropy(ŷ, y) = mean(.-sum(y .* logsoftmax(ŷ; dims = 1); dims = 1));\n```\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Linear Regression Model Function in Julia\nDESCRIPTION: Implements a fully customizable linear regression model as custom_model(W, b, x) = @. W*x + b, where W is the weight matrix, b the bias, and x the feature matrix. The @. macro enables broadcasting for element-wise computation, accommodating matrix or scalar weights and biases. Used as the core prediction utility in later steps.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_5\n\nLANGUAGE: julia\nCODE:\n```\njulia> custom_model(W, b, x) = @. W*x + b\n```\n\n----------------------------------------\n\nTITLE: Evaluating Training Progress via Post-Train Loss in Julia\nDESCRIPTION: Calculates updated training loss after a single optimizer step, enabling assessment of model improvement. Inputs are the currently trained model and training data; output is a floating point loss value, which should decrease relative to pre-training loss.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/overview.md#_snippet_9\n\nLANGUAGE: julia\nCODE:\n```\njulia> loss(predict, x_train, y_train)\n116.38745f0\n```\n\n----------------------------------------\n\nTITLE: Advanced Initializer Signatures for Layers in Flux.jl (Julia)\nDESCRIPTION: Shows usage of initializers that accept additional keyword arguments such as gain or a specific random number generator. Both glorot_uniform with a gain parameter and randn32 with a custom MersenneTwister are used to construct Dense layers. The setup block imports Flux and Random, demonstrating integration of RNG control when initializing layers. Inputs are the annotations to the init parameter; output is Dense layer construction.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/utilities.md#_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\n```jldoctest; setup = :(using Flux, Random)\njulia> Dense(4 => 5, tanh; init=Flux.glorot_uniform(gain=2))\nDense(4 => 5, tanh)  # 25 parameters\n\njulia> Dense(4 => 5, tanh; init=Flux.randn32(MersenneTwister(1)))\nDense(4 => 5, tanh)  # 25 parameters\n```\n```\n\n----------------------------------------\n\nTITLE: Destructuring and Rebuilding Flux Models - Julia\nDESCRIPTION: This snippet demonstrates creating a nested Chain model in Flux, flattening its parameters with destructure to obtain a flat parameter vector and a callable to rebuild the model, and reconstructing the model with new parameters. It requires Flux with Chain and Dense layers, and uses destructure to enable parameter manipulation for optimizers that expect flat arrays. The key inputs are the model and the flat parameters, and outputs include the flat parameter array and function to reconstruct the model; limitations include requiring compatible parameter sizes during rebuiling.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/destructure.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\njulia> model = Chain(Dense(2=>1, tanh), Dense(1=>1))\nChain(\n  Dense(2 => 1, tanh),                  # 3 parameters\n  Dense(1 => 1),                        # 2 parameters\n)                   # Total: 4 arrays, 5 parameters, 276 bytes.\n\njulia> flat, rebuild = Flux.destructure(model)\n(Float32[0.863101, 1.2454957, 0.0, -1.6345707, 0.0], Restructure(Chain, ..., 5))\n\njulia> rebuild(zeros(5))  # same structure, new parameters\nChain(\n  Dense(2 => 1, tanh),                  # 3 parameters  (all zero)\n  Dense(1 => 1),                        # 2 parameters  (all zero)\n)                   # Total: 4 arrays, 5 parameters, 276 bytes.\n```\n\n----------------------------------------\n\nTITLE: Visualizing Curve Fit Results with Plots.jl in Julia\nDESCRIPTION: This snippet visualizes both the ground truth function and neural network predictions after training. It plots the function f(x) = 2x - x^3 and overlays model3's output at sampled input points. Dependencies: Plots.jl, trained Flux model. Inputs: None (plots curves from -2 to 2). Outputs: Rendered plot for visual inspection. Model input interface must match what trained model expects.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/basics.md#_snippet_23\n\nLANGUAGE: julia\nCODE:\n```\nusing Plots\nplot(x -> 2x-x^3, -2, 2, label=\"truth\")\nscatter!(x -> model3([x]), -2:0.1f0:2, label=\"fitted\")\n```\n\n----------------------------------------\n\nTITLE: Defining Custom onecold Function in Julia\nDESCRIPTION: Defines a custom `custom_onecold` function in Julia to convert a one-hot encoded label matrix back to class names. It finds the index of the maximum value in each column (using `argmax`) and then maps these indices to class names stored in the `classes` variable, returning a vector of class names.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_17\n\nLANGUAGE: julia\nCODE:\n```\n```jldoctest logistic_regression\njulia> function custom_onecold(labels_onehot)\n           max_idx = [x[1] for x in argmax(labels_onehot; dims=1)]\n           return vec(classes[max_idx])\n       end;\n\njulia> custom_onecold(custom_y_onehot)\n150-element Vector{String}:\n \"Iris-setosa\"\n \"Iris-setosa\"\n \"Iris-setosa\"\n \"Iris-setosa\"\n \"Iris-setosa\"\n \"Iris-setosa\"\n \"Iris-setosa\"\n \"Iris-setosa\"\n \"Iris-setosa\"\n \"Iris-setosa\"\n ⋮\n \"Iris-virginica\"\n \"Iris-virginica\"\n \"Iris-virginica\"\n \"Iris-virginica\"\n \"Iris-virginica\"\n \"Iris-virginica\"\n \"Iris-virginica\"\n \"Iris-virginica\"\n \"Iris-virginica\"\n```\n```\n\n----------------------------------------\n\nTITLE: Initializing Weights for DCGAN Convolutions - Julia\nDESCRIPTION: Defines a helper function, `dcgan_init`, for initializing neural network weights from a Gaussian distribution with mean 0 and standard deviation 0.02, as recommended in the DCGAN paper. Intended for use as the `init` argument in Flux\\'s layer constructors to enhance model convergence. Returns a randomized Float32 array of the provided shape.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-10-08-dcgan-mnist.md#_snippet_4\n\nLANGUAGE: julia\nCODE:\n```\n# Function for initializing the model weights with values \n# sampled from a Gaussian distribution with \\u03bc=0 and \\u03c3=0.02\ndcgan_init(shape...) = randn(Float32, shape) * 0.02f0\n```\n\n----------------------------------------\n\nTITLE: Defining Training Arguments Structure in Julia\nDESCRIPTION: Defines a mutable struct `TrainArgs` using the `Base.@kwdef` macro to easily set default values for training hyperparameters. This includes the learning rate (`lr`), the number of training epochs (`epochs`), the batch size for mini-batch processing (`batch_size`), and the directory path (`savepath`) for saving the trained model.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-02-07-convnet.md#_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nBase.@kwdef mutable struct TrainArgs\n   lr::Float64 = 3e-3\n   epochs::Int = 20\n   batch_size = 128\n   savepath::String = \"./\"\nend\n```\n\n----------------------------------------\n\nTITLE: Cosine Annealing Learning Rate Schedule with ParameterSchedulers.jl in Flux.jl (Julia)\nDESCRIPTION: This snippet shows how to use the ParameterSchedulers.jl package to implement a cosine annealing learning rate policy in a Flux.jl training loop. The schedule varies the learning rate between 1e-4 and 1e-2 over 10 epochs, updating the optimizer state at each iteration. Dependencies include the ParameterSchedulers.jl package, a defined `model`, and `Flux`. The example uses Momentum as the optimizer and supports both iteration and direct indexing of the schedule function.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/training/training.md#_snippet_17\n\nLANGUAGE: julia\nCODE:\n```\nusing ParameterSchedulers\n\nopt_state = Flux.setup(Momentum(), model)\nschedule = Cos(\\u03bb0 = 1e-4, \\u03bb1 = 1e-2, period = 10)\nfor (eta, epoch) in zip(schedule, 1:100)\n  Flux.adjust!(opt_state, eta)\n  # your training code here\nend\n```\n\n----------------------------------------\n\nTITLE: Generating Initial Predictions from an Untrained Dense Layer in Julia\nDESCRIPTION: Applies the untrained `predict` model to the training features (`x_train`), outputting a matrix of raw predictions. This step demonstrates how model parameters before optimization affect prediction quality. Input is the training data matrix; output is predicted values, generally inaccurate prior to training.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/overview.md#_snippet_4\n\nLANGUAGE: julia\nCODE:\n```\njulia> predict(x_train)\n1×6 Matrix{Float32}:\n 0.0  0.906654  1.81331  2.71996  3.62662  4.53327\n```\n\n----------------------------------------\n\nTITLE: Computing Gradients with Manual L2 Regularisation in Flux.jl (Julia)\nDESCRIPTION: This snippet demonstrates how to calculate gradients for a dense model while manually adding an L2 penalty to the loss function in Flux.jl. It requires a defined model `densemodel`, a custom loss function `my_loss`, and data `input` and `label`. The penalty is computed by summing the squared weights and biases, scaled by a constant (here 0.42). The function returns the gradient object with respect to the model parameters. Input shapes and proper model definition are prerequisites, and the approach scales poorly for models with many parameters.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/training/training.md#_snippet_11\n\nLANGUAGE: julia\nCODE:\n```\ngrads = Flux.gradient(densemodel) do m\n  result = m(input)\n  penalty = sum(abs2, m.weight)/2 + sum(abs2, m.bias)/2\n  my_loss(result, label) + 0.42f0 * penalty\nend\n```\n\n----------------------------------------\n\nTITLE: Accessing Weights and Biases of a Flux Dense Layer in Julia\nDESCRIPTION: Accesses the trained or initialized parameters (weight matrix and bias vector) of a Flux Dense layer, which are stored as arrays. Can be used to synchronize parameters with custom models and for inspection. Requires a previously created Dense instance from Flux.jl.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_10\n\nLANGUAGE: julia\nCODE:\n```\njulia> flux_model.weight, flux_model.bias\n(Float32[-1.2678515;;], Float32[0.0])\n```\n\n----------------------------------------\n\nTITLE: Testing Generator Output Dimensions - Flux.jl - Julia\nDESCRIPTION: Demonstrates a sanity test for the Generator: constructs a Generator with 100-dimensional noise input, generates a batch of 3 fake images, and uses Julia's @assert macro to verify the output shape is (28, 28, 1, 3), matching MNIST dimensions. Assumes Generator function is defined and Flux is installed; fails if output dimensions are unexpected.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-10-08-dcgan-mnist.md#_snippet_6\n\nLANGUAGE: julia\nCODE:\n```\n# Create a dummy generator of latent dim 100\ngenerator = Generator(100)\nnoise = randn(Float32, 100, 3) # The last axis is the batch size\n\n# Feed the random noise to the generator\ngen_image = generator(noise)\n@assert size(gen_image) == (28, 28, 1, 3)\n```\n\n----------------------------------------\n\nTITLE: Verifying the Output Shape of the Custom Logistic Regression Model in Julia\nDESCRIPTION: Calls the `custom_model` with the manually initialized weights `W`, bias `b`, and the entire feature set `x`. It then uses the `size` function to check and confirm that the output dimensions are (3, 150), corresponding to predictions for 3 classes across all 150 data points.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_9\n\nLANGUAGE: julia\nCODE:\n```\njulia> custom_model(W, b, x) |> size\n(3, 150)\n```\n\n----------------------------------------\n\nTITLE: Testing Discriminator Output Dimensions - Flux.jl - Julia\nDESCRIPTION: Shows a test to validate the Discriminator model by running it on generated images from the Generator and checking the output dimensions. The result should be (1, batch_size), here (1, 3), for correct binary logit output. Designed for quick functional verification during model assembly.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-10-08-dcgan-mnist.md#_snippet_8\n\nLANGUAGE: julia\nCODE:\n```\n# Dummy Discriminator\ndiscriminator = Discriminator()\n# We pass the generated image to the discriminator\nlogits = discriminator(gen_image)\n@assert size(logits) == (1, 3)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Softmax Activation Function in Julia\nDESCRIPTION: Defines a Julia function `custom_softmax` that implements the softmax activation. It takes a matrix `x` (typically the output of the linear layer) and applies the softmax formula element-wise: exponentiates each element and then normalizes by the sum of exponentiated values across each column (`dims=1`), yielding probability distributions.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_7\n\nLANGUAGE: julia\nCODE:\n```\njulia> custom_softmax(x) = exp.(x) ./ sum(exp.(x), dims=1)\ncustom_softmax (generic function with 1 method)\n```\n\n----------------------------------------\n\nTITLE: Extracting Trainable Model Parameters with trainables - Julia\nDESCRIPTION: Shows how to extract all trainable parameters from a Flux model as a list of arrays using Flux.trainables. This is useful for manual parameter inspection or mutation, and any modifications to the returned arrays directly affect the model. Prerequisites: a defined Flux model. Returns a Vector of AbstractArray. Limitation: Only parameters marked as trainable are included.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/destructure.md#_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\njulia> Flux.trainables(model)\n5-element Vector{AbstractArray}:\n  [0.863101 1.2454957]\n  [0.0]\n  [1.290355429422727;;]\n  [0.0]\n```\n\n----------------------------------------\n\nTITLE: Saving Flux Model State using Flux.state and JLD2 in Julia\nDESCRIPTION: Defines a custom model struct `MyModel`, instantiates it, extracts its trainable parameters and state using `Flux.state`, and saves this state to a file named `mymodel.jld2` using the JLD2.jl library. This is the recommended method for robust model persistence.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/saving.md#_snippet_0\n\nLANGUAGE: jldoctest saving\nCODE:\n```\njulia> using Flux\n\njulia> struct MyModel\n           net\n       end\n\njulia> Flux.@layer MyModel\n\njulia> MyModel() = MyModel(Chain(Dense(10 => 5, relu), Dense(5 => 2)));\n\njulia> model = MyModel()\nMyModel(\n  Chain(\n    Dense(10 => 5, relu),               # 55 parameters\n    Dense(5 => 2),                      # 12 parameters\n  ),\n)                   # Total: 4 arrays, 67 parameters, 484 bytes.\n\njulia> model_state = Flux.state(model);\n\njulia> using JLD2\n\njulia> jldsave(\"mymodel.jld2\"; model_state)\n\n```\n\n----------------------------------------\n\nTITLE: Stacking Recurrent Layers Using Chain in Flux.jl (Julia)\nDESCRIPTION: Illustrates the use of Flux's Chain container to create composite models by stacking multiple recurrent layers and regularization. This example chains two LSTM layers with a Dropout layer between them. Inputs are (input size, sequence length) tensors, outputs have shape determined by the last LSTM layer. Requires Flux's LSTM and Dropout support.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/recurrence.md#_snippet_6\n\nLANGUAGE: Julia\nCODE:\n```\nstacked_rnn = Chain(LSTM(3 => 5), Dropout(0.5), LSTM(5 => 5))\nx = rand(Float32, 3, 4)\ny = stacked_rnn(x)\n```\n\n----------------------------------------\n\nTITLE: Checking GPU Functionality with CUDA in Julia\nDESCRIPTION: This snippet checks if the CUDA library is functional on the user\\'s system for GPU computation in Julia. It requires the CUDA.jl package to be installed and imports it using \\\"using CUDA\\\". The function \\\"CUDA.functional()\\\" returns true if CUDA and a supported GPU are present, otherwise returns false. The expected input is just the function call, and it outputs a boolean indicating GPU support. This check must be performed in a Julia REPL or script.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/gpu.md#_snippet_13\n\nLANGUAGE: julia-repl\nCODE:\n```\njulia> using CUDA\n\njulia> CUDA.functional()\ntrue\n```\n\n----------------------------------------\n\nTITLE: Checking GPU Functionality with AMDGPU in Julia\nDESCRIPTION: This snippet demonstrates how to check for AMD GPU support via the AMDGPU.jl library in Julia. It requires the user to have AMDGPU.jl installed and an AMD GPU present on their machine. After importing AMDGPU with \\\"using AMDGPU\\\", the code checks if the GPU and associated library (optionally :MIOpen) are functional using \\\"AMDGPU.functional()\\\" and \\\"AMDGPU.functional(:MIOpen)\\\". Outputs are booleans indicating compatibility; all commands are intended for use in a Julia REPL or script.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/gpu.md#_snippet_14\n\nLANGUAGE: julia-repl\nCODE:\n```\njulia> using AMDGPU\n\njulia> AMDGPU.functional()\ntrue\n\njulia> AMDGPU.functional(:MIOpen)\ntrue\n```\n\n----------------------------------------\n\nTITLE: Running Multiple Training Epochs Manually in Julia\nDESCRIPTION: Implements a simple training loop that calls the `train_custom_model!` function repeatedly for a fixed number of epochs (40). After the loop completes, it displays the final values of the weights `W`, bias `b`, and the resulting loss calculated by `custom_loss`.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_20\n\nLANGUAGE: julia\nCODE:\n```\njulia> for i = 1:40\n          train_custom_model!(custom_loss, W, b, x, y)\n       end\n```\n\nLANGUAGE: julia\nCODE:\n```\n\njulia> W, b, custom_loss(W, b, x, y)\n(Float32[4.2422233], Float32[2.2460847], 7.6680417f0)\n```\n\n----------------------------------------\n\nTITLE: Initializing Model Weights and Biases for Linear Regression in Julia\nDESCRIPTION: Initializes the model parameters: W as a 1x1 matrix with a random Float32 value and b as a Float32 vector with zero. These initializations use the rand function and literal allocation, setting the stage for learning during training iterations. No dependencies beyond Julia's Base modules.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_6\n\nLANGUAGE: julia\nCODE:\n```\njulia> W = rand(Float32, 1, 1)\n1\\u00d71 Matrix{Float32}:\n 0.99285793\n\njulia> b = [0.0f0]\n1-element Vector{Float32}:\n 0.0\n```\n\n----------------------------------------\n\nTITLE: Initializing an Optimizer State with Adam in Flux.jl (Julia)\nDESCRIPTION: This snippet initializes an optimizer state for the Adam optimizer in Flux.jl using the current model. `Flux.setup` prepares the optimizer to interface with the model's trainable parameters for future gradient-based updating. Dependencies include a defined model and the `Flux` library. The resulting `opt_state` is used in training loops and for optimizer adjustments such as learning rate or state updates.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/training/training.md#_snippet_13\n\nLANGUAGE: julia\nCODE:\n```\nopt_state = Flux.setup(Adam(0.1), model)\n```\n\n----------------------------------------\n\nTITLE: Importing ML and Utility Packages with Julia using Flux\nDESCRIPTION: Imports essential Julia packages needed for GAN development in Flux, including MLDatasets for easy MNIST access, data loaders, CUDA for GPU acceleration, Zygote for automatic differentiation, and UnicodePlots for in-terminal plotting. All packages must be installed prior to usage; GPU support is optional but enabled if CUDA is available. This step supplies fundamental dependencies for the remainder of the tutorial.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-10-14-vanilla-gan.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nusing MLDatasets: MNIST\nusing Flux.Data: DataLoader\nusing Flux\nusing CUDA\nusing Zygote\nusing UnicodePlots\n```\n\n----------------------------------------\n\nTITLE: Importing Flux and Defining Target Function in Julia\nDESCRIPTION: This snippet sets up the Julia environment for machine learning by importing the Flux library and defining an `actual` function that models the relationship to be learned (linear: 4x + 2). The `actual` function is used to generate both training and testing labels in subsequent steps. Required dependency is Flux, and output is a callable function for target value computation.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/overview.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\njulia> using Flux\n\njulia> actual(x) = 4x + 2\nactual (generic function with 1 method)\n```\n\n----------------------------------------\n\nTITLE: Moving Arrays and Flux Models from GPU Back to CPU with Flux.jl in Julia\nDESCRIPTION: Details how to bring GPU arrays and models (e.g., CuArray-backed) back to CPU memory using cpu(). Essential for model serialization and state extraction. Inputs can include arrays or Flux models; outputs are expected to be CPU-resident equivalents, preserving value types as Float32 or model structures.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/gpu.md#_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\ncpu(cW) isa Array{Float32, 2}\n\nmodel2 = cpu(c_model)  # copy model back to CPU\nmodel2(x)\n\n```\n\n----------------------------------------\n\nTITLE: Loading Required Libraries for DCGAN - Flux.jl - Julia\nDESCRIPTION: Imports essential Julia libraries and functions for data handling, neural network modeling, loss calculation, and GPU computation. This includes partitioning utilities, statistical methods, image handling, Flux neural network components and optimizers, loss functions, dataset loading, and CUDA support. Assumes all packages have been installed prior to use.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-10-08-dcgan-mnist.md#_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nusing Base.Iterators: partition\nusing Printf\nusing Statistics\nusing Random\nusing Images\nusing Flux: params, DataLoader\nusing Flux.Optimise: update!\nusing Flux.Losses: logitbinarycrossentropy\nusing MLDatasets: MNIST\nusing CUDA\n```\n\n----------------------------------------\n\nTITLE: Batching Data Vectors into Matrices using `reduce` in Julia\nDESCRIPTION: Demonstrates the recommended way to prepare data for batch processing. It uses `reduce(hcat, xs)` and `reduce(hcat, ys)` to concatenate vectors of feature vectors (`xs`) and target vectors (`ys`) into single matrices (`x_batch`, `y_batch`). Using `reduce` is preferred over splatting (`hcat(xs...)`) for better performance.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/performance.md#_snippet_4\n\nLANGUAGE: julia\nCODE:\n```\n```julia\nx_batch = reduce(hcat, xs)\ny_batch = reduce(hcat, ys)\n...\n```\n```\n\n----------------------------------------\n\nTITLE: Calculating Gradients Explicitly with `Flux.gradient` in Julia\nDESCRIPTION: Illustrates an alternative syntax for `Flux.gradient` without using a `do` block. An anonymous function `m -> loss(m(input), label)` is explicitly passed as the first argument to calculate the gradient of the loss with respect to the `model`'s parameters. The model execution `m(input)` and loss calculation must occur inside this function.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/training/training.md#_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\ngrads = Flux.gradient(m -> loss(m(input), label), model)\n```\n\n----------------------------------------\n\nTITLE: Saving Checkpoints with Additional Metadata (e.g., Loss) using JLD2 in Julia\nDESCRIPTION: Shows how to save additional information, such as the current test loss obtained from a hypothetical `testloss()` function, alongside the model state in a time-stamped checkpoint file. This facilitates analysis and potentially reverting to the best-performing model.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/saving.md#_snippet_4\n\nLANGUAGE: julia\nCODE:\n```\njldsave(\"model-$(now()).jld2\", model_state = Flux.state(m), loss = testloss())\n\n```\n\n----------------------------------------\n\nTITLE: Saving Flux Models Directly as Structs using BSON in Julia\nDESCRIPTION: Illustrates saving a Flux model (`Chain`) directly as a Julia struct using the `BSON.@save` macro. BSON.jl is convenient as it can handle serializing anonymous functions often used in Flux models. However, this method is less robust to version changes than saving state.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/saving.md#_snippet_6\n\nLANGUAGE: jldoctest saving\nCODE:\n```\njulia> using Flux\n\njulia> model = Chain(Dense(10 =>  5, NNlib.relu), Dense(5 =>  2));\n\njulia> using BSON: @save\n\njulia> @save \"mymodel.bson\" model\n\n```\n\n----------------------------------------\n\nTITLE: Saving Flux Model and Optimizer State Together using JLD2 in Julia\nDESCRIPTION: Demonstrates saving both the model's state (`Flux.state`) and the optimizer's state (`Flux.setup`) to the same checkpoint file using JLD2. This is crucial for exactly resuming training, preserving the learning rate schedule and momentum of the optimizer.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/saving.md#_snippet_5\n\nLANGUAGE: julia\nCODE:\n```\nmodel = MyModel()\nopt_state = Flux.setup(AdamW(), model)\n\n# ... train model ...\n\nmodel_state = Flux.state(model)\njldsave(\"checkpoint_epoch=42.jld2\"; model_state, opt_state)\n\n```\n\n----------------------------------------\n\nTITLE: Installing Julia Packages via Pkg REPL Command and Script\nDESCRIPTION: Demonstrates two ways to install Julia packages: using the REPL in package mode, and via script-friendly Pkg module commands. The example adds MLDatasets, a required dataset library. Users must have Julia and internet access; if running as a script, import Pkg first. No output is produced unless successful installation messages are shown. Limitation: Only works in environments where Julia and associated binaries are available.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-10-14-vanilla-gan.md#_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\n> import Pkg\n> Pkg.add(\"MLDatasets\")\n```\n\n----------------------------------------\n\nTITLE: Defining a Vanilla RNN Cell in Julia\nDESCRIPTION: Defines a basic vanilla RNN cell from primitives without external dependencies. The cell takes an input vector and a hidden state, computes the next hidden state via linear transformations and tanh activation. The code runs a manual forward pass over a sequence of inputs, collecting outputs stepwise. All parameters and data generation are explicit; no Flux dependencies are assumed. Inputs and outputs are vectors, lengths controlled by predefined parameters.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/recurrence.md#_snippet_0\n\nLANGUAGE: Julia\nCODE:\n```\noutput_size = 5\ninput_size = 2\nWxh = randn(Float32, output_size, input_size)\nWhh = randn(Float32, output_size, output_size)\nb = zeros(Float32, output_size)\n\nfunction rnn_cell(x, h)\n    h = tanh.(Wxh * x .+ Whh * h .+ b)\n    return h, h\nend\n\nseq_len = 3\n# dummy input data\nx = [rand(Float32, input_size) for i = 1:seq_len] \n# random initial hidden state\nh0 = zeros(Float32, output_size) \n\ny = []\nht = h0\nfor xt in x\n    yt, ht = rnn_cell(xt, ht)\n    y = [y; [yt]]  # concatenate in non-mutating (AD friendly) way\nend\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic Training Dataset in Julia\nDESCRIPTION: Shows the simplest way to create a training dataset in Julia for Flux. It involves creating individual input (`x`) and label (`y`) examples (here, a random matrix and vector) and storing them as a tuple `(x, y)`. A `Vector` of such tuples constitutes the basic `train_set`.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/training/training.md#_snippet_6\n\nLANGUAGE: julia\nCODE:\n```\nx = randn(28, 28)\ny = rand(10)\ndata = [(x, y)]\n```\n\n----------------------------------------\n\nTITLE: Defining Type-Unstable Activation Function in Julia\nDESCRIPTION: Demonstrates an activation function `my_tanh` that explicitly converts the output of `tanh(x)` to `Float64`. This function is type-unstable if the input `x` is `Float32`, leading to significant performance degradation due to mixed-type operations in subsequent layers.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/performance.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\n```julia\nmy_tanh(x) = Float64(tanh(x))\n```\n```\n\n----------------------------------------\n\nTITLE: Defining the Training Function for the Discriminator in Julia with Flux\nDESCRIPTION: Implements the training step for the discriminator using real and fake data. Concatenates inputs, sets target labels (1 for real, 0 for fake), computes predictions, and applies binary cross-entropy loss. Gradients are calculated using Zygote.pullback for custom backward pass, and parameters are updated using the ADAM optimizer. Expects all tensors on GPU if used. Inputs: discriminator, real_data, fake_data. Outputs: loss value (scalar). Limitation: Assumes opt_dscr is globally defined and updated in-place.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-10-14-vanilla-gan.md#_snippet_6\n\nLANGUAGE: julia\nCODE:\n```\nfunction train_dscr!(discriminator, real_data, fake_data)\n    this_batch = size(real_data)[end] # Number of samples in the batch\n    # Concatenate real and fake data into one big vector\n    all_data = hcat(real_data, fake_data)\n\n    # Target vector for predictions: 1 for real data, 0 for fake data.\n    all_target = [ones(eltype(real_data), 1, this_batch) zeros(eltype(fake_data), 1, this_batch)] |> gpu;\n\n    ps = Flux.params(discriminator)\n    loss, pullback = Zygote.pullback(ps) do\n        preds = discriminator(all_data)\n        loss = Flux.Losses.binarycrossentropy(preds, all_target)\n    end\n    # To get the gradients we evaluate the pullback with 1.0 as a seed gradient.\n    grads = pullback(1f0)\n\n    # Update the parameters of the discriminator with the gradients we calculated above\n    Flux.update!(opt_dscr, Flux.params(discriminator), grads)\n    \n    return loss \nend\n```\n\n----------------------------------------\n\nTITLE: Performing Manual Parameter Updates with Gradient Descent in Julia\nDESCRIPTION: Demonstrates the low-level process of updating model parameters using the basic gradient descent rule. It utilizes `fmap` (a Flux utility) to apply the update `p .= p .- η .* g` to each parameter `p` in the `model`, using its corresponding gradient `g` from `grads[1]` and a learning rate `η`. This shows the core operation performed by `Flux.update!` with a simple `Descent` optimizer.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/training/training.md#_snippet_4\n\nLANGUAGE: julia\nCODE:\n```\nη = 0.01   # learning rate\n\n# For each parameter array, update\n# according to the corresponding gradient:\nfmap(model, grads[1]) do p, g\n  p .= p .- η .* g\nend\n```\n\n----------------------------------------\n\nTITLE: Visualizing Images from the CIFAR-10 Dataset in Julia - julia\nDESCRIPTION: Defines an image conversion function from data array to RGB image via colorview and permutation, and visualizes a random training instance with Plots.plot. Essential for ML workflows involving vision to confirm data correctness and distributions. Requires ImageCore and Plots; input is an image array sampled from dataset, output is a displayed image.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-blitz.md#_snippet_16\n\nLANGUAGE: julia\nCODE:\n```\nimage(x) = colorview(RGB, permutedims(x, (3, 2, 1)))\nplot(image(train_x[:,:,:,rand(1:end)]))\n```\n\n----------------------------------------\n\nTITLE: Using Enzyme.gradient Directly with Flux Models in Julia\nDESCRIPTION: Shows how to use Enzyme's native `Enzyme.gradient` function directly with a Flux model and a loss function. It specifies `Reverse` mode for reverse-mode automatic differentiation. The resulting gradient object mirrors the structure of the original model (`Chain` in this case) but contains the gradients. The example verifies that the gradient values are consistent with those obtained using `Flux.gradient`.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/training/enzyme.md#_snippet_2\n\nLANGUAGE: julia-repl\nCODE:\n```\njulia> grads_e = Enzyme.gradient(Reverse, (m,x,y) -> sum(abs2, m(x) .- y), model, Const(x1), Const(y1))\n(Chain(Dense(784 => 32, σ), Dense(32 => 10), softmax), nothing, nothing)\n\njulia> grads_f[1].layers[2].bias ≈ grads_e[1].layers[2].bias\ntrue\n```\n\n----------------------------------------\n\nTITLE: Using and Testing a Custom Split Layer in a Model Pipeline (Julia)\nDESCRIPTION: This code snippet integrates the custom Split layer into a larger model using Flux's Chain, showing how to create a network with multiple output branches. The result is a tuple of output vectors. Requires prior definition of Split, Flux, CUDA, and expected input sizes.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/custom_layers.md#_snippet_10\n\nLANGUAGE: julia\nCODE:\n```\nmodel = Chain(\n              Dense(10 => 5),\n              Split(Dense(5 => 1, tanh), Dense(5 => 3, tanh), Dense(5 => 2))\n             ) |> gpu\n\nmodel(gpu(rand(10)))\n# returns a tuple with three float vectors\n```\n\n----------------------------------------\n\nTITLE: Validating Custom Softmax Output Properties (Range and Sum) in Julia\nDESCRIPTION: Validates the output produced by the `custom_model` incorporating the `custom_softmax`. First, it checks if all output probability values fall within the expected range [0, 1] using `all(0 .<= ... .<= 1)`. Second, it confirms that the sum of probabilities for each data point (sum across columns, `dims=1`) equals 1, ensuring valid probability distributions.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_10\n\nLANGUAGE: julia\nCODE:\n```\njulia> all(0 .<= custom_model(W, b, x) .<= 1)\ntrue\n\njulia> sum(custom_model(W, b, x), dims=1)\n1×150 Matrix{Float32}:\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  …  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n```\n\n----------------------------------------\n\nTITLE: Broadcasting and Vectorized Operations in Julia - julia\nDESCRIPTION: These code snippets illustrate Julia's powerful broadcasting, tiling, and elementwise arithmetic capabilities using the dot syntax. Examples include adding scalars or vectors to entire arrays with .+, transposing and broadcasting ranges, and using elementwise multiplication to create patterns like times tables. The snippets require only Julia base, and all operations preserve array shapes, handling dimensionality either by broadcasting or transposition. Inputs typically are arrays or number ranges, and outputs are new arrays with operations applied.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-blitz.md#_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nx .+ 1\n```\n\nLANGUAGE: julia\nCODE:\n```\nzeros(5,5) .+ (1:5)\n```\n\nLANGUAGE: julia\nCODE:\n```\nzeros(5,5) .+ (1:5)'\n```\n\nLANGUAGE: julia\nCODE:\n```\n(1:5) .* (1:5)'\n```\n\n----------------------------------------\n\nTITLE: Alternative CNN Construction using @autosize Macro in Julia\nDESCRIPTION: Presents an alternative way to conclude the `make_model` function using the `@autosize` macro. Instead of manually calculating the output size with `Flux.outputsize` and then creating the `Dense` layer, `@autosize` infers the required input size for the `Dense` layer directly from the provided input dimensions `(width, height, inchannels, 1)` and the preceding layers in the `Chain`.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/outputsize.md#_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\n# compute the output dimensions & construct appropriate Dense layer:\nreturn @autosize (width, height, inchannels, 1) Chain(conv_layers..., Flux.flatten, Dense(_ => nclasses))\nend\n```\n\n----------------------------------------\n\nTITLE: Defining and Executing Training Loop for Flux Model in Julia\nDESCRIPTION: Shows the initial loss of the `flux_model`, defines a `train_flux_model!` function to perform a single training step (calculating gradients for the `flux_loss` and updating the model's weight and bias parameters directly using the gradient information), and then runs a training loop similar to the custom one. The loop iterates up to 500 times, calling `train_flux_model!`, with early stopping if `flux_accuracy` reaches 0.98.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_27\n\nLANGUAGE: julia\nCODE:\n```\n```jldoctest logistic_regression; filter = r\"[+-]?([0-9]*[.])?[0-9]+(f[+-]*[0-9])?\"\njulia> flux_loss(flux_model, x, flux_y_onehot)\n1.215731131385928\n\njulia> function train_flux_model!(f_loss, model, features, labels_onehot)\n           dLdm, _, _ = gradient(f_loss, model, features, labels_onehot)\n           @. model[1].weight = model[1].weight - 0.1 * dLdm[:layers][1][:weight]\n           @. model[1].bias = model[1].bias - 0.1 * dLdm[:layers][1][:bias]\n       end;\n\njulia> for i = 1:500\n            train_flux_model!(flux_loss, flux_model, x, flux_y_onehot);\n            flux_accuracy(x, y) >= 0.98 && break\n       end\n```\n```\n\n----------------------------------------\n\nTITLE: Evaluating Output and Size of Flux Dense Model in Julia\nDESCRIPTION: Runs the flux_model on the full feature dataset x and checks output size and the first prediction, comparing to ground truth. This ensures that the model configuration aligns correctly and output dimensions are as expected prior to loss computation. Assumes input x matches the layer's input shape.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_11\n\nLANGUAGE: julia\nCODE:\n```\njulia> flux_model(x) |> size\n(1, 61)\n\njulia> flux_model(x)[1], y[1]\n(-1.8525281f0, -7.0f0)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Struct and Closure Equivalence in Julia for Encapsulation\nDESCRIPTION: This code compares a closure and struct-based encapsulation by inspecting their fields and methods to highlight their internal similarities in Julia. It displays structural introspection using dump and demonstrates method calls. Required dependencies: Julia Base. Inputs: poly3 (closure), poly3s (struct instance); outputs: structural details via dump, boolean comparisons, and method listings. Offers insights into how Julia treats encapsulation forms similarly.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/basics.md#_snippet_4\n\nLANGUAGE: julia\nCODE:\n```\ndump(poly3), dump(poly3s)  # both contain θ3: Array\npoly3s.θ3 == poly3.θ3 == θ  # field called :θ3 has same value\nmethods(poly3)\nmethods(poly3s)  # each has 1 method, accepting x\n```\n\n----------------------------------------\n\nTITLE: Numerical Differentiation for Scalar Functions in Julia\nDESCRIPTION: This snippet illustrates how to approximate the derivative of poly1 at a point using finite differences. By evaluating the function at two nearby input values, the numerical slope is computed and refined by reducing the increment. Required dependencies: Julia Base. Inputs: function poly1 and numeric increment; outputs: approximate derivatives, showcasing the basics of numerical differentiation. Limitation: not as accurate as analytical differentiation.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/basics.md#_snippet_6\n\nLANGUAGE: julia\nCODE:\n```\njulia> (poly1(5 + 0.1) - poly1(5)) / 0.1\n2.010000000000005\n\njulia> (poly1(5 + 0.001) - poly1(5)) / 0.001  # answer is getting close to 2\n2.000100000003613\n```\n\n----------------------------------------\n\nTITLE: Referencing Core Training and Optimizer Functions in Flux/Optimisers (Julia)\nDESCRIPTION: Documents the primary functions for Flux model training: `Flux.Train.setup` (and `Optimisers.setup`) initialize the optimizer state, `Flux.Train.train!` performs a single training step, and `Optimisers.update`/`update!` apply optimization rules to calculate gradients and update model parameters. Flux's `setup` version handles mutable parameter arrays common in Flux models.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/training/reference.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nFlux.Train.setup\nFlux.Train.train!(loss, model, data, state)\nOptimisers.update\nOptimisers.update!\nOptimisers.setup\n```\n\n----------------------------------------\n\nTITLE: Plotting the tanh Activation Function using UnicodePlots in Julia\nDESCRIPTION: This Julia REPL code snippet demonstrates visualizing the hyperbolic tangent (`tanh`) function's curve between x=-3 and x=3 using the `lineplot` function from the `UnicodePlots` package. The surrounding context discusses using `tanh` as an activation function in Flux models, noting that Flux might automatically replace the slower Base `tanh` with `NNlib.tanh_fast`.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/models/activation.md#_snippet_0\n\nLANGUAGE: julia-repl\nCODE:\n```\njulia> using UnicodePlots\n\njulia> lineplot(tanh, -3, 3, height=7)\n           ┌────────────────────────────────────────┐        \n         1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⣀⠤⠔⠒⠒⠉⠉⠉⠉⠉⠉⠉⠉⠉│ tanh(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⡠⠖⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⡰⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n   f(x)    │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⡤⡯⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⠎⠁⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠴⠊⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n        -1 │⣀⣀⣀⣀⣀⣀⣀⣀⣀⡤⠤⠔⠒⠉⠁⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           └────────────────────────────────────────┘        \n           ⠀-3⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀3⠀        \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀        \n```\n\n----------------------------------------\n\nTITLE: Inspecting Iris Dataset Targets and Features in Julia\nDESCRIPTION: Displays the target labels (`y`) as a 1x150 matrix containing string representations of the Iris species. It also provides a summary of the feature matrix (`x`), showing its dimensions (4 features x 150 samples) and data type (Float64).\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\njulia> y\n1×150 Matrix{InlineStrings.String15}:\n \"Iris-setosa\"  \"Iris-setosa\"  …  \"Iris-virginica\"  \"Iris-virginica\"\n\njulia> x |> summary\n\"4×150 Matrix{Float64}\"\n```\n\n----------------------------------------\n\nTITLE: Synchronizing Weight Parameter With Flux Model in Julia\nDESCRIPTION: Sets the custom model's weight W to exactly match the weight in the flux_model, facilitating an apples-to-apples comparison of the two models' behaviors. This follows parameter inspection and is a manual override to ensure experimental consistency. Input: value copied from the Flux model's weight parameter.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_13\n\nLANGUAGE: julia\nCODE:\n```\njulia> W = Float32[1.1412252]\n1-element Vector{Float32}:\n 1.1412252\n```\n\n----------------------------------------\n\nTITLE: Computing Higher-Order Derivatives via Automatic Differentiation - julia\nDESCRIPTION: This example shows how to compose Flux's gradient operator to evaluate second derivatives (i.e., the derivative of the derivative function), illustrating Zygote's support for higher-order gradients. The code is dependency-light, needing only Flux; the input is a scalar, and the output is a float (constant in this example).\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-blitz.md#_snippet_5\n\nLANGUAGE: julia\nCODE:\n```\nddf(x) = gradient(df, x)[1]\n\nddf(5)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Mean Squared Error Loss Function in Julia\nDESCRIPTION: Implements a function custom_loss(weights, biases, features, labels) computing mean squared error between predictions and true labels. The function works with broadcasted differences and returns a scalar loss for training. No dependencies except the previously-defined custom_model; key inputs: parameter arrays and feature/label matrices.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_8\n\nLANGUAGE: julia\nCODE:\n```\njulia> function custom_loss(weights, biases, features, labels)\n           ŷ = custom_model(weights, biases, features)\n           sum((labels .- ŷ).^2) / length(features)\n       end;\n\njulia> custom_loss(W, b, x, y)\n23.772217f0\n```\n\n----------------------------------------\n\nTITLE: Updating Model Parameters Manually in Julia\nDESCRIPTION: Shows the manual update step for weights `W` and bias `b` according to the gradient descent algorithm. It uses the previously calculated gradients (`dLdW`, `dLdb`) and a hardcoded learning rate of 0.1. The `.=` operator performs an in-place broadcasted update.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_17\n\nLANGUAGE: julia\nCODE:\n```\njulia> W .= W .- 0.1 .* dLdW\n1-element Vector{Float32}:\n 1.8144473\n```\n\nLANGUAGE: julia\nCODE:\n```\njulia> b .= b .- 0.1 .* dLdb\n1-element Vector{Float32}:\n 0.41325632\n```\n\n----------------------------------------\n\nTITLE: Checking Standard Deviation of Training Data in Julia\nDESCRIPTION: Calculates the standard deviation of the training features (`x_train`) using the `std` function from the `Statistics` package. This is done to show that the features have varying scales, motivating the need for normalization before training.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_25\n\nLANGUAGE: julia\nCODE:\n```\njulia> std(x_train)\n134.06786f0\n```\n\n----------------------------------------\n\nTITLE: Manual Parameter Update with Gradient Descent in Julia - julia\nDESCRIPTION: This snippet implements a manual parameter update pass using the gradient produced in the previous code, following the rule weights = weights - eta * gradient. Standard elementwise broadcasting and parameter iteration with a custom learning rate are showcased. No extra dependencies beyond Flux; input is learning rate, model gradients, parameter array(s), and output is updated parameters (in place).\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-blitz.md#_snippet_11\n\nLANGUAGE: julia\nCODE:\n```\nη = 0.1\nfor (k, p) in trainables(model, path=true)\n    p .+= -η * getkeypath(grad, p)\nend\n```\n\n----------------------------------------\n\nTITLE: Gradient Descent Update Equations\nDESCRIPTION: Mathematical equations defining the update rules for weights (W) and biases (b) in the gradient descent algorithm. The parameters are updated by subtracting the product of the learning rate (η) and the respective gradient of the loss function (dL/dW or dL/db).\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_15\n\nLANGUAGE: math\nCODE:\n```\n\\begin{aligned}\nW &= W - \\eta * \\frac{dL}{dW} \\\\\nb &= b - \\eta * \\frac{dL}{db}\n\\end{aligned}\n```\n\n----------------------------------------\n\nTITLE: Referencing Optimizer State Modification Functions in Optimisers (Julia)\nDESCRIPTION: Documents functions from Optimisers.jl, used within Flux, for dynamically modifying the optimizer state: `adjust!` changes hyperparameters like learning rate, `freeze!` prevents updates to selected parameters, and `thaw!` resumes updates for previously frozen parameters. These functions mutate the state object returned by `setup`.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/training/reference.md#_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nOptimisers.adjust!\nOptimisers.freeze!\nOptimisers.thaw!\n```\n\n----------------------------------------\n\nTITLE: Defining and Calculating Labels Using a Linear Function in Julia\nDESCRIPTION: Defines a linear function mapping inputs to outputs using the formula y = 3x + 2. The @. macro is used for broadcasting, enabling element-wise operations on matrices. Produces the label matrix y for all x, suitable for regression tasks, with no external dependencies and operates directly on the feature matrix.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\njulia> f(x) = @. 3x + 2;\n\njulia> y = f(x)\n```\n\n----------------------------------------\n\nTITLE: Checking Final Loss After Training Flux Model in Julia\nDESCRIPTION: Calculates the final loss of the trained `model` on the normalized training data (`x_train_n`, `y_train`) using the previously defined `loss` function. This shows the model's performance after the convergence-based training loop has completed.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_31\n\nLANGUAGE: julia\nCODE:\n```\njulia> loss(model, x_train_n, y_train)\n27.1272f0\n```\n\n----------------------------------------\n\nTITLE: Evaluating Custom Loss Function in Julia\nDESCRIPTION: Computes the loss using the `custom_loss` function with the updated weights `W` and bias `b` on the given data `x` and `y`. This is used to check if the training step successfully reduced the model's error.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_18\n\nLANGUAGE: julia\nCODE:\n```\njulia> custom_loss(W, b, x, y)\n17.157953f0\n```\n\n----------------------------------------\n\nTITLE: Creating Output Image Grid - Flux.jl - Julia\nDESCRIPTION: This function constructs an image grid from generated images output by a GAN, taking into account the normalization range of the generator outputs. It utilizes Julia's array manipulation capabilities and domain-specific image types to properly de-normalize and layout images for visualization. Dependencies include Flux.jl for tensor handling and Images.jl for image representation; inputs require a generator, input noise, and hyperparameter object specifying output dimensions.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-10-08-dcgan-mnist.md#_snippet_11\n\nLANGUAGE: julia\nCODE:\n```\nfunction create_output_image(gen, fixed_noise, hparams)\n    fake_images = cpu(gen.(fixed_noise))\n    image_array = reduce(vcat, reduce.(hcat, partition(fake_images, hparams.output_dim)))\n    image_array = permutedims(dropdims(image_array; dims=(3, 4)), (2, 1))\n    image_array = @. Gray(image_array + 1f0) / 2f0\n    return image_array\nend\n```\n\n----------------------------------------\n\nTITLE: Comparing Custom onecold with Flux.onecold in Julia\nDESCRIPTION: Compares the output of the custom `custom_onecold` function with Flux's built-in `Flux.onecold` function. It applies both functions to equivalent one-hot encoded data (`flux_y_onehot`, `custom_y_onehot`) and uses element-wise comparison (`.==`) and `all()` to verify that they produce identical results.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/logistic_regression.md#_snippet_18\n\nLANGUAGE: julia\nCODE:\n```\n```jldoctest logistic_regression\njulia> istrue = Flux.onecold(flux_y_onehot, classes) .== custom_onecold(custom_y_onehot);\n\njulia> all(istrue)\ntrue\n```\n```\n\n----------------------------------------\n\nTITLE: Importing Flux and Plots in Julia\nDESCRIPTION: Imports the required packages for building and visualizing machine learning models in Julia: Flux for ML constructs and Plots for plotting. Both packages must be installed in the environment (e.g., via Pkg). No arguments or parameters; simply prepares the workspace for subsequent model building and visualization.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\njulia> using Flux, Plots\n```\n\n----------------------------------------\n\nTITLE: Re-exporting and Documenting MLUtils Utility Functions in Julia\nDESCRIPTION: This snippet lists several utility functions from MLUtils (and a few from Flux) that are exposed to assist with dataset preparation, batching, normalization, observation manipulation, and related data pipeline tasks. Each function has a specific role: e.g., batch forms batches, batchsize queries batch sizes, flatten reshapes data, and so on. Though only function names are given, all are accessible in Flux due to re-export. Inputs and outputs depend on the specific function; dependencies include MLUtils.jl and optionally Flux.jl itself for functions like Flux.flatten. Limitations or required parameters are described in individual function docs.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/data/mlutils.md#_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nMLUtils.batch\nMLUtils.batchsize\nMLUtils.batchseq\nMLUtils.BatchView\nMLUtils.chunk\nMLUtils.eachobs\nMLUtils.fill_like\nMLUtils.filterobs\nFlux.flatten\nMLUtils.flatten\nMLUtils.getobs\nMLUtils.getobs!\nMLUtils.joinobs\nMLUtils.group_counts\nMLUtils.group_indices\nMLUtils.groupobs\nMLUtils.kfolds\nMLUtils.leavepout\nMLUtils.mapobs\nMLUtils.numobs\nMLUtils.normalise\nMLUtils.obsview\nMLUtils.ObsView\nMLUtils.ones_like\nMLUtils.oversample\nMLUtils.randobs\nMLUtils.rand_like\nMLUtils.randn_like\nMLUtils.rpad_constant\nMLUtils.shuffleobs\nMLUtils.splitobs\nMLUtils.unbatch\nMLUtils.undersample\nMLUtils.unsqueeze\nMLUtils.unstack\nMLUtils.zeros_like\n```\n\n----------------------------------------\n\nTITLE: Creating and Querying Arrays in Julia - julia\nDESCRIPTION: These snippets demonstrate initializing vectors and matrices in Julia using literals and built-in functions such as rand, ones, zeros, and randn. The focus is on showing how to declare arrays of different dimensions and random sampling, as well as casting them to Float32 and BigFloat for different precisions. The provided code also covers functions for checking array length, shape, and indexing, which are foundational for any numerical computation in Julia. No external dependencies are required unless using types like BigFloat (which is built-in). Inputs and outputs are simple array instances and their attributes.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2024-04-10-blitz.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nx = [1, 2, 3]\n```\n\nLANGUAGE: julia\nCODE:\n```\nx = [1 2; 3 4]\n```\n\nLANGUAGE: julia\nCODE:\n```\nx = rand(5, 3)\n```\n\nLANGUAGE: julia\nCODE:\n```\nx = rand(BigFloat, 5, 3)\n\nx = rand(Float32, 5, 3)\n```\n\nLANGUAGE: julia\nCODE:\n```\nlength(x)\n```\n\nLANGUAGE: julia\nCODE:\n```\nsize(x)\n```\n\nLANGUAGE: julia\nCODE:\n```\nx\n\nx[2, 3]\n```\n\nLANGUAGE: julia\nCODE:\n```\nx[:, 3]\n```\n\nLANGUAGE: julia\nCODE:\n```\nx + x\n\nx - x\n```\n\n----------------------------------------\n\nTITLE: Listing Functors.jl and Flux Functions for Documentation Generation\nDESCRIPTION: This block uses the `@docs` syntax from Documenter.jl to automatically insert the documentation for the specified Flux and Functors.jl functions and macros into the generated documentation page. It lists core components for working with Flux model structures (`@layer`, `@functor`, `fmap`, etc.) and custom walk types.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/models/functors.md#_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\nFlux.@layer\nFunctors.@leaf\nFunctors.@functor\nFunctors.fmap\nFunctors.fmap_with_path\nFunctors.isleaf\nFunctors.children\nFunctors.fcollect\nFunctors.functor\nFunctors.fmapstructure\nFunctors.fmapstructure_with_path\nFunctors.execute\nFunctors.AbstractWalk\nFunctors.ExcludeWalk\nFunctors.CachedWalk\n```\n```\n\n----------------------------------------\n\nTITLE: Documenting ChainRulesCore Non-Differentiability APIs in Julia\nDESCRIPTION: This snippet summarizes the macros and functions provided by ChainRulesCore.jl to mark code as non-differentiable or to ignore certain operations during automatic differentiation. It documents the usage of ignore_derivatives and @non_differentiable, which allow users to prevent AD systems from attempting to compute gradients for specified operations. These entries are relevant for advanced users who need to customize the behavior of the automatic differentiation pipeline in Julia.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/training/zygote.md#_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nChainRulesCore.ignore_derivatives\nChainRulesCore.@non_differentiable\n```\n\n----------------------------------------\n\nTITLE: Listing MLDataDevices Functions for Documentation in Julia\nDESCRIPTION: Uses the `@docs` macro from Documenter.jl to automatically include the documentation for specified functions and types from the `MLDataDevices` package within the generated documentation. This lists functions related to device detection (cpu/gpu), backend selection, data transfer, and device management utilities provided by MLDataDevices and re-exported by Flux.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/data/mldatadevices.md#_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\nMLDataDevices.cpu_device\nMLDataDevices.default_device_rng\nMLDataDevices.functional\nMLDataDevices.get_device\nMLDataDevices.gpu_device\nMLDataDevices.gpu_backend!\nMLDataDevices.get_device_type\nMLDataDevices.isleaf\nMLDataDevices.loaded\nMLDataDevices.reset_gpu_device!\nMLDataDevices.set_device!\nMLDataDevices.supported_gpu_backends\nMLDataDevices.DeviceIterator\n```\n```\n\n----------------------------------------\n\nTITLE: Importing Necessary Packages for MNIST ConvNet in Julia\nDESCRIPTION: Imports required Julia packages: Flux for deep learning functionalities, MLDatasets for accessing the MNIST dataset, Statistics for utility functions like mean, Flux utilities for specific operations (onehotbatch, onecold, logitcrossentropy, params), Base.Iterators for data partitioning, Printf for formatted printing, BSON for saving model parameters, and CUDA for GPU acceleration. It also configures CUDA to disallow scalar operations for performance.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-02-07-convnet.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nusing Flux, MLDatasets, Statistics\nusing Flux: onehotbatch, onecold, logitcrossentropy, params\nusing MLDatasets: MNIST\nusing Base.Iterators: partition\nusing Printf, BSON\nusing CUDA\nCUDA.allowscalar(false)\n```\n\n----------------------------------------\n\nTITLE: Importing Packages for Complex Regression in Julia\nDESCRIPTION: Imports the required Julia packages for the second part of the tutorial involving the Boston Housing dataset. `Flux` is for the machine learning framework, `Statistics` for statistical functions (like `std`), `MLDatasets` for loading the dataset, and `DataFrames` for data manipulation.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_22\n\nLANGUAGE: julia\nCODE:\n```\njulia> using Flux, Statistics, MLDatasets, DataFrames\n```\n\n----------------------------------------\n\nTITLE: Listing Flux Functions Supporting Enzyme Integration (Julia @docs)\nDESCRIPTION: Provides a list of Flux function signatures specifically adapted for Enzyme.jl integration, suitable for use with documentation generators like Documenter.jl (`@docs`). It includes methods for `Flux.gradient`, `Flux.withgradient`, and `Flux.train!` that accept arguments of type `Flux.EnzymeCore.Const` or `Flux.EnzymeCore.Duplicated`, indicating Enzyme should be used for differentiation.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/training/enzyme.md#_snippet_4\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\nFlux.gradient(f, args::Union{Flux.EnzymeCore.Const, Flux.EnzymeCore.Duplicated}...)\nFlux.withgradient(f, args::Union{Flux.EnzymeCore.Const, Flux.EnzymeCore.Duplicated}...)\nFlux.train!(loss, model::Flux.EnzymeCore.Duplicated, data, opt)\n```\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages with Pkg - Julia\nDESCRIPTION: Initializes a Julia project environment and installs the necessary dependencies (Images, Flux, MLDatasets, CUDA, Parameters) for building and training a DCGAN using the package manager. Ensures the application is run in an isolated environment, suitable for reproducible research or tutorials. Expects internet connectivity and may take several minutes depending on connection speed.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/old_tutorials/2021-10-08-dcgan-mnist.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nusing Pkg\n\n# Activate a new project environment in the current directory\nPkg.activate(\".\")\n# Add the required packages to the environment\nPkg.add([\"Images\", \"Flux\", \"MLDatasets\", \"CUDA\", \"Parameters\"])\n```\n\n----------------------------------------\n\nTITLE: Applying Random Scaling to Input Features in Julia\nDESCRIPTION: Multiplies each input feature x by a corresponding random Float32 scalar, introducing noise or randomness to the synthetic dataset. The rand function generates 61 random numbers, which are reshaped to match x's dimensions. This approach simulates real-world variance and prevents overfitting to perfect data; it requires the standard Julia random module.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/tutorials/linear_regression.md#_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\njulia> x = x .* reshape(rand(Float32, 61), (1, 61));\n```\n\n----------------------------------------\n\nTITLE: Referencing OptimiserChain Documentation in Julia Docs\nDESCRIPTION: Uses the `@docs` directive to pull in the documentation for `Optimisers.OptimiserChain` from the Optimisers.jl package into the Flux.jl documentation, explaining how multiple optimizers or rules can be chained together sequentially.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/training/optimisers.md#_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\nOptimisers.OptimiserChain\n```\n\n----------------------------------------\n\nTITLE: Referencing NNlib Padding Functions in Julia\nDESCRIPTION: Lists various array padding functions provided by NNlib.jl for documentation generation using Julia's `@docs` macro. Covers common padding strategies like circular, constant, reflect, repeat, symmetric, and zero padding.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/models/nnlib.md#_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\nNNlib.pad_circular\nNNlib.pad_constant\nNNlib.pad_reflect\nNNlib.pad_repeat\nNNlib.pad_symmetric\nNNlib.pad_zeros\n```\n```\n\n----------------------------------------\n\nTITLE: Referencing Softmax Functions in Julia\nDESCRIPTION: Lists softmax and log-softmax functions from NNlib.jl for documentation generation using Julia's `@docs` macro. `NNlib.logsoftmax` is notably used internally by `Flux.logitcrossentropy`.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/models/nnlib.md#_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\nsoftmax\nlogsoftmax\n```\n```\n\n----------------------------------------\n\nTITLE: Referencing NNlib Loss Functions in Julia\nDESCRIPTION: Lists the Connectionist Temporal Classification (CTC) loss function (`ctc_loss`) from NNlib.jl for documentation generation using Julia's `@docs` macro.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/models/nnlib.md#_snippet_10\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\nctc_loss\n```\n```\n\n----------------------------------------\n\nTITLE: Referencing NNlib Upsampling and Pixel Shuffle Functions in Julia\nDESCRIPTION: Lists upsampling functions (nearest neighbor, linear, bilinear, trilinear) and their gradients, along with the `pixel_shuffle` function from NNlib.jl, for documentation generation using Julia's `@docs` macro. These are used as backends for Flux's `Upsample` and `PixelShuffle` layers.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/models/nnlib.md#_snippet_6\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\nupsample_nearest\nupsample_linear\n∇upsample_linear\nupsample_bilinear\n∇upsample_bilinear\nupsample_trilinear\n∇upsample_trilinear\npixel_shuffle\n```\n```\n\n----------------------------------------\n\nTITLE: Referencing NNlib Gather and Scatter Functions in Julia\nDESCRIPTION: Lists gather and scatter operations from NNlib.jl for documentation generation using Julia's `@docs` macro. Includes standard and in-place versions (`gather`, `gather!`, `scatter`, `scatter!`). `NNlib.gather` serves as the backend for Flux's `Embedding` layer.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/models/nnlib.md#_snippet_8\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\nNNlib.gather\nNNlib.gather!\nNNlib.scatter\nNNlib.scatter!\n```\n```\n\n----------------------------------------\n\nTITLE: Defining and Documenting Built-in Layer Types in Flux.jl (Julia)\nDESCRIPTION: These documentation directives and Julia code snippets list and detail the built-in neural network layers available in Flux.jl. They use Julia's documentation tools (such as @docs) to generate structured help pages for each group of layers. No executable layer definitions are present here; instead, this is a compilation of references leveraging Julia syntax for documentation. Usage presumes that the user has Flux.jl installed and is familiar with Julia's documentation system.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/models/layers.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\nDense\nFlux.Bilinear\nFlux.Scale\n```\n```\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\nConv\nConvTranspose\nCrossCor\nDepthwiseConv\nSamePad\n```\n```\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\nMultiHeadAttention\n```\n```\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\nAdaptiveMaxPool\nMaxPool\nGlobalMaxPool\nAdaptiveMeanPool\nMeanPool\nGlobalMeanPool\n```\n```\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\nUpsample\nPixelShuffle\n```\n```\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\nFlux.Embedding\nFlux.EmbeddingBag\n```\n```\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\nChain\nFlux.activations\nMaxout\nSkipConnection\nParallel\nPairwiseFusion\n```\n```\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\nRecurrence\nRNNCell\nRNN\nLSTMCell\nLSTM\nGRUCell\nGRU\nGRUv3Cell\nGRUv3\nFlux.initialstates\n```\n```\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\nBatchNorm\nDropout\nAlphaDropout\nLayerNorm\nInstanceNorm\nGroupNorm\nWeightNorm\nFlux.remove_weight_norms\nFlux.normalise\n```\n```\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\ntestmode!\ntrainmode!\n```\n```\n\n----------------------------------------\n\nTITLE: Flux.jl MIT License Text\nDESCRIPTION: The full text of the MIT 'Expat' License as applied to the Flux.jl package. It grants broad permissions for usage, modification, and distribution, provided the copyright notice and license text are included. The software is provided 'as is' without warranty.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/LICENSE.md#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n> Copyright (c) 2016-19: Julia Computing, INc., Mike Innes and Contributors\n>\n> Permission is hereby granted, free of charge, to any person obtaining\n> a copy of this software and associated documentation files (the\n> \"Software\"), to deal in the Software without restriction, including\n> without limitation the rights to use, copy, modify, merge, publish,\n> distribute, sublicense, and/or sell copies of the Software, and to\n> permit persons to whom the Software is furnished to do so, subject to\n> the following conditions:\n>\n> The above copyright notice and this permission notice shall be\n> included in all copies or substantial portions of the Software.\n>\n> THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n> EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n> MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n> IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n> CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n> TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n> SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n```\n\n----------------------------------------\n\nTITLE: Referencing NNlib Dropout Functions in Julia\nDESCRIPTION: Lists the dropout functions from NNlib.jl for documentation generation using Julia's `@docs` macro. Includes the standard `dropout` function and its in-place version `dropout!`.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/models/nnlib.md#_snippet_5\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\nNNlib.dropout\nNNlib.dropout!\n```\n```\n\n----------------------------------------\n\nTITLE: Embedding an Image using Raw HTML\nDESCRIPTION: This snippet uses the `@raw html` directive (specific to documentation systems like Documenter.jl) to embed an HTML `<img>` tag directly into the output. It displays an image located at the specified relative path, presumably illustrating the training loss.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/guide/models/quickstart.md#_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<img align=\"right\" width=\"300px\" src=\"../../../assets/quickstart/loss.png\">\n```\n\n----------------------------------------\n\nTITLE: Configuring Documentation Generation Metadata in Julia\nDESCRIPTION: Sets metadata for the Julia documentation generator (Documenter.jl). It specifies `MLDataDevices` as the current module for resolving links and enables the `CollapsedDocStrings` option for a more compact display.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/data/mldatadevices.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\n```@meta\nCurrentModule = MLDataDevices\nCollapsedDocStrings = true\n```\n```\n\n----------------------------------------\n\nTITLE: Referencing Optimiser Types from Optimisers.jl in Julia Docs\nDESCRIPTION: This block uses Julia's documentation system (`@docs`) to automatically insert documentation for various standard optimizer types (like Descent, Adam, Momentum, etc.) directly from the `Optimisers.jl` package into the current Flux.jl documentation page. It serves as a reference list for available optimizers.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/training/optimisers.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nOptimisers.Descent\nOptimisers.Momentum\nOptimisers.Nesterov\nOptimisers.RMSProp\nOptimisers.Adam\nOptimisers.RAdam\nOptimisers.AdaMax\nOptimisers.AdaGrad\nOptimisers.AdaDelta\nOptimisers.AMSGrad\nOptimisers.NAdam\nOptimisers.AdamW\nOptimisers.OAdam\nOptimisers.AdaBelief\nOptimisers.Lion\n```\n\n----------------------------------------\n\nTITLE: Referencing NNlib Pooling Functions in Julia\nDESCRIPTION: Lists pooling-related functions and types from NNlib.jl for documentation generation using Julia's `@docs` macro. This includes `PoolDims`, `maxpool`, `meanpool`, and `lpnormpool`, which serve as backends for various Flux pooling layers like `MaxPool`, `MeanPool`, and their adaptive/global variants.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/models/nnlib.md#_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\nNNlib.PoolDims\nNNlib.lpnormpool\nNNlib.maxpool\nNNlib.meanpool\n```\n```\n\n----------------------------------------\n\nTITLE: Listing Flux GPU/CPU Utility Functions for Documentation Generation\nDESCRIPTION: This block uses the `@docs` syntax from Documenter.jl to include documentation for Flux utility functions related to moving models or data between the CPU and GPU. It specifically lists `cpu` and the `gpu` function, noting its different methods for generic types and Flux DataLoaders.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/models/functors.md#_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\ncpu\ngpu(::Any)\ngpu(::Flux.DataLoader)\n```\n```\n\n----------------------------------------\n\nTITLE: Referencing Decay Types from Optimisers.jl in Julia Docs\nDESCRIPTION: Uses the `@docs` directive to include documentation for decay types like `SignDecay` and `WeightDecay` from `Optimisers.jl` within the Flux.jl documentation. These decays can be composed with other optimizers.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/training/optimisers.md#_snippet_4\n\nLANGUAGE: julia\nCODE:\n```\nOptimisers.SignDecay\nOptimisers.WeightDecay\n```\n\n----------------------------------------\n\nTITLE: Referencing NNlib Sampling Functions in Julia\nDESCRIPTION: Lists grid sampling functions and their gradients (`grid_sample`, `∇grid_sample`) from NNlib.jl for documentation generation using Julia's `@docs` macro.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/models/nnlib.md#_snippet_9\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\ngrid_sample\n∇grid_sample\n```\n```\n\n----------------------------------------\n\nTITLE: Documenter.jl Metadata Block\nDESCRIPTION: This is a metadata block used by Documenter.jl, a documentation generator for Julia. The `CollapsedDocStrings = true` setting configures how docstrings are displayed in the generated documentation, likely collapsing them by default.\nSOURCE: https://github.com/fluxml/flux.jl/blob/master/docs/src/reference/models/functors.md#_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\n```@meta\nCollapsedDocStrings = true\n```\n```"
  }
]
[
  {
    "owner": "dbt-labs",
    "repo": "docs.getdbt.com",
    "content": "TITLE: Configuring Model Properties in dbt YAML Files\nDESCRIPTION: This YAML schema defines the structure for configuring dbt model properties including descriptions, configurations, tests, and column specifications. Properties are declared in YAML files within the models directory and can include metadata, tests, constraints, versions, and column-level configurations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/model-properties.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  # Model name must match the filename of a model -- including case sensitivity\n  - [name](/reference/resource-properties/model_name): model_name \n    [description](/reference/resource-properties/description): <markdown_string>\n    [docs](/reference/resource-configs/docs):\n      show: true | false\n      node_color: <color_id> # Use name (such as node_color: purple) or hex code with quotes (such as node_color: \"#cd7f32\")\n    [latest_version](/reference/resource-properties/latest_version): <version_identifier>\n    [deprecation_date](/reference/resource-properties/deprecation_date): <YAML_DateTime>\n    [access](/reference/resource-configs/access): private | protected | public\n    [config](/reference/resource-properties/config):\n      [<model_config>](/reference/model-configs): <config_value>\n    [constraints](/reference/resource-properties/constraints):\n      - <constraint>\n    [tests](/reference/resource-properties/data-tests):\n      - <test>\n      - ... # declare additional data tests\n    [columns](/reference/resource-properties/columns):\n      - name: <column_name> # required\n        [description](/reference/resource-properties/description): <markdown_string>\n        [meta](/reference/resource-configs/meta): {<dictionary>}\n        [quote](/reference/resource-properties/columns#quote): true | false\n        [constraints](/reference/resource-properties/constraints):\n          - <constraint>\n        [tests](/reference/resource-properties/data-tests):\n          - <test>\n          - ... # declare additional data tests\n        [tags](/reference/resource-configs/tags): [<string>]\n        \n        # only required in conjunction with time_spine key\n        [granularity](/docs/build/metricflow-time-spine#creating-a-time-spine-table): <[any supported time granularity](/docs/build/dimensions?dimension=time_gran)> \n\n      - name: ... # declare properties of additional columns\n\n    [time_spine](/docs/build/metricflow-time-spine):\n      standard_granularity_column: <column_name>\n\n    [versions](/reference/resource-properties/versions):\n      - [v](/reference/resource-properties/versions#v): <version_identifier> # required\n        [defined_in](/reference/resource-properties/versions#defined-in): <definition_file_name>\n        [description](/reference/resource-properties/description): <markdown_string>\n        [docs](/reference/resource-configs/docs):\n          show: true | false\n        [access](/reference/resource-configs/access): private | protected | public\n        [constraints](/reference/resource-properties/constraints):\n          - <constraint>\n        [config](/reference/resource-properties/config):\n          [<model_config>](/reference/model-configs): <config_value>\n        [tests](/reference/resource-properties/data-tests):\n          - <test>\n          - ... # declare additional data tests\n        columns:\n          # include/exclude columns from the top-level model properties\n          - [include](/reference/resource-properties/versions#include): <include_value>\n            [exclude](/reference/resource-properties/versions#include): <exclude_list>\n          # specify additional columns\n          - name: <column_name> # required\n            [quote](/reference/resource-properties/columns#quote): true | false\n            [constraints](/reference/resource-properties/constraints):\n              - <constraint>\n            [tests](/reference/resource-properties/data-tests):\n              - <test>\n              - ... # declare additional data tests\n            [tags](/reference/resource-configs/tags): [<string>]\n        - v: ... # declare additional versions\n```\n\n----------------------------------------\n\nTITLE: Running dbt Models Command\nDESCRIPTION: The basic command to execute dbt models for data transformation in your data warehouse.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/models.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndbt run\n```\n\n----------------------------------------\n\nTITLE: Configuring Materialization in dbt Project File (YAML)\nDESCRIPTION: This snippet shows how to configure materialization for models in the dbt_project.yml file. It uses the 'materialized' config option to specify the materialization type for a given resource path.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/materialized.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n[config-version]: 2\n\nmodels:\n  [<resource-path>]:\n    +materialized: [<materialization_name>]\n```\n\n----------------------------------------\n\nTITLE: Creating Simple Metrics Examples in YAML\nDESCRIPTION: This code snippet provides practical examples of creating simple metrics in YAML. It demonstrates how to define metrics for customer count and large orders, showcasing the use of various parameters like fill_nulls_with, join_to_timespine, and filters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/simple.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n  metrics: \n    - name: customers\n      description: Count of customers\n      type: simple # Pointers to a measure you created in a semantic model\n      label: Count of customers\n      type_params:\n        measure: \n          name: customers # The measure you are creating a proxy of.\n          fill_nulls_with: 0 \n          join_to_timespine: true\n          alias: customer_count\n          filter: {{ Dimension('customer__customer_total') }} >= 20\n    - name: large_orders\n      description: \"Order with order values over 20.\"\n      type: simple\n      label: Large orders\n      type_params:\n        measure: \n          name: orders\n      filter: | # For any metric you can optionally include a filter on dimension values\n        {{Dimension('customer__order_total_dim')}} >= 20\n```\n\n----------------------------------------\n\nTITLE: Implementing Incremental Models with Unique Key in dbt\nDESCRIPTION: An example of an incremental model for calculating daily active users with a unique_key parameter to ensure existing rows are updated rather than duplicated when the model runs multiple times per day.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/incremental-models.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        materialized='incremental',\n        unique_key='date_day'\n    )\n}}\n\nselect\n    date_trunc('day', event_at) as date_day,\n    count(distinct user_id) as daily_active_users\n\nfrom {{ ref('app_data_events') }}\n\n\n{% if is_incremental() %}\n\n  -- this filter will only be applied on an incremental run\n  -- (uses >= to include records arriving later on the same day as the last run of this model)\n  where date_day >= (select coalesce(max(date_day), '1900-01-01') from {{ this }})\n\n{% endif %}\n\ngroup by 1\n```\n\n----------------------------------------\n\nTITLE: Basic Python Model Template Structure\nDESCRIPTION: The minimal required structure for a Python model in dbt, showing the model function signature that accepts dbt and session parameters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/python-models.md#2025-04-09_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef model(dbt, session):\n\n    ...\n\n    return final_df\n```\n\n----------------------------------------\n\nTITLE: Implementing a Well-Structured dbt SQL Model with Proper Field Styling\nDESCRIPTION: An example dbt model that demonstrates recommended styling practices including organized column grouping by data type, consistent naming conventions, and proper transformations. The model sources data from a raw orders table and applies appropriate renaming and type conversions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-style/1-how-we-style-our-dbt-models.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nwith\n\nsource as (\n\n    select * from {{ source('ecom', 'raw_orders') }}\n\n),\n\nrenamed as (\n\n    select\n\n        ----------  ids\n        id as order_id,\n        store_id as location_id,\n        customer as customer_id,\n\n        ---------- strings\n        status as order_status,\n\n        ---------- numerics\n        (order_total / 100.0)::float as order_total,\n        (tax_paid / 100.0)::float as tax_paid,\n\n        ---------- booleans\n        is_fulfilled,\n\n        ---------- dates\n        date(order_date) as ordered_date,\n\n        ---------- timestamps\n        ordered_at\n\n    from source\n\n)\n\nselect * from renamed\n```\n\n----------------------------------------\n\nTITLE: Complete Source Configuration Example\nDESCRIPTION: A comprehensive example of source configuration for a project with custom sources and packages, showing how to selectively enable and disable sources at different levels.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/source-configs.md#2025-04-09_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\nname: jaffle_shop\nconfig-version: 2\n...\nsources:\n  # project names\n  jaffle_shop:\n    +enabled: true\n\n  events:\n    # source names\n    clickstream:\n      # table names\n      pageviews:\n        +enabled: false\n      link_clicks:\n        +enabled: true\n```\n\n----------------------------------------\n\nTITLE: Implementing Incremental Python Model in Snowpark\nDESCRIPTION: This snippet demonstrates how to create an incremental Python model using Snowpark in dbt. It shows how to configure the model, reference upstream data, and filter for new rows based on either the maximum timestamp or a rolling window.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/materializations.md#2025-04-09_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport snowflake.snowpark.functions as F\n\ndef model(dbt, session):\n    dbt.config(materialized = \"incremental\")\n    df = dbt.ref(\"upstream_table\")\n\n    if dbt.is_incremental:\n\n        # only new rows compared to max in current table\n        max_from_this = f\"select max(updated_at) from {dbt.this}\"\n        df = df.filter(df.updated_at >= session.sql(max_from_this).collect()[0][0])\n\n        # or only rows from the past 3 days\n        df = df.filter(df.updated_at >= F.dateadd(\"day\", F.lit(-3), F.current_timestamp()))\n\n    ...\n\n    return df\n```\n\n----------------------------------------\n\nTITLE: Defining Cumulative Metrics in YAML\nDESCRIPTION: This snippet shows how to define cumulative metrics in a YAML file, including parameters for specifying the measure and time window.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metrics-overview.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  - name: wau_rolling_7\n    type: cumulative\n    label: Weekly active users\n    type_params:\n      measure:\n        name: active_users\n        fill_nulls_with: 0\n        join_to_timespine: true\n      cumulative_type_params:\n        window: 7 days\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Snapshot Properties in YAML\nDESCRIPTION: YAML configuration template for defining snapshot properties including name, description, meta information, docs configuration, tests, and column specifications. Supports both pre-1.9 and post-1.9 dbt versions with identical structure.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/snapshot-properties.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsnapshots:\n  - name: <snapshot name>\n    description: <markdown_string>\n    meta: {<dictionary>}\n    docs:\n      show: true | false\n      node_color: <color_id>\n    config:\n      <snapshot_config>: <config_value>\n    tests:\n      - <test>\n      - ...\n    columns:\n      - name: <column name>\n        description: <markdown_string>\n        meta: {<dictionary>}\n        quote: true | false\n        tags: [<string>]\n        tests:\n          - <test>\n          - ...\n      - ...\n\n    - name: ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Incremental Model with Schema Change Handling\nDESCRIPTION: SQL configuration for an incremental model that specifies materialization type, unique key, and schema change behavior set to fail on schema changes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/incremental-models.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        materialized='incremental',\n        unique_key='date_day',\n        on_schema_change='fail'\n    )\n}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Post-Hook for Model in SQL (dbt)\nDESCRIPTION: Example of using a config block to set a post-hook that runs an ALTER TABLE statement after building a model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/hooks-operations.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    post_hook=[\n      \"alter table {{ this }} ...\"\n    ]\n) }}\n```\n\n----------------------------------------\n\nTITLE: Creating Customer Analysis SQL Model in dbt\nDESCRIPTION: A SQL model that joins customer and order data to analyze customer ordering patterns. It calculates first order date, most recent order date, and total number of orders per customer.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/sql-models.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nwith customer_orders as (\n    select\n        customer_id,\n        min(order_date) as first_order_date,\n        max(order_date) as most_recent_order_date,\n        count(order_id) as number_of_orders\n\n    from jaffle_shop.orders\n\n    group by 1\n)\n\nselect\n    customers.customer_id,\n    customers.first_name,\n    customers.last_name,\n    customer_orders.first_order_date,\n    customer_orders.most_recent_order_date,\n    coalesce(customer_orders.number_of_orders, 0) as number_of_orders\n\nfrom jaffle_shop.customers\n\nleft join customer_orders using (customer_id)\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Models in SQL Files with Config Blocks (Pre-1.9)\nDESCRIPTION: Example of configuring a dbt model directly in its SQL file using a config() Jinja macro. This shows all configuration options that can be applied at the individual model level for versions before 1.9.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/model-configs.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    [enabled]=true | false,\n    [tags]=\"<string>\" | [\"<string>\"],\n    [pre_hook]=\"<sql-statement>\" | [\"<sql-statement>\"],\n    [post_hook]=\"<sql-statement>\" | [\"<sql-statement>\"],\n    [database]=\"<string>\",\n    [schema]=\"<string>\",\n    [alias]=\"<string>\",\n    [persist_docs]={<dict>},\n    [meta]={<dict>},\n    [grants]={<dict>},\n    [contract]={<dictionary>}\n) }}\n```\n\n----------------------------------------\n\nTITLE: CSV Inline Format Unit Test Configuration in YAML\nDESCRIPTION: Demonstrates using inline CSV format for mock data in dbt unit tests. Shows how to specify CSV data directly in the YAML configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/data-formats.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nunit_tests:\n  - name: test_my_model\n    model: my_model\n    given:\n      - input: ref('my_model_a')\n        format: csv\n        rows: |\n          id,name\n          1,gerda\n          2,michelle\n```\n\n----------------------------------------\n\nTITLE: Complete Semantic Model Definition in YAML for dbt\nDESCRIPTION: This YAML snippet provides a complete semantic model definition for orders in dbt. It includes entities, dimensions, and measures, demonstrating two organizational approaches: co-located and parallel sub-folder.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-build-our-metrics/semantic-layer-3-build-semantic-models.md#2025-04-09_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_models:\n  - name: orders\n    defaults:\n      agg_time_dimension: ordered_at\n    description: |\n      Order fact table. This table is at the order grain with one row per order.\n\n    model: ref('stg_orders')\n\n    entities:\n      - name: order_id\n        type: primary\n      - name: location\n        type: foreign\n        expr: location_id\n      - name: customer\n        type: foreign\n        expr: customer_id\n\n    dimensions:\n      - name: ordered_at\n        expr: date_trunc('day', ordered_at)\n        # use date_trunc(ordered_at, DAY) if using BigQuery\n        type: time\n        type_params:\n          time_granularity: day\n      - name: is_large_order\n        type: categorical\n        expr: case when order_total > 50 then true else false end\n\n    measures:\n      - name: order_total\n        description: The total revenue for each order.\n        agg: sum\n      - name: order_count\n        description: The count of individual orders.\n        expr: 1\n        agg: sum\n      - name: tax_paid\n        description: The total tax paid on each order.\n        agg: sum\n```\n\n----------------------------------------\n\nTITLE: Processing Discovery API Results with Python\nDESCRIPTION: This Python script demonstrates how to query the Discovery API, extract node definitions, construct a lineage graph, and assign layers to the graph for visualization.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/discovery-use-cases-and-examples.md#2025-04-09_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport networkx as nx\nimport os\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport requests\nfrom collections import defaultdict\n\n# Write Discovery API query\ngql_query = \"\"\"\nquery Definition($environmentId: BigInt!, $first: Int!) {\n*[ADD QUERY HERE]*\n}\n\n\"\"\"\n\n# Define query variables\nvariables = {\n    \"environmentId\": *[ADD ENV ID HERE]*,\n    \"first\": 500\n}\n\n\n# Query the API\ndef query_discovery_api(auth_token, gql_query, variables):\n    response = requests.post('https://metadata.cloud.getdbt.com/beta/graphql',\n        headers={\"authorization\": \"Bearer \"+auth_token, \"content-type\": \"application/json\"},\n        json={\"query\": gql_query, \"variables\": variables})\n    data = response.json()['data']['environment']\n\n    return data\n\n\n# Extract nodes for graph\ndef extract_node_definitions(api_response):\n    nodes = []\n    node_types = [\"models\", \"sources\", \"seeds\", \"snapshots\", \"exposures\"]  # support for metrics and semanticModels coming soon\n    for node_type in node_types:\n        if node_type in api_response[\"definition\"]:\n            for node_edge in api_response[\"definition\"][node_type][\"edges\"]:\n                node_edge[\"node\"][\"type\"] = node_type\n                nodes.append(node_edge[\"node\"])\n    nodes_df = pd.DataFrame(nodes)\n\t\treturn nodes_df\n\n\n# Construct the graph\ndef create_generic_lineage_graph(nodes_df):\n    G = nx.DiGraph()\n    for _, node in nodes_df.iterrows():\n        G.add_node(node[\"uniqueId\"], name=node[\"name\"], type=node[\"type\"])\n    for _, node in nodes_df.iterrows():\n        if node[\"type\"] not in [\"sources\", \"seeds\"]:\n          for parent in node[\"parents\"]:\n              G.add_edge(parent[\"uniqueId\"], node[\"uniqueId\"])\n    return G\n\n\n# Assign graph layers\ndef assign_layers(G):\n    layers = {}\n    layer_counts = defaultdict(int)\n    for node in nx.topological_sort(G):\n        layer = 0\n        for parent in G.predecessors(node):\n            layer = max(layers[parent] + 1, layer)\n        layers[node] = layer\n        layer_counts[layer] += 1\n    nx.set_node_attributes(G, layers, \"layer\")\n    return layer_counts\n```\n\n----------------------------------------\n\nTITLE: Defining a Generic Data Test in SQL\nDESCRIPTION: This SQL snippet demonstrates how to create a generic 'not_null' test that can be reused across different models and columns. It uses Jinja templating to parameterize the model and column name.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/data-tests.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{% test not_null(model, column_name) %}\n\n    select *\n    from {{ model }}\n    where {{ column_name }} is null\n\n{% endtest %}\n```\n\n----------------------------------------\n\nTITLE: Basic Import CTE Example in SQL\nDESCRIPTION: Demonstrates the proper way to structure an import CTE with column selection and filtering in dbt projects. Shows how to reference other models and apply date filters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-style/2-how-we-style-our-sql.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nwith\n\norders as (\n\n    select\n        order_id,\n        customer_id,\n        order_total,\n        order_date\n\n    from {{ ref('orders') }}\n\n    where order_date >= '2020-01-01'\n\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Individual dbt Model\nDESCRIPTION: SQL model with configuration block that specifies materialization type and schema settings for an individual model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/sql-models.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized=\"view\",\n    schema=\"marketing\"\n) }}\n\nwith customer_orders as ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Version in Snowflake\nDESCRIPTION: Shows how to specify Python version for Snowpark models in Snowflake. Allows selection between Python versions 3.9, 3.10, or 3.11.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/python-models.md#2025-04-09_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef model(dbt, session):\n    dbt.config(\n        materialized = \"table\",\n        python_version=\"3.11\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring DuckDB with Custom Plugins in YAML\nDESCRIPTION: Profile configuration that loads and configures multiple plugins for DuckDB, including Google Sheets integration, SQLAlchemy connectivity, and custom UDF modules. Each plugin has its own configuration options.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/duckdb-configs.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ndefault:\n  outputs:\n    dev:\n      type: duckdb\n      path: /tmp/dbt.duckdb\n      plugins:\n        - module: gsheet\n          config:\n            method: oauth\n        - module: sqlalchemy\n          alias: sql\n          config:\n            connection_url: \"{{ env_var('DBT_ENV_SECRET_SQLALCHEMY_URI') }}\"\n        - module: path.to.custom_udf_module\n```\n\n----------------------------------------\n\nTITLE: Defining Semantic Models with Entities for Joins in YAML\nDESCRIPTION: This YAML snippet defines two semantic models, 'transactions' and 'user_signup', with entities that will be used as join keys. It demonstrates how to set up primary and foreign entities, measures, and dimensions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/join-logic.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_models:\n  - name: transactions\n    entities:\n      - name: id\n        type: primary\n      - name: user\n        type: foreign\n        expr: user_id\n    measures:\n      - name: average_purchase_price\n        agg: avg\n        expr: purchase_price\n  - name: user_signup\n    entities:\n      - name: user\n        type: primary\n        expr: user_id\n    dimensions:\n      - name: type\n        type: categorical\n```\n\n----------------------------------------\n\nTITLE: Using ref Function for Model Dependencies\nDESCRIPTION: Example showing how to use the ref function to create dependencies between models in dbt, referencing upstream staging tables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/sql-models.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nwith customers as (\n\n    select * from {{ ref('stg_customers') }}\n\n),\n\norders as (\n\n    select * from {{ ref('stg_orders') }}\n\n),\n\n...\n```\n\n----------------------------------------\n\nTITLE: Creating Staging Model for Customers in SQL\nDESCRIPTION: This SQL snippet creates a staging model for customers, selecting and renaming relevant columns from the jaffle_shop.customers table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/athena-qs.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    id as customer_id,\n    first_name,\n    last_name\n\nfrom jaffle_shop.customers\n```\n\n----------------------------------------\n\nTITLE: Generating Selective dbt Documentation\nDESCRIPTION: This command generates documentation for selected nodes in a dbt project. It uses the '--select' flag to limit the nodes included in the catalog.json file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/cmd-docs.md#2025-04-09_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndbt docs generate --select +orders\n```\n\n----------------------------------------\n\nTITLE: Complex SQL Query with Multiple CTEs and Joins\nDESCRIPTION: Comprehensive example showing multiple CTEs, joins, conditional logic, and proper formatting for complex SQL queries in dbt, including proper indentation and field organization.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-style/2-how-we-style-our-sql.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nwith\n\nmy_data as (\n\n    select\n        field_1,\n        field_2,\n        field_3,\n        cancellation_date,\n        expiration_date,\n        start_date\n\n    from {{ ref('my_data') }}\n\n),\n\nsome_cte as (\n\n    select\n        id,\n        field_4,\n        field_5\n\n    from {{ ref('some_cte') }}\n\n),\n\nsome_cte_agg as (\n\n    select\n        id,\n        sum(field_4) as total_field_4,\n        max(field_5) as max_field_5\n\n    from some_cte\n\n    group by 1\n\n),\n\njoined as (\n\n    select\n        my_data.field_1,\n        my_data.field_2,\n        my_data.field_3,\n\n        -- use line breaks to visually separate calculations into blocks\n        case\n            when my_data.cancellation_date is null\n                and my_data.expiration_date is not null\n                then expiration_date\n            when my_data.cancellation_date is null\n                then my_data.start_date + 7\n            else my_data.cancellation_date\n        end as cancellation_date,\n\n        some_cte_agg.total_field_4,\n        some_cte_agg.max_field_5\n\n    from my_data\n\n    left join some_cte_agg\n        on my_data.id = some_cte_agg.id\n\n    where my_data.field_1 = 'abc' and\n        (\n            my_data.field_2 = 'def' or\n            my_data.field_2 = 'ghi'\n        )\n\n    having count(*) > 1\n\n)\n\nselect * from joined\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Models in YAML\nDESCRIPTION: This snippet shows how to configure a dbt model using the 'config' property in a YAML file. It allows setting various model-specific configurations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/config.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: <model_name>\n    config:\n      [<model_config>](/reference/model-configs): <config_value>\n      ...\n```\n\n----------------------------------------\n\nTITLE: Saved Query Commands\nDESCRIPTION: Examples of running saved queries in both dbt Cloud and dbt Core environments.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metricflow-commands.md#2025-04-09_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ndbt sl query --saved-query <name> # In dbt Cloud\n\nmf query --saved-query <name> # In dbt Core\n```\n\n----------------------------------------\n\nTITLE: Example dbt Model Configuration in YAML\nDESCRIPTION: A demonstration of properly formatted YAML for a dbt model definition with columns and tests. Shows proper indentation, list formatting, and organization of model metadata including column descriptions and test definitions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-style/5-how-we-style-our-yaml.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: events\n    columns:\n      - name: event_id\n        description: This is a unique identifier for the event\n        tests:\n          - unique\n          - not_null\n\n      - name: event_time\n        description: \"When the event occurred in UTC (eg. 2018-01-01 12:00:00)\"\n        tests:\n          - not_null\n\n      - name: user_id\n        description: The ID of the user who recorded the event\n        tests:\n          - not_null\n          - relationships:\n              to: ref('users')\n              field: id\n```\n\n----------------------------------------\n\nTITLE: Adding Tests and Documentation in dbt\nDESCRIPTION: A markdown file containing steps for implementing tests and documentation in dbt models, including instructions for viewing generated documentation through the Develop interface.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/quickstarts/test-and-document-your-project.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n## Add tests to your models\n\n<Snippet path=\"tutorial-add-tests-to-models\" />\n\n## Document your models\n\n<Snippet path=\"tutorial-document-your-models\" />\n\n3. Click the book icon in the Develop interface to launch documentation in a new tab.\n\n#### FAQs\n\n<FAQ path=\"Docs/long-descriptions\" />\n<FAQ path=\"Docs/sharing-documentation\" />\n```\n\n----------------------------------------\n\nTITLE: Defining a Basic Python Model in dbt\nDESCRIPTION: The basic structure of a Python model in dbt, showing the required model function with dbt and session parameters that returns a DataFrame.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/python-models.md#2025-04-09_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ...\n\ndef model(dbt, session):\n\n    my_sql_model_df = dbt.ref(\"my_sql_model\")\n\n    final_df = ...  # stuff you can't write in SQL!\n\n    return final_df\n```\n\n----------------------------------------\n\nTITLE: Configuring Source Data Tests in DBT\nDESCRIPTION: YAML configuration for defining data tests on source tables and their columns, including test arguments and configurations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/data-tests.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: <source_name>\n    tables:\n    - name: <table_name>\n      tests:\n        - [<test_name>]\n        - [<test_name>]:\n            <argument_name>: <argument_value>\n            [config]:\n              [<test_config>]: <config-value>\n\n      columns:\n        - name: <column_name>\n          tests:\n            - [<test_name>]\n            - [<test_name>]:\n                <argument_name>: <argument_value>\n                [config]:\n                  [<test_config>]: <config-value>\n```\n\n----------------------------------------\n\nTITLE: Defining Constraints in PostgreSQL dbt Model\nDESCRIPTION: This snippet shows how to define constraints in a PostgreSQL dbt model, including not_null, primary_key, and check constraints. It demonstrates the use of the 'contract' config to enforce constraints.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/constraints.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: dim_customers\n    config:\n      contract:\n        enforced: true\n    columns:\n      - name: id\n        data_type: int\n        constraints:\n          - type: not_null\n          - type: primary_key\n          - type: check\n            expression: \"id > 0\"\n      - name: customer_name\n        data_type: text\n      - name: first_transaction_date\n        data_type: date\n```\n\n----------------------------------------\n\nTITLE: Moving dbt_cloud.yml file in Bash\nDESCRIPTION: Example command to move the dbt_cloud.yml file from the Downloads folder to the .dbt directory using the mv command in Bash.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/configure-cloud-cli.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmv ~/Downloads/dbt_cloud.yml ~/.dbt/dbt_cloud.yml\n```\n\n----------------------------------------\n\nTITLE: Configuring Seed Properties in YAML for dbt\nDESCRIPTION: This YAML schema demonstrates how to define seed properties in dbt. It includes options for setting seed names, descriptions, documentation visibility, configs, tests, and detailed column properties. The structure allows for multiple seeds to be defined within a single file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/seed-properties.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nseeds:\n  - name: <string>\n    [description](/reference/resource-properties/description): <markdown_string>\n    [docs](/reference/resource-configs/docs):\n      show: true | false\n      node_color: <color_id> # Use name (such as node_color: purple) or hex code with quotes (such as node_color: \"#cd7f32\")\n    [config](/reference/resource-properties/config):\n      [<seed_config>](/reference/seed-configs): <config_value>\n    [tests](/reference/resource-properties/data-tests):\n      - <test>\n      - ... # declare additional tests\n    columns:\n      - name: <column name>\n        [description](/reference/resource-properties/description): <markdown_string>\n        [meta](/reference/resource-configs/meta): {<dictionary>}\n        [quote](/reference/resource-properties/columns#quote): true | false\n        [tags](/reference/resource-configs/tags): [<string>]\n        [tests](/reference/resource-properties/data-tests):\n          - <test>\n          - ... # declare additional tests\n\n      - name: ... # declare properties of additional columns\n\n  - name: ... # declare properties of additional seeds\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Materializations in DBT Project\nDESCRIPTION: Sets materialization types for different project components - configuring 'jaffle_shop' models as tables and 'example' models as views.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/quickstarts/change-way-model-materialized.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  jaffle_shop:\n    +materialized: table\n    example:\n      +materialized: view\n```\n\n----------------------------------------\n\nTITLE: Configuring DuckDB with Secrets Manager for S3 Access in YAML\nDESCRIPTION: Configuration that utilizes DuckDB's Secrets Manager to store and manage AWS credentials for S3 access. This approach separates credentials from database settings for better security.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/duckdb-configs.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ndefault:\n  outputs:\n    dev:\n      type: duckdb\n      path: /tmp/dbt.duckdb\n      extensions:\n        - httpfs\n        - parquet\n      secrets:\n        - type: s3\n          region: my-aws-region\n          key_id: \"{{ env_var('S3_ACCESS_KEY_ID') }}\"\n          secret: \"{{ env_var('S3_SECRET_ACCESS_KEY') }}\"\n  target: dev\n```\n\n----------------------------------------\n\nTITLE: Defining Semantic Models for Multi-Hop Joins in YAML\nDESCRIPTION: This YAML configuration defines three semantic models (sales, user_signup, and country) to demonstrate multi-hop joins in MetricFlow. It shows how to set up entities, measures, and dimensions across related tables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/join-logic.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_models:\n  - name: sales\n    defaults:\n      agg_time_dimension: first_ordered_at\n    entities:\n      - name: id\n        type: primary\n      - name: user_id\n        type: foreign\n    measures:\n      - name: average_purchase_price\n        agg: avg\n        expr: purchase_price\n    dimensions:\n      - name: metric_time\n        type: time\n        type_params:\n  - name: user_signup\n    entities:\n      - name: user_id\n        type: primary\n      - name: country_id\n        type: unique\n    dimensions:\n      - name: signup_date\n        type: time\n      - name: country_dim\n\n  - name: country\n    entities:\n      - name: country_id\n        type: primary\n    dimensions:\n      - name: country_name\n        type: categorical\n```\n\n----------------------------------------\n\nTITLE: Creating Final Staged Customer Model with Deletion Status in dbt\nDESCRIPTION: Creates a staging model that joins the base customer data with deletion information. This demonstrates joining in separate delete tables pattern by adding an is_deleted flag to the customer records.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-structure/2-staging.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n-- stg_jaffle_shop__customers.sql\n\nwith\n\ncustomers as (\n\n    select * from {{ ref('base_jaffle_shop__customers') }}\n\n),\n\ndeleted_customers as (\n\n    select * from {{ ref('base_jaffle_shop__deleted_customers') }}\n\n),\n\njoin_and_mark_deleted_customers as (\n\n    select\n        customers.*,\n        case\n            when deleted_customers.deleted_at is not null then true\n            else false\n        end as is_deleted\n\n    from customers\n\n    left join deleted_customers on customers.customer_id = deleted_customers.customer_id\n\n)\n\nselect * from join_and_mark_deleted_customers\n```\n\n----------------------------------------\n\nTITLE: Defining Ratio Metric Structure in YAML\nDESCRIPTION: Complete specification for defining ratio metrics in dbt, showing all possible parameters and their structure. This template includes required and optional fields like name, description, type, label, and type_params with numerator and denominator components.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/ratio-metrics.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  - name: The metric name # Required\n    description: the metric description # Optional\n    type: ratio # Required\n    label: String that defines the display value in downstream tools. (such as orders_total or \"orders_total\") #Required\n    type_params: # Required\n      numerator: The name of the metric used for the numerator, or structure of properties # Required\n        name: Name of metric used for the numerator # Required\n        filter: Filter for the numerator # Optional\n        alias: Alias for the numerator # Optional\n      denominator: The name of the metric used for the denominator, or structure of properties # Required\n        name: Name of metric used for the denominator # Required\n        filter: Filter for the denominator # Optional\n        alias: Alias for the denominator # Optional\n```\n\n----------------------------------------\n\nTITLE: Configuring Tests and Documentation for dbt Seeds with YAML Schema File\nDESCRIPTION: This YAML configuration shows how to set up testing and documentation for a seed file named 'country_codes'. It defines the seed description and adds unique and not_null tests for both the country_code and country_name columns.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Tests/testing-seeds.md#2025-04-09_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nversion: 2\n\nseeds:\n  - name: country_codes\n    description: A mapping of two letter country codes to country names\n    columns:\n      - name: country_code\n        tests:\n          - unique\n          - not_null\n      - name: country_name\n        tests:\n          - unique\n          - not_null\n```\n\n----------------------------------------\n\nTITLE: Creating Project-Specific Documentation Overviews\nDESCRIPTION: Example of creating custom overview pages for different dbt packages within a project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/documentation.md#2025-04-09_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n{% docs __dbt_utils__ %}\n# Utility macros\nOur dbt project heavily uses this suite of utility macros, especially:\n- `surrogate_key`\n- `test_equality`\n- `pivot`\n{% enddocs %}\n\n{% docs __snowplow__ %}\n# Snowplow sessionization\nOur organization uses this package of transformations to roll Snowplow events\nup to page views and sessions.\n{% enddocs %}\n```\n\n----------------------------------------\n\nTITLE: Using Whitespace Control in Jinja\nDESCRIPTION: SQL model with Jinja whitespace control (using hyphens in Jinja tags) to produce cleaner compiled SQL without excessive whitespace.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/using-jinja.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n{%- set payment_methods = [\"bank_transfer\", \"credit_card\", \"gift_card\"] -%}\n\nselect\norder_id,\n{%- for payment_method in payment_methods %}\nsum(case when payment_method = '{{payment_method}}' then amount end) as {{payment_method}}_amount\n{%- if not loop.last %},{% endif -%}\n{% endfor %}\nfrom {{ ref('raw_payments') }}\ngroup by 1\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Incremental Materialization in dbt\nDESCRIPTION: Basic configuration for an incremental model in dbt, showing how to set the materialization type to 'incremental' in the config block.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/incremental-models.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{{{\n    config(\n        materialized='incremental'\n    )\n}}\n\nselect ...\n\n```\n\n----------------------------------------\n\nTITLE: Basic Semantic Model YAML Structure\nDESCRIPTION: Defines the basic required structure for a semantic model configuration in YAML format, showing all mandatory and optional components.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/semantic-models.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_models:\n  - name: the_name_of_the_semantic_model\n    description: same as always\n    model: ref('some_model')\n    defaults:\n      agg_time_dimension: dimension_name\n    entities:\n      - see more information in entities\n    measures:\n      - see more information in the measures section\n    dimensions:\n      - see more information in the dimensions section\n    primary_entity: >\n      if the semantic model has no primary entity, then this property is required.\n```\n\n----------------------------------------\n\nTITLE: Configuring and Testing Python Models in YAML\nDESCRIPTION: Example of configuring, documenting, and testing a Python model using a YAML configuration file, showing how to apply materializations, tags, and tests.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/python-models.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: my_python_model\n\n    # Document within the same codebase\n    description: My transformation written in Python\n\n    # Configure in ways that feel intuitive and familiar\n    config:\n      materialized: table\n      tags: ['python']\n\n    # Test the results of my Python transformation\n    columns:\n      - name: id\n        # Standard validation for 'grain' of Python results\n        tests:\n          - unique\n          - not_null\n    tests:\n      # Write your own validation logic (in SQL) for Python results\n      - [custom_generic_test](/best-practices/writing-custom-generic-tests)\n```\n\n----------------------------------------\n\nTITLE: Implementing Derived Metric for Month-over-Month Revenue Growth in dbt YAML\nDESCRIPTION: This snippet creates a derived metric to calculate month-over-month revenue growth. It uses the 'revenue' metric twice, once for the current month and once offset by a month, and calculates the percentage growth.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-build-our-metrics/semantic-layer-5-advanced-metrics.md#2025-04-09_snippet_2\n\nLANGUAGE: yml\nCODE:\n```\n- name: revenue_growth_mom\n  description: \"Percentage growth of revenue compared to 1 month ago. Excluded tax\"\n  type: derived\n  label: Revenue Growth % M/M\n  type_params:\n    expr: (current_revenue - revenue_prev_month) * 100 / revenue_prev_month\n    metrics:\n      - name: revenue\n        alias: current_revenue\n      - name: revenue\n        offset_window: 1 month\n        alias: revenue_prev_month\n```\n\n----------------------------------------\n\nTITLE: Configuring Model and Column Descriptions in YAML\nDESCRIPTION: Example showing how to add descriptions and tests to models and columns in a dbt project using YAML configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/documentation.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: events\n    description: This table contains clickstream events from the marketing website\n\n    columns:\n      - name: event_id\n        description: This is a unique identifier for the event\n        tests:\n          - unique\n          - not_null\n\n      - name: user-id\n        quote: true\n        description: The user who performed the event\n        tests:\n          - not_null\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Append Strategy Macro\nDESCRIPTION: Example of defining a custom macro for the 'append' incremental strategy. This macro returns SQL for appending new data to the target relation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/incremental-strategy.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\n{% macro get_incremental_append_sql(arg_dict) %}\n\n  {% do return(some_custom_macro_with_sql(arg_dict[\"target_relation\"], arg_dict[\"temp_relation\"], arg_dict[\"unique_key\"], arg_dict[\"dest_columns\"], arg_dict[\"incremental_predicates\"])) %}\n\n{% endmacro %}\n\n\n{% macro some_custom_macro_with_sql(target_relation, temp_relation, unique_key, dest_columns, incremental_predicates) %}\n\n    {%- set dest_cols_csv = get_quoted_csv(dest_columns | map(attribute=\"name\")) -%}\n\n    insert into {{ target_relation }} ({{ dest_cols_csv }})\n    (\n        select {{ dest_cols_csv }}\n        from {{ temp_relation }}\n    )\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Models in Project YAML File (1.9+)\nDESCRIPTION: Example of configuring dbt models in the dbt_project.yml file for version 1.9 and later. This includes all previous configuration options plus the new event_time parameter which designates a time field for incremental models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/model-configs.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  [<resource-path>]:\n    [+][enabled]: true | false\n    [+][tags]: <string> | [<string>]\n    [+][pre-hook]: <sql-statement> | [<sql-statement>]\n    [+][post-hook]: <sql-statement> | [<sql-statement>]\n    [+][database]: <string>\n    [+][schema]: <string>\n    [+][alias]: <string>\n    [+][persist_docs]: <dict>\n    [+][full_refresh]: <boolean>\n    [+][meta]: {<dictionary>}\n    [+][grants]: {<dictionary>}\n    [+][contract]: {<dictionary>}\n    [+][event_time]: my_time_field\n```\n\n----------------------------------------\n\nTITLE: Configuring Query Tags in dbt Project YAML for Snowflake\nDESCRIPTION: Project-level configuration to set query tags for Snowflake queries. Query tags are useful for later searching and analysis in Snowflake's QUERY_HISTORY view.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snowflake-configs.md#2025-04-09_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  [<resource-path>]:\n    +query_tag: dbt_special\n```\n\n----------------------------------------\n\nTITLE: Configuring Snowflake User/Password Authentication in dbt (v1.9 and later)\nDESCRIPTION: This YAML configuration sets up basic user/password authentication for Snowflake in dbt versions 1.9 and later. It includes updated default behavior for 'reuse_connections' based on 'client_session_keep_alive' setting.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/snowflake-setup.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmy-snowflake-db:\n  target: dev\n  outputs:\n    dev:\n      type: snowflake\n      account: [account id]\n\n      # User/password auth\n      user: [username]\n      password: [password]\n\n      role: [user role]\n      database: [database name]\n      warehouse: [warehouse name]\n      schema: [dbt schema]\n      threads: [1 or more]\n      client_session_keep_alive: False\n      query_tag: [anything]\n\n      # optional\n      connect_retries: 0 # default 0\n      connect_timeout: 10 # default: 10\n      retry_on_database_errors: False # default: false\n      retry_all: False  # default: false\n      reuse_connections: True # default: True if client_session_keep_alive is False, otherwise None\n```\n\n----------------------------------------\n\nTITLE: Configuring a Timestamp Strategy Snapshot in YAML\nDESCRIPTION: YAML configuration for a snapshot that uses the timestamp strategy. This example specifies the schema, unique key, strategy type, and the column to use for detecting changes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/snapshots.md#2025-04-09_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  - name: orders_snapshot_timestamp\n    relation: source('jaffle_shop', 'orders')\n    config:\n      schema: snapshots\n      unique_key: id\n      strategy: timestamp\n      updated_at: updated_at\n```\n\n----------------------------------------\n\nTITLE: Databricks Model Configuration in YAML\nDESCRIPTION: YAML configuration example for setting up a Databricks Python model with job cluster submission method.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/python-models.md#2025-04-09_snippet_20\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\nmodels:\n  - name: my_python_model\n    config:\n      submission_method: job_cluster\n      job_cluster_config:\n        spark_version: ...\n        node_type_id: ...\n```\n\n----------------------------------------\n\nTITLE: Implementing Staging Model Transformations in SQL\nDESCRIPTION: A standard staging model example that demonstrates common transformation patterns including renaming, type casting, basic computations, and categorization. The model uses CTEs to organize source data extraction and transformations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-structure/2-staging.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n-- stg_stripe__payments.sql\n\nwith\n\nsource as (\n\n    select * from {{ source('stripe','payment') }}\n\n),\n\nrenamed as (\n\n    select\n        -- ids\n        id as payment_id,\n        orderid as order_id,\n\n        -- strings\n        paymentmethod as payment_method,\n        case\n            when payment_method in ('stripe', 'paypal', 'credit_card', 'gift_card') then 'credit'\n            else 'cash'\n        end as payment_type,\n        status,\n\n        -- numerics\n        amount as amount_cents,\n        amount / 100.0 as amount,\n\n        -- booleans\n        case\n            when status = 'successful' then true\n            else false\n        end as is_completed_payment,\n\n        -- dates\n        date_trunc('day', created) as created_date,\n\n        -- timestamps\n        created::timestamp_ltz as created_at\n\n    from source\n\n)\n\nselect * from renamed\n```\n\n----------------------------------------\n\nTITLE: Declaring Sources in YAML for dbt\nDESCRIPTION: This YAML snippet shows how to declare sources in dbt, specifying the source name, database, schema, and tables. It demonstrates defining multiple sources and their respective tables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/sources.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: jaffle_shop\n    database: raw  \n    schema: jaffle_shop  \n    tables:\n      - name: orders\n      - name: customers\n\n  - name: stripe\n    tables:\n      - name: payments\n```\n\n----------------------------------------\n\nTITLE: Configuring Materialized Views with Config Block\nDESCRIPTION: SQL model configuration for Redshift materialized views using Jinja config blocks to specify materialization type, distribution style, sort keys, and refresh behavior.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/redshift-configs.md#2025-04-09_snippet_5\n\nLANGUAGE: jinja\nCODE:\n```\n{{ config(\n    [materialized]=\"materialized_view\",\n    [on_configuration_change]=\"apply\" | \"continue\" | \"fail\",\n    [dist]=\"all\" | \"auto\" | \"even\" | \"<field-name>\",\n    [sort]=[\"<field-name>\"],\n    [sort_type]=\"auto\" | \"compound\" | \"interleaved\",\n    [auto_refresh]=true | false,\n    [backup]=true | false,\n) }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Database for Snapshots in dbt_project.yml\nDESCRIPTION: This snippet shows how to specify a custom database for snapshots in your dbt_project.yml file. This configuration will store the snapshot in the 'snapshots' database rather than the default target database.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/database.md#2025-04-09_snippet_2\n\nLANGUAGE: yml\nCODE:\n```\nsnapshots:\n  your_project:\n    your_snapshot:\n      +database: snapshots\n```\n\n----------------------------------------\n\nTITLE: Implementing a Conversion Metric in YAML\nDESCRIPTION: This YAML snippet demonstrates how to define a specific conversion metric for measuring the conversion rate from website visits to purchases within a 7-day window. It includes base and conversion measures, entity, time window, and a filter.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/conversion-metrics.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- name: visit_to_buy_conversion_rate_7d\n  description: \"Conversion rate from visiting to transaction in 7 days\"\n  type: conversion\n  label: Visit to buy conversion rate (7-day window)\n  type_params:\n    conversion_type_params:\n      base_measure:\n        name: visits\n        fill_nulls_with: 0\n        filter: {{ Dimension('visits__referrer_id') }} = 'facebook'\n      conversion_measure:\n        name: sellers\n      entity: user\n      window: 7 days\n```\n\n----------------------------------------\n\nTITLE: Configuring Source Freshness in YAML for dbt\nDESCRIPTION: This YAML snippet shows how to configure source freshness checks in dbt. It includes setting warning and error thresholds, specifying the loaded_at_field, and demonstrating hierarchical application of freshness configs.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/sources.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: jaffle_shop\n    database: raw\n    freshness: # default freshness\n      warn_after: {count: 12, period: hour}\n      error_after: {count: 24, period: hour}\n    loaded_at_field: _etl_loaded_at\n\n    tables:\n      - name: orders\n        freshness: # make this a little more strict\n          warn_after: {count: 6, period: hour}\n          error_after: {count: 12, period: hour}\n\n      - name: customers # this inherits the default freshness defined in the jaffle_shop source block at the beginning\n\n\n      - name: product_skus\n        freshness: null # do not check freshness for this table\n```\n\n----------------------------------------\n\nTITLE: Executing dbt Build Command in Production Job\nDESCRIPTION: The command used to build dbt models in a production environment. This command executes all models, tests, and documentation generation in the project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/quickstarts/schedule-a-job.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndbt build\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Tests in schema.yml\nDESCRIPTION: YAML configuration that defines test specifications for dbt models including unique constraints, null checks, accepted values, and relationship validations. The configuration covers multiple models (customers, stg_customers, stg_orders) with various test types.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/tutorial-add-tests-to-models.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: customers\n    columns:\n      - name: customer_id\n        tests:\n          - unique\n          - not_null\n\n  - name: stg_customers\n    columns:\n      - name: customer_id\n        tests:\n          - unique\n          - not_null\n\n  - name: stg_orders\n    columns:\n      - name: order_id\n        tests:\n          - unique\n          - not_null\n      - name: status\n        tests:\n          - accepted_values:\n              values: ['placed', 'shipped', 'completed', 'return_pending', 'returned']\n      - name: customer_id\n        tests:\n          - not_null\n          - relationships:\n              to: ref('stg_customers')\n              field: customer_id\n```\n\n----------------------------------------\n\nTITLE: Testing and Documenting Sources in YAML for dbt\nDESCRIPTION: This YAML snippet demonstrates how to add tests and documentation to sources in dbt. It includes descriptions for sources and tables, as well as column-level tests and descriptions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/sources.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: jaffle_shop\n    description: This is a replica of the Postgres database used by our app\n    tables:\n      - name: orders\n        description: >\n          One record per order. Includes cancelled and deleted orders.\n        columns:\n          - name: id\n            description: Primary key of the orders table\n            tests:\n              - unique\n              - not_null\n          - name: status\n            description: Note that the status can change over time\n\n      - name: ...\n\n  - name: ...\n```\n\n----------------------------------------\n\nTITLE: Granting Snowflake Permissions to dbt Role\nDESCRIPTION: SQL commands to grant necessary permissions to a Snowflake role for accessing raw data and creating objects in the analytics database. These commands are used to troubleshoot permission issues when running dbt commands.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/snowflake-qs.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\ngrant all on database raw to role snowflake_role_name;\ngrant all on database analytics to role snowflake_role_name;\n\ngrant all on schema raw.jaffle_shop to role snowflake_role_name;\ngrant all on schema raw.stripe to role snowflake_role_name;\n\ngrant all on all tables in database raw to role snowflake_role_name;\ngrant all on future tables in database raw to role snowflake_role_name;\n```\n\n----------------------------------------\n\nTITLE: Creating a Customers Model in dbt for Azure Synapse Analytics\nDESCRIPTION: SQL code for a dbt model that transforms raw customer and order data into a denormalized customers table with order statistics. The model joins customer information with aggregated order data to provide a comprehensive view of customer ordering patterns.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/azure-synapse-analytics-qs.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nwith customers as (\n\nselect\n    ID as customer_id,\n    FIRST_NAME as first_name,\n    LAST_NAME as last_name\n\nfrom dbo.customers\n),\n\norders as (\n\n    select\n        ID as order_id,\n        USER_ID as customer_id,\n        ORDER_DATE as order_date,\n        STATUS as status\n\n    from dbo.orders\n),\n\ncustomer_orders as (\n\n    select\n        customer_id,\n\n        min(order_date) as first_order_date,\n        max(order_date) as most_recent_order_date,\n        count(order_id) as number_of_orders\n\n    from orders\n\n    group by customer_id\n),\n\nfinal as (\n\n    select\n        customers.customer_id,\n        customers.first_name,\n        customers.last_name,\n        customer_orders.first_order_date,\n        customer_orders.most_recent_order_date,\n        coalesce(customer_orders.number_of_orders, 0) as number_of_orders\n\n    from customers\n\n    left join customer_orders on customers.customer_id = customer_orders.customer_id\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Writing Tests for dbt Models\nDESCRIPTION: This YAML configuration defines tests for the dbt model, ensuring data integrity. It shows how to implement data quality checks in the dbt approach.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-07-19-migrating-from-stored-procs.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n \nmodels:\n  - name: stg_orders\n    columns:\n      - name: orderid\n        tests:\n          - unique\n          - not_null\n```\n\n----------------------------------------\n\nTITLE: Displaying Project File Structure in Shell\nDESCRIPTION: Shows the complete file tree of the example Jaffle Shop dbt project, illustrating the recommended folder structure and file organization for staging, intermediate, and mart layers.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-structure/1-guide-overview.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\njaffle_shop\n README.md\n analyses\n seeds\n    employees.csv\n dbt_project.yml\n macros\n    cents_to_dollars.sql\n models\n    intermediate\n       finance\n           _int_finance__models.yml\n           int_payments_pivoted_to_orders.sql\n    marts\n       finance\n          _finance__models.yml\n          orders.sql\n          payments.sql\n       marketing\n           _marketing__models.yml\n           customers.sql\n    staging\n       jaffle_shop\n          _jaffle_shop__docs.md\n          _jaffle_shop__models.yml\n          _jaffle_shop__sources.yml\n          base\n             base_jaffle_shop__customers.sql\n             base_jaffle_shop__deleted_customers.sql\n          stg_jaffle_shop__customers.sql\n          stg_jaffle_shop__orders.sql\n       stripe\n           _stripe__models.yml\n           _stripe__sources.yml\n           stg_stripe__payments.sql\n    utilities\n        all_dates.sql\n packages.yml\n snapshots\n tests\n     assert_positive_value_for_total_amount.sql\n```\n\n----------------------------------------\n\nTITLE: Configuring Enabled Models in dbt SQL\nDESCRIPTION: Example of enabling or disabling a specific model in its SQL file using the config block. This configuration applies only to the current model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/enabled.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n  enabled=true | false\n) }}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Example Source Configuration for Jaffle Shop\nDESCRIPTION: A practical example of source configuration for a 'jaffle_shop' source with orders and customers tables, demonstrating how to set database connections, schema information, and apply tests to columns.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/source-properties.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: jaffle_shop\n    database: raw\n    schema: public\n    loader: emr # informational only (free text)\n    loaded_at_field: _loaded_at # configure for all sources\n\n    # meta fields are rendered in auto-generated documentation\n    meta:\n      contains_pii: true\n      owner: \"@alice\"\n\n    # Add tags to this source\n    tags:\n      - ecom\n      - pii\n\n    quoting:\n      database: false\n      schema: false\n      identifier: false\n\n    tables:\n      - name: orders\n        identifier: Orders_\n        loaded_at_field: updated_at # override source defaults\n        columns:\n          - name: id\n            tests:\n              - unique\n\n          - name: price_in_usd\n            tests:\n              - not_null\n\n      - name: customers\n        quoting:\n          identifier: true # override source defaults\n        columns:\n            tests:\n              - unique\n```\n\n----------------------------------------\n\nTITLE: Configuring Materializations in dbt_project.yml\nDESCRIPTION: This example demonstrates how to configure materializations for different model directories in the dbt_project.yml file. The configuration materializes all models in the 'events' directory as tables while allowing 'csvs' directory models to use the default view materialization.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/materializations.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# The following dbt_project.yml configures a project that looks like this:\n# .\n#  models\n#      csvs\n#         employees.sql\n#         goals.sql\n#      events\n#          stg_event_log.sql\n#          stg_event_sessions.sql\n\nname: my_project\nversion: 1.0.0\nconfig-version: 2\n\nmodels:\n  my_project:\n    events:\n      # materialize all models in models/events as tables\n      +materialized: table\n    csvs:\n      # this is redundant, and does not need to be set\n      +materialized: view\n```\n\n----------------------------------------\n\nTITLE: Basic SQL RIGHT JOIN Syntax\nDESCRIPTION: The basic syntax for creating a RIGHT JOIN in SQL. Shows how to join two tables based on a common ID field.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/joins/sql-right-join.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    <fields>\nfrom <table_1> as t1\nright join <table_2> as t2\non t1.id = t2.id\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Directories in dbt_project.yml\nDESCRIPTION: This snippet demonstrates how to configure models by directory structure in the dbt_project.yml file. It shows how to namespace configurations to your project and apply different materializations to models in different directories.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/model-configs.md#2025-04-09_snippet_9\n\nLANGUAGE: yml\nCODE:\n```\nname: dbt_labs\n\nmodels:\n  # Be sure to namespace your model configs to your project name\n  dbt_labs:\n\n    # This configures models found in models/events/\n    events:\n      +enabled: true\n      +materialized: view\n\n      # This configures models found in models/events/base\n      # These models will be ephemeral, as the config above is overridden\n      base:\n        +materialized: ephemeral\n\n      ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Model Properties in BigQuery using YAML\nDESCRIPTION: Example of setting configuration parameters for a Python model in BigQuery using YAML configuration, specifying the model name and submission method.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/python-models.md#2025-04-09_snippet_23\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\nmodels:\n  - name: my_python_model\n    config:\n      submission_method: serverless\n```\n\n----------------------------------------\n\nTITLE: Check Strategy with Updated_at Configuration\nDESCRIPTION: Example showing how to configure check strategy with an updated_at timestamp column for more precise change tracking.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/snapshots.md#2025-04-09_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  - name: orders_snapshot\n    relation: ref('stg_orders')\n    config:\n      schema: snapshots\n      unique_key: order_id\n      strategy: check\n      check_cols:\n        - status\n        - is_cancelled\n      updated_at: updated_at\n```\n\n----------------------------------------\n\nTITLE: Installing psycopg2 Instead of psycopg2-binary for dbt-postgres\nDESCRIPTION: Bash commands to replace the default psycopg2-binary with psycopg2 for production environments. This script installs dbt-postgres, uninstalls psycopg2-binary, and installs the equivalent version of psycopg2.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/postgres-setup.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install dbt-postgres\nif [[ $(pip show psycopg2-binary) ]]; then\n    PSYCOPG2_VERSION=$(pip show psycopg2-binary | grep Version | cut -d \" \" -f 2)\n    pip uninstall -y psycopg2-binary && pip install psycopg2==$PSYCOPG2_VERSION\nfi\n```\n\n----------------------------------------\n\nTITLE: Adding Description to Singular Data Test\nDESCRIPTION: This snippet demonstrates how to add a description to a singular data test that checks for non-negative values in the 'payments' model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/description.md#2025-04-09_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\ndata_tests:\n  - name: assert_total_payment_amount_is_positive\n    description: >\n      Refunds have a negative amount, so the total amount should always be >= 0.\n      Therefore return records where total amount < 0 to make the test fail.\n```\n\n----------------------------------------\n\nTITLE: Defining sources in YAML configuration\nDESCRIPTION: Example YAML configuration defining sources for a dbt project. The configuration defines a 'jaffle_shop' source with 'customers' and 'orders' tables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/source.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: jaffle_shop # this is the source_name\n    database: raw\n\n    tables:\n      - name: customers # this is the table_name\n      - name: orders\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Project Settings in YAML\nDESCRIPTION: This YAML snippet demonstrates the structure of a dbt_project.yml file, including project name, version, profile, and model configurations. It shows how to set default materialization strategies and configure specific models or directories.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/resource-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nname: 'jaffle_shop'\nversion: '1.0.0'\nconfig-version: 2\nprofile: 'jaffle_shop'\n\nmodel-paths: [\"models\"]\nanalysis-paths: [\"analyses\"]\ntest-paths: [\"tests\"]\nseed-paths: [\"seeds\"]\nmacro-paths: [\"macros\"]\nsnapshot-paths: [\"snapshots\"]\n\ntarget-path: \"target\"\nclean-targets: [\"target\", \"dbt_packages\"]\n\nmodels:\n  jaffle_shop:\n    +materialized: table\n    staging:\n      +materialized: view\n\nvars:\n  my_first_variable: True\n  my_second_variable: 2020\n```\n\n----------------------------------------\n\nTITLE: Basic ref Function Usage in SQL\nDESCRIPTION: The basic syntax for using the ref function in dbt to reference another node. This creates a dependency and returns a Relation object that compiles to the full database object name.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/ref.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect * from {{ ref(\"node_name\") }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Merge Update Columns in SQL Model\nDESCRIPTION: Example of using the 'merge_update_columns' config with the 'merge' strategy. This configuration specifies which columns should be updated during the merge operation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/incremental-strategy.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(\n    materialized = 'incremental',\n    unique_key = 'id',\n    merge_update_columns = ['email', 'ip_address'],\n    ...\n  )\n}}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Primary Entity in MetricFlow YAML\nDESCRIPTION: This snippet demonstrates how to define a primary entity in a MetricFlow semantic model configuration. It includes settings for measures, aggregation time dimension, and the primary entity declaration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/semantic-models.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_model:\n  name: bookings_monthly_source\n  description: bookings_monthly_source\n  defaults:\n    agg_time_dimension: ds\n  model: ref('bookings_monthly_source')\n  measures:\n    - name: bookings_monthly\n      agg: sum\n      create_metric: true\n  primary_entity: booking_id\n```\n\n----------------------------------------\n\nTITLE: Example of Dimensions in dbt Semantic Layer Model\nDESCRIPTION: This snippet demonstrates how dimensions are used within a semantic model definition, including time and categorical dimensions with various parameters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/dimensions.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_models:\n  - name: transactions\n    description: A record for every transaction that takes place. Carts are considered multiple transactions for each SKU. \n    model: {{ ref('fact_transactions') }}\n    defaults:\n      agg_time_dimension: order_date\n# --- entities --- \n  entities: \n    - name: transaction\n      type: primary\n      ...\n# --- measures --- \n  measures: \n      ... \n# --- dimensions ---\n  dimensions:\n    - name: order_date\n      type: time\n      type_params:\n        time_granularity: day\n      label: \"Date of transaction\" # Recommend adding a label to provide more context to users consuming the data\n      config: \n        meta:\n          data_owner: \"Finance team\"\n      expr: ts\n    - name: is_bulk\n      type: categorical\n      expr: case when quantity > 10 then true else false end\n    - name: type\n      type: categorical\n```\n\n----------------------------------------\n\nTITLE: Configuring Materialized Views in dbt_project.yml\nDESCRIPTION: Sets various configurations for materialized views in the dbt_project.yml file. This includes options for clustering, partitioning, auto-refresh, and other BigQuery-specific settings.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/bigquery-configs.md#2025-04-09_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  [<resource-path>]:\n    [+]materialized: materialized_view\n    [+]on_configuration_change: apply | continue | fail\n    [+]cluster_by: <field-name> | [<field-name>]\n    [+]partition_by:\n      - field: <field-name>\n      - data_type: timestamp | date | datetime | int64\n      - granularity: hour | day | month | year\n        # only if `data_type` is 'int64'\n      - range:\n        - start: <integer>\n        - end: <integer>\n        - interval: <integer>\n    [+]enable_refresh: true | false\n    [+]refresh_interval_minutes: <float>\n    [+]max_staleness: <interval>\n    [+]description: <string>\n    [+]labels: {<label-name>: <label-value>}\n    [+]hours_to_expiration: <integer>\n    [+]kms_key_name: <path-to-key>\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Database for Snapshots in YAML Config File\nDESCRIPTION: This snippet demonstrates how to configure a custom database for snapshots using a snapshot_name.yml file. This approach uses the version 2 YAML schema to define snapshot configurations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/database.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsnapshots:\n  - name: snapshot_name\n    [config](/reference/resource-properties/config):\n      database: snapshots\n```\n\n----------------------------------------\n\nTITLE: Importing BreakingChanges Component in Markdown\nDESCRIPTION: This snippet imports a BreakingChanges component from a file and uses it to display information about breaking changes in dbt contracts.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/govern/model-contracts.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\nimport BreakingChanges from '/snippets/_versions-contracts.md';\n\n<BreakingChanges \nvalue=\"Removing a contracted model by deleting, renaming, or disabling it (dbt v1.9 or higher).\"\nvalue2=\"versioned models will raise an error. unversioned models will raise a warning.\"\n/>\n```\n\n----------------------------------------\n\nTITLE: Configuring Materializations in SQL Model Files\nDESCRIPTION: This snippet shows how to configure materialization directly inside SQL model files using the config() macro. The example configures a table materialization with Redshift-specific sort and distribution keys.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/materializations.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(materialized='table', sort='timestamp', dist='user_id') }}\n\nselect *\nfrom ...\n```\n\n----------------------------------------\n\nTITLE: Defining Derived Metrics in YAML\nDESCRIPTION: This snippet demonstrates how to define derived metrics in a YAML file, including the expression and referenced metrics.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metrics-overview.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  - name: order_gross_profit\n    description: Gross profit from each order.\n    type: derived\n    label: Order gross profit\n    type_params:\n      expr: revenue - cost\n      metrics:\n        - name: order_total\n          alias: revenue\n        - name: order_cost\n          alias: cost\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Models in SQL Config Block\nDESCRIPTION: This snippet illustrates how to configure dbt models directly in the SQL file using a config block. It provides options for materialization, SQL headers, configuration change handling, and unique keys. This method allows for model-specific configurations within the SQL file itself.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/model-configs.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    [materialized]=\"<materialization_name>\",\n    [sql_header]=\"<string>\"\n    [on_configuration_change]: apply | continue | fail # Only for materialized views for supported adapters\n    [unique_key]='column_name_or_expression'\n    [batch_size]='day' | 'hour' | 'month' | 'year'\n    [begin]=\"<ISO formatted date or datetime (like, \\\"2024-01-15T12:00:00Z\\\")\"\n    [lookback]= <integer>\n    [concurrent_batches]= true | false\n) }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Incremental Materialization in Doris/SelectDB (Config Block)\nDESCRIPTION: Creates an incremental Doris table with 'unique' model type using config block approach. Defines unique keys, partitioning strategy, distribution parameters, and custom properties within the model file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/doris-configs.md#2025-04-09_snippet_5\n\nLANGUAGE: jinja\nCODE:\n```\n{{ config(\n    materialized = \"incremental\",\n    unique_key = [ \"<column-name>\", ... ],\n    partition_by = [ \"<column-name>\", ... ],\n    partition_type = \"<engine-type>\",\n    partition_by_init = [\"<pertition-init>\", ... ]\n    distributed_by = [ \"<column-name>\", ... ],\n    buckets = \"int\",\n    properties = {\"<key>\":\"<value>\",...}\n      ...\n    ]\n) }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Project-wide Indexes\nDESCRIPTION: Shows how to configure indexes for multiple models at once using the project configuration file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/postgres-configs.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  project_name:\n    subdirectory:\n      +indexes:\n        - columns: ['column_a']\n          type: hash\n```\n\n----------------------------------------\n\nTITLE: Attempting to Reference Private Group Model\nDESCRIPTION: Shows an example of attempting to reference a private model from another group, which will result in an error.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/groups.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nselect * from {{ ref('finance_private_model') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Macro Properties in YAML for dbt\nDESCRIPTION: This YAML snippet demonstrates the structure for declaring macro properties in dbt. It includes fields for macro name, description, docs visibility, metadata, and argument details. The configuration can be placed in any file named 'properties.yml' or similar, located in the macros directory.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/macro-properties.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmacros:\n  - name: <macro name>\n    description: <markdown_string>\n    docs:\n      show: true | false\n    meta: {<dictionary>}\n    arguments:\n      - name: <arg name>\n        type: <string>\n        description: <markdown_string>\n      - ... # declare properties of additional arguments\n\n  - name: ... # declare properties of additional macros\n```\n\n----------------------------------------\n\nTITLE: Debugging Test Failures with dbt show\nDESCRIPTION: Example of using dbt show to debug a failing uniqueness test. It demonstrates how to quickly preview test failures in the terminal to identify duplicate values in a specific column.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/show.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ dbt build -s \"my_model_with_duplicates\"\n13:22:47 .0\n...\n13:22:48 Completed with 1 error and 0 warnings:\n13:22:48\n13:22:48 Failure in test unique_my_model_with_duplicates (models/schema.yml)\n13:22:48   Got 1 result, configured to fail if not 0\n13:22:48\n13:22:48   compiled code at target/compiled/my_dbt_project/models/schema.yml/unique_my_model_with_duplicates_id.sql\n13:22:48\n13:22:48 Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2\n\n$ dbt show -s \"unique_my_model_with_duplicates_id\"\n13:22:53 Running with dbt=1.5.0\n13:22:53 Found 4 models, 2 tests, 0 snapshots, 0 analyses, 309 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups\n13:22:53\n13:22:53 Concurrency: 5 threads (target='dev')\n13:22:53\n13:22:53 Previewing node 'unique_my_model_with_duplicates_id':\n| unique_field | n_records |\n| ------------ | --------- |\n|            1 |         2 |\n```\n\n----------------------------------------\n\nTITLE: Basic Derived Metrics Structure in YAML\nDESCRIPTION: Basic structure and required parameters for defining derived metrics in MetricFlow. Shows the complete specification including name, description, type, label, and type_params fields.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/derived-metrics.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  - name: the metric name # Required\n    description: the metric description # Optional\n    type: derived # Required\n    label: The value that will be displayed in downstream tools #Required\n    type_params: # Required\n      expr: the derived expression # Required\n      metrics: # The list of metrics used in the derived metrics # Required\n        - name: the name of the metrics. must reference a metric you have already defined # Required\n          alias: optional alias for the metric that you can use in the expr # Optional\n          filter: optional filter to apply to the metric # Optional\n          offset_window: set the period for the offset window, such as 1 month. This will return the value of the metric one month from the metric time. # Optional\n```\n\n----------------------------------------\n\nTITLE: Casting Column Types in SQL using '::' Syntax\nDESCRIPTION: This SQL snippet shows an alternative syntax for casting column types using the '::' operator, which is supported in some modern data warehouses like Redshift, Snowflake, and Postgres. It casts 'order_id' as an integer and 'order_price' as a numeric with 6 total digits and 2 decimal places.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Project/define-a-column-type.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    order_id::integer,\n    order_price::numeric(6,2) -- you might find this in Redshift, Snowflake, and Postgres\nfrom {{ ref('stg_orders') }}\n```\n\n----------------------------------------\n\nTITLE: Defining Test Class and Fixtures for dbt Adapter Testing in Python\nDESCRIPTION: This snippet shows how to create a test class with fixtures for configuring a dbt project, including project configuration, seeds, and models. It demonstrates the structure for setting up a test environment for dbt adapter plugins.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# class must begin with 'Test'\nclass TestExample:\n    \"\"\"\n    Methods in this class will be of two types:\n    1. Fixtures defining the dbt \"project\" for this test case.\n       These are scoped to the class, and reused for all tests in the class.\n    2. Actual tests, whose names begin with 'test_'.\n       These define sequences of dbt commands and 'assert' statements.\n    \"\"\"\n    \n    # configuration in dbt_project.yml\n    @pytest.fixture(scope=\"class\")\n    def project_config_update(self):\n        return {\n          \"name\": \"example\",\n          \"models\": {\"+materialized\": \"view\"}\n        }\n\n    # everything that goes in the \"seeds\" directory\n    @pytest.fixture(scope=\"class\")\n    def seeds(self):\n        return {\n            \"my_seed.csv\": my_seed_csv,\n        }\n\n    # everything that goes in the \"models\" directory\n    @pytest.fixture(scope=\"class\")\n    def models(self):\n        return {\n            \"my_model.sql\": my_model_sql,\n            \"my_model.yml\": my_model_yml,\n        }\n```\n\n----------------------------------------\n\nTITLE: Configuring Microbatch Model in Postgres\nDESCRIPTION: Example of configuring a microbatch incremental model in Postgres with required configurations including unique_key, event_time, begin date, and batch size. The model selects transaction data from a sales source.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/incremental-microbatch.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized='incremental',\n    incremental_strategy='microbatch',\n    unique_key='sales_id', ## required for dbt-postgres\n    event_time='transaction_date',\n    begin='2023-01-01',\n    batch_size='day'\n) }}\n\nselect\n    sales_id,\n    transaction_date,\n    customer_id,\n    product_id,\n    total_amount\nfrom {{ source('sales', 'transactions') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Database Profiles in YAML\nDESCRIPTION: Example profiles.yml file demonstrating how to configure multiple database targets (dev and prod) with connection details for Postgres.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/connection-profiles.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# example profiles.yml file\njaffle_shop:\n  target: dev\n  outputs:\n    dev:\n      type: postgres\n      host: localhost\n      user: alice\n      password: <password>\n      port: 5432\n      dbname: jaffle_shop\n      schema: dbt_alice\n      threads: 4\n\n    prod:  # additional prod target\n      type: postgres\n      host: prod.db.example.com\n      user: alice\n      password: <prod_password>\n      port: 5432\n      dbname: jaffle_shop\n      schema: analytics\n      threads: 8\n```\n\n----------------------------------------\n\nTITLE: Configuring Model-Specific Grants in SQL\nDESCRIPTION: Shows how to set grants for a specific model within its SQL file using the config block. This example grants select privileges to 'user_c'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/grants.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(grants = {'select': ['user_c']}) }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Snowflake Warehouse for Individual Model\nDESCRIPTION: SQL model configuration showing how to set a specific warehouse size for a single model using config block. Includes example transformation logic for session analysis.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snowflake-configs.md#2025-04-09_snippet_16\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(\n    materialized='table',\n    snowflake_warehouse='EXTRA_LARGE'\n  )\n}}\n\nwith\n\naggregated_page_events as (\n\n    select\n        session_id,\n        min(event_time) as session_start,\n        max(event_time) as session_end,\n        count(*) as count_page_views\n    from {{ source('snowplow', 'event') }}\n    group by 1\n\n),\n\nindex_sessions as (\n\n    select\n        *,\n        row_number() over (\n            partition by session_id\n            order by session_start\n        ) as page_view_in_session_index\n    from aggregated_page_events\n\n)\n\nselect * from index_sessions\n```\n\n----------------------------------------\n\nTITLE: Configuring Microsoft Entra ID Interactive Authentication in profiles.yml for Fabric\nDESCRIPTION: Configuration for Microsoft Fabric connection using Microsoft Entra ID interactive authentication. This method supports Multi-Factor Authentication prompts and requires the ODBC Driver for SQL Server.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/fabric-setup.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: fabric\n      driver: 'ODBC Driver 18 for SQL Server' # (The ODBC Driver installed on your system)\n      server: hostname or IP of your server\n      port: 1433\n      database: exampledb\n      schema: schema_name\n      authentication: ActiveDirectoryInteractive\n      user: bill.gates@microsoft.com\n```\n\n----------------------------------------\n\nTITLE: Generating YAML for Multiple dbt Models using Codegen\nDESCRIPTION: This command generates YAML documentation for multiple models in the activity_based_interest folder using the dbt Codegen package.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-04-generating-dynamic-docs.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndbt run-operation generate_model_yaml --args '{\"model_names\": [ \"activity_based_interest_activated\", \"activity_based_interest_deactivated\", \"activity_based_interest_updated\", \"downgrade_interest_level_for_user\", \"f_activity_based_interest\", \"set_inactive_interest_rate_after_july_1st_in_bec_for_user\", \"set_inactive_interest_rate_from_july_1st_in_bec_for_user\", \"set_interest_levels_from_june_1st_in_bec_for_user\"] }'\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Project Settings\nDESCRIPTION: YAML configuration for a dbt project that specifies materialization settings and schema organization for different model directories.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/sql-models.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nname: jaffle_shop\nconfig-version: 2\n...\n\nmodels:\n  jaffle_shop: # this matches the `name:`` config\n    +materialized: view # this applies to all models in the current project\n    marts:\n      +materialized: table # this applies to all models in the `marts/` directory\n      marketing:\n        +schema: marketing # this applies to all models in the `marts/marketing/`` directory\n```\n\n----------------------------------------\n\nTITLE: Using the source function in SQL\nDESCRIPTION: Basic syntax for referencing a source table using the source function, which takes source_name and table_name as arguments.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/source.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect * from {{ source(\"source_name\", \"table_name\") }}\n```\n\n----------------------------------------\n\nTITLE: Defining Simple Metrics Schema in YAML\nDESCRIPTION: This snippet shows the complete specification for defining simple metrics in YAML format. It includes all possible parameters and their descriptions, demonstrating the structure required for creating simple metrics in dbt's Semantic Layer.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/simple.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  - name: The metric name # Required\n    description: the metric description # Optional\n    type: simple # Required\n    label: The value that will be displayed in downstream tools # Required\n    type_params: # Required\n      measure: \n        name: The name of your measure # Required\n        alias: The alias applied to the measure. # Optional\n        filter: The filter applied to the measure. # Optional\n        fill_nulls_with: Set value instead of null  (such as zero) # Optional\n        join_to_timespine: true/false # Boolean that indicates if the aggregated measure should be joined to the time spine table to fill in missing dates. # Optional\n```\n\n----------------------------------------\n\nTITLE: Defining Conversion Metric Structure in YAML\nDESCRIPTION: This YAML structure outlines the complete specification for defining conversion metrics in dbt. It includes required and optional parameters such as name, description, type, label, and various type-specific parameters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/conversion-metrics.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  - name: The metric name # Required\n    description: The metric description # Optional\n    type: conversion # Required\n    label: YOUR_LABEL # Required\n    type_params: # Required\n      conversion_type_params: # Required\n        entity: ENTITY # Required\n        calculation: CALCULATION_TYPE # Optional. default: conversion_rate. options: conversions(buys) or conversion_rate (buys/visits), and more to come.\n        base_measure: \n          name: The name of the measure # Required\n          fill_nulls_with: Set the value in your metric definition instead of null (such as zero) # Optional\n          join_to_timespine: true/false # Boolean that indicates if the aggregated measure should be joined to the time spine table to fill in missing dates. Default `false`. # Optional\n          filter: The filter used to apply to the base measure. # Optional\n        conversion_measure:\n          name: The name of the measure # Required\n          fill_nulls_with: Set the value in your metric definition instead of null (such as zero) # Optional\n          join_to_timespine: true/false # Boolean that indicates if the aggregated measure should be joined to the time spine table to fill in missing dates. Default `false`. # Optional\n        window: TIME_WINDOW # Optional. default: infinity. window to join the two events. Follows a similar format as time windows elsewhere (such as 7 days)\n        constant_properties: # Optional. List of constant properties default: None\n          - base_property: DIMENSION or ENTITY # Required. A reference to a dimension/entity of the semantic model linked to the base_measure\n            conversion_property: DIMENSION or ENTITY # Same as base above, but to the semantic model of the conversion_measure\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Metric in MetricFlow YAML Configuration\nDESCRIPTION: This YAML configuration defines a simple metric named 'order_total' that sums the total order amount. The configuration includes the metric name, description, type, display label, and the measure it references.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_sl-define-metrics.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  - name: order_total\n    description: Sum of total order amount. Includes tax + revenue.\n    type: simple\n    label: Order Total\n    type_params:\n      measure: order_total\n```\n\n----------------------------------------\n\nTITLE: Creating Staging Model for Customers in dbt\nDESCRIPTION: This SQL snippet creates a staging model for customers, selecting and renaming columns from the customers table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/microsoft-fabric-qs.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    ID as customer_id,\n    FIRST_NAME as first_name,\n    LAST_NAME as last_name\n\nfrom dbo.customers\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Models in Project YAML File (Pre-1.9)\nDESCRIPTION: Example of configuring dbt models in the dbt_project.yml file. This shows the available configuration options for versions before 1.9, including enabling/disabling models, tags, hooks, database settings, schemas, and other model properties.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/model-configs.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  [<resource-path>]:\n    [+][enabled]: true | false\n    [+][tags]: <string> | [<string>]\n    [+][pre-hook]: <sql-statement> | [<sql-statement>]\n    [+][post-hook]: <sql-statement> | [<sql-statement>]\n    [+][database]: <string>\n    [+][schema]: <string>\n    [+][alias]: <string>\n    [+][persist_docs]: <dict>\n    [+][full_refresh]: <boolean>\n    [+][meta]: {<dictionary>}\n    [+][grants]: {<dictionary>}\n    [+][contract]: {<dictionary>}\n```\n\n----------------------------------------\n\nTITLE: Incremental Model Configuration in dbt (SQL)\nDESCRIPTION: This code block shows the configuration for an incremental model in dbt, specifying the materialization, unique key, and incremental strategy.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/migrate-from-stored-procedures.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        materialized='incremental',\n        unique_key='ride_id',\n        incremental_strategy='merge'\n    )\n}}\n```\n\n----------------------------------------\n\nTITLE: Implementing dbt Python Model with Pandas\nDESCRIPTION: Example of a dbt Python model that processes order data using pandas DataFrame operations. The model configures package dependencies, references staging tables, and performs time-based calculations with customer order data. It demonstrates proper formatting according to black standards and includes essential dbt configurations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-style/3-how-we-style-our-python.md#2025-04-09_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\n\ndef model(dbt, session):\n    # set length of time considered a churn\n    pd.Timedelta(days=2)\n\n    dbt.config(enabled=False, materialized=\"table\", packages=[\"pandas==1.5.2\"])\n\n    orders_relation = dbt.ref(\"stg_orders\")\n\n    # converting a DuckDB Python Relation into a pandas DataFrame\n    orders_df = orders_relation.df()\n\n    orders_df.sort_values(by=\"ordered_at\", inplace=True)\n    orders_df[\"previous_order_at\"] = orders_df.groupby(\"customer_id\")[\n        \"ordered_at\"\n    ].shift(1)\n    orders_df[\"next_order_at\"] = orders_df.groupby(\"customer_id\")[\"ordered_at\"].shift(\n        -1\n    )\n    return orders_df\n```\n\n----------------------------------------\n\nTITLE: Using log Function for Jinja Debugging in dbt\nDESCRIPTION: The log function in dbt can be used to print objects to the command line for debugging Jinja templates. This allows you to inspect variables and expressions during template rendering.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Project/debugging-jinja.md#2025-04-09_snippet_2\n\nLANGUAGE: jinja\nCODE:\n```\n{{ log(object_to_debug) }}\n```\n\n----------------------------------------\n\nTITLE: Pinning and Specifying Provider for Private Packages in dbt YAML\nDESCRIPTION: Illustrates how to pin private packages to specific versions and specify the Git provider in the packages.yml file. This is useful for version control and when using multiple Git integrations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/packages.md#2025-04-09_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\npackages:\n  - private: dbt-labs/awesome_repo\n    revision: \"0.9.5\" # Pin to a tag, branch, or complete 40-character commit hash\n  \npackages:\n  - private: dbt-labs/awesome_repo\n    provider: \"github\" # GitHub and Azure are currently supported. GitLab is coming soon.\n```\n\n----------------------------------------\n\nTITLE: Using Custom Append Strategy in SQL Model\nDESCRIPTION: Example of using a custom 'append' incremental strategy in a SQL model. This model configuration specifies the use of the 'append' strategy.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/incremental-strategy.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized=\"incremental\",\n    incremental_strategy=\"append\",\n) }}\n\nselect * from {{ ref(\"some_model\") }}\n```\n\n----------------------------------------\n\nTITLE: DBT Snapshot with Full Configuration (Pre-1.9)\nDESCRIPTION: Complete snapshot configuration example using target_database and target_schema approach for versions before 1.9.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/snapshots.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot orders_snapshot %}\n\n{{\n    config(\n      target_database='analytics',\n      target_schema='snapshots',\n      unique_key='id',\n\n      strategy='timestamp',\n      updated_at='updated_at',\n    )\n}}\n\nselect * from {{ source('jaffle_shop', 'orders') }}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Executing dbt Run Command for Model Training\nDESCRIPTION: This bash command executes the dbt run command to train the logistic regression model defined in the train_test_position Python script.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_32\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --select train_test_position\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Properties Using dbt_project.yml\nDESCRIPTION: An example of using cascading configurations in dbt_project.yml to set default materializations and schemas for different parts of the project. This approach allows for defining default behaviors at the directory level rather than configuring each model individually.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-structure/5-the-rest-of-the-project.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n-- dbt_project.yml\n\nmodels:\n  jaffle_shop:\n    staging:\n      +materialized: view\n    intermediate:\n      +materialized: ephemeral\n    marts:\n      +materialized: table\n      finance:\n        +schema: finance\n      marketing:\n        +schema: marketing\n```\n\n----------------------------------------\n\nTITLE: Creating Grant Select Operation Macro in SQL (dbt)\nDESCRIPTION: Defines a macro that grants select privileges on all tables and views in the current schema to a specified role. This macro can be invoked as an operation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/hooks-operations.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{% macro grant_select(role) %}\n{% set sql %}\n    grant usage on schema {{ target.schema }} to role {{ role }};\n    grant select on all tables in schema {{ target.schema }} to role {{ role }};\n    grant select on all views in schema {{ target.schema }} to role {{ role }};\n{% endset %}\n\n{% do run_query(sql) %}\n{% do log(\"Privileges granted\", info=True) %}\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Enforcing a Model Contract in dbt YAML\nDESCRIPTION: YAML configuration for enforcing a model contract in dbt, specifying column names, data types, and constraints.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/govern/model-contracts.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: dim_customers\n    config:\n      contract:\n        enforced: true\n    columns:\n      - name: customer_id\n        data_type: int\n        constraints:\n          - type: not_null\n      - name: customer_name\n        data_type: string\n      ...\n```\n\n----------------------------------------\n\nTITLE: Selecting from Sources in SQL for dbt\nDESCRIPTION: This SQL snippet demonstrates how to use the {{ source() }} function in dbt to reference source tables in a model. It shows joining data from multiple source tables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/sources.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect\n  ...\n\nfrom {{ source('jaffle_shop', 'orders') }}\n\nleft join {{ source('jaffle_shop', 'customers') }} using (customer_id)\n```\n\n----------------------------------------\n\nTITLE: Creating Orders Table in Snowflake\nDESCRIPTION: SQL command to create the orders table in the jaffle_shop schema with columns for id, user_id, order_date, status, and a timestamp field for ETL tracking. This prepares the table structure for loading sample order data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/snowflake-qs.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\ncreate table raw.jaffle_shop.orders\n( id integer,\n  user_id integer,\n  order_date date,\n  status varchar,\n  _etl_loaded_at timestamp default current_timestamp\n);\n```\n\n----------------------------------------\n\nTITLE: Debugging Dependency Cycle Error in dbt\nDESCRIPTION: This snippet shows an error caused by a circular dependency in the dbt DAG (Directed Acyclic Graph). The error message indicates a cycle between models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/debug-errors.md#2025-04-09_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\n$ dbt run\nRunning with dbt=1.7.1-rc\n\nEncountered an error:\nFound a cycle: model.jaffle_shop.customers --> model.jaffle_shop.stg_customers --> model.jaffle_shop.customers\n```\n\n----------------------------------------\n\nTITLE: Incremental Model with Delete+Insert Strategy\nDESCRIPTION: Configuring an incremental model with the 'delete+insert' strategy and a unique key. This approach first deletes records matching the incremental condition, then inserts the updated versions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/trino-configs.md#2025-04-09_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n      materialized = 'incremental',\n      unique_key='user_id',\n      incremental_strategy='delete+insert',\n      )\n}}\nselect * from {{ ref('users') }}\n{% if is_incremental() %}\n  where updated_ts > (select max(updated_ts) from {{ this }})\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Validating Webhook Authenticity in Python\nDESCRIPTION: This code snippet demonstrates how to verify the authenticity of a webhook received from dbt Cloud using Python. It validates the request by comparing the Authorization header with a SHA256 hash of the request body using your secret token as the key.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/webhooks.md#2025-04-09_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nauth_header = request.headers.get('authorization', None)\napp_secret = os.environ['MY_DBT_CLOUD_AUTH_TOKEN'].encode('utf-8')\nsignature = hmac.new(app_secret, request_body, hashlib.sha256).hexdigest()\nreturn signature == auth_header\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Upsolver Authentication in dbt Profiles\nDESCRIPTION: YAML configuration for Upsolver in the dbt profiles.yml file. It shows how to set up user/token authentication, specify the API URL, database, schema, and number of threads.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/upsolver-setup.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmy-upsolver-db:\n  target: dev\n  outputs:\n    dev:\n      type: upsolver\n      api_url: https://mt-api-prod.upsolver.com\n\n      user: [username]\n      token: [token]\n\n      database: [database name]\n      schema: [schema name]\n      threads: [1 or more]\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Models Inline\nDESCRIPTION: Shows how to configure Python models within the model file using the dbt.config() method, similar to the config() macro in SQL models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/python-models.md#2025-04-09_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef model(dbt, session):\n\n    # setting configuration\n    dbt.config(materialized=\"table\")\n```\n\n----------------------------------------\n\nTITLE: String Splitting in SQL with dbt\nDESCRIPTION: Macro to split a string using a delimiter and return a specific part (1-based indexing).\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/cross-database-macros.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt.split_part(string_text='column_to_split', delimiter_text='delimiter_column', part_number=1) }}\n{{ dbt.split_part(string_text=\"'1|2|3'\", delimiter_text=\"'|'\", part_number=1) }}\n```\n\n----------------------------------------\n\nTITLE: Building Complex Customer Model with dbt in SQL\nDESCRIPTION: This SQL snippet demonstrates a more complex dbt model that combines staging models for customers and orders. It uses dbt's ref function to reference other models and performs joins and aggregations to create a comprehensive customer view.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/athena-qs.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nwith customers as (\n\n    select * from {{ ref('stg_customers') }}\n\n),\n\norders as (\n\n    select * from {{ ref('stg_orders') }}\n\n),\n\ncustomer_orders as (\n\n    select\n        customer_id,\n\n        min(order_date) as first_order_date,\n        max(order_date) as most_recent_order_date,\n        count(order_id) as number_of_orders\n\n    from orders\n\n    group by 1\n\n),\n\nfinal as (\n\n    select\n        customers.customer_id,\n        customers.first_name,\n        customers.last_name,\n        customer_orders.first_order_date,\n        customer_orders.most_recent_order_date,\n        coalesce(customer_orders.number_of_orders, 0) as number_of_orders\n\n    from customers\n\n    left join customer_orders using (customer_id)\n\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Refactoring Staging Orders Model to Use Sources in SQL for dbt\nDESCRIPTION: This SQL query refactors the staging orders model to use the source function, referencing the jaffle_shop source and orders table defined in the sources.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/bigquery-qs.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    id as order_id,\n    user_id as customer_id,\n    order_date,\n    status\n\nfrom {{ source('jaffle_shop', 'orders') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Data Tests in Project File\nDESCRIPTION: Example of data test-specific configurations in dbt_project.yml. This shows how to configure test properties like fail_calc, limit, severity, and more at the project level.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/data-test-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntests:\n  [<resource-path>]:\n    [+][fail_calc]: <string>\n    [+][limit]: <integer>\n    [+][severity]: error | warn\n    [+][error_if]: <string>\n    [+][warn_if]: <string>\n    [+][store_failures]: true | false\n    [+][where]: <string>\n\n```\n\n----------------------------------------\n\nTITLE: Defining Cumulative Revenue Metric with Granularity Options in dbt YAML\nDESCRIPTION: This YAML snippet defines a cumulative revenue metric with granularity options using the 'period_agg' parameter. It demonstrates how to set up a cumulative metric that calculates the cumulative revenue for all orders, with the option to specify how the metric is aggregated over the requested period.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/cumulative-metrics.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n- name: cumulative_revenue\n  description: The cumulative revenue for all orders.\n  label: Cumulative revenue (all-time)\n  type: cumulative\n  type_params:\n    measure: revenue\n    cumulative_type_params:\n      period_agg: first # Optional. Defaults to first. Accepted values: first|end|average\n```\n\n----------------------------------------\n\nTITLE: Specifying Row Limit in SQL Query for dbt Cloud IDE Preview\nDESCRIPTION: This SQL snippet demonstrates how to set a custom row limit when using the Preview button in the dbt Cloud IDE. It allows you to control the number of rows returned, overriding the default 500-row limit.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/dbt-cloud-ide/ide-user-interface.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM table limit 100\n```\n\n----------------------------------------\n\nTITLE: Semantic Model Configuration in YAML Schema\nDESCRIPTION: Shows how to configure semantic model properties using the config block in the schema file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/semantic-models.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_models:\n  - name: orders\n    config:\n      enabled: true | false\n      group: some_group\n      meta:\n        some_key: some_value\n```\n\n----------------------------------------\n\nTITLE: Cross-Project Model Referencing with ref Function\nDESCRIPTION: Demonstrates how to reference models from different projects or packages using the two-argument variant of the ref function, which specifies both the project/package name and the model name.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/ref.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nselect * from {{ ref('project_or_package', 'model_name') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Authorized Views in SQL Model\nDESCRIPTION: Sets the grant_access_to config for a specific model in its SQL file. This configuration specifies which datasets the view should have access to, enabling creation of authorized views in BigQuery.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/bigquery-configs.md#2025-04-09_snippet_16\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    grant_access_to=[\n      {'project': 'project_1', 'dataset': 'dataset_1'},\n      {'project': 'project_2', 'dataset': 'dataset_2'}\n    ]\n) }}\n```\n\n----------------------------------------\n\nTITLE: Dynamic Payment Method Amounts using Jinja\nDESCRIPTION: SQL model that uses Jinja to dynamically generate payment amount columns for different payment methods using a for loop.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/jinja-macros.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{% set payment_methods = [\"bank_transfer\", \"credit_card\", \"gift_card\"] %}\n\nselect\n    order_id,\n    {% for payment_method in payment_methods %}\n    sum(case when payment_method = '{{payment_method}}' then amount end) as {{payment_method}}_amount,\n    {% endfor %}\n    sum(amount) as total_amount\nfrom app_data.payments\ngroup by 1\n```\n\n----------------------------------------\n\nTITLE: Compiled SQL for Source Freshness Check\nDESCRIPTION: The compiled SQL query that dbt runs to check source freshness based on the configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/freshness.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nselect\n  max(_etl_loaded_at) as max_loaded_at,\n  convert_timezone('UTC', current_timestamp()) as snapshotted_at\nfrom raw.jaffle_shop.orders\n\nwhere datediff('day', _etl_loaded_at, current_timestamp) < 2\n```\n\n----------------------------------------\n\nTITLE: Modular Macros for Column Value Retrieval in dbt\nDESCRIPTION: Two macros: a generic get_column_values for retrieving distinct column values, and a specific get_payment_methods that uses the generic macro.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/using-jinja.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\n{% macro get_column_values(column_name, relation) %}\n\n{% set relation_query %}\nselect distinct\n{{ column_name }}\nfrom {{ relation }}\norder by 1\n{% endset %}\n\n{% set results = run_query(relation_query) %}\n\n{% if execute %}\n{# Return the first column #}\n{% set results_list = results.columns[0].values() %}\n{% else %}\n{% set results_list = [] %}\n{% endif %}\n\n{{ return(results_list) }}\n\n{% endmacro %}\n\n\n{% macro get_payment_methods() %}\n\n{{ return(get_column_values('payment_method', ref('raw_payments'))) }}\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Example: Hiding a Model in Documentation\nDESCRIPTION: Demonstrates how to mark a specific model as hidden in the documentation using the docs property.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/docs.md#2025-04-09_snippet_8\n\nLANGUAGE: yml\nCODE:\n```\nmodels:\n  - name: sessions__tmp\n    docs:\n      show: false\n```\n\n----------------------------------------\n\nTITLE: Creating Documentation Blocks with Markdown\nDESCRIPTION: Demonstration of creating a docs block using markdown to document a table's purpose and data source details.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/documentation.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n{% docs table_events %}\n\nThis table contains clickstream events from the marketing website.\n\nThe events in this table are recorded by [Snowplow](http://github.com/snowplow/snowplow) and piped into the warehouse on an hourly basis. The following pages of the marketing site are tracked:\n - /\n - /about\n - /team\n - /contact-us\n\n{% enddocs %}\n```\n\n----------------------------------------\n\nTITLE: Displaying dbt Project Directory Structure\nDESCRIPTION: An example of a well-organized dbt project directory structure showing models organized by staging, intermediate, marts, and utilities folders, with consistent file naming patterns and organization.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-structure/5-the-rest-of-the-project.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nmodels\n intermediate\n    finance\n        _int_finance__models.yml\n        int_payments_pivoted_to_orders.sql\n marts\n    finance\n       _finance__models.yml\n       orders.sql\n       payments.sql\n    marketing\n        _marketing__models.yml\n        customers.sql\n staging\n    jaffle_shop\n       _jaffle_shop__docs.md\n       _jaffle_shop__models.yml\n       _jaffle_shop__sources.yml\n       base\n          base_jaffle_shop__customers.sql\n          base_jaffle_shop__deleted_customers.sql\n       stg_jaffle_shop__customers.sql\n       stg_jaffle_shop__orders.sql\n    stripe\n        _stripe__models.yml\n        _stripe__sources.yml\n        stg_stripe__payments.sql\n utilities\n     all_dates.sql\n```\n\n----------------------------------------\n\nTITLE: Disabling a Specific Source from a Package\nDESCRIPTION: Example showing how to disable a specific source from another package by qualifying the resource path with both a package name and a source name.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/source-configs.md#2025-04-09_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  events:\n    clickstream:\n      +enabled: false\n```\n\n----------------------------------------\n\nTITLE: Creating an Hourly Time Spine in SQL\nDESCRIPTION: Creates a time spine table at hour granularity using dbt's date_spine macro. The table generates hours from January 1, 2000, to January 1, 2025, but filters to only include hours from 4 years ago to 30 days in the future.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metricflow-time-spine.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        materialized = 'table',\n    )\n}}\n\nwith hours as (\n\n    {{\n        dbt.date_spine(\n            'hour',\n            \"to_date('01/01/2000','mm/dd/yyyy')\",\n            \"to_date('01/01/2025','mm/dd/yyyy')\"\n        )\n    }}\n\n),\n\nfinal as (\n    select cast(date_hour as timestamp) as date_hour\n    from hours\n)\n\nselect * from final\n-- filter the time spine to a specific range\nwhere date_day > dateadd(year, -4, current_timestamp()) \nand date_hour < dateadd(day, 30, current_timestamp())\n```\n\n----------------------------------------\n\nTITLE: BigQuery Data Types in dbt Unit Tests\nDESCRIPTION: Shows BigQuery-specific data type handling including structs, arrays, geography, and JSON types. Includes special note about handling all fields in BigQuery structs and demonstrates nested struct definitions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/data-types.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nunit_tests:\n  - name: test_my_data_types\n    model: fct_data_types\n    given:\n      - input: ref('stg_data_types')\n        rows:\n         - int_field: 1\n           float_field: 2.0\n           str_field: my_string\n           str_escaped_field: \"my,cool'string\"\n           date_field: 2020-01-02\n           timestamp_field: 2013-11-03 00:00:00-0\n           timestamptz_field: 2013-11-03 00:00:00-0\n           bigint_field: 1\n           geography_field: 'st_geogpoint(75, 45)'\n           json_field: {\"name\": \"Cooper\", \"forname\": \"Alice\"}\n           str_array_field: ['a','b','c']\n           int_array_field: [1, 2, 3]\n           date_array_field: ['2020-01-01']\n           struct_field: 'struct(\"Isha\" as name, 22 as age)'\n           struct_of_struct_field: 'struct(struct(1 as id, \"blue\" as color) as my_struct)'\n           struct_array_field: ['struct(st_geogpoint(75, 45) as my_point)', 'struct(st_geogpoint(75, 35) as my_point)']\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Packages in YAML\nDESCRIPTION: Example packages.yml configuration showing various package types including Hub packages, git repositories, and subdirectory specifications with version pinning.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/deps.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\npackages:\n  - package: dbt-labs/dbt_utils\n    version: 0.7.1\n  - package: brooklyn-data/dbt_artifacts\n    version: 1.2.0\n    install-prerelease: true\n  - package: dbt-labs/codegen\n    version: 0.4.0\n  - package: calogica/dbt_expectations\n    version: 0.4.1\n  - git: https://github.com/dbt-labs/dbt-audit-helper.git\n    revision: 0.4.0\n  - git: \"https://github.com/dbt-labs/dbt-labs-experimental-features\"\n    subdirectory: \"materialized-views\"\n    revision: 0.0.1\n  - package: dbt-labs/snowplow\n    version: 0.13.0\n```\n\n----------------------------------------\n\nTITLE: Pivoting Payments Data with Jinja and SQL in dbt\nDESCRIPTION: An intermediate model that demonstrates pivoting payment data to order grain using Jinja templating. The model aggregates payment amounts by payment method and calculates total amounts for successful payments.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-structure/3-intermediate.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n-- int_payments_pivoted_to_orders.sql\n\n{%- set payment_methods = ['bank_transfer','credit_card','coupon','gift_card'] -%}\n\nwith\n\npayments as (\n\n   select * from {{ ref('stg_stripe__payments') }}\n\n),\n\npivot_and_aggregate_payments_to_order_grain as (\n\n   select\n      order_id,\n      {% for payment_method in payment_methods -%}\n\n         sum(\n            case\n               when payment_method = '{{ payment_method }}' and\n                    status = 'success'\n               then amount\n               else 0\n            end\n         ) as {{ payment_method }}_amount,\n\n      {%- endfor %}\n      sum(case when status = 'success' then amount end) as total_amount\n\n   from payments\n\n   group by 1\n\n)\n\nselect * from pivot_and_aggregate_payments_to_order_grain\n```\n\n----------------------------------------\n\nTITLE: Implementing Model Versioning with Contract Enforcement\nDESCRIPTION: YAML configuration demonstrating how to set up model versions. It shows version 2 of dim_customers with column inclusion/exclusion patterns and references to the original model definition.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-01-evolving-data-engineer-craft.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: dim_customers\n    latest_version: 2\n    config:\n      contract:\n        enforced: true\n    columns:\n      - name: id\n        data_type: integer\n        description: hello\n        constraints:\n          - type: not_null\n          - type: primary_key # not enforced  -- will warn & include in DDL\n          - type: check       # not supported -- will warn & exclude from DDL\n            expression: \"id > 0\"\n        tests:\n          - unique            # primary_key constraint is not enforced, so also verify with a dbt test\n      - name: customer_name\n        data_type: text\n      - name: first_transaction_date\n        data_type: date\n    versions:\n      - v: 2\n        columns: \n          - include: '*'\n            exclude: ['first_transaction_date']\n      - v: 1\n        columns:\n          - include: '*'\n        defined_in: dim_customers\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Models in Property YAML File\nDESCRIPTION: This snippet demonstrates how to configure dbt models in a properties.yml file. It allows specifying configurations for individual models, including materialization, SQL headers, and unique keys. The model name must match the filename of the model, including case sensitivity.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/model-configs.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: [<model-name>] #  Must match the filename of a model -- including case sensitivity.\n    config:\n      [materialized]: <materialization_name>\n      [sql_header]: <string>\n      [on_configuration_change]: apply | continue | fail # Only for materialized views on supported adapters\n      [unique_key]: <column_name_or_expression>\n      [batch_size]: day | hour | month | year\n      [begin]: \"<ISO formatted date or datetime (like, \\\"2024-01-15T12:00:00Z\\\")\"\n      [lookback]: <integer>\n      [concurrent_batches]: true | false\n```\n\n----------------------------------------\n\nTITLE: Basic SELECT Query in dbt Projects\nDESCRIPTION: A simple SELECT statement example that retrieves order_id, customer_id, and order_date columns from a referenced orders model, limiting the results to 3 rows. This demonstrates the fundamental syntax of a SELECT statement in a dbt project using the ref function.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/statements/sql-select.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\torder_id, --your first column you want selected\n\tcustomer_id, --your second column you want selected\n\torder_date --your last column you want selected (and so on)\nfrom {{ ref('orders') }} --the table/view/model you want to select from\nlimit 3\n```\n\n----------------------------------------\n\nTITLE: Configuring Extrica Profiles in YAML\nDESCRIPTION: Example profiles.yml configuration for dbt-extrica showing both development and production environments. Includes required fields for JWT authentication, connection details, and runtime configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/extrica-setup.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n<profile-name>:\n  outputs:\n    dev:\n      type: extrica\n      method: jwt \n      username: [username for jwt auth]\n      password: [password for jwt auth]  \n      host: [extrica hostname]\n      port: [port number]\n      schema: [dev_schema]\n      catalog: [catalog_name]\n      threads: [1 or more]\n\n    prod:\n      type: extrica\n      method: jwt \n      username: [username for jwt auth]\n      password: [password for jwt auth]  \n      host: [extrica hostname]\n      port: [port number]\n      schema: [dev_schema]\n      catalog: [catalog_name]\n      threads: [1 or more]\n  target: dev\n```\n\n----------------------------------------\n\nTITLE: Configuring Dynamic Tables in SQL Model\nDESCRIPTION: In-model configuration block for dynamic tables using Jinja templating, specifying materialization and refresh settings.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snowflake-configs.md#2025-04-09_snippet_4\n\nLANGUAGE: jinja\nCODE:\n```\n{{ config(\n    materialized=\"dynamic_table\",\n    on_configuration_change=\"apply\" | \"continue\" | \"fail\",\n    target_lag=\"downstream\" | \"<integer> seconds | minutes | hours | days\",\n    snowflake_warehouse=\"<warehouse-name>\",\n    refresh_mode=\"AUTO\" | \"FULL\" | \"INCREMENTAL\",\n    initialize=\"ON_CREATE\" | \"ON_SCHEDULE\"\n) }}\n```\n\n----------------------------------------\n\nTITLE: Array Function Implementation in dbt\nDESCRIPTION: Examples of array manipulation functions including array_append, array_concat, and array_construct.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/cross-database-macros.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt.array_append(\"array_column\", \"element_column\") }}\n{{ dbt.array_concat(\"array_column_1\", \"array_column_2\") }}\n{{ dbt.array_construct([\"column_1\", \"column_2\", \"column_3\"]) }}\n```\n\n----------------------------------------\n\nTITLE: Defining a Relationships Generic Test in SQL\nDESCRIPTION: Example of a more complex custom generic test that requires additional arguments beyond the standard ones. This 'relationships' test verifies that column values exist in a referenced table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/custom-generic-tests.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{% test relationships(model, column_name, field, to) %}\n\nwith parent as (\n\n    select\n        {{ field }} as id\n\n    from {{ to }}\n\n),\n\nchild as (\n\n    select\n        {{ column_name }} as id\n\n    from {{ model }}\n\n)\n\nselect *\nfrom child\nwhere id is not null\n  and id not in (select id from parent)\n\n{% endtest %}\n```\n\n----------------------------------------\n\nTITLE: Refactoring Staging Customers Model to Use Sources in SQL for dbt\nDESCRIPTION: This SQL query refactors the staging customers model to use the source function, referencing the jaffle_shop source and customers table defined in the sources.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/bigquery-qs.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    id as customer_id,\n    first_name,\n    last_name\n\nfrom {{ source('jaffle_shop', 'customers') }}\n```\n\n----------------------------------------\n\nTITLE: Azure DevOps Pipeline Configuration\nDESCRIPTION: Azure DevOps pipeline configuration that runs a dbt Cloud job when changes are pushed to main branch. Includes variable definitions and Python setup steps.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/custom-cicd-pipelines.md#2025-04-09_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nname: Run dbt Cloud Job\n\ntrigger: [ main ]\n\nvariables:\n  DBT_URL:                 https://cloud.getdbt.com\n  DBT_JOB_CAUSE:           'Azure Pipeline CI Job'\n  DBT_ACCOUNT_ID:          00000\n  DBT_PROJECT_ID:          00000\n  DBT_PR_JOB_ID:           00000\n\nsteps:\n  - task: UsePythonVersion@0\n    inputs:\n      versionSpec: '3.7'\n    displayName: 'Use Python 3.7'\n\n  - script: |\n      python -m pip install requests\n    displayName: 'Install python dependencies'\n\n  - script: |\n      python -u ./python/run_and_monitor_dbt_job.py\n    displayName: 'Run dbt job '\n    env:\n      DBT_API_KEY: $(DBT_API_KEY)\n```\n\n----------------------------------------\n\nTITLE: Configuring Ratio Metrics in YAML for dbt Semantic Layer\nDESCRIPTION: This snippet shows how to define ratio metrics, including 'cancellation_rate' and 'enterprise_cancellation_rate'. It demonstrates setting numerator and denominator metrics, applying filters, and using dimensions in the filter expressions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metrics-overview.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  - name: cancellation_rate\n    type: ratio\n    label: Cancellation rate\n    type_params:\n      numerator: cancellations\n      denominator: transaction_amount\n    filter: |   \n      {{ Dimension('customer__country') }} = 'MX'\n  - name: enterprise_cancellation_rate\n    type: ratio\n    type_params:\n      numerator:\n        name: cancellations\n        filter: {{ Dimension('company__tier') }} = 'enterprise'  \n      denominator: transaction_amount\n    filter: | \n      {{ Dimension('customer__country') }} = 'MX' \n```\n\n----------------------------------------\n\nTITLE: DBT Snapshot YAML Configuration\nDESCRIPTION: YAML-based configuration format for DBT snapshots, showing all available configuration options.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/snapshots.md#2025-04-09_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  - name: string\n    relation: relation\n    description: markdown_string\n    config:\n      database: string\n      schema: string\n      alias: string\n      strategy: timestamp | check\n      unique_key: column_name_or_expression\n      check_cols: [column_name] | all\n      updated_at: column_name\n      snapshot_meta_column_names: dictionary\n      dbt_valid_to_current: string\n      hard_deletes: ignore | invalidate | new_record\n```\n\n----------------------------------------\n\nTITLE: Testing DBT Models with Accepted Values\nDESCRIPTION: Example of using the accepted_values test in DBT where intentionally omitting values from the list will cause the test to fail. This demonstrates how to debug test failures and understand test results.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/tutorial-next-steps-tests.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\naccepted_values:\n  values: ['order_status_1', 'order_status_2']\n```\n\n----------------------------------------\n\nTITLE: Boolean OR Aggregation in SQL with dbt\nDESCRIPTION: Macro to perform logical OR operation across a group of boolean values.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/cross-database-macros.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt.bool_or(\"boolean_column\") }}\n{{ dbt.bool_or(\"integer_column = 3\") }}\n```\n\n----------------------------------------\n\nTITLE: Setting Indirect Selection Environment Variable for dbt\nDESCRIPTION: Example of setting an environment variable to configure indirect selection for dbt commands.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/indirect-selection.md#2025-04-09_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$ export DBT_INDIRECT_SELECTION=cautious\ndbt run\n```\n\n----------------------------------------\n\nTITLE: dbt Model File Extensions\nDESCRIPTION: The file extensions used for creating dbt models, supporting both SQL and Python implementations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/models.md#2025-04-09_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n.sql\n.py\n```\n\n----------------------------------------\n\nTITLE: Creating Staging Model for Orders in SQL\nDESCRIPTION: SQL query to create a staging model for orders, selecting and renaming relevant columns from the source table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/sl-snowflake-qs.md#2025-04-09_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\n  select\n    id as order_id,\n    user_id as customer_id,\n    order_date,\n    status\n  from {{ source('jaffle_shop', 'orders') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Sort and Dist Keys in Redshift Models (SQL with Jinja)\nDESCRIPTION: Examples of how to configure sort keys and dist keys in Redshift table models to optimize query performance. Includes single sort key, multiple sort keys, and interleaved sort key configurations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/redshift-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n-- Example with one sort key\n{{ config(materialized='table', sort='reporting_day', dist='unique_id') }}\n\nselect ...\n\n\n-- Example with multiple sort keys\n{{ config(materialized='table', sort=['category', 'region', 'reporting_day'], dist='received_at') }}\n\nselect ...\n\n\n-- Example with interleaved sort keys\n{{ config(materialized='table',\n          sort_type='interleaved'\n          sort=['category', 'region', 'reporting_day'],\n          dist='unique_id')\n}}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Querying Exposures Information with GraphQL\nDESCRIPTION: This GraphQL query retrieves detailed information about all exposures in a specific job, including owner details, URLs, and information about parent sources and models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/schema-discovery-job-exposures.mdx#2025-04-09_snippet_0\n\nLANGUAGE: graphql\nCODE:\n```\n{\n  job(id: 123) {\n    exposures(jobId: 123) {\n      runId\n      projectId\n      name\n      uniqueId\n      resourceType\n      ownerName\n      url\n      ownerEmail\n      parentsSources {\n        uniqueId\n        sourceName\n        name\n        state\n        maxLoadedAt\n        criteria {\n          warnAfter {\n            period\n            count\n          }\n          errorAfter {\n            period\n            count\n          }\n        }\n        maxLoadedAtTimeAgoInS\n      }\n      parentsModels {\n        uniqueId\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Categorical Dimension in dbt Semantic Layer\nDESCRIPTION: This snippet demonstrates how to define a categorical dimension using a SQL expression to create a boolean flag for bulk transactions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/dimensions.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ndimensions: \n  - name: is_bulk_transaction\n    type: categorical\n    expr: case when quantity > 10 then true else false end\n    config:\n      meta:\n        usage: \"Filter to identify bulk transactions, like where quantity > 10.\"\n```\n\n----------------------------------------\n\nTITLE: Using Cents to Dollars Macro in Model\nDESCRIPTION: Example of using the cents_to_dollars macro within a staging model to transform payment amounts.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/jinja-macros.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nselect\n  id as payment_id,\n  {{ cents_to_dollars('amount') }} as amount_usd,\n  ...\nfrom app_data.payments\n```\n\n----------------------------------------\n\nTITLE: Check Strategy Snapshot Example\nDESCRIPTION: Example demonstrating how snapshot meta-fields are populated using the check strategy, including the handling of updated records and hard deletes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/snapshots.md#2025-04-09_snippet_19\n\nLANGUAGE: sql\nCODE:\n```\n| id | status  |  dbt_valid_from   | dbt_valid_to     | dbt_updated_at   | dbt_is_deleted |\n|----|---------|------------------|------------------|------------------|----------------|\n| 1  | pending |  2024-01-01 11:00 | 2024-01-01 11:30 | 2024-01-01 11:00 | False          |\n| 1  | shipped | 2024-01-01 11:30 | 2024-01-01 11:40 | 2024-01-01 11:30 | False          |\n| 1  | deleted |  2024-01-01 11:40 |                  | 2024-01-01 11:40 | True           |\n```\n\n----------------------------------------\n\nTITLE: Using Jinja For Loop in SQL Model\nDESCRIPTION: Improved SQL query that uses a Jinja for loop to iterate through payment methods and generate the case statements dynamically, reducing repetition.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/using-jinja.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect\norder_id,\n{% for payment_method in [\"bank_transfer\", \"credit_card\", \"gift_card\"] %}\nsum(case when payment_method = '{{payment_method}}' then amount end) as {{payment_method}}_amount,\n{% endfor %}\nsum(amount) as total_amount\nfrom {{ ref('raw_payments') }}\ngroup by 1\n```\n\n----------------------------------------\n\nTITLE: Creating Customer Analysis Model in dbt\nDESCRIPTION: Implements a dbt model that joins customer and order data to create a comprehensive customer analysis view.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/teradata-qs.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nwith customers as (\n\n   select\n       id as customer_id,\n       first_name,\n       last_name\n\n   from jaffle_shop.customers\n\n),\n\norders as (\n\n   select\n       id as order_id,\n       user_id as customer_id,\n       order_date,\n       status\n\n   from jaffle_shop.orders\n\n),\n\ncustomer_orders as (\n\n   select\n       customer_id,\n\n       min(order_date) as first_order_date,\n       max(order_date) as most_recent_order_date,\n       count(order_id) as number_of_orders\n\n   from orders\n\n   group by 1\n\n),\n\nfinal as (\n\n   select\n       customers.customer_id,\n       customers.first_name,\n       customers.last_name,\n       customer_orders.first_order_date,\n       customer_orders.most_recent_order_date,\n       coalesce(customer_orders.number_of_orders, 0) as number_of_orders\n\n   from customers\n\n   left join customer_orders on customers.customer_id = customer_orders.customer_id\n\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Defining Multiple Entity Types in MetricFlow YAML\nDESCRIPTION: This example shows how to configure multiple entities with different types (primary, foreign) in a MetricFlow semantic model. It demonstrates the use of 'name' and 'expr' parameters for entity definitions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/semantic-models.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nentity:\n  - name: transaction\n    type: primary\n  - name: order\n    type: foreign\n    expr: id_order\n  - name: user\n    type: foreign\n    expr: substring(id_order FROM 2)\n```\n\n----------------------------------------\n\nTITLE: Full YAML Example of Selectors\nDESCRIPTION: A complete example of a complex selector using Full YAML syntax with union, intersection, and exclude operations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/yaml-selectors.md#2025-04-09_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nselectors:\n  - name: nightly_diet_snowplow\n    description: \"Non-incremental Snowplow models that power nightly exports\"\n    definition:\n      # Optional `union` and `intersection` keywords map to the ` ` and `,` set operators:\n      union:\n        - intersection:\n            - method: source\n              value: snowplow\n              childrens_parents: true\n            - method: tag\n              value: nightly\n        - method: path\n          value: models/export\n        - exclude:\n            - intersection:\n                - method: package\n                  value: snowplow\n                - method: config.materialized\n                  value: incremental\n            - method: fqn\n              value: export_performance_timing\n```\n\n----------------------------------------\n\nTITLE: Creating a Dynamic Query Tag Macro for Snowflake\nDESCRIPTION: A custom macro that sets the query tag to the model's name for each query. This overrides the default set_query_tag macro to provide more specific query tagging in Snowflake.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snowflake-configs.md#2025-04-09_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\n{% macro set_query_tag() -%}\n  {% set new_query_tag = model.name %} \n  {% if new_query_tag %}\n    {% set original_query_tag = get_current_query_tag() %}\n    {{ log(\"Setting query_tag to '\" ~ new_query_tag ~ \"'. Will reset to '\" ~ original_query_tag ~ \"' after materialization.\") }}\n    {% do run_query(\"alter session set query_tag = '{}'\".format(new_query_tag)) %}\n    {{ return(original_query_tag)}}\n  {% endif %}\n  {{ return(none)}}\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Incremental Model Implementation - Snowpark\nDESCRIPTION: Example of implementing an incremental Python model using Snowpark, showing filtering logic for new data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/python-models.md#2025-04-09_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport snowflake.snowpark.functions as F\n\ndef model(dbt, session):\n    dbt.config(materialized = \"incremental\")\n    df = dbt.ref(\"upstream_table\")\n\n    if dbt.is_incremental:\n\n        # only new rows compared to max in current table\n        max_from_this = f\"select max(updated_at) from {dbt.this}\"\n        df = df.filter(df.updated_at >= session.sql(max_from_this).collect()[0][0])\n\n        # or only rows from the past 3 days\n        df = df.filter(df.updated_at >= F.dateadd(\"day\", F.lit(-3), F.current_timestamp()))\n\n    ...\n\n    return df\n```\n\n----------------------------------------\n\nTITLE: Implementing UDFs in PySpark Python Models with Decorator Syntax\nDESCRIPTION: Example of creating and using a UDF in a PySpark Python model using decorator syntax. The function adds random values to temperatures and is applied to a DataFrame column.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/python-models.md#2025-04-09_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport pyspark.sql.types as T\nimport pyspark.sql.functions as F\nimport numpy\n\n# use a 'decorator' for more readable code\n@F.udf(returnType=T.DoubleType())\ndef add_random(x):\n    random_number = numpy.random.normal()\n    return x + random_number\n\ndef model(dbt, session):\n    dbt.config(\n        materialized = \"table\",\n        packages = [\"numpy\"]\n    )\n\n    temps_df = dbt.ref(\"temperatures\")\n\n    # warm things up, who knows by how much\n    df = temps_df.withColumn(\"degree_plus_random\", add_random(\"degree\"))\n    return df\n```\n\n----------------------------------------\n\nTITLE: Running Modified Models with dbt Build Command\nDESCRIPTION: Command to run and test only modified nodes and their children in DAG order using state-based selection. This optimizes CI runs by executing only necessary components that have changed.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/set-up-ci.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ndbt build --select state:modified+\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Sources in YAML\nDESCRIPTION: This snippet demonstrates how to configure dbt sources using the 'config' property in a YAML file. It shows configurations for both the source and individual tables within the source.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/config.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: <source_name>\n    config:\n      [<source_config>](/reference/source-configs): <config_value>\n    tables:\n      - name: <table_name>\n        config:\n          [<source_config>](/reference/source-configs): <config_value>\n```\n\n----------------------------------------\n\nTITLE: Assigning Meta Properties in Semantic Model YAML\nDESCRIPTION: Example showing how to assign metadata properties like data_owner and used_in_reporting to dimensions, entities, and measures within a semantic model definition. This configuration allows teams to specify ownership and usage information for different model components.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/meta.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_models:\n  - name: semantic_model\n    ...\n    dimensions:\n      - name: order_date\n        type: time\n        config:\n          meta:\n            data_owner: \"Finance team\"\n            used_in_reporting: true\n    entities:\n      - name: customer_id\n        type: primary\n        config:\n          meta:\n            description: \"Unique identifier for customers\"\n            data_owner: \"Sales team\"\n            used_in_reporting: false\n    measures:\n      - name: count_of_users\n        expr: user_id\n        config:\n          meta:\n            used_in_reporting: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Models in dbt_project.yml for Databricks\nDESCRIPTION: Configuration in dbt_project.yml to set defaults for Python models in a specific subfolder, including submission method, notebook creation settings, and cluster ID.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/python-models.md#2025-04-09_snippet_21\n\nLANGUAGE: yaml\nCODE:\n```\n# dbt_project.yml\nmodels:\n  project_name:\n    subfolder:\n      # set defaults for all .py models defined in this subfolder\n      +submission_method: all_purpose_cluster\n      +create_notebook: False\n      +cluster_id: abcd-1234-wxyz\n```\n\n----------------------------------------\n\nTITLE: Configuring Microsoft Entra ID Username & Password Authentication for Azure SQL in dbt\nDESCRIPTION: This configuration sets up dbt to connect to Azure SQL using Microsoft Entra ID username and password authentication. It requires specifying the ODBC driver, server details, and authentication credentials in the profiles.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/mssql-setup.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: sqlserver\n      driver: 'ODBC Driver 18 for SQL Server' # (The ODBC Driver installed on your system)\n      server: hostname or IP of your server\n      port: 1433\n      database: exampledb\n      schema: schema_name\n      authentication: ActiveDirectoryPassword\n      user: bill.gates@microsoft.com\n      password: iheartopensource\n```\n\n----------------------------------------\n\nTITLE: Configuring Materialization in dbt Model File (Jinja)\nDESCRIPTION: This snippet shows how to set the materialization type directly in a dbt model SQL file using a Jinja config block. It allows for model-specific materialization configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/materialized.md#2025-04-09_snippet_2\n\nLANGUAGE: jinja\nCODE:\n```\n{{ config(\n  materialized=\"[<materialization_name>]\"\n) }}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Configuring tnsnames.ora for Oracle Connection\nDESCRIPTION: Example configuration for tnsnames.ora file to define database connection details.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/oracle-setup.md#2025-04-09_snippet_9\n\nLANGUAGE: text\nCODE:\n```\ndb2022adb_high = (description =\n                 (retry_count=20)(retry_delay=3)\n                 (address=(protocol=tcps)\n                 (port=1522)\n                 (host=adb.example.oraclecloud.com))\n                 (connect_data=(service_name=example_high.adb.oraclecloud.com))\n                 (security=(ssl_server_cert_dn=\"CN=adb.example.oraclecloud.com,\n                 OU=Oracle BMCS US,O=Oracle Corporation,L=Redwood City,ST=California,C=US\")))\n```\n\n----------------------------------------\n\nTITLE: Defining a Group in dbt YAML Configuration\nDESCRIPTION: This snippet demonstrates how to create a group in dbt to define the owner of a set of models. It uses the '__groups.yml' file to specify the group name and owner details.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-mesh/mesh-4-implementation.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ngroups: \n  - name: marketing\n    owner:\n        name: Ben Jaffleck \n        email: ben.jaffleck@jaffleshop.com\n```\n\n----------------------------------------\n\nTITLE: Referencing Python Models in SQL\nDESCRIPTION: Demonstrates how to reference a Python model in a downstream SQL model using the ref() function, showing the bidirectional integration between SQL and Python models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/python-models.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nwith upstream_python_model as (\n\n    select * from {{ ref('my_python_model') }}\n\n),\n\n...\n```\n\n----------------------------------------\n\nTITLE: dbt Run Command Output\nDESCRIPTION: This snippet shows the console output when running dbt run. It demonstrates how dbt builds the models in the correct order based on the automatically inferred dependencies, with stg_jaffle_shop__orders being built before customer_orders.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Models/create-dependencies.md#2025-04-09_snippet_1\n\nLANGUAGE: txt\nCODE:\n```\n$ dbt run\nRunning with dbt=1.6.0\nFound 2 models, 28 tests, 0 snapshots, 0 analyses, 130 macros, 0 operations, 0 seed files, 3 sources\n\n11:42:52 | Concurrency: 8 threads (target='dev_snowflake')\n11:42:52 |\n11:42:52 | 1 of 2 START view model dbt_claire.stg_jaffle_shop__orders........... [RUN]\n11:42:55 | 1 of 2 OK creating view model dbt_claire.stg_jaffle_shop__orders..... [CREATE VIEW in 2.50s]\n11:42:55 | 2 of 2 START relation dbt_claire.customer_orders..................... [RUN]\n11:42:56 | 2 of 2 OK creating view model dbt_claire.customer_orders............. [CREATE VIEW in 0.60s]\n11:42:56 | Finished running 2 view models in 15.13s.\n\n\nDone. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2\n```\n\n----------------------------------------\n\nTITLE: Configuring Data Platform Organization for Direct Promotion\nDESCRIPTION: This table shows the configuration of database and schema settings for different dbt Cloud environments in a Direct Promotion strategy. It maps environment names to their corresponding database and schema settings in the data platform.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2025-01-28-git-branching-strategies-and-dbt.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Environment Name | **Database** | **Schema** |\n| --- | --- | --- |\n| Development | `development` | User-specified in Profile Settings > Credentials |\n| Continuous Integration | `development` | Any safe default, like `dev_ci` (it doesn't even have to exist). The job we intend to set up will override the schema here anyway to denote the unique PR. |\n| Production | `production` | `analytics` |\n```\n\n----------------------------------------\n\nTITLE: Referencing a Snapshot in a Downstream Model\nDESCRIPTION: SQL model that references a snapshot using the 'ref' function. This demonstrates how to use snapshot data in downstream transformations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/snapshots.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\nselect * from {{ ref('orders_snapshot') }}\n```\n\n----------------------------------------\n\nTITLE: Defining Measures and Cumulative Metrics without Window in YAML\nDESCRIPTION: This example demonstrates how to define measures and cumulative metrics without specifying a window, useful for calculating running totals like current revenue and active subscriptions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/cumulative-metrics.md#2025-04-09_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nmeasures:\n  - name: revenue\n    description: Total revenue\n    agg: sum\n    expr: revenue\n  - name: subscription_count\n    description: Count of active subscriptions\n    agg: sum\n    expr: event_type\nmetrics:\n  - name: current_revenue\n    description: Current revenue\n    label: Current Revenue\n    type: cumulative\n    type_params:\n      measure: revenue\n  - name: active_subscriptions\n    description: Count of active subscriptions\n    label: Active Subscriptions\n    type: cumulative\n    type_params:\n      measure: subscription_count\n```\n\n----------------------------------------\n\nTITLE: Casting Column Types in SQL using CAST Function\nDESCRIPTION: This SQL snippet demonstrates how to use the CAST function to explicitly define column types in a dbt model. It casts 'order_id' as an integer and 'order_price' as a double with 6 total digits and 2 decimal places.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Project/define-a-column-type.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    cast(order_id as integer),\n    cast(order_price as double(6,2)) -- a more generic way of doing type conversion\nfrom {{ ref('stg_orders') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Enabled Tests in dbt YAML\nDESCRIPTION: Example of enabling or disabling tests in the dbt_project.yml file. This configuration applies to all tests in the specified resource path.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/enabled.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\ntests:\n  [<resource-path>]:\n    +enabled: true | false\n```\n\n----------------------------------------\n\nTITLE: Problematic Query Execution in dbt\nDESCRIPTION: Example of a SQL query that will fail during parse phase because it assumes query execution results are available when execute is False.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/execute.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{% set payment_method_query %}\nselect distinct\npayment_method\nfrom {{ ref('raw_payments') }}\norder by 1\n{% endset %}\n\n{% set results = run_query(payment_method_query) %}\n\n{# Return the first column #}\n{% set payment_methods = results.columns[0].values() %}\n```\n\n----------------------------------------\n\nTITLE: Querying Models by Database, Schema, and Identifier in GraphQL\nDESCRIPTION: This query demonstrates how to find a specific model by providing its unique database, schema, and identifier. It returns only the uniqueId field of the matching model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/schema-discovery-job-models.mdx#2025-04-09_snippet_0\n\nLANGUAGE: graphql\nCODE:\n```\n{\n  job(id: 123) {\n    models(database:\"analytics\", schema: \"analytics\", identifier:\"dim_customers\") {\n      uniqueId\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Saved Query in semantic_model.yml (dbt 1.8)\nDESCRIPTION: Example of setting up a saved query in the semantic_model.yml file for dbt version 1.8. It includes configuration for caching, query parameters, and exports.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/saved-queries.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nsaved_queries:\n  - name: test_saved_query\n    description: \"{{ doc('saved_query_description') }}\"\n    label: Test saved query\n    config:\n      cache:\n        enabled: true  # Or false if you want it disabled by default\n    query_params:\n      metrics:\n        - simple_metric\n      group_by:\n        - \"Dimension('user__ds')\"\n      where:\n        - \"{{ Dimension('user__ds', 'DAY') }} <= now()\"\n        - \"{{ Dimension('user__ds', 'DAY') }} >= '2023-01-01'\"\n    exports:\n      - name: my_export\n        config:\n          export_as: table\n          alias: my_export_alias\n          schema: my_export_schema_name\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Batch Size for Seeds in dbt Project Configuration\nDESCRIPTION: Project configuration that sets the maximum batch size for seed insertions to 200 rows, which helps avoid exceeding SQL Server's parameter limit of 2100.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/mssql-configs.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nvars:\n  max_batch_size: 200 # Any integer less than or equal to 2100 will do.\n```\n\n----------------------------------------\n\nTITLE: Configuring Saved Query in semantic_model.yml (dbt 1.7 and lower)\nDESCRIPTION: Example of setting up a saved query in the semantic_model.yml file for dbt version 1.7 and lower. It includes query parameters and exports configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/saved-queries.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nsaved_queries:\n  - name: test_saved_query\n    description: \"{{ doc('saved_query_description') }}\"\n    label: Test saved query\n    query_params:\n      metrics:\n        - simple_metric\n      group_by:\n        - \"Dimension('user__ds')\"\n      where:\n        - \"{{ Dimension('user__ds', 'DAY') }} <= now()\"\n        - \"{{ Dimension('user__ds', 'DAY') }} >= '2023-01-01'\"\n    exports:\n      - name: my_export\n        config:\n          export_as: table\n          alias: my_export_alias\n          schema: my_export_schema_name\n```\n\n----------------------------------------\n\nTITLE: DBT Snapshot with Source Query\nDESCRIPTION: Example of a snapshot block containing a select statement that references a source table using DBT's source macro.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snapshots-jinja-legacy.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot orders_snapshot %}\n\nselect * from {{ source('jaffle_shop', 'orders') }}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Enabled Semantic Models in dbt YAML\nDESCRIPTION: Example of enabling or disabling semantic models in the dbt_project.yml and semantic_models.yml files. This configuration can be applied to all semantic models or specific semantic model definitions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/enabled.md#2025-04-09_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic-models:\n  [<resource-path>]:\n    +enabled: true | false\n```\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_models:\n  - name: [<semantic_model_name>]\n    config:\n      enabled: true | false\n```\n\n----------------------------------------\n\nTITLE: Creating .dbt directory in Bash\nDESCRIPTION: Command to create a hidden .dbt directory in the user's home folder using the mkdir command in Bash.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/configure-cloud-cli.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmkdir ~/.dbt\n```\n\n----------------------------------------\n\nTITLE: Configuring Cluster for Materialized View in SQL\nDESCRIPTION: This snippet demonstrates how to configure a specific cluster for a materialized view using the config block in a SQL file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/materialize-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(materialized='materializedview', cluster='not_default') }}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Calculating Conversion Rate with Aggregation in SQL\nDESCRIPTION: SQL query that joins base measures (visits) with conversion measures (purchases) to calculate the conversion rate by day, with proper null handling and aggregation logic.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/conversion-metrics.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nselect\n  coalesce(subq_3.metric_time__day, subq_13.metric_time__day) as metric_time__day,\n  cast(max(subq_13.buys) as double) / cast(nullif(max(subq_3.visits), 0) as double) as visit_to_buy_conversion_rate_7d\nfrom ( -- base measure\n  select\n    metric_time__day,\n    sum(visits) as mqls\n  from (\n    select\n      date_trunc('day', first_contact_date) as metric_time__day,\n      1 as visits\n    from visits\n  ) subq_2\n  group by\n    metric_time__day\n) subq_3\nfull outer join ( -- conversion measure\n  select\n    metric_time__day,\n    sum(buys) as sellers\n  from (\n    -- ...\n    -- The output of this subquery is the table produced in Step 3. The SQL is hidden for legibility.\n    -- To see the full SQL output, add --explain to your conversion metric query. \n  ) subq_10\n  group by\n    metric_time__day\n) subq_13\non\n  subq_3.metric_time__day = subq_13.metric_time__day\ngroup by\n  metric_time__day\n```\n\n----------------------------------------\n\nTITLE: Calculating Monthly Orders and Distinct Customers using SQL COUNT\nDESCRIPTION: Example query demonstrating how to use COUNT and COUNT DISTINCT with GROUP BY to analyze order and customer data by month. The query calculates total orders and unique customers per month from a Jaffle Shop dataset.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/aggregate-functions/sql-count.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\tdate_part('month', order_date) as order_month,\n\tcount(order_id) as count_all_orders,\n\tcount(distinct(customer_id)) as count_distinct_customers\nfrom {{ ref('orders') }}\ngroup by 1\n```\n\n----------------------------------------\n\nTITLE: Unit Test Example with SQL Input and Fixture Output\nDESCRIPTION: This example illustrates a unit test using SQL format for input data and a fixture file for expected output. It tests the 'dim_customers' model, showcasing how to use SQL queries to generate mock input data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/unit-tests.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nunit_tests:\n  - name: test_is_valid_email_address # this is the unique name of the test\n    model: dim_customers # name of the model I'm unit testing\n    given: # the mock data for your inputs\n      - input: ref('stg_customers')\n        rows:\n         - {email: cool@example.com,     email_top_level_domain: example.com}\n         - {email: cool@unknown.com,     email_top_level_domain: unknown.com}\n         - {email: badgmail.com,         email_top_level_domain: gmail.com}\n         - {email: missingdot@gmailcom,  email_top_level_domain: gmail.com}\n      - input: ref('top_level_email_domains')\n        format: sql\n        rows: |\n          select 'example.com' as tld union all\n          select 'gmail.com' as tld\n    expect: # the expected output given the inputs above\n      format: sql\n      fixture: valid_email_address_fixture_output\n```\n\n----------------------------------------\n\nTITLE: Basic SQL HAVING Clause Syntax\nDESCRIPTION: Demonstrates the basic syntax structure for using HAVING clause in SQL queries after GROUP BY statement.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/clauses/sql-having.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\t-- query\nfrom <table>\ngroup by <field(s)>\nhaving condition\n[optional order by]\n```\n\n----------------------------------------\n\nTITLE: Configuring OAuth via gcloud Authentication in profiles.yml\nDESCRIPTION: Configuration for connecting to BigQuery using OAuth via gcloud authentication method. Requires local OAuth setup via gcloud CLI.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/bigquery-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmy-bigquery-db:\n  target: dev\n  outputs:\n    dev:\n      type: bigquery\n      method: oauth\n      project: GCP_PROJECT_ID\n      dataset: DBT_DATASET_NAME\n      threads: 4\n```\n\n----------------------------------------\n\nTITLE: Configuring Incremental Model with Merge Strategy for Apache Hudi in SQL\nDESCRIPTION: This SQL snippet demonstrates how to configure an incremental model using the merge strategy for Apache Hudi. It includes settings for unique_key and file_format, and shows how to filter data for incremental runs.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/glue-configs.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized='incremental',\n    incremental_strategy='merge',\n    unique_key='user_id',\n    file_format='hudi'\n) }}\n\nwith new_events as (\n\n    select * from {{ ref('events') }}\n\n    {% if is_incremental() %}\n    where date_day >= date_add(current_date, -1)\n    {% endif %}\n\n)\n\nselect\n    user_id,\n    max(date_day) as last_seen\n\nfrom events\ngroup by 1\n```\n\n----------------------------------------\n\nTITLE: Using YAML Configuration Files for dbt Dependencies\nDESCRIPTION: Configuration files that define package and project dependencies in dbt. The packages.yml supports Jinja templating and private packages, while dependencies.yml supports both package and project dependencies but without Jinja support.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_packages_or_dependencies.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# packages.yml - For package dependencies with Jinja support\npackages:\n  - package: dbt-labs/dbt_utils\n    version: 0.9.2\n\n# dependencies.yml - For both package and project dependencies\ndependencies:\n  packages:\n    - name: dbt-labs/dbt_utils\n      version: 0.9.2\n  projects:\n    - name: other_project\n      version: 1.0.0\n```\n\n----------------------------------------\n\nTITLE: Configuring Specific Model\nDESCRIPTION: Demonstrates how to apply configuration to a specific model using its full path in the project structure.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/resource-path.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nname: jaffle_shop\n\nmodels:\n  jaffle_shop:\n    staging:\n      stripe:\n        payments:\n          +enabled: false # this will apply to only one model\n```\n\n----------------------------------------\n\nTITLE: Configuring Saved Query in semantic_model.yml (dbt 1.9+)\nDESCRIPTION: Example of setting up a saved query in the semantic_model.yml file for dbt version 1.9 and higher. It includes configuration for caching, query parameters, and exports.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/saved-queries.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nsaved_queries:\n  - name: test_saved_query\n    description: \"{{ doc('saved_query_description') }}\"\n    label: Test saved query\n    config:\n      cache:\n        enabled: true | false\n        tags: 'my_tag'\n    query_params:\n      metrics:\n        - simple_metric\n      group_by:\n        - \"Dimension('user__ds')\"\n      where:\n        - \"{{ Dimension('user__ds', 'DAY') }} <= now()\"\n        - \"{{ Dimension('user__ds', 'DAY') }} >= '2023-01-01'\"\n    exports:\n      - name: my_export\n        config:\n          export_as: table \n          alias: my_export_alias\n          schema: my_export_schema_name\n```\n\n----------------------------------------\n\nTITLE: Defining Required Teradata Profile Fields in Markdown\nDESCRIPTION: This snippet outlines the required fields for a Teradata profile in dbt, including user, password, schema, and transaction mode. It provides details on each parameter's purpose and its equivalent in the Teradata JDBC Driver.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/teradata-setup.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\nParameter               | Default     | Type           | Description\n----------------------- | ----------- | -------------- | ---\n`user`                  |             | string         | Specifies the database username. Equivalent to the Teradata JDBC Driver `USER` connection parameter.\n`password`              |             | string         | Specifies the database password. Equivalent to the Teradata JDBC Driver `PASSWORD` connection parameter.\n`schema`              |             | string         | Specifies the initial database to use after logon, instead of the user's default database.\n`tmode`                 | `\"ANSI\"` | string         | Specifies the transaction mode. Only `ANSI` mode is currently supported.\n```\n\n----------------------------------------\n\nTITLE: Configuring Time Dimensions with is_partition in dbt Semantic Layer (v1.9+)\nDESCRIPTION: YAML configuration for time dimensions with is_partition flag and time_granularity. This shows how to set up created_at and deleted_at dimensions and their corresponding measures.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/dimensions.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ndimensions: \n  - name: created_at\n    type: time\n    label: \"Date of creation\"\n    expr: ts_created # ts_created is the underlying column name from the table \n    config:\n      meta:\n        notes: \"Only valid for orders from 2022 onward\"\n    is_partition: True\n    type_params:\n      time_granularity: day\n  - name: deleted_at\n    type: time\n    label: \"Date of deletion\"\n    expr: ts_deleted # ts_deleted is the underlying column name from the table\n    is_partition: True \n    type_params:\n      time_granularity: day\n\nmeasures:\n  - name: users_deleted\n    expr: 1\n    agg: sum\n    agg_time_dimension: deleted_at\n  - name: users_created\n    expr: 1\n    agg: sum\n```\n\n----------------------------------------\n\nTITLE: Implementing Incremental Models with Filtering Logic in dbt\nDESCRIPTION: A complete example of an incremental model that includes filtering logic using the is_incremental() macro to process only new records since the last run, and applies a slow function to the data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/incremental-models.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        materialized='incremental'\n    )\n}}\n\nselect\n    *,\n    my_slow_function(my_column)\n\nfrom {{ ref('app_data_events') }}\n\n{% if is_incremental() %}\n\n  -- this filter will only be applied on an incremental run\n  -- (uses >= to include records whose timestamp occurred since the last run of this model)\n  -- (If event_time is NULL or the table is truncated, the condition will always be true and load all records)\nwhere event_time >= (select coalesce(max(event_time),'1900-01-01') from {{ this }} )\n\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Model-Level Schema Configuration\nDESCRIPTION: SQL configuration block to set custom schema for an individual model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/schema.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    schema='marketing'\n) }}\n```\n\n----------------------------------------\n\nTITLE: Complete dbt Project Configuration Example\nDESCRIPTION: Comprehensive example showing all available configurations in the dbt_project.yml file, including project settings, path configurations, model settings, and runtime hooks.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt_project.yml.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nname: string\n\nconfig-version: 2\nversion: version\n\nprofile: profilename\n\nmodel-paths: [directorypath]\nseed-paths: [directorypath]\ntest-paths: [directorypath]\nanalysis-paths: [directorypath]\nmacro-paths: [directorypath]\nsnapshot-paths: [directorypath]\ndocs-paths: [directorypath]\nasset-paths: [directorypath]\n\npackages-install-path: directorypath\n\nclean-targets: [directorypath]\n\nquery-comment: string\n\nrequire-dbt-version: version-range | [version-range]\n\nflags:\n  <global-configs>\n\ndbt-cloud:\n  project-id: project_id # Required\n  defer-env-id: environment_id # Optional\n\nexposures:\n  +enabled: true | false\n\nquoting:\n  database: true | false\n  schema: true | false\n  identifier: true | false\n\nmetrics:\n  <metric-configs>\n\nmodels:\n  <model-configs>\n\nseeds:\n  <seed-configs>\n\nsemantic-models:\n  <semantic-model-configs>\n\nsaved-queries:\n  <saved-queries-configs>\n\nsnapshots:\n  <snapshot-configs>\n\nsources:\n  <source-configs>\n  \ntests:\n  <test-configs>\n\nvars:\n  <variables>\n\non-run-start: sql-statement | [sql-statement]\non-run-end: sql-statement | [sql-statement]\n\ndispatch:\n  - macro_namespace: packagename\n    search_order: [packagename]\n\nrestrict-access: true | false\n```\n\n----------------------------------------\n\nTITLE: Defining Entities and Measures in YAML for dbt Semantic Layer\nDESCRIPTION: This snippet defines entities and measures for a transaction-based semantic model. It includes primary and foreign keys, as well as various aggregations for transaction amounts.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/measures.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n# --- entities ---\n    entities:\n      - name: transaction_id\n        type: primary\n      - name: customer_id\n        type: foreign\n      - name: store_id\n        type: foreign\n      - name: product_id\n        type: foreign\n\n# --- measures ---\n    measures:\n      - name: transaction_amount_usd\n        description: Total usd value of transactions\n        expr: transaction_amount_usd\n        agg: sum\n      - name: transaction_amount_usd_avg\n        description: Average usd value of transactions\n        expr: transaction_amount_usd\n        agg: average\n      - name: transaction_amount_usd_max\n        description: Maximum usd value of transactions\n        expr: transaction_amount_usd\n        agg: max\n      - name: transaction_amount_usd_min\n        description: Minimum usd value of transactions\n        expr: transaction_amount_usd\n        agg: min\n      - name: quick_buy_transactions \n        description: The total transactions bought as quick buy\n        expr: quick_buy_flag \n        agg: sum_boolean \n      - name: distinct_transactions_count\n        description: Distinct count of transactions \n        expr: transaction_id\n        agg: count_distinct\n      - name: transaction_amount_avg \n        description: The average value of transactions \n        expr: transaction_amount_usd\n        agg: average \n      - name: transactions_amount_usd_valid # Notice here how we use expr to compute the aggregation based on a condition\n        description: The total usd value of valid transactions only\n        expr: case when is_valid = True then transaction_amount_usd else 0 end \n        agg: sum\n      - name: transactions\n        description: The average value of transactions.\n        expr: transaction_amount_usd\n        agg: average\n      - name: p99_transaction_value\n        description: The 99th percentile transaction value\n        expr: transaction_amount_usd\n        agg: percentile\n        agg_params:\n          percentile: .99\n          use_discrete_percentile: False # False calculates the continuous percentile, True calculates the discrete percentile.\n      - name: median_transaction_value\n        description: The median transaction value\n        expr: transaction_amount_usd\n        agg: median\n```\n\n----------------------------------------\n\nTITLE: Referencing a source in a dbt SQL model\nDESCRIPTION: This SQL snippet demonstrates how to reference a source in a dbt model using the source() function. It shows the dbt SQL and the resulting compiled SQL after the source reference is resolved.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/schema.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nselect * from {{ source('jaffle_shop', 'orders') }}\n```\n\nLANGUAGE: sql\nCODE:\n```\nselect * from postgres_backend_public_schema.orders\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure DevOps PR Template URL in dbt Cloud\nDESCRIPTION: This snippet shows the PR template URL format for Azure DevOps repositories in dbt Cloud. It includes parameters for the organization, project, repository, and branch references.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/git/pr-template.md#2025-04-09_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://dev.azure.com/<org>/<project>/_git/<repo>/pullrequestcreate?sourceRef={{source}}&targetRef={{destination}}\n```\n\n----------------------------------------\n\nTITLE: Complete Incremental Model in dbt (SQL)\nDESCRIPTION: This comprehensive example demonstrates a complete incremental model in dbt, including the use of the is_incremental() macro for conditional logic.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/migrate-from-stored-procedures.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\nWITH\n\nusing_clause AS (\n\n    SELECT\n        ride_id,\n        subtotal,\n        tip,\n        max(load_timestamp) as load_timestamp\n\n    FROM {{ ref('rides_to_load') }}\n\n\n    {% if is_incremental() %}\n\n        WHERE load_timestamp > (SELECT max(load_timestamp) FROM {{ this }})\n\n    {% endif %}\n\n),\n\nupdates AS (\n\n    SELECT\n        ride_id,\n        subtotal,\n        tip,\n        load_timestamp\n\n    FROM using_clause\n\n    {% if is_incremental() %}\n\n        WHERE ride_id IN (SELECT ride_id FROM {{ this }})\n\n    {% endif %}\n\n),\n\ninserts AS (\n\n    SELECT\n        ride_id,\n        subtotal,\n        NVL(tip, 0, tip),\n        load_timestamp\n\n    FROM using_clause\n\n    WHERE ride_id NOT IN (SELECT ride_id FROM updates)\n\n)\n\nSELECT * FROM updates UNION inserts\n```\n\n----------------------------------------\n\nTITLE: Configuring Column Types in seeds/properties.yml\nDESCRIPTION: Shows an alternative approach to specify column types using a properties.yml file in the seeds directory. Uses the version 2 configuration format.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/column_types.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nseeds:\n  - name: country_codes\n    config:\n      column_types:\n        country_code: varchar(2)\n        country_name: varchar(32)\n```\n\n----------------------------------------\n\nTITLE: Basic DBT Snapshot Structure\nDESCRIPTION: Demonstrates the minimal structure required for a DBT snapshot block.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/snapshots.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot orders_snapshot %}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Defining Policy Tags for Column-Level Security in BigQuery\nDESCRIPTION: Demonstrates how to implement column-level security in BigQuery by setting policy tags as column properties in a YAML configuration file. Note that column-level persist_docs must be enabled for this to work.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/bigquery-configs.md#2025-04-09_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n- name: policy_tag_table\n  columns:\n    - name: field\n      policy_tags:\n        - 'projects/<gcp-project>/locations/<location>/taxonomies/<taxonomy>/policyTags/<tag>'\n```\n\n----------------------------------------\n\nTITLE: Querying Model Contracts and Versions with GraphQL\nDESCRIPTION: This GraphQL query retrieves information about model contracts, versions, and catalog details for public models in a given environment.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/discovery-use-cases-and-examples.md#2025-04-09_snippet_12\n\nLANGUAGE: graphql\nCODE:\n```\nquery {\n  environment(id: 123) {\n    applied {\n      models(first: 100, filter: { access: public }) {\n        edges {\n          node {\n            name\n            latestVersion\n            contractEnforced\n            constraints {\n              name\n              type\n              expression\n              columns\n            }\n            catalog {\n              columns {\n                name\n                type\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring High Availability (HA) Table in dbt\nDESCRIPTION: Demonstrates how to configure a high availability table using Glue Catalog table versions. This approach avoids downtime during table recreation by using a temporary table and location swapping mechanism.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/athena-configs.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized='table',\n    ha=true,\n    format='parquet',\n    table_type='hive',\n    partitioned_by=['status'],\n    s3_data_naming='table_unique'\n) }}\n\nselect 'a' as user_id,\n       'pi'     as user_name,\n       'active' as status\nunion all\nselect 'b'        as user_id,\n       'sh'       as user_name,\n       'disabled' as status\n```\n\n----------------------------------------\n\nTITLE: Returning Data as an Expression in dbt Macro (SQL/Jinja)\nDESCRIPTION: This macro demonstrates how to use the return function as an expression to output a list directly. The returned data can be used in SQL queries or other macros.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/return.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{% macro get_data() %}\n\n  {{ return([1,2,3]) }}\n  \n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Running dbt Tests Command\nDESCRIPTION: Bash command to execute tests on specific models in the dbt project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_41\n\nLANGUAGE: bash\nCODE:\n```\ndbt test --select fastest_pit_stops_by_constructor lap_times_moving_avg\n```\n\n----------------------------------------\n\nTITLE: SQL Model Definition with Potential Contract Violation\nDESCRIPTION: A SQL model that would violate the contract defined in the YAML. The customer_id is defined as a string ('abc123') but the contract requires it to be an integer, which will cause a compilation error.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/contract.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nselect\n  'abc123' as customer_id,\n  'My Best Customer' as customer_name\n```\n\n----------------------------------------\n\nTITLE: DATEADD Function in Databricks SQL\nDESCRIPTION: Shows the syntax for the date_add function in Databricks. It takes startDate and numDays as parameters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/date-functions/sql-dateadd.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\ndate_add( {{ startDate }}, {{ numDays }} )\n```\n\n----------------------------------------\n\nTITLE: Complete Semantic Model Configuration Example\nDESCRIPTION: Demonstrates a comprehensive semantic model configuration with multiple models, including transactions and customers with their respective entities, dimensions, and measures.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/semantic-models.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_models:\n  - name: transaction\n    model: ref('fact_transactions')\n    description: \"Transaction fact table at the transaction level. This table contains one row per transaction and includes the transaction timestamp.\"\n    defaults:\n      agg_time_dimension: transaction_date\n\n    entities:\n      - name: transaction\n        type: primary\n        expr: transaction_id\n      - name: customer\n        type: foreign\n        expr: customer_id\n\n    dimensions:\n      - name: transaction_date\n        type: time\n        type_params:\n          time_granularity: day\n\n      - name: transaction_location\n        type: categorical\n        expr: order_country\n\n    measures:\n      - name: transaction_total\n        description: \"The total value of the transaction.\"\n        agg: sum\n\n      - name: sales\n        description: \"The total sale of the transaction.\"\n        agg: sum\n        expr: transaction_total\n\n      - name: median_sales\n        description: \"The median sale of the transaction.\"\n        agg: median\n        expr: transaction_total\n\n  - name: customers\n    model: ref('dim_customers')\n    description: \"A customer dimension table.\"\n\n    entities:\n      - name: customer\n        type: primary\n        expr: customer_id\n\n    dimensions:\n      - name: first_name\n        type: categorical\n```\n\n----------------------------------------\n\nTITLE: Custom Function Definition in Python Model\nDESCRIPTION: Example of defining and using a custom function within a Python model using Snowpark.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/python-models.md#2025-04-09_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef add_one(x):\n    return x + 1\n\ndef model(dbt, session):\n    dbt.config(materialized=\"table\")\n    temps_df = dbt.ref(\"temperatures\")\n\n    # warm things up just a little\n    df = temps_df.withColumn(\"degree_plus_one\", add_one(temps_df[\"degree\"]))\n    return df\n```\n\n----------------------------------------\n\nTITLE: Using Quoted Column Names in dbt Macro\nDESCRIPTION: Demonstrates the correct way to pass a column name as a string argument to a dbt macro using quotes. The example shows how to use the cents_to_dollars macro with the 'amount' column name properly quoted.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Jinja/quoting-column-names.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{{ cents_to_dollars('amount') }} as amount_usd\n```\n\n----------------------------------------\n\nTITLE: Filtering Customer Names with SQL LIKE Operator\nDESCRIPTION: This SQL query demonstrates the LIKE operator by selecting customers whose first names start with the uppercase letter 'J'. The example uses the Jaffle Shop's customers table and filters records where first_name begins with 'J', then orders results by user_id.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/operators/sql-like.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    user_id,\n    first_name\nfrom {{ ref('customers') }}\nwhere first_name like 'J%'\norder by 1\n```\n\n----------------------------------------\n\nTITLE: Specifying Internally Hosted Tarball Package\nDESCRIPTION: This snippet shows how to specify a package using an internally hosted tarball URL, which is useful for organizations with specific security requirements.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/packages.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\npackages:\n  - tarball: https://codeload.github.com/dbt-labs/dbt-utils/tar.gz/0.9.6\n    name: 'dbt_utils'\n```\n\n----------------------------------------\n\nTITLE: Run Query Result Length Check\nDESCRIPTION: Example showing how to check if run_query returned any results using the length filter within an execute block.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/run_query.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{% if execute %}\n{% set results = run_query(payment_methods_query) %}\n{% if results|length > 0 %}\n  \t-- do something with `results` here...\n{% else %}\n    -- do fallback here...\n{% endif %}\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Configuring BigFrames Submission Method in dbt_project.yml\nDESCRIPTION: YAML configuration to set the BigFrames submission method at the project level in dbt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-bigframes.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  my_dbt_project:\n    submission_method: bigframes\n    python_models:\n      +materialized: view\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Packages in dbt_project.yml\nDESCRIPTION: Example of configuring models, seeds, and variables for a Snowplow package in dbt_project.yml. Demonstrates how to override default configurations and set package-specific variables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/packages.md#2025-04-09_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nvars:\n  snowplow:\n    'snowplow:timezone': 'America/New_York'\n    'snowplow:page_ping_frequency': 10\n    'snowplow:events': \"{{ ref('sp_base_events') }}\"\n    'snowplow:context:web_page': \"{{ ref('sp_base_web_page_context') }}\"\n    'snowplow:context:performance_timing': false\n    'snowplow:context:useragent': false\n    'snowplow:pass_through_columns': []\n\nmodels:\n  snowplow:\n    +schema: snowplow\n\nseeds:\n  snowplow:\n    +schema: snowplow_seeds\n```\n\n----------------------------------------\n\nTITLE: Configuring Timestamp Strategy for dbt Snapshots in YAML (dbt v1.9+)\nDESCRIPTION: YAML configuration for a snapshot using the timestamp strategy. The updated_at field specifies which column contains the last-updated timestamp for detecting changes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/strategy.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n- [name: snapshot_name](/reference/resource-configs/snapshot_name):\n  relation: source('my_source', 'my_table')\n  config:\n    strategy: timestamp\n    updated_at: column_name\n```\n\n----------------------------------------\n\nTITLE: Configuring Hooks in dbt_project.yml\nDESCRIPTION: Basic configuration of pre-hooks and post-hooks in the dbt project configuration file for models, seeds, and snapshots.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/pre-hook-post-hook.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  [<resource-path>]:\n    +pre-hook: SQL-statement | [SQL-statement]\n    +post-hook: SQL-statement | [SQL-statement]\n```\n\n----------------------------------------\n\nTITLE: CLI-style Full Example of YAML Selectors\nDESCRIPTION: A complete example of a complex selector using CLI-style syntax with union, intersection, and exclude operations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/yaml-selectors.md#2025-04-09_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nselectors:\n  - name: nightly_diet_snowplow\n    description: \"Non-incremental Snowplow models that power nightly exports\"\n    definition:\n\n      # Optional `union` and `intersection` keywords map to the ` ` and `,` set operators:\n      union:\n        - intersection:\n            - '@source:snowplow'\n            - 'tag:nightly'\n        - 'models/export'\n        - exclude:\n            - intersection:\n                - 'package:snowplow'\n                - 'config.materialized:incremental'\n            - export_performance_timing\n```\n\n----------------------------------------\n\nTITLE: Authentication Header Format for GraphQL API\nDESCRIPTION: Shows how to format the authentication header using a dbt Cloud service token for GraphQL API requests.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-graphql.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n{\"Authorization\": \"Bearer <SERVICE TOKEN>\"}\n```\n\n----------------------------------------\n\nTITLE: Defining a Cumulative Metric with Grain-to-Date in YAML (v1.9+)\nDESCRIPTION: This example shows how to define a cumulative metric 'orders_last_month_to_date' using a monthly grain-to-date approach in MetricFlow version 1.9 and later.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/cumulative-metrics.md#2025-04-09_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\n- name: orders_last_month_to_date\n  label: Orders month to date\n  type: cumulative\n  type_params:\n    measure: order_count\n    cumulative_type_params:\n      grain_to_date: month\n```\n\n----------------------------------------\n\nTITLE: Running dbt Snapshot Command\nDESCRIPTION: Example of running the 'dbt snapshot' command from the command line and its output. This command creates or updates snapshot tables based on defined snapshot configurations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/snapshots.md#2025-04-09_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n$ dbt snapshot\nRunning with dbt=1.9.0\n\n15:07:36 | Concurrency: 8 threads (target='dev')\n15:07:36 |\n15:07:36 | 1 of 1 START snapshot snapshots.orders_snapshot...... [RUN]\n15:07:36 | 1 of 1 OK snapshot snapshots.orders_snapshot..........[SELECT 3 in 1.82s]\n15:07:36 |\n15:07:36 | Finished running 1 snapshots in 0.68s.\n\nCompleted successfully\n\nDone. PASS=2 ERROR=0 SKIP=0 TOTAL=1\n```\n\n----------------------------------------\n\nTITLE: Implementing Merge Incremental Strategy in dbt-Spark SQL\nDESCRIPTION: Example of an incremental model using the merge strategy with Delta file format. The model processes events data and maintains user_id uniqueness with incremental processing logic.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/spark-configs.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized='incremental',\n    file_format='delta', # or 'iceberg' or 'hudi'\n    unique_key='user_id',\n    incremental_strategy='merge'\n) }}\n\nwith new_events as (\n\n    select * from {{ ref('events') }}\n\n    {% if is_incremental() %}\n    where date_day >= date_add(current_date, -1)\n    {% endif %}\n\n)\n\nselect\n    user_id,\n    max(date_day) as last_seen\n\nfrom events\ngroup by 1\n```\n\n----------------------------------------\n\nTITLE: Custom Snapshot Strategy Configuration in YAML (dbt v1.9+)\nDESCRIPTION: YAML configuration for using a custom-defined snapshot strategy called 'timestamp_with_deletes'. Custom strategies must be implemented as macros in your dbt project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/strategy.md#2025-04-09_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  - name: my_custom_snapshot\n    relation: source('my_source', 'my_table')\n    config:\n      strategy: timestamp_with_deletes\n      updated_at: updated_at_column\n      unique_key: id\n```\n\n----------------------------------------\n\nTITLE: Configuring Table Clustering in BigQuery with dbt\nDESCRIPTION: Demonstrates how to configure table clustering in BigQuery using dbt. Examples are provided for clustering on a single column and multiple columns.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/bigquery-configs.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized = \"table\",\n    cluster_by = \"order_id\",\n)}}\n\nselect * from ...\n```\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized = \"table\",\n    cluster_by = [\"customer_id\", \"order_id\"],\n)}}\n\nselect * from ...\n```\n\n----------------------------------------\n\nTITLE: Retrieving Snowflake PrivateLink Configuration in SQL\nDESCRIPTION: SQL command to retrieve the PrivateLink configuration from a Snowflake account. This function generates the necessary configuration details required for setting up PrivateLink connections.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/secure/snowflake-privatelink.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nUSE ROLE ACCOUNTADMIN;\nSYSTEM$GET_PRIVATELINK_CONFIG;\n```\n\n----------------------------------------\n\nTITLE: Semantic Model with Model Reference\nDESCRIPTION: Demonstrates how to reference a logical model and add description to a semantic model definition.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-build-our-metrics/semantic-layer-3-build-semantic-models.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_models:\n  - name: orders\n    description: |\n      Model containing order data. The grain of the table is the order id.\n    model: ref('stg_orders')\n    entities: ...\n    dimensions: ...\n    measures: ...\n```\n\n----------------------------------------\n\nTITLE: Applying UPPER Function to a Customer Table\nDESCRIPTION: Example of using the UPPER function on a customer's first name field in a SELECT statement with dbt's ref function. This converts only the first_name field to uppercase while leaving other fields unchanged.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/string-functions/sql-upper.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect \n\tcustomer_id,\n\tupper(first_name) as first_name,\n\tlast_name\nfrom {{ ref('customers') }}\n```\n\n----------------------------------------\n\nTITLE: GitHub Actions Workflow for dbt Cloud Job\nDESCRIPTION: GitHub Actions workflow configuration that triggers a dbt Cloud job when changes are pushed to the main branch. Sets up environment variables and executes a Python script to run and monitor the dbt job.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/custom-cicd-pipelines.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\non:\n  push:\n    branches:\n      - 'main'\n\njobs:\n  run_dbt_cloud_job:\n    name: Run dbt Cloud Job\n    runs-on: ubuntu-latest\n\n    env:\n      DBT_ACCOUNT_ID: 00000\n      DBT_PROJECT_ID: 00000\n      DBT_PR_JOB_ID:  00000\n      DBT_API_KEY: ${{ secrets.DBT_API_KEY }}\n      DBT_JOB_CAUSE: 'GitHub Pipeline CI Job'\n      DBT_JOB_BRANCH: ${{ github.ref_name }}\n\n    steps:\n      - uses: \"actions/checkout@v4\"\n      - uses: \"actions/setup-python@v5\"\n        with:\n          python-version: \"3.9\"\n      - name: Run dbt Cloud job\n        run: \"python python/run_and_monitor_dbt_job.py\"\n```\n\n----------------------------------------\n\nTITLE: Relation Management Example\nDESCRIPTION: Demonstrates how to rename relations and handle backups\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/adapter.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n{%- set old_relation = adapter.get_relation(\n      database=this.database,\n      schema=this.schema,\n      identifier=this.identifier) -%}\n\n{%- set backup_relation = adapter.get_relation(\n      database=this.database,\n      schema=this.schema,\n      identifier=this.identifier ~ \"__dbt_backup\") -%}\n\n{% do adapter.rename_relation(old_relation, backup_relation) %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Postgres Profile in profiles.yml\nDESCRIPTION: YAML configuration for setting up a Postgres target in dbt's profiles.yml file. Includes essential connection parameters like host, user, password, port, and database name, along with optional settings for connection behavior.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/postgres-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ncompany-name:\n  target: dev\n  outputs:\n    dev:\n      type: postgres\n      host: [hostname]\n      user: [username]\n      password: [password]\n      port: [port]\n      dbname: [database name] # or database instead of dbname\n      schema: [dbt schema]\n      threads: [optional, 1 or more]\n      [keepalives_idle](#keepalives_idle): 0 # default 0, indicating the system default. See below\n      connect_timeout: 10 # default 10 seconds\n      [retries](#retries): 1  # default 1 retry on error/timeout when opening connections\n      [search_path](#search_path): [optional, override the default postgres search_path]\n      [role](#role): [optional, set the role dbt assumes when executing queries]\n      [sslmode](#sslmode): [optional, set the sslmode used to connect to the database]\n      [sslcert](#sslcert): [optional, set the sslcert to control the certifcate file location]\n      [sslkey](#sslkey): [optional, set the sslkey to control the location of the private key]\n      [sslrootcert](#sslrootcert): [optional, set the sslrootcert config value to a new file path in order to customize the file location that contain root certificates]\n```\n\n----------------------------------------\n\nTITLE: Aliasing a Snapshot in SQL File\nDESCRIPTION: Assigns an alias directly in the snapshots/your_snapshot.sql file using a config block.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/alias.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    alias=\"the_best_snapshot\"\n) }}\n```\n\n----------------------------------------\n\nTITLE: Accessing Groups in dbt Graph\nDESCRIPTION: Example macro demonstrating how to access group information and retrieve group owners using the graph context variable.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/graph.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n{% macro get_group_owner_for(group_name) %}\n\n  {% set groups = graph.groups.values() %}\n  \n  {% set owner = (groups | selectattr('owner', 'equalto', group_name) | list).pop() %}\n\n  {{ return(owner) }}\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Time Dimensions with is_partition in dbt Semantic Layer (v1.8 and earlier)\nDESCRIPTION: YAML configuration for time dimensions with is_partition flag and time_granularity for dbt Semantic Layer v1.8 and earlier.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/dimensions.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\ndimensions: \n  - name: created_at\n    type: time\n    label: \"Date of creation\"\n    expr: ts_created # ts_created is the underlying column name from the table \n    is_partition: True\n    type_params:\n      time_granularity: day\n  - name: deleted_at\n    type: time\n    label: \"Date of deletion\"\n    expr: ts_deleted # ts_deleted is the underlying column name from the table\n    is_partition: True \n    type_params:\n      time_granularity: day\n\nmeasures:\n  - name: users_deleted\n    expr: 1\n    agg: sum\n    agg_time_dimension: deleted_at\n  - name: users_created\n    expr: 1\n    agg: sum\n```\n\n----------------------------------------\n\nTITLE: Configuring Databricks Tables in dbt 1.7\nDESCRIPTION: Configuration options for Databricks tables in dbt v1.7, including file formats, location, partitioning, clustering options, table properties and compression settings.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/databricks-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Option    | Description    | Required?  | Model support | Example     |\n|-----------|---------|-------------------|---------------|-------------|\n| file_format         | The file format to use when creating tables (`parquet`, `delta`, `hudi`, `csv`, `json`, `text`, `jdbc`, `orc`, `hive` or `libsvm`).       | Optional       | SQL, Python   | `delta`   |\n| location_root       | The created table uses the specified directory to store its data. The table alias is appended to it.   | Optional    | SQL, Python   | `/mnt/root`   |\n| partition_by   | Partition the created table by the specified columns. A directory is created for each partition.| Optional | SQL, Python   | `date_day` |\n| liquid_clustered_by | Cluster the created table by the specified columns. Clustering method is based on [Delta's Liquid Clustering feature](https://docs.databricks.com/en/delta/clustering.html). Available since dbt-databricks 1.6.2. | Optional   | SQL           | `date_day`   |\n| clustered_by   | Each partition in the created table will be split into a fixed number of buckets by the specified columns.   | Optional    | SQL, Python   | `country_code`           |\n| buckets    | The number of buckets to create while clustering  | Required if `clustered_by` is specified | SQL, Python   | `8`    |\n| tblproperties   | [Tblproperties](https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-tblproperties.html) to be set on the created table   | Optional          | SQL, Python<sup>*</sup> | `{'this.is.my.key': 12}` |\n| compression      | Set the compression algorithm.    | Optional      | SQL, Python   | `zstd`   |\n```\n\n----------------------------------------\n\nTITLE: Defining Measures in YAML for dbt Semantic Model\nDESCRIPTION: This YAML snippet defines measures for a semantic model in dbt. It includes definitions for order_total and tax_paid, specifying their names, descriptions, and aggregation methods.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-build-our-metrics/semantic-layer-3-build-semantic-models.md#2025-04-09_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nmeasures:\n  - name: order_total\n    description: The total amount for each order including taxes.\n    agg: sum\n  - name: tax_paid\n    description: The total tax paid on each order.\n    agg: sum\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Indexes with Post-Hooks in dbt SQL Model\nDESCRIPTION: SQL model configuration that disables columnstore and uses post-hooks to create clustered and non-clustered indexes on the resulting table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/mssql-configs.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config({\n        \"as_columnstore\": false,\n        \"materialized\": 'table',\n        \"post-hook\": [\n            \"{{ create_clustered_index(columns = ['row_id', 'row_id_complement'], unique=True) }}\",\n            \"{{ create_nonclustered_index(columns = ['modified_date']) }}\",\n            \"{{ create_nonclustered_index(columns = ['row_id'], includes = ['modified_date']) }}\",\n        ]\n    })\n\n}}\n\nselect *\nfrom ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Incremental Models with Predicates for Filtering\nDESCRIPTION: Configuration snippet for an incremental model that uses incremental_predicates to filter rows in both target and source tables. This approach helps optimize merge operations by limiting the scope of data that needs to be processed.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-models-on-databricks.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{\n\nconfig(\n\nmaterialized='incremental',\n\nincremental_strategy = 'merge',\n\nunique_key = 'id',\n\nincremental_predicates = [\n\n\"dbt_internal_target.create_at >= '2023-01-01'\",\t\"dbt_internal_source.create_at >= '2023-01-01'\"],\n\n)\n\n}}\n```\n\n----------------------------------------\n\nTITLE: Using Returned Data in SQL Model (SQL/Jinja)\nDESCRIPTION: This SQL model demonstrates how to use the data returned from a macro (get_data()) in a SELECT statement. It iterates over the returned list to create a comma-separated list of values.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/return.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nselect\n  -- getdata() returns a list!\n  {% for i in get_data() %}\n    {{ i }}\n    {%- if not loop.last %},{% endif -%}\n  {% endfor %}\n```\n\n----------------------------------------\n\nTITLE: Configuring event_time for page_views model in YAML\nDESCRIPTION: YAML configuration that specifies the event_time column for a page_views model. This configuration is crucial for microbatch incremental models to correctly filter time-based data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/incremental-microbatch.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: page_views\n    config:\n      event_time: page_view_start\n```\n\n----------------------------------------\n\nTITLE: Running Modified Models with Slim CI in dbt\nDESCRIPTION: Commands for running and testing only models that have been modified in a PR, by comparing to previous production artifacts. This allows for faster CI processes by limiting scope to changed models and their dependencies.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/best-practice-workflows.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndbt run -s state:modified+ --defer --state path/to/prod/artifacts\ndbt test -s state:modified+ --defer --state path/to/prod/artifacts\n```\n\n----------------------------------------\n\nTITLE: Implementing Incremental Python Model in PySpark\nDESCRIPTION: This snippet shows how to create an incremental Python model using PySpark in dbt. It demonstrates configuring the model, referencing upstream data, and filtering for new rows based on either the maximum timestamp or a rolling window.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/materializations.md#2025-04-09_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport pyspark.sql.functions as F\n\ndef model(dbt, session):\n    dbt.config(materialized = \"incremental\")\n    df = dbt.ref(\"upstream_table\")\n\n    if dbt.is_incremental:\n\n        # only new rows compared to max in current table\n        max_from_this = f\"select max(updated_at) from {dbt.this}\"\n        df = df.filter(df.updated_at >= session.sql(max_from_this).collect()[0][0])\n\n        # or only rows from the past 3 days\n        df = df.filter(df.updated_at >= F.date_add(F.current_timestamp(), F.lit(-3)))\n\n    ...\n\n    return df\n```\n\n----------------------------------------\n\nTITLE: Configuring Enabled Models in dbt YAML\nDESCRIPTION: Example of enabling or disabling models in the dbt_project.yml file. This configuration applies to all models in the specified resource path.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/enabled.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  [<resource-path>]:\n    +enabled: true | false\n```\n\n----------------------------------------\n\nTITLE: Conditional SQL Query Based on Target Name\nDESCRIPTION: A SQL query that demonstrates conditional logic based on the target name. When not in production, it limits the data queried to only records created since the start of the current month.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/custom-target-names.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect *\nfrom a_big_table\n\n-- limit the amount of data queried in dev\n{% if target.name != 'prod' %}\nwhere created_at > date_trunc('month', current_date)\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Model in dbt\nDESCRIPTION: This snippet demonstrates how to set configuration for a Python model in dbt using the dbt.config() method. It shows how to set the materialization strategy to 'table'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndef model(dbt, session):\n\n    # setting configuration\n    dbt.config(materialized=\"table\")\n```\n\n----------------------------------------\n\nTITLE: Creating Staging Model for Orders in SQL\nDESCRIPTION: This SQL snippet creates a staging model for orders, selecting and renaming relevant columns from the jaffle_shop.orders table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/athena-qs.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    id as order_id,\n    user_id as customer_id,\n    order_date,\n    status\n\nfrom jaffle_shop.orders\n```\n\n----------------------------------------\n\nTITLE: Creating a Staging Customer Model in SQL\nDESCRIPTION: This SQL code creates a staging model for customer data, selecting and renaming columns from the raw customers table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/snowflake-qs.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    id as customer_id,\n    first_name,\n    last_name\n\nfrom raw.jaffle_shop.customers\n```\n\n----------------------------------------\n\nTITLE: Creating a Staging Orders Model in SQL\nDESCRIPTION: A SQL query that selects and renames columns from the jaffle_shop_orders table to create a staging orders model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/databricks-qs.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    id as order_id,\n    user_id as customer_id,\n    order_date,\n    status\n\nfrom jaffle_shop_orders\n```\n\n----------------------------------------\n\nTITLE: Defining Model Versions and Contracts in YAML\nDESCRIPTION: Comprehensive YAML configuration defining model versions, contracts, column specifications, and deprecation dates for the fct_orders model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/mesh-qs.md#2025-04-09_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: fct_orders\n    access: public\n    description: \"Customer and order details\"\n    latest_version: 2\n    config:\n      contract:\n        enforced: true\n    columns:\n      - name: order_id\n        data_type: number\n        description: \"\"\n\n      - name: order_date\n        data_type: date\n        description: \"\"\n\n      - name: status\n        data_type: varchar\n        description: \"Indicates the status of the order\"\n\n      - name: is_return\n        data_type: boolean\n        description: \"Indicates if an order was returned\"\n\n      - name: customer_id\n        data_type: number\n        description: \"\"\n\n      - name: first_name\n        data_type: varchar\n        description: \"\"\n\n      - name: last_name\n        data_type: varchar\n        description: \"\"\n\n      - name: first_order_date\n        data_type: date\n        description: \"\"\n\n      - name: days_as_customer_at_purchase\n        data_type: number\n        description: \"Days between this purchase and customer's first purchase\"\n\n    versions:\n    \n      - v: 1\n        deprecation_date: 2024-06-30 00:00:00.00+00:00\n        columns:\n          - include: all\n            exclude: [is_return]\n        \n      - v: 2\n        columns:\n          - include: all\n            exclude: [status]\n```\n\n----------------------------------------\n\nTITLE: Defining Sources in YAML for dbt\nDESCRIPTION: This YAML file defines sources for the dbt project, specifying the database, schema, and tables for the jaffle_shop source. It includes descriptions for better documentation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/bigquery-qs.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n    - name: jaffle_shop\n      description: This is a replica of the Postgres database used by our app\n      database: dbt-tutorial\n      schema: jaffle_shop\n      tables:\n          - name: customers\n            description: One record per customer.\n          - name: orders\n            description: One record per order. Includes cancelled and deleted orders.\n```\n\n----------------------------------------\n\nTITLE: Storing API Secrets in Zapier Storage\nDESCRIPTION: Python code for securely storing dbt Cloud webhook key and Mode API credentials in Zapier's StoreClient. This temporary setup code allows you to reference these secrets in your Zap without exposing them as plaintext.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/zapier-refresh-mode-report.md#2025-04-09_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nstore = StoreClient('abc123') #replace with your UUID secret\nstore.set('DBT_WEBHOOK_KEY', 'abc123') #replace with your dbt Cloud API token\nstore.set('MODE_API_TOKEN', 'abc123') #replace with your Mode API Token\nstore.set('MODE_API_SECRET', 'abc123') #replace with your Mode API Secret\n```\n\n----------------------------------------\n\nTITLE: YAML Surrogate Key Configuration\nDESCRIPTION: Example of creating a surrogate key by combining multiple columns\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/entities.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nentities:\n  - name: brand_target_key # Entity name or identified.\n    type: foreign # This can be any entity type key. \n    expr: date_key || '|' || brand_code # Defines the expression for linking fields to form the surrogate key.\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Database for Models in dbt\nDESCRIPTION: This snippet shows how to specify a custom database for models in your dbt_project.yml file. This configuration will cause the model to be created in the 'reporting' database instead of the default target database.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/database.md#2025-04-09_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nmodels:\n  your_project:\n    sales_metrics:\n      +database: reporting\n```\n\n----------------------------------------\n\nTITLE: Creating a BigQuery Temporary UDF with SQL Header\nDESCRIPTION: This example demonstrates how to use the set_sql_header macro to create a BigQuery Temporary UDF. It creates a function to convert 'yes'/'no' strings to boolean values.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/sql_header.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\n-- Supply a SQL header:\n{% call set_sql_header(config) %}\n  CREATE TEMPORARY FUNCTION yes_no_to_boolean(answer STRING)\n  RETURNS BOOLEAN AS (\n    CASE\n    WHEN LOWER(answer) = 'yes' THEN True\n    WHEN LOWER(answer) = 'no' THEN False\n    ELSE NULL\n    END\n  );\n{%- endcall %}\n\n-- Supply your model code:\n\n\nselect yes_no_to_boolean(yes_no) from {{ ref('other_model') }}\n```\n\n----------------------------------------\n\nTITLE: DBT Snapshot with Source Query\nDESCRIPTION: Shows how to include a select statement within a snapshot block to define the data to be tracked.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/snapshots.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot orders_snapshot %}\n\nselect * from {{ source('jaffle_shop', 'orders') }}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Cumulative Metrics with Window and Grain-to-Date in YAML (v1.9+)\nDESCRIPTION: This example demonstrates how to define cumulative metrics using both a 1-month window and a monthly grain-to-date approach in MetricFlow version 1.9 and later.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/cumulative-metrics.md#2025-04-09_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  - name: cumulative_order_total_l1m\n    label: Cumulative order total (L1M)\n    description: Trailing 1-month cumulative order amount\n    type: cumulative\n    type_params:\n      measure: order_total\n      cumulative_type_params:\n        window: 1 month\n  - name: cumulative_order_total_mtd\n    label: Cumulative order total (MTD)\n    description: The month-to-date value of all orders\n    type: cumulative\n    type_params:\n      measure: order_total\n      cumulative_type_params:\n        grain_to_date: month\n        period_agg: first\n```\n\n----------------------------------------\n\nTITLE: Configuring Snapshots using Config Block\nDESCRIPTION: This snippet shows how to configure snapshots using a config block within a Jinja SQL file. It includes options for enabled, tags, alias, and hooks.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/snapshot-configs.md#2025-04-09_snippet_6\n\nLANGUAGE: jinja\nCODE:\n```\n{{ config(\n    [enabled]=true | false,\n    [tags]=\"<string>\" | [\"<string>\"],\n    [alias]=\"<string>\", \n    [pre_hook]=\"<sql-statement>\" | [\"<sql-statement>\"],\n    [post_hook]=\"<sql-statement>\" | [\"<sql-statement>\"]\n    [persist_docs]={<dict>}\n    [grants]={<dict>}\n) }}\n```\n\n----------------------------------------\n\nTITLE: Updating Python DAG File with dbt Cloud Account and Job IDs\nDESCRIPTION: Python code snippet showing where to add your dbt Cloud account_id and job_id in the Airflow DAG file to connect with your dbt Cloud job.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/airflow-and-dbt-cloud.md#2025-04-09_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# For the dbt Cloud Job URL https://YOUR_ACCESS_URL/#/accounts/16173/projects/36467/jobs/65767/\n# The account_id is 16173 and the job_id is 65767\n# Update lines 34 and 35\nACCOUNT_ID = \"16173\"\nJOB_ID = \"65767\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Databricks Python Model Submission\nDESCRIPTION: Example of configuring a Databricks Python model's submission method using inline model configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/python-models.md#2025-04-09_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef model(dbt, session):\n    dbt.config(\n        submission_method=\"all_purpose_cluster\",\n        create_notebook=True,\n        cluster_id=\"abcd-1234-wxyz\"\n    )\n    ...\n```\n\n----------------------------------------\n\nTITLE: Filtered Source Freshness Check SQL in dbt\nDESCRIPTION: This SQL snippet demonstrates a source freshness check query with an added filter. The filter helps prevent full table scans on large tables by limiting the range of data checked.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/sources.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nselect\n  max(_etl_loaded_at) as max_loaded_at,\n  convert_timezone('UTC', current_timestamp()) as calculated_at\nfrom raw.jaffle_shop.orders\nwhere _etl_loaded_at >= date_sub(current_date(), interval 1 day)\n```\n\n----------------------------------------\n\nTITLE: SQL Model with Type Casting for Contracts in SingleStore\nDESCRIPTION: Shows how to use explicit type casting with the SingleStore ':>' operator to ensure constants match the data types defined in model contracts. This approach prevents type mismatch errors when working with contracts.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/singlestore-configs.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nselect\n  ('abc123' :> int) as customer_id,\n  ('My Best Customer' :> text) as customer_name\n```\n\n----------------------------------------\n\nTITLE: Configuring Dataproc Serverless in profiles.yml for BigQuery\nDESCRIPTION: Example configuration in profiles.yml for running Python models on Dataproc Serverless with BigQuery, including custom container image settings.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/python-models.md#2025-04-09_snippet_24\n\nLANGUAGE: yaml\nCODE:\n```\nmy-profile:\n    target: dev\n    outputs:\n        dev:\n        type: bigquery\n        method: oauth\n        project: abc-123\n        dataset: my_dataset\n        \n        # for dbt Python models to be run on Dataproc Serverless\n        gcs_bucket: dbt-python\n        dataproc_region: us-central1\n        submission_method: serverless\n        dataproc_batch:\n            runtime_config:\n                container_image: {HOSTNAME}/{PROJECT_ID}/{IMAGE}:{TAG}\n```\n\n----------------------------------------\n\nTITLE: Configuring Dataproc Serverless in profiles.yml for BigQuery\nDESCRIPTION: Example configuration in profiles.yml for running Python models on Dataproc Serverless with BigQuery, including custom container image settings.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/python-models.md#2025-04-09_snippet_24\n\nLANGUAGE: yaml\nCODE:\n```\nmy-profile:\n    target: dev\n    outputs:\n        dev:\n        type: bigquery\n        method: oauth\n        project: abc-123\n        dataset: my_dataset\n        \n        # for dbt Python models to be run on Dataproc Serverless\n        gcs_bucket: dbt-python\n        dataproc_region: us-central1\n        submission_method: serverless\n        dataproc_batch:\n            runtime_config:\n                container_image: {HOSTNAME}/{PROJECT_ID}/{IMAGE}:{TAG}\n```\n\n----------------------------------------\n\nTITLE: Defining Model Contract with Column Specifications and Constraints in dbt YAML\nDESCRIPTION: Example of a dbt model YAML file that enforces a contract with column definitions including data types and constraints. This shows how to specify column names, data types, and constraints like 'not_null'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/contract.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: dim_customers\n    config:\n      materialized: table\n      contract:\n        enforced: true\n    columns:\n      - name: customer_id\n        data_type: int\n        constraints:\n          - type: not_null\n      - name: customer_name\n        data_type: string\n      - name: non_integer\n        data_type: numeric(38,3)\n```\n\n----------------------------------------\n\nTITLE: Configuring SCD Type II Semantic Model for Sales Tiers\nDESCRIPTION: YAML configuration for a semantic model that implements an SCD Type II table tracking sales person tiers. Includes time dimension configurations for start and end dates, and natural entity specification for the sales person.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/dimensions.md#2025-04-09_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_models:\n  - name: sales_person_tiers\n    description: SCD Type II table of tiers for salespeople \n    model: {{ ref('sales_person_tiers') }}\n    defaults:\n      agg_time_dimension: tier_start\n\n    dimensions:\n      - name: tier_start\n        type: time\n        label: \"Start date of tier\"\n        expr: start_date\n        type_params:\n          time_granularity: day\n          validity_params:\n            is_start: True\n      - name: tier_end \n        type: time\n        label: \"End date of tier\"\n        expr: end_date\n        type_params:\n          time_granularity: day\n          validity_params:\n            is_end: True\n      - name: tier\n        type: categorical\n\n    primary_entity: sales_person\n\n    entities:\n      - name: sales_person\n        type: natural \n        expr: sales_person_id\n```\n\n----------------------------------------\n\nTITLE: Listing Entities Command in dbt Cloud and Core\nDESCRIPTION: Command syntax for listing all unique entities in both dbt Cloud and dbt Core environments, with options to filter by metrics.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metricflow-commands.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndbt sl list entities --metrics <metric_name> # In dbt Cloud \n\nmf list entities --metrics <metric_name> # In dbt Core\n\nOptions:\n  --metrics SEQUENCE  List entities by given metrics (intersection). Ex. --metrics bookings,messages\n  --help              Show this message and exit.\n```\n\n----------------------------------------\n\nTITLE: Setting Variables via Command Line\nDESCRIPTION: Shows different ways to pass variables to dbt using the --vars command line argument. Includes examples of both JSON and YAML style variable definitions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/project-variables.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ dbt run --vars '{\"key\": \"value\"}'\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ dbt run --vars '{\"key\": \"value\", \"date\": 20180101}'\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ dbt run --vars '{key: value, date: 20180101}'\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ dbt run --vars 'key: value'\n```\n\n----------------------------------------\n\nTITLE: Defining Model Name in dbt YAML Configuration\nDESCRIPTION: Shows the basic structure for declaring a model name in a dbt YAML configuration file. The model name must exactly match the corresponding model filename, including case sensitivity, to ensure proper configuration application and metadata handling in dbt Explorer.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/model_name.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: model_name\n```\n\n----------------------------------------\n\nTITLE: Creating Virtual Environment on Windows\nDESCRIPTION: Commands to create and activate a Python virtual environment on Windows systems, including environment creation, activation, and Python path verification.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/pip-install.md#2025-04-09_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npy -m venv env\n```\n\nLANGUAGE: shell\nCODE:\n```\nenv\\Scripts\\activate\n```\n\nLANGUAGE: shell\nCODE:\n```\nwhere python\n```\n\nLANGUAGE: shell\nCODE:\n```\nenv\\Scripts\\python\n```\n\n----------------------------------------\n\nTITLE: Defining Model Constraints in dbt YAML Configuration\nDESCRIPTION: Example YAML configuration showing how to define constraints at both model and column levels. Includes syntax for primary keys, foreign keys, check constraints, and more. Requires the model contract to be enforced.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/constraints.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: <model_name>\n    \n    # required\n    config:\n      contract: {enforced: true}\n    \n    # model-level constraints\n    constraints:\n      - type: primary_key\n        columns: [first_column, second_column, ...]\n      - type: foreign_key # multi_column\n        columns: [first_column, second_column, ...]\n        to: ref('my_model_to') | source('source', 'source_table')\n        to_columns: [other_model_first_column, other_model_second_columns, ...]\n      - type: check\n        columns: [first_column, second_column, ...]\n        expression: \"first_column != second_column\"\n        name: human_friendly_name\n      - type: ...\n    \n    columns:\n      - name: first_column\n        data_type: string\n        \n        # column-level constraints\n        constraints:\n          - type: not_null\n          - type: unique\n          - type: foreign_key\n            to: ref('my_model_to') | source('source', 'source_table')\n            to_columns: [other_model_column]\n          - type: ...\n```\n\n----------------------------------------\n\nTITLE: Using the 'merge' Incremental Strategy in Vertica\nDESCRIPTION: Example of the 'merge' incremental strategy which matches records based on a unique_key (promotion_key in this case). This strategy updates matching records and inserts new ones, providing a true upsert operation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/vertica-configs.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{{ config( materialized = 'incremental', incremental_strategy = 'merge',  unique_key='promotion_key'   )  }}\n      \n      \n          select * FROM  public.promotion_dimension\n```\n\nLANGUAGE: sql\nCODE:\n```\n      merge into \"VMart\".\"public\".\"samp\" as DBT_INTERNAL_DEST using \"samp__dbt_tmp\" as DBT_INTERNAL_SOURCE\n          on DBT_INTERNAL_DEST.\"promotion_key\" = DBT_INTERNAL_SOURCE.\"promotion_key\"\n  \n        when matched then update set\n        \"promotion_key\" = DBT_INTERNAL_SOURCE.\"promotion_key\", \"price_reduction_type\" = DBT_INTERNAL_SOURCE.\"price_reduction_type\", \"promotion_media_type\" = DBT_INTERNAL_SOURCE.\"promotion_media_type\", \"display_type\" = DBT_INTERNAL_SOURCE.\"display_type\", \"coupon_type\" = DBT_INTERNAL_SOURCE.\"coupon_type\", \"ad_media_name\" = DBT_INTERNAL_SOURCE.\"ad_media_name\", \"display_provider\" = DBT_INTERNAL_SOURCE.\"display_provider\", \"promotion_cost\" = DBT_INTERNAL_SOURCE.\"promotion_cost\", \"promotion_begin_date\" = DBT_INTERNAL_SOURCE.\"promotion_begin_date\", \"promotion_end_date\" = DBT_INTERNAL_SOURCE.\"promotion_end_date\"\n        \n        when not matched then insert\n          (\"promotion_key\", \"price_reduction_type\", \"promotion_media_type\", \"display_type\", \"coupon_type\",\n           \"ad_media_name\", \"display_provider\", \"promotion_cost\", \"promotion_begin_date\", \"promotion_end_date\")\n        values\n        (\n          DBT_INTERNAL_SOURCE.\"promotion_key\", DBT_INTERNAL_SOURCE.\"price_reduction_type\", DBT_INTERNAL_SOURCE.\"promotion_media_type\", DBT_INTERNAL_SOURCE.\"display_type\", DBT_INTERNAL_SOURCE.\"coupon_type\", DBT_INTERNAL_SOURCE.\"ad_media_name\", DBT_INTERNAL_SOURCE.\"display_provider\", DBT_INTERNAL_SOURCE.\"promotion_cost\", DBT_INTERNAL_SOURCE.\"promotion_begin_date\", DBT_INTERNAL_SOURCE.\"promotion_end_date\"\n        )\n```\n\n----------------------------------------\n\nTITLE: MetricFlow Complex Metric Definition\nDESCRIPTION: YAML configuration for defining complex metrics with ratio calculations and filters for returning customers.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/about-metricflow.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  - name: food_order_pct_of_order_total_returning\n    description: Revenue from food orders from returning customers\n    label: \"Food % of Order Total\"\n    type: ratio\n    type_params:\n      numerator: food_order\n      denominator: order_total\n    filter: |\n      {{ Dimension('customer__is_new_customer') }} = false\n```\n\n----------------------------------------\n\nTITLE: Customizing Batch Size for Seed Files\nDESCRIPTION: A macro to customize the batch size for seed file processing to avoid the header line length limit in Python's HTTP client. This helps when dealing with large seed files by adjusting how many rows are handled in a single prepared statement.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/trino-configs.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{% macro trino__get_batch_size() %}\n  {{ return(10000) }} -- Adjust this number as you see fit\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Access Inline in SQL Model Definition\nDESCRIPTION: Sets the access level directly in a SQL model file using the config() macro. This approach configures the model to be publicly accessible to any group, package, or project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/access.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n-- models/my_public_model.sql\n\n{{ config(access = \"public\") }}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Creating Base Deleted Customers Model in dbt for Staging Layer\nDESCRIPTION: Creates a base model for deleted customers from the jaffle_shop source. This extracts customer deletion data that will be joined with the main customer model in the staging layer.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-structure/2-staging.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n-- base_jaffle_shop__deleted_customers.sql\n\nwith\n\nsource as (\n\n    select * from {{ source('jaffle_shop','customer_deletes') }}\n\n),\n\ndeleted_customers as (\n\n    select\n        id as customer_id,\n        deleted as deleted_at\n\n    from source\n\n)\n\nselect * from deleted_customers\n```\n\n----------------------------------------\n\nTITLE: Specifying Packages in YAML Configuration\nDESCRIPTION: This snippet shows how to specify different types of packages in the packages.yml or dependencies.yml file of a dbt project. It includes examples for hub packages, git packages, and local packages.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/packages.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\npackages:\n  - package: dbt-labs/snowplow\n    version: 0.7.0\n\n  - git: \"https://github.com/dbt-labs/dbt-utils.git\"\n    revision: 0.9.2\n\n  - local: /opt/dbt/redshift\n```\n\n----------------------------------------\n\nTITLE: Executing dbt Build Command\nDESCRIPTION: Example output of running 'dbt build' command showing the execution of seeds, models, tests and snapshots with timing and status information. The example demonstrates successful execution of 1 seed, 1 view model, 4 tests, and 1 snapshot.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/build.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ dbt build\nRunning with dbt=1.9.0-b2\nFound 1 model, 4 tests, 1 snapshot, 1 analysis, 341 macros, 0 operations, 1 seed file, 2 sources, 2 exposures\n\n18:49:43 | Concurrency: 1 threads (target='dev')\n18:49:43 |\n18:49:43 | 1 of 7 START seed file dbt_jcohen.my_seed............................ [RUN]\n18:49:43 | 1 of 7 OK loaded seed file dbt_jcohen.my_seed........................ [INSERT 2 in 0.09s]\n18:49:43 | 2 of 7 START view model dbt_jcohen.my_model.......................... [RUN]\n18:49:43 | 2 of 7 OK created view model dbt_jcohen.my_model..................... [CREATE VIEW in 0.12s]\n18:49:43 | 3 of 7 START test not_null_my_seed_id................................ [RUN]\n18:49:43 | 3 of 7 PASS not_null_my_seed_id...................................... [PASS in 0.05s]\n18:49:43 | 4 of 7 START test unique_my_seed_id.................................. [RUN]\n18:49:43 | 4 of 7 PASS unique_my_seed_id........................................ [PASS in 0.03s]\n18:49:43 | 5 of 7 START snapshot snapshots.my_snapshot.......................... [RUN]\n18:49:43 | 5 of 7 OK snapshotted snapshots.my_snapshot.......................... [INSERT 0 5 in 0.27s]\n18:49:43 | 6 of 7 START test not_null_my_model_id............................... [RUN]\n18:49:43 | 6 of 7 PASS not_null_my_model_id..................................... [PASS in 0.03s]\n18:49:43 | 7 of 7 START test unique_my_model_id................................. [RUN]\n18:49:43 | 7 of 7 PASS unique_my_model_id....................................... [PASS in 0.02s]\n18:49:43 |\n18:49:43 | Finished running 1 seed, 1 view model, 4 tests, 1 snapshot in 1.01s.\n\nCompleted successfully\n\nDone. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Row Limits in dbt Semantic Layer Queries\nDESCRIPTION: Example of how to override the default 100 row limit in dbt Semantic Layer queries using the --limit option. This is useful when more data is needed during development.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metricflow-commands.md#2025-04-09_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\ndbt sl list metrics --limit 1000\n```\n\n----------------------------------------\n\nTITLE: Enabling Automatic Clustering in Project YAML for Snowflake\nDESCRIPTION: Project-level configuration to enable automatic clustering for Snowflake tables. This only has an effect if manual clustering is still enabled for your Snowflake account.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snowflake-configs.md#2025-04-09_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  +automatic_clustering: true\n```\n\n----------------------------------------\n\nTITLE: Data Transformation with Python in dbt-oracle\nDESCRIPTION: Advanced Python model that executes a SQL query, performs data transformations to calculate anomaly scores, and adds a new column to the DataFrame.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/oracle-setup.md#2025-04-09_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndef model(dbt, session):\n    dbt.config(materialized=\"table\")\n    dbt.config(async_flag=True)\n    dbt.config(timeout=1800)\n\n    sql = f\"\"\"SELECT customer.cust_first_name,\n       customer.cust_last_name,\n       customer.cust_gender,\n       customer.cust_marital_status,\n       customer.cust_street_address,\n       customer.cust_email,\n       customer.cust_credit_limit,\n       customer.cust_income_level\n    FROM sh.customers customer, sh.countries country\n    WHERE country.country_iso_code = ''US''\n    AND customer.country_id = country.country_id\"\"\"\n\n    # session.sync(query) will run the sql query and returns a oml.core.DataFrame\n    us_potential_customers = session.sync(query=sql)\n\n    # Compute an ad-hoc anomaly score on the credit limit\n    median_credit_limit = us_potential_customers[\"CUST_CREDIT_LIMIT\"].median()\n    mean_credit_limit = us_potential_customers[\"CUST_CREDIT_LIMIT\"].mean()\n    anomaly_score = (us_potential_customers[\"CUST_CREDIT_LIMIT\"] - median_credit_limit)/(median_credit_limit - mean_credit_limit)\n\n    # Add a new column \"CUST_CREDIT_ANOMALY_SCORE\"\n    us_potential_customers = us_potential_customers.concat({\"CUST_CREDIT_ANOMALY_SCORE\": anomaly_score.round(3)})\n\n    # Return potential customers dataset as a oml.core.DataFrame\n    return us_potential_customers\n```\n\n----------------------------------------\n\nTITLE: Configuring Automatic Registration of External Models in dbt-duckdb YAML\nDESCRIPTION: Demonstrates how to configure automatic registration of upstream external models in dbt-duckdb. This is useful when using :memory: as the DuckDB database to ensure external models are available for dependent models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/duckdb-configs.md#2025-04-09_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\non-run-start:\n  - \"{{ register_upstream_external_models() }}\"\n```\n\n----------------------------------------\n\nTITLE: Creating First Python Model with BigFrames in dbt\nDESCRIPTION: Python code for a dbt model using BigFrames submission method, referencing a SQL model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-bigframes.md#2025-04-09_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef model(dbt, session):\n    dbt.config(submission_method=\"bigframes\")\n    bdf = dbt.ref(\"my_sql_model\") #loading from prev step\n    return bdf\n```\n\n----------------------------------------\n\nTITLE: Configuring Layer BigQuery Profile in YAML\nDESCRIPTION: Sample profiles.yml configuration for Layer BigQuery integration, including Layer authentication and BigQuery connection settings.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/layer-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlayer-profile:\n  target: dev\n  outputs:\n    dev:\n      # Layer authentication\n      type: layer_bigquery\n      layer_api_key: [the API Key to access your Layer account (opt)]\n      # Bigquery authentication\n      method: service-account\n      project: [GCP project id]\n      dataset: [the name of your dbt dataset]\n      threads: [1 or more]\n      keyfile: [/path/to/bigquery/keyfile.json]\n```\n\n----------------------------------------\n\nTITLE: Basic Ratio Metric Example\nDESCRIPTION: A simple example of a ratio metric that calculates the ratio of food orders to total orders. This implementation uses the basic structure with just numerator and denominator references to existing metrics.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/ratio-metrics.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  - name: food_order_pct\n    description: \"The food order count as a ratio of the total order count\"\n    label: Food order ratio\n    type: ratio\n    type_params: \n      numerator: food_orders\n      denominator: orders\n```\n\n----------------------------------------\n\nTITLE: Running Full Refresh for Incremental Model in dbt\nDESCRIPTION: Command to force dbt to rebuild an incremental model from scratch using the --full-refresh flag, including downstream models with the + operator.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/incremental-models.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ dbt run --full-refresh --select my_incremental_model+\n```\n\n----------------------------------------\n\nTITLE: Setting Unique Primary Index in Seed Configuration for Teradata\nDESCRIPTION: Configuring a unique primary index on the GlobalID column for a seed table in Teradata to optimize query performance and enforce uniqueness.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/teradata-configs.md#2025-04-09_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  <project-name>:\n    index: \"UNIQUE PRIMARY INDEX ( GlobalID )\"\n```\n\n----------------------------------------\n\nTITLE: Logging Batch Details Example\nDESCRIPTION: Practical example of using model.batch to log batch details including ID and event time range.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/model.md#2025-04-09_snippet_4\n\nLANGUAGE: jinja\nCODE:\n```\n{% if model.batch %}\n  {{ log(\"Processing batch with ID: \" ~ model.batch.id, info=True) }}\n  {{ log(\"Batch event time range: \" ~ model.batch.event_time_start ~ \" to \" ~ model.batch.event_time_end, info=True) }}\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Fact Tables with Jinja Config Block in Firebolt\nDESCRIPTION: In-file configuration using Jinja syntax to define a Firebolt fact table with primary index and aggregating indexes directly in a SQL model file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/firebolt-configs.md#2025-04-09_snippet_3\n\nLANGUAGE: jinja\nCODE:\n```\n{{ config(\n    materialized = \"table\"\n    table_type = \"fact\"\n    primary_index = [ \"<column-name>\", ... ],\n    indexes = [\n      {\n        \"index_type\": \"aggregating\"\n        \"key_columns\": [ \"<column-name>\", ... ],\n        \"aggregation\": [ \"<agg-sql>\", ... ],\n      },\n      ...\n    ]\n) }}\n```\n\n----------------------------------------\n\nTITLE: Implementing Adapter Dispatch Pattern in Jinja2 for dbt\nDESCRIPTION: Demonstrates the adapter dispatch pattern that allows dbt to call database-specific implementations of common SQL operations. It shows the main dispatch macro and examples of default, Redshift, and BigQuery implementations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_12\n\nLANGUAGE: jinja2\nCODE:\n```\n{# dbt will call this macro by name, providing any arguments #}\n{% macro create_table_as(temporary, relation, sql) -%}\n\n  {# dbt will dispatch the macro call to the relevant macro #}\n  {{ return(\n      adapter.dispatch('create_table_as')(temporary, relation, sql)\n     ) }}\n{%- endmacro %}\n\n\n\n{# If no macro matches the specified adapter, \"default\" will be used #}\n{% macro default__create_table_as(temporary, relation, sql) -%}\n   ...\n{%- endmacro %}\n\n\n\n{# Example which defines special logic for Redshift #}\n{% macro redshift__create_table_as(temporary, relation, sql) -%}\n   ...\n{%- endmacro %}\n\n\n\n{# Example which defines special logic for BigQuery #}\n{% macro bigquery__create_table_as(temporary, relation, sql) -%}\n   ...\n{%- endmacro %}\n```\n\n----------------------------------------\n\nTITLE: SQL Join Pattern for SCD Type II Dimensions in dbt Semantic Layer\nDESCRIPTION: SQL query showing how metrics are joined with SCD Type II dimension tables using valid_from and valid_to time ranges.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/dimensions.md#2025-04-09_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\nselect metric_time, dimensions_1, sum(1) as num_events\nfrom events a\nleft outer join scd_dimensions b\non \n  a.entity_key = b.entity_key \n  and a.metric_time >= b.valid_from \n  and (a.metric_time < b. valid_to or b.valid_to is null)\ngroup by 1, 2\n```\n\n----------------------------------------\n\nTITLE: Default generate_schema_name Macro Implementation in SQL/Jinja\nDESCRIPTION: This snippet shows the default implementation of the generate_schema_name macro in dbt. It concatenates the default schema with a custom schema name if provided.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/debug-schema-names.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{% macro generate_schema_name(custom_schema_name, node) -%}\n\n    {%- set default_schema = target.schema -%}\n    {%- if custom_schema_name is none -%}\n\n        {{ default_schema }}\n\n    {%- else -%}\n\n        {{ default_schema }}_{{ custom_schema_name | trim }}\n\n    {%- endif -%}\n\n{%- endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Creating a Daily Time Spine in SQL\nDESCRIPTION: Creates a time spine table at day granularity using dbt's date_spine macro. The table generates days from January 1, 2000, to January 1, 2025, but filters to only include dates from 4 years ago to 30 days in the future.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metricflow-time-spine.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        materialized = 'table',\n    )\n}}\n\nwith days as (\n\n    {{\n        dbt.date_spine(\n            'day',\n            \"to_date('01/01/2000','mm/dd/yyyy')\",\n            \"to_date('01/01/2025','mm/dd/yyyy')\"\n        )\n    }}\n\n),\n\nfinal as (\n    select cast(date_day as date) as date_day\n    from days\n)\n\nselect * from final\nwhere date_day > dateadd(year, -4, current_timestamp()) \nand date_day < dateadd(day, 30, current_timestamp())\n```\n\n----------------------------------------\n\nTITLE: Microbatch Strategy Implementation in Databricks SQL\nDESCRIPTION: Demonstrates the microbatch strategy implementation using replace_where, including event time handling and date-based processing.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/databricks-configs.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized='incremental',\n    file_format='delta',\n    incremental_strategy = 'microbatch'\n    event_time='date' # Use 'date' as the grain for this microbatch table\n) }}\n\nwith new_events as (\n\n    select * from {{ ref('events') }}\n\n)\n\nselect\n    user_id,\n    date,\n    count(*) as visits\n\nfrom events\ngroup by 1, 2\n```\n\n----------------------------------------\n\nTITLE: Cents to Dollars Conversion Macro\nDESCRIPTION: Macro definition that converts cent amounts to dollars with configurable decimal scale.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/jinja-macros.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{% macro cents_to_dollars(column_name, scale=2) %}\n    ({{ column_name }} / 100)::numeric(16, {{ scale }})\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Dimension Tables with Jinja Config Block in Firebolt\nDESCRIPTION: In-file configuration using Jinja syntax to define a Firebolt dimension table directly in a SQL model file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/firebolt-configs.md#2025-04-09_snippet_7\n\nLANGUAGE: jinja\nCODE:\n```\n{{ config(\n    materialized = \"table\",\n    table_type = \"dimension\",\n    ...\n) }}\n```\n\n----------------------------------------\n\nTITLE: List Aggregation in SQL with dbt\nDESCRIPTION: Macro to concatenate group values with a delimiter, optional ordering and limit.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/cross-database-macros.md#2025-04-09_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt.listagg(measure=\"column_to_agg\", delimiter_text=\"','\", order_by_clause=\"order by order_by_column\", limit_num=10) }}\n```\n\n----------------------------------------\n\nTITLE: Querying Dimensions with Entity Specification\nDESCRIPTION: Example demonstrating how to query a dimension (is_food_order) with the primary entity specification (order_id). Shows how to group the order_total metric by the is_food_order dimension to filter orders.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metricflow-commands.md#2025-04-09_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ndbt sl query --metrics order_total --group-by order_id__is_food_order # In dbt Cloud\n\nmf query --metrics order_total --group-by order_id__is_food_order # In dbt Core\n```\n\nLANGUAGE: bash\nCODE:\n```\n Success  - query completed after 1.70 seconds\n| METRIC_TIME   | IS_FOOD_ORDER   |   ORDER_TOTAL |\n|:--------------|:----------------|---------------:|\n| 2017-06-16    | True            |         499.27 |\n| 2017-06-16    | False           |         292.90 |\n| 2017-06-17    | True            |         431.24 |\n| 2017-06-17    | False           |          27.11 |\n| 2017-06-18    | True            |         466.45 |\n| 2017-06-18    | False           |          24.24 |\n| 2017-06-19    | False           |         300.98 |\n| 2017-06-19    | True            |         448.11 |\n```\n\n----------------------------------------\n\nTITLE: Implementing Date Addition Macros with Adapter Dispatch in dbt\nDESCRIPTION: This SQL snippet demonstrates how to implement date addition macros with adapter dispatch in dbt, including default, postgres, and redshift-specific implementations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/dispatch.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\n{% macro dateadd(datepart, interval, from_date_or_timestamp) %}\n    {{ return(adapter.dispatch('dateadd')(datepart, interval, from_date_or_timestamp)) }}\n{% endmacro %}\n\n{% macro default__dateadd(datepart, interval, from_date_or_timestamp) %}\n    dateadd({{ datepart }}, {{ interval }}, {{ from_date_or_timestamp }})\n{% endmacro %}\n\n{% macro postgres__dateadd(datepart, interval, from_date_or_timestamp) %}\n    {{ from_date_or_timestamp }} + ((interval '1 {{ datepart }}') * ({{ interval }}))\n{% endmacro %}\n\n{# Use default syntax instead of postgres syntax #}\n{% macro redshift__dateadd(datepart, interval, from_date_or_timestamp) %}\n    {{ return(default__dateadd(datepart, interval, from_date_or_timestamp) }}\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Accessing invocation_args_dict and dbt_metadata_envs in SQL (dbt)\nDESCRIPTION: This snippet shows how to access and print the `invocation_args_dict` and `dbt_metadata_envs` variables in a dbt SQL model. These variables provide information about the CLI invocation and environment variables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/flags.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n-- invocation_args_dict:\n-- {{ invocation_args_dict }}\n\n-- dbt_metadata_envs:\n-- {{ dbt_metadata_envs }}\n\nselect 1 as id\n```\n\n----------------------------------------\n\nTITLE: Configuring Dataproc Serverless for Python Models in BigQuery\nDESCRIPTION: Configuration to run dbt Python models using Dataproc Serverless instead of a persistent cluster. This includes detailed specification of batch parameters, network configuration, and resource allocation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/bigquery-setup.md#2025-04-09_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nmy-profile:\n  target: dev\n  outputs:\n    dev:\n      type: bigquery\n      method: oauth\n      project: abc-123\n      dataset: my_dataset\n      \n      # for dbt Python models to be run on Dataproc Serverless\n      gcs_bucket: dbt-python\n      dataproc_region: us-central1\n      submission_method: serverless\n      dataproc_batch:\n        batch_id: MY_CUSTOM_BATCH_ID # Supported in v1.7+\n        environment_config:\n          execution_config:\n            service_account: dbt@abc-123.iam.gserviceaccount.com\n            subnetwork_uri: regions/us-central1/subnetworks/dataproc-dbt\n        labels:\n          project: my-project\n          role: dev\n        runtime_config:\n          properties:\n            spark.executor.instances: \"3\"\n            spark.driver.memory: 1g\n```\n\n----------------------------------------\n\nTITLE: Creating Denormalized Facts Table in SQL - fct_results.sql\nDESCRIPTION: Creates a denormalized mega table combining race results with circuit information and pit stop data. Joins multiple staging tables while maintaining proper granularity to avoid fanout issues.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_20\n\nLANGUAGE: sql\nCODE:\n```\nwith int_results as (\n\n    select * from {{ ref('int_results') }}\n\n),\n\nint_pit_stops as (\n    select\n        race_id,\n        driver_id,\n        max(total_pit_stops_per_race) as total_pit_stops_per_race\n    from {{ ref('int_pit_stops') }}\n    group by 1,2\n),\n\ncircuits as (\n\n    select * from {{ ref('stg_f1_circuits') }}\n),\nbase_results as (\n    select\n        result_id,\n        int_results.race_id,\n        race_year,\n        race_round,\n        int_results.circuit_id,\n        int_results.circuit_name,\n        circuit_ref,\n        location,\n        country,\n        latitude,\n        longitude,\n        altitude,\n        total_pit_stops_per_race,\n        race_date,\n        race_time,\n        int_results.driver_id,\n        driver,\n        driver_number,\n        drivers_age_years,\n        driver_nationality,\n        constructor_id,\n        constructor_name,\n        constructor_nationality,\n        grid,\n        position,\n        position_text,\n        position_order,\n        points,\n        laps,\n        results_time_formatted,\n        results_milliseconds,\n        fastest_lap,\n        results_rank,\n        fastest_lap_time_formatted,\n        fastest_lap_speed,\n        status_id,\n        status,\n        dnf_flag\n    from int_results\n    left join circuits\n        on int_results.circuit_id=circuits.circuit_id\n    left join int_pit_stops\n        on int_results.driver_id=int_pit_stops.driver_id and int_results.race_id=int_pit_stops.race_id\n)\n\nselect * from base_results\n```\n\n----------------------------------------\n\nTITLE: Using Jinja in YAML Files for dbt Configuration\nDESCRIPTION: Demonstrates the usage of Jinja, vars, and env_var in various dbt YAML configuration files. Explains limitations and best practices for different file types.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/dbt-tips.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# Example of using vars in schema.yml\nmodels:\n  - name: my_model\n    columns:\n      - name: {{ var('column_name') }}\n        tests:\n          - unique\n\n# Example of using env_var in profiles.yml\nmy_profile:\n  target: dev\n  outputs:\n    dev:\n      type: postgres\n      host: {{ env_var('DB_HOST') }}\n      port: 5432\n      user: {{ env_var('DB_USER') }}\n      pass: {{ env_var('DBT_ENV_SECRET_DB_PASSWORD') }}\n      dbname: my_database\n      schema: my_schema\n```\n\n----------------------------------------\n\nTITLE: Basic SQL Model Definition\nDESCRIPTION: Initial SQL model definition for dim_customers with customer_id and country_name columns.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/govern/model-versions.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n-- lots of sql\n\nfinal as (\n  \n    select\n        customer_id,\n        country_name\n    from ...\n\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Generating YAML for a Single dbt Model using Codegen\nDESCRIPTION: This command uses the dbt Codegen package to generate YAML documentation for a single model named 'activity_based_interest_activated'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-04-generating-dynamic-docs.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndbt run-operation generate_model_yaml --args '{\"model_names\": [ \"activity_based_interest_activated\"] }'\n```\n\n----------------------------------------\n\nTITLE: Required Structure for Window-Based Cumulative Metrics in SQL\nDESCRIPTION: Example showing the required structure when using a window in cumulative metric definitions. The metric_time dimension must be included in the SQL query since the accumulation window is based on metric time.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/cumulative-metrics.md#2025-04-09_snippet_16\n\nLANGUAGE: sql\nCODE:\n```\nselect\n  count(distinct subq_3.distinct_users) as weekly_active_users,\n  subq_3.metric_time\nfrom (\n  select\n    subq_2.distinct_users as distinct_users,\n    subq_1.metric_time as metric_time\ngroup by\n  subq_3.metric_time\n```\n\n----------------------------------------\n\nTITLE: Configuring Iceberg Table Format in dbt for Snowflake\nDESCRIPTION: This YAML configuration snippet shows how to set up an Iceberg table format for a dbt model in Snowflake. It specifies the materialization type as 'table' and sets the table_type to 'iceberg'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-10-04-iceberg-is-an-implementation-detail.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{{ config(\n    materialized='table',\n    snowflake_table_type='iceberg'\n) }}\n```\n\n----------------------------------------\n\nTITLE: Pulling dbt Docker Image from GitHub Packages\nDESCRIPTION: This command pulls a dbt Docker image from GitHub Packages. Replace <db_adapter_name> with the desired database adapter and <version_tag> with the appropriate version or 'latest'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/docker-install.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull ghcr.io/dbt-labs/<db_adapter_name>:<version_tag>\n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Insert Job in dbt for Upsolver\nDESCRIPTION: This snippet demonstrates how to configure a SQL insert job in dbt for Upsolver. It uses the 'incremental' materialization with 'insert' strategy and allows specifying options like sync and primary_key.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/upsolver-configs.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(  materialized='incremental',\n            sync=True|False,\n            map_columns_by_name=True|False,\n            incremental_strategy='insert',\n            options={\n              'option_name': 'option_value'\n            },\n            primary_key=[{}]\n          )\n}}\nSELECT ...\nFROM {{ ref(<model>) }}\nWHERE ...\nGROUP BY ...\nHAVING COUNT(DISTINCT orderid::string) ...\n```\n\n----------------------------------------\n\nTITLE: Configuring External Sources in dbt-duckdb YAML\nDESCRIPTION: Shows how to configure external sources in dbt-duckdb using YAML. It demonstrates setting up external locations for Parquet files and handling special cases for individual tables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/duckdb-configs.md#2025-04-09_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  - name: external_source\n    meta:\n      external_location: \"s3://my-bucket/my-sources/{name}.parquet\"\n    tables:\n      - name: source1\n      - name: source2\n        config:\n          external_location: \"read_parquet(['s3://my-bucket/my-sources/source2a.parquet', 's3://my-bucket/my-sources/source2b.parquet'])\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Snowflake Warehouse\nDESCRIPTION: SQL command to create a new warehouse in Snowflake with XSMALL size. This warehouse will be used for data processing operations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\ncreate or replace warehouse COMPUTE_WH with warehouse_size=XSMALL\n```\n\n----------------------------------------\n\nTITLE: Querying Predicted Driver Positions in SQL\nDESCRIPTION: This SQL query retrieves the predicted positions alongside the feature variables used for prediction, ordered by the predicted position to show potential podium finishers.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_36\n\nLANGUAGE: sql\nCODE:\n```\nselect * from {{ ref('predict_position') }} order by position_predicted\n```\n\n----------------------------------------\n\nTITLE: Building Final Customers Model in dbt\nDESCRIPTION: This SQL snippet creates a final customers model by joining the staged customers and orders data. It calculates order statistics and uses dbt's ref function to reference other models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/microsoft-fabric-qs.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nwith customers as (\n\n    select * from {{ ref('stg_customers') }}\n\n),\n\norders as (\n\n    select * from {{ ref('stg_orders') }}\n\n),\n\ncustomer_orders as (\n\n    select\n        customer_id,\n\n        min(order_date) as first_order_date,\n        max(order_date) as most_recent_order_date,\n        count(order_id) as number_of_orders\n\n    from orders\n\n    group by customer_id\n\n),\n\nfinal as (\n\n    select\n        customers.customer_id,\n        customers.first_name,\n        customers.last_name,\n        customer_orders.first_order_date,\n        customer_orders.most_recent_order_date,\n        coalesce(customer_orders.number_of_orders, 0) as number_of_orders\n\n    from customers\n\n    left join customer_orders on customers.customer_id = customer_orders.customer_id\n\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Incremental Iceberg Table with Update Condition\nDESCRIPTION: Sets up an incremental Iceberg table with a merge strategy that includes an update condition. This example shows how to conditionally update records based on ID values using a custom filter.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/athena-configs.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n        materialized='incremental',\n        incremental_strategy='merge',\n        unique_key=['id'],\n        update_condition='target.id > 1',\n        schema='sandbox'\n    )\n}}\n\n{% if is_incremental() %}\n\nselect * from (\n    values\n    (1, 'v1-updated')\n    , (2, 'v2-updated')\n) as t (id, value)\n\n{% else %}\n\nselect * from (\n    values\n    (-1, 'v-1')\n    , (0, 'v0')\n    , (1, 'v1')\n    , (2, 'v2')\n) as t (id, value)\n\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Configuring CSV External Source with Special Handling in dbt-duckdb YAML\nDESCRIPTION: Illustrates configuring an external CSV source with special handling in dbt-duckdb. It shows how to specify custom CSV reading options and use the oldstyle formatter to avoid string formatting conflicts.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/duckdb-configs.md#2025-04-09_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  - name: flights_source\n    tables:\n      - name: flights\n        config:\n          external_location: \"read_csv('flights.csv', types={'FlightDate': 'DATE'}, names=['FlightDate', 'UniqueCarrier'])\"\n          formatter: oldstyle\n```\n\n----------------------------------------\n\nTITLE: Referencing sources in SQL models\nDESCRIPTION: Example SQL model showing how to reference source tables using the source function. The model joins the 'customers' and 'orders' tables from the 'jaffle_shop' source.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/source.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nselect\n  ...\n\nfrom {{ source('jaffle_shop', 'customers') }}\n\nleft join {{ source('jaffle_shop', 'orders') }} using (customer_id)\n```\n\n----------------------------------------\n\nTITLE: Sorting Query Results in dbt Semantic Layer\nDESCRIPTION: Examples demonstrating how to sort query results in the dbt Semantic Layer using the --order-by option. The examples show sorting in both ascending (default) and descending (using - prefix) order.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metricflow-commands.md#2025-04-09_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\ndbt sl query --metrics order_total --group-by metric_time --order-by -metric_time\n```\n\nLANGUAGE: bash\nCODE:\n```\ndbt sl query --metrics order_total --order-by metric_time,-revenue\n```\n\n----------------------------------------\n\nTITLE: Configuring Thrift Connection for Spark in dbt Profile\nDESCRIPTION: YAML configuration for connecting to a Thrift server in front of a Spark cluster. This includes settings for host, port, authentication, and optional parameters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/spark-setup.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: spark\n      method: thrift\n      schema: [database/schema name]\n      host: [hostname]\n      \n      # optional\n      port: [port]              # default 10001\n      user: [user]\n      auth: [e.g. KERBEROS]\n      kerberos_service_name: [e.g. hive]\n      use_ssl: [true|false]   # value of hive.server2.use.SSL, default false\n      server_side_parameters:\n        \"spark.driver.memory\": \"4g\" \n```\n\n----------------------------------------\n\nTITLE: Running Singular Tests in dbt\nDESCRIPTION: This command selects and runs only singular tests in dbt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/test-selection-examples.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndbt test --select \"test_type:singular\"\n```\n\n----------------------------------------\n\nTITLE: Setting Extended Attribute for Snowflake MFA Authentication in dbt Cloud\nDESCRIPTION: YAML configuration for enabling multi-factor authentication with Snowflake in dbt Cloud's environment settings. This authenticator setting enables Duo-powered MFA when connecting to Snowflake.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/connect-data-platform/connect-snowflake.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nauthenticator: username_password_mfa\n```\n\n----------------------------------------\n\nTITLE: Traditional SQL Table Creation with Type Specification\nDESCRIPTION: This snippet shows the traditional approach to creating tables with explicit column type definitions, followed by a separate insert statement. This is contrasted with dbt's approach.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Models/specifying-column-types.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\ncreate table dbt_alice.my_table\n  id integer,\n  created timestamp;\n\ninsert into dbt_alice.my_table (\n  select id, created from some_other_table\n)\n```\n\n----------------------------------------\n\nTITLE: Apache Spark Table Analysis Example\nDESCRIPTION: Example demonstrating how to analyze tables after creation in Apache Spark using post-hooks.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/pre-hook-post-hook.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  jaffle_shop:\n    marts:\n      finance:\n        +post-hook:\n          - \"analyze table {{ this }} compute statistics for all columns\"\n          - \"{{ analyze_table() }}\"\n```\n\n----------------------------------------\n\nTITLE: Access Control Example\nDESCRIPTION: Demonstrates how to implement group-based access control between models\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/group.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: finance_model\n    access: private\n    group: finance\n  - name: marketing_model\n    group: marketing\n```\n\n----------------------------------------\n\nTITLE: Implementing UDFs in Snowpark Python Models\nDESCRIPTION: Example of creating and using a UDF in a Snowpark Python model to add random values to temperatures. The function is registered using F.udf() and applied to a DataFrame column.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/python-models.md#2025-04-09_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport snowflake.snowpark.types as T\nimport snowflake.snowpark.functions as F\nimport numpy\n\ndef register_udf_add_random():\n    add_random = F.udf(\n        # use 'lambda' syntax, for simple functional behavior\n        lambda x: x + numpy.random.normal(),\n        return_type=T.FloatType(),\n        input_types=[T.FloatType()]\n    )\n    return add_random\n\ndef model(dbt, session):\n\n    dbt.config(\n        materialized = \"table\",\n        packages = [\"numpy\"]\n    )\n\n    temps_df = dbt.ref(\"temperatures\")\n\n    add_random = register_udf_add_random()\n\n    # warm things up, who knows by how much\n    df = temps_df.withColumn(\"degree_plus_random\", add_random(\"degree\"))\n    return df\n```\n\n----------------------------------------\n\nTITLE: Implementing incremental logic in SQL models\nDESCRIPTION: Example SQL model that demonstrates how to use the is_incremental() function to conditionally filter data. When the model runs incrementally, it only processes new records based on a timestamp column.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/run.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect * from all_events\n\n-- if the table already exists and `--full-refresh` is\n-- not set, then only add new records. otherwise, select\n-- all records.\n{% if is_incremental() %}\n   where collector_tstamp > (\n     select coalesce(max(max_tstamp), '0001-01-01') from {{ this }}\n   )\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Querying Seed Information in GraphQL for dbt Jobs\nDESCRIPTION: This GraphQL query retrieves information about a specific seed within a dbt job. It shows how to query details like database, schema, unique ID, name, status, and any error messages for a seed identified by its unique ID.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/schema-discovery-job-seed.mdx#2025-04-09_snippet_0\n\nLANGUAGE: graphql\nCODE:\n```\n{\n  job(id: 123) {\n    seed(uniqueId: \"seed.jaffle_shop.raw_customers\") {\n      database\n      schema\n      uniqueId\n      name\n      status\n      error\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring unique_key in dbt_project.yml for Models\nDESCRIPTION: Example of setting unique_key globally for models in a specific directory using dbt_project.yml. This approach applies the same unique key configuration to multiple models at once.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/unique_key.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nname: jaffle_shop\n\nmodels:\n  jaffle_shop:\n    staging:\n      +unique_key: id\n```\n\n----------------------------------------\n\nTITLE: Configuring Generic Test Severity in YAML\nDESCRIPTION: Example of configuring severity thresholds for a unique test on a specific column in a dbt model. Sets error threshold at >1000 failures and warning threshold at >10 failures.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/severity.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: large_table\n    columns:\n      - name: slightly_unreliable_column\n        tests:\n          - unique:\n              config:\n                severity: error\n                error_if: \">1000\"\n                warn_if: \">10\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Snowflake Warehouses in dbt Project\nDESCRIPTION: Configuration example showing how to set different warehouse sizes for different model groups in a dbt project. Demonstrates setting default warehouse size and overriding for specific model folders.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snowflake-configs.md#2025-04-09_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\nname: my_project\nversion: 1.0.0\n\n...\n\nmodels:\n  +snowflake_warehouse: \"EXTRA_SMALL\"    # use the `EXTRA_SMALL` warehouse for all models in the project...\n  my_project:\n    clickstream:\n      +snowflake_warehouse: \"EXTRA_LARGE\"    # ...except for the models in the `clickstream` folder, which will use the `EXTRA_LARGE` warehouse.\n\nsnapshots:\n  +snowflake_warehouse: \"EXTRA_LARGE\"    # all Snapshot models are configured to use the `EXTRA_LARGE` warehouse.\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of dbt ls Command\nDESCRIPTION: The basic syntax for using the dbt ls command, including various optional arguments for resource selection and output formatting.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/list.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndbt ls\n     [--resource-type {model,semantic_model,source,seed,snapshot,metric,test,exposure,analysis,default,all}]\n     [--select SELECTION_ARG [SELECTION_ARG ...]]\n     [--models SELECTOR [SELECTOR ...]]\n     [--exclude SELECTOR [SELECTOR ...]]\n     [--selector YML_SELECTOR_NAME]\n     [--output {json,name,path,selector}]\n     [--output-keys KEY_NAME [KEY_NAME]]\n```\n\n----------------------------------------\n\nTITLE: Creating Simple Pandas DataFrame in dbt Python Model\nDESCRIPTION: This snippet demonstrates how to create a simple Pandas DataFrame within a dbt Python model. It configures the model as a table materialization and returns a DataFrame with a single column 'A' containing values [1, 2, 3, 4].\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/athena-configs.md#2025-04-09_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\n\ndef model(dbt, session):\n    dbt.config(materialized=\"table\")\n\n    model_df = pd.DataFrame({\"A\": [1, 2, 3, 4]})\n\n    return model_df\n```\n\n----------------------------------------\n\nTITLE: Configuring Check Strategy for dbt Snapshots in YAML (dbt v1.9+)\nDESCRIPTION: YAML configuration for a snapshot using the check strategy. The check_cols parameter can be a list of columns to check for changes or 'all' to check all columns.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/strategy.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n- [name: snapshot_name](/reference/resource-configs/snapshot_name):\n  relation: source('my_source', 'my_table')\n  config:\n    strategy: check\n    check_cols: [column_name] | \"all\"\n```\n\n----------------------------------------\n\nTITLE: Basic MetricFlow Query with Dimension Filtering\nDESCRIPTION: Demonstrates querying order total metrics filtered by food orders and time dimension using MetricFlow CLI in dbt Core.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metricflow-commands.md#2025-04-09_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nmf query --metrics order_total --group-by order_id__is_food_order --where \"{{ Dimension('order_id__is_food_order') }} = True and TimeDimension('metric_time', 'week') }} >= '2024-02-01'\"\n```\n\n----------------------------------------\n\nTITLE: Package Access Restriction Configuration\nDESCRIPTION: Shows how to configure access restrictions for package models in dbt_project.yml.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/govern/model-access.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nrestrict-access: True  # default is False\n```\n\n----------------------------------------\n\nTITLE: Referencing Docs Blocks in YAML Configuration\nDESCRIPTION: Example of how to reference a docs block within a schema.yml file using the doc() Jinja function.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/documentation.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: events\n    description: '{{ doc(\"table_events\") }}'\n\n    columns:\n      - name: event_id\n        description: This is a unique identifier for the event\n        tests:\n            - unique\n            - not_null\n```\n\n----------------------------------------\n\nTITLE: Basic macro-paths Configuration in dbt_project.yml\nDESCRIPTION: The basic syntax for specifying the macro-paths configuration in dbt_project.yml, which defines where dbt will look for macro files.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/macro-paths.md#2025-04-09_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nmacro-paths: [directorypath]\n```\n\n----------------------------------------\n\nTITLE: Data Preparation for Formula 1 ML Models in Python\nDESCRIPTION: A Python dbt model that prepares Formula 1 race data for machine learning by filtering for years 2010-2020, handling null values, standardizing constructor names, and creating driver and constructor confidence metrics based on DNF ratios.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndef model(dbt, session):\n    # dbt configuration\n    dbt.config(packages=[\"pandas\"])\n\n    # get upstream data\n    fct_results = dbt.ref(\"fct_results\").to_pandas()\n\n    # provide years so we do not hardcode dates in filter command\n    start_year=2010\n    end_year=2020\n\n    # describe the data for a full decade\n    data =  fct_results.loc[fct_results['RACE_YEAR'].between(start_year, end_year)]\n\n    # convert string to an integer\n    data['POSITION'] = data['POSITION'].astype(float)\n\n    # we cannot have nulls if we want to use total pit stops \n    data['TOTAL_PIT_STOPS_PER_RACE'] = data['TOTAL_PIT_STOPS_PER_RACE'].fillna(0)\n\n    # some of the constructors changed their name over the year so replacing old names with current name\n    mapping = {'Force India': 'Racing Point', 'Sauber': 'Alfa Romeo', 'Lotus F1': 'Renault', 'Toro Rosso': 'AlphaTauri'}\n    data['CONSTRUCTOR_NAME'].replace(mapping, inplace=True)\n\n    # create confidence metrics for drivers and constructors\n    dnf_by_driver = data.groupby('DRIVER').sum(numeric_only=True)['DNF_FLAG']\n    driver_race_entered = data.groupby('DRIVER').count()['DNF_FLAG']\n    driver_dnf_ratio = (dnf_by_driver/driver_race_entered)\n    driver_confidence = 1-driver_dnf_ratio\n    driver_confidence_dict = dict(zip(driver_confidence.index,driver_confidence))\n\n    dnf_by_constructor = data.groupby('CONSTRUCTOR_NAME').sum(numeric_only=True)['DNF_FLAG']\n    constructor_race_entered = data.groupby('CONSTRUCTOR_NAME').count()['DNF_FLAG']\n    constructor_dnf_ratio = (dnf_by_constructor/constructor_race_entered)\n    constructor_relaiblity = 1-constructor_dnf_ratio\n    constructor_relaiblity_dict = dict(zip(constructor_relaiblity.index,constructor_relaiblity))\n\n    data['DRIVER_CONFIDENCE'] = data['DRIVER'].apply(lambda x:driver_confidence_dict[x])\n    data['CONSTRUCTOR_RELAIBLITY'] = data['CONSTRUCTOR_NAME'].apply(lambda x:constructor_relaiblity_dict[x])\n\n    #removing retired drivers and constructors\n    active_constructors = ['Renault', 'Williams', 'McLaren', 'Ferrari', 'Mercedes',\n                        'AlphaTauri', 'Racing Point', 'Alfa Romeo', 'Red Bull',\n                        'Haas F1 Team']\n    active_drivers = ['Daniel Ricciardo', 'Kevin Magnussen', 'Carlos Sainz',\n                    'Valtteri Bottas', 'Lance Stroll', 'George Russell',\n                    'Lando Norris', 'Sebastian Vettel', 'Kimi Rikknen',\n                    'Charles Leclerc', 'Lewis Hamilton', 'Daniil Kvyat',\n                    'Max Verstappen', 'Pierre Gasly', 'Alexander Albon',\n                    'Sergio Prez', 'Esteban Ocon', 'Antonio Giovinazzi',\n                    'Romain Grosjean','Nicholas Latifi']\n\n    # create flags for active drivers and constructors so we can filter downstream              \n    data['ACTIVE_DRIVER'] = data['DRIVER'].apply(lambda x: int(x in active_drivers))\n    data['ACTIVE_CONSTRUCTOR'] = data['CONSTRUCTOR_NAME'].apply(lambda x: int(x in active_constructors))\n    \n    return data\n```\n\n----------------------------------------\n\nTITLE: Complete Seed Configuration Example\nDESCRIPTION: Comprehensive example showing various seed configurations including schema overrides and column type specifications for multiple seeds.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/seed-configs.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nname: jaffle_shop\n...\nseeds:\n  jaffle_shop:\n    +enabled: true\n    +schema: seed_data\n    country_codes:\n      +column_types:\n        country_code: varchar(2)\n        country_name: varchar(32)\n    marketing:\n      +schema: marketing\n```\n\n----------------------------------------\n\nTITLE: Creating an Ephemeral Model to Generate a unique_key\nDESCRIPTION: Example of creating an ephemeral model that generates a unique ID column to be used as a unique key in a snapshot. This approach separates the key generation logic from the snapshot definition.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/unique_key.md#2025-04-09_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(materialized='ephemeral') }}\n\nselect\n  transaction_id || '-' || line_item_id as id,\n  *\nfrom {{ source('erp', 'transactions') }}\n```\n\n----------------------------------------\n\nTITLE: Using SQL ILIKE for Case-Insensitive Pattern Matching\nDESCRIPTION: Example query demonstrating how to use the ILIKE operator to categorize payment methods based on whether they contain the word 'card', regardless of case sensitivity. Uses a CASE statement with ILIKE to create a new column classifying payments.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/operators/sql-ilike.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n   payment_id,\n   order_id,\n   payment_method,\n   case when payment_method ilike '%card' then 'card_payment' else 'non_card_payment' end as was_card\nfrom {{ ref('payments') }}\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Semantic Model with Various Measure Aggregations\nDESCRIPTION: A complete semantic model example showing multiple measure aggregation types including sum, average, max, min, sum_boolean, count_distinct, percentile, and median. Also demonstrates using expressions to compute conditional aggregations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/measures.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_models:\n  - name: transactions\n    description: A record of every transaction that takes place. Carts are considered  multiple transactions for each sku.\n    model: ref('schema.transactions')\n    defaults:\n      agg_time_dimension: transaction_date\n\n# --- entities ---\n    entities:\n      - name: transaction_id\n        type: primary\n      - name: customer_id\n        type: foreign\n      - name: store_id\n        type: foreign\n      - name: product_id\n        type: foreign\n\n# --- measures ---\n    measures:\n      - name: transaction_amount_usd\n        description: Total usd value of transactions\n        expr: transaction_amount_usd\n        agg: sum\n        config:\n          meta:\n            used_in_reporting: true\n      - name: transaction_amount_usd_avg\n        description: Average usd value of transactions\n        expr: transaction_amount_usd\n        agg: average\n      - name: transaction_amount_usd_max\n        description: Maximum usd value of transactions\n        expr: transaction_amount_usd\n        agg: max\n      - name: transaction_amount_usd_min\n        description: Minimum usd value of transactions\n        expr: transaction_amount_usd\n        agg: min\n      - name: quick_buy_transactions \n        description: The total transactions bought as quick buy\n        expr: quick_buy_flag \n        agg: sum_boolean \n      - name: distinct_transactions_count\n        description: Distinct count of transactions \n        expr: transaction_id\n        agg: count_distinct\n      - name: transaction_amount_avg \n        description: The average value of transactions \n        expr: transaction_amount_usd\n        agg: average \n      - name: transactions_amount_usd_valid # Notice here how we use expr to compute the aggregation based on a condition\n        description: The total usd value of valid transactions only\n        expr: case when is_valid = True then transaction_amount_usd else 0 end \n        agg: sum\n      - name: transactions\n        description: The average value of transactions.\n        expr: transaction_amount_usd\n        agg: average\n      - name: p99_transaction_value\n        description: The 99th percentile transaction value\n        expr: transaction_amount_usd\n        agg: percentile\n        agg_params:\n          percentile: .99\n          use_discrete_percentile: False # False calculates the continuous percentile, True calculates the discrete percentile.\n      - name: median_transaction_value\n        description: The median transaction value\n        expr: transaction_amount_usd\n        agg: median\n        \n# --- dimensions ---\n    dimensions:\n      - name: transaction_date\n        type: time\n        expr: date_trunc('day', ts) # expr refers to underlying column ts\n        type_params:\n          time_granularity: day\n      - name: is_bulk_transaction\n        type: categorical\n        expr: case when quantity > 10 then true else false end\n```\n\n----------------------------------------\n\nTITLE: Configuring View Materialization in Doris/SelectDB (Project YAML)\nDESCRIPTION: Creates a Doris view using the project YAML configuration approach. This configuration is applied at the project level for the specified resource path.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/doris-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  <resource-path>:\n    +materialized: view\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Data Tests in DBT\nDESCRIPTION: YAML configuration for defining data tests on models, including both column-level and model-level tests with optional configuration parameters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/data-tests.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: <model_name>\n    tests:\n      - [<test_name>]:\n          <argument_name>: <argument_value>\n          [config]:\n            [<test_config>]: <config-value>\n\n    [columns]:\n      - name: <column_name>\n        tests:\n          - [<test_name>]\n          - [<test_name>]:\n              <argument_name>: <argument_value>\n              [config]:\n                [<test_config>]: <config-value>\n```\n\n----------------------------------------\n\nTITLE: Configuring MongoDB Source in YAML\nDESCRIPTION: This snippet demonstrates the YAML configuration for integrating data from a MongoDB source. It includes options for collection selection, column exclusion, and various job settings.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/upsolver-configs.md#2025-04-09_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\ncollection_include_list: ('<regexFilter>', ...)\nexclude_columns: ('<exclude_column>', ...)\ncolumn_transformations:\n  '<column>': '<expression>'\nskip_snapshots: True/False\nend_at: '<timestamp>/NOW'\ncompute_cluster: '<compute_cluster>'\nsnapshot_parallelism: <integer>\ncomment: '<comment>'\n```\n\n----------------------------------------\n\nTITLE: Granting BigQuery Permissions Using SQL\nDESCRIPTION: This snippet shows how to configure grants in a BigQuery SQL model by using the config function to grant the dataViewer role to a specific user.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/grants.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(grants = {'roles/bigquery.dataViewer': ['user:someone@yourcompany.com']}) }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Additional Partition Settings in BigQuery with dbt\nDESCRIPTION: Shows how to configure additional partition settings such as requiring partition filters and setting partition expiration in BigQuery using dbt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/bigquery-configs.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized = 'table',\n    partition_by = {\n      \"field\": \"created_at\",\n      \"data_type\": \"timestamp\",\n      \"granularity\": \"day\"\n    },\n    require_partition_filter = true,\n    partition_expiration_days = 7\n)}}\n```\n\n----------------------------------------\n\nTITLE: Sample Run Results JSON for Compiled Model\nDESCRIPTION: Example of the run_results.json output after compiling a model. Shows status, timing information, compilation details, and the relation name that would be created in the database.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/artifacts/run-results-json.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n    {\n      \"status\": \"success\",\n      \"timing\": [\n        {\n          \"name\": \"compile\",\n          \"started_at\": \"2023-10-12T16:35:28.510434Z\",\n          \"completed_at\": \"2023-10-12T16:35:28.519086Z\"\n        },\n        {\n          \"name\": \"execute\",\n          \"started_at\": \"2023-10-12T16:35:28.521633Z\",\n          \"completed_at\": \"2023-10-12T16:35:28.521641Z\"\n        }\n      ],\n      \"thread_id\": \"Thread-2\",\n      \"execution_time\": 0.0408780574798584,\n      \"adapter_response\": {},\n      \"message\": null,\n      \"failures\": null,\n      \"unique_id\": \"model.my_project.my_model\",\n      \"compiled\": true,\n      \"compiled_code\": \"select now() as created_at\",\n      \"relation_name\": \"\\\"postgres\\\".\\\"dbt_dbeatty\\\".\\\"my_model\\\"\"\n    }\n```\n\n----------------------------------------\n\nTITLE: Profiling dbt Commands with py-spy\nDESCRIPTION: This shell command installs py-spy and uses it to record a speedscope profile of a dbt parse command. It demonstrates an alternative method for performance profiling of dbt operations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/record-timing-info.md#2025-04-09_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython -m pip install py-spy\nsudo py-spy record -s -f speedscope -- dbt parse\n```\n\n----------------------------------------\n\nTITLE: Configuring KMS Encryption in BigQuery with dbt\nDESCRIPTION: Shows how to configure Customer Managed Encryption Keys (CMEK) for BigQuery tables using dbt's kms_key_name configuration in the dbt_project.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/bigquery-configs.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nname: my_project\nversion: 1.0.0\n\n...\n\nmodels:\n  my_project:\n    encrypted:\n      +kms_key_name: 'projects/PROJECT_ID/locations/global/keyRings/test/cryptoKeys/quickstart'\n```\n\n----------------------------------------\n\nTITLE: Configuring Clustering for Snowflake Tables\nDESCRIPTION: Model configuration that specifies clustering keys for a Snowflake table. This improves query performance by maintaining data organization according to the specified columns, in this case, session_start.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snowflake-configs.md#2025-04-09_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(\n    materialized='table',\n    cluster_by=['session_start']\n  )\n}}\n\nselect\n  session_id,\n  min(event_time) as session_start,\n  max(event_time) as session_end,\n  count(*) as count_pageviews\n\nfrom {{ source('snowplow', 'event') }}\ngroup by 1\n```\n\n----------------------------------------\n\nTITLE: Configuring Iceberg Table with Lake Formation Tags in SQL\nDESCRIPTION: Example of configuring an incremental Iceberg table with Lake Formation tags using inline SQL configuration. The configuration sets up table type, schema name, and comprehensive Lake Formation tag configurations for both table and column-level tagging.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/athena-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(\n    materialized='incremental',\n    incremental_strategy='append',\n    on_schema_change='append_new_columns',\n    table_type='iceberg',\n    schema='test_schema',\n    lf_tags_config={\n          'enabled': true,\n          'tags': {\n            'tag1': 'value1',\n            'tag2': 'value2'\n          },\n          'tags_columns': {\n            'tag1': {\n              'value1': ['column1', 'column2'],\n              'value2': ['column3', 'column4']\n            }\n          },\n          'inherited_tags': ['tag1', 'tag2']\n    }\n  )\n}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Labels in BigQuery with dbt\nDESCRIPTION: Demonstrates how to specify labels for BigQuery tables and views using dbt. Examples are provided for configuring labels in both model files and the dbt_project.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/bigquery-configs.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized = \"table\",\n    labels = {'contains_pii': 'yes', 'contains_pie': 'no'}\n)}}\n\nselect * from {{ ref('another_model') }}\n```\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  my_project:\n    snowplow:\n      +labels:\n        domain: clickstream\n    finance:\n      +labels:\n        domain: finance\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Model with Custom Compute HTTP Path\nDESCRIPTION: Shows how to configure a Python model to use a specific compute resource for Python code execution by directly specifying an HTTP path in the model config.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/databricks-configs.md#2025-04-09_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef model(dbt, session):\n    dbt.config(\n      http_path=\"sql/protocolv1/...\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Installing dbt Core and DuckDB on Mac using Virtual Environment\nDESCRIPTION: Commands to create a Python virtual environment, activate it, upgrade pip, and install the required dependencies for dbt Core with DuckDB on Mac.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/duckdb-qs.md#2025-04-09_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython3 -m venv venv\nsource venv/bin/activate\npython3 -m pip install --upgrade pip\npython3 -m pip install -r requirements.txt\nsource venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Advanced Ratio Metric with Filters\nDESCRIPTION: An advanced example of a ratio metric that calculates food orders to total orders, with location filters and aliases applied to both numerator and denominator. This demonstrates how to apply filters to customize the ratio calculation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/ratio-metrics.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  - name: food_order_pct\n    description: \"The food order count as a ratio of the total order count, filtered by location\"\n    label: Food order ratio by location\n    type: ratio\n    type_params:\n      numerator:\n        name: food_orders\n        filter: location = 'New York'\n        alias: ny_food_orders\n      denominator:\n        name: orders\n        filter: location = 'New York'\n        alias: ny_orders\n```\n\n----------------------------------------\n\nTITLE: Implementing Merge Strategy with Hudi in DBT\nDESCRIPTION: Example of implementing the merge strategy using Hudi in DBT, including configuration for unique key and custom Hudi options.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/glue-setup.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized='incremental',\n    incremental_strategy='merge',\n    unique_key='user_id',\n    file_format='hudi',\n    hudi_options={\n        'hoodie.datasource.write.precombine.field': 'eventtime',\n    }\n) }}\n\nwith new_events as (\n\n    select * from {{ ref('events') }}\n\n    {% if is_incremental() %}\n    where date_day >= date_add(current_date, -1)\n    {% endif %}\n\n)\n\nselect\n    user_id,\n    max(date_day) as last_seen\n\nfrom events\ngroup by 1\n```\n\n----------------------------------------\n\nTITLE: Configuring Incremental Materialization in Doris/SelectDB (Project YAML)\nDESCRIPTION: Creates an incremental Doris table with 'unique' model type using project YAML configuration. Specifies unique keys, partitioning, distribution, and other properties at the project level.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/doris-configs.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  <resource-path>:\n    +materialized: incremental\n    +unique_key: [ <column-name>, ... ],\n    +partition_by: [ <column-name>, ... ],\n    +partition_type: <engine-type>,\n    +partition_by_init: [<pertition-init>, ... ]\n    +distributed_by: [ <column-name>, ... ],\n    +buckets: int,\n    +properties: {<key>:<value>,...}\n```\n\n----------------------------------------\n\nTITLE: Implementing DATE_TRUNC with dbt Core Macros\nDESCRIPTION: Example of using dbt's cross-database DATE_TRUNC macro with the jaffle shop dataset to truncate order_date to week, month, and year levels without worrying about platform-specific syntax.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-07-05-date-trunc-sql-love-letter.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\torder_id,\n\torder_date,\n\t{{ date_trunc(\"week\", \"order_date\") }} as order_week,\n\t{{ date_trunc(\"month\", \"order_date\") }} as order_month,\n\t{{ date_trunc(\"year\", \"order_date\") }} as order_year\nfrom {{ ref('orders') }}\n```\n\n----------------------------------------\n\nTITLE: Setting Databricks Table Properties in dbt\nDESCRIPTION: Configuration block for setting table properties in Databricks. This example shows how to enable Delta table optimization features by configuring tblproperties.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/databricks-configs.md#2025-04-09_snippet_24\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    tblproperties={\n      'delta.autoOptimize.optimizeWrite' : 'true',\n      'delta.autoOptimize.autoCompact' : 'true'\n    }\n ) }}\n```\n\n----------------------------------------\n\nTITLE: Implementing Customers Mart Model in SQL\nDESCRIPTION: Shows the SQL implementation of a customers mart model, combining customer data with aggregated order information to create a comprehensive view of customer activity.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-structure/4-marts.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n-- customers.sql\n\nwith\n\ncustomers as (\n\n    select * from {{ ref('stg_jaffle_shop__customers')}}\n\n),\n\norders as (\n\n    select * from {{ ref('orders')}}\n\n),\n\ncustomer_orders as (\n\n    select\n        customer_id,\n        min(order_date) as first_order_date,\n        max(order_date) as most_recent_order_date,\n        count(order_id) as number_of_orders,\n        sum(amount) as lifetime_value\n\n    from orders\n\n    group by 1\n\n),\n\ncustomers_and_customer_orders_joined as (\n\n    select\n        customers.customer_id,\n        customers.first_name,\n        customers.last_name,\n        customer_orders.first_order_date,\n        customer_orders.most_recent_order_date,\n        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,\n        customer_orders.lifetime_value\n\n    from customers\n\n    left join customer_orders on customers.customer_id = customer_orders.customer_id\n\n)\n\nselect * from customers_and_customer_orders_joined\n```\n\n----------------------------------------\n\nTITLE: Accessing Configuration in Python Model\nDESCRIPTION: Python model showing how to access configuration values and implement conditional logic based on target environment.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/python-models.md#2025-04-09_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef model(dbt, session):\n    target_name = dbt.config.get(\"target_name\")\n    specific_var = dbt.config.get(\"specific_var\")\n    specific_env_var = dbt.config.get(\"specific_env_var\")\n\n    orders_df = dbt.ref(\"fct_orders\")\n\n    # limit data in dev\n    if target_name == \"dev\":\n        orders_df = orders_df.limit(500)\n```\n\n----------------------------------------\n\nTITLE: Querying Failed Models and Tests with GraphQL\nDESCRIPTION: This GraphQL query retrieves the latest run results across all jobs in the environment, returning only the models and tests that errored or failed. It's useful for diagnosing issues with deployments that result in delayed or incorrect data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/discovery-use-cases-and-examples.md#2025-04-09_snippet_6\n\nLANGUAGE: graphql\nCODE:\n```\nquery ($environmentId: BigInt!, $first: Int!) {\n  environment(id: $environmentId) {\n    applied {\n      models(first: $first, filter: { lastRunStatus: error }) {\n        edges {\n          node {\n            name\n            executionInfo {\n              lastRunId\n            }\n          }\n        }\n      }\n      tests(first: $first, filter: { status: \"fail\" }) {\n        edges {\n          node {\n            name\n            executionInfo {\n              lastRunId\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Transaction Fact Table Semantic Model\nDESCRIPTION: YAML configuration for a semantic model representing the transactions fact table. Includes entity definitions, measures, and dimensions that can be linked with the SCD table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/dimensions.md#2025-04-09_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_models: \n  - name: transactions \n    description: |\n      Each row represents one transaction.\n      There is a transaction, product, sales_person, and customer id for \n      every transaction. There is only one transaction id per \n      transaction. The `metric_time` or date is reflected in UTC.\n    model: {{ ref('fact_transactions') }}\n    defaults:\n      agg_time_dimension: metric_time\n\n    entities:\n      - name: transaction_id\n        type: primary\n      - name: customer\n        type: foreign\n        expr: customer_id\n      - name: product\n        type: foreign\n        expr: product_id\n      - name: sales_person\n        type: foreign\n        expr: sales_person_id\n\n    measures:\n      - name: transactions\n        expr: 1\n        agg: sum\n      - name: gross_sales\n        expr: sales_price\n        agg: sum\n      - name: sales_persons_with_a_sale\n        expr: sales_person_id\n        agg: count_distinct\n\n    dimensions:\n      - name: metric_time\n        type: time\n        label: \"Date of transaction\"\n        is_partition: true\n        type_params:\n          time_granularity: day\n      - name: sales_geo\n        type: categorical\n```\n\n----------------------------------------\n\nTITLE: Using schemas Variable for Granting Permissions in Redshift\nDESCRIPTION: This snippet demonstrates how to use the 'schemas' variable in dbt's on-run-end hooks to grant privileges across all schemas where dbt builds relations. It includes three grant statements: usage on schema, select on all tables, and default privileges for future tables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/schemas.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n...\n\non-run-end:\n  - \"{% for schema in schemas%}grant usage on schema {{ schema }} to group reporter;{% endfor%}\"\n  - \"{% for schema in schemas %}grant select on all tables in schema {{ schema }} to group reporter;{% endfor%}\"\n  - \"{% for schema in schemas %}alter default privileges in schema {{ schema }}  grant select on tables to group reporter;{% endfor %}\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Seed CSV File for Country Codes in dbt\nDESCRIPTION: Example of a CSV seed file containing country codes and names. This file would be placed in the seeds directory of your dbt project with a .csv extension.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/seeds.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ncountry_code,country_name\nUS,United States\nCA,Canada\nGB,United Kingdom\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring Table Materialization with Drop Strategy in Model\nDESCRIPTION: Configuration for table materialization using the 'drop' strategy in a model file. This approach drops and re-creates the table, which is useful for overcoming table rename limitations in AWS Glue.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/trino-configs.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(\n    materialized = 'table',\n    on_table_exists = 'drop`\n  )\n}}\n```\n\n----------------------------------------\n\nTITLE: Complex DBT Selection with Exclusions\nDESCRIPTION: Advanced example combining multiple selection criteria with exclusions. Shows how to select models based on source data and tags while excluding specific packages and materialization types.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/putting-it-together.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --select \"@source:snowplow,tag:nightly models/export\" --exclude \"package:snowplow,config.materialized:incremental export_performance_timing\"\n```\n\n----------------------------------------\n\nTITLE: Defining a Microbatch Incremental Model in SQL\nDESCRIPTION: SQL model definition that configures a sessions table as a microbatch incremental model. It specifies materialization, incremental strategy, event_time column, begin date, and batch size parameters while joining page_views and customers data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/incremental-microbatch.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized='incremental',\n    incremental_strategy='microbatch',\n    event_time='session_start',\n    begin='2020-01-01',\n    batch_size='day'\n) }}\n\nwith page_views as (\n\n    -- this ref will be auto-filtered\n    select * from {{ ref('page_views') }}\n\n),\n\ncustomers as (\n\n    -- this ref won't\n    select * from {{ ref('customers') }}\n\n),\n\nselect\n  page_views.id as session_id,\n  page_views.page_view_start as session_start,\n  customers.*\n  from page_views\n  left join customers\n    on page_views.customer_id = customer.id\n```\n\n----------------------------------------\n\nTITLE: Configuring Meta in Schema Files for Models\nDESCRIPTION: Shows how to set meta properties for models and their columns in schema.yml files.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/meta.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: model_name\n    config:\n      meta: {<dictionary>}\n\n    columns:\n      - name: column_name\n        meta: {<dictionary>}\n```\n\n----------------------------------------\n\nTITLE: Querying Sample Data in Databricks SQL\nDESCRIPTION: SQL queries to verify the successful loading of sample data into Databricks tables. These queries select all columns from the jaffle_shop_customers, jaffle_shop_orders, and stripe_payments tables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/databricks-qs.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect * from default.jaffle_shop_customers\nselect * from default.jaffle_shop_orders\nselect * from default.stripe_payments\n```\n\n----------------------------------------\n\nTITLE: Structuring profiles.yml for dbt Core connections\nDESCRIPTION: This YAML snippet demonstrates the structure of a profiles.yml file for dbt Core. It includes sections for global configurations (deprecated in v1.8+), profile names, targets, and placeholders for database-specific connection details. The file allows for multiple profiles and targets to be defined.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/profiles.yml.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nconfig:\n  send_anonymous_usage_stats: <true | false>\n  use_colors: <true | false>\n  partial_parse: <true | false>\n  printer_width: <integer>\n  write_json: <true | false>\n  warn_error: <true | false>\n  warn_error_options: <include: all | include: [<error-name>] | include: all, exclude: [<error-name>]>\n  log_format: <text | json | default>\n  debug: <true | false>\n  version_check: <true | false>\n  fail_fast: <true | false>\n  indirect_selection: <eager | cautious | buildable | empty>\n  use_experimental_parser: <true | false>\n  static_parser: <true | false>\n  cache_selected_only: <true | false>\n  populate_cache: <true | false>\n\n<profile-name>:\n  target: <target-name> # this is the default target\n  outputs:\n    <target-name>:\n      type: <bigquery | postgres | redshift | snowflake | other>\n      schema: <schema_identifier>\n      threads: <natural_number>\n\n      ### database-specific connection details\n      ...\n\n    <target-name>: # additional targets\n      ...\n\n<profile-name>: # additional profiles\n  ...\n```\n\n----------------------------------------\n\nTITLE: Advanced usage of + prefix for disambiguation in dbt_project.yml\nDESCRIPTION: This example shows when the + prefix is required for disambiguation - specifically with dictionary configs like persist_docs and when config keys match directory names (e.g., 'tags'). It demonstrates the difference between the tags config and the tags resource path.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/plus-prefix.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nname: jaffle_shop\nconfig-version: 2\n\n...\n\nmodels:\n  +persist_docs: # this config is a dictionary, so needs a + prefix\n    relation: true\n    columns: true\n\n  jaffle_shop:\n    schema: my_schema # a plus prefix is optional here\n    +tags: # this is the tag config\n      - \"hello\"\n    tags: # whereas this is the tag resource path\n      # The below config applies to models in the\n      # models/tags/ directory.\n      # Note: you don't _need_ a leading + here,\n      # but it wouldn't hurt.\n      materialized: view\n\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Project-Level Test Severity in YAML\nDESCRIPTION: Example of setting default test severity configurations at the project level in dbt_project.yml. Sets warning severity for all tests and specific warning threshold for package tests.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/severity.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ntests:\n  +severity: warn  # all tests\n\n  <package_name>:\n    +warn_if: >10 # tests in <package_name>\n```\n\n----------------------------------------\n\nTITLE: Using a Generated ID as unique_key in a Snapshot\nDESCRIPTION: Example of using a generated ID column as the unique key in a snapshot. This approach simplifies the snapshot configuration by referencing a pre-created unique identifier.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/unique_key.md#2025-04-09_snippet_13\n\nLANGUAGE: jinja2\nCODE:\n```\n\n{% snapshot transaction_items_snapshot %}\n\n    {{\n        config(\n          unique_key=\"id\",\n          ...\n        )\n    }}\n\nselect\n    transaction_id || '-' || line_item_id as id,\n    *\nfrom {{ source('erp', 'transactions') }}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Glue Data Catalog Documentation Persistence\nDESCRIPTION: YAML configuration to persist dbt documentation to the Glue Data Catalog. Enables storing model descriptions as table properties and column descriptions as column parameters in the Glue catalog.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/athena-configs.md#2025-04-09_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: test_deduplicate\n    description: another value\n    config:\n      persist_docs:\n        relation: true\n        columns: true\n      meta:\n        test: value\n    columns:\n      - name: id\n        meta:\n          primary_key: true\n```\n\n----------------------------------------\n\nTITLE: Calculating Lap Time Trends and Moving Averages in Python with dbt and Snowpark\nDESCRIPTION: This Python model calculates lap time averages and 5-year moving averages for Formula 1 races. It demonstrates data aggregation, rolling averages, and column manipulation using pandas.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndef model(dbt, session):\n    # dbt configuration\n    dbt.config(packages=[\"pandas\"])\n\n    # get upstream data\n    lap_times = dbt.ref(\"int_lap_times_years\").to_pandas()\n\n    # describe the data\n    lap_times[\"LAP_TIME_SECONDS\"] = lap_times[\"LAP_TIME_MILLISECONDS\"]/1000\n    lap_time_trends = lap_times.groupby(by=\"RACE_YEAR\")[\"LAP_TIME_SECONDS\"].mean().to_frame()\n    lap_time_trends.reset_index(inplace=True)\n    lap_time_trends[\"LAP_MOVING_AVG_5_YEARS\"] = lap_time_trends[\"LAP_TIME_SECONDS\"].rolling(5).mean()\n    lap_time_trends.columns = lap_time_trends.columns.str.upper()\n    \n    return lap_time_trends.round(1)\n```\n\n----------------------------------------\n\nTITLE: Configuring View Security Mode in Project Config\nDESCRIPTION: Project-level configuration for view materialization with 'invoker' security mode. This sets the default security mode for all view materializations in the specified path.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/trino-configs.md#2025-04-09_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  path:\n    materialized: view\n    +view_security: invoker\n```\n\n----------------------------------------\n\nTITLE: Configuring Dremio Cloud Profile in YAML\nDESCRIPTION: YAML configuration for connecting to Dremio Cloud in dbt. Includes settings for cloud host, project ID, object storage, and authentication using a personal access token.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/dremio-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n[project name]:\n  outputs:\n    dev:\n      cloud_host: api.dremio.cloud\n      cloud_project_id: [project ID]\n      object_storage_source: [name]\n      object_storage_path: [path]\n      dremio_space: [name]\n      dremio_space_folder: [path]\n      pat: [personal access token]\n      threads: [integer >= 1]\n      type: dremio\n      use_ssl: true\n      user: [email address]\n  target: dev\n```\n\n----------------------------------------\n\nTITLE: Configuring Table Materialization in Doris/SelectDB (Project YAML)\nDESCRIPTION: Creates a Doris table with specific configuration parameters using the project YAML approach. Includes options for duplicate keys, partitioning, distribution, and other Doris-specific properties.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/doris-configs.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  <resource-path>:\n    +materialized: table\n    +duplicate_key: [ <column-name>, ... ],\n    +partition_by: [ <column-name>, ... ],\n    +partition_type: <engine-type>,\n    +partition_by_init: [<pertition-init>, ... ]\n    +distributed_by: [ <column-name>, ... ],\n    +buckets: int,\n    +properties: {<key>:<value>,...}\n```\n\n----------------------------------------\n\nTITLE: BigQuery Incremental Model with Dynamic Partition Detection\nDESCRIPTION: Sets up an incremental model that dynamically detects partitions to overwrite. It uses the _dbt_max_partition BigQuery SQL variable to filter for only new data when the model runs incrementally.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/bigquery-configs.md#2025-04-09_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(\n    materialized = 'incremental',\n    partition_by = {'field': 'session_start', 'data_type': 'timestamp'},\n    incremental_strategy = 'insert_overwrite'\n  )\n}}\n\nwith events as (\n\n  select * from {{ref('events')}}\n\n  {% if is_incremental() %}\n\n    -- recalculate latest day's data + previous\n    -- NOTE: The _dbt_max_partition variable is used to introspect the destination table\n    where date(event_timestamp) >= date_sub(date(_dbt_max_partition), interval 1 day)\n\n{% endif %}\n\n),\n\n... rest of model ...\n```\n\n----------------------------------------\n\nTITLE: Setting Variables at the Top of a Model\nDESCRIPTION: SQL model that defines payment methods as a Jinja variable at the top of the file for better readability and maintainability.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/using-jinja.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{% set payment_methods = [\"bank_transfer\", \"credit_card\", \"gift_card\"] %}\n\nselect\norder_id,\n{% for payment_method in payment_methods %}\nsum(case when payment_method = '{{payment_method}}' then amount end) as {{payment_method}}_amount,\n{% endfor %}\nsum(amount) as total_amount\nfrom {{ ref('raw_payments') }}\ngroup by 1\n```\n\n----------------------------------------\n\nTITLE: Version-Aware Staging Model\nDESCRIPTION: SQL implementation showing how to handle different versions of business logic in staging models\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-07-12-change-data-capture.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\tcost_id,\n\t...,\n\tcost + tax as final_cost, -- old logic\n        1 || '-' || dbt_valid_from as version\nfrom costs_snapshot\nwhere dbt_valid_from <= to_timestamp('02/10/22 08:00:00')\n\nunion all\n\nselect\n\tcost_id,\n\t...,\n\tcost as final_cost, -- new logic\n        2 || '-' || dbt_valid_from as version\nfrom costs_snapshot\nwhere to_timestamp('02/10/22 08:00:00') between dbt_valid_to and coalesce(dbt_valid_from, to_timestamp('01/01/99 00:00:00'))\n```\n\n----------------------------------------\n\nTITLE: Using SQL SUM Function with GROUP BY in dbt\nDESCRIPTION: This query demonstrates how to use the SQL SUM function to calculate the total order amount for each customer from a dbt model reference. It groups results by customer_id and limits output to 3 rows.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/aggregate-functions/sql-sum.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\tcustomer_id,\n\tsum(order_amount) as all_orders_amount\nfrom {{ ref('orders') }}\ngroup by 1\nlimit 3\n```\n\n----------------------------------------\n\nTITLE: Querying Test Information with GraphQL\nDESCRIPTION: This GraphQL query retrieves information about a specific test, including its run ID, account ID, project ID, unique ID, name, column name, and state. The test result state can be 'error', 'fail', 'warn', or 'pass' in order of severity.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/schema-discovery-job-test.mdx#2025-04-09_snippet_0\n\nLANGUAGE: graphql\nCODE:\n```\n{\n  job(id: 123) {\n    test(uniqueId: \"test.internal_analytics.not_null_metrics_id\") {\n      runId\n      accountId\n      projectId\n      uniqueId\n      name\n      columnName\n      state\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running SQLFluff lint command with dbt Cloud CLI\nDESCRIPTION: Example of how to use the dbt Cloud CLI to run SQLFluff linting on SQL files, with optional paths and flags.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/configure-cloud-cli.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndbt sqlfluff lint [PATHS]... [flags]\n```\n\n----------------------------------------\n\nTITLE: Configuring Streaming Table in Databricks\nDESCRIPTION: Demonstrates how to configure a model as a streaming table in Databricks. Streaming tables are an alternative to incremental tables powered by Delta Live Tables and require Unity Catalog and serverless SQL Warehouses.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/databricks-configs.md#2025-04-09_snippet_20\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n   materialized = 'streaming_table'\n ) }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Table Expiration in dbt_project.yml\nDESCRIPTION: Sets the hours_to_expiration config for models in the dbt_project.yml file. This configuration determines how long a table will exist before being automatically deleted by BigQuery.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/bigquery-configs.md#2025-04-09_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  [<resource-path>]:\n    +hours_to_expiration: 6\n```\n\n----------------------------------------\n\nTITLE: Configuring Dynamic Tables in Properties YAML\nDESCRIPTION: Model-specific configuration for dynamic tables in properties.yml file, defining materialization, change behavior, and performance settings.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snowflake-configs.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: [<model-name>]\n    config:\n      materialized: dynamic_table\n      on_configuration_change: apply | continue | fail\n      target_lag: downstream | <time-delta>\n      snowflake_warehouse: <warehouse-name>\n      refresh_mode: AUTO | FULL | INCREMENTAL\n      initialize: ON_CREATE | ON_SCHEDULE\n```\n\n----------------------------------------\n\nTITLE: Checking Source Freshness SQL Query in dbt\nDESCRIPTION: This SQL snippet shows the query dbt uses to check source freshness. It selects the maximum value of the loaded_at_field and the current timestamp for comparison.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/sources.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nselect\n  max(_etl_loaded_at) as max_loaded_at,\n  convert_timezone('UTC', current_timestamp()) as calculated_at\nfrom raw.jaffle_shop.orders\n```\n\n----------------------------------------\n\nTITLE: Configuring Snapshot Meta Column Names in YAML Schema File\nDESCRIPTION: Example of customizing snapshot metadata column names in a dbt YAML schema file. This configuration allows changing the default column names used for tracking change history in Type 2 SCD implementations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snapshot_meta_column_names.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  - name: <snapshot_name>\n    config:\n      snapshot_meta_column_names:\n        dbt_valid_from: <string>\n        dbt_valid_to: <string>\n        dbt_scd_id: <string>\n        dbt_updated_at: <string>\n        dbt_is_deleted: <string>\n\n```\n\n----------------------------------------\n\nTITLE: SQL Inner Join Example with Car Data\nDESCRIPTION: Shows a practical example of joining two tables (car_type and car_color) using user_id as the join key. Returns matching records between tables with specific car details.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/joins/sql-inner-join.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect\n   car_type.user_id as user_id,\n   car_type.car_type as type,\n   car_color.car_color as color\nfrom {{ ref('car_type') }} as car_type\ninner join {{ ref('car_color') }} as car_color\non car_type.user_id = car_color.user_id\n```\n\n----------------------------------------\n\nTITLE: Setting Snowflake Session Parameters for All Models\nDESCRIPTION: This YAML configuration shows how to set Snowflake session parameters for all models using sql_header in the dbt_project.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/sql_header.md#2025-04-09_snippet_5\n\nLANGUAGE: yml\nCODE:\n```\nconfig-version: 2\n\nmodels:\n  +sql_header: \"alter session set timezone = 'Australia/Sydney';\"\n```\n\n----------------------------------------\n\nTITLE: Incremental Model with Append Strategy\nDESCRIPTION: Configuring an incremental model with the default 'append' strategy. This strategy only adds new records that match the incremental condition without modifying existing data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/trino-configs.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n      materialized = 'incremental')\n}}\nselect * from {{ ref('events') }}\n{% if is_incremental() %}\n  where event_ts > (select max(event_ts) from {{ this }})\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Identifying Power Users Per Account in SQL\nDESCRIPTION: CTE that identifies three types of power users for each corporate account: the first user (by creation date), most active user (by number of events), and user with most orders. Uses array aggregation and ordering to select specific users.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-02-07-customer-360-view-census-playbook.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\ncorporate_power_users as (\n   select\n       corporate_email,\n       get(array_agg(user_id) within group (order by created_at asc), 0)::int as first_user_id,\n       get(array_agg(user_id) within group (order by number_of_events desc), 0)::int as most_active_user_id,\n       get(array_agg(user_id) within group (order by number_of_orders desc), 0)::int as most_orders_user_id\n   from {{ ref('jafflegaggle_contacts') }}\n   where corporate_email is not null\n   group by 1\n),\n```\n\n----------------------------------------\n\nTITLE: Example Seed File with Pipe Delimiter\nDESCRIPTION: This text snippet shows the format of a seed file using a pipe (|) as the delimiter.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/delimiter.md#2025-04-09_snippet_3\n\nLANGUAGE: text\nCODE:\n```\ncol_a|col_b|col_c\n1|2|3\n4|5|6\n...\n```\n\n----------------------------------------\n\nTITLE: Querying Dataset and Column Metadata with GraphQL\nDESCRIPTION: This GraphQL query retrieves metadata about a specific model, including its description, tags, and column information from the catalog.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/discovery-use-cases-and-examples.md#2025-04-09_snippet_13\n\nLANGUAGE: graphql\nCODE:\n```\nquery ($environmentId: BigInt!, $first: Int!) {\n  environment(id: $environmentId) {\n    applied {\n      models(\n        first: $first\n        filter: {\n          database: \"analytics\"\n          schema: \"prod\"\n          identifier: \"customers\"\n        }\n      ) {\n        edges {\n          node {\n            name\n            description\n            tags\n            meta\n            catalog {\n              columns {\n                name\n                description\n                type\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Private Packages in dbt YAML\nDESCRIPTION: Demonstrates how to configure private packages in the packages.yml file using the private key. This method allows cloning package repos using existing dbt Cloud Git integration without needing an access token or environment variable.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/packages.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\npackages:\n  - private: dbt-labs/awesome_repo # your-org/your-repo path\n  - package: normal packages\n  [...]\n```\n\n----------------------------------------\n\nTITLE: Using Multiple Columns as unique_key in an Incremental Model\nDESCRIPTION: Example of using multiple columns as a composite unique key for an incremental model. This approach is useful when a single column isn't sufficient to uniquely identify records.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/unique_key.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized='incremental',\n    unique_key=['order_id', 'location_id']\n) }}\n\nwith...\n```\n\n----------------------------------------\n\nTITLE: Defining Sources in YAML\nDESCRIPTION: This YAML code defines sources for the jaffle_shop database, including customer and order tables, which can be referenced in dbt models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/snowflake-qs.md#2025-04-09_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n    - name: jaffle_shop\n      description: This is a replica of the Postgres database used by our app\n      database: raw\n      schema: jaffle_shop\n      tables:\n          - name: customers\n            description: One record per customer.\n          - name: orders\n            description: One record per order. Includes cancelled and deleted orders.\n```\n\n----------------------------------------\n\nTITLE: Defining Dimensions in dbt Semantic Layer YAML\nDESCRIPTION: This snippet shows the complete specification for defining dimensions in a dbt Semantic Layer semantic model. It includes all possible parameters and their descriptions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/dimensions.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndimensions:\n  - name: Name of the group that will be visible to the user in downstream tools # Required\n    type: Categorical or Time # Required\n    label: Recommended adding a string that defines the display value in downstream tools. # Optional\n    type_params: Specific type params such as if the time is primary or used as a partition # Required\n    description: Same as always # Optional\n    expr: The column name or expression. If not provided the default is the dimension name # Optional\n```\n\n----------------------------------------\n\nTITLE: Configuring Query Band in dbt_project.yml for dbt-teradata\nDESCRIPTION: Shows how to set a query band at the project level in the dbt_project.yml file, including dynamic model name substitution.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/teradata-configs.md#2025-04-09_snippet_21\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n Project_name:\n    +query_band: \"app=dbt;model={model};\"\n```\n\n----------------------------------------\n\nTITLE: Configuring on_configuration_change in properties.yml\nDESCRIPTION: Defines the on_configuration_change behavior in a properties YAML file. This approach lets you specify how dbt should handle configuration changes for specific models when using materialized views.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/on_configuration_change.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: [<model-name>]\n    config:\n      [materialized]: <materialization_name>\n      on_configuration_change: apply | continue | fail\n```\n\n----------------------------------------\n\nTITLE: Filter Syntax Examples for dbt Semantic Layer Metrics\nDESCRIPTION: This snippet demonstrates various filter syntax options for referencing entities, dimensions, time dimensions, and metrics in dbt Semantic Layer. It shows how to use Jinja templating to construct filters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metrics-overview.md#2025-04-09_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nfilter: | \n  {{ Entity('entity_name') }}\n\nfilter: |  \n  {{ Dimension('primary_entity__dimension_name') }}\n\nfilter: |  \n  {{ TimeDimension('time_dimension', 'granularity') }}\n\nfilter: |  \n {{ Metric('metric_name', group_by=['entity_name']) }}  \n```\n\n----------------------------------------\n\nTITLE: Configuring Package Dependencies in YAML\nDESCRIPTION: Install upstream packages from hub.getdbt.com by specifying them in the packages.yml file. This example shows how to include the dbt-utils package with a version range.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/building-packages.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\npackages:\n  - package: dbt-labs/dbt_utils\n    version: [\"\\>0.6.5\", \"0.7.0\"]\n```\n\n----------------------------------------\n\nTITLE: Defining Model Contract in YAML with Column Constraints and Tests\nDESCRIPTION: A YAML configuration example for the 'dim_customers' model with contract enforcement, column definitions, data types, constraints, and tests. It defines schema constraints including not_null and primary_key.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-01-evolving-data-engineer-craft.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: dim_customers\n    config:\n      contract:\n        enforced: true\n    columns:\n      - name: id\n        data_type: integer\n        description: hello\n        constraints:\n          - type: not_null\n          - type: primary_key # not enforced  -- will warn & include in DDL\n          - type: check       # not supported -- will warn & exclude from DDL\n            expression: \"id > 0\"\n        tests:\n          - unique            # primary_key constraint is not enforced\n      - name: customer_name\n        data_type: text\n      - name: first_transaction_date\n        data_type: date\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Grants in dbt YAML for Teradata\nDESCRIPTION: Setting select permissions for specific users on a dbt model in Teradata through the model's YAML configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/teradata-configs.md#2025-04-09_snippet_18\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: model_name\n    config:\n      grants:\n        select: ['user_a', 'user_b']\n```\n\n----------------------------------------\n\nTITLE: Configuring Grants for Models in YAML\nDESCRIPTION: Demonstrates how to set grants for a specific model in a YAML schema file. This configuration grants select privileges to 'reporter' and 'bi' roles.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/grants.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: specific_model\n    config:\n      grants:\n        select: ['reporter', 'bi']\n```\n\n----------------------------------------\n\nTITLE: Configuring on_configuration_change in dbt_project.yml\nDESCRIPTION: Sets the on_configuration_change behavior in the project YAML file. This configuration controls how dbt responds to configuration changes in materialized views, with options to apply changes, continue with warnings, or fail the run.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/on_configuration_change.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  [<resource-path>]:\n    [+][materialized]: <materialization_name>\n    [+]on_configuration_change: apply | continue | fail\n```\n\n----------------------------------------\n\nTITLE: Using zip Context Method with Default Value in Jinja\nDESCRIPTION: Example of using the zip context method with a non-iterable value and a default parameter. When one argument is not iterable, the default value (empty list) is returned instead of throwing an error.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/zip.md#2025-04-09_snippet_1\n\nLANGUAGE: jinja\nCODE:\n```\n{% set my_list_a = 12 %}\n{% set my_list_b = ['alice', 'bob'] %}\n{% set my_zip = zip(my_list_a, my_list_b, default = []) | list %}\n{% do log(my_zip) %}  {# [] #}\n```\n\n----------------------------------------\n\nTITLE: Versioned Model Reference Example in SQL\nDESCRIPTION: SQL queries demonstrating how to reference different versions of a model using the ref function.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/mesh-qs.md#2025-04-09_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\nselect * from {{ ref('fct_orders', v=1) }}\nselect * from {{ ref('fct_orders', v=2) }}\nselect * from {{ ref('fct_orders') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Grants for dbt Models in Trino\nDESCRIPTION: This YAML configuration demonstrates how to set up grants for a dbt model in Trino, specifying select permissions for specific roles.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/trino-configs.md#2025-04-09_snippet_18\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: NAME_OF_YOUR_MODEL\n    config:\n      grants:\n        select: ['reporter', 'bi']\n```\n\n----------------------------------------\n\nTITLE: Comparing Queries Between Legacy ETL and New dbt Models with audit_helper\nDESCRIPTION: This snippet demonstrates how to use the audit_helper.compare_queries macro to validate data consistency between a legacy ETL table and a refactored dbt model. It defines two query sets and compares them using a primary key, showing how to verify that data transformation logic produces the same results.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-03-23-audit-helper.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{# in dbt Develop #}\n\n\n{% set old_fct_orders_query %}\nselect\n    id as order_id,\n    amount,\n    customer_id\nfrom old_etl_schema.fct_orders\n{% endset %}\n\n\n{% set new_fct_orders_query %}\nselect\n    order_id,\n    amount,\n    customer_id\nfrom {{ ref('fct_orders') }}\n{% endset %}\n\n\n{{ audit_helper.compare_queries(\n    a_query=old_fct_orders_query,\n    b_query=new_fct_orders_query,\n    primary_key=\"order_id\"\n) }}\n```\n\n----------------------------------------\n\nTITLE: Configuring the 'fail' Parameter for Incremental Models in Vertica\nDESCRIPTION: Example of using the 'fail' parameter for on_schema_change configuration, which causes the model run to fail if schema changes are detected. This is useful for enforcing schema consistency.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/vertica-configs.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{config(materialized = 'incremental',on_schema_change='fail')}} \n      \n      \n      select * from {{ ref('seed_added') }}\n```\n\nLANGUAGE: text\nCODE:\n```\n    \n            The source and target schemas on this incremental model are out of sync!\n              They can be reconciled in several ways:\n                - set the `on_schema_change` config to either append_new_columns or sync_all_columns, depending on your situation.\n                - Re-run the incremental model with `full_refresh: True` to update the target schema.\n                - update the schema manually and re-run the process.\n\n              Additional troubleshooting context:\n                 Source columns not in target: {{ schema_changes_dict['source_not_in_target'] }}\n                 Target columns not in source: {{ schema_changes_dict['target_not_in_source'] }}\n                 New column types: {{ schema_changes_dict['new_target_types'] }}\n```\n\n----------------------------------------\n\nTITLE: Using FROM with dbt ref Function in SQL Query\nDESCRIPTION: This snippet demonstrates how to use dbt's ref function in a FROM statement to select data from an 'orders' model. The query selects three columns (order_id, customer_id, and order_date) and limits the results to three rows. When compiled, dbt will automatically translate the ref function to the appropriate schema path.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/statements/sql-from.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\torder_id, --select your columns\n\tcustomer_id,\n\torder_date\nfrom {{ ref('orders') }} --the table/view/model you want to select from\nlimit 3\n```\n\n----------------------------------------\n\nTITLE: Configuring Table Expiration in SQL Model\nDESCRIPTION: Sets the hours_to_expiration config for a specific model in its SQL file. This configuration determines how long the resulting table will exist before being automatically deleted by BigQuery.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/bigquery-configs.md#2025-04-09_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    hours_to_expiration = 6\n) }}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Versioning dbt Commands for v1.5 and Later (Shell)\nDESCRIPTION: Example of using VersionBlock to show dbt commands for version 1.5 and later. This block demonstrates the use of the --defer and --state options.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/contributing/single-sourcing-content.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n<VersionBlock firstVersion=\"1.5\">\n\n```shell\n$ dbt run --select [...] --defer --state path/to/artifacts\n$ dbt test --select [...] --defer --state path/to/artifacts\n```\n\n</VersionBlock>\n```\n\n----------------------------------------\n\nTITLE: Executing Union Selection in dbt Run\nDESCRIPTION: Demonstrates how to run multiple models and their ancestors using space-separated selectors to create a union. This example runs snowplow_sessions, all its ancestors, fct_orders, and all its ancestors.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/set-operators.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --select \"+snowplow_sessions +fct_orders\"\n```\n\n----------------------------------------\n\nTITLE: Running Tests on Sources in dbt\nDESCRIPTION: These commands demonstrate how to run tests on various source configurations in dbt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/test-selection-examples.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# tests on all sources\ndbt test --select \"source:*\"\n\n# tests on one source\ndbt test --select \"source:jaffle_shop\"\n\n# tests on two or more specific sources\ndbt test --select \"source:jaffle_shop source:raffle_bakery\"\n\n# tests on one source table\ndbt test --select \"source:jaffle_shop.customers\"\n\n# tests on everything _except_ sources\ndbt test --exclude \"source:*\"\n```\n\n----------------------------------------\n\nTITLE: Updated Data Tests Configuration\nDESCRIPTION: Examples of the new data_tests syntax introduced in dbt v1.8.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/data-tests.md#2025-04-09_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: orders\n    columns:\n      - name: order_id\n        data_tests:\n          - unique\n          - not_null\n```\n\nLANGUAGE: yaml\nCODE:\n```\ndata_tests:\n  +store_failures: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Starrocks Model in dbt Project YAML\nDESCRIPTION: This snippet shows how to configure a Starrocks model in the dbt_project.yml file. It includes options for materialization, keys, table type, distribution, partitioning, and other Starrocks-specific properties.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/starrocks-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  <resource-path>:\n    materialized: table       // table or view or materialized_view\n    keys: ['id', 'name', 'some_date']\n    table_type: 'PRIMARY'     // PRIMARY or DUPLICATE or UNIQUE\n    distributed_by: ['id']\n    buckets: 3                // default 10\n    partition_by: ['some_date']\n    partition_by_init: [\"PARTITION p1 VALUES [('1971-01-01 00:00:00'), ('1991-01-01 00:00:00')),PARTITION p1972 VALUES [('1991-01-01 00:00:00'), ('1999-01-01 00:00:00'))\"]\n    properties: [{\"replication_num\":\"1\", \"in_memory\": \"true\"}]\n    refresh_method: 'async' // only for materialized view default manual\n```\n\n----------------------------------------\n\nTITLE: Listing Objects in Snowflake Stage\nDESCRIPTION: This SQL command lists the objects stored in the MODELSTAGE in Snowflake, which should include the saved logistic regression model file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_33\n\nLANGUAGE: sql\nCODE:\n```\nlist @modelstage\n```\n\n----------------------------------------\n\nTITLE: Global configuration for state modification comparison\nDESCRIPTION: Configuration to reduce false positives for state:modified when configs differ between environments\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/2024-release-notes.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nstate_modified_compare_more_unrendered_values: true\n```\n\n----------------------------------------\n\nTITLE: Using the 'append' Incremental Strategy in Vertica\nDESCRIPTION: Example of the default 'append' incremental strategy which inserts new records without updating existing data. This example uses the is_incremental() macro to filter records based on the max product_key.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/vertica-configs.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(  materialized='incremental',     incremental_strategy='append'  ) }} \n\n\n    select * from  public.product_dimension\n\n\n    {% if is_incremental() %} \n    \n        where product_key > (select max(product_key) from {{this }}) \n    \n    \n    {% endif %}\n```\n\nLANGUAGE: sql\nCODE:\n```\n   insert into \"VMart\".\"public\".\"samp\" (\n\n        \"product_key\", \"product_version\", \"product_description\", \"sku_number\", \"category_description\", \n        \"department_description\", \"package_type_description\", \"package_size\", \"fat_content\", \"diet_type\",\n        \"weight\", \"weight_units_of_measure\", \"shelf_width\", \"shelf_height\", \"shelf_depth\", \"product_price\",\n        \"product_cost\", \"lowest_competitor_price\", \"highest_competitor_price\", \"average_competitor_price\", \"discontinued_flag\")\n    (\n          select \"product_key\", \"product_version\", \"product_description\", \"sku_number\", \"category_description\", \"department_description\", \"package_type_description\", \"package_size\", \"fat_content\", \"diet_type\", \"weight\", \"weight_units_of_measure\", \"shelf_width\", \"shelf_height\", \"shelf_depth\", \"product_price\", \"product_cost\", \"lowest_competitor_price\", \"highest_competitor_price\", \"average_competitor_price\", \"discontinued_flag\"\n\n          from \"samp__dbt_tmp\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Running MetricFlow Validations Commands\nDESCRIPTION: Commands to run validations in dbt Cloud and dbt Core environments. dbt Cloud requires developer credentials for IDE/CLI validation and deployment credentials for CI validation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/validation.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndbt sl validate # dbt Cloud users\nmf validate-configs # dbt Core users\n```\n\n----------------------------------------\n\nTITLE: Fetching Defined Metrics with Semantic Layer JDBC API\nDESCRIPTION: SQL query to fetch all metrics defined in your dbt project using the semantic_layer.metrics() function.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-jdbc.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nselect * from {{ \n\tsemantic_layer.metrics() \n}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic DuckDB Profile with Extensions and S3 Settings in YAML\nDESCRIPTION: A basic dbt profile configuration for DuckDB that includes loading the httpfs and parquet extensions, and configuring S3 connection settings for cloud storage access.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/duckdb-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: duckdb\n      path: 'file_path/database_name.duckdb'\n      extensions:\n        - httpfs\n        - parquet\n      settings:\n        s3_region: my-aws-region\n        s3_access_key_id: \"{{ env_var('S3_ACCESS_KEY_ID') }}\"\n        s3_secret_access_key: \"{{ env_var('S3_SECRET_ACCESS_KEY') }}\"\n```\n\n----------------------------------------\n\nTITLE: Backfill Command for Microbatch Models\nDESCRIPTION: Example of running a backfill operation on a microbatch model by specifying a date range to reprocess.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/incremental-microbatch.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --event-time-start \"2024-09-01\" --event-time-end \"2024-09-04\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Specific Generic Test Instance\nDESCRIPTION: Configuration example for setting fail_calc on a specific instance of a generic (schema) test within a model definition.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/fail_calc.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: my_model\n    columns:\n      - name: my_columns\n        tests:\n          - unique:\n              config:\n                fail_calc: \"case when count(*) > 0 then sum(n_records) else 0 end\"\n```\n\n----------------------------------------\n\nTITLE: Converting List to Set with the set() Method in Jinja\nDESCRIPTION: Demonstrates how to convert a list with duplicate values to a set containing only unique values using the set() method. The resulting set contains each value only once.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/set.md#2025-04-09_snippet_0\n\nLANGUAGE: jinja\nCODE:\n```\n{% set my_list = [1, 2, 2, 3] %}\n{% set my_set = set(my_list) %}\n{% do log(my_set) %}  {# {1, 2, 3} #}\n```\n\n----------------------------------------\n\nTITLE: Casting Column Types in dbt Models\nDESCRIPTION: This snippet demonstrates how to cast columns to specific data types in a dbt model query. The example shows casting a 'created' field to a timestamp type.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Models/specifying-column-types.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    id,\n    created::timestamp as created\nfrom some_other_table\n```\n\n----------------------------------------\n\nTITLE: Configuring unique_key in Snapshot YAML Files (dbt v1.9+)\nDESCRIPTION: Example of configuring unique_key for a snapshot in a YAML file (available in dbt v1.9+). This approach defines the snapshot configuration separate from the SQL implementation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/unique_key.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  - name: orders_snapshot\n    relation: source('my_source', 'my_table')\n    [config](/reference/snapshot-configs):\n      unique_key: order_id\n```\n\n----------------------------------------\n\nTITLE: YAML Entity Configuration Example\nDESCRIPTION: Practical example of defining multiple entities in a semantic model\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/entities.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nentities:\n  - name: transaction\n    type: primary\n    expr: id_transaction\n  - name: order\n    type: foreign\n    expr: id_order\n  - name: user\n    type: foreign\n    expr: substring(id_order from 2)\n    entities:\n  - name: transaction\n    type: \n    description: A description of the field or role the entity takes in this table ## Optional\n    expr: The field that denotes that entity (transaction_id).  \n          Defaults to name if unspecified.\n    [config]:\n      [meta]:\n        data_owner: \"Finance team\"\n```\n\n----------------------------------------\n\nTITLE: Stopping the Airflow Docker Containers\nDESCRIPTION: Command to stop the Airflow Docker containers when you're done with the guide, which will shut down all running services.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/airflow-and-dbt-cloud.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ astrocloud dev stop\n\n[+] Running 3/3\n  Container airflow-dbt-cloud_e3fe3c-webserver-1  Stopped    7.5s\n  Container airflow-dbt-cloud_e3fe3c-scheduler-1  Stopped    3.3s\n  Container airflow-dbt-cloud_e3fe3c-postgres-1   Stopped    0.3s\n```\n\n----------------------------------------\n\nTITLE: Creating Staging Payments Model\nDESCRIPTION: SQL model that transforms raw payment data into a staging table with selected and renamed columns.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/mesh-qs.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nwith payments as (\n    select * from {{ source('stripe', 'payment') }}\n),\n\nfinal as (\n    select \n        id as payment_id,\n        orderID as order_id,\n        paymentMethod as payment_method,\n        amount,\n        created as payment_date \n    from payments\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Configuring Databricks Connection Fields in dbt Cloud\nDESCRIPTION: This table outlines the required fields for setting up a Databricks connection in dbt Cloud. It includes the server hostname, HTTP path, and optional catalog name.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/connect-data-platform/connect-databricks.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Field | Description | Examples |\n| ----- | ----------- | -------- |\n| Server Hostname | The hostname of the Databricks account to connect to | dbc-a2c61234-1234.cloud.databricks.com |\n| HTTP Path | The HTTP path of the Databricks cluster or SQL warehouse | /sql/1.0/warehouses/1a23b4596cd7e8fg |\n| Catalog | Name of Databricks Catalog (optional) | Production |\n```\n\n----------------------------------------\n\nTITLE: Configuring Rockset Connection Profile in dbt\nDESCRIPTION: YAML configuration for connecting dbt to Rockset. Specifies the required parameters including workspace, API key, and API server in the profiles.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/rockset-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nrockset:\n  target: dev\n  outputs:\n    dev:\n      type: rockset\n      workspace: [schema]\n      api_key: [api_key]\n      api_server: [api_server] # (Default is api.rs2.usw2.rockset.com)\n```\n\n----------------------------------------\n\nTITLE: Setting Temporary Relation Type in Model Configuration for Snowflake\nDESCRIPTION: Model-level configuration to set the temporary relation type (table or view) within a specific dbt model SQL file. This controls whether Snowflake incremental builds use a view or table as the temporary relation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snowflake-configs.md#2025-04-09_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n{{ config(\n    tmp_relation_type=\"table | view\", ## If not defined, view is the default.\n) }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Conditional Materialization in properties.yml using dbt Context Variables\nDESCRIPTION: This example demonstrates how to configure a model to materialize differently based on the environment using Jinja templating in properties.yml. The model will be materialized as a view in development but as a table in production or CI contexts.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/properties-yml-context.md#2025-04-09_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\n# Configure this model to be materialized as a view\n# in development and a table in production/CI contexts\n\nmodels:\n  - name: dim_customers\n    config:\n      materialized: \"{{ 'view' if target.name == 'dev' else 'table' }}\"\n```\n\n----------------------------------------\n\nTITLE: Project-Level Semantic Model Configuration\nDESCRIPTION: Demonstrates how to configure semantic model properties at the project level in dbt_project.yml.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/semantic-models.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic-models:\n  my_project_name:\n    +enabled: true | false\n    +group: some_group\n    +meta:\n      some_key: some_value\n```\n\n----------------------------------------\n\nTITLE: Creating a Context Driven Test in SQL for dbt\nDESCRIPTION: This SQL snippet shows how to create a Context Driven Test (Singular Test) in dbt. It checks for customers with purchase dates before 1900, demonstrating how to set up more complex, specific tests.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-01-24-aggregating-test-failures.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n        tags=['check_purchase_date_in_range', 'customer'],\n        alias='ad_hoc__check_purchase_date_in_range\n    )\n}}\n\nSELECT\n    id,\n    purchase_date\nFROM\n    {{ ref('customer') }}\nWHERE purchase_date < '1900-01-01'\n```\n\n----------------------------------------\n\nTITLE: Query SQL String Construction Examples\nDESCRIPTION: Different approaches to constructing SQL queries using Jinja, showing both correct and incorrect methods.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/dont-nest-your-curlies.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{# Either of these work #}\n\n{% set query_sql = 'select * from ' ~ ref('my_model') %}\n\n{% set query_sql %}\nselect * from {{ ref('my_model') }}\n{% endset %}\n\n{# This does not #}\n{% set query_sql = \"select * from {{ ref('my_model')}}\" %}\n```\n\n----------------------------------------\n\nTITLE: Calculating Fastest Pit Stops by Constructor in Python with dbt and Snowpark\nDESCRIPTION: This Python model calculates the fastest pit stops by constructor for the 2021 Formula 1 season. It uses pandas and numpy for data manipulation, and demonstrates method chaining, data aggregation, and formatting techniques.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\n\ndef model(dbt, session):\n    # dbt configuration\n    dbt.config(packages=[\"pandas\",\"numpy\"])\n\n    # get upstream data\n    pit_stops_joined = dbt.ref(\"pit_stops_joined\").to_pandas()\n\n    # provide year so we do not hardcode dates \n    year=2021\n\n    # describe the data\n    pit_stops_joined[\"PIT_STOP_SECONDS\"] = pit_stops_joined[\"PIT_STOP_MILLISECONDS\"]/1000\n    fastest_pit_stops = pit_stops_joined[(pit_stops_joined[\"RACE_YEAR\"]==year)].groupby(by=\"CONSTRUCTOR_NAME\")[\"PIT_STOP_SECONDS\"].describe().sort_values(by='mean')\n    fastest_pit_stops.reset_index(inplace=True)\n    fastest_pit_stops.columns = fastest_pit_stops.columns.str.upper()\n    \n    return fastest_pit_stops.round(2)\n```\n\n----------------------------------------\n\nTITLE: Example of Check Strategy in SQL (dbt v1.8 and earlier)\nDESCRIPTION: Complete SQL/Jinja example of a snapshot using the check strategy for an orders table, with specific columns defined for change detection.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/strategy.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot orders_snapshot_check %}\n\n    {{\n        config(\n          target_schema='snapshots',\n          strategy='check',\n          unique_key='id',\n          check_cols=['status', 'is_cancelled'],\n        )\n    }}\n\n    select * from {{ source('jaffle_shop', 'orders') }}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Indirect Selection Modes in YAML Selectors\nDESCRIPTION: Setting different indirect selection modes (eager, cautious, buildable, empty) for specific selection criteria to control test selection behavior.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/yaml-selectors.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n- union:\n    - method: fqn\n      value: model_a\n      indirect_selection: eager  # default: will include all tests that touch model_a\n    - method: fqn\n      value: model_b\n      indirect_selection: cautious  # will not include tests touching model_b\n                        # if they have other unselected parents\n    - method: fqn\n      value: model_c\n      indirect_selection: buildable  # will not include tests touching model_c\n                        # if they have other unselected parents (unless they have an ancestor that is selected)\n    - method: fqn\n      value: model_d\n      indirect_selection: empty  # will include tests for only the selected node and ignore all tests attached to model_d\n```\n\n----------------------------------------\n\nTITLE: Configuring a dbt Model to Use a Custom Incremental Strategy\nDESCRIPTION: This snippet shows how to configure a dbt model to use a custom incremental strategy. The model is set to use the 'insert_only' incremental strategy defined in the custom macros.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/incremental-strategy.md#2025-04-09_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized=\"incremental\",\n    incremental_strategy=\"insert_only\",\n    ...\n) }}\n\n...\n```\n\n----------------------------------------\n\nTITLE: Triggering dbt Cloud Job with Prefect 2\nDESCRIPTION: This code demonstrates how to trigger a dbt Cloud job using Prefect 2. It uses the trigger_dbt_cloud_job_run_and_wait_for_completion flow from the prefect-dbt library.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/deployment-tools.md#2025-04-09_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom prefect import flow\nfrom prefect_dbt.cloud import DbtCloudCredentials\nfrom prefect_dbt.cloud.jobs import trigger_dbt_cloud_job_run_and_wait_for_completion\n\n@flow\ndef trigger_dbt_cloud_job():\n    dbt_cloud_credentials = DbtCloudCredentials.load(\"MY-BLOCK-NAME\")\n    trigger_dbt_cloud_job_run_and_wait_for_completion(\n        dbt_cloud_credentials=dbt_cloud_credentials,\n        job_id=12345,\n    )\n\nif __name__ == \"__main__\":\n    trigger_dbt_cloud_job()\n```\n\n----------------------------------------\n\nTITLE: Configuring Snowflake Key Pair Authentication in dbt (v1.8 and earlier)\nDESCRIPTION: This YAML configuration sets up key pair authentication for Snowflake in dbt versions 1.8 and earlier. It includes options for specifying the private key path and passphrase, along with other essential Snowflake connection parameters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/snowflake-setup.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmy-snowflake-db:\n  target: dev\n  outputs:\n    dev:\n      type: snowflake\n      account: [account id]\n      user: [username]\n      role: [user role]\n\n      # Keypair config\n      private_key_path: [path/to/private.key]\n      # or private_key instead of private_key_path\n      private_key_passphrase: [passphrase for the private key, if key is encrypted]\n\n      database: [database name]\n      warehouse: [warehouse name]\n      schema: [dbt schema]\n      threads: [1 or more]\n      client_session_keep_alive: False\n      query_tag: [anything]\n\n      # optional\n      connect_retries: 0 # default 0\n      connect_timeout: 10 # default: 10\n      retry_on_database_errors: False # default: false\n      retry_all: False  # default: false\n      reuse_connections: False\n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Header for dbt Snapshots\nDESCRIPTION: This snippet illustrates how to set the sql_header configuration for a dbt snapshot using the config block within a snapshot SQL file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/sql_header.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot [snapshot_name](snapshot_name) %}\n\n{{ config(\n  sql_header=\"<sql-statement>\"\n) }}\n\nselect ...\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Integer Range Partitioning in BigQuery with dbt\nDESCRIPTION: Demonstrates how to set up integer range partitioning in BigQuery using dbt. This example partitions the table based on the user_id field.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/bigquery-configs.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized='table',\n    partition_by={\n      \"field\": \"user_id\",\n      \"data_type\": \"int64\",\n      \"range\": {\n        \"start\": 0,\n        \"end\": 100,\n        \"interval\": 10\n      }\n    }\n)}}\n\nselect\n  user_id,\n  event_name,\n  created_at\n\nfrom {{ ref('events') }}\n```\n\n----------------------------------------\n\nTITLE: Hive Table Configuration with Parquet Format\nDESCRIPTION: Configuration example for creating a Hive table with Parquet file format and partitioning specifications using dbt model properties.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/watsonx-presto-config.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(\n    materialized='table',\n    properties={\n      \"format\": \"'PARQUET'\", -- Specifies the file format\n      \"partitioned_by\": \"ARRAY['id']\", -- Defines the partitioning column(s)\n    }\n  )\n}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Segmentation Across All Nodes in Vertica dbt Model\nDESCRIPTION: This example shows how to use both 'segmented_by_string' and 'segmented_by_all_nodes' config parameters to segment a Vertica table by a specific column and distribute it across all cluster nodes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/vertica-configs.md#2025-04-09_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\n{{ config( materialized='table', segmented_by_string='product_key' ,segmented_by_all_nodes='True' )  }}  \n\nselect * from public.product_dimension\n```\n\n----------------------------------------\n\nTITLE: Running Common Ancestors Using Intersection\nDESCRIPTION: Shows how to select only the common ancestors between two models using comma-separated selectors without spaces. This example finds common ancestors between snowplow_sessions and fct_orders.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/set-operators.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --select \"+snowplow_sessions,+fct_orders\"\n```\n\n----------------------------------------\n\nTITLE: Basic dbtRunner Usage in Python\nDESCRIPTION: Shows how to initialize a dbtRunner, create CLI arguments as a list of strings, invoke a dbt command, and inspect the results from the returned dbtRunnerResult object.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/programmatic-invocations.md#2025-04-09_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dbt.cli.main import dbtRunner, dbtRunnerResult\n\n# initialize\ndbt = dbtRunner()\n\n# create CLI args as a list of strings\ncli_args = [\"run\", \"--select\", \"tag:my_tag\"]\n\n# run the command\nres: dbtRunnerResult = dbt.invoke(cli_args)\n\n# inspect the results\nfor r in res.result:\n    print(f\"{r.node.name}: {r.status}\")\n```\n\n----------------------------------------\n\nTITLE: Running dbt Tests with Various Selection Options\nDESCRIPTION: Examples of using the dbt test command with different --select options to run specific types of tests or tests for specific models and packages.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/test.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# run tests for one_specific_model\ndbt test --select \"one_specific_model\"\n\n# run tests for all models in package\ndbt test --select \"some_package.*\"\n\n# run only tests defined singularly\ndbt test --select \"test_type:singular\"\n\n# run only tests defined generically\ndbt test --select \"test_type:generic\"\n\n# run singular tests limited to one_specific_model\ndbt test --select \"one_specific_model,test_type:singular\"\n\n# run generic tests limited to one_specific_model\ndbt test --select \"one_specific_model,test_type:generic\"\n```\n\n----------------------------------------\n\nTITLE: Materialized View Configuration Examples\nDESCRIPTION: Demonstrates different ways to configure materialized views in dbt including project file, property file, and config block approaches.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/postgres-configs.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  [<resource-path>]:\n    [+]materialized: materialized_view\n    [+]on_configuration_change: apply | continue | fail\n    [+]indexes:\n      - columns: [<column-name>]\n        unique: true | false\n        type: hash | btree\n```\n\n----------------------------------------\n\nTITLE: Installing dbt Semantic Layer SDK - Async Version\nDESCRIPTION: Installation command for the asynchronous version of the dbt Semantic Layer SDK using pip.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-python-sdk.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install \"dbt-sl-sdk[sync]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Table Materialization in dbt Project File\nDESCRIPTION: Project-level configuration that sets default materialization as view and specifies staging models as tables without columnstore indexes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/mssql-configs.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  your_project_name:\n    materialized: view\n    staging:\n      materialized: table\n      as_columnstore: False\n```\n\n----------------------------------------\n\nTITLE: Combining Directory and Tag Filters with Intersection\nDESCRIPTION: Shows how to select models that match multiple criteria using intersection. This example selects models that are both in the marts/finance directory and tagged as nightly.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/set-operators.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --select \"marts.finance,tag:nightly\"\n```\n\n----------------------------------------\n\nTITLE: Listing Dimensions Command in dbt Cloud and Core\nDESCRIPTION: Command syntax for listing unique dimensions for metrics in both dbt Cloud and dbt Core. Displays only common dimensions when querying multiple metrics.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metricflow-commands.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndbt sl list dimensions --metrics <metric_name> # In dbt Cloud\n\nmf list dimensions --metrics <metric_name> # In dbt Core\n\nOptions:\n  --metrics SEQUENCE  List dimensions by given metrics (intersection). Ex. --metrics bookings,messages\n  --help              Show this message and exit.\n```\n\n----------------------------------------\n\nTITLE: Loading Order Data from S3 into Snowflake\nDESCRIPTION: SQL command to copy order data from a public S3 bucket into the previously created orders table. The command specifies CSV format with a header row to skip.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/snowflake-qs.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\ncopy into raw.jaffle_shop.orders (id, user_id, order_date, status)\nfrom 's3://dbt-tutorial-public/jaffle_shop_orders.csv'\nfile_format = (\n    type = 'CSV'\n    field_delimiter = ','\n    skip_header = 1\n    );\n```\n\n----------------------------------------\n\nTITLE: Sample Run Results JSON for Test Execution\nDESCRIPTION: Example of the run_results.json output after running tests on a model. Shows test results for both 'unique' and 'not_null' tests, including timing, execution details, and the compiled SQL that was used for testing.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/artifacts/run-results-json.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n  \"results\": [\n    {\n      \"status\": \"pass\",\n      \"timing\": [\n        {\n          \"name\": \"compile\",\n          \"started_at\": \"2023-10-12T17:20:51.279437Z\",\n          \"completed_at\": \"2023-10-12T17:20:51.317312Z\"\n        },\n        {\n          \"name\": \"execute\",\n          \"started_at\": \"2023-10-12T17:20:51.319812Z\",\n          \"completed_at\": \"2023-10-12T17:20:51.441967Z\"\n        }\n      ],\n      \"thread_id\": \"Thread-2\",\n      \"execution_time\": 0.1807551383972168,\n      \"adapter_response\": {\n        \"_message\": \"SELECT 1\",\n        \"code\": \"SELECT\",\n        \"rows_affected\": 1\n      },\n      \"message\": null,\n      \"failures\": 0,\n      \"unique_id\": \"test.my_project.unique_my_model_created_at.a9276afbbb\",\n      \"compiled\": true,\n      \"compiled_code\": \"\\n    \\n    \\n\\nselect\\n    created_at as unique_field,\\n    count(*) as n_records\\n\\nfrom \\\"postgres\\\".\\\"dbt_dbeatty\\\".\\\"my_model\\\"\\nwhere created_at is not null\\ngroup by created_at\\nhaving count(*) > 1\\n\\n\\n\",\n      \"relation_name\": null\n    },\n    {\n      \"status\": \"pass\",\n      \"timing\": [\n        {\n          \"name\": \"compile\",\n          \"started_at\": \"2023-10-12T17:20:51.274049Z\",\n          \"completed_at\": \"2023-10-12T17:20:51.295237Z\"\n        },\n        {\n          \"name\": \"execute\",\n          \"started_at\": \"2023-10-12T17:20:51.296361Z\",\n          \"completed_at\": \"2023-10-12T17:20:51.491327Z\"\n        }\n      ],\n      \"thread_id\": \"Thread-1\",\n      \"execution_time\": 0.22345590591430664,\n      \"adapter_response\": {\n        \"_message\": \"SELECT 1\",\n        \"code\": \"SELECT\",\n        \"rows_affected\": 1\n      },\n      \"message\": null,\n      \"failures\": 0,\n      \"unique_id\": \"test.my_project.not_null_my_model_created_at.9b412fbcc7\",\n      \"compiled\": true,\n      \"compiled_code\": \"\\n    \\n    \\n\\n\\n\\nselect *\\nfrom \\\"postgres\\\".\\\"dbt_dbeatty\\\".\\\"my_model\\\"\\nwhere created_at is null\\n\\n\\n\",\n      \"relation_name\": \"\\\"postgres\\\".\\\"dbt_dbeatty_dbt_test__audit\\\".\\\"not_null_my_model_created_at\\\"\"\n    }\n  ],\n```\n\n----------------------------------------\n\nTITLE: Querying with Limit Clause\nDESCRIPTION: Example showing how to limit the number of results returned from a semantic layer query using the limit parameter. This query returns only the top 10 results for the specified metrics and time dimension.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-jdbc.md#2025-04-09_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\nselect * from {{\nsemantic_layer.query(metrics=['food_order_amount', 'order_gross_profit'],\n  group_by=[Dimension('metric_time')],\n  limit=10)\n  }}\n```\n\n----------------------------------------\n\nTITLE: Defining Groups and Access Modifiers\nDESCRIPTION: Shows how to define groups with owners and set different access levels (public, private, protected) for models in YAML configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/govern/model-access.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ngroups:\n  - name: customer_success\n    owner:\n      name: Customer Success Team\n      email: cx@jaffle.shop\n\nmodels:\n  - name: dim_customers\n    group: customer_success\n    access: public\n    \n  - name: int_customer_history_rollup\n    group: customer_success\n    access: private\n    \n  - name: stg_customer__survey_results\n    group: customer_success\n    access: protected\n```\n\n----------------------------------------\n\nTITLE: Configuring event_time in Model Properties\nDESCRIPTION: Example of setting event_time in the model properties YAML file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/event-time.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: model_name\n    [config]:\n      event_time: my_time_field\n```\n\n----------------------------------------\n\nTITLE: Running dbt compile with --select Option\nDESCRIPTION: This command compiles and displays the SQL for a specific node named 'stg_orders'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/compile.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndbt compile --select \"stg_orders\"\n```\n\n----------------------------------------\n\nTITLE: Filtering Records in dbt (SQL)\nDESCRIPTION: This snippet demonstrates how to filter out records in a dbt model, effectively translating a DELETE operation into a SELECT statement.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/migrate-from-stored-procedures.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM {{ ref('stg_orders') }} WHERE order_status IS NOT NULL\n```\n\n----------------------------------------\n\nTITLE: Adding Grants Using the '+' Prefix in SQL\nDESCRIPTION: Demonstrates how to add grants to existing ones using the '+' prefix in a model's SQL file. This configuration adds 'user_c' to the list of grantees for the select privilege.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/grants.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(grants = {'+select': ['user_c']}) }}\n```\n\n----------------------------------------\n\nTITLE: SQL Example of Full Outer Join in MetricFlow\nDESCRIPTION: This SQL query shows how MetricFlow handles full outer joins between multiple fact tables, in this case 'sales' and 'returns'. It ensures all data points are captured from both tables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/join-logic.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nselect\n  sales.user_id,\n  sales.total_sales,\n  returns.total_returns\nfrom sales\nfull outer join returns\n  on sales.user_id = returns.user_id\nwhere sales.user_id is not null or returns.user_id is not null;\n```\n\n----------------------------------------\n\nTITLE: Defining Basic dbt Adapter Test Cases in Python\nDESCRIPTION: Base test case implementation importing and extending standard dbt adapter test classes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\n\nfrom dbt.tests.adapter.basic.test_base import BaseSimpleMaterializations\nfrom dbt.tests.adapter.basic.test_singular_tests import BaseSingularTests\nfrom dbt.tests.adapter.basic.test_singular_tests_ephemeral import BaseSingularTestsEphemeral\nfrom dbt.tests.adapter.basic.test_empty import BaseEmpty\nfrom dbt.tests.adapter.basic.test_ephemeral import BaseEphemeral\nfrom dbt.tests.adapter.basic.test_incremental import BaseIncremental\nfrom dbt.tests.adapter.basic.test_generic_tests import BaseGenericTests\nfrom dbt.tests.adapter.basic.test_snapshot_check_cols import BaseSnapshotCheckCols\nfrom dbt.tests.adapter.basic.test_snapshot_timestamp import BaseSnapshotTimestamp\nfrom dbt.tests.adapter.basic.test_adapter_methods import BaseAdapterMethod\n\nclass TestSimpleMaterializationsMyAdapter(BaseSimpleMaterializations):\n    pass\n\n\nclass TestSingularTestsMyAdapter(BaseSingularTests):\n    pass\n\n\nclass TestSingularTestsEphemeralMyAdapter(BaseSingularTestsEphemeral):\n    pass\n\n\nclass TestEmptyMyAdapter(BaseEmpty):\n    pass\n\n\nclass TestEphemeralMyAdapter(BaseEphemeral):\n    pass\n\n\nclass TestIncrementalMyAdapter(BaseIncremental):\n    pass\n\n\nclass TestGenericTestsMyAdapter(BaseGenericTests):\n    pass\n\n\nclass TestSnapshotCheckColsMyAdapter(BaseSnapshotCheckCols):\n    pass\n\n\nclass TestSnapshotTimestampMyAdapter(BaseSnapshotTimestamp):\n    pass\n\n\nclass TestBaseAdapterMethod(BaseAdapterMethod):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Basic SQL Inner Join Structure\nDESCRIPTION: Demonstrates the basic syntax structure for creating an inner join between two tables using a single join key.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/joins/sql-inner-join.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    <fields>\nfrom <table_1> as t1\ninner join <table_2> as t2\non t1.id = t2.id\n```\n\n----------------------------------------\n\nTITLE: Customer Data Transformation Model\nDESCRIPTION: Complex SQL transformation combining customer and order data with CTEs to create a consolidated customer view with order history\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/redshift-qs.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nwith customers as (\n\n    select\n        id as customer_id,\n        first_name,\n        last_name\n\n    from jaffle_shop.customers\n\n),\n\norders as (\n\n    select\n        id as order_id,\n        user_id as customer_id,\n        order_date,\n        status\n\n    from jaffle_shop.orders\n\n),\n\ncustomer_orders as (\n\n    select\n        customer_id,\n\n        min(order_date) as first_order_date,\n        max(order_date) as most_recent_order_date,\n        count(order_id) as number_of_orders\n\n    from orders\n\n    group by 1\n\n),\n\nfinal as (\n\n    select\n        customers.customer_id,\n        customers.first_name,\n        customers.last_name,\n        customer_orders.first_order_date,\n        customer_orders.most_recent_order_date,\n        coalesce(customer_orders.number_of_orders, 0) as number_of_orders\n\n    from customers\n\n    left join customer_orders using (customer_id)\n\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Running dbt retry after a successful dbt run\nDESCRIPTION: This snippet shows the output when attempting to use the retry command after a successful dbt run. Since there are no failed operations to retry, the command reports 'Nothing to do'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/retry.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nRunning with dbt=1.6.1\nRegistered adapter: duckdb=1.6.0\nFound 5 models, 3 seeds, 20 tests, 0 sources, 0 exposures, 0 metrics, 348 macros, 0 groups, 0 semantic models\n \nNothing to do. Try checking your model configs and model specification args\n```\n\n----------------------------------------\n\nTITLE: Configuring Infer Authentication in profiles.yml\nDESCRIPTION: This snippet shows the YAML configuration format needed in profiles.yml to connect dbt to Infer. It includes the required fields such as type, url, username, apikey, and data_config for the underlying data warehouse.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/infer-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n<profile-name>:\n  target: <target-name>\n  outputs:\n    <target-name>:\n      type: infer\n      url: \"<infer-api-endpoint>\"\n      username: \"<infer-api-username>\"\n      apikey: \"<infer-apikey>\"\n      data_config:\n        [configuration for your underlying data warehouse]  \n```\n\n----------------------------------------\n\nTITLE: Verifying dbt Core Installation Status using Bash\nDESCRIPTION: Command to check if dbt Core is already installed on your system by checking the path to the dbt executable.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/cloud-cli-installation.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nwhich dbt\n```\n\n----------------------------------------\n\nTITLE: YAML Entity Configuration v1.9+\nDESCRIPTION: Complete specification for configuring entities in semantic models with metadata support\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/entities.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_models:\n  - name: semantic_model_name\n   ..rest of the semantic model config\n    entities:\n      - name: entity_name  ## Required\n        type: Primary, natural, foreign, or unique ## Required\n        description: A description of the field or role the entity takes in this table  ## Optional\n        expr: The field that denotes that entity (transaction_id).  ## Optional\n              Defaults to name if unspecified.  \n        [config]: Specify configurations for entity.  ## Optional\n          [meta]: {<dictionary>} Set metadata for a resource and organize resources. Accepts plain text, spaces, and quotes.  ## Optional\n```\n\n----------------------------------------\n\nTITLE: Overriding Redshift List Relations Macro in dbt\nDESCRIPTION: This snippet mentions the need to override the 'redshift__list_relations_without_caching' macro in dbt to enable the use of 'Proxy Views' for efficient development. It's a crucial part of their development flow to reduce unnecessary data replication.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-04-17-dbt-squared.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nredshift__list_relations_without_caching\n```\n\n----------------------------------------\n\nTITLE: Schema Operations Example\nDESCRIPTION: Shows how to create and drop schemas using adapter methods\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/adapter.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{% do adapter.create_schema(api.Relation.create(database=target.database, schema=\"my_schema\")) %}\n```\n\n----------------------------------------\n\nTITLE: Compiled SQL for Oct 1, 2024 Batch\nDESCRIPTION: The compiled SQL that dbt generates for the Oct 1, 2024 batch of the sessions model. It shows how dbt automatically filters the page_views model based on the configured event_time column.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/incremental-microbatch.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nwith page_views as (\n\n    select * from (\n        -- filtered on configured event_time\n        select * from \"analytics\".\"page_views\"\n        where page_view_start >= '2024-10-01 00:00:00'  -- Oct 1\n        and page_view_start < '2024-10-02 00:00:00'\n    )\n\n),\n\ncustomers as (\n\n    select * from \"analytics\".\"customers\"\n\n),\n\n...\n```\n\n----------------------------------------\n\nTITLE: Installing dbt Adapter Plugins from Source\nDESCRIPTION: Commands to clone and install a dbt adapter plugin (using dbt-redshift as an example) from its GitHub repository. These steps will install the adapter from its source code.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/source-install.md#2025-04-09_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/dbt-labs/dbt-redshift.git\ncd dbt-redshift\npython -m pip install .\n```\n\n----------------------------------------\n\nTITLE: Configuring Conditional Project Hooks for Source Freshness\nDESCRIPTION: YAML configuration showing how to conditionally execute project hooks based on the command type. This example demonstrates preventing a hook from running during 'source freshness' commands when the 'source_freshness_run_project_hooks' flag is enabled.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/behavior-changes.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\non-run-start:\n  - '{{ ... if flags.WHICH != \\'freshness\\' }}'\n```\n\n----------------------------------------\n\nTITLE: Configuring Incremental Predicates in SQL Model\nDESCRIPTION: Example of using 'incremental_predicates' in a SQL model file. This configuration demonstrates how to set up incremental predicates along with other incremental configs.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/incremental-strategy.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(\n    materialized = 'incremental',\n    unique_key = 'id',\n    cluster_by = ['session_start'],  \n    incremental_strategy = 'merge',\n    incremental_predicates = [\n      \"DBT_INTERNAL_DEST.session_start > dateadd(day, -7, current_date)\"\n    ]\n  )\n}}\n\n...\n```\n\n----------------------------------------\n\nTITLE: Creating New Git Branch\nDESCRIPTION: Command to create and checkout a new Git branch for development work.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/manual-install-qs.md#2025-04-09_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ngit checkout -b add-customers-model\n```\n\n----------------------------------------\n\nTITLE: Implementing Incremental Materialization in Python with dbt-oracle\nDESCRIPTION: Python model that demonstrates incremental materialization by filtering for new data based on a timestamp column.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/oracle-setup.md#2025-04-09_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef model(dbt, session):\n    # Must be either table or incremental\n    dbt.config(materialized=\"incremental\")\n    # oml.DataFrame representing a datasource\n    sales_cost_df = dbt.ref(\"sales_cost\")\n\n    if dbt.is_incremental:\n        cr = session.cursor()\n        result = cr.execute(f\"select max(cost_timestamp) from {dbt.this.identifier}\")\n        max_timestamp = result.fetchone()[0]\n        # filter new rows\n        sales_cost_df = sales_cost_df[sales_cost_df[\"COST_TIMESTAMP\"] > max_timestamp]\n\n    return sales_cost_df\n```\n\n----------------------------------------\n\nTITLE: Setting SQL Header for Snapshots in Project Configuration\nDESCRIPTION: This YAML configuration shows how to set the sql_header for snapshots at the project level in the dbt_project.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/sql_header.md#2025-04-09_snippet_3\n\nLANGUAGE: yml\nCODE:\n```\nsnapshots:\n  [<resource-path>](/reference/resource-configs/resource-path):\n    +sql_header: <sql-statement>\n```\n\n----------------------------------------\n\nTITLE: Configuring Index and Distribution in a SQL Model File for Azure Synapse\nDESCRIPTION: Example of how to set up a Heap index and Round Robin distribution type within a SQL model file using the config() macro. This configuration affects how the table is physically stored in Azure Synapse.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/azuresynapse-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{{\\n    config(\\n        index='HEAP',\\n        dist='ROUND_ROBIN'\\n        )\\n}}\\n\\nselect *\\nfrom ...\n```\n\n----------------------------------------\n\nTITLE: Semantic Layer CLI Command - Export All Saved Queries\nDESCRIPTION: New dbt Cloud CLI command 'export-all' that enables exporting multiple or all saved queries at once, replacing the previous requirement of explicitly specifying individual saved queries.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/2024-release-notes.md#2025-04-09_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ndbt sl export-all\n```\n\n----------------------------------------\n\nTITLE: Configuring Index for View in SQL\nDESCRIPTION: This SQL snippet shows how to configure an index for a view, specifying the columns and cluster for the index.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/materialize-configs.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(materialized='view',\n          indexes=[{'columns': ['col_a'], 'cluster': 'cluster_a'}]) }}\n          indexes=[{'columns': ['symbol']}]) }}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: File-based Selection\nDESCRIPTION: Selecting models by their filename\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/methods.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# These are equivalent\ndbt run --select \"file:some_model.sql\"\ndbt run --select \"some_model.sql\"\ndbt run --select \"some_model\"\n```\n\n----------------------------------------\n\nTITLE: Serving dbt Documentation with Custom Host\nDESCRIPTION: This command serves the dbt documentation with a specified host. It uses the '--host' flag to set a custom host for the webserver. Available from version 1.8.2 onwards.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/cmd-docs.md#2025-04-09_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ndbt docs serve --host \"\"\n```\n\n----------------------------------------\n\nTITLE: Aggregating Order Statuses Using SQL ARRAY_AGG\nDESCRIPTION: Example query that demonstrates using ARRAY_AGG to create an array of distinct order statuses grouped by month. Uses date truncation and shows how to combine ARRAY_AGG with GROUP BY and ORDER BY clauses.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/aggregate-functions/sql-array-agg.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    date_trunc('month', order_date) as order_month,\n    array_agg(distinct status) as status_array\nfrom  {{ ref('orders') }}\ngroup by 1\norder by 1\n```\n\n----------------------------------------\n\nTITLE: Pandas Integration Example\nDESCRIPTION: Example of converting query results to a Pandas DataFrame using the to_pandas() method.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-python-sdk.md#2025-04-09_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# ... initialize client\n\narrow_table = client.query(...)\npandas_df = arrow_table.to_pandas()\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Index for View in SQL\nDESCRIPTION: This SQL configuration demonstrates how to set up a default index for a view that uses all columns.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/materialize-configs.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(materialized='view',\n    indexes=[{'default': True}]) }}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Saved Query Configuration Parameters Table Structure\nDESCRIPTION: Defines the required and optional parameters for configuring saved queries in MetricFlow, including name, description, label, query parameters, and export configurations. The structure varies slightly between dbt versions 1.7, 1.8, and 1.9+.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/saved-queries.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nname: string           # Required: Name of the saved query object\ndescription: string    # Required: Description of the saved query\nlabel: string          # Required: Display name shown in downstream tools\nconfig:                # Optional: Configuration settings\n  cache:\n    enabled: boolean   # Optional: Enable/disable query caching\nquery_params:         # Required: Query parameters structure\n  metrics: [string]   # Optional: List of metrics to query\n  group_by: [string]  # Optional: List of dimensions to group by\n  where: [string]     # Optional: List of filter conditions\nexports:              # Optional: Export configurations\n  - name: string      # Required: Export name\n    config:           # Required: Export settings\n      export_as: string  # Required: Export type (table/view)\n      schema: string     # Optional: Target schema\n      alias: string      # Optional: Table alias\n```\n\n----------------------------------------\n\nTITLE: Configuring Infer Profile in profiles.yml\nDESCRIPTION: YAML configuration template for setting up Infer connection in dbt profiles.yml file, including API endpoint, credentials and data warehouse configuration\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/infer-setup.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n<profile-name>:\n  target: <target-name>\n  outputs:\n    <target-name>:\n      type: infer\n      url: \"<infer-api-endpoint>\"\n      username: \"<infer-api-username>\"\n      apikey: \"<infer-apikey>\"\n      data_config:\n        [configuration for your underlying data warehouse]\n```\n\n----------------------------------------\n\nTITLE: Configuring Compression Settings in Greenplum dbt Model\nDESCRIPTION: Sets up table compression using ZLIB algorithm with specific compression level and block size. Includes appendonly setting which is required for compression.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/greenplum-configs.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        ...\n        appendonly='true',\n        compresstype='ZLIB',\n        compresslevel=3,\n        blocksize=32768\n        ...\n    )\n}}\n\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Executing dbt clone Command with Various Options\nDESCRIPTION: This snippet demonstrates different ways to use the 'dbt clone' command, including cloning all models, selecting specific models, using full-refresh, and specifying thread count. It showcases the flexibility of the command for various cloning scenarios.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/clone.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# clone all of my models from specified state to my target schema(s)\ndbt clone --state path/to/artifacts\n\n# clone one_specific_model of my models from specified state to my target schema(s)\ndbt clone --select \"one_specific_model\" --state path/to/artifacts\n\n# clone all of my models from specified state to my target schema(s) and recreate all pre-existing relations in the current target\ndbt clone --state path/to/artifacts --full-refresh\n\n# clone all of my models from specified state to my target schema(s), running up to 50 clone statements in parallel\ndbt clone --state path/to/artifacts --threads 50\n```\n\n----------------------------------------\n\nTITLE: Building a Customer Model in dbt\nDESCRIPTION: SQL query for creating a customer model in dbt that joins customer and order data from the Jaffle Shop sample dataset. The model transforms raw data into a denormalized view of customer information with order statistics.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/athena-qs.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nwith customers as (\n\n    select\n        id as customer_id,\n        first_name,\n        last_name\n\n    from jaffle_shop.customers\n\n),\n\norders as (\n\n    select\n        id as order_id,\n        user_id as customer_id,\n        order_date,\n        status\n\n    from jaffle_shop.orders\n\n),\n\ncustomer_orders as (\n\n    select\n        customer_id,\n\n        min(order_date) as first_order_date,\n        max(order_date) as most_recent_order_date,\n        count(order_id) as number_of_orders\n\n    from orders\n\n    group by 1\n\n),\n\nfinal as (\n\n    select\n        customers.customer_id,\n        customers.first_name,\n        customers.last_name,\n        customer_orders.first_order_date,\n        customer_orders.most_recent_order_date,\n        coalesce(customer_orders.number_of_orders, 0) as number_of_orders\n\n    from customers\n\n    left join customer_orders using (customer_id)\n\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Configuring Meta Properties in dbt_project.yml\nDESCRIPTION: Example demonstrating how to assign metadata properties to dimensions using the +meta syntax in the project configuration file. This approach allows for centralized metadata management at the project level.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/meta.md#2025-04-09_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic-models:\n  jaffle_shop:\n    ...\n    [dimensions]:\n      - name: order_date\n        config:\n          meta:\n            data_owner: \"Finance team\"\n            used_in_reporting: true\n```\n\n----------------------------------------\n\nTITLE: Listing JSON Output with Custom Keys in dbt\nDESCRIPTION: Example of using dbt ls to list resources in JSON format with specific output keys.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/list.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ dbt ls --select snowplow.* --output json --output-keys \"name resource_type description\"\n{\"name\": \"snowplow_events\", \"description\": \"This is a pretty cool model\",  ...}\n{\"name\": \"snowplow_page_views\", \"description\": \"This model is even cooler\",  ...}\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring a Cumulative Metric with Window in YAML (v1.9+)\nDESCRIPTION: This example demonstrates how to define a cumulative metric 'weekly_customers' with a 7-day window and additional configuration options in MetricFlow version 1.9 and later.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/cumulative-metrics.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics: \n  - name: weekly_customers\n  type: cumulative\n  type_params:\n    measure: customers\n    cumulative_type_params:\n      window: 7 days\n      period_agg: first\n```\n\n----------------------------------------\n\nTITLE: Configuring Test Failure Storage in YAML\nDESCRIPTION: This YAML configuration enables storing test failures and specifies a custom schema suffix for test views.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/materialize-configs.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ntests:\n  project_name:\n    +store_failures: true\n    +schema: test\n```\n\n----------------------------------------\n\nTITLE: Testing Incremental Models with YAML Config\nDESCRIPTION: YAML configuration for unit testing an incremental model in both full refresh and incremental modes. The tests override the is_incremental() macro to test different execution paths.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/unit-tests.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nunit_tests:\n  - name: my_incremental_model_full_refresh_mode\n    model: my_incremental_model\n    overrides:\n      macros:\n        # unit test this model in \"full refresh\" mode\n        is_incremental: false \n    given:\n      - input: ref('events')\n        rows:\n          - {event_id: 1, event_time: 2020-01-01}\n    expect:\n      rows:\n        - {event_id: 1, event_time: 2020-01-01}\n\n  - name: my_incremental_model_incremental_mode\n    model: my_incremental_model\n    overrides:\n      macros:\n        # unit test this model in \"incremental\" mode\n        is_incremental: true \n    given:\n      - input: ref('events')\n        rows:\n          - {event_id: 1, event_time: 2020-01-01}\n          - {event_id: 2, event_time: 2020-01-02}\n          - {event_id: 3, event_time: 2020-01-03}\n      - input: this \n        # contents of current my_incremental_model\n        rows:\n          - {event_id: 1, event_time: 2020-01-01}\n    expect:\n      # what will be inserted/merged into my_incremental_model\n      rows:\n        - {event_id: 2, event_time: 2020-01-02}\n        - {event_id: 3, event_time: 2020-01-03}\n```\n\n----------------------------------------\n\nTITLE: Configuring BitBucket PR Template URL in dbt Cloud\nDESCRIPTION: This snippet shows the PR template URL format for BitBucket repositories in dbt Cloud. It includes source and destination branch parameters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/git/pr-template.md#2025-04-09_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://bitbucket.org/<org>/<repo>/pull-requests/new?source={{source}}&dest={{destination}}\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Profile for Athena Connection in YAML\nDESCRIPTION: This YAML configuration sets up the dbt profile for connecting to Athena. It specifies various parameters such as S3 staging and data directories, region, database, schema, and AWS profile. The configuration also includes options for threading and retry attempts.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/athena-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndefault:\n  outputs:\n    dev:\n      type: athena\n      s3_staging_dir: [s3_staging_dir]\n      s3_data_dir: [s3_data_dir]\n      s3_data_naming: [table_unique] # the type of naming convention used when writing to S3\n      region_name: [region_name]\n      database: [database name]\n      schema: [dev_schema]\n      aws_profile_name: [optional profile to use from your AWS shared credentials file.]\n      threads: [1 or more]\n      num_retries: [0 or more] # number of retries performed by the adapter. Defaults to 5\n  target: dev\n```\n\n----------------------------------------\n\nTITLE: dbt Custom Constraints with Tags YAML\nDESCRIPTION: This YAML configuration shows how to implement tag-based masking policies with contracts and constraints. It defines a model with a custom constraint that sets a tag on the id column.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/constraints.md#2025-04-09_snippet_20\n\nLANGUAGE: yml\nCODE:\n```\nmodels:\n  - name: my_model\n    config:\n      contract:\n        enforced: true\n      materialized: table\n    columns:\n      - name: id\n        data_type: int\n        constraints:\n          - type: custom\n            expression: \"tag (my_tag = 'my_value')\" #  A custom SQL expression used to enforce a specific constraint on a column.\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced Table Options in SQL Model for Teradata\nDESCRIPTION: Setting multiple complex table options including fallback protection, journaling, checksum, merge block ratio, and isolated loading for a Teradata table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/teradata-configs.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n{{\n  config(\n      materialized=\"table\",\n      table_option=\"NO FALLBACK, NO JOURNAL, CHECKSUM = ON,\n        NO MERGEBLOCKRATIO,\n        WITH CONCURRENT ISOLATED LOADING FOR ALL\"\n  )\n}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Source Column Properties in DBT YAML\nDESCRIPTION: YAML configuration for defining column properties in DBT sources, including table definitions with column specifications.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/columns.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: <source_name>\n    tables:\n    - name: <table_name>\n      columns:\n        - name: <column_name>\n          description: <markdown_string>\n          data_type: <string>\n          quote: true | false\n          tests: ...\n          tags: ...\n          meta: ...\n        - name: <another_column>\n          ...\n```\n\n----------------------------------------\n\nTITLE: Basic Variable Usage in SQL Model\nDESCRIPTION: Example of using the var() function to inject a variable value into a SQL query. This demonstrates how to reference a variable that must be defined in the project configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/var.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect * from events where event_type = '{{ var(\"event_type\") }}'\n```\n\n----------------------------------------\n\nTITLE: Configuring Group Assignment in SQL Model\nDESCRIPTION: Shows how to assign a model to a group using in-file configuration within a SQL model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/groups.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(group = 'finance') }}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Creating Historical RFM Segmentation Model in SQL\nDESCRIPTION: Creates a comprehensive RFM (Recency, Frequency, Monetary) segmentation model that tracks customer segments over time on a monthly basis. The model calculates customer scores and segments based on payment history, incorporating historical tracking through date-based snapshots.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-08-building-a-historical-user-segmentation-model-with-dbt.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nWITH payments AS(\n    SELECT *\n    FROM ref {{'fact_payments'}}\n),\nmonths AS(\n\tSELECT NOW() AS date_month\n    UNION ALL\n    SELECT DISTINCT date_month AS date_month\n    FROM ref {{'dim_calendar'}}\n),\npayments_with_months AS(\n    SELECT  user_id,\n            date_month,\n            payment_date,\n            payment_id,\n            payment_amount\n    FROM months\n        JOIN payments ON payment_date <= date_month \n),\nrfm_values AS (\n    SELECT  user_id,\n            date_month,\n            MAX(payment_date) AS max_payment_date,\n            date_month - MAX(payment_date) AS recency,\n            COUNT(DISTINCT payment_id) AS frequency,\n            SUM(payment_amount) AS monetary\n    FROM payments_with_months\n    GROUP BY user_id, date_month\n),\nrfm_percentiles AS (\n    SELECT  user_id,\n            date_month,\n            recency,\n            frequency,\n            monetary,\n            PERCENT_RANK() OVER (ORDER BY recency DESC) AS recency_percentile,\n            PERCENT_RANK() OVER (ORDER BY frequency ASC) AS frequency_percentile,\n            PERCENT_RANK() OVER (ORDER BY monetary ASC) AS monetary_percentile\n    FROM rfm_values\n),\nrfm_scores AS(\n    SELECT  *,\n            CASE\n                WHEN recency_percentile >= 0.8 THEN 5\n                WHEN recency_percentile >= 0.6 THEN 4\n                WHEN recency_percentile >= 0.4 THEN 3\n                WHEN recency_percentile >= 0.2 THEN 2\n                ELSE 1\n                END AS recency_score,\n            CASE\n                WHEN frequency_percentile >= 0.8 THEN 5\n                WHEN frequency_percentile >= 0.6 THEN 4\n                WHEN frequency_percentile >= 0.4 THEN 3\n                WHEN frequency_percentile >= 0.2 THEN 2\n                ELSE 1\n                END AS frequency_score,\n            CASE\n                WHEN monetary_percentile >= 0.8 THEN 5\n                WHEN monetary_percentile >= 0.6 THEN 4\n                WHEN monetary_percentile >= 0.4 THEN 3\n                WHEN monetary_percentile >= 0.2 THEN 2\n                ELSE 1\n                END AS monetary_score\n    FROM rfm_percentiles\n),\nrfm_segment AS(\nSELECT *,\n        CASE\n            WHEN recency_score <= 2\n                AND frequency_score <= 2 THEN 'Hibernating'\n            WHEN recency_score <= 2\n                AND frequency_score <= 4 THEN 'At Risk'\n            WHEN recency_score <= 2\n                AND frequency_score <= 5 THEN 'Cannot Lose Them'\n            WHEN recency_score <= 3\n                AND frequency_score <= 2 THEN 'About to Sleep'\n            WHEN recency_score <= 3\n                AND frequency_score <= 3 THEN 'Need Attention'\n            WHEN recency_score <= 4\n                AND frequency_score <= 1 THEN 'Promising'\n            WHEN recency_score <= 4\n                AND frequency_score <= 3 THEN 'Potential Loyalists'\n            WHEN recency_score <= 4\n                AND frequency_score <= 5 THEN 'Loyal Customers'\n            WHEN recency_score <= 5\n                AND frequency_score <= 1 THEN 'New Customers'\n            WHEN recency_score <= 5\n                AND frequency_score <= 3 THEN 'Potential Loyalists'\n            ELSE 'Champions'\n        END AS rfm_segment\nFROM  rfm_scores\n)\nSELECT *\nFROM rfm_segment\n```\n\n----------------------------------------\n\nTITLE: Enhanced dbt YAML with Relationships for Staff Member Model\nDESCRIPTION: This YAML snippet expands on the previous example by adding relationship tests and more specific accepted_values tests. It demonstrates how to include references to other tables and specify categorical values for certain fields.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-07-17-GPT-and-dbt-test.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: STAFF_MEMBER\n    description: This table contains information about the staff members.\n    columns:\n      - name: ID\n        description: The unique identifier for the staff member.\n        tests:\n          - unique\n          - not_null\n      - name: CREATEDATETIME\n        description: The timestamp when the record was created.\n        tests:\n          - not_null\n      - name: UPDATEDATETIME\n        description: The timestamp when the record was last updated.\n      - name: VERSION\n        description: Version number of the record.\n      - name: FIRSTNAME\n        description: The first name of the staff member.\n        tests:\n          - not_null\n      - name: JOBTITLE\n        description: The job title of the staff member. This is a categorical field.\n        tests:\n          - not_null\n          - accepted_values:\n              values: ['Job Title 1', 'Job Title 2', 'Job Title 3'] # replace these with actual job titles\n      - name: LASTNAME\n        description: The last name of the staff member.\n        tests:\n          - not_null\n      - name: MIDDLENAME\n        description: The middle name of the staff member.\n      - name: ISCARADMIN\n        description: Boolean value indicating if the staff member is a care administrator.\n        tests:\n          - accepted_values:\n              values: ['true', 'false']\n      - name: ISARCHIVED\n        description: Boolean value indicating if the staff member record is archived.\n        tests:\n          - accepted_values:\n              values: ['true', 'false']\n      - name: COMMUNITYID\n        description: Identifier for the community of the staff member.\n        tests:\n          - relationships:\n              to: STAGING.COMMUNITY.ID # replace with actual reference table\n              field: ID\n      - name: ENTERPRISEID\n        description: Identifier for the enterprise of the staff member.\n        tests:\n          - relationships:\n              to: STAGING.ENTERPRISE.ID # replace with actual reference table\n              field: ID\n      - name: ISDELETED\n        description: Boolean value indicating if the staff member record is deleted.\n        tests:\n          - accepted_values:\n              values: ['true', 'false']\n```\n\n----------------------------------------\n\nTITLE: Configuring Generic Test Block Failure Storage\nDESCRIPTION: Demonstrates setting default store_failures configuration for all instances of a generic test within its test block definition.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/store_failures.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{% test <testname>(model, column_name) %}\n\n{{ config(store_failures = false) }}\n\nselect ...\n\n{% endtest %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Dynamic Source Databases Using target.name in YAML\nDESCRIPTION: This YAML configuration demonstrates how to dynamically set the database name for a source based on the target environment (dev/qa/prod). It uses Jinja conditionals to determine the appropriate raw database for each environment.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/target.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n \nsources:\n  - name: source_name \n    database: |\n      {%- if  target.name == \"dev\" -%} raw_dev\n      {%- elif target.name == \"qa\"  -%} raw_qa\n      {%- elif target.name == \"prod\"  -%} raw_prod\n      {%- else -%} invalid_database\n      {%- endif -%}\n    schema: source_schema\n```\n\n----------------------------------------\n\nTITLE: Using config.require for Mandatory Configurations\nDESCRIPTION: This example shows how to use the config.require function to enforce mandatory configuration parameters in a materialization. If the specified configuration is not provided, it will result in a compilation error.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/config.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{% materialization incremental, default -%}\n  {%- set unique_key = config.require('unique_key') -%}\n  ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Iceberg Table Materialization in dbt SQL Model\nDESCRIPTION: This SQL snippet demonstrates how to configure an Iceberg table materialization in a dbt model file. It sets the materialization type to 'table', specifies the table format as 'iceberg', and sets the external volume.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snowflake-configs.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(\n    materialized = \"table\",\n    table_format=\"iceberg\",\n    external_volume=\"s3_iceberg_snow\",\n  )\n}}\n\nselect * from {{ ref('raw_orders') }}\n```\n\n----------------------------------------\n\nTITLE: Listing Optional Teradata Profile Fields in Markdown\nDESCRIPTION: This snippet provides a comprehensive list of optional connection parameters for a Teradata profile in dbt. It includes details on various aspects of the connection, such as authentication methods, timeouts, encryption, and SSL configurations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/teradata-setup.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\nParameter               | Default     | Type           | Description\n----------------------- | ----------- | -------------- | ---\n`account`               |             | string         | Specifies the database account. Equivalent to the Teradata JDBC Driver `ACCOUNT` connection parameter.\n`browser`               |             | string         | Specifies the command to open the browser for Browser Authentication, when logmech is BROWSER. Browser Authentication is supported for Windows and macOS. Equivalent to the Teradata JDBC Driver BROWSER connection parameter.\n`browser_tab_timeout`   |   `\"5\"`     | quoted integer | Specifies the number of seconds to wait before closing the browser tab after Browser Authentication is completed. The default is 5 seconds. The behavior is under the browser's control, and not all browsers support automatic closing of browser tabs.\n`browser_timeout`       |   `\"180\"`   | quoted integer | Specifies the number of seconds that the driver will wait for Browser Authentication to complete. The default is 180 seconds (3 minutes).\n`column_name`           | `\"false\"`   | quoted boolean | Controls the behavior of cursor `.description` sequence `name` items. Equivalent to the Teradata JDBC Driver `COLUMN_NAME` connection parameter. False specifies that a cursor `.description` sequence `name` item provides the AS-clause name if available, or the column name if available, or the column title. True specifies that a cursor `.description` sequence `name` item provides the column name if available, but has no effect when StatementInfo parcel support is unavailable.\n`connect_timeout`       |  `\"10000\"`  | quoted integer | Specifies the timeout in milliseconds for establishing a TCP socket connection. Specify 0 for no timeout. The default is 10 seconds (10000 milliseconds).\n`cop`                   | `\"true\"`    | quoted boolean | Specifies whether COP Discovery is performed. Equivalent to the Teradata JDBC Driver `COP` connection parameter.\n`coplast`               | `\"false\"`   | quoted boolean | Specifies how COP Discovery determines the last COP hostname. Equivalent to the Teradata JDBC Driver `COPLAST` connection parameter. When `coplast` is `false` or omitted, or COP Discovery is turned off, then no DNS lookup occurs for the coplast hostname. When `coplast` is `true`, and COP Discovery is turned on, then a DNS lookup occurs for a coplast hostname.\n`port`                  | `\"1025\"`    | quoted integer | Specifies the database port number. Equivalent to the Teradata JDBC Driver `DBS_PORT` connection parameter.\n`encryptdata`           | `\"false\"`   | quoted boolean | Controls encryption of data exchanged between the driver and the database. Equivalent to the Teradata JDBC Driver `ENCRYPTDATA` connection parameter.\n`fake_result_sets`      | `\"false\"`   | quoted boolean | Controls whether a fake result set containing statement metadata precedes each real result set.\n`field_quote`           | `\"\\\"\"`      | string         | Specifies a single character string used to quote fields in a CSV file.\n`field_sep`             | `\",\"`       | string         | Specifies a single character string used to separate fields in a CSV file. Equivalent to the Teradata JDBC Driver `FIELD_SEP` connection parameter.\n`host`                  |             | string         | Specifies the database hostname.\n`https_port`            | `\"443\"`     | quoted integer | Specifies the database port number for HTTPS/TLS connections. Equivalent to the Teradata JDBC Driver `HTTPS_PORT` connection parameter.\n`lob_support`           | `\"true\"`    | quoted boolean | Controls LOB support. Equivalent to the Teradata JDBC Driver `LOB_SUPPORT` connection parameter.\n`log`                   | `\"0\"`       | quoted integer | Controls debug logging. Somewhat equivalent to the Teradata JDBC Driver `LOG` connection parameter. This parameter's behavior is subject to change in the future. This parameter's value is currently defined as an integer in which the 1-bit governs function and method tracing, the 2-bit governs debug logging, the 4-bit governs transmit and receive message hex dumps, and the 8-bit governs timing. Compose the value by adding together 1, 2, 4, and/or 8.\n`logdata`               |             | string         | Specifies extra data for the chosen logon authentication method. Equivalent to the Teradata JDBC Driver `LOGDATA` connection parameter.\n`logon_timeout`         | `\"0\"`       | quoted integer | Specifies the logon timeout in seconds. Zero means no timeout.\n`logmech`               | `\"TD2\"`     | string         | Specifies the logon authentication method. Equivalent to the Teradata JDBC Driver `LOGMECH` connection parameter. Possible values are `TD2` (the default), `JWT`, `LDAP`, `BROWSER`, `KRB5` for Kerberos, or `TDNEGO`.\n`max_message_body`      | `\"2097000\"` | quoted integer | Specifies the maximum Response Message size in bytes. Equivalent to the Teradata JDBC Driver `MAX_MESSAGE_BODY` connection parameter.\n`partition`             | `\"DBC/SQL\"` | string         | Specifies the database partition. Equivalent to the Teradata JDBC Driver `PARTITION` connection parameter.\n`request_timeout`       |   `\"0\"`     | quoted integer | Specifies the timeout for executing each SQL request. Zero means no timeout.\n`retries`               |   `0`       | integer        | Allows an adapter to automatically try again when the attempt to open a new connection on the database has a transient, infrequent error. This option can be set using the retries configuration. Default value is 0. The default wait period between connection attempts is one second. retry_timeout (seconds) option allows us to adjust this waiting period.\n`runstartup`            |  \"false\"    | quoted boolean | Controls whether the user's STARTUP SQL request is executed after logon. For more information, refer to User STARTUP SQL Request. Equivalent to the Teradata JDBC Driver RUNSTARTUP connection parameter. If retries is set to 3, the adapter will try to establish a new connection three times if an error occurs.\n`sessions`              |             | quoted integer | Specifies the number of data transfer connections for FastLoad or FastExport. The default (recommended) lets the database choose the appropriate number of connections. Equivalent to the Teradata JDBC Driver SESSIONS connection parameter.\n`sip_support`           | `\"true\"`    | quoted boolean | Controls whether StatementInfo parcel is used. Equivalent to the Teradata JDBC Driver `SIP_SUPPORT` connection parameter.\n`sp_spl`                | `\"true\"`    | quoted boolean | Controls whether stored procedure source code is saved in the database when a SQL stored procedure is created. Equivalent to the Teradata JDBC Driver SP_SPL connection parameter.\n`sslca`                 |             | string         | Specifies the file name of a PEM file that contains Certificate Authority (CA) certificates for use with `sslmode` values `VERIFY-CA` or `VERIFY-FULL`. Equivalent to the Teradata JDBC Driver `SSLCA` connection parameter.\n`sslcrc`                | `\"ALLOW\"`   | string         | Equivalent to the Teradata JDBC Driver SSLCRC connection parameter. Values are case-insensitive.<br/>&bull; ALLOW provides \"soft fail\" behavior such that communication failures are ignored during certificate revocation checking. <br/>&bull; REQUIRE mandates that certificate revocation checking must succeed.\n`sslcapath`             |             | string         | Specifies a directory of PEM files that contain Certificate Authority (CA) certificates for use with `sslmode` values `VERIFY-CA` or `VERIFY-FULL`. Only files with an extension of `.pem` are used. Other files in the specified directory are not used. Equivalent to the Teradata JDBC Driver `SSLCAPATH` connection parameter.\n`sslcipher`             |             | string         | Specifies the TLS cipher for HTTPS/TLS connections. Equivalent to the Teradata JDBC Driver `SSLCIPHER` connection parameter.\n`sslmode`               | `\"PREFER\"`  | string         | Specifies the mode for connections to the database. Equivalent to the Teradata JDBC Driver `SSLMODE` connection parameter.<br/>&bull; `DISABLE` disables HTTPS/TLS connections and uses only non-TLS connections.<br/>&bull; `ALLOW` uses non-TLS connections unless the database requires HTTPS/TLS connections.<br/>&bull; `PREFER` uses HTTPS/TLS connections unless the database does not offer HTTPS/TLS connections.<br/>&bull; `REQUIRE` uses only HTTPS/TLS connections.<br/>&bull; `VERIFY-CA` uses only HTTPS/TLS connections and verifies that the server certificate is valid and trusted.<br/>&bull; `VERIFY-FULL` uses only HTTPS/TLS connections, verifies that the server certificate is valid and trusted, and verifies that the server certificate matches the database hostname.\n`sslprotocol`           | `\"TLSv1.2\"` | string         | Specifies the TLS protocol for HTTPS/TLS connections. Equivalent to the Teradata JDBC Driver `SSLPROTOCOL` connection parameter.\n```\n\n----------------------------------------\n\nTITLE: Configuring Cumulative Metrics with Window and Grain-to-Date in YAML (v1.8 and earlier)\nDESCRIPTION: This snippet shows how to define cumulative metrics using both a 1-month window and a monthly grain-to-date approach in MetricFlow versions 1.8 and earlier.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/cumulative-metrics.md#2025-04-09_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  - name: cumulative_order_total_l1m\n    label: Cumulative order total (L1M)\n    description: Trailing 1-month cumulative order amount\n    type: cumulative\n    type_params:\n      measure: order_total\n    window: 1 month\n  - name: cumulative_order_total_mtd\n    label: Cumulative order total (MTD)\n    description: The month-to-date value of all orders\n    type: cumulative\n    type_params:\n      measure: order_total\n      grain_to_date: month\n```\n\n----------------------------------------\n\nTITLE: Aliasing a Seed for country_codes.csv\nDESCRIPTION: Sets an alias for the seed at seeds/country_codes.csv in the dbt_project.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/alias.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  jaffle_shop:\n    country_codes:\n      +alias: country_mappings\n```\n\n----------------------------------------\n\nTITLE: Configuring Data Tests in YAML Properties File\nDESCRIPTION: Example of how to configure tests in a YAML properties file. This shows the syntax for configuring both resource-level and column-level generic tests with various configuration options.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/data-test-configs.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\n<resource_type>:\n  - name: <resource_name>\n    tests:\n      - <test_name>: # # Actual name of the test. For example, dbt_utils.equality\n          name: # Human friendly name for the test. For example, equality_fct_test_coverage\n          [description]: \"markdown formatting\"\n          <argument_name>: <argument_value>\n          [config]:\n            [fail_calc]: <string>\n            [limit]: <integer>\n            [severity]: error | warn\n            [error_if]: <string>\n            [warn_if]: <string>\n            [store_failures]: true | false\n            [where]: <string>\n\n    [columns]:\n      - name: <column_name>\n        tests:\n          - <test_name>:\n              name: \n              [description]: \"markdown formatting\"\n              <argument_name>: <argument_value>\n              [config]:\n                [fail_calc]: <string>\n                [limit]: <integer>\n                [severity]: error | warn\n                [error_if]: <string>\n                [warn_if]: <string>\n                [store_failures]: true | false\n                [where]: <string>\n```\n\n----------------------------------------\n\nTITLE: Configuring a Single Model in SQL File\nDESCRIPTION: This example shows how to configure a specific dbt model directly in its SQL file using the config() macro. The configuration sets the materialization type as table and specifies sort and distribution keys.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/model-configs.md#2025-04-09_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(\n    materialized = \"table\",\n    sort = 'event_time',\n    dist = 'event_id'\n  )\n}}\n\n\nselect * from ...\n```\n\n----------------------------------------\n\nTITLE: Setting Model Access Using Older Method in Properties YAML\nDESCRIPTION: Demonstrates the older method of configuring access for a specific model in a properties.yml file. This approach is still supported and sets the model to be publicly accessible.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/access.md#2025-04-09_snippet_1\n\nLANGUAGE: yml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: my_public_model\n    access: public # Older method, still supported\n```\n\n----------------------------------------\n\nTITLE: Controlling File Log Color via Command Line Flags\nDESCRIPTION: These commands demonstrate how to enable or disable color output for file logs in dbt using command-line flags.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/print-output.md#2025-04-09_snippet_5\n\nLANGUAGE: text\nCODE:\n```\ndbt --use-colors-file run\ndbt --no-use-colors-file run\n```\n\n----------------------------------------\n\nTITLE: Example: Custom Node Color in SQL Model File\nDESCRIPTION: Demonstrates how to configure node color directly in a SQL model file using the config() macro, which will override project and schema-level configurations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/docs.md#2025-04-09_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        materialized = 'view',\n        tags=['finance'],\n        docs={'node_color': 'red'}\n    )\n}}\n\nwith orders as (\n    \n    select * from {{ ref('stg_tpch_orders') }} \n\n),\norder_item as (\n    \n    select * from {{ ref('order_items') }}\n\n),\norder_item_summary as (\n\n    select \n        order_key,\n        sum(gross_item_sales_amount) as gross_item_sales_amount,\n        sum(item_discount_amount) as item_discount_amount,\n        sum(item_tax_amount) as item_tax_amount,\n        sum(net_item_sales_amount) as net_item_sales_amount\n    from order_item\n    group by\n        1\n),\nfinal as (\n\n    select \n\n        orders.order_key, \n        orders.order_date,\n        orders.customer_key,\n        orders.status_code,\n        orders.priority_code,\n        orders.clerk_name,\n        orders.ship_priority,\n                \n        1 as order_count,                \n        order_item_summary.gross_item_sales_amount,\n        order_item_summary.item_discount_amount,\n        order_item_summary.item_tax_amount,\n        order_item_summary.net_item_sales_amount\n    from\n        orders\n        inner join order_item_summary\n            on orders.order_key = order_item_summary.order_key\n)\nselect \n    *\nfrom\n    final\n\norder by\n    order_date\n```\n\n----------------------------------------\n\nTITLE: Using DATE_TRUNC in Snowflake and Databricks\nDESCRIPTION: Demonstrates the syntax for using the DATE_TRUNC function in Snowflake and Databricks, where the date part is the first argument.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/date-functions/sql-date-trunc.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\ndate_trunc(<date_part>, <date/time field>)\n```\n\n----------------------------------------\n\nTITLE: Fully Specified Versioned Model Schema\nDESCRIPTION: Complete YAML schema definition with full specifications for each version of the model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/govern/model-versions.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: dim_customers\n    latest_version: 1\n    \n    versions:\n      - v: 2\n        config:\n          materialized: table\n          contract: {enforced: true}\n        columns:\n          - name: customer_id\n            description: This is the primary key\n            data_type: int\n      \n      - v: 1\n        config:\n          materialized: table\n          contract: {enforced: true}\n        columns:\n          - name: customer_id\n            description: This is the primary key\n            data_type: int\n          - name: country_name\n            description: Where this customer lives\n            data_type: varchar\n```\n\n----------------------------------------\n\nTITLE: Updating Expression_is_true Test Syntax in dbt\nDESCRIPTION: This YAML snippet demonstrates the change in syntax for the expression_is_true test. The 'condition' argument is replaced with 'where', which is now available natively to all tests in dbt Core.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/core-upgrade/11-Older versions/upgrading-to-dbt-utils-v1.0.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: old_syntax\n    tests:\n      - dbt_utils.expression_is_true:\n          expression: \"col_a + col_b = total\"\n          #replace this...\n          condition: \"created_at > '2018-12-31'\" \n\n  - name: new_syntax\n    tests:\n      - dbt_utils.expression_is_true:\n          expression: \"col_a + col_b = total\"\n          # ...with this...\n          where: \"created_at > '2018-12-31'\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Access in dbt_project.yml for a Subfolder\nDESCRIPTION: Sets default access level for all models within a subfolder using the dbt_project.yml file. This approach applies the private access modifier to all models in the specified subfolder.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/access.md#2025-04-09_snippet_3\n\nLANGUAGE: yml\nCODE:\n```\nmodels:\n  my_project_name:\n    subfolder_name:\n      +group: my_group\n      +access: private  # sets default for all models in this subfolder\n```\n\n----------------------------------------\n\nTITLE: Configuring Measures in YAML (dbt v1.8 and earlier)\nDESCRIPTION: YAML specification for defining measures within semantic models in dbt v1.8 and earlier. This example shows the structure without the config property which was introduced in later versions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/measures.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_models:\n  - name: semantic_model_name\n   ..rest of the semantic model config\n    measures:\n      - name: The name of the measure\n        description: 'same as always' ## Optional\n        agg: the aggregation type.\n        expr: the field\n        agg_params: 'specific aggregation properties such as a percentile'  ## Optional\n        agg_time_dimension: The time field. Defaults to the default agg time dimension for the semantic model. ##  Optional\n        non_additive_dimension: 'Use these configs when you need non-additive dimensions.' ## Optional\n```\n\n----------------------------------------\n\nTITLE: Configuring Redshift Profile with IAM User Authentication in YAML\nDESCRIPTION: This YAML configuration sets up a Redshift profile using IAM User authentication via AWS Profile. It includes parameters specific to IAM authentication, as well as optional Redshift configurations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/redshift-setup.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmy-redshift-db:\n  target: dev\n  outputs:\n    dev:\n      type: redshift\n      method: iam\n      cluster_id: CLUSTER_ID\n      host: hostname.region.redshift.amazonaws.com\n      user: alice\n      iam_profile: analyst\n      region: us-east-1\n      dbname: analytics\n      schema: analytics\n      port: 5439\n\n      # Optional Redshift configs:\n      threads: 4\n      connect_timeout: None \n      retries: 1 \n      role: None\n      sslmode: prefer \n      ra3_node: true  \n      autocommit: true  \n      autocreate: true  \n      db_groups: ['ANALYSTS']\n```\n\n----------------------------------------\n\nTITLE: Disabling JSON Artifact Generation in dbt CLI\nDESCRIPTION: This command demonstrates how to run dbt without generating JSON artifacts. The --no-write-json flag is used to disable JSON serialization, which can potentially speed up dbt invocations or prevent overwriting artifacts from previous runs.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/json-artifacts.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ndbt run --no-write-json \n```\n\n----------------------------------------\n\nTITLE: Staging Formula 1 Results Data\nDESCRIPTION: Creates a staging model for Formula 1 race results, including position, points, and fastest lap information\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nwith\n\nsource  as (\n\n    select * from {{ source('formula1','results') }}\n\n),\n\nrenamed as (\n    select\n        resultid as result_id,\n        raceid as race_id,\n        driverid as driver_id,\n        constructorid as constructor_id,\n        number as driver_number,\n        grid,\n        position::int as position,\n        positiontext as position_text,\n        positionorder as position_order,\n        points,\n        laps,\n        time as results_time_formatted,\n        milliseconds as results_milliseconds,\n        fastestlap as fastest_lap,\n        rank as results_rank,\n        fastestlaptime as fastest_lap_time_formatted,\n        fastestlapspeed::decimal(6,3) as fastest_lap_speed,\n        statusid as status_id\n    from source\n)\n\nselect * from renamed\n```\n\n----------------------------------------\n\nTITLE: Staging Formula 1 Results Data\nDESCRIPTION: Creates a staging model for Formula 1 race results, including position, points, and fastest lap information\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nwith\n\nsource  as (\n\n    select * from {{ source('formula1','results') }}\n\n),\n\nrenamed as (\n    select\n        resultid as result_id,\n        raceid as race_id,\n        driverid as driver_id,\n        constructorid as constructor_id,\n        number as driver_number,\n        grid,\n        position::int as position,\n        positiontext as position_text,\n        positionorder as position_order,\n        points,\n        laps,\n        time as results_time_formatted,\n        milliseconds as results_milliseconds,\n        fastestlap as fastest_lap,\n        rank as results_rank,\n        fastestlaptime as fastest_lap_time_formatted,\n        fastestlapspeed::decimal(6,3) as fastest_lap_speed,\n        statusid as status_id\n    from source\n)\n\nselect * from renamed\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Seed Hooks in project.yml\nDESCRIPTION: Shows the available hook types that can be used with dbt seeds. Hooks can be configured in the dbt_project.yml file and include pre-hooks, post-hooks, on-run-start, and on-run-end hooks.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Seeds/seed-hooks.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndbt_project.yml:\n\n# Example hook configuration\nhooks:\n  pre-hook: \"<sql-statement>\"\n  post-hook: \"<sql-statement>\"\n  on-run-start: \"<sql-statement>\"\n  on-run-end: \"<sql-statement>\"\n```\n\n----------------------------------------\n\nTITLE: Advanced Orders Snapshot with Coalesced Timestamp\nDESCRIPTION: SQL snapshot implementation using a coalesced timestamp field for more reliable change tracking.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/updated_at.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot orders_snapshot %}\n\n{{\n    config(\n      target_schema='snapshots',\n      unique_key='id',\n\n      strategy='timestamp',\n      updated_at='updated_at_for_snapshot'\n    )\n}}\n\nselect\n    *,\n    coalesce(updated_at, created_at) as updated_at_for_snapshot\n\nfrom {{ source('jaffle_shop', 'orders') }}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Exploring JSON Structure of dbt Semantic Manifest\nDESCRIPTION: This code snippet shows the structure of the semantic_manifest.json file with its top-level keys including semantic_models, metrics, project_configuration, and saved_queries. The example illustrates how semantic models, metrics, and queries are defined within the manifest.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/artifacts/sl-manifest.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"semantic_models\": [\n        {\n            \"name\": \"semantic model name\",\n            \"defaults\": null,\n            \"description\": \"semantic model description\",\n            \"node_relation\": {\n                \"alias\": \"model alias\",\n                \"schema_name\": \"model schema\",\n                \"database\": \"model db\",\n                \"relation_name\": \"Fully qualified relation name\"\n            },\n            \"entities\": [\"entities in the semantic model\"],\n            \"measures\": [\"measures in the semantic model\"],\n            \"dimensions\": [\"dimensions in the semantic model\" ],\n    \"metrics\": [\n        {\n            \"name\": \"name of the metric\",\n            \"description\": \"metric description\",\n            \"type\": \"metric type\",\n            \"type_params\": {\n                \"measure\": {\n                    \"name\": \"name for measure\",\n                    \"filter\": \"filter for measure\",\n                    \"alias\": \"alias for measure\"\n                },\n                \"numerator\": null,\n                \"denominator\": null,\n                \"expr\": null,\n                \"window\": null,\n                \"grain_to_date\": null,\n                \"metrics\": [\"metrics used in defining the metric. this is used in derived metrics\"],\n                \"input_measures\": []\n            },\n            \"filter\": null,\n            \"metadata\": null\n        }\n    ],\n    \"project_configuration\": {\n        \"time_spine_table_configurations\": [\n            {\n                \"location\": \"fully qualified table name for timespine\",\n                \"column_name\": \"date column\",\n                \"grain\": \"day\"\n            }\n        ],\n        \"metadata\": null,\n        \"dsi_package_version\": {}\n    },\n    \"saved_queries\": [\n        {\n            \"name\": \"name of the saved query\",\n            \"query_params\": {\n                \"metrics\": [\n                    \"metrics used in the saved query\"\n                ],\n                \"group_by\": [\n                    \"TimeDimension('model_primary_key__date_column', 'day')\",\n                    \"Dimension('model_primary_key__metric_one')\",\n                    \"Dimension('model__dimension')\"\n                ],\n                \"where\": null\n            },\n            \"description\": \"Description of the saved query\",\n            \"metadata\": null,\n            \"label\": null,\n            \"exports\": [\n                {\n                    \"name\": \"saved_query_name\",\n                    \"config\": {\n                        \"export_as\": \"view\",\n                        \"schema_name\": null,\n                        \"alias\": null\n                    }\n                }\n            ]\n        }\n    ]\n}\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Incremental Model with Append Strategy in SQL\nDESCRIPTION: This SQL snippet demonstrates how to configure an incremental model using the append strategy. It includes a conditional statement to filter data based on the last processed timestamp when running incrementally.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/glue-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized='incremental',\n    incremental_strategy='append',\n) }}\n\n--  All rows returned by this query will be appended to the existing table\n\nselect * from {{ ref('events') }}\n{% if is_incremental() %}\n  where event_ts > (select max(event_ts) from {{ this }})\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Starting Local Airflow Deployment with Astro CLI\nDESCRIPTION: Command to start a local Airflow deployment using the Astro CLI, which sets up all necessary components in Docker containers.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/airflow-and-dbt-cloud.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nastro dev start\n```\n\n----------------------------------------\n\nTITLE: Executing a Statement Block in SQL\nDESCRIPTION: This snippet demonstrates how to use a statement block to fetch distinct states from a users table. It includes a dependency declaration and uses the statement function with fetch_result set to True.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/statement-blocks.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n-- depends_on: {{ ref('users') }}\n\n{%- call statement('states', fetch_result=True) -%}\n\n    select distinct state from {{ ref('users') }}\n\n{%- endcall -%}\n```\n\n----------------------------------------\n\nTITLE: Adding Tests to dbt Models via Schema.yml\nDESCRIPTION: Creates a schema.yml file that defines tests for the models in the project. This includes uniqueness, not-null, accepted values, and referential integrity tests that help validate data quality.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/teradata-qs.md#2025-04-09_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: bi_customers\n    columns:\n      - name: customer_id\n        tests:\n          - unique\n          - not_null\n\n  - name: stg_customers\n    columns:\n      - name: customer_id\n        tests:\n          - unique\n          - not_null\n\n  - name: stg_orders\n    columns:\n      - name: order_id\n        tests:\n          - unique\n          - not_null\n      - name: status\n        tests:\n          - accepted_values:\n              values: ['placed', 'shipped', 'completed', 'return_pending', 'returned']\n      - name: customer_id\n        tests:\n          - not_null\n          - relationships:\n              to: ref('stg_customers')\n              field: customer_id\n```\n\n----------------------------------------\n\nTITLE: Configuring External Tables in dbt_project.yml for Firebolt S3 Ingestion\nDESCRIPTION: Complete configuration for defining an external table in Firebolt that sources data from S3. Includes settings for S3 credentials, object patterns, compression, partitioning, and column definitions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/firebolt-configs.md#2025-04-09_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  - name: firebolt_external\n    schema: \"{{ target.schema }}\"\n    loader: S3\n\n    tables:\n      - name: <table-name>\n        external:\n          url: 's3://<bucket_name>/'\n          object_pattern: '<regex>'\n          type: '<type>'\n          credentials:\n            aws_key_id: <key-id>\n            aws_secret_key: <key-secret>\n          object_pattern: '<regex>'\n          compression: '<compression-type>'\n          partitions:\n            - name: <partition-name>\n              data_type: <partition-type>\n              regex: '<partition-definition-regex>'\n          columns:\n            - name: <column-name>\n              data_type: <type>\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Credentials Class for dbt Adapter in Python\nDESCRIPTION: Implements a custom Credentials class for a dbt adapter, including required fields, optional fields, and methods for type identification and connection debugging.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nfrom dbt.adapters.base import Credentials\n\n\n@dataclass\nclass MyAdapterCredentials(Credentials):\n    host: str\n    port: int = 1337\n    username: Optional[str] = None\n    password: Optional[str] = None\n\n    @property\n    def type(self):\n        return 'myadapter'\n\n    @property\n    def unique_field(self):\n        \"\"\"\n        Hashed and included in anonymous telemetry to track adapter adoption.\n        Pick a field that can uniquely identify one team/organization building with this adapter\n        \"\"\"\n        return self.host\n\n    def _connection_keys(self):\n        \"\"\"\n        List of keys to display in the `dbt debug` output.\n        \"\"\"\n        return ('host', 'port', 'database', 'username')\n```\n\n----------------------------------------\n\nTITLE: Setting flags via command line options in dbt\nDESCRIPTION: Examples of setting dbt flags using command line options for specific invocations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/about-global-configs.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --fail-fast # set to True for this specific invocation\ndbt run --no-fail-fast # set to False\n```\n\n----------------------------------------\n\nTITLE: Advanced JSON Query Comment Macro in dbt\nDESCRIPTION: A sophisticated Jinja macro that generates a JSON-formatted comment with extensive metadata about the executing node and context.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/query-comment.md#2025-04-09_snippet_16\n\nLANGUAGE: jinja2\nCODE:\n```\n{% macro query_comment(node) %}\n    {%- set comment_dict = {} -%}\n    {%- do comment_dict.update(\n        app='dbt',\n        dbt_version=dbt_version,\n        profile_name=target.get('profile_name'),\n        target_name=target.get('target_name'),\n    ) -%}\n    {%- if node is not none -%}\n      {%- do comment_dict.update(\n        file=node.original_file_path,\n        node_id=node.unique_id,\n        node_name=node.name,\n        resource_type=node.resource_type,\n        package_name=node.package_name,\n        relation={\n            \"database\": node.database,\n            \"schema\": node.schema,\n            \"identifier\": node.identifier\n        }\n      ) -%}\n    {% else %}\n      {%- do comment_dict.update(node_id='internal') -%}\n    {%- endif -%}\n    {% do return(tojson(comment_dict)) %}\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Data Tests in SQL Config Block\nDESCRIPTION: Example of using the config() macro in a SQL test file to configure test-specific options like fail_calc, limit, severity, and more. This method applies configurations to a specific test.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/data-test-configs.md#2025-04-09_snippet_1\n\nLANGUAGE: jinja\nCODE:\n```\n{{ config(\n    [fail_calc] = \"<string>\",\n    [limit] = <integer>,\n    [severity] = \"error | warn\",\n    [error_if] = \"<string>\",\n    [warn_if] = \"<string>\",\n    [store_failures] = true | false,\n    [where] = \"<string>\"\n) }}\n\n```\n\n----------------------------------------\n\nTITLE: Compiling dbt Analyses\nDESCRIPTION: This command compiles all analysis files in a dbt project, generating runnable SQL that can be found in the target/compiled directory.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/analyses.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndbt compile\n```\n\n----------------------------------------\n\nTITLE: Configuring Complex Indexing with Secondary Index in SQL Model for Teradata\nDESCRIPTION: Setting a primary index, range partitioning, and a secondary index with load identity in a Teradata table for optimized query performance.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/teradata-configs.md#2025-04-09_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\n{{\n  config(\n      materialized=\"table\",\n      index=\"PRIMARY INDEX(id)\n      PARTITION BY RANGE_N(create_date\n                    BETWEEN DATE '2020-01-01'\n                    AND     DATE '2021-01-01'\n                    EACH INTERVAL '1' MONTH)\n      INDEX index_attrA (attrA) WITH LOAD IDENTITY\"\n  )\n}}\n```\n\n----------------------------------------\n\nTITLE: Adding Where Clause to Filter Query Results\nDESCRIPTION: Example demonstrating how to use the where clause to filter query results. Shows filtering for orders that are food orders and from a specific time period using the Dimension template wrapper.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metricflow-commands.md#2025-04-09_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n# In dbt Cloud \ndbt sl query --metrics order_total --group-by order_id__is_food_order --where \"{{ Dimension('order_id__is_food_order') }} = True and {{ TimeDimension('metric_time', 'week') }} >= '2024-02-01'\"\n```\n\n----------------------------------------\n\nTITLE: Configuring an Incremental Model with Schema Change Failure in dbt\nDESCRIPTION: This SQL snippet shows the configuration for an incremental model that fails on schema changes. It demonstrates how to set up an incremental model with a unique key and schema change behavior.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/clone-incremental-models.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{\\n    config(\\n        materialized='incremental',\\n        unique_key='unique_id',\\n        on_schema_change='fail'\\n    )\\n}}\n```\n\n----------------------------------------\n\nTITLE: Custom Seeds Directory Configuration\nDESCRIPTION: Example showing how to use a custom directory name instead of the default 'seeds' directory.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/seed-paths.md#2025-04-09_snippet_3\n\nLANGUAGE: yml\nCODE:\n```\nseed-paths: [\"custom_seeds\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Incremental Materialization in dbt for ClickHouse\nDESCRIPTION: This snippet shows how to configure a dbt model as an incremental ClickHouse table, including options for engine type, order_by, partition_by, unique_key, and inserts_only using either the project YAML file or a config block in the model SQL file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/clickhouse-configs.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  <resource-path>:\n    +materialized: incremental\n    +order_by: [ <column-name>, ... ]\n    +engine: <engine-type>\n    +partition_by: [ <column-name>, ... ]\n    +unique_key: [ <column-name>, ... ]\n    +inserts_only: [ True|False ]\n```\n\nLANGUAGE: jinja\nCODE:\n```\n{{ config(\n    materialized = \"incremental\",\n    engine = \"<engine-type>\",\n    order_by = [ \"<column-name>\", ... ],\n    partition_by = [ \"<column-name>\", ... ],\n    unique_key = [ \"<column-name>\", ... ],\n    inserts_only = [ True|False ],\n      ...\n    ]\n) }}\n```\n\n----------------------------------------\n\nTITLE: Adding Description to Unit Test\nDESCRIPTION: This example shows how to add a description to a unit test that checks the proper truncation of a timestamp to a date in the 'stg_locations' model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/description.md#2025-04-09_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nunit_tests:\n  - name: test_does_location_opened_at_trunc_to_date\n    description: \"Check that opened_at timestamp is properly truncated to a date.\"\n    model: stg_locations\n    given:\n      - input: source('ecom', 'raw_stores')\n        rows:\n          - {id: 1, name: \"Rego Park\", tax_rate: 0.2, opened_at: \"2016-09-01T00:00:00\"}\n          - {id: 2, name: \"Jamaica\", tax_rate: 0.1, opened_at: \"2079-10-27T23:59:59.9999\"}\n    expect:\n      rows:\n        - {location_id: 1, location_name: \"Rego Park\", tax_rate: 0.2, opened_date: \"2016-09-01\"}\n        - {location_id: 2, location_name: \"Jamaica\", tax_rate: 0.1, opened_date: \"2079-10-27\"}\n```\n\n----------------------------------------\n\nTITLE: Creating a Fiscal Calendar Model in SQL\nDESCRIPTION: This SQL snippet creates a fiscal calendar by building upon an existing time spine. It defines custom fiscal years starting in October and fiscal weeks shifted by one week from the standard calendar.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/mf-time-spine.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nwith date_spine as (\n\n        select \n            date_day,\n            extract(year from date_day) as calendar_year,\n            extract(week from date_day) as calendar_week\n\n        from {{ ref('time_spine_daily') }}\n\n    ),\n\n    fiscal_calendar as (\n\n        select\n            date_day,\n            -- Define custom fiscal year starting in October\n            case \n                when extract(month from date_day) >= 10 \n                    then extract(year from date_day) + 1\n                else extract(year from date_day) \n            end as fiscal_year,\n\n            -- Define fiscal weeks (e.g., shift by 1 week)\n            extract(week from date_day) + 1 as fiscal_week\n\n        from date_spine\n\n    )\n\n    select * from fiscal_calendar\n```\n\n----------------------------------------\n\nTITLE: Configuring Timestamp Strategy in dbt_project.yml\nDESCRIPTION: YAML configuration for setting timestamp strategy at the project level in dbt_project.yml. This applies the configuration to all snapshots in the specified resource path.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/strategy.md#2025-04-09_snippet_2\n\nLANGUAGE: yml\nCODE:\n```\nsnapshots:\n  [<resource-path>](/reference/resource-configs/resource-path):\n    +strategy: timestamp\n    +updated_at: column_name\n```\n\n----------------------------------------\n\nTITLE: Creating a Customer Model in SQL for dbt\nDESCRIPTION: This SQL query creates a customer model by joining and aggregating data from customers and orders tables. It calculates first and most recent order dates, and the number of orders for each customer.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/bigquery-qs.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nwith customers as (\n\n    select\n        id as customer_id,\n        first_name,\n        last_name\n\n    from `dbt-tutorial`.jaffle_shop.customers\n\n),\n\norders as (\n\n    select\n        id as order_id,\n        user_id as customer_id,\n        order_date,\n        status\n\n    from `dbt-tutorial`.jaffle_shop.orders\n\n),\n\ncustomer_orders as (\n\n    select\n        customer_id,\n\n        min(order_date) as first_order_date,\n        max(order_date) as most_recent_order_date,\n        count(order_id) as number_of_orders\n\n    from orders\n\n    group by 1\n\n),\n\nfinal as (\n\n    select\n        customers.customer_id,\n        customers.first_name,\n        customers.last_name,\n        customer_orders.first_order_date,\n        customer_orders.most_recent_order_date,\n        coalesce(customer_orders.number_of_orders, 0) as number_of_orders\n\n    from customers\n\n    left join customer_orders using (customer_id)\n\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Tests in YAML\nDESCRIPTION: This snippet shows how to configure dbt tests using the 'config' property in a YAML file. It demonstrates setting configurations for both resource-level and column-level tests.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/config.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\n<resource_type>:\n  - name: <resource_name>\n    tests:\n      - [<test_name>](#test_name):\n          <argument_name>: <argument_value>\n          config:\n            <test_config>: <config-value>\n            ...\n\n    [columns](/reference/resource-properties/columns):\n      - name: <column_name>\n        tests:\n          - [<test_name>](#test_name)\n          - [<test_name>](#test_name):\n              <argument_name>: <argument_value>\n              config:\n                [<test_config>](/reference/data-test-configs): <config-value>\n                ...\n```\n\n----------------------------------------\n\nTITLE: SQL CASE WHEN Example with Order Values\nDESCRIPTION: Practical example showing how to use CASE WHEN to categorize order amounts into buckets (low, medium, high) based on value ranges. Uses the Jaffle Shop orders table as a reference.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/statements/sql-case-statement.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    order_id,\n    round(amount) as amount,\n    case when amount between 0 and 10 then 'low'\n         when amount between 11 and 20 then 'medium'\n         else 'high'\n    end as order_value_bucket\nfrom {{ ref('orders') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Lookback in Properties YAML\nDESCRIPTION: Defines the lookback value of 2 for the user_sessions model in a properties YAML file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/lookback.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: user_sessions\n    config:\n      lookback: 2\n```\n\n----------------------------------------\n\nTITLE: GitHub Workflow File Structure\nDESCRIPTION: Shows the file structure for GitHub Actions workflow implementation\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/custom-cicd-pipelines.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmy_awesome_project\n python\n    run_and_monitor_dbt_job.py\n .github\n    workflows\n       dbt_run_on_merge.yml\n       lint_on_push.yml\n```\n\n----------------------------------------\n\nTITLE: Implementing Log Function in Python for DBT\nDESCRIPTION: Python implementation of the dbt log function that writes messages to either a log file or both log file and stdout. Takes a message string and an optional info boolean parameter to control output destination.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/log.md#2025-04-09_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n    def log(msg: str, info: bool = False) -> str: \n        \"\"\"Logs a line to either the log file or stdout.\n\n        :param msg: The message to log\n        :param info: If `False`, write to the log file. If `True`, write to\n            both the log file and stdout.\n\n        > macros/my_log_macro.sql\n\n            {% macro some_macro(arg1, arg2) %}\n              {{ log(\"Running some_macro: \" ~ arg1 ~ \", \" ~ arg2) }}\n            {% endmacro %}\"\"\"\n        if info:\n            fire_event(JinjaLogInfo(msg=msg, node_info=get_node_info()))\n        else:\n            fire_event(JinjaLogDebug(msg=msg, node_info=get_node_info()))\n        return \"\"\n```\n\n----------------------------------------\n\nTITLE: Checking Specific Columns for Changes in Check Strategy\nDESCRIPTION: An expanded example showing how to configure a snapshot to check specific columns for changes using the check strategy. This approach allows for targeted change detection on selected columns.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snapshots-jinja-legacy.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot orders_snapshot_check %}\n\n    {{\n        config(\n          strategy='check',\n          unique_key='id',\n          check_cols=['status', 'is_cancelled'],\n        )\n    }}\n\n    select * from {{ source('jaffle_shop', 'orders') }}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Creating Snowflake Security Integration for Okta OAuth Authentication\nDESCRIPTION: SQL command for creating a security integration in Snowflake that enables external OAuth authentication with Okta. This integration maps Okta user claims to Snowflake users and enables OAuth token-based authentication.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/manage-access/external-oauth.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\ncreate security integration your_integration_name\ntype = external_oauth\nenabled = true\nexternal_oauth_type = okta\nexternal_oauth_issuer = ''\nexternal_oauth_jws_keys_url = ''\nexternal_oauth_audience_list = ('')\nexternal_oauth_token_user_mapping_claim = 'sub'\nexternal_oauth_snowflake_user_mapping_attribute = 'email_address'\nexternal_oauth_any_role_mode = 'ENABLE'\n```\n\n----------------------------------------\n\nTITLE: License Functionality Matrix\nDESCRIPTION: Table showing the different capabilities and permissions available to different license types in dbt Cloud.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/manage-access/cloud-seats-and-users.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Functionality | Developer User | Read-Only Users | IT Users* |\n| ------------- | -------------- | --------------- | -------- |\n| Use the dbt Cloud IDE |  |  |  |\n| Use the dbt Cloud CLI |  |  |  |\n| Use Jobs |  |  |  |\n| Manage Account |  |  |  |\n| API Access |  |  |  |\n| Use [dbt Explorer] |   |  |   |\n| Use [Source Freshness] |  |  |  |\n| Use [Docs] |  |  |  |\n| Receive [Job notifications] |   |    |   |\n```\n\n----------------------------------------\n\nTITLE: Complete Snapshot Example with invalidate_hard_deletes (SQL)\nDESCRIPTION: Full example of a snapshot configuration using SQL format for dbt version 1.8 and earlier, including all relevant snapshot settings.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/invalidate_hard_deletes.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot orders_snapshot %}\n\n    {{\n        config(\n          target_schema='snapshots',\n          strategy='timestamp',\n          unique_key='id',\n          updated_at='updated_at',\n          invalidate_hard_deletes=True,\n        )\n    }}\n\n    select * from {{ source('jaffle_shop', 'orders') }}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Listing Saved Queries in dbt Semantic Layer\nDESCRIPTION: Commands to list all available saved queries in the dbt Semantic Layer. The basic command shows all queries, while the --show-exports flag displays each export listed under a saved query.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metricflow-commands.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndbt sl list saved-queries\n```\n\nLANGUAGE: bash\nCODE:\n```\ndbt sl list saved-queries --show-exports\n```\n\nLANGUAGE: bash\nCODE:\n```\ndbt sl list saved-queries --show-exports\n\nThe list of available saved queries:\n- new_customer_orders\n  exports:\n       - Export(new_customer_orders_table, exportAs=TABLE)\n       - Export(new_customer_orders_view, exportAs=VIEW)\n       - Export(new_customer_orders, alias=orders, schemas=customer_schema, exportAs=TABLE)\n```\n\n----------------------------------------\n\nTITLE: Local Environment Setup Commands\nDESCRIPTION: Series of bash commands for setting up the local development environment including Python virtual environment configuration, AWS CLI installation, and required package installations for dbt-glue.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/glue-setup.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 --version\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo yum install git\npython3 -m venv dbt_venv\nsource dbt_venv/bin/activate\npython3 -m pip install --upgrade pip\n```\n\nLANGUAGE: bash\nCODE:\n```\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo yum install gcc krb5-devel.x86_64 python3-devel.x86_64 -y\npip3 install upgrade boto3\n```\n\nLANGUAGE: bash\nCODE:\n```\npip3 install dbt-glue\n```\n\n----------------------------------------\n\nTITLE: Accessing Exposures in dbt Graph\nDESCRIPTION: Example showing how to access exposures and generate documentation about model dependencies using the graph context variable.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/graph.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{% set exposures = [] -%}\n{% for exposure in graph.exposures.values() -%}\n  {%- if model['unique_id'] in exposure.depends_on.nodes -%}\n    {%- do exposures.append(exposure) -%}\n  {%- endif -%}\n{%- endfor %}\n\n-- HELLO database administrator! Before dropping this view,\n-- please be aware that doing so will affect:\n\n{% for exposure in exposures %}\n--   * {{ exposure.name }} ({{ exposure.type }})\n{% endfor %}\n```\n\n----------------------------------------\n\nTITLE: Basic Run Query Example\nDESCRIPTION: Simple example of using run_query to execute a query and print results.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/run_query.md#2025-04-09_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n{% set results = run_query('select 1 as id') %}\n{% do results.print_table() %}\n\n-- do something with `results` here...\n```\n\n----------------------------------------\n\nTITLE: Naming a Snapshot 'orders_snapshot' in SQL (dbt 1.8 and earlier)\nDESCRIPTION: This example demonstrates how to name a snapshot 'orders_snapshot' using SQL and Jinja in dbt versions 1.8 and earlier. It shows the basic structure of a named snapshot block.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snapshot_name.md#2025-04-09_snippet_3\n\nLANGUAGE: jinja2\nCODE:\n```\n{% snapshot orders_snapshot %}\n...\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Sources JSON Structure Example\nDESCRIPTION: Illustrates the key elements in a sources.json file including metadata, elapsed time, and results array containing freshness check details for each source. Each result includes unique identifiers, timestamp information, criteria, status and execution timing.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/artifacts/sources-json.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"metadata\": {},\n  \"elapsed_time\": 1.23,\n  \"results\": [\n    {\n      \"unique_id\": \"source.project_name.source_name.table_name\",\n      \"max_loaded_at\": \"2023-01-01T00:00:00Z\",\n      \"snapshotted_at\": \"2023-01-01T12:00:00Z\",\n      \"max_loaded_at_time_ago_in_s\": 43200,\n      \"criteria\": {\n        \"warn_after\": { \"count\": 12, \"period\": \"hour\" },\n        \"error_after\": { \"count\": 24, \"period\": \"hour\" }\n      },\n      \"status\": \"pass\",\n      \"execution_time\": 0.5,\n      \"timing\": [\n        { \"name\": \"compile\", \"started_at\": \"...\", \"completed_at\": \"...\" },\n        { \"name\": \"execute\", \"started_at\": \"...\", \"completed_at\": \"...\" }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: SQL Example of Left Join in MetricFlow\nDESCRIPTION: This SQL query demonstrates how MetricFlow performs a left join between the 'transactions' and 'user_signup' tables. It joins on the user_id and selects relevant columns from both tables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/join-logic.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nselect\n  transactions.user_id,\n  transactions.purchase_price,\n  user_signup.type\nfrom transactions\nleft outer join user_signup\n  on transactions.user_id = user_signup.user_id\nwhere transactions.purchase_price is not null\ngroup by\n  transactions.user_id,\n  user_signup.type;\n```\n\n----------------------------------------\n\nTITLE: BigQuery Job Labels Configuration\nDESCRIPTION: Configuration to include query comment items as job labels in BigQuery executions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/query-comment.md#2025-04-09_snippet_11\n\nLANGUAGE: yml\nCODE:\n```\nquery-comment:\n  job-label: True\n```\n\n----------------------------------------\n\nTITLE: Fetching Dimension Values with Semantic Layer JDBC API\nDESCRIPTION: SQL query to fetch dimension values for one or multiple metrics and a single dimension using the semantic_layer.dimension_values() function.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-jdbc.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nselect * from {{ \nsemantic_layer.dimension_values(metrics=['food_order_amount'], group_by=['customer__customer_name'])}}\n```\n\n----------------------------------------\n\nTITLE: Including Data Tests in DBT Test Command\nDESCRIPTION: Command to include all data tests when running DBT tests using the --resource-type flag. Available in DBT version 1.9 and higher.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/resource-type.md#2025-04-09_snippet_5\n\nLANGUAGE: text\nCODE:\n```\ndbt test --resource-type test\n```\n\n----------------------------------------\n\nTITLE: Configuring SSO Authentication with Okta URL in Snowflake for dbt\nDESCRIPTION: YAML configuration for Snowflake with Okta URL SSO authentication. This setup requires specifying the Okta account URL as the authenticator along with Okta username and password credentials.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/snowflake-setup.md#2025-04-09_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nmy-snowflake-db:\n  target: dev\n  outputs:\n    dev:\n      type: snowflake\n      account: [account id] # Snowflake <account_name>\n      user: [username] # Snowflake username\n      role: [user role] # Snowflake user role\n\n      # SSO config -- The three following fields are REQUIRED\n      authenticator: [Okta account URL]\n      username: [Okta username]\n      password: [Okta password]\n\n      database: [database name] # Snowflake database name\n      warehouse: [warehouse name] # Snowflake warehouse name\n      schema: [dbt schema]\n      threads: [between 1 and 8]\n      client_session_keep_alive: False\n      query_tag: [anything]\n\n      # optional\n      connect_retries: 0 # default 0\n      connect_timeout: 10 # default: 10\n      retry_on_database_errors: False # default: false\n      retry_all: False  # default: false\n      reuse_connections: True # default: True if client_session_keep_alive is False, otherwise None\n```\n\n----------------------------------------\n\nTITLE: SQL CROSS JOIN Example with dbt References\nDESCRIPTION: Illustrates a practical example of a cross join between a 'users' table and a 'date_spine' table using dbt's ref function. This query creates a cartesian product of all users and dates, resulting in a unique combination of user per date per row.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/joins/sql-cross-join.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect\n   users.user_id as user_id,\n   date.date as date\nfrom {{ ref('users') }} as users\ncross join {{ ref('date_spine') }} as date\norder by 1\n```\n\n----------------------------------------\n\nTITLE: Setting Unique Primary Index in SQL Model Configuration for Teradata\nDESCRIPTION: Configuring a unique primary index on the GlobalID column for a table in Teradata to optimize query performance and enforce uniqueness.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/teradata-configs.md#2025-04-09_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\n{{\n  config(\n      materialized=\"table\",\n      index=\"UNIQUE PRIMARY INDEX ( GlobalID )\"\n  )\n}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Resource Property File with Version Specified\nDESCRIPTION: Example of a resource property file with a version tag specified. The version tag controls how dbt processes property files. For dbt versions up to 1.4, this tag is required and must be set to 2.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/version.md#2025-04-09_snippet_1\n\nLANGUAGE: yml\nCODE:\n```\nversion: 2  # Only 2 is accepted by dbt versions up to 1.4.latest.\n\nmodels: \n    ...\n```\n\n----------------------------------------\n\nTITLE: Example of Accepted Values Test Configuration\nDESCRIPTION: YAML configuration showing how to implement accepted_values tests for both string and numeric values.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/data-tests.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: orders\n    columns:\n      - name: status\n        tests:\n          - accepted_values:\n              values: ['placed', 'shipped', 'completed', 'returned']\n\n      - name: status_id\n        tests:\n          - accepted_values:\n              values: [1, 2, 3, 4]\n              quote: false\n```\n\n----------------------------------------\n\nTITLE: SQL Query for Order Total Metric in Traditional Implementation\nDESCRIPTION: A simple SQL query demonstrating how the order_total metric would be calculated without MetricFlow, by summing the order_total column from the orders table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/about-metricflow.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect sum(order_total) as order_total from orders\n```\n\n----------------------------------------\n\nTITLE: Customizing dbt's Built-in Generic Tests in SQL\nDESCRIPTION: Template for overriding dbt's built-in generic tests by creating a test block with the same name in your project. This example shows how to customize the 'unique' test.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/custom-generic-tests.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\n{% test unique(model, column_name) %}\n\n    -- whatever SQL you'd like!\n\n{% endtest %}\n```\n\n----------------------------------------\n\nTITLE: Configuring a Basic Snapshot in YAML\nDESCRIPTION: YAML configuration for a snapshot named 'orders_snapshot' that uses the timestamp strategy. The configuration specifies the source relation, schema, database, unique key, strategy, and timestamp column.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/snapshots.md#2025-04-09_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  - name: orders_snapshot\n    relation: source('jaffle_shop', 'orders')\n    config:\n      schema: snapshots\n      database: analytics\n      unique_key: id\n      strategy: timestamp\n      updated_at: updated_at\n      dbt_valid_to_current: \"to_date('9999-12-31')\" # Specifies that current records should have `dbt_valid_to` set to `'9999-12-31'` instead of `NULL`.\n```\n\n----------------------------------------\n\nTITLE: Basic Check Strategy Snapshot in SQL\nDESCRIPTION: Example of implementing a check strategy snapshot that monitors specific columns for changes in orders data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/snapshots.md#2025-04-09_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot orders_snapshot_check %}\n\n    {{\n        config(\n          target_schema='snapshots',\n          strategy='check',\n          unique_key='id',\n          check_cols=['status', 'is_cancelled'],\n        )\n    }}\n\n    select * from {{ source('jaffle_shop', 'orders') }}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Legacy Snapshot in SQL for dbt\nDESCRIPTION: This snippet demonstrates how to configure a snapshot using the legacy syntax in a .sql file. It includes essential configurations such as target database, schema, unique key, and strategy.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/snapshots.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot orders_snapshot %}\n\n{{\n    config(\n      target_database='analytics',\n      target_schema='snapshots',\n      unique_key='id',\n\n      strategy='timestamp',\n      updated_at='updated_at',\n    )\n}}\n\nselect * from {{ source('jaffle_shop', 'orders') }}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Accessing Compiled SQL for Debugging in dbt\nDESCRIPTION: To debug Jinja in dbt, check the compiled SQL files in the target/compiled/<your_project>/ directory. This allows you to see the final SQL that dbt generates after processing Jinja templates.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Project/debugging-jinja.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntarget/compiled/<your_project>/\n```\n\n----------------------------------------\n\nTITLE: Configuring Starrocks Model in Properties YAML\nDESCRIPTION: This snippet demonstrates how to configure a Starrocks model in the models/properties.yml file. It includes the same configuration options as the project YAML, but in a different format specific to the properties file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/starrocks-configs.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: <model-name>\n    config:\n      materialized: table       // table or view or materialized_view\n      keys: ['id', 'name', 'some_date']\n      table_type: 'PRIMARY'     // PRIMARY or DUPLICATE or UNIQUE\n      distributed_by: ['id']\n      buckets: 3                // default 10\n      partition_by: ['some_date']\n      partition_by_init: [\"PARTITION p1 VALUES [('1971-01-01 00:00:00'), ('1991-01-01 00:00:00')),PARTITION p1972 VALUES [('1991-01-01 00:00:00'), ('1999-01-01 00:00:00'))\"]\n      properties: [{\"replication_num\":\"1\", \"in_memory\": \"true\"}]\n      refresh_method: 'async' // only for materialized view default manual\n```\n\n----------------------------------------\n\nTITLE: Converting INSERT to dbt Model in SQL\nDESCRIPTION: Demonstrates how to convert a simple INSERT-SELECT statement into a dbt model. The example shows the transformation of an INSERT statement for returned orders into a corresponding dbt model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/migrate-from-stored-procedures.md#2025-04-09_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nINSERT INTO returned_orders (order_id, order_date, total_return)\n\nSELECT order_id, order_date, total FROM orders WHERE type = 'return'\n```\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT\n    order_id as order_id,\n    order_date as order_date,\n    total as total_return\n\nFROM {{ ref('orders') }}\n\nWHERE type = 'return'\n```\n\n----------------------------------------\n\nTITLE: Static Schema Generation with Developer Identities\nDESCRIPTION: Macro for generating schema names that uses a single schema for all developers while appending developer identifiers. Handles different environments (DEV, CI) differently.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/customize-schema-alias.md#2025-04-09_snippet_1\n\nLANGUAGE: jinja\nCODE:\n```\n{% macro generate_schema_name(custom_schema_name, node) -%}\n\n    {%- set default_schema = target.schema -%}\n    {%- if custom_schema_name is none -%}\n\n        {{ default_schema }}\n\n    {%- elif  env_var('DBT_ENV_TYPE','DEV') != 'CI' -%}\n        \n        {{ custom_schema_name | trim }}\n\n    {%- else -%}\n\n        {{ default_schema }}_{{ custom_schema_name | trim }}\n\n    {%- endif -%}\n\n{%- endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Enabled Seeds in dbt YAML\nDESCRIPTION: Example of enabling or disabling seeds in the dbt_project.yml file. This configuration applies to all seeds in the specified resource path.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/enabled.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  [<resource-path>]:\n    +enabled: true | false\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Is_Even Generic Test in SQL\nDESCRIPTION: This snippet shows how to create a custom generic test named 'is_even' that checks if values in a specified column are even numbers. The test accepts the standard arguments 'model' and 'column_name'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/custom-generic-tests.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{% test is_even(model, column_name) %}\n\nwith validation as (\n\n    select\n        {{ column_name }} as even_field\n\n    from {{ model }}\n\n),\n\nvalidation_errors as (\n\n    select\n        even_field\n\n    from validation\n    -- if this is true, then even_field is actually odd!\n    where (even_field % 2) = 1\n\n)\n\nselect *\nfrom validation_errors\n\n{% endtest %}\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Metrics in YAML\nDESCRIPTION: This snippet shows how to configure dbt metrics using the 'config' property in a YAML file. It allows setting configurations such as 'enabled', 'group', and 'meta'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/config.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmetrics:\n  - name: <metric_name>\n    config:\n      [enabled](/reference/resource-configs/enabled): true | false\n      [group](/reference/resource-configs/group): <string>\n      [meta](/reference/resource-configs/meta): {dictionary}\n```\n\n----------------------------------------\n\nTITLE: Example: Custom Node Color in schema.yml\nDESCRIPTION: Shows how to override project-level node color configuration for a specific model in a schema.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/docs.md#2025-04-09_snippet_11\n\nLANGUAGE: yml\nCODE:\n```\nmodels:\n  - name: dim_customers\n    description: Customer dimensions table\n    docs:\n      node_color: '#000000'\n```\n\n----------------------------------------\n\nTITLE: Setting Up Local OAuth for BigQuery with gcloud\nDESCRIPTION: Shell command to authenticate locally with Google Cloud for BigQuery access. This sets up application-default credentials with appropriate scopes for BigQuery, Google Drive, and IAM testing.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/bigquery-setup.md#2025-04-09_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\ngcloud auth application-default login \\\n  --scopes=https://www.googleapis.com/auth/bigquery,\\\nhttps://www.googleapis.com/auth/drive.readonly,\\\nhttps://www.googleapis.com/auth/iam.test\n```\n\n----------------------------------------\n\nTITLE: Configuring File Log Color in profiles.yml\nDESCRIPTION: This YAML configuration in profiles.yml disables color output specifically for file logs in dbt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/print-output.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nconfig:\n  use_colors_file: False\n```\n\n----------------------------------------\n\nTITLE: Customer Order Date Analysis using SQL MIN\nDESCRIPTION: Query that retrieves the first and last order dates for each customer from the jaffle_shop orders table. Uses MIN and MAX functions with GROUP BY to analyze customer order history.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/aggregate-functions/sql-min.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\tcustomer_id,\n\tmin(order_date) as first_order_date,\n\tmax(order_date) as last_order_date\nfrom {{ ref('orders') }}\ngroup by 1\nlimit 3\n```\n\n----------------------------------------\n\nTITLE: Installing Oracle Instant Client on Linux (Bash)\nDESCRIPTION: Commands for downloading, unzipping, and configuring Oracle Instant Client libraries on Linux systems. Includes steps for installing dependencies and setting up environment variables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/oracle-setup.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p /opt/oracle\ncd /opt/oracle\nunzip instantclient-basic-linux.x64-21.6.0.0.0.zip\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo yum install libaio\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo sh -c \"echo /opt/oracle/instantclient_21_6 > /etc/ld.so.conf.d/oracle-instantclient.conf\"\nsudo ldconfig\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport LD_LIBRARY_PATH=/opt/oracle/instantclient_21_6:$LD_LIBRARY_PATH\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport TNS_ADMIN=/opt/oracle/your_config_dir\n```\n\n----------------------------------------\n\nTITLE: Compiled SQL Query for dbt Source\nDESCRIPTION: This SQL snippet shows the compiled version of the source query. It demonstrates how dbt translates the source reference into the actual database, schema, and table names.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Project/source-has-bad-name.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nselect * from raw.postgres_backend_public_schema.api_orders\n```\n\n----------------------------------------\n\nTITLE: Example: Invalid Node Color Configuration\nDESCRIPTION: Shows an example of an invalid node color configuration that would cause a compile error during documentation generation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/docs.md#2025-04-09_snippet_13\n\nLANGUAGE: yml\nCODE:\n```\nmodels:\n  tpch:\n    marts:\n      core:\n        materialized: table\n        +docs:\n          node_color: \"aweioohafio23f\"\n```\n\n----------------------------------------\n\nTITLE: Testing dbt Cloud Webhook\nDESCRIPTION: GET request to test a specific webhook configuration and verify its functionality.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/webhooks.md#2025-04-09_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nGET https://{your access URL}/api/v3/accounts/{account_id}/webhooks/subscription/{webhook_id}/test\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"data\": {\n        \"verification_error\": null,\n        \"verification_status_code\": \"200\"\n    },\n    \"status\": {\n        \"code\": 200\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up dbt Test Case Class in Python\nDESCRIPTION: Python code for setting up a dbt test case class using pytest. It imports necessary utilities and fixtures, and defines the structure for a test class.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\nfrom dbt.tests.util import run_dbt\n\n# our file contents\nfrom tests.functional.example.fixtures import (\n    my_seed_csv,\n    my_model_sql,\n    my_model_yml,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment-Based Authentication for Azure SQL in dbt\nDESCRIPTION: This configuration uses environment variables to determine the authentication method for Azure SQL. It relies on environment variables as described in the Microsoft documentation for EnvironmentCredential.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/mssql-setup.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: sqlserver\n      driver: 'ODBC Driver 18 for SQL Server' # (The ODBC Driver installed on your system)\n      server: hostname or IP of your server\n      port: 1433\n      database: exampledb\n      schema: schema_name\n      authentication: environment\n```\n\n----------------------------------------\n\nTITLE: Excluding Unit Tests from DBT Build\nDESCRIPTION: Command to exclude all unit tests from the DBT build process using the --exclude-resource-type flag. Only available in DBT version 1.8 and higher.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/resource-type.md#2025-04-09_snippet_2\n\nLANGUAGE: text\nCODE:\n```\ndbt build --exclude-resource-type unit_test\n```\n\n----------------------------------------\n\nTITLE: Basic SQL ORDER BY Syntax\nDESCRIPTION: Demonstrates the basic syntax for using ORDER BY clause in SQL queries. Shows the fundamental structure for ordering results by specified columns in ascending or descending order.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/clauses/sql-order-by.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\tcolumn_1,\n\tcolumn_2\nfrom source_table\norder by <field(s)> <asc/desc>\n```\n\n----------------------------------------\n\nTITLE: Creating an Analysis SQL File in dbt\nDESCRIPTION: This SQL snippet demonstrates how to create an analysis file in dbt. It uses dbt's ref function to reference other models and performs a running total calculation across journal entries grouped by account.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/analyses.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n-- analyses/running_total_by_account.sql\n\nwith journal_entries as (\n\n  select *\n  from {{ ref('quickbooks_adjusted_journal_entries') }}\n\n), accounts as (\n\n  select *\n  from {{ ref('quickbooks_accounts_transformed') }}\n\n)\n\nselect\n  txn_date,\n  account_id,\n  adjusted_amount,\n  description,\n  account_name,\n  sum(adjusted_amount) over (partition by account_id order by id rows unbounded preceding)\nfrom journal_entries\norder by account_id, id\n```\n\n----------------------------------------\n\nTITLE: Schema Reference for Catalog JSON Structure\nDESCRIPTION: Defines the key structure of the catalog.json artifact file, including metadata, nodes, sources, and error information. Used to store database object metadata for dbt documentation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/artifacts/catalog-json.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"metadata\": {},\n  \"nodes\": {\n    \"<unique_id>\": {\n      \"unique_id\": \"<resource_type>.<package>.<resource_name>\",\n      \"metadata\": {\n        \"type\": \"table|view\",\n        \"database\": \"string\",\n        \"schema\": \"string\",\n        \"name\": \"string\",\n        \"comment\": \"string\",\n        \"owner\": \"string\"\n      },\n      \"columns\": [\n        {\n          \"name\": \"string\",\n          \"type\": \"string\",\n          \"comment\": \"string\",\n          \"index\": \"number\"\n        }\n      ],\n      \"stats\": {}\n    }\n  },\n  \"sources\": {\n    \"<unique_id>\": {}\n  },\n  \"errors\": []\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Statement Results in SQL\nDESCRIPTION: This snippet shows how to access the results of a previously executed statement block using the load_result function. It demonstrates extracting data and response information from the result object.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/statement-blocks.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{%- set states = load_result('states') -%}\n{%- set states_data = states['data'] -%}\n{%- set states_status = states['response'] -%}\n```\n\n----------------------------------------\n\nTITLE: Using the warn-error flag in dbt CLI\nDESCRIPTION: Example of using the --warn-error flag to convert all dbt warnings into errors when executing commands.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/warnings.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ndbt --warn-error run\n```\n\n----------------------------------------\n\nTITLE: BigQuery Incremental Model with Copy Partitions Optimization\nDESCRIPTION: Configures an incremental model to use the copy table API with partition decorators instead of merge statements. This technique can significantly reduce time and cost for large datasets as it doesn't incur data insertion costs.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/bigquery-configs.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized=\"incremental\",\n    incremental_strategy=\"insert_overwrite\",\n    partition_by={\n      \"field\": \"created_date\",\n      \"data_type\": \"timestamp\",\n      \"granularity\": \"day\",\n      \"time_ingestion_partitioning\": true,\n      \"copy_partitions\": true\n    }\n) }}\n\nselect\n  user_id,\n  event_name,\n  created_at,\n  -- values of this column must match the data type + granularity defined above\n  timestamp_trunc(created_at, day) as created_date\n\nfrom {{ ref('events') }}\n```\n\n----------------------------------------\n\nTITLE: Upgrading dbt Adapters and Core\nDESCRIPTION: Commands for upgrading dbt Core and adapter versions, including specific version targeting.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/pip-install.md#2025-04-09_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npython -m pip install --upgrade dbt-ADAPTER_NAME\n```\n\nLANGUAGE: shell\nCODE:\n```\npython -m pip install --upgrade dbt-core\n```\n\nLANGUAGE: shell\nCODE:\n```\npython -m pip install --upgrade dbt-core==1.9\n```\n\n----------------------------------------\n\nTITLE: Configuring DuckDB with Filesystem Access Using fsspec in YAML\nDESCRIPTION: Configuration for dbt-duckdb that enables cloud storage access through fsspec. This example shows how to set up S3 connectivity with authentication credentials defined as environment variables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/duckdb-configs.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndefault:\n  outputs:\n    dev:\n      type: duckdb\n      path: /tmp/dbt.duckdb\n      filesystems:\n        - fs: s3\n          anon: false\n          key: \"{{ env_var('S3_ACCESS_KEY_ID') }}\"\n          secret: \"{{ env_var('S3_SECRET_ACCESS_KEY') }}\"\n          client_kwargs:\n            endpoint_url: \"http://localhost:4566\"\n  target: dev\n```\n\n----------------------------------------\n\nTITLE: Moving dbt_cloud.yml configuration file on Windows\nDESCRIPTION: This command moves the dbt_cloud.yml configuration file from the Downloads folder to the .dbt folder in the user's profile directory. This operation ensures the dbt Cloud CLI can locate the necessary configuration file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/configure-cloud-cli.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nmove %USERPROFILE%\\Downloads\\dbt_cloud.yml %USERPROFILE%\\.dbt\\dbt_cloud.yml\n```\n\n----------------------------------------\n\nTITLE: Running dbt docs generate in dbt Cloud IDE\nDESCRIPTION: This command is used in the dbt Cloud IDE to generate documentation for the dbt project as it exists in the development environment. It's run in the Command Bar of the IDE.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/build-and-view-your-docs.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n`dbt docs generate`\n```\n\n----------------------------------------\n\nTITLE: Basic Model Path Configuration in dbt\nDESCRIPTION: Demonstrates the basic syntax for configuring model paths in dbt_project.yml, using a directorypath datatype.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/model-paths.md#2025-04-09_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nmodel-paths: [directorypath]\n```\n\n----------------------------------------\n\nTITLE: Setting Project-wide and Seed-Specific Delimiters in dbt_project.yml\nDESCRIPTION: This YAML configuration sets a default delimiter for all seeds in the project and overrides it for a specific seed.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/delimiter.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  jaffle_shop: \n    +delimiter: \"|\" # default delimiter for seeds in jaffle_shop project will be \"|\"\n    seed_a:\n      +delimiter: \",\" # delimiter for seed_a will be \",\"\n```\n\n----------------------------------------\n\nTITLE: MetricFlow Extended Semantic Model\nDESCRIPTION: YAML configuration adding additional dimensions like is_food_order to the orders semantic model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/about-metricflow.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_models:\n  - name: orders\n    description: |\n      A model containing order data. The grain of the table is the order id.\n    model: ref('orders')  #The name of the dbt model and schema\n    defaults:\n      agg_time_dimension: metric_time\n    entities: # Entities, which usually correspond to keys in the table\n      - name: order_id\n        type: primary\n      - name: customer\n        type: foreign\n        expr: customer_id\n    measures: # Measures, which are the aggregations on the columns in the table.\n      - name: order_total\n        agg: sum\n    dimensions: # Dimensions are either categorical or time. They add additional context to metrics and the typical querying pattern is Metric by Dimension.\n      - name: metric_time\n        expr: cast(ordered_at as date)\n        type: time\n        type_params:\n          time_granularity: day\n      - name: is_food_order\n        type: categorical\n```\n\n----------------------------------------\n\nTITLE: Basic CTE Structure Example in SQL\nDESCRIPTION: Demonstrates the basic structure of CTEs in a dbt model, including Jinja comments and the recommended final select statement pattern.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-style/2-how-we-style-our-sql.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nwith\n\nevents as (\n\n    ...\n\n),\n\n{# CTE comments go here #}\nfiltered_events as (\n\n    ...\n\n)\n\nselect * from filtered_events\n```\n\n----------------------------------------\n\nTITLE: Incorporating Environment Variables in profiles.yml\nDESCRIPTION: Demonstrates how to use env_var function in profiles.yml to securely include database credentials from environment variables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/env_var.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprofile:\n  target: prod\n  outputs:\n    prod:\n      type: postgres\n      host: 127.0.0.1\n      # IMPORTANT: Make sure to quote the entire Jinja string here\n      user: \"{{ env_var('DBT_USER') }}\"\n      password: \"{{ env_var('DBT_PASSWORD') }}\"\n      ....\n```\n\n----------------------------------------\n\nTITLE: Refactoring Customer Model with Staging Models in SQL\nDESCRIPTION: A SQL query that refactors the customer model to use the staging models (stg_customers and stg_orders) created earlier. It uses dbt's ref function to reference these models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/databricks-qs.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nwith customers as (\n\n    select * from {{ ref('stg_customers') }}\n\n),\n\norders as (\n\n    select * from {{ ref('stg_orders') }}\n\n),\n\ncustomer_orders as (\n\n    select\n        customer_id,\n\n        min(order_date) as first_order_date,\n        max(order_date) as most_recent_order_date,\n        count(order_id) as number_of_orders\n\n    from orders\n\n    group by 1\n\n),\n\nfinal as (\n\n    select\n        customers.customer_id,\n        customers.first_name,\n        customers.last_name,\n        customer_orders.first_order_date,\n        customer_orders.most_recent_order_date,\n        coalesce(customer_orders.number_of_orders, 0) as number_of_orders\n\n    from customers\n\n    left join customer_orders using (customer_id)\n\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Loading Payment Data into Snowflake SQL\nDESCRIPTION: This SQL command loads payment data from an S3 bucket CSV file into the previously created payment table. It specifies the CSV format and skips the header row.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/sl-snowflake-qs.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\ncopy into raw.stripe.payment (id, orderid, paymentmethod, status, amount, created)\nfrom 's3://dbt-tutorial-public/stripe_payments.csv'\nfile_format = (\n    type = 'CSV'\n    field_delimiter = ','\n    skip_header = 1\n    );\n```\n\n----------------------------------------\n\nTITLE: Configuring Schema in dbt Project Configuration\nDESCRIPTION: Project-level configuration in dbt_project.yml that sets the schema for marketing models to 'marketing'\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Models/model-custom-schemas.md#2025-04-09_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nname: jaffle_shop\n...\n\nmodels:\n  jaffle_shop:\n    marketing:\n      schema: marketing # seeds in the `models/mapping/ subdirectory will use the marketing schema\n```\n\n----------------------------------------\n\nTITLE: Batch Processing Python Model in dbt-duckdb\nDESCRIPTION: Demonstrates how to process large datasets in chunks using a Python model in dbt-duckdb. It uses PyArrow for efficient data handling and allows for manipulation of larger-than-memory datasets.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/duckdb-configs.md#2025-04-09_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\n\ndef batcher(batch_reader: pa.RecordBatchReader):\n    for batch in batch_reader:\n        df = batch.to_pandas()\n        # Do some operations on the DF...\n        # ...then yield back a new batch\n        yield pa.RecordBatch.from_pandas(df)\n\ndef model(dbt, session):\n    big_model = dbt.ref(\"big_model\")\n    batch_reader = big_model.record_batch(100_000)\n    batch_iter = batcher(batch_reader)\n    return pa.RecordBatchReader.from_batches(batch_reader.schema, batch_iter)\n```\n\n----------------------------------------\n\nTITLE: Default Selectors in dbt\nDESCRIPTION: Defining a default selector that will be used when no selection criteria are provided in commands.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/yaml-selectors.md#2025-04-09_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nselectors:\n  - name: root_project_only\n    description: >\n        Only resources from the root project.\n        Excludes resources defined in installed packages.\n    default: true\n    definition:\n      method: package\n      value: <my_root_project_name>\n```\n\n----------------------------------------\n\nTITLE: Configuring Redshift Profile with Database Authentication in YAML\nDESCRIPTION: This YAML configuration sets up a Redshift profile using database (password-based) authentication. It includes both required and optional parameters for connecting to a Redshift cluster.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/redshift-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ncompany-name:\n  target: dev\n  outputs:\n    dev:\n      type: redshift\n      host: hostname.region.redshift.amazonaws.com\n      user: username\n      password: password1\n      dbname: analytics\n      schema: analytics\n      port: 5439\n\n      # Optional Redshift configs:\n      sslmode: prefer\n      role: None\n      ra3_node: true \n      autocommit: true \n      threads: 4\n      connect_timeout: None\n```\n\n----------------------------------------\n\nTITLE: Configuring Enabled Snapshots in dbt YAML (v1.9+)\nDESCRIPTION: Example of enabling or disabling a specific snapshot in its YAML file. This method is recommended for dbt version 1.9 and later.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/enabled.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsnapshots:\n  - name: snapshot_name\n    config:\n      enabled: true | false\n```\n\n----------------------------------------\n\nTITLE: Creating a One Big Table (OBT) Using SQL and dbt\nDESCRIPTION: This SQL snippet creates an OBT by joining a fact table with multiple dimension tables. It uses dbt macros to select columns and performs left joins based on surrogate keys. The OBT combines data from sales, product, customer, credit card, address, order status, and date dimensions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-04-18-building-a-kimball-dimensional-model-with-dbt.md#2025-04-09_snippet_19\n\nLANGUAGE: sql\nCODE:\n```\nwith f_sales as (\n    select * from {{ ref('fct_sales') }}\n),\n\nd_customer as (\n    select * from {{ ref('dim_customer') }}\n),\n\nd_credit_card as (\n    select * from {{ ref('dim_credit_card') }}\n),\n\nd_address as (\n    select * from {{ ref('dim_address') }}\n),\n\nd_order_status as (\n    select * from {{ ref('dim_order_status') }}\n),\n\nd_product as (\n    select * from {{ ref('dim_product') }}\n),\n\nd_date as (\n    select * from {{ ref('dim_date') }}\n)\n\nselect\n    {{ dbt_utils.star(from=ref('fct_sales'), relation_alias='f_sales', except=[\n        \"product_key\", \"customer_key\", \"creditcard_key\", \"ship_address_key\", \"order_status_key\", \"order_date_key\"\n    ]) }},\n    {{ dbt_utils.star(from=ref('dim_product'), relation_alias='d_product', except=[\"product_key\"]) }},\n    {{ dbt_utils.star(from=ref('dim_customer'), relation_alias='d_customer', except=[\"customer_key\"]) }},\n    {{ dbt_utils.star(from=ref('dim_credit_card'), relation_alias='d_credit_card', except=[\"creditcard_key\"]) }},\n    {{ dbt_utils.star(from=ref('dim_address'), relation_alias='d_address', except=[\"address_key\"]) }},\n    {{ dbt_utils.star(from=ref('dim_order_status'), relation_alias='d_order_status', except=[\"order_status_key\"]) }},\n    {{ dbt_utils.star(from=ref('dim_date'), relation_alias='d_date', except=[\"date_key\"]) }}\nfrom f_sales\nleft join d_product on f_sales.product_key = d_product.product_key\nleft join d_customer on f_sales.customer_key = d_customer.customer_key\nleft join d_credit_card on f_sales.creditcard_key = d_credit_card.creditcard_key\nleft join d_address on f_sales.ship_address_key = d_address.address_key\nleft join d_order_status on f_sales.order_status_key = d_order_status.order_status_key\nleft join d_date on f_sales.order_date_key = d_date.date_key\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Saved Queries in YAML\nDESCRIPTION: This snippet shows how to configure dbt saved queries using the 'config' property in a YAML file. It allows setting various configurations including cache settings, export options, and resource grouping.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/config.md#2025-04-09_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsaved-queries:\n  - name: <saved-query-name>\n    config:\n      [cache](/docs/build/saved-queries#parameters): \n        enabled: true | false\n      [enabled](/reference/resource-configs/enabled): true | false\n      [export_as](/docs/build/saved-queries#parameters): view | table \n      [group](/reference/resource-configs/group): <string>\n      [meta](/reference/resource-configs/meta): {dictionary}\n      [schema](/reference/resource-configs/schema): <string>\n```\n\n----------------------------------------\n\nTITLE: Defining Resource-Specific Configurations in dbt_project.yml\nDESCRIPTION: This snippet shows how to define resource-specific configurations in the dbt_project.yml file. It's one of the methods to set configurations that apply to only one dbt resource type.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_config-description-resource.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndbt_project.yml\n```\n\n----------------------------------------\n\nTITLE: Configuring Meta in dbt_project.yml for Models\nDESCRIPTION: Demonstrates how to configure meta properties for models in the project configuration file using resource paths.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/meta.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  [<resource-path>]:\n    +meta: {<dictionary>}\n```\n\n----------------------------------------\n\nTITLE: Permission Set Reference - Project Level\nDESCRIPTION: Reference to project-level permission identifier used in dbt Cloud configuration\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/manage-access/enterprise-permissions.md#2025-04-09_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nproject-level\n```\n\n----------------------------------------\n\nTITLE: YAML Frontmatter Configuration for dbt Explorer Documentation\nDESCRIPTION: YAML configuration block defining metadata for the dbt Explorer workshop documentation page, including title, description, tags, and other page attributes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/explorer-qs.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: \"Quickstart for the dbt Explorer workshop\"\nid: \"explorer-quickstart\"\ndescription: \"Use this guide to build and define metrics, set up the dbt Cloud Semantic Layer, and query them using Google Sheets.\"\nsidebar_label: \"Quickstart dbt Explorer\"\nicon: 'guides'\nhide_table_of_contents: true\ntags: ['Explorer', 'Snowflake', 'dbt Cloud','Quickstart']\nkeywords: ['dbt Explorer','Mesh','dbt Cloud', 'Snowflake', 'Multi-Project']\nlevel: 'Beginner'\nrecently_updated: true\n---\n```\n\n----------------------------------------\n\nTITLE: Defining Cumulative Metrics in dbt YAML (version 1.8 and earlier)\nDESCRIPTION: This YAML snippet defines three cumulative metrics for dbt versions 1.8 and earlier: all-time cumulative order total, trailing 1-month cumulative order total, and month-to-date cumulative order total. It uses 'type_params' to specify measure, window, and grain_to_date parameters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/cumulative-metrics.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  - name: cumulative_order_total\n    label: Cumulative order total (All-Time)    \n    description: The cumulative value of all orders\n    type: cumulative\n    type_params:\n      measure: \n        name: order_total\n  \n  - name: cumulative_order_total_l1m\n    label: Cumulative order total (L1M)   \n    description: Trailing 1-month cumulative order total\n    type: cumulative\n    type_params:\n      measure: \n        name: order_total\n      window: 1 month\n  \n  - name: cumulative_order_total_mtd\n    label: Cumulative order total (MTD)\n    description: The month-to-date value of all orders\n    type: cumulative\n    type_params:\n      measure: \n        name: order_total\n      grain_to_date: month\n```\n\n----------------------------------------\n\nTITLE: Setting Transient Property for a Specific Model in Snowflake\nDESCRIPTION: Model-level configuration to explicitly set a table as transient in Snowflake. This overrides project defaults and controls whether Snowflake preserves history for the table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snowflake-configs.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(materialized='table', transient=true) }}\n\nselect * from ...\n```\n\n----------------------------------------\n\nTITLE: Querying Customer Orders in Redshift\nDESCRIPTION: This SQL query performs customer order analysis in Amazon Redshift by joining customer and order tables. It calculates key metrics like first and most recent order dates and total order count. The query uses schema.table notation following Redshift's naming conventions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/tutorial-sql-query.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nwith customers as (\n\n    select\n        id as customer_id,\n        first_name,\n        last_name\n\n    from jaffle_shop.customers\n\n),\n\norders as (\n\n    select\n        id as order_id,\n        user_id as customer_id,\n        order_date,\n        status\n\n    from jaffle_shop.orders\n\n),\n\ncustomer_orders as (\n\n    select\n        customer_id,\n\n        min(order_date) as first_order_date,\n        max(order_date) as most_recent_order_date,\n        count(order_id) as number_of_orders\n\n    from orders\n\n    group by 1\n\n),\n\nfinal as (\n\n    select\n        customers.customer_id,\n        customers.first_name,\n        customers.last_name,\n        customer_orders.first_order_date,\n        customer_orders.most_recent_order_date,\n        coalesce(customer_orders.number_of_orders, 0) as number_of_orders\n\n    from customers\n\n    left join customer_orders using (customer_id)\n\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Meta Configuration in SQL Models\nDESCRIPTION: Shows how to configure meta properties directly in SQL model files using config blocks.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/meta.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(meta = {\n    'single_key': 'override'\n}) }}\n\nselect 1 as id\n```\n\n----------------------------------------\n\nTITLE: Fetching Data Platform Dialect Query\nDESCRIPTION: GraphQL query to retrieve the dialect or data platform used for the dbt Semantic Layer connection.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-graphql.md#2025-04-09_snippet_1\n\nLANGUAGE: graphql\nCODE:\n```\n{\n  environmentInfo(environmentId: BigInt!) {\n    dialect\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Customers Model Version Example in YAML\nDESCRIPTION: Practical example showing version configuration for a customers model with different column specifications across versions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/versions.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: customers\n    columns:\n      - name: customer_id\n        description: Unique identifier for this table\n        data_type: text\n        constraints:\n          - type: not_null\n        tests:\n          - unique\n      - name: customer_country\n        data_type: text\n        description: \"Country where the customer currently lives\"\n      - name: first_purchase_date\n        data_type: date\n    \n    versions:\n      - v: 4\n      \n      - v: 3\n        columns:\n          - include: \"*\"\n          - name: customer_country\n            data_type: text\n            description: \"Country where the customer first lived at time of first purchase\"\n      \n      - v: 2\n        columns:\n          - include: \"*\"\n            exclude:\n              - customer_country\n      \n      - v: 1\n        columns:\n          - include: []\n          - name: id\n            data_type: int\n```\n\n----------------------------------------\n\nTITLE: Querying Common Dimensions for Multiple Metrics\nDESCRIPTION: Query to fetch dimensions shared between multiple metrics using the dimensions() function.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-jdbc.md#2025-04-09_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nselect * from {{ \n\tsemantic_layer.dimensions(metrics=['food_order_amount', 'order_gross_profit'])\n\t}}\n```\n\n----------------------------------------\n\nTITLE: Setting Deprecation Date in DBT Model (Version 2 Schema)\nDESCRIPTION: Example of setting a future deprecation date for a dbt model using the version 2 schema. This shows how to indicate a model that will be deprecated in the future.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/deprecation_date.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\nmodels:\n  - name: my_model\n    description: deprecating in the future\n    deprecation_date: 2999-01-01 00:00:00.00+00:00\n```\n\n----------------------------------------\n\nTITLE: Column Quoting Example in DBT\nDESCRIPTION: Example showing how to configure column quoting for a Stripe payment source table, particularly useful for Snowflake compatibility.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/columns.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: stripe\n    tables:\n      - name: payment\n        columns:\n          - name: orderID\n            quote: true\n            tests:\n              - not_null\n```\n\n----------------------------------------\n\nTITLE: Creating OAuth Security Integration in Snowflake\nDESCRIPTION: SQL template for creating an external OAuth security integration in Snowflake. This configuration sets up the connection between Snowflake and the identity provider (Okta in this example) for OAuth authentication.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/manage-access/external-oauth.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\ncreate security integration your_integration_name\ntype = external_oauth\nenabled = true\nexternal_oauth_type = okta\nexternal_oauth_issuer = ''\nexternal_oauth_jws_keys_url = ''\nexternal_oauth_audience_list = ('')\nexternal_oauth_token_user_mapping_claim = 'sub'\nexternal_oauth_snowflake_user_mapping_attribute = 'email_address'\nexternal_oauth_any_role_mode = 'ENABLE'\n```\n\n----------------------------------------\n\nTITLE: Creating Payment Table in Snowflake\nDESCRIPTION: SQL command to create the payment table in the stripe schema with columns for id, orderid, paymentmethod, status, amount, created date, and a timestamp field for batch tracking. This prepares the table structure for loading sample payment data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/snowflake-qs.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\ncreate table raw.stripe.payment \n( id integer,\n  orderid integer,\n  paymentmethod varchar,\n  status varchar,\n  amount integer,\n  created date,\n  _batched_at timestamp default current_timestamp\n);\n```\n\n----------------------------------------\n\nTITLE: Example of Relationships Test Configuration\nDESCRIPTION: YAML configuration demonstrating how to set up referential integrity tests between tables using the relationships test.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/data-tests.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: orders\n    columns:\n      - name: customer_id\n        tests:\n          - relationships:\n              to: ref('customers')\n              field: id\n```\n\n----------------------------------------\n\nTITLE: Configuring Sources in dbt_project.yml (v1.9+)\nDESCRIPTION: YAML configuration for sources in dbt_project.yml for dbt version 1.9 and later. This snippet shows how to configure enabled status, event_time field, and metadata for sources using the resource path syntax.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/source-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  [<resource-path>]:\n    [+][enabled]: true | false\n    [+][event_time]: my_time_field\n    [+][meta]:\n      key: value\n\n```\n\n----------------------------------------\n\nTITLE: Timestamp Strategy Snapshot Example\nDESCRIPTION: Example showing how snapshot meta-fields are populated using the timestamp strategy, including the handling of updated records and hard deletes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/snapshots.md#2025-04-09_snippet_18\n\nLANGUAGE: sql\nCODE:\n```\n| id | status  | updated_at       | dbt_valid_from   | dbt_valid_to     | dbt_updated_at   | dbt_is_deleted |\n|----|---------|------------------|------------------|------------------|------------------|----------------|\n| 1  | pending | 2024-01-01 10:47 | 2024-01-01 10:47 | 2024-01-01 11:05 | 2024-01-01 10:47 | False          |\n| 1  | shipped | 2024-01-01 11:05 | 2024-01-01 11:05 | 2024-01-01 11:20 | 2024-01-01 11:05 | False          |\n| 1  | deleted | 2024-01-01 11:20 | 2024-01-01 11:20 |                  | 2024-01-01 11:20 | True           |\n```\n\n----------------------------------------\n\nTITLE: Authenticating with fly.io - Login\nDESCRIPTION: Command to log in to an existing fly.io account via CLI\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/serverless-datadog.md#2025-04-09_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nflyctl auth login\n```\n\n----------------------------------------\n\nTITLE: Running dbt Core with Airflow BashOperator\nDESCRIPTION: This snippet shows how to use the BashOperator in Airflow to run dbt Core commands. It assumes dbt is installed in a virtual environment to avoid dependency conflicts.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/deployment-tools.md#2025-04-09_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.utils.dates import days_ago\n\nwith DAG(\n    dag_id=\"example_dbt_core\",\n    default_args={\"owner\": \"astronomer\"},\n    start_date=days_ago(1),\n    schedule_interval=None,\n) as dag:\n    dbt_run = BashOperator(\n        task_id=\"dbt_run\",\n        bash_command=\"source /path/to/dbt/venv/bin/activate && dbt run\",\n    )\n\n    dbt_run\n```\n\n----------------------------------------\n\nTITLE: Configuring asset-paths for Image Files in Documentation\nDESCRIPTION: Example of configuring the asset-paths to include an 'assets' directory for documentation. This allows images in the assets directory to be accessed in project documentation descriptions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/asset-paths.md#2025-04-09_snippet_3\n\nLANGUAGE: yml\nCODE:\n```\nasset-paths: [\"assets\"]\n```\n\n----------------------------------------\n\nTITLE: Querying Model Historical Runs in GraphQL for dbt Cloud Discovery API\nDESCRIPTION: This GraphQL query retrieves the most recent run results for a specific model, including details such as run ID, elapsed time, execution time, and status. It's useful for analyzing the performance of individual models over time.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/discovery-use-cases-and-examples.md#2025-04-09_snippet_1\n\nLANGUAGE: graphql\nCODE:\n```\nquery ModelHistoricalRuns(\n  $environmentId: BigInt!\n  $uniqueId: String\n  $lastRunCount: Int\n) {\n  environment(id: $environmentId) {\n    applied {\n      modelHistoricalRuns(\n        uniqueId: $uniqueId\n        lastRunCount: $lastRunCount\n      ) {\n        name\n        runId\n        runElapsedTime\n        runGeneratedAt\n        executionTime\n        executeStartedAt\n        executeCompletedAt\n        status\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Order and Customer Data in SQL with dbt Jinja\nDESCRIPTION: This SQL query uses dbt Jinja functions to reference models and calculates yearly metrics for unique customers, cities, and total order revenue.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/access-query-page.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nwith \n\norders as (\n    select * from {{ ref('orders') }}\n),\n\ncustomers as (\n    select * from {{ ref('customers') }}\n)\n\nselect \n    date_trunc('year', ordered_at) as order_year,\n    count(distinct orders.customer_id) as unique_customers,\n    count(distinct orders.location_id) as unique_cities,\n    to_char(sum(orders.order_total), '999,999,999.00') as total_order_revenue\nfrom orders\njoin customers\n    on orders.customer_id = customers.customer_id\ngroup by 1\norder by 1\n```\n\n----------------------------------------\n\nTITLE: Adding Network Policy to Snowflake Security Integration\nDESCRIPTION: SQL command to alter a security integration in Snowflake to use a specific network policy. This may be needed when troubleshooting connection issues to ensure dbt Cloud IP addresses are properly allow-listed.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/manage-access/set-up-snowflake-oauth.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nALTER SECURITY INTEGRATION <security_integration_name>\nSET NETWORK_POLICY = <network_policy_name> ;\n```\n\n----------------------------------------\n\nTITLE: Complete ERP Component Data After Updates\nDESCRIPTION: This SQL-like table structure shows the complete state of the ERP component data after all updates, including the initial assembly, tube replacement, and wheel replacement. It demonstrates how the time-varying ragged hierarchy is represented in the transactional model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-02-modeling-ragged-time-varying-hierarchies.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n| assembly_id | component_id | installed_at | removed_at |\n| -             | -              | -              | -            |\n|               | Bike-1         | 2023-01-01     |              |\n| Bike-1        | Frame-1        | 2023-01-01     |              |\n| Bike-1        | Wheel-1        | 2023-01-01     | 2023-08-01   |\n| Wheel-1       | Rim-1          | 2023-01-01     | 2023-08-01   |\n| Wheel-1       | Tire-1         | 2023-01-01     | 2023-08-01   |\n| Tire-1        | Tube-1         | 2023-01-01     | 2023-06-01   |\n| Tire-1        | Tube-2         | 2023-06-01     | 2023-08-01   |\n| Bike-1        | Wheel-2        | 2023-08-01     |              |\n| Wheel-2       | Rim-2          | 2023-08-01     |              |\n| Wheel-2       | Tire-2         | 2023-08-01     |              |\n| Tire-2        | Tube-3         | 2023-08-01     |              |\n```\n\n----------------------------------------\n\nTITLE: Configuring Lake Formation Tags in dbt_project.yml\nDESCRIPTION: YAML configuration example showing how to set up Lake Formation tags at the project level in dbt_project.yml. This allows applying consistent Lake Formation tagging across multiple models within the dbt project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/athena-configs.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n  +lf_tags_config:\n    enabled: true\n    tags:\n      tag1: value1\n      tag2: value2\n    tags_columns:\n      tag1:\n        value1: [ column1, column2 ]\n    inherited_tags: [ tag1, tag2 ]\n```\n\n----------------------------------------\n\nTITLE: Replace Where Strategy Implementation in Databricks SQL\nDESCRIPTION: Shows implementation of replace_where strategy which selectively overwrites data matching specified predicates. Requires delta file format and Databricks Runtime 12.0+.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/databricks-configs.md#2025-04-09_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized='incremental',\n    file_format='delta',\n    incremental_strategy = 'replace_where'\n    incremental_predicates = 'user_id >= 10000' # Never replace users with ids < 10000\n) }}\n\nwith new_events as (\n\n    select * from {{ ref('events') }}\n\n    {% if is_incremental() %}\n    where date_day >= date_add(current_date, -1)\n    {% endif %}\n\n)\n\nselect\n    user_id,\n    max(date_day) as last_seen\n\nfrom events\ngroup by 1\n```\n\n----------------------------------------\n\nTITLE: Configuring Staging Model Materialization in YAML\nDESCRIPTION: dbt project configuration that sets all staging models to be materialized as views, optimizing for freshness and warehouse space efficiency.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-structure/2-staging.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# dbt_project.yml\n\nmodels:\n  jaffle_shop:\n    staging:\n      +materialized: view\n```\n\n----------------------------------------\n\nTITLE: Configuring External Tables Package in packages.yml for Firebolt\nDESCRIPTION: Configuration for adding the dbt-external-tables package dependency, which enables managing S3 to Firebolt data ingestion directly through dbt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/firebolt-configs.md#2025-04-09_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\npackages:\n  - package: dbt-labs/dbt_external_tables\n    version: <version>\n```\n\n----------------------------------------\n\nTITLE: Using the Built-in Alternative Schema Generation Pattern\nDESCRIPTION: Implementation that enables dbt's alternative schema generation pattern. This pattern uses custom schemas directly in production but falls back to target schemas in development environments.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/custom-schemas.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n-- put this in macros/get_custom_schema.sql\n\n{% macro generate_schema_name(custom_schema_name, node) -%}\n    {{ generate_schema_name_for_env(custom_schema_name, node) }}\n{%- endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Ingestion-Time Partitioning in BigQuery with dbt\nDESCRIPTION: Shows how to set up ingestion-time partitioning in BigQuery using dbt. This approach uses a specified column as input for the _PARTITIONTIME pseudocolumn.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/bigquery-configs.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized=\"incremental\",\n    partition_by={\n      \"field\": \"created_date\",\n      \"data_type\": \"timestamp\",\n      \"granularity\": \"day\",\n      \"time_ingestion_partitioning\": true\n    }\n) }}\n\nselect\n  user_id,\n  event_name,\n  created_at,\n  -- values of this column must match the data type + granularity defined above\n  timestamp_trunc(created_at, day) as created_date\n\nfrom {{ ref('events') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Enabled Tests in dbt SQL\nDESCRIPTION: Example of enabling or disabling a specific test in its SQL file using the config block. This configuration applies only to the current test.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/enabled.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\n{% test <testname>() %}\n\n{{ config(\n  enabled=true | false\n) }}\n\nselect ...\n\n{% endtest %}\n```\n\n----------------------------------------\n\nTITLE: Assigning Extended Attributes to Environment via API in dbt Cloud\nDESCRIPTION: This API request assigns previously created extended attributes to a specific environment using the extended attributes ID obtained from the creation response.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/connect-data-platform/connnect-bigquery.md#2025-04-09_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ncurl --request POST \\\n--url https://cloud.getdbt.com/api/v3/accounts/XXXXX/projects/YYYYY/environments/EEEEE/ \\\n--header 'Accept: application/json' \\\n--header 'Authorization: Bearer ZZZZZZ' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"extended_attributes_id\": FFFFF\n}'\n```\n\n----------------------------------------\n\nTITLE: Specifying Multiple Source Loaders in dbt YAML Configuration\nDESCRIPTION: This example demonstrates how to specify different loaders for multiple sources in a dbt YAML file. It shows two sources, 'jaffle_shop' loaded by Fivetran and 'stripe' loaded by Stitch, along with their respective tables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/loader.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: jaffle_shop\n    loader: fivetran\n    tables:\n      - name: orders\n      - name: customers\n\n  - name: stripe\n    loader: stitch\n    tables:\n      - name: payments\n```\n\n----------------------------------------\n\nTITLE: Semantic Model Meta Configuration\nDESCRIPTION: Example of configuring meta properties for semantic models including data ownership and reporting usage flags.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/meta.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_models:\n  - name: transaction \n    model: ref('fact_transactions')\n    description: \"Transaction fact table at the transaction level. This table contains one row per transaction and includes the transaction timestamp.\"\n    defaults:\n      agg_time_dimension: transaction_date\n    config:\n      meta:\n        data_owner: \"Finance team\"\n        used_in_reporting: true\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Current Timestamp Macro for Trino in dbt\nDESCRIPTION: This SQL macro defines a custom current_timestamp function for Trino with microsecond precision (6 digits) to be used in dbt snapshots.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/trino-configs.md#2025-04-09_snippet_17\n\nLANGUAGE: sql\nCODE:\n```\n{% macro trino__current_timestamp() %}\n    current_timestamp(6)\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Verifying Data Load\nDESCRIPTION: SQL SELECT statements to verify successful data loading into the tables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/redshift-qs.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nselect * from jaffle_shop.customers;\nselect * from jaffle_shop.orders;\nselect * from stripe.payment;\n```\n\n----------------------------------------\n\nTITLE: Configuring Segmentation by String in Vertica dbt Model\nDESCRIPTION: This snippet demonstrates how to use the 'segmented_by_string' config parameter to segment a Vertica table by a specific column. It creates a table segmented by the 'product_key' column.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/vertica-configs.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\n{{ config( materialized='table', segmented_by_string='product_key'  )  }}  \n\nselect * from public.product_dimension\n```\n\n----------------------------------------\n\nTITLE: Using re Module for Regular Expressions\nDESCRIPTION: Demonstrates regular expression pattern matching using the re module to validate an S3 path string.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/modules.md#2025-04-09_snippet_2\n\nLANGUAGE: jinja\nCODE:\n```\n{% set my_string = 's3://example/path' %}\n{% set s3_path_pattern = 's3://[a-z0-9-_/]+' %}\n\n{% set re = modules.re %}\n{% set is_match = re.match(s3_path_pattern, my_string, re.IGNORECASE) %}\n{% if not is_match %}\n    {%- do exceptions.raise_compiler_error(\n        my_string ~ ' is not a valid s3 path'\n    ) -%}\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Running State Modified Tests in Shell\nDESCRIPTION: Shell commands demonstrating how to run dbt tests on modified resources while excluding relationship tests.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/state-comparison-caveats.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndbt run -s \"state:modified\"\ndbt test -s \"state:modified\" --exclude \"test_name:relationships\"\n```\n\n----------------------------------------\n\nTITLE: Job-based Query Before Deprecation - GraphQL\nDESCRIPTION: Example of a job-based query using Int data type for IDs before the September 2023 deprecation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/2023-release-notes.md#2025-04-09_snippet_1\n\nLANGUAGE: graphql\nCODE:\n```\nquery ($jobId: Int!) {\n    models(jobId: $jobId){\n        uniqueId\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Source Loader in dbt YAML Configuration\nDESCRIPTION: This snippet shows the basic structure for defining sources in a dbt YAML file, including the 'loader' property. The loader property is used to specify the tool that loads the source data into the warehouse, though it's only for documentation purposes and not used by dbt itself.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/loader.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: <source_name>\n    database: <database_name>\n    loader: <string>\n    tables:\n      - ...\n```\n\n----------------------------------------\n\nTITLE: Executing Single Seed Build in dbt CLI\nDESCRIPTION: Command to build a specific seed file named 'country_codes' using dbt's select flag. The select option allows targeting individual seeds rather than running all seeds at once.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Seeds/build-one-seed.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ dbt seed --select country_codes\n```\n\n----------------------------------------\n\nTITLE: Fetching Available Metrics Query\nDESCRIPTION: GraphQL query structure for retrieving available metrics from the environment.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-graphql.md#2025-04-09_snippet_2\n\nLANGUAGE: graphql\nCODE:\n```\nmetrics(environmentId: BigInt!): [Metric!]!\n```\n\n----------------------------------------\n\nTITLE: Running Models Downstream of a Seed with dbt CLI\nDESCRIPTION: This command runs all models that depend on a seed named 'country_codes'. The plus sign in the selection syntax indicates that all downstream models should be included in the run.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Runs/run-downstream-of-seed.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ dbt run --select country_codes+\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of run_started_at Variable in SQL\nDESCRIPTION: This snippet demonstrates how to use the run_started_at variable to get the date portion of the run start timestamp in YYYY-MM-DD format using Python's strftime method.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/run_started_at.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\t'{{ run_started_at.strftime(\"%Y-%m-%d\") }}' as date_day\n\nfrom ...\n```\n\n----------------------------------------\n\nTITLE: Validating Webhooks and Refreshing Mode Reports with Python in Zapier\nDESCRIPTION: Python code that validates incoming dbt Cloud webhooks using HMAC signatures and triggers a Mode dashboard refresh via the Mode API when a dbt Cloud job completes successfully. The code securely retrieves credentials from Zapier's StoreClient and includes request parameters for the Mode API.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/zapier-refresh-mode-report.md#2025-04-09_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport hashlib\nimport hmac\nimport json\n\n#replace with the report token you want to run\naccount_username = 'YOUR_MODE_ACCOUNT_USERNAME_HERE'\nreport_token = 'YOUR_REPORT_TOKEN_HERE'\n\nauth_header = input_data['auth_header']\nraw_body = input_data['raw_body']\n\n# Access secret credentials\nsecret_store = StoreClient('YOUR_SECRET_HERE')\nhook_secret = secret_store.get('DBT_WEBHOOK_KEY')\nusername = secret_store.get('MODE_API_TOKEN')\npassword = secret_store.get('MODE_API_SECRET')\n\n# Validate the webhook came from dbt Cloud\nsignature = hmac.new(hook_secret.encode('utf-8'), raw_body.encode('utf-8'), hashlib.sha256).hexdigest()\n\nif signature != auth_header:\n  raise Exception(\"Calculated signature doesn't match contents of the Authorization header. This webhook may not have been sent from dbt Cloud.\")\n\nfull_body = json.loads(raw_body)\nhook_data = full_body['data'] \n\nif hook_data['runStatus'] == \"Success\":\n\n  # Create a report run with the Mode API\n  url = f'https://app.mode.com/api/{account_username}/reports/{report_token}/run'\n\n  params = {\n    'parameters': {\n      \"user_id\": 123, \n      \"location\": \"San Francisco\"\n    } \n  }\n  headers = {\n    'Content-Type': 'application/json',\n    'Accept': 'application/hal+json'\n  }\n  response = requests.post(\n    url, \n    json=params, \n    headers=headers, \n    auth=HTTPBasicAuth(username, password)\n  )\n  response.raise_for_status()\n\nreturn\n```\n\n----------------------------------------\n\nTITLE: Configuring Service Account File Authentication\nDESCRIPTION: Configuration for connecting to BigQuery using a service account keyfile authentication method.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/bigquery-setup.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmy-bigquery-db:\n  target: dev\n  outputs:\n    dev:\n      type: bigquery\n      method: service-account\n      project: GCP_PROJECT_ID\n      dataset: DBT_DATASET_NAME\n      threads: 4\n      keyfile: /PATH/TO/BIGQUERY/keyfile.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Column Properties in DBT YAML\nDESCRIPTION: YAML configuration for defining column properties in DBT models, including data type, description, quote settings, tests, tags, and meta information.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/columns.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: <model_name>\n    columns:\n      - name: <column_name>\n        data_type: <string>\n        description: <markdown_string>\n        quote: true | false\n        tests: ...\n        tags: ...\n        meta: ...\n      - name: <another_column>\n        ...\n```\n\n----------------------------------------\n\nTITLE: Databricks DDL for Table Creation\nDESCRIPTION: This SQL shows the DDL that Databricks generates to create a table. It creates a Delta table and populates it with sample data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/constraints.md#2025-04-09_snippet_18\n\nLANGUAGE: sql\nCODE:\n```\n  create or replace table schema_name.my_model \n  using delta \n  as\n    select\n      1 as id,\n      'My Favorite Customer' as customer_name,\n      cast('2019-01-01' as date) as first_transaction_date\n```\n\n----------------------------------------\n\nTITLE: Creating a Predictor Model in MindsDB with dbt\nDESCRIPTION: This SQL snippet demonstrates how to create a predictor model in MindsDB using dbt. The model is configured with a 'predictor' materialization, specifying the integration name, field to predict, and optional configuration parameters for the trained model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/mindsdb-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n-- my_first_model.sql    \n    {{\n        config(\n            materialized='predictor',\n            integration='photorep',\n            predict='name',\n            predict_alias='name',\n            using={\n                'encoders.location.module': 'CategoricalAutoEncoder',\n                'encoders.rental_price.module': 'NumericEncoder'\n            }\n        )\n    }}\n      select * from stores\n```\n\n----------------------------------------\n\nTITLE: Handling Unpinned Git Package Warning\nDESCRIPTION: Example showing the warning message displayed for unpinned Git packages and how to suppress it using the warn-unpinned configuration flag in packages.yml.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/packages.md#2025-04-09_snippet_13\n\nLANGUAGE: text\nCODE:\n```\nThe git package \"https://github.com/dbt-labs/dbt-utils.git\" is not pinned.\nThis can introduce breaking changes into your project without warning!\n```\n\nLANGUAGE: yaml\nCODE:\n```\npackages:\n  - git: https://github.com/dbt-labs/dbt-utils.git\n    warn-unpinned: false\n```\n\n----------------------------------------\n\nTITLE: Configuring Generic Test Storage in YAML\nDESCRIPTION: Example of configuring storage options for generic tests (not_null and unique) in a model definition file, demonstrating different storage types for different tests.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/store_failures_as.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: my_model\n    columns:\n      - name: id\n        tests:\n          - not_null:\n              config:\n                store_failures_as: view\n          - unique:\n              config:\n                store_failures_as: ephemeral\n```\n\n----------------------------------------\n\nTITLE: Adding dbt Cloud Network Rules to Snowflake Network Policies\nDESCRIPTION: SQL command to modify an existing Snowflake network policy to include the previously created dbt Cloud access rule. This allows dbt Cloud connections through the specified network rule.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/secure/snowflake-privatelink.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nALTER NETWORK POLICY <network_policy_name>\n  ADD ALLOWED_NETWORK_RULE_LIST =('allow_dbt_cloud_access');\n```\n\n----------------------------------------\n\nTITLE: Implementing LEFT JOIN with dbt References\nDESCRIPTION: This example shows a practical implementation of a LEFT JOIN using dbt's ref function. It joins two tables (car_type and car_color) based on user_id, demonstrating how to select specific fields from each table and alias the results.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/joins/sql-left-join.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect\n   car_type.user_id as user_id,\n   car_type.car_type as type,\n   car_color.car_color as color\nfrom {{ ref('car_type') }} as car_type\nleft join {{ ref('car_color') }} as car_color\non car_type.user_id = car_color.user_id\n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Copy Job in dbt for Upsolver\nDESCRIPTION: This snippet shows how to set up a SQL copy job in dbt for Upsolver. It uses the 'incremental' materialization and allows specifying various options such as sync, source, and partition_by.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/upsolver-configs.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(  materialized='incremental',\n            sync=True|False,\n            source = 'S3'| 'KAFKA' | ... ,\n            options={\n              'option_name': 'option_value'\n            },\n            partition_by=[{}]\n          )\n}}\nSELECT * FROM {{ ref(<model>) }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Analysis Paths with Default Directory\nDESCRIPTION: Illustrates the configuration using the default 'analyses' directory. This is the value populated by the 'dbt init' command and is a common convention in dbt projects.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/analysis-paths.md#2025-04-09_snippet_3\n\nLANGUAGE: yml\nCODE:\n```\nanalysis-paths: [\"analyses\"]\n```\n\n----------------------------------------\n\nTITLE: Querying with Metric Alias Example\nDESCRIPTION: Practical example of using a metric alias in a query, showing how to rename order_total to total_revenue_global in the result set while grouping by metric_time.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-jdbc.md#2025-04-09_snippet_31\n\nLANGUAGE: sql\nCODE:\n```\nselect * from {{\n    semantic_layer.query(metrics=[Metric(\"order_total\", alias=\"total_revenue_global\")], group_by=['metric_time'])\n}}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Package Macro with Namespace in SQL\nDESCRIPTION: Demonstrates how to create a dispatched macro within a package by specifying both the macro name and namespace. Package maintainers must always provide the macro_namespace argument.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/dispatch.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{% macro concat(fields) -%}\n  {{ return(adapter.dispatch('concat', 'dbt_utils')(fields)) }}\n{%- endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Configuring SCD Type II Dimension Validity Parameters in dbt Semantic Layer\nDESCRIPTION: YAML configuration for setting up validity parameters for SCD Type II dimensions in dbt Semantic Layer, showing how to define tier_start and tier_end dimensions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/dimensions.md#2025-04-09_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\n- name: tier_start #  The name of the dimension.\n  type: time # The type of dimension (such as time)\n  label: \"Start date of tier\" # A readable label for the dimension\n  expr: start_date # Expression or column name the dimension represents\n  type_params: # Additional parameters for the dimension type\n    time_granularity: day # Specifies the granularity of the time dimension (such as day)\n    validity_params: # Defines the validity window\n      is_start: True # Indicates the start of the validity period. \n- name: tier_end \n  type: time\n  label: \"End date of tier\"\n  expr: end_date\n  type_params:\n    time_granularity: day\n    validity_params:\n      is_end: True # Indicates the end of the validity period.\n```\n\n----------------------------------------\n\nTITLE: Basic Semantic Model Configuration in YAML\nDESCRIPTION: Example structure for defining semantic models in YAML files, including entities, measures, and dimensions. This is part of the initial configuration process for the dbt Semantic Layer.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/use-dbt-semantic-layer/sl-faqs.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# Example semantic model structure\nversion: 2\nmodels:\n  - name: semantic_model\n    entities:\n      - name: entity_name\n        type: primary\n    measures:\n      - name: measure_name\n        agg: sum\n    dimensions:\n      - name: dimension_name\n        type: categorical\n```\n\n----------------------------------------\n\nTITLE: Creating Databases in Snowflake\nDESCRIPTION: SQL commands to create raw and analytics databases in Snowflake using the sysadmin role.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/database-permissions/snowflake-permissions.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nuse role sysadmin;\ncreate database raw;\ncreate database analytics;\n```\n\n----------------------------------------\n\nTITLE: Configuring Complex Indexing with Secondary Index in Seed Configuration for Teradata\nDESCRIPTION: Setting a primary index, range partitioning, and a secondary index with load identity in a Teradata seed table for optimized query performance.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/teradata-configs.md#2025-04-09_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  <project-name>:\n    index: \"PRIMARY INDEX(id)\n      PARTITION BY RANGE_N(create_date\n                    BETWEEN DATE '2020-01-01'\n                    AND     DATE '2021-01-01'\n                    EACH INTERVAL '1' MONTH)\n      INDEX index_attrA (attrA) WITH LOAD IDENTITY\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Doris Connection Profile in YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure the dbt profile for connecting to Doris. It includes settings for host, port, schema, username, and password.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/doris-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndbt-doris:\n  target: dev\n  outputs:\n    dev:\n      type: doris\n      host: 127.0.0.1\n      port: 9030\n      schema: database_name\n      username: username\n      password: password\n```\n\n----------------------------------------\n\nTITLE: Using date_trunc Macro in DBT SQL\nDESCRIPTION: Examples of using the date_trunc macro to truncate timestamps to different date parts. Takes datepart and date expression as arguments.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/cross-database-macros.md#2025-04-09_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt.date_trunc(\"day\", \"updated_at\") }}\n{{ dbt.date_trunc(\"month\", \"updated_at\") }}\n{{ dbt.date_trunc(\"year\", \"'2016-03-09'\") }}\n```\n\nLANGUAGE: sql\nCODE:\n```\ndate_trunc('day', updated_at)\ndate_trunc('month', updated_at)\ndate_trunc('year', '2016-03-09')\n```\n\n----------------------------------------\n\nTITLE: Configuring Singular Test Storage in SQL\nDESCRIPTION: Example of a singular test configuration that stores test failures as a table. The test is a simple custom check that always fails by selecting 1 where 1=0.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/store_failures_as.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(store_failures_as=\"table\") }}\n\n-- custom singular test\nselect 1 as id\nwhere 1=0\n```\n\n----------------------------------------\n\nTITLE: Installing dbt Cloud CLI with Homebrew\nDESCRIPTION: Commands to add the dbt-labs/dbt-cli tap and install the dbt Cloud CLI package using Homebrew on macOS.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/cloud-cli-installation.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbrew tap dbt-labs/dbt-cli\nbrew install dbt\n```\n\n----------------------------------------\n\nTITLE: Configuring Nested Source Tables\nDESCRIPTION: Shows how to disable a source table that is nested in a YAML file within a subfolder structure.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/resource-path.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  your_project_name:\n    subdirectory_name:\n      source_name:\n        source_table_name:\n          +enabled: false\n```\n\n----------------------------------------\n\nTITLE: Example Usage of dbt DATEADD Macro\nDESCRIPTION: Demonstrates an example of using the dbt DATEADD macro to add one month to a specific date. This standardized syntax compiles to the appropriate syntax for the chosen data warehouse.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/date-functions/sql-dateadd.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n{{ dateadd(datepart=\"month\", interval=1, from_date_or_timestamp=\"'2021-08-12'\") }}\n```\n\n----------------------------------------\n\nTITLE: Jinja SQL Template for Source Freshness Check\nDESCRIPTION: The Jinja SQL template used by dbt to generate the source freshness check query.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/freshness.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nselect\n  max({{ loaded_at_field }}) as max_loaded_at,\n  {{ current_timestamp() }} as snapshotted_at\nfrom {{ source }}\n{% if filter %}\nwhere {{ filter }}\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Setting flags in dbt_project.yml\nDESCRIPTION: Example of setting a flag in the dbt_project.yml file to define project-wide defaults.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/about-global-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# dbt_project.yml\nflags:\n  # set default for running this project -- anywhere, anytime, by anyone\n  fail_fast: true\n```\n\n----------------------------------------\n\nTITLE: Azure CLI Authentication Profile\nDESCRIPTION: YAML configuration for dbt profiles.yml using Azure CLI authentication method.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/fabric-setup.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: fabric\n      driver: 'ODBC Driver 18 for SQL Server'\n      server: hostname or IP of your server\n      port: 1433\n      database: exampledb\n      schema: schema_name\n      authentication: CLI\n```\n\n----------------------------------------\n\nTITLE: Displaying Merge Conflict Markers in Git\nDESCRIPTION: This code snippet shows the standard Git conflict markers that appear in files with merge conflicts. It includes the current changes (HEAD), the conflicting changes, and the branch identifier.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/git/merge-conflicts.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<<<<<< HEAD\n    your current code\n======\n    conflicting code\n>>>>>> (some branch identifier)\n```\n\n----------------------------------------\n\nTITLE: Querying Multiple Tables with Similar Names in SQL\nDESCRIPTION: This SQL example demonstrates the confusion users might face when encountering poorly named models with similar names in their data warehouse. It shows how analysts might try to determine which table is correct by testing different queries.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-05-17-stakeholder-friendly-model-names.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n-- select * from analytics.order  limit 10\n-- select * from analytics.orders  limit 10\nselect * from analytics.orders_new  limit 10\n```\n\n----------------------------------------\n\nTITLE: Example of PEM Certificate Format for SAML Integration\nDESCRIPTION: An example of the X.509 Certificate in PEM format required for the SAML integration between OneLogin and dbt Cloud. The certificate is used to establish trust between the identity provider (OneLogin) and the service provider (dbt Cloud).\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/manage-access/set-up-sso-saml-2.0.md#2025-04-09_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n-----BEGIN CERTIFICATE-----\nMIIC8DCCAdigAwIBAgIQSANTIKwxA1221kqhkiG9w0dbtLabsBAQsFADA0MTIwMAYDVQQD\nEylNaWNyb3NvZnQgQXp1cmUgRmVkZXJhdGVkIFNTTyBDZXJ0aWZpY2F0ZTAeFw0yMzEyMjIwMDU1\nMDNaFw0yNjEyMjIwMDU1MDNaMDQxMjAwBgNVBAMTKU1pY3Jvc29mdCBBenVyZSBGZWRlcmF0ZWQg\nU1NPIENlcnRpZmljYXRlMIIBIjANBgkqhkiG9w0BAEFAAFRANKIEMIIBCgKCAQEAqfXQGc/D8ofK\naXbPXftPotqYLEQtvqMymgvhFuUm+bQ9YSpS1zwNQ9D9hWVmcqis6gO/VFw61e0lFnsOuyx+XMKL\nrJjAIsuWORavFqzKFnAz7hsPrDw5lkNZaO4T7tKs+E8N/Qm4kUp5omZv/UjRxN0XaD+o5iJJKPSZ\nPBUDo22m+306DE6ZE8wqxT4jTq4g0uXEitD2ZyKaD6WoPRETZELSl5oiCB47Pgn/mpqae9o0Q2aQ\nLP9zosNZ07IjKkIfyFKMP7xHwzrl5a60y0rSIYS/edqwEhkpzaz0f8QW5pws668CpZ1AVgfP9TtD\nY1EuxBSDQoY5TLR8++2eH4te0QIDAQABMA0GCSqGSIb3DmAKINgAA4IBAQCEts9ujwaokRGfdtgH\n76kGrRHiFVWTyWdcpl1dNDvGhUtCRsTC76qwvCcPnDEFBebVimE0ik4oSwwQJALExriSvxtcNW1b\nqvnY52duXeZ1CSfwHkHkQLyWBANv8ZCkgtcSWnoHELLOWORLD4aSrAAY2s5hP3ukWdV9zQscUw2b\nGwN0/bTxxQgA2NLZzFuHSnkuRX5dbtrun21USPTHMGmFFYBqZqwePZXTcyxp64f3Mtj3g327r/qZ\nsquyPSq5BrF4ivguYoTcGg4SCP7qfiNRFyBUTTERFLYU0n46MuPmVC7vXTsPRQtNRTpJj/b2gGLk\n1RcPb1JosS1ct5Mtjs41\n-----END CERTIFICATE-----\n```\n\n----------------------------------------\n\nTITLE: Configuring Snapshot Schema\nDESCRIPTION: Configuration examples for setting custom schema for snapshots using both dbt_project.yml and snapshot YAML files.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/schema.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  your_project:\n    your_snapshot:\n      +schema: snapshots\n```\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsnapshots:\n  - name: snapshot_name\n    config:\n      schema: snapshots\n```\n\n----------------------------------------\n\nTITLE: Aliasing a Seed in seeds/properties.yml\nDESCRIPTION: Specifies an alias for the 'product_categories' seed in the seeds/properties.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/alias.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nseeds:\n  - name: product_categories\n    config:\n      alias: categories_data\n```\n\n----------------------------------------\n\nTITLE: Adding Description to Generic Data Test\nDESCRIPTION: This example illustrates how to add a description to a generic data test that checks for unique values in a column of the 'orders' model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/description.md#2025-04-09_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: orders\n    columns:\n      - name: order_id\n        tests:\n          - unique:\n              description: \"The order_id is unique for every row in the orders model\"\n```\n\n----------------------------------------\n\nTITLE: Fetching Queryable Granularities for Metrics with Semantic Layer JDBC API\nDESCRIPTION: SQL query to fetch queryable time granularities for specified metrics using the semantic_layer.queryable_granularities() function. This returns granularities that make sense for the primary time dimension of the metrics.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-jdbc.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nselect * from {{\n    semantic_layer.queryable_granularities(metrics=['food_order_amount', 'order_gross_profit'])}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Model with YAML\nDESCRIPTION: YAML configuration for a Python model showing how to set materialization type and access target, var, and env_var values.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/python-models.md#2025-04-09_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: my_python_model\n    config:\n      materialized: table\n      target_name: \"{{ target.name }}\"\n      specific_var: \"{{ var('SPECIFIC_VAR') }}\"\n      specific_env_var: \"{{ env_var('SPECIFIC_ENV_VAR') }}\"\n```\n\n----------------------------------------\n\nTITLE: Basic LOWER Function Syntax in SQL\nDESCRIPTION: Demonstrates the basic syntax for using the LOWER function to convert string values to lowercase.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-06-30-lower-sql-function.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nlower('<string_value>')\n```\n\n----------------------------------------\n\nTITLE: Installing dbt-infer Package with pip\nDESCRIPTION: Command to install the dbt-infer adapter package via pip package manager\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/infer-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npip install dbt-infer\n```\n\n----------------------------------------\n\nTITLE: Using a custom schema name for a dbt source\nDESCRIPTION: This example shows how to use a different name for the schema in your dbt configuration than the actual schema name in your database. It's useful for simplifying complex schema names.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/schema.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: jaffle_shop\n    schema: postgres_backend_public_schema\n    tables:\n      - name: orders\n```\n\n----------------------------------------\n\nTITLE: Specifying Minimum dbt Version Requirement\nDESCRIPTION: Example of setting a minimum dbt version requirement using the >= operator. This project will run with any version of dbt greater than or equal to 1.0.0.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/require-dbt-version.md#2025-04-09_snippet_2\n\nLANGUAGE: yml\nCODE:\n```\nrequire-dbt-version: \">=1.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Extracting Month and Calculating Average Order Amount using SQL DATE_PART\nDESCRIPTION: This SQL query demonstrates how to use the DATE_PART function to extract the month from an order date and calculate the average order amount for each month. It uses the Jaffle Shop's 'orders' table as a reference.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/date-functions/sql-datepart.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\tdate_part('month', order_date) as order_month,\n\tround(avg(amount)) as avg_order_amount\nfrom {{ ref('orders') }}\ngroup by 1\n```\n\n----------------------------------------\n\nTITLE: Building Surrogate Key from Columns in dbt\nDESCRIPTION: A dbt macro that creates a surrogate key from specified columns in a relation, excluding certain columns. It uses dbt_utils functions to filter columns and generate a surrogate key.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-04-19-complex-deduplication.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{%- macro build_key_from_columns(dbt_relation, exclude=[]) -%}\n\n{% set cols = dbt_utils.get_filtered_columns_in_relation(dbt_relation, exclude)  %}\n\n{{ return(dbt_utils.surrogate_key(cols)) }}\n\n{%- endmacro -%}\n```\n\n----------------------------------------\n\nTITLE: Disabling concurrent batches in dbt_project.yml\nDESCRIPTION: Sets concurrent_batches to false for specific models in the dbt_project.yml file to ensure sequential batch processing, particularly for cumulative metrics models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/concurrent_batches.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  my_project:\n    cumulative_metrics_model:\n      +concurrent_batches: false\n```\n\n----------------------------------------\n\nTITLE: Extracting dbt Cloud CLI on Linux\nDESCRIPTION: Commands to extract the downloaded dbt Cloud CLI archive and verify the installation on Linux.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/cloud-cli-installation.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ntar -xf dbt_0.29.9_linux_amd64.tar.gz\n./dbt --version\n```\n\n----------------------------------------\n\nTITLE: Configuring Snapshots in YAML Properties File\nDESCRIPTION: This snippet demonstrates how to configure snapshots using a YAML properties file. It includes options for enabled, tags, alias, hooks, and other configuration parameters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/snapshot-configs.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsnapshots:\n  - name: [<snapshot-name>]\n    relation: source('my_source', 'my_table')\n    config:\n      [enabled]: true | false\n      [tags]: <string> | [<string>]\n      [alias]: <string>\n      [pre_hook]: <sql-statement> | [<sql-statement>]\n      [post_hook]: <sql-statement> | [<sql-statement>]\n      [persist_docs]: {<dict>}\n      [grants]: {<dictionary>}\n      [event_time]: my_time_field\n```\n\n----------------------------------------\n\nTITLE: BigQuery OAuth Configuration Table\nDESCRIPTION: Configuration settings table for setting up OAuth client ID in BigQuery, including application type, name, and redirect URI specifications.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/manage-access/set-up-bigquery-oauth.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Config                       | Value           |\n| ---------------------------- | --------------- |\n| **Application type**         | Web application |\n| **Name**                     | dbt Cloud       |\n| **Authorized redirect URIs** | `REDIRECT_URI`  |\n```\n\n----------------------------------------\n\nTITLE: Configuring Kerberos Authentication for Trino in dbt profiles.yml\nDESCRIPTION: This snippet demonstrates how to set up Kerberos authentication for a Trino connection in the dbt profiles.yml file. It includes necessary parameters such as keytab, krb5_config, and principal.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/trino-setup.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ntrino:\n  target: dev\n  outputs:\n    dev:\n      type: trino\n      method: kerberos\n      user: commander\n      keytab: /tmp/trino.keytab\n      krb5_config: /tmp/krb5.conf\n      principal: trino@EXAMPLE.COM\n      host: trino.example.com\n      port: 443\n      database: analytics\n      schema: public\n```\n\n----------------------------------------\n\nTITLE: Using Shorthand Syntax for Node Selection\nDESCRIPTION: Examples of using dbt's shorthand language for defining subsets of nodes, including operators like +, @, *, and ,.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/syntax.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# multiple arguments can be provided to --select\ndbt run --select \"my_first_model my_second_model\"\n\n# select my_model and all of its children\ndbt run --select \"my_model+\"     \n\n# select my_model, its children, and the parents of its children\ndbt run --models @my_model          \n\n# these arguments can be projects, models, directory paths, tags, or sources\ndbt run --select \"tag:nightly my_model finance.base.*\"\n\n# use methods and intersections for more complex selectors\ndbt run --select \"path:marts/finance,tag:nightly,config.materialized:table\"\n```\n\n----------------------------------------\n\nTITLE: Setting Global Target Schema for All Snapshots\nDESCRIPTION: Example showing how to set a common target schema named 'snapshots' for all snapshot resources in the project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/target_schema.md#2025-04-09_snippet_2\n\nLANGUAGE: yml\nCODE:\n```\nsnapshots:\n  +target_schema: snapshots\n```\n\n----------------------------------------\n\nTITLE: Creating Fact Table for Orders in SQL\nDESCRIPTION: SQL query to create a fact table for orders, joining and aggregating data from staging models for orders and payments.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/sl-snowflake-qs.md#2025-04-09_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\nwith orders as  (\n   select * from {{ ref('stg_orders' )}}\n),\n\n\npayments as (\n   select * from {{ ref('stg_payments') }}\n),\n\n\norder_payments as (\n   select\n       order_id,\n       sum(case when status = 'success' then amount end) as amount\n\n\n   from payments\n   group by 1\n),\n\n\nfinal as (\n\n\n   select\n       orders.order_id,\n       orders.customer_id,\n       orders.order_date,\n       coalesce(order_payments.amount, 0) as amount\n\n\n   from orders\n   left join order_payments using (order_id)\n)\n\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Creating Snowflake OAuth Security Integration\nDESCRIPTION: SQL query to create a security integration in Snowflake for OAuth authentication with dbt Cloud. This integration enables SSO via Snowflake and includes configuration for refresh tokens and secondary roles.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/manage-access/set-up-snowflake-oauth.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE OR REPLACE SECURITY INTEGRATION DBT_CLOUD\n  TYPE = OAUTH\n  ENABLED = TRUE\n  OAUTH_CLIENT = CUSTOM\n  OAUTH_CLIENT_TYPE = 'CONFIDENTIAL'\n  OAUTH_REDIRECT_URI = '<REDIRECT_URI>'\n  OAUTH_ISSUE_REFRESH_TOKENS = TRUE\n  OAUTH_REFRESH_TOKEN_VALIDITY = 7776000;\n  OAUTH_USE_SECONDARY_ROLES = 'IMPLICIT';  -- Required for secondary roles\n```\n\n----------------------------------------\n\nTITLE: Configuring Snowflake Python Model with External Package\nDESCRIPTION: Example of a dbt Python model configuration that uses external packages imported from a Snowflake stage. The model demonstrates data transformation using pandas DataFrame with an external package integration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/python-models.md#2025-04-09_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef model(dbt, session):\n    # Configure the model\n    dbt.config(\n        materialized=\"table\",\n        imports=[\"@mystage/mycustompackage.zip\"],  # Specify the external package location\n    )\n    \n    # Example data transformation using the imported package\n    # (Assuming `some_external_package` has a function we can call)\n    data = {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"score\": [85, 90, 88]\n    }\n    df = pd.DataFrame(data)\n\n    # Process data with the external package\n    df[\"adjusted_score\"] = df[\"score\"].apply(lambda x: some_external_package.adjust_score(x))\n    \n    # Return the DataFrame as the model output\n    return df\n```\n\n----------------------------------------\n\nTITLE: Defining require-dbt-version in dbt_project.yml\nDESCRIPTION: Basic configuration syntax for specifying dbt version requirements in a project file. This configuration restricts the project to run only with compatible dbt versions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/require-dbt-version.md#2025-04-09_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nrequire-dbt-version: version-range | [version-range]\n```\n\n----------------------------------------\n\nTITLE: Setting Default Values for Environment Variables in dbt_project.yml\nDESCRIPTION: Shows how to use env_var function with a default value in dbt_project.yml to avoid compilation errors when the environment variable isn't available.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/env_var.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n...\nmodels:\n  jaffle_shop:\n    +materialized: \"{{ env_var('DBT_MATERIALIZATION', 'view') }}\"\n```\n\n----------------------------------------\n\nTITLE: Setting Query Tag for a Specific Model in Snowflake\nDESCRIPTION: Model-level configuration to set a custom query tag for a specific Snowflake model. This overrides any project-level query tag settings.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snowflake-configs.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    query_tag = 'dbt_special'\n) }}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Resolving Duplicate Tests with Custom Names in dbt YAML\nDESCRIPTION: This YAML configuration demonstrates how to use custom names to differentiate between similar tests with different configurations, avoiding compilation errors.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/data-tests.md#2025-04-09_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: orders\n    columns:\n      - name: status\n        tests:\n          - accepted_values:\n              name: unexpected_order_status_today\n              values: ['placed', 'shipped', 'completed', 'returned']\n              config:\n                where: \"order_date = current_date\"\n          - accepted_values:\n              name: unexpected_order_status_yesterday\n              values: ['placed', 'shipped', 'completed', 'returned']\n              config:\n                where: \"order_date = (current_date - interval '1 day')\" # PostgreSQL\n```\n\n----------------------------------------\n\nTITLE: Generated SQL for Ratio Metrics Using Different Semantic Models\nDESCRIPTION: An example of the SQL generated when computing a ratio metric where numerator and denominator come from different semantic models. The system creates subqueries for each metric and joins them on common dimensions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/ratio-metrics.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nselect\n  subq_15577.metric_time as metric_time,\n  cast(subq_15577.mql_queries_created_test as double) / cast(nullif(subq_15582.distinct_query_users, 0) as double) as mql_queries_per_active_user\nfrom (\n  select\n    metric_time,\n    sum(mql_queries_created_test) as mql_queries_created_test\n  from (\n    select\n      cast(query_created_at as date) as metric_time,\n      case when query_status in ('PENDING','MODE') then 1 else 0 end as mql_queries_created_test\n    from prod_dbt.mql_query_base mql_queries_test_src_2552 \n  ) subq_15576\n  group by\n    metric_time\n) subq_15577\ninner join (\n  select\n    metric_time,\n    count(distinct distinct_query_users) as distinct_query_users\n  from (\n    select\n      cast(query_created_at as date) as metric_time,\n      case when query_status in ('MODE','PENDING') then email else null end as distinct_query_users\n    from prod_dbt.mql_query_base mql_queries_src_2585 \n  ) subq_15581\n  group by\n    metric_time\n) subq_15582\non\n  (\n    (\n      subq_15577.metric_time = subq_15582.metric_time\n    ) or (\n      (\n        subq_15577.metric_time is null\n      ) and (\n        subq_15582.metric_time is null\n      )\n    )\n  )\n```\n\n----------------------------------------\n\nTITLE: Enabling Models Conditionally with as_bool Filter in dbt YAML Configuration\nDESCRIPTION: This snippet demonstrates how to use the as_bool Jinja filter in a dbt_project.yml file to conditionally enable or disable a set of models based on the target name. The filter coerces the result of a Jinja expression into a boolean value.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/as_bool.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  my_project:\n    for_export:\n      enabled: \"{{ (target.name == 'prod') | as_bool }}\"\n```\n\n----------------------------------------\n\nTITLE: Example of Check Strategy in YAML (dbt v1.9+)\nDESCRIPTION: Complete YAML example of a snapshot using the check strategy for an orders table, specifying which specific columns to check for changes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/strategy.md#2025-04-09_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  - name: orders_snapshot_check\n    relation: source('jaffle_shop', 'orders')\n    config:\n      schema: snapshots\n      unique_key: id\n      strategy: check\n      check_cols:\n        - status\n        - is_cancelled\n```\n\n----------------------------------------\n\nTITLE: Setting Model Access Using New Method in Properties YAML (v1.7+)\nDESCRIPTION: Shows the newer method (for dbt v1.7 or higher) of configuring access for a model in a properties.yml file. This approach uses the config property to set the model as publicly accessible.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/access.md#2025-04-09_snippet_2\n\nLANGUAGE: yml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: my_public_model\n    config:\n      access: public # newly supported in v1.7\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Semantic Models in YAML\nDESCRIPTION: This snippet demonstrates how to configure dbt semantic models using the 'config' property in a YAML file. It allows setting configurations such as 'enabled', 'group', and 'meta'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/config.md#2025-04-09_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsemantic_models:\n  - name: <semantic_model_name>\n    config:\n      [enabled](/reference/resource-configs/enabled): true | false\n      [group](/reference/resource-configs/group): <string>\n      [meta](/reference/resource-configs/meta): {dictionary}\n```\n\n----------------------------------------\n\nTITLE: Running dbt Commands with Default Selectors\nDESCRIPTION: Examples of unqualified dbt commands that will use the default selector and commands with explicit selection criteria that override the default.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/yaml-selectors.md#2025-04-09_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ndbt build\ndbt source freshness\ndbt docs generate\n```\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --select  \"model_a\"\ndbt run --exclude model_a\n```\n\n----------------------------------------\n\nTITLE: Basic EXTRACT Function Syntax in SQL\nDESCRIPTION: Demonstrates the basic syntax of the EXTRACT function in SQL, showing how to extract a specified date part from a date/time field.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-06-30-extract-sql-function.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextract(<date_part> from <date/time field>)\n```\n\n----------------------------------------\n\nTITLE: Versioned Model Schema with Diffs\nDESCRIPTION: YAML schema definition showing version differences using include/exclude pattern for column changes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/govern/model-versions.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: dim_customers\n    latest_version: 1\n    config:\n      materialized: table\n      contract: {enforced: true}\n    columns:\n      - name: customer_id\n        description: This is the primary key\n        data_type: int\n      - name: country_name\n        description: Where this customer lives\n        data_type: varchar\n    \n    versions:\n    \n      - v: 1\n        \n      - v: 2\n        columns:\n          - include: all\n            exclude: [country_name]\n```\n\n----------------------------------------\n\nTITLE: Configuring Test Results Schema\nDESCRIPTION: Configuration to store test results in a custom schema with failure storage enabled.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/schema.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ntests:\n  +store_failures: true\n  +schema: test_results\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Model Path in dbt_project.yml\nDESCRIPTION: This snippet shows how to modify the model-paths configuration in dbt_project.yml to use a custom directory ('transformations') instead of the default 'models' directory for storing dbt model files.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Models/configurable-model-path.md#2025-04-09_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nmodel-paths: [\"transformations\"]\n```\n\n----------------------------------------\n\nTITLE: Recommended Test Path Configuration\nDESCRIPTION: Shows the recommended way to specify test paths using relative paths in the project configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/test-paths.md#2025-04-09_snippet_1\n\nLANGUAGE: yml\nCODE:\n```\ntest-paths: [\"test\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring OAuth Token-Based Authentication with Refresh Token\nDESCRIPTION: Configuration for connecting to BigQuery using OAuth token-based authentication with refresh token capabilities.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/bigquery-setup.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmy-bigquery-db:\n  target: dev\n  outputs:\n    dev:\n      type: bigquery\n      method: oauth-secrets\n      project: GCP_PROJECT_ID\n      dataset: DBT_DATASET_NAME\n      threads: 4\n      refresh_token: TOKEN\n      client_id: CLIENT_ID\n      client_secret: CLIENT_SECRET\n      token_uri: REDIRECT_URI\n```\n\n----------------------------------------\n\nTITLE: All Dimensions Query\nDESCRIPTION: Query using query_with_all_group_bys to fetch metrics with all valid dimensions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-jdbc.md#2025-04-09_snippet_18\n\nLANGUAGE: sql\nCODE:\n```\nselect * from {{\n    semantic_layer.query_with_all_group_bys(metrics =['revenue','orders','food_orders'],\n    compile= True)\n}}\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Incremental Strategy Macros in dbt\nDESCRIPTION: This snippet demonstrates how to define custom incremental strategy macros in dbt. It includes an example 'insert_only' strategy implementation with two macros: one that serves as the entry point for the strategy and another that contains the actual SQL implementation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/incremental-strategy.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\n{% macro get_incremental_insert_only_sql(arg_dict) %}\n\n  {% do return(some_custom_macro_with_sql(arg_dict[\"target_relation\"], arg_dict[\"temp_relation\"], arg_dict[\"unique_key\"], arg_dict[\"dest_columns\"], arg_dict[\"incremental_predicates\"])) %}\n\n{% endmacro %}\n\n\n{% macro some_custom_macro_with_sql(target_relation, temp_relation, unique_key, dest_columns, incremental_predicates) %}\n\n    {%- set dest_cols_csv = get_quoted_csv(dest_columns | map(attribute=\"name\")) -%}\n\n    insert into {{ target_relation }} ({{ dest_cols_csv }})\n    (\n        select {{ dest_cols_csv }}\n        from {{ temp_relation }}\n    )\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Querying Saved Queries in GraphQL\nDESCRIPTION: Executes a previously saved query identified by the name 'new_customer_orders'. This approach is useful for frequently used queries without having to specify all the parameters each time.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-graphql.md#2025-04-09_snippet_15\n\nLANGUAGE: graphql\nCODE:\n```\nmutation {\n  createQuery(\n    environmentId: \"123\"\n    savedQuery: \"new_customer_orders\"\n  ) {\n    queryId\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using DATEDIFF dbt Macro\nDESCRIPTION: Demonstrates how to use the DATEDIFF dbt macro to calculate the difference between two dates. This macro provides a standardized way to use DATEDIFF across different databases.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-07-05-datediff-sql-love-letter.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\t*,\n\t{{ datediff(\"order_date\", \"'2022-06-09'\", \"day\") }}\nfrom {{ ref('orders') }}\n```\n\n----------------------------------------\n\nTITLE: Setting user profile configurations in profiles.yml (Deprecated)\nDESCRIPTION: Shows the deprecated method of setting default values for global configurations in the profiles.yml file. This approach sets defaults for all projects using the profile directory, typically for projects running on the local machine.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/project-flags.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nconfig:\n  <THIS-CONFIG>: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Decodable Profile in dbt\nDESCRIPTION: A YAML configuration for the profiles.yml file that defines how dbt connects to Decodable. It includes required fields like account_name and profile_name, along with optional parameters for controlling test materialization, timeout settings, and namespacing.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/decodable-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndbt-decodable:       \n  target: dev         \n  outputs:           \n    dev:              \n      type: decodable\n      database: None  \n      schema: None    \n      account_name: [your account]          \n      profile_name: [name of the profile]   \n      materialize_tests: [true | false]     \n      timeout: [ms]                         \n      preview_start: [earliest | latest]    \n      local_namespace: [namespace prefix]   \n```\n\n----------------------------------------\n\nTITLE: Overriding Database and Schema for a Package Source in dbt\nDESCRIPTION: Example of overriding the database and schema properties for a source defined in the GitHub Source package. This allows users to specify their own database and schema names while keeping other source configurations from the package.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/overrides.md#2025-04-09_snippet_1\n\nLANGUAGE: yml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: github\n    overrides: github_source\n\n    database: RAW\n    schema: github_data\n```\n\n----------------------------------------\n\nTITLE: Defining Basic Exposure Properties in YAML\nDESCRIPTION: Template showing the structure and available properties for defining exposures in dbt YAML files. Includes required and optional fields like name, type, maturity, owner information, and dependencies.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/exposure-properties.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nexposures:\n  - name: <string_with_underscores>\n    description: <markdown_string>\n    type: {dashboard, notebook, analysis, ml, application}\n    url: <string>\n    maturity: {high, medium, low}\n    enabled: true | false\n    tags: [<string>]\n    meta: {<dictionary>}\n    owner:\n      name: <string>\n      email: <string>\n    \n    depends_on:\n      - ref('model')\n      - ref('seed')\n      - source('name', 'table')\n      - metric('metric_name')\n      \n    label: \"Human-Friendly Name for this Exposure!\"\n    config:\n      enabled: true | false\n\n  - name: ... # declare properties of additional exposures\n```\n\n----------------------------------------\n\nTITLE: Using dbt_utils.star() for Dynamic Column Selection in SQL\nDESCRIPTION: Demonstrates the use of the dbt_utils.star() macro to dynamically select columns in a dbt model. This approach simplifies column selection when updating specific fields in a table with many columns.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/migrate-from-stored-procedures.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT\n    {{ dbt_utils.star(from=ref('stg_orders'), except=['type']) }},\n    CASE\n        WHEN total < 0 THEN 'return'\n        ELSE type\n    END AS type,\n\nFROM {{ ref('stg_orders') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring IBM DB2 Connection in profiles.yml\nDESCRIPTION: Example configuration for connecting to IBM DB2 in the profiles.yml file. Specifies connection parameters including schema, database, host, port, protocol, username, and password.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/ibmdb2-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: ibmdb2\n      schema: analytics\n      database: test\n      host: localhost\n      port: 50000\n      protocol: TCPIP\n      username: my_username\n      password: my_password\n```\n\n----------------------------------------\n\nTITLE: Configuring Project-wide Late Binding Views in dbt_project.yml\nDESCRIPTION: YAML configuration to set all views in a dbt project to be late-binding by default, improving stability during production deployments.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/redshift-configs.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  +bind: false # Materialize all views as late-binding\n  project_name:\n    ....\n```\n\n----------------------------------------\n\nTITLE: Defining Jaffle Shop Sources in YAML\nDESCRIPTION: YAML configuration for defining sources from the jaffle_shop database, including customers and orders tables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/sl-snowflake-qs.md#2025-04-09_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n - name: jaffle_shop\n   database: raw\n   schema: jaffle_shop\n   tables:\n     - name: customers\n     - name: orders\n```\n\n----------------------------------------\n\nTITLE: Configuring Hybrid Columnar Compression for Archival in Oracle Table Materialization\nDESCRIPTION: These snippets show how to configure Hybrid Columnar Compression for archival in Oracle table materialization using dbt. It demonstrates both LOW and HIGH compression options, with HIGH providing the highest compression ratio.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/oracle-configs.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{{config(materialized='table', table_compression_clause='COLUMN STORE COMPRESS FOR ARCHIVE LOW')}}\nSELECT c.cust_id, c.cust_first_name, c.cust_last_name\nfrom {{ source('sh_database', 'customers') }} c\n```\n\nLANGUAGE: sql\nCODE:\n```\n{{config(materialized='table', table_compression_clause='COLUMN STORE COMPRESS FOR ARCHIVE HIGH')}}\nSELECT c.cust_id, c.cust_first_name, c.cust_last_name\nfrom {{ source('sh_database', 'customers') }} c\n```\n\n----------------------------------------\n\nTITLE: Disabling Color Output in profiles.yml\nDESCRIPTION: This YAML configuration in profiles.yml turns off the default colorized output in the terminal for dbt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/print-output.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nconfig:\n  use_colors: False\n```\n\n----------------------------------------\n\nTITLE: Using N-Plus Operator for Specific Degree Node Selection in dbt Commands\nDESCRIPTION: Shows how to use the 'n-plus' operator in dbt commands to select models and their dependencies up to a specific degree. This allows for more fine-grained control over the selection of ancestors and descendants.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/graph-operators.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --select \"my_model+1\"        # select my_model and its first-degree descendants\ndbt run --select \"2+my_model\"        # select my_model, its first-degree ancestors (\"parents\"), and its second-degree ancestors (\"grandparents\")\ndbt run --select \"3+my_model+4\"      # select my_model, its ancestors up to the 3rd degree, and its descendants down to the 4th degree\n```\n\n----------------------------------------\n\nTITLE: Running Tests on All dbt Sources\nDESCRIPTION: This command runs tests on all sources defined in your dbt project. It uses the --select flag with a wildcard pattern to target all sources.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Tests/testing-sources.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n  dbt test --select \"source:*\"\n```\n\n----------------------------------------\n\nTITLE: Adding Entities to Semantic Model\nDESCRIPTION: Configuration adding primary and foreign entity definitions to the semantic model for handling order and customer IDs.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/sl-snowflake-qs.md#2025-04-09_snippet_19\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_models:\n  - name: orders\n    defaults:\n      agg_time_dimension: order_date\n    description: |\n      Order fact table. This table's grain is one row per order.\n    model: ref('fct_orders')\n    entities: \n      - name: order_id\n        type: primary\n      - name: customer\n        expr: customer_id\n        type: foreign\n```\n\n----------------------------------------\n\nTITLE: Installing dbt-watsonx-spark and dbt-core\nDESCRIPTION: Command to install both dbt-core and dbt-watsonx-spark adapter using pip. This is necessary because adapters no longer automatically install dbt-core from version 1.8 onwards.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/watsonx-spark-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npython -m pip install dbt-core dbt-watsonx-spark\n```\n\n----------------------------------------\n\nTITLE: Configuring View Materialization in Doris/SelectDB (Config Block)\nDESCRIPTION: Creates a Doris view using the config block approach directly in the model file. This configuration is applied at the individual model level.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/doris-configs.md#2025-04-09_snippet_1\n\nLANGUAGE: jinja\nCODE:\n```\n{{ config(materialized = \"view\") }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Docs for Seeds in schema.yml\nDESCRIPTION: Shows how to configure documentation visibility and node color for specific seeds in a schema.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/docs.md#2025-04-09_snippet_3\n\nLANGUAGE: yml\nCODE:\n```\nversion: 2\n\nseeds:\n  - name: seed_name\n    docs:\n      show: true | false\n      node_color: color_id # Use name (such as node_color: purple) or hex code with quotes (such as node_color: \"#cd7f32\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Hybrid Columnar Compression for Query in Oracle Table Materialization\nDESCRIPTION: These snippets demonstrate how to configure Hybrid Columnar Compression for querying in Oracle table materialization using dbt. It shows both LOW and HIGH compression options, which are useful in data warehouse environments.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/oracle-configs.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{config(materialized='table', table_compression_clause='COLUMN STORE COMPRESS FOR QUERY LOW')}}\nSELECT c.cust_id, c.cust_first_name, c.cust_last_name\nfrom {{ source('sh_database', 'customers') }} c\n```\n\nLANGUAGE: sql\nCODE:\n```\n{{config(materialized='table', table_compression_clause='COLUMN STORE COMPRESS FOR QUERY HIGH')}}\nSELECT c.cust_id, c.cust_first_name, c.cust_last_name\nfrom {{ source('sh_database', 'customers') }} c\n```\n\n----------------------------------------\n\nTITLE: Configuring Unlogged Tables in SQL Model\nDESCRIPTION: Demonstrates how to configure an unlogged table in a dbt SQL model for improved performance. Unlogged tables bypass the write-ahead log for faster operations but with reduced safety guarantees.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/postgres-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(materialized='table', unlogged=True) }}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Adding a Tag to a Singular Test Example\nDESCRIPTION: Example showing how to add a tag to a singular test using the config() macro in a SQL test file. This applies the tag to this specific test only.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/data-test-configs.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(tags = ['my_tag']) }}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Access in Schema YAML\nDESCRIPTION: Defines access modifiers for a model in a schema.yml file, allowing you to set access as private, protected, or public. This controls which models can reference this model based on group membership.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/access.md#2025-04-09_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: model_name\n    access: private | protected | public\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Models in Properties YAML File (1.9+)\nDESCRIPTION: Example of configuring dbt models using a properties.yml file for version 1.9 and later. This includes all previous configuration options plus the new event_time parameter for designating a time field.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/model-configs.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: [<model-name>] #  Must match the filename of a model -- including case sensitivity.\n    config:\n      [enabled]: true | false\n      [tags]: <string> | [<string>]\n      [pre_hook]: <sql-statement> | [<sql-statement>]\n      [post_hook]: <sql-statement> | [<sql-statement>]\n      [database]: <string>\n      [schema]: <string>\n      [alias]: <string>\n      [persist_docs]: <dict>\n      [full_refresh]: <boolean>\n      [meta]: {<dictionary>}\n      [grants]: {<dictionary>}\n      [contract]: {<dictionary>}\n      [event_time]: my_time_field\n```\n\n----------------------------------------\n\nTITLE: Customizing Package Installation Path in dbt_project.yml\nDESCRIPTION: Example of configuring dbt to install packages in a subdirectory named 'packages' instead of the default 'dbt_packages' directory. This directory is typically git-ignored.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/packages-install-path.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\npackages-install-path: packages\n```\n\n----------------------------------------\n\nTITLE: Defining a Basic Materialization Block in dbt\nDESCRIPTION: Shows the basic syntax for defining a materialization block in dbt, which allows for adapter-specific implementations where needed.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/create-new-materializations.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{% materialization [materialization name], [\"specified adapter\" | default] %}\n...\n{% endmaterialization %}\n```\n\n----------------------------------------\n\nTITLE: Referencing Custom Identifier Source in dbt SQL\nDESCRIPTION: This SQL snippet demonstrates how to reference a source table with a custom identifier in a dbt model. It shows both the dbt SQL and the compiled SQL, highlighting how the custom identifier is used.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/identifier.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nselect * from {{ source('jaffle_shop', 'orders') }}\n```\n\nLANGUAGE: sql\nCODE:\n```\nselect * from jaffle_shop.api_orders\n```\n\n----------------------------------------\n\nTITLE: Alternative Method to Disable Query Comments\nDESCRIPTION: Configuration to disable query comments in dbt by explicitly setting to null.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/query-comment.md#2025-04-09_snippet_6\n\nLANGUAGE: yml\nCODE:\n```\nquery-comment: null\n\n```\n\n----------------------------------------\n\nTITLE: Defining snapshot-paths in dbt_project.yml\nDESCRIPTION: Specifies the configuration for snapshot-paths in the dbt_project.yml file. This setting allows you to customize the directories where dbt looks for snapshots.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/snapshot-paths.md#2025-04-09_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nsnapshot-paths: [directorypath]\n```\n\n----------------------------------------\n\nTITLE: SQL Schema Creation with Authorization in Microsoft Fabric\nDESCRIPTION: Example SQL statement showing how schema authorization is applied when creating schemas in Microsoft Fabric. This is especially useful when authenticating with a principal who has permissions based on a group like a Microsoft Entra ID group.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/fabric-setup.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nCREATE SCHEMA [schema_name] AUTHORIZATION [schema_authorization]\n```\n\n----------------------------------------\n\nTITLE: Successful dbt debug Output\nDESCRIPTION: Example of successful output from the dbt debug command, showing that the configuration files are valid and the database connection is working correctly.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/debug-errors.md#2025-04-09_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n$ dbt debug\nRunning with dbt=1.7.1\nUsing profiles.yml file at /Users/alice/.dbt/profiles.yml\nUsing dbt_project.yml file at /Users/alice/jaffle-shop-dbt/dbt_project.yml\n\nConfiguration:\n  profiles.yml file [OK found and valid]\n  dbt_project.yml file [OK found and valid]\n\nRequired dependencies:\n - git [OK found]\n\nConnection:\n  ...\n  Connection test: OK connection ok\n```\n\n----------------------------------------\n\nTITLE: Creating Pit Stops Intermediate Model in SQL\nDESCRIPTION: Processes pit stop data and calculates total pit stops per race using window functions. The model preserves individual pit stop details while adding an aggregated count feature by partitioning over race_id and driver_id.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_16\n\nLANGUAGE: sql\nCODE:\n```\nwith stg_f1__pit_stops as\n(\n    select * from {{ ref('stg_f1_pit_stops') }}\n),\n\npit_stops_per_race as (\n    select\n        race_id,\n        driver_id,\n        stop_number,\n        lap,\n        lap_time_formatted,\n        pit_stop_duration_seconds,\n        pit_stop_milliseconds,\n        max(stop_number) over (partition by race_id,driver_id) as total_pit_stops_per_race\n    from stg_f1__pit_stops\n)\n\nselect * from pit_stops_per_race\n```\n\n----------------------------------------\n\nTITLE: Configuring Database Overrides in dbt_project.yml\nDESCRIPTION: Example configuration that changes all models in the jaffle_shop project to be built into a database called jaffle_shop. For BigQuery users, this would be configured as a project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/custom-databases.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nname: jaffle_shop\n\nmodels:\n  jaffle_shop:\n    +database: jaffle_shop\n\n    # For BigQuery users:\n    # project: jaffle_shop\n```\n\n----------------------------------------\n\nTITLE: Configuring SQLite Profile in YAML for dbt\nDESCRIPTION: Example configuration for setting up a SQLite target in the profiles.yml file. It includes required fields like type, threads, database, schema, and schemas_and_paths, as well as optional fields like extensions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/sqlite-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: sqlite\n      threads: 1\n      database: 'database'\n      schema: 'main'\n      schemas_and_paths:\n        main: 'file_path/database_name.db'\n      schema_directory: 'file_path'\n      #optional fields\n      extensions:\n        - \"/path/to/sqlean/crypto.so\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Individual Snapshot in YAML\nDESCRIPTION: This snippet demonstrates how to configure an individual snapshot using a YAML file, including strategy, unique key, and other parameters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/snapshot-configs.md#2025-04-09_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n - name: orders_snapshot\n   relation: source('jaffle_shop', 'orders')\n   config:\n     unique_key: id\n     strategy: timestamp\n     updated_at: updated_at\n     persist_docs:\n       relation: true\n       columns: true\n```\n\n----------------------------------------\n\nTITLE: Installing dbt-spark with ODBC and PyHive Dependencies\nDESCRIPTION: Commands to install dbt-spark with additional dependencies for ODBC and PyHive connections. This is necessary for connecting to Databricks via ODBC driver or to Spark clusters via thrift or http methods.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/spark-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: zsh\nCODE:\n```\n# odbc connections\n$ python -m pip install \"dbt-spark[ODBC]\"\n\n# thrift or http connections\n$ python -m pip install \"dbt-spark[PyHive]\"\n```\n\n----------------------------------------\n\nTITLE: Custom Directory Configuration\nDESCRIPTION: Example of using a custom directory name 'transformations' instead of the default 'models' directory for dbt models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/model-paths.md#2025-04-09_snippet_3\n\nLANGUAGE: yml\nCODE:\n```\nmodel-paths: [\"transformations\"]\n```\n\n----------------------------------------\n\nTITLE: Single Dimension Value Query\nDESCRIPTION: Query to fetch all values for a specific dimension without any metrics.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-jdbc.md#2025-04-09_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nselect * from {{\n    semantic_layer.query(group_by=['customer__customer_type'])\n                  }}\n```\n\n----------------------------------------\n\nTITLE: Training and Saving Logistic Regression Model with Snowpark and scikit-learn\nDESCRIPTION: This Python script trains a logistic regression model to predict driver positions using Snowpark and scikit-learn. It handles data preprocessing, model training, evaluation, and saving the model to a Snowflake stage. The script also organizes the data into training and testing sets.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nimport snowflake.snowpark.functions as F\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix, balanced_accuracy_score\nimport io\nfrom sklearn.linear_model import LogisticRegression\nfrom joblib import dump, load\nimport joblib\nimport logging\nimport sys\nfrom joblib import dump, load\n\nlogger = logging.getLogger(\"mylog\")\n\ndef save_file(session, model, path, dest_filename):\n    input_stream = io.BytesIO()\n    joblib.dump(model, input_stream)\n    session._conn.upload_stream(input_stream, path, dest_filename)\n    return \"successfully created file: \" + path\n\ndef model(dbt, session):\n    dbt.config(\n        packages = ['numpy','scikit-learn','pandas','numpy','joblib','cachetools'],\n        materialized = \"table\",\n        tags = \"train\"\n    )\n    # Create a stage in Snowflake to save our model file\n    session.sql('create or replace stage MODELSTAGE').collect()\n\n    #session._use_scoped_temp_objects = False\n    version = \"1.0\"\n    logger.info('Model training version: ' + version)\n\n    # read in our training and testing upstream dataset\n    test_train_df = dbt.ref(\"train_test_dataset\")\n\n    #  cast snowpark df to pandas df\n    test_train_pd_df = test_train_df.to_pandas()\n    target_col = \"POSITION_LABEL\"\n\n    # split out covariate predictors, x, from our target column position_label, y.\n    split_X = test_train_pd_df.drop([target_col], axis=1)\n    split_y = test_train_pd_df[target_col]\n\n    # Split out our training and test data into proportions\n    X_train, X_test, y_train, y_test  = train_test_split(split_X, split_y, train_size=0.7, random_state=42)\n    train = [X_train, y_train]\n    test = [X_test, y_test]\n    # now we are only training our one model to deploy\n    # we are keeping the focus on the workflows and not algorithms for this lab!\n    model = LogisticRegression()\n\n    # fit the preprocessing pipeline and the model together \n    model.fit(X_train, y_train)   \n    y_pred = model.predict_proba(X_test)[:,1]\n    predictions = [round(value) for value in y_pred]\n    balanced_accuracy =  balanced_accuracy_score(y_test, predictions)\n\n    # Save the model to a stage\n    save_file(session, model, \"@MODELSTAGE/driver_position_\"+version, \"driver_position_\"+version+\".joblib\" )\n    logger.info('Model artifact:' + \"@MODELSTAGE/driver_position_\"+version+\".joblib\")\n\n    # Take our pandas training and testing dataframes and put them back into snowpark dataframes\n    snowpark_train_df = session.write_pandas(pd.concat(train, axis=1, join='inner'), \"train_table\", auto_create_table=True, create_temp_table=True)\n    snowpark_test_df = session.write_pandas(pd.concat(test, axis=1, join='inner'), \"test_table\", auto_create_table=True, create_temp_table=True)\n\n    # Union our training and testing data together and add a column indicating train vs test rows\n    return  snowpark_train_df.with_column(\"DATASET_TYPE\", F.lit(\"train\")).union(snowpark_test_df.with_column(\"DATASET_TYPE\", F.lit(\"test\")))\n```\n\n----------------------------------------\n\nTITLE: Creating Staging Model for Orders in dbt\nDESCRIPTION: This SQL snippet creates a staging model for orders, selecting and renaming columns from the orders table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/azure-synapse-analytics-qs.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    ID as order_id,\n    USER_ID as customer_id,\n    ORDER_DATE as order_date,\n    STATUS as status\n\nfrom dbo.orders\n```\n\n----------------------------------------\n\nTITLE: Querying Metrics with the Semantic Layer Command\nDESCRIPTION: This dbt Semantic Layer command queries a revenue metric grouped by time. It demonstrates how to validate that the time spine is working correctly with MetricFlow by executing a time-based aggregation of metrics.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/mf-time-spine.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndbt sl query --metrics revenue --group-by metric_time\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Models in Project YAML File\nDESCRIPTION: This snippet shows how to configure dbt models in the dbt_project.yml file. It includes options for materialization, SQL headers, configuration change handling, and unique keys. The configuration can be applied to specific resource paths.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/model-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  [<resource-path>]:\n    [+][materialized]: <materialization_name>\n    [+][sql_header]: <string>\n    [+][on_configuration_change]: apply | continue | fail # Only for materialized views on supported adapters\n    [+][unique_key]: <column_name_or_expression>\n    [+][batch_size]: day | hour | month | year\n    [+][begin]: \"<ISO formatted date or datetime (like, \\\"2024-01-15T12:00:00Z\\\")\"\n    [+][lookback]: <integer>\n    [+][concurrent_batches]: true | false\n```\n\n----------------------------------------\n\nTITLE: Configuring Snapshots in dbt_project.yml\nDESCRIPTION: This snippet shows how to configure snapshots in the dbt_project.yml file. It includes various configuration options such as enabled, tags, alias, hooks, and more.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/snapshot-configs.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  [<resource-path>]:\n    [+][enabled]: true | false\n    [+][tags]: <string> | [<string>]\n    [+][alias]: <string>\n    [+][pre-hook]: <sql-statement> | [<sql-statement>]\n    [+][post-hook]: <sql-statement> | [<sql-statement>]\n    [+][persist_docs]: {<dict>}\n    [+][grants]: {<dict>}\n    [+][event_time]: my_time_field\n```\n\n----------------------------------------\n\nTITLE: Filtering Orders with SQL WHERE Clause in dbt\nDESCRIPTION: This snippet demonstrates how to use a WHERE clause in a SELECT statement to filter orders that have not been returned. It uses dbt's ref function to reference the 'orders' model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/clauses/sql-where.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\torder_id,\n\tcustomer_id,\n\tamount\nfrom {{ ref('orders') }}\nwhere status != 'returned'\n```\n\n----------------------------------------\n\nTITLE: Creating a New Webhook Subscription with dbt Cloud API\nDESCRIPTION: This POST request creates a new outbound webhook. It requires the account's access URL and account ID as path parameters, and includes webhook details in the request body.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/webhooks.md#2025-04-09_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nPOST https://{your access URL}/api/v3/accounts/{account_id}/webhooks/subscriptions\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n\t\"event_types\": [\n\t\t\t\"job.run.started\"\n\t],\n\t\"name\": \"Webhook for jobs\",\n\t\"client_url\": \"https://test.com\",\n\t\"active\": true,\n\t\"description\": \"A webhook for when jobs are started\",\n\t\"job_ids\": [\n\t\t\t123,\n\t\t\t321\n\t]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive Connector for Incremental Overwrite in Trino\nDESCRIPTION: This snippet shows how to set the insert-existing-partitions-behavior setting for a Hive connector in Trino to enable OVERWRITE functionality.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/trino-configs.md#2025-04-09_snippet_12\n\nLANGUAGE: ini\nCODE:\n```\n<hive-catalog-name>.insert-existing-partitions-behavior=OVERWRITE\n```\n\n----------------------------------------\n\nTITLE: Using the no-write-json flag in dbt for state comparison\nDESCRIPTION: This command uses the --no-write-json flag to prevent manifest.json from being overwritten during state comparison operations. It executes the 'ls' command while selecting only modified models based on the state in the target directory.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_recommendation-overwriting-manifest.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndbt --no-write-json ls --select state:modified --state target\n```\n\n----------------------------------------\n\nTITLE: Configuring External Sources in dbt YAML\nDESCRIPTION: This YAML snippet shows how to configure external sources in dbt, specifically for a Hive catalog. It defines the schema and table name for the external source.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/starrocks-configs.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  - name: external_example\n    schema: hive_catalog.hive_db\n    tables:\n      - name: hive_table_name\n```\n\n----------------------------------------\n\nTITLE: Exposure-based Selection\nDESCRIPTION: Selecting models that feed into specific exposures\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/methods.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --select \"+exposure:weekly_kpis\"                # run all models that feed into the weekly_kpis exposure\ndbt test --select \"+exposure:*\"                         # test all resources upstream of all exposures\ndbt ls --select \"+exposure:*\" --resource-type source    # list all source tables upstream of all exposures\n```\n\n----------------------------------------\n\nTITLE: Setting up Snowflake Environment for dbt Semantic Layer\nDESCRIPTION: SQL commands to create a virtual warehouse, databases for raw and analytics data, and schemas for jaffle_shop and stripe data in Snowflake in preparation for the dbt Semantic Layer.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/sl-snowflake-qs.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n-- Create a virtual warehouse named 'transforming'\ncreate warehouse transforming;\n\n-- Create two databases: one for raw data and another for analytics\ncreate database raw;\ncreate database analytics;\n\n-- Within the 'raw' database, create two schemas: 'jaffle_shop' and 'stripe'\ncreate schema raw.jaffle_shop;\ncreate schema raw.stripe;\n```\n\n----------------------------------------\n\nTITLE: Excluding Models from dbt Run Command\nDESCRIPTION: This snippet demonstrates how to use the --exclude flag with dbt run to remove specific models from the selected set. It selects all models in 'my_package' and their children, except 'a_big_model' and its children.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/exclude.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --select \"my_package\".*+ --exclude \"my_package.a_big_model+\"\n```\n\n----------------------------------------\n\nTITLE: Authenticating with fly.io - Signup\nDESCRIPTION: Command to sign up for a new fly.io account via CLI\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/serverless-datadog.md#2025-04-09_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nflyctl auth signup\n```\n\n----------------------------------------\n\nTITLE: Example profiles.yml Configuration\nDESCRIPTION: Example of a profiles.yml file that defines connection details for a PostgreSQL database. The profile name must match what's referenced in the dbt_project.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/debug-errors.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\njaffle_shop: # this does not match the profile: key\n  target: dev\n\n  outputs:\n    dev:\n      type: postgres\n      schema: dbt_alice\n      ... # other connection details\n```\n\n----------------------------------------\n\nTITLE: Configuring Compute Resources in Databricks Profile\nDESCRIPTION: Demonstrates how to set up multiple compute resources in your Databricks profile, allowing you to specify different SQL warehouses or clusters for different models. This includes setting a default compute and defining named compute resources with different HTTP paths.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/databricks-configs.md#2025-04-09_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nprofile-name:\n  target: target-name # this is the default target\n  outputs:\n    target-name:\n      type: databricks\n      catalog: optional catalog name if you are using Unity Catalog\n      schema: schema name # Required        \n      host: yourorg.databrickshost.com # Required\n\n      ### This path is used as the default compute\n      http_path: /sql/your/http/path # Required        \n      \n      ### New compute section\n      compute:\n\n        ### Name that you will use to refer to an alternate compute\n       Compute1:\n          http_path: '/sql/your/http/path' # Required of each alternate compute\n\n        ### A third named compute, use whatever name you like\n        Compute2:\n          http_path: '/some/other/path' # Required of each alternate compute\n      ...\n\n    target-name: # additional targets\n      ...\n      ### For each target, you need to define the same compute,\n      ### but you can specify different paths\n      compute:\n\n        ### Name that you will use to refer to an alternate compute\n        Compute1:\n          http_path: '/sql/your/http/path' # Required of each alternate compute\n\n        ### A third named compute, use whatever name you like\n        Compute2:\n          http_path: '/some/other/path' # Required of each alternate compute\n      ...\n```\n\n----------------------------------------\n\nTITLE: Creating Folder Structure for GitHub Actions Workflow\nDESCRIPTION: Demonstrates the folder structure needed for setting up GitHub Actions workflow for SQLFluff linting in a dbt project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/set-up-ci.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nmy_awesome_project\n .github\n    workflows\n       lint_on_push.yml\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Payload for Run Started Event\nDESCRIPTION: This is an example of the JSON payload that dbt Cloud will send to your webhook endpoint when a job run has started. It includes details about the job, environment, project, and run status.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/webhooks.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"accountId\": 1,\n  \"webhooksID\": \"wsu_12345abcde\",\n  \"eventId\": \"wev_2L6Z3l8uPedXKPq9D2nWbPIip7Z\",\n  \"timestamp\": \"2023-01-31T19:28:15.742843678Z\",\n  \"eventType\": \"job.run.started\",\n  \"webhookName\": \"test\",\n  \"data\": {\n    \"jobId\": \"123\",\n    \"jobName\": \"Daily Job (dbt build)\",\n    \"runId\": \"12345\",\n    \"environmentId\": \"1234\",\n    \"environmentName\": \"Production\",\n    \"dbtVersion\": \"1.0.0\",\n    \"projectName\": \"Snowflake Github Demo\",\n    \"projectId\": \"167194\",\n    \"runStatus\": \"Running\",\n    \"runStatusCode\": 3,\n    \"runStatusMessage\": \"None\",\n    \"runReason\": \"Kicked off from UI by test@test.com\",\n    \"runStartedAt\": \"2023-01-31T19:28:07Z\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Combining Multiple INSERTs with UNION ALL in dbt SQL\nDESCRIPTION: Shows how to handle multiple INSERT statements into the same table by using UNION ALL in a dbt model. The example combines data from US and EU customers into a single all_customers model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/migrate-from-stored-procedures.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE all_customers\n\nINSERT INTO all_customers SELECT * FROM us_customers\n\nINSERT INTO all_customers SELECT * FROM eu_customers\n```\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT * FROM {{ ref('us_customers') }}\n\nUNION ALL\n\nSELECT * FROM {{ ref('eu_customers') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Failure Limit for Specific Generic Test in YAML\nDESCRIPTION: This snippet shows how to set a failure limit for a specific instance of a generic (schema) test in a YAML file. It limits the number of failures to 1000 for an 'accepted_values' test on a column.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/limit.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: large_table\n    columns:\n      - name: very_unreliable_column\n        tests:\n          - accepted_values:\n              values: [\"a\", \"b\", \"c\"]\n              config:\n                limit: 1000  # will only include the first 1000 failures\n```\n\n----------------------------------------\n\nTITLE: Adding a Tag to a Generic Test Example\nDESCRIPTION: Example showing how to add a tag to a specific instance of a generic test in a YAML properties file. This configuration will only apply to the specified test.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/data-test-configs.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: my_model\n    columns:\n      - name: id\n        tests:\n          - unique:\n              tags: ['my_tag']\n```\n\n----------------------------------------\n\nTITLE: Loading Relation Example\nDESCRIPTION: Demonstrates how to check if a relation exists using load_relation\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/adapter.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{% set relation_exists = load_relation(ref('my_model')) is not none %}\n{% if relation_exists %}\n      {{ log(\"my_model has already been built\", info=true) }}\n{% else %}\n      {{ log(\"my_model doesn't exist in the warehouse. Maybe it was dropped?\", info=true) }}\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Incremental Model with Merge Strategy\nDESCRIPTION: Configuring an incremental model with the 'merge' strategy, which uses Trino's MERGE statement to insert new records and update existing ones based on a unique key.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/trino-configs.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n      materialized = 'incremental',\n      unique_key='user_id',\n      incremental_strategy='merge',\n      )\n}}\nselect * from {{ ref('users') }}\n{% if is_incremental() %}\n  where updated_ts > (select max(updated_ts) from {{ this }})\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Using Advanced Macro with Node Context\nDESCRIPTION: Configuration to use an advanced query comment macro that requires the node context parameter.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/query-comment.md#2025-04-09_snippet_17\n\nLANGUAGE: yml\nCODE:\n```\nquery-comment: \"{{ query_comment(node) }}\"\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Hash UDF for Snapshots in Teradata\nDESCRIPTION: Setting a custom hash UDF for snapshots in Teradata instead of the default HASHROW function, allowing for custom hash generation for the dbt_scd_id column.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/teradata-configs.md#2025-04-09_snippet_17\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot snapshot_example %}\n{{\n  config(\n    target_schema='snapshots',\n    unique_key='id',\n    strategy='check',\n    check_cols=[\"c2\"],\n    snapshot_hash_udf='GLOBAL_FUNCTIONS.hash_md5'\n  )\n}}\nselect * from {{ ref('order_payments') }}\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Sharded Tables as Source in BigQuery\nDESCRIPTION: This example shows how to reference sharded tables as a source in BigQuery using the identifier parameter. It uses a wildcard (*) in the identifier to match multiple sharded tables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/identifier.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: ga\n    tables:\n      - name: events\n        identifier: \"events_*\"\n```\n\n----------------------------------------\n\nTITLE: Recommended Use of Relative Paths for macro-paths\nDESCRIPTION: An example showing the recommended approach of using relative paths for the macro-paths configuration, which is the preferred method in dbt projects.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/macro-paths.md#2025-04-09_snippet_1\n\nLANGUAGE: yml\nCODE:\n```\nmacro-paths: [\"macros\"]\n```\n\n----------------------------------------\n\nTITLE: Example of Invalid dbt_project.yml\nDESCRIPTION: An example of a dbt_project.yml file with an invalid configuration key ('hello') that causes a runtime error. The file needs to be corrected by removing or replacing the invalid key.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/debug-errors.md#2025-04-09_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nname: jaffle_shop\nhello: world # this is not allowed\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Quote Columns in dbt_project.yml for Teradata\nDESCRIPTION: Setting the quote_columns configuration for seeds in Teradata to either false or true depending on whether CSV column headers have spaces.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/teradata-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  +quote_columns: false  #or `true` if you have csv column headers with spaces\n```\n\n----------------------------------------\n\nTITLE: Disabling Anonymous Usage Stats in dbt Project Configuration\nDESCRIPTION: Configuration snippet to opt out of dbt's anonymous usage statistics collection by adding flags to the dbt_project.yml file. This setting allows dbt Core users to disable telemetry.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/usage-stats.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nflags:\n  send_anonymous_usage_stats: false\n```\n\n----------------------------------------\n\nTITLE: Disabling a Specific Table from a Source\nDESCRIPTION: Example showing how to disable a specific table from a source by qualifying the resource path with a package name, source name, and table name.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/source-configs.md#2025-04-09_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  events:\n    clickstream:\n      pageviews:\n        +enabled: false\n```\n\n----------------------------------------\n\nTITLE: Project-level Updated_at Configuration\nDESCRIPTION: YAML configuration in dbt_project.yml for setting default updated_at configuration for snapshots in specified resource paths.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/updated_at.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  [<resource-path>]:\n    +strategy: timestamp\n    +updated_at: column_name\n```\n\n----------------------------------------\n\nTITLE: Writing Multi-line Descriptions Using > Symbol in dbt YAML\nDESCRIPTION: This example shows how to write a longer description that spans multiple lines using the '>' symbol in a dbt YAML file. Line breaks are removed and Markdown formatting can be used. This method is recommended for simple, single-paragraph descriptions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Docs/long-descriptions.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  version: 2\n\n  models:\n  - name: customers\n    description: >\n      Lorem ipsum **dolor** sit amet, consectetur adipisicing elit, sed do eiusmod\n      tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam,\n      quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo\n      consequat.\n```\n\n----------------------------------------\n\nTITLE: Exclude Functionality in YAML Selectors\nDESCRIPTION: Two equivalent ways to use the exclude functionality in YAML selectors to apply set difference operations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/yaml-selectors.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n- method: tag\n  value: nightly\n  exclude:\n    - \"@tag:daily\"\n```\n\nLANGUAGE: yaml\nCODE:\n```\n- union:\n    - method: tag\n      value: nightly\n    - exclude:\n       - method: tag\n         value: daily\n```\n\n----------------------------------------\n\nTITLE: Paginating Metadata Calls with Semantic Layer JDBC API\nDESCRIPTION: SQL queries demonstrating how to paginate results for semantic_layer.metrics() and semantic_layer.dimensions() calls using page_size and page_number parameters. This allows retrieving a subset of results instead of the full result set.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-jdbc.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\n-- Retrieves the 5th page with a page size of 10 metrics\nselect * from {{ semantic_layer.metrics(page_size=10, page_number=5) }}\n\n-- Retrieves the 1st page with a page size of 10 metrics\nselect * from {{ semantic_layer.metrics(page_size=10) }}\n\n-- Retrieves all metrics without pagination\nselect * from {{ semantic_layer.metrics() }}\n```\n\n----------------------------------------\n\nTITLE: Creating a Cents to Dollars Conversion Macro\nDESCRIPTION: This SQL macro converts cents to dollars by dividing the input column value by 100 and formatting the result as a numeric value with a configurable scale.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/arguments.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{% macro cents_to_dollars(column_name, scale=2) %}\n    ({{ column_name }} / 100)::numeric(16, {{ scale }})\n{% endmacro %}\n\n```\n\n----------------------------------------\n\nTITLE: Converting List to Set with the set_strict() Method in Jinja\nDESCRIPTION: Shows how to convert a list with duplicate values to a set using the set_strict() method. Like the regular set() method, it produces a set of unique values from the input list.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/set.md#2025-04-09_snippet_3\n\nLANGUAGE: jinja\nCODE:\n```\n{% set my_list = [1, 2, 2, 3] %}\n{% set my_set = set(my_list) %}\n{% do log(my_set) %}  {# {1, 2, 3} #}\n```\n\n----------------------------------------\n\nTITLE: Defining Dependencies in dbt\nDESCRIPTION: Configuration of both package and project dependencies in dependencies.yml, showing how to specify package versions and project references.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/govern/project-dependencies.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\npackages:\n  - package: dbt-labs/dbt_utils\n    version: 1.1.1\n\nprojects:\n  - name: jaffle_finance  # case sensitive and matches the 'name' in the 'dbt_project.yml'\n```\n\n----------------------------------------\n\nTITLE: Configuring DuckDB Profile with AWS S3 Integration\nDESCRIPTION: Example YAML configuration for connecting dbt to DuckDB with S3 access. Shows how to specify the database path, load extensions (httpfs and parquet), and configure AWS credentials using environment variables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/duckdb-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: duckdb\n      path: 'file_path/database_name.duckdb'\n      extensions:\n        - httpfs\n        - parquet\n      settings:\n        s3_region: my-aws-region\n        s3_access_key_id: \"{{ env_var('S3_ACCESS_KEY_ID') }}\"\n        s3_secret_access_key: \"{{ env_var('S3_SECRET_ACCESS_KEY') }}\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Event Time for a Source (v1.9+)\nDESCRIPTION: Example showing how to configure the event_time field for a source, which is used to represent the actual timestamp of events rather than a loading date. Available in dbt Cloud Latest or dbt Core 1.9+.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/source-configs.md#2025-04-09_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  events:\n    clickstream:\n      +event_time: event_timestamp\n```\n\n----------------------------------------\n\nTITLE: Project-Level Test Storage Configuration in YAML\nDESCRIPTION: Project-wide configuration in dbt_project.yml showing how to set different storage options for different project subfolders.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/store_failures_as.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nname: \"my_project\"\nversion: \"1.0.0\"\nconfig-version: 2\nprofile: \"sandcastle\"\n\ntests:\n  my_project:\n    +store_failures_as: table\n    my_subfolder_1:\n      +store_failures_as: view\n    my_subfolder_2:\n      +store_failures_as: ephemeral\n```\n\n----------------------------------------\n\nTITLE: Staging Model for Coalesced Timestamp\nDESCRIPTION: SQL model that creates a coalesced timestamp column combining updated_at and created_at fields.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/updated_at.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nselect  * coalesce (updated_at, created_at) as updated_at_for_snapshot\nfrom {{ source('jaffle_shop', 'orders') }}\n```\n\n----------------------------------------\n\nTITLE: Refactoring Staging Order Model to Use Source in SQL\nDESCRIPTION: This SQL code refactors the staging order model to use the source() function, referencing the jaffle_shop source defined in the YAML file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/snowflake-qs.md#2025-04-09_snippet_16\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    id as order_id,\n    user_id as customer_id,\n    order_date,\n    status\n\nfrom {{ source('jaffle_shop', 'orders') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring View Security Mode in Model\nDESCRIPTION: Configuration for view materialization with 'invoker' security mode in a model file. This controls whether the view operates with the privileges of the view creator or the view invoker.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/trino-configs.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(\n    materialized = 'view',\n    view_security = 'invoker'\n  )\n}}\n```\n\n----------------------------------------\n\nTITLE: Running dbt with Relative Time Sample (Days)\nDESCRIPTION: Command to run a specific customer staging model with a 3-day sample size to reduce processing time and resource usage.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/sample-flag.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --select path/to/stg_customers --sample=\"3 days\"\n```\n\n----------------------------------------\n\nTITLE: DATEADD Function in Postgres SQL\nDESCRIPTION: Demonstrates the date addition syntax in Postgres, which doesn't have a built-in dateadd function. It uses from_date, interval, and datepart in its syntax.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/date-functions/sql-dateadd.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{{ from_date }} + (interval '{{ interval }} {{ datepart }}')\n```\n\n----------------------------------------\n\nTITLE: Logging Registered Adapter Version\nDESCRIPTION: This snippet shows an example log message indicating the version of a registered adapter being used by dbt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/core-versions.md#2025-04-09_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n[0m13:13:48.572182 [info ] [MainThread]: Registered adapter: snowflake=1.9.0\n```\n\n----------------------------------------\n\nTITLE: Configuring Table Materialization in SQL\nDESCRIPTION: Shows how to configure a SQL model to materialize as a table using a Jinja config block. The materialized argument is set to 'table' to create a permanent table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/materializations/materializations-guide-3-configuring-materializations.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        materialized='table'\n    )\n}}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Including Data Tests in DBT Build\nDESCRIPTION: Command to include all data tests in the DBT build process using the --resource-type flag.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/resource-type.md#2025-04-09_snippet_3\n\nLANGUAGE: text\nCODE:\n```\ndbt build --resource-type test\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Source in YAML\nDESCRIPTION: This snippet shows the YAML configuration options for integrating data from a Kafka source. It includes both required and optional parameters for source and job options.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/upsolver-configs.md#2025-04-09_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\ntopic: '<topic>'\nexclude_columns: ('<exclude_column>', ...)\ndeduplicate_with: \n  COLUMNS: ['col1', 'col2']\n  WINDOW: 'N HOURS'\nconsumer_properties: '<consumer_properties>'\nreader_shards: <integer>\nstore_raw_data: True/False\nstart_from: 'BEGINNING/NOW'\nend_at: '<timestamp>/NOW'\ncompute_cluster: '<compute_cluster>'\nrun_parallelism: <integer>\ncontent_type: 'AUTO/CSV/...'\ncompression: 'AUTO/GZIP/...'\ncolumn_transformations:\n  '<column>': '<expression>'\ncommit_interval: '<N MINUTE[S]/HOUR[S]/DAY[S]>'\nskip_validations: ('MISSING_TOPIC')\nskip_all_validations: True/False\ncomment: '<comment>'\n```\n\n----------------------------------------\n\nTITLE: Debugging Runtime Error: Invalid dbt_project.yml\nDESCRIPTION: Error message shown when the dbt_project.yml file contains invalid configuration keys that are not recognized by dbt. In this example, 'hello' is not a valid configuration key.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/debug-errors.md#2025-04-09_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nEncountered an error while reading the project:\n  ERROR: Runtime Error\n  at path []: Additional properties are not allowed ('hello' was unexpected)\n\nError encountered in /Users/alice/jaffle-shop-dbt/dbt_project.yml\nEncountered an error:\nRuntime Error\n  Could not run dbt\n```\n\n----------------------------------------\n\nTITLE: Implementing Microbatch Incremental Strategy in dbt\nDESCRIPTION: Example of using dbt's microbatch incremental strategy which automates the filtering logic. The configuration specifies event_time field, day-based batching, a 3-day lookback window, and a beginning date for processing.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/incremental-microbatch.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\n{{\\n    config(\\n        materialized='incremental',\\n        incremental_strategy='microbatch',\\n        event_time='event_occured_at',\\n        batch_size='day',\\n        lookback=3,\\n        begin='2020-01-01',\\n        full_refresh=false\\n    )\\n}}\\n\\nselect * from {{ ref('stg_events') }} -- this ref will be auto-filtered\\n\n```\n\n----------------------------------------\n\nTITLE: Disabling All Tests from a Package\nDESCRIPTION: Example showing how to disable all tests from a specific package using the dbt_project.yml file. This is useful for skipping tests from third-party packages.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/data-test-configs.md#2025-04-09_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\ntests:\n  package_name:\n    +enabled: false\n```\n\n----------------------------------------\n\nTITLE: Example Query Structure for LLM Training\nDESCRIPTION: Python dictionary structure showing how to format example queries for training the LLM on handling time-based queries and metrics aggregation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-05-02-semantic-layer-llm.md#2025-04-09_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"metrics\": \"revenue, costs, profit\",\n    \"dimensions\": \"department, salesperson, cost_center, metric_time, product__product_category\",\n    \"question\": \"Give me YTD revenue by department?\",\n    \"result\": Query.model_validate(\n        {\n            \"metrics\": [{\"name\": \"revenue\"}],\n            \"groupBy\": [{\"name\": \"department\"}],\n            \"where\": [\n                {\n                    \"sql\": \"{{ TimeDimension('metric_time', 'DAY') }} >= date_trunc('year', current_date)\"\n                }\n            ],\n        }\n    )\n    .model_dump_json()\n    .replace(\"{\", \"{{\")\n    .replace(\"}\", \"}}\"),\n}\n```\n\n----------------------------------------\n\nTITLE: Orders Snapshot with Timestamp (dbt 1.8)\nDESCRIPTION: SQL snapshot implementation using Jinja templating, configuring updated_at with complete snapshot configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/updated_at.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot orders_snapshot %}\n\n{{\n    config(\n      target_schema='snapshots',\n      unique_key='id',\n\n      strategy='timestamp',\n      updated_at='updated_at'\n    )\n}}\n\nselect * from {{ source('jaffle_shop', 'orders') }}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Dimension Tables in dbt_project.yml for Firebolt\nDESCRIPTION: Project-level configuration to define dimension tables in Firebolt. Dimension tables are simpler than fact tables and do not support aggregation indexes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/firebolt-configs.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  <resource-path>:\n    +materialized: table\n    +table_type: dimension\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Check Columns in DBT Snapshot YAML (v1.9+)\nDESCRIPTION: Example of configuring check columns in a DBT snapshot using YAML format. Shows how to specify individual columns to track for changes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/check_cols.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n- name: snapshot_name\n  relation: source('my_source', 'my_table')\n  config:\n    schema: string\n    unique_key: column_name_or_expression\n    strategy: check\n    check_cols:\n      - column_name\n```\n\n----------------------------------------\n\nTITLE: Configuring Profile in dbt_project.yml\nDESCRIPTION: Shows how to specify the profile name in the dbt_project.yml file. The profile name must match exactly with what's defined in profiles.yml.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/debug-errors.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nprofile: jaffle_shops # note the plural\n```\n\n----------------------------------------\n\nTITLE: Configuring Unsegmented Table Across All Nodes in Vertica dbt Model\nDESCRIPTION: This snippet demonstrates how to use the 'no_segmentation' config parameter to create an unsegmented Vertica table distributed across all nodes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/vertica-configs.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\n{{config(materialized='table',no_segmentation='true')}}\n\nselect * from public.product_dimension\n```\n\n----------------------------------------\n\nTITLE: Selective Cache Population in dbt CLI\nDESCRIPTION: Command to run dbt with cache population limited to selected models only, demonstrating how to optimize performance when working with a specific subset of models like Salesforce data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/cache.md#2025-04-09_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ndbt --cache-selected-only run --select salesforce\n```\n\n----------------------------------------\n\nTITLE: Configuring hard_deletes in SQL snapshot file\nDESCRIPTION: SQL-level configuration for hard_deletes along with other snapshot settings in a snapshot SQL file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/hard-deletes.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        unique_key='id',\n        strategy='timestamp',\n        updated_at='updated_at',\n        hard_deletes='ignore' | 'invalidate' | 'new_record'\n    )\n}}\n```\n\n----------------------------------------\n\nTITLE: Model Owner and Maturity Example\nDESCRIPTION: Example showing how to designate a model owner and specify model maturity using meta properties.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/meta.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: users\n    meta:\n      owner: \"@alice\"\n      model_maturity: in dev\n```\n\n----------------------------------------\n\nTITLE: Configuring Table Materialization Without Columnstore in dbt SQL Model\nDESCRIPTION: A SQL model that disables the default columnstore behavior by setting the as_columnstore configuration to false.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/mssql-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{{{\n    config(\n        as_columnstore=false\n        )\n}}}\n\nselect *\nfrom ...\n```\n\n----------------------------------------\n\nTITLE: Troubleshooting Node Selection with dbt List Command\nDESCRIPTION: Examples of using the dbt ls command to preview and debug node selection syntax.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/syntax.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndbt ls --select \"path/to/my/models\" # Lists all models in a specific directory.\ndbt ls --select \"source_status:fresher+\" # Shows sources updated since the last dbt source freshness run.\ndbt ls --select state:modified+ # Displays nodes modified in comparison to a previous state.\ndbt ls --select \"result:<status>+\" state:modified+ --state ./<dbt-artifact-path> # Lists nodes that match certain [result statuses](/reference/node-selection/syntax#the-result-status) and are modified.\n```\n\n----------------------------------------\n\nTITLE: Correct Full Refresh Command for Microbatch\nDESCRIPTION: Demonstrates the correct way to perform a full refresh on a microbatch model by specifying both event-time-start and event-time-end parameters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/incremental-microbatch.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --full-refresh --event-time-start \"2024-01-01\" --event-time-end \"2024-02-01\"\n```\n\n----------------------------------------\n\nTITLE: Schema Permission Grant SQL\nDESCRIPTION: SQL command to grant necessary schema permissions for test failure storage functionality.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/store_failures.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\ncreate schema if not exists dev_username_dbt_test__audit authorization username;\n```\n\n----------------------------------------\n\nTITLE: Microsoft Entra ID Password Authentication Profile\nDESCRIPTION: YAML configuration for dbt profiles.yml using Microsoft Entra ID username and password authentication method.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/fabric-setup.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: fabric\n      driver: 'ODBC Driver 18 for SQL Server'\n      server: hostname or IP of your server\n      port: 1433\n      database: exampledb\n      schema: schema_name\n      authentication: ActiveDirectoryPassword\n      user: bill.gates@microsoft.com\n      password: iheartopensource\n```\n\n----------------------------------------\n\nTITLE: Using Multiple Columns as unique_key in a Snapshot (dbt v1.9+)\nDESCRIPTION: Example of using multiple columns as a composite unique key for a snapshot in dbt v1.9+. This configuration allows for complex primary key relationships across multiple fields.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/unique_key.md#2025-04-09_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  - name: orders_snapshot\n    relation: source('jaffle_shop', 'orders')\n    config:\n      schema: snapshots\n      unique_key: \n        - order_id\n        - product_id\n      strategy: timestamp\n      updated_at: updated_at\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced Table Options in Seed Configuration for Teradata\nDESCRIPTION: Setting multiple complex table options including fallback protection, journaling, checksum, merge block ratio, and isolated loading for a Teradata seed table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/teradata-configs.md#2025-04-09_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  <project-name>:\n    table_option: \"NO FALLBACK, NO JOURNAL, CHECKSUM = ON,\n      NO MERGEBLOCKRATIO,\n      WITH CONCURRENT ISOLATED LOADING FOR ALL\"\n```\n\n----------------------------------------\n\nTITLE: Reusing Manifest Objects in dbtRunner\nDESCRIPTION: Advanced usage pattern showing how to parse a Manifest object once and reuse it in subsequent commands to avoid re-parsing project files, along with example introspection to validate model descriptions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/programmatic-invocations.md#2025-04-09_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dbt.cli.main import dbtRunner, dbtRunnerResult\nfrom dbt.contracts.graph.manifest import Manifest\n\n# use 'parse' command to load a Manifest\nres: dbtRunnerResult = dbtRunner().invoke([\"parse\"])\nmanifest: Manifest = res.result\n\n# introspect manifest\n# e.g. assert every public model has a description\nfor node in manifest.nodes.values():\n    if node.resource_type == \"model\" and node.access == \"public\":\n        assert node.description != \"\", f\"{node.name} is missing a description\"\n\n# reuse this manifest in subsequent commands to skip parsing\ndbt = dbtRunner(manifest=manifest)\ncli_args = [\"run\", \"--select\", \"tag:my_tag\"]\nres = dbt.invoke(cli_args)\n```\n\n----------------------------------------\n\nTITLE: Graph Context Variable Structure Example\nDESCRIPTION: Example JSON structure showing how the graph context variable organizes project nodes including models, sources, exposures, metrics, and groups.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/graph.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"nodes\": {\n    \"model.my_project.model_name\": {\n      \"unique_id\": \"model.my_project.model_name\",\n      \"config\": {\"materialized\": \"table\", \"sort\": \"id\"},\n      \"tags\": [\"abc\", \"123\"],\n      \"path\": \"models/path/to/model_name.sql\",\n      ...\n    },\n    ...\n  },\n  \"sources\": {\n    \"source.my_project.snowplow.event\": {\n      \"unique_id\": \"source.my_project.snowplow.event\",\n      \"database\": \"analytics\",\n      \"schema\": \"analytics\",\n      \"tags\": [\"abc\", \"123\"],\n      \"path\": \"models/path/to/schema.yml\",\n      ...\n    },\n    ...\n  },\n  \"exposures\": {\n    \"exposure.my_project.traffic_dashboard\": {\n      \"unique_id\": \"exposure.my_project.traffic_dashboard\",\n      \"type\": \"dashboard\",\n      \"maturity\": \"high\",\n      \"path\": \"models/path/to/schema.yml\",\n      ...\n    },\n    ...\n  },\n  \"metrics\": {\n    \"metric.my_project.count_all_events\": {\n      \"unique_id\": \"metric.my_project.count_all_events\",\n      \"type\": \"count\",\n      \"path\": \"models/path/to/schema.yml\",\n      ...\n    },\n    ...\n  },\n  \"groups\": {\n    \"group.my_project.finance\": {\n      \"unique_id\": \"group.my_project.finance\",\n      \"name\": \"finance\",\n      \"owner\": {\n        \"email\": \"finance@jaffleshop.com\"\n      }\n      ...\n    },\n    ...\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Custom Subdirectory Configuration for macro-paths\nDESCRIPTION: An example showing how to configure dbt to use a custom subdirectory named 'custom_macros' instead of the default 'macros' directory for storing macro files.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/macro-paths.md#2025-04-09_snippet_3\n\nLANGUAGE: yml\nCODE:\n```\nmacro-paths: [\"custom_macros\"]\n```\n\n----------------------------------------\n\nTITLE: Querying Granularities\nDESCRIPTION: GraphQL query to fetch queryable granularities for metric time dimensions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-graphql.md#2025-04-09_snippet_4\n\nLANGUAGE: graphql\nCODE:\n```\nqueryableGranularities(\n  environmentId: BigInt!\n  metrics: [MetricInput!]!\n): [TimeGranularity!]!\n```\n\n----------------------------------------\n\nTITLE: Dynamic SQL Generation using dbt-utils Package in dbt\nDESCRIPTION: Model using the dbt-utils package to dynamically generate SQL for summarizing payment method amounts. It demonstrates the use of Jinja loops and macros from external packages.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/using-jinja.md#2025-04-09_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\n{%- set payment_methods = dbt_utils.get_column_values(\n    table=ref('raw_payments'),\n    column='payment_method'\n) -%}\n\nselect\norder_id,\n{%- for payment_method in payment_methods %}\nsum(case when payment_method = '{{payment_method}}' then amount end) as {{payment_method}}_amount\n{%- if not loop.last %},{% endif -%}\n{% endfor %}\nfrom {{ ref('raw_payments') }}\ngroup by 1\n```\n\n----------------------------------------\n\nTITLE: Implementing LOWER Function in Customer Data Query\nDESCRIPTION: Example query showing how to use the LOWER function to standardize customer first and last names in a data model. Uses dbt's ref function to reference the customers table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-06-30-lower-sql-function.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect \n\tcustomer_id,\n\tlower(first_name) as first_name,\n\tlower(last_name) as last_name\nfrom {{ ref('customers') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Sources in properties.yml (v1.8 and earlier)\nDESCRIPTION: YAML configuration for sources in property files (models/properties.yml) for dbt version 1.8 and earlier. Shows how to define sources with config blocks that specify enabled status and metadata.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/source-configs.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: [<source-name>]\n    [config]:\n      [enabled]: true | false\n      [meta]: {<dictionary>}\n    tables:\n      - name: [<source-table-name>]\n        [config]:\n          [enabled]: true | false\n          [meta]: {<dictionary>}\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Docs for Snapshots in dbt_project.yml\nDESCRIPTION: Demonstrates how to configure documentation visibility and node color for snapshots in the dbt_project.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/docs.md#2025-04-09_snippet_4\n\nLANGUAGE: yml\nCODE:\n```\nsnapshots:\n  [<resource-path>]:\n    +docs:\n      show: true | false\n      node_color: color_id # Use name (such as node_color: purple) or hex code with quotes (such as node_color: \"#cd7f32\")\n\n```\n\n----------------------------------------\n\nTITLE: Configuring StarRocks Authentication in dbt profiles.yml\nDESCRIPTION: Example configuration for connecting to StarRocks using username/password authentication. The profile includes essential connection parameters like host, port, schema, and credentials.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/starrocks-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmy-starrocks-db:\n  target: dev\n  outputs:\n    dev:\n      type: starrocks\n      host: localhost\n      port: 9030\n      schema: analytics\n      \n      # User/password auth\n      username: your_starrocks_username\n      password: your_starrocks_password\n```\n\n----------------------------------------\n\nTITLE: Using Replace Cross-DB Macro in Teradata\nDESCRIPTION: Examples of using the replace cross-database macro with dbt in Teradata. The macro can be called using either the dbt namespace or directly, and it replaces occurrences of a substring with another substring.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/teradata-setup.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt.replace(\"string_text_column\", \"old_chars\", \"new_chars\") }}\n{{ replace('abcgef', 'g', 'd') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Complex Partitioning in Vertica dbt Model\nDESCRIPTION: This snippet demonstrates how to use multiple partitioning config parameters including 'partition_by_string', 'partition_by_group_by_string', and 'partition_by_active_count' for advanced partitioning scenarios.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/vertica-configs.md#2025-04-09_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\n{{ config( materialized='table', \npartition_by_string='employee_age',    \npartition_by_group_by_string=\"\"\"\n                                  CASE WHEN employee_age < 5 THEN 1\n                                  WHEN employee_age>50 THEN 2\n                                  ELSE 3 END\"\"\",\n\npartition_by_active_count = 2) }}\n\nselect * FROM public.employee_dimension\n```\n\n----------------------------------------\n\nTITLE: Error Message When Package Contains No Dispatch Macros in dbt\nDESCRIPTION: This error appears when a package listed in the search_order of a dispatch configuration doesn't contain any macros that could be dispatched. Despite the error message suggesting the package is missing, it actually means the package doesn't have any dispatchable macros.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Troubleshooting/dispatch-could-not-find-package.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nCompilation Error\n  In dispatch: Could not find package 'my_project'\n```\n\n----------------------------------------\n\nTITLE: Setting Up Databricks Secret Scope for dbt Cloud API\nDESCRIPTION: Commands to create a Databricks secret scope and add a dbt Cloud API key to it. This allows secure storage of API credentials for use in Databricks workflows.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/how-to-use-databricks-workflows-to-run-dbt-cloud-jobs.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# In this example we set up a secret scope and key called \"dbt-cloud\" and \"api-key\" respectively.\ndatabricks secrets create-scope --scope <YOUR_SECRET_SCOPE>\ndatabricks secrets put --scope  <YOUR_SECRET_SCOPE> --key  <YOUR_SECRET_KEY> --string-value \"<YOUR_DBT_CLOUD_API_KEY>\"\n```\n\n----------------------------------------\n\nTITLE: Using a Concatenated Expression as unique_key in a Snapshot (dbt v1.8)\nDESCRIPTION: Example of creating a composite unique key by concatenating two columns for a snapshot in dbt v1.8 and earlier. This approach uses string concatenation to create a single unique identifier from multiple fields.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/unique_key.md#2025-04-09_snippet_11\n\nLANGUAGE: jinja2\nCODE:\n```\n{% snapshot transaction_items_snapshot %}\n\n    {{\n        config(\n          unique_key=\"transaction_id||'-'||line_item_id\",\n          ...\n        )\n    }}\n\nselect\n    transaction_id||'-'||line_item_id as id,\n    *\nfrom {{ source('erp', 'transactions') }}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: DBT Jinja Macro with Log Function Example\nDESCRIPTION: Example SQL/Jinja macro demonstrating how to use the log function within a dbt macro to log information about macro execution with arguments.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/log.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{% macro some_macro(arg1, arg2) %}\n\n\t{{ log(\"Running some_macro: \" ~ arg1 ~ \", \" ~ arg2) }}\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Data with Jafgen\nDESCRIPTION: Command to generate synthetic Jaffle Shop data for a specified number of years using the jafgen package.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/duckdb-qs.md#2025-04-09_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\njafgen [number of years to generate] # e.g. jafgen 6\n```\n\n----------------------------------------\n\nTITLE: Translating MERGE to dbt Model (SQL)\nDESCRIPTION: This snippet shows how to translate a MERGE operation into a dbt model using CTEs and UNION.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/migrate-from-stored-procedures.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nWITH\n\nusing_clause AS (\n\n    SELECT\n        ride_id,\n        subtotal,\n        tip\n\n    FROM {{ ref('rides_to_load') }}\n\n),\n\nupdates AS (\n\n    SELECT\n        ride_id,\n        subtotal,\n        tip\n\n    FROM using_clause\n\n),\n\ninserts AS (\n\n    SELECT\n        ride_id,\n        subtotal,\n        NVL(tip, 0, tip)\n\n    FROM using_clause\n\n)\n\nSELECT *\n\nFROM updates\n\nUNION inserts\n```\n\n----------------------------------------\n\nTITLE: Configuring YAML Front Matter for Blog Post\nDESCRIPTION: This YAML snippet defines metadata for the blog post, including title, description, authors, tags, and other publishing information.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-11-22-move-spreadsheets-to-your-dwh.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: \"How to move data from spreadsheets into your data warehouse\"\ndescription: \"A thankless, humble, and inevitable task: getting spreadsheet data into your data warehouse. Let's look at some of the different options, and the pros and cons of each.\"\nslug: moving-spreadsheet-data\n\nauthors: [joel_labes]\n\ntags: [analytics craft]\nhide_table_of_contents: false\n\ndate: 2022-11-23\nis_featured: true\n---\n```\n\n----------------------------------------\n\nTITLE: Configuring Incremental Strategy in dbt_project.yml\nDESCRIPTION: Example of setting the incremental strategy for all models in the dbt_project.yml file. This configuration applies the 'insert_overwrite' strategy to all models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/incremental-strategy.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  +incremental_strategy: \"insert_overwrite\"\n```\n\n----------------------------------------\n\nTITLE: Querying Model Dependencies and Freshness with GraphQL\nDESCRIPTION: This GraphQL query retrieves metadata on the latest execution for a particular model and its dependencies. It investigates when each model, snapshot, source, or seed feeding into a given model was last executed or loaded, helping gauge the freshness of the data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/discovery-use-cases-and-examples.md#2025-04-09_snippet_8\n\nLANGUAGE: graphql\nCODE:\n```\nquery ($environmentId: BigInt!, $first: Int!) {\n  environment(id: $environmentId) {\n    applied {\n      models(\n        first: $first\n        filter: { uniqueIds: \"MODEL.PROJECT.MODEL_NAME\" }\n      ) {\n        edges {\n          node {\n            name\n            ancestors(types: [Model, Source, Seed, Snapshot]) {\n              ... on ModelAppliedStateNestedNode {\n                name\n                resourceType\n                materializedType\n                executionInfo {\n                  executeCompletedAt\n                }\n              }\n              ... on SourceAppliedStateNestedNode {\n                sourceName\n                name\n                resourceType\n                freshness {\n                  maxLoadedAt\n                }\n              }\n              ... on SnapshotAppliedStateNestedNode {\n                name\n                resourceType\n                executionInfo {\n                  executeCompletedAt\n                }\n              }\n              ... on SeedAppliedStateNestedNode {\n                name\n                resourceType\n                executionInfo {\n                  executeCompletedAt\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Stripe Sources in YAML\nDESCRIPTION: YAML configuration for defining sources from the stripe database, including the payment table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/sl-snowflake-qs.md#2025-04-09_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n - name: stripe\n   database: raw\n   schema: stripe\n   tables:\n     - name: payment\n```\n\n----------------------------------------\n\nTITLE: Configuring Enabled Saved Queries in dbt YAML\nDESCRIPTION: Example of enabling or disabling saved queries in the dbt_project.yml and semantic_models.yml files. This configuration can be applied to all saved queries or specific saved query definitions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/enabled.md#2025-04-09_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nsaved-queries:\n  [<resource-path>]:\n    +enabled: true | false\n```\n\nLANGUAGE: yaml\nCODE:\n```\nsaved_queries:\n  - name: [<saved_query_name>]\n    config:\n      enabled: true | false\n```\n\n----------------------------------------\n\nTITLE: Project-level invalidate_hard_deletes Configuration\nDESCRIPTION: Setting invalidate_hard_deletes at the project level in dbt_project.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/invalidate_hard_deletes.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  [<resource-path>]:\n    +strategy: timestamp\n    +invalidate_hard_deletes: true\n```\n\n----------------------------------------\n\nTITLE: Project-Level Exposure Configurations\nDESCRIPTION: Example of setting project-level configurations for exposures in the dbt_project.yml file, demonstrating how to enable/disable exposures at the project level.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/exposure-properties.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nname: 'project_name'\n\n# rest of dbt_project.yml\n\nexposures:\n  +enabled: true\n```\n\n----------------------------------------\n\nTITLE: Creating a Staging Customers Model in dbt\nDESCRIPTION: Creates a staging model for customer data that selects and transforms data from the raw customers table. This model will be referenced by other models in the dbt project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/teradata-qs.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nselect\n   id as customer_id,\n   first_name,\n   last_name\n\nfrom jaffle_shop.customers\n```\n\n----------------------------------------\n\nTITLE: Setting dbt Configuration via Environment Variables\nDESCRIPTION: Demonstrates how to set a dbt configuration option using an environment variable with the required DBT_ prefix before running a dbt command.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/environment-variable-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n$ export DBT_<THIS-CONFIG>=True\ndbt run\n```\n\n----------------------------------------\n\nTITLE: Configuring unique_key in Snapshot SQL File (dbt v1.8)\nDESCRIPTION: Example of configuring unique_key in the config block of a snapshot SQL file for dbt versions 1.8 and earlier. This approach embeds the configuration directly in the snapshot definition.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/unique_key.md#2025-04-09_snippet_4\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ config(\n  unique_key=\"column_name\"\n) }}\n```\n\n----------------------------------------\n\nTITLE: PII Configuration for Source Columns\nDESCRIPTION: Example demonstrating how to mark source columns as containing PII using meta properties.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/meta.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: salesforce\n    tables:\n      - name: account\n        meta:\n          contains_pii: true\n        columns:\n          - name: email\n            meta:\n              contains_pii: true\n```\n\n----------------------------------------\n\nTITLE: Deserializing YAML to Python Object using fromyaml in dbt\nDESCRIPTION: This snippet demonstrates how to use the fromyaml context method to convert a YAML string into a Python dictionary. It then shows how to access and modify the resulting data structure.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/fromyaml.md#2025-04-09_snippet_0\n\nLANGUAGE: jinja\nCODE:\n```\n{% set my_yml_str -%}\n\ndogs:\n - good\n - bad\n\n{%- endset %}\n\n{% set my_dict = fromyaml(my_yml_str) %}\n\n{% do log(my_dict['dogs'], info=true) %}\n-- [\"good\", \"bad\"]\n\n{% do my_dict['dogs'].pop() %}\n{% do log(my_dict['dogs'], info=true) %}\n-- [\"good\"]\n```\n\n----------------------------------------\n\nTITLE: Defining asset-paths in dbt_project.yml\nDESCRIPTION: Basic syntax for configuring asset-paths in the dbt_project.yml file. This setting accepts a list of directory paths that will be copied to the target directory during docs generation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/asset-paths.md#2025-04-09_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nasset-paths: [directorypath]\n```\n\n----------------------------------------\n\nTITLE: Practical Source Quoting Example in YAML\nDESCRIPTION: Shows a real-world example of configuring quoting for a jaffle_shop source with different settings for orders and customers tables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/quoting.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: jaffle_shop\n    database: raw\n    quoting:\n      database: true\n      schema: true\n      identifier: true\n\n    tables:\n      - name: orders\n      - name: customers\n        # This overrides the `jaffle_shop` quoting config\n        quoting:\n          identifier: false\n```\n\n----------------------------------------\n\nTITLE: Serializing Python Dictionary to JSON String using tojson\nDESCRIPTION: Demonstrates how to use the tojson context method to convert a Python dictionary into a JSON string. The example shows creating a dictionary, serializing it with tojson, and logging the result.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/tojson.md#2025-04-09_snippet_0\n\nLANGUAGE: jinja\nCODE:\n```\n{% set my_dict = {\"abc\": 123} %}\n{% set my_json_string = tojson(my_dict) %}\n\n{% do log(my_json_string) %}\n```\n\n----------------------------------------\n\nTITLE: Configuring batch_size in SQL model config block\nDESCRIPTION: Sets the batch_size configuration to 'day' for a user_sessions model directly in the SQL file's config block. This example also specifies that the model should be materialized as incremental.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/batch_size.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized='incremental',\n    batch_size='day'\n) }}\n```\n\n----------------------------------------\n\nTITLE: Displaying Error Message for Schema Version Mismatch in Python\nDESCRIPTION: This code snippet shows an error message that dbt displays when there's a mismatch in schema versions between the current dbt version and the state artifacts being used.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/core-upgrade/11-Older versions/15-upgrading-to-v1.1.md#2025-04-09_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nExpected a schema version of \"https://schemas.getdbt.com/dbt/manifest/v5.json\" in <state-path>/manifest.json, but found \"https://schemas.getdbt.com/dbt/manifest/v4.json\". Are you running with a different version of dbt?\n```\n\n----------------------------------------\n\nTITLE: Deserializing JSON to Python with fromjson Method in dbt\nDESCRIPTION: This example demonstrates how to use the fromjson context method to convert a JSON string into a Python dictionary and access its values. The method takes a JSON string as input and returns the corresponding Python object, allowing for manipulation of JSON data within dbt templates.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/fromjson.md#2025-04-09_snippet_0\n\nLANGUAGE: jinja\nCODE:\n```\n{% set my_json_str = '{\"abc\": 123}' %}\n{% set my_dict = fromjson(my_json_str) %}\n\n{% do log(my_dict['abc']) %}\n```\n\n----------------------------------------\n\nTITLE: Time-based Metric Grouping Query\nDESCRIPTION: Query to fetch multiple metrics grouped by time using the metric_time dimension.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-jdbc.md#2025-04-09_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nselect * from {{\n\tsemantic_layer.query(metrics=['food_order_amount','order_gross_profit'], \n\tgroup_by=['metric_time'])\n\t}}\n```\n\n----------------------------------------\n\nTITLE: Running dbt Build Command in Shell\nDESCRIPTION: Example of running the dbt build command in a terminal to compile, run, and test your dbt project with DuckDB.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/duckdb-qs.md#2025-04-09_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n/workspaces/test (main) $ dbt build\n```\n\n----------------------------------------\n\nTITLE: Model Stacking Example with ref Function\nDESCRIPTION: Demonstrates how models can be stacked on top of each other using the ref function to create dependencies between them. The example shows a raw model and a second model that references it.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/ref.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect *\nfrom public.raw_data\n```\n\nLANGUAGE: sql\nCODE:\n```\nselect *\nfrom {{ref('model_a')}}\n```\n\n----------------------------------------\n\nTITLE: Loading Payment Data from S3 into Snowflake\nDESCRIPTION: SQL command to copy payment data from a public S3 bucket into the previously created payment table. The command specifies CSV format with a header row to skip.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/snowflake-qs.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\ncopy into raw.stripe.payment (id, orderid, paymentmethod, status, amount, created)\nfrom 's3://dbt-tutorial-public/stripe_payments.csv'\nfile_format = (\n    type = 'CSV'\n    field_delimiter = ','\n    skip_header = 1\n    );\n```\n\n----------------------------------------\n\nTITLE: Configuring OAuth Authentication for Trino in dbt profiles.yml\nDESCRIPTION: This snippet demonstrates how to configure OAuth 2.0 authentication for a Trino connection in the dbt profiles.yml file. It includes the host, catalog, schema, and port details for a Starburst Galaxy connection.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/trino-setup.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nsandbox-galaxy:\n  target: oauth\n  outputs:\n    oauth:\n      type: trino\n      method: oauth\n      host: bunbundersders.trino.galaxy-dev.io\n      catalog: dbt_target\n      schema: dataders\n      port: 443\n```\n\n----------------------------------------\n\nTITLE: Project-Wide Schema Configuration\nDESCRIPTION: Example of applying schema configuration to all seeds in a dbt project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/seed-configs.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  +schema: seed_data\n```\n\n----------------------------------------\n\nTITLE: Using help flags with dbt environment commands\nDESCRIPTION: Examples of how to access help documentation for the dbt environment command and its subcommands using the --help or -h flags.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/dbt-environment.md#2025-04-09_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndbt environment [command] --help\n```\n\nLANGUAGE: shell\nCODE:\n```\ndbt environment [command] -h\n```\n\n----------------------------------------\n\nTITLE: Configuring Insecure Hive Connection in dbt Profiles\nDESCRIPTION: YAML configuration for connecting to a local Hive instance without authentication. This method is recommended only for testing purposes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/hive-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: hive\n      host: localhost\n      port: PORT # default value: 10000\n      schema: SCHEMA_NAME\n      \n```\n\n----------------------------------------\n\nTITLE: Exposures Health Criteria Table in Markdown\nDESCRIPTION: Markdown table defining the health state criteria for dbt exposures, including states for Healthy, Caution, and Degraded conditions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/data-health-signals.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| **Health state** | **Criteria**   |\n|-------------------|---------------|\n|  Healthy\t| All of the following must be true: <br /><br />- Underlying sources are fresh<br />- Underlying models built successfully<br />- Underlying models' tests passing<br /> |\n|  Caution\t| One of the following must be true: <br /><br />- At least one underlying source's freshness checks returned a warning<br />- At least one underlying model was skipped<br />- At least one underlying model's tests returned a warning<br /> |   \n|  Degraded\t| One of the following must be true: <br /><br />- At least one underlying source's freshness checks returned an error<br />- At least one underlying model did not build successfully<br />- At least one model's tests returned an error |\n```\n\n----------------------------------------\n\nTITLE: Configuring Partition by String in Vertica dbt Model\nDESCRIPTION: This example shows how to use the 'partition_by_string' config parameter to partition a Vertica table based on a specific column.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/vertica-configs.md#2025-04-09_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\n{{ config( materialized='table', partition_by_string='employee_age' )}} \n\nselect * FROM public.employee_dimension\n```\n\n----------------------------------------\n\nTITLE: Importing Tools Component for Excel Integration\nDESCRIPTION: Importing a reusable component that provides information about the Excel integration with the Semantic Layer. This component displays specific bullet points about limitations and includes an image of the query builder.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud-integrations/semantic-layer/excel.md#2025-04-09_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport Tools from '/snippets/_sl-excel-gsheets.md';\n\n<Tools \ntype=\"Microsoft Excel\"\nbullet_1=\"Results that take longer than one minute to load into Excel will fail. This limit only applies to the loading process, not the time it takes for the data platform to run the query.\"\nbullet_2=\"If you're using this extension, make sure you're signed into Microsoft with the same Excel profile you used to set up the Add-In. Log in with one profile at a time as using multiple  profiles at once might cause issues.\"\nbullet_3=\"Note that only standard granularities are currently available, custom time granularities aren't currently supported for this integration.\"\nqueryBuilder=\"/img/docs/dbt-cloud/semantic-layer/query-builder.png\"\n/>\n```\n\n----------------------------------------\n\nTITLE: Basic SQL Full Outer Join Syntax\nDESCRIPTION: Basic syntax template for creating a full outer join between two tables using a single join key.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/joins/sql-full-outer-join.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    <fields>\nfrom <table_1> as t1\nfull outer join <table_1> as t2\non t1.id = t2.id\n```\n\n----------------------------------------\n\nTITLE: Creating Virtual Environment for dbt Cloud CLI\nDESCRIPTION: Command to create a new Python virtual environment named 'dbt-cloud' for isolating the dbt Cloud CLI installation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/cloud-cli-installation.md#2025-04-09_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npython3 -m venv dbt-cloud\n```\n\n----------------------------------------\n\nTITLE: Accessing Metrics in dbt Graph\nDESCRIPTION: Example macro showing how to access metrics and generate metric-specific SQL using the graph context variable.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/graph.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{% macro get_metric_sql_for(metric_name) %}\n\n  {% set metrics = graph.metrics.values() %}\n  \n  {% set metric = (metrics | selectattr('name', 'equalto', metric_name) | list).pop() %}\n\n  {% set metric_sql = get_metric_timeseries_sql(\n      relation = metric['model'],\n      type = metric['type'],\n      expression = metric['sql'],\n      ...\n  ) %}\n\n  {{ return(metric_sql) }}\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Querying Sources by Schema in GraphQL\nDESCRIPTION: This GraphQL query finds all sources in a specific schema and returns their unique IDs and states (pass, error, fail). It queries the job with ID 123 and filters sources to the 'analytics' schema.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/schema-discovery-job-sources.mdx#2025-04-09_snippet_1\n\nLANGUAGE: graphql\nCODE:\n```\n{\n  job(id: 123) {\n    sources(schema: \"analytics\") {\n      uniqueId\n      state\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing dbt Project\nDESCRIPTION: Commands to create and navigate to a new dbt project named jaffle_shop.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/manual-install-qs.md#2025-04-09_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndbt init jaffle_shop\ncd jaffle_shop\n```\n\n----------------------------------------\n\nTITLE: Configuring SingleStore Target in profiles.yml\nDESCRIPTION: YAML configuration for setting up a SingleStore target in the profiles.yml file. It includes both required and optional parameters for connecting to a SingleStore database.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/singlestore-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsinglestore:\n  target: dev\n  outputs:\n    dev:\n      type: singlestore\n      host: [hostname]  # optional, default localhost\n      port: [port number]  # optional, default 3306\n      user: [user]  # optional, default root\n      password: [password]  # optional, default empty\n      database: [database name]  # required\n      schema: [prefix for tables that dbt will generate]  # required\n      threads: [1 or more]  # optional, default 1\n```\n\n----------------------------------------\n\nTITLE: Configuring Table Indexes in SQL Model\nDESCRIPTION: Demonstrates how to define custom indexes for a table model including hash and unique indexes on specific columns.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/postgres-configs.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized = 'table',\n    indexes=[\n      {'columns': ['column_a'], 'type': 'hash'},\n      {'columns': ['column_a', 'column_b'], 'unique': True},\n    ]\n)}}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Querying Snapshot Information in GraphQL\nDESCRIPTION: Example GraphQL query that retrieves information about all snapshots in a specific job, including unique IDs, names, execution times, and environment details. The query demonstrates how to access snapshot data using the job ID as an entry point.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/schema-discovery-job-snapshots.mdx#2025-04-09_snippet_0\n\nLANGUAGE: graphql\nCODE:\n```\n{\n  job(id: 123) {\n    snapshots {\n      uniqueId\n      name\n      executionTime\n      environmentId\n      executeStartedAt\n      executeCompletedAt\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Docs for Snapshots in schema.yml\nDESCRIPTION: Shows how to configure documentation visibility and node color for specific snapshots in a schema.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/docs.md#2025-04-09_snippet_5\n\nLANGUAGE: yml\nCODE:\n```\nversion: 2\n\nsnapshots:\n  - name: snapshot_name\n    docs:\n      show: true | false\n      node_color: color_id # Use name (such as node_color: purple) or hex code with quotes (such as node_color: \"#cd7f32\")\n```\n\n----------------------------------------\n\nTITLE: Configuring SQLFluff with dbtonic Linting Rules\nDESCRIPTION: An example configuration for SQLFluff that implements dbt-specific (dbtonic) linting rules. This configuration sets up rules for SQL formatting, indentation, capitalization policies, and more to align with dbt Labs' styling best practices.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/dbt-cloud-ide/lint-format.md#2025-04-09_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[sqlfluff]\ntemplater = dbt\n# This change (from jinja to dbt templater) will make linting slower\n# because linting will first compile dbt code into data warehouse code.\nrunaway_limit = 10\nmax_line_length = 80\nindent_unit = space\n\n[sqlfluff:indentation]\ntab_space_size = 4\n\n[sqlfluff:layout:type:comma]\nspacing_before = touch\nline_position = trailing\n\n[sqlfluff:rules:capitalisation.keywords] \ncapitalisation_policy = lower\n\n[sqlfluff:rules:aliasing.table]\naliasing = explicit\n\n[sqlfluff:rules:aliasing.column]\naliasing = explicit\n\n[sqlfluff:rules:aliasing.expression]\nallow_scalar = False\n\n[sqlfluff:rules:capitalisation.identifiers]\nextended_capitalisation_policy = lower\n\n[sqlfluff:rules:capitalisation.functions]\ncapitalisation_policy = lower\n\n[sqlfluff:rules:capitalisation.literals]\ncapitalisation_policy = lower\n\n[sqlfluff:rules:ambiguous.column_references]  # Number in group by\ngroup_by_and_order_by_style = implicit\n```\n\n----------------------------------------\n\nTITLE: Configuring Project-Specific Models\nDESCRIPTION: Demonstrates how to apply configuration to all models within a specific project using the project name.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/resource-path.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nname: jaffle_shop\n\nmodels:\n  jaffle_shop:\n    +enabled: false # this will apply to all models in your project, but not any installed packages\n```\n\n----------------------------------------\n\nTITLE: Multiple Column Check Example (v1.8)\nDESCRIPTION: Example showing how to configure multiple columns to check for changes using Jinja/SQL format.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/check_cols.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot orders_snapshot_check %}\n\n    {{\n        config(\n          strategy='check',\n          unique_key='id',\n          check_cols=['status', 'is_cancelled'],\n        )\n    }}\n\n    select * from {{ source('jaffle_shop', 'orders') }}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Defining Behavior Change Flags Table in Markdown\nDESCRIPTION: A markdown table listing the behavior change flags for dbt-databricks, including their introduction version and maturity status.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/databricks-changes.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Flag                          | `dbt-databricks`: Intro | `dbt-databricks`: Maturity |\n| ----------------------------- | ----------------------- | -------------------------- |\n| `use_info_schema_for_columns` | 1.9.0                   | TBD                        |\n| `use_user_folder_for_python`  | 1.9.0                   | TBD                        |\n```\n\n----------------------------------------\n\nTITLE: Configuring event_time for Seeds\nDESCRIPTION: Examples of setting event_time configuration for seed resources.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/event-time.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  [resource-path:]:\n    +event_time: my_time_field\n```\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  - name: seed_name\n    [config]:\n      event_time: my_time_field\n```\n\n----------------------------------------\n\nTITLE: Running dbt Build Command Excluding Specific Project Tags\nDESCRIPTION: A dbt CLI command that builds all models except those tagged with 'smaller_subset_project'. This allows selective execution of models after project consolidation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Project/consolidate-projects.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndbt build --exclude tag:smaller_subset_project\n```\n\n----------------------------------------\n\nTITLE: Error Message from Failed Query\nDESCRIPTION: The error message returned when attempting to access query results during parse phase.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/execute.md#2025-04-09_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nEncountered an error:\nCompilation Error in model order_payment_methods (models/order_payment_methods.sql)\n  'None' has no attribute 'table'\n```\n\n----------------------------------------\n\nTITLE: MetricFlow Basic Semantic Model Definition\nDESCRIPTION: YAML configuration defining semantic models for orders and customers, including measures and dimensions for order_total calculation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/about-metricflow.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_models:\n  - name: orders    # The name of the semantic model\n    description: |\n      A model containing order data. The grain of the table is the order id.\n    model: ref('orders') #The name of the dbt model and schema\n    defaults:\n      agg_time_dimension: metric_time\n    entities: # Entities, which usually correspond to keys in the table. \n      - name: order_id\n        type: primary\n      - name: customer\n        type: foreign\n        expr: customer_id\n    measures:   # Measures, which are the aggregations on the columns in the table.\n      - name: order_total\n        agg: sum\n    dimensions: # Dimensions are either categorical or time. They add additional context to metrics and the typical querying pattern is Metric by Dimension.\n      - name: metric_time\n        expr: cast(ordered_at as date)\n        type: time\n        type_params:\n          time_granularity: day\n  - name: customers    # The name of the second semantic model\n    description: >\n      Customer dimension table. The grain of the table is one row per\n        customer.\n    model: ref('customers') #The name of the dbt model and schema\n    defaults:\n      agg_time_dimension: first_ordered_at\n    entities: # Entities, which  usually correspond to keys in the table.\n      - name: customer \n        type: primary\n        expr: customer_id\n    dimensions: # Dimensions are either categorical or time. They add additional context to metrics and the typical querying pattern is Metric by Dimension.\n      - name: is_new_customer\n        type: categorical\n        expr: case when first_ordered_at is not null then true else false end\n      - name: first_ordered_at\n        type: time\n        type_params:\n          time_granularity: day\n```\n\n----------------------------------------\n\nTITLE: Defining Default Failure Limit for Generic Test Block in SQL\nDESCRIPTION: This snippet shows how to set a default failure limit for all instances of a generic (schema) test by configuring it inside the test block definition. It sets the limit to 500.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/limit.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{% test <testname>(model, column_name) %}\n\n{{ config(limit = 500) }}\n\nselect ...\n\n{% endtest %}\n```\n\n----------------------------------------\n\nTITLE: Declaring dbt Group Definition in YAML\nDESCRIPTION: Demonstrates how to define a group in a YAML file with name and owner properties including email, slack, and github identifiers.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/groups.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ngroups:\n  - name: finance\n    owner:\n      # 'name' or 'email' is required; additional properties allowed\n      email: finance@jaffleshop.com\n      slack: finance-data\n      github: finance-data-team\n```\n\n----------------------------------------\n\nTITLE: Configuring Updated_at in Jinja/SQL (dbt 1.8)\nDESCRIPTION: Jinja template configuration for setting updated_at parameter in a snapshot config block.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/updated_at.md#2025-04-09_snippet_1\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ config(\n  strategy=\"timestamp\",\n  updated_at=\"column_name\"\n) }}\n```\n\n----------------------------------------\n\nTITLE: Configuring event_time in dbt_project.yml for Models\nDESCRIPTION: Example of setting event_time configuration for models in the project configuration file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/event-time.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  [resource-path:]:\n    +event_time: my_time_field\n```\n\n----------------------------------------\n\nTITLE: Querying Parent Models and Sources in GraphQL\nDESCRIPTION: GraphQL query to fetch information about a model's parent models and sources, including run IDs, unique identifiers, execution time, and state information.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/schema-discovery-job-model.mdx#2025-04-09_snippet_0\n\nLANGUAGE: graphql\nCODE:\n```\n{\n  job(id: 123) {\n    model(uniqueId: \"model.jaffle_shop.dim_user\") {\n      parentsModels {\n        runId\n        uniqueId\n        executionTime\n      }\n      parentsSources {\n        runId\n        uniqueId\n        state\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Defensive Macro Checking in dbt Packages\nDESCRIPTION: A macro that defensively checks if a required function (date_spine) is available in the global dbt namespace before using it. This example demonstrates how to write robust code that will fail gracefully with a helpful error message if dependencies are missing.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_config-dbt-version-check.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{% macro a_few_days_in_september() %}\n\n    {% if not dbt.get('date_spine') %}\n      {{ exceptions.raise_compiler_error(\"Expected to find the dbt.date_spine macro, but it could not be found\") }}\n    {% endif %}\n\n    {{ date_spine(\"day\", \"cast('2020-01-01' as date)\", \"cast('2030-12-31' as date)\") }}\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Right String Extraction in SQL with dbt\nDESCRIPTION: Macro to extract N rightmost characters from a string value.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/cross-database-macros.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt.right(\"string_text_column\", \"length_column\") }}\n{{ dbt.right(\"string_text_column\", \"3\") }}\n```\n\n----------------------------------------\n\nTITLE: Accessing Granularities from Metrics\nDESCRIPTION: GraphQL query to access granularities through the metrics endpoint.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-graphql.md#2025-04-09_snippet_6\n\nLANGUAGE: graphql\nCODE:\n```\n{\n  metrics(environmentId: BigInt!) {\n    name\n    dimensions {\n      name\n      queryableGranularities\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Dremio Profile\nDESCRIPTION: YAML configuration for dbt profiles.yml file to set up connection to Dremio Cloud.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dremio-lakehouse.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ndremioSamples:\n  outputs:\n    cloud_dev:\n      dremio_space: dev\n      dremio_space_folder: no_schema\n      object_storage_path: dev\n      object_storage_source: $scratch\n      pat: <this_is_the_personal_access_token>\n      cloud_host: api.dremio.cloud\n      cloud_project_id: <id_of_project_you_belong_to>\n      threads: 1\n      type: dremio\n      use_ssl: true\n      user: <your_username>\n  target: dev\n```\n\n----------------------------------------\n\nTITLE: Configuring Secure Views in Snowflake\nDESCRIPTION: Configuration example for creating secure views in Snowflake through dbt. Shows how to set up secure views for sensitive data models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snowflake-configs.md#2025-04-09_snippet_18\n\nLANGUAGE: yaml\nCODE:\n```\nname: my_project\nversion: 1.0.0\n\nmodels:\n  my_project:\n    sensitive:\n      +materialized: view\n      +secure: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Dremio Software Profile with Personal Access Token in YAML\nDESCRIPTION: YAML configuration for connecting to Dremio Software in dbt using a personal access token for authentication. Includes settings for host, port, object storage, and SSL usage.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/dremio-setup.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n[project name]:\n  outputs:\n    dev:\n      pat: [personal access token]\n      port: [port]\n      software_host: [hostname or IP address]\n      object_storage_source: [name\n      object_storage_path: [path]\n      dremio_space: [name]\n      dremio_space_folder: [path]\n      threads: [integer >= 1]\n      type: dremio\n      use_ssl: [true|false]\n      user: [username]\n  target: dev\n```\n\n----------------------------------------\n\nTITLE: Configuring Seed-Specific Options in dbt Project File\nDESCRIPTION: Shows how to configure seed-specific options like quote_columns, column_types, and delimiter in the dbt_project.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/seed-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  [<resource-path>]:\n    [+][quote_columns]: true | false\n    [+][column_types]: {column_name: datatype}\n    [+][delimiter]: <string>\n```\n\n----------------------------------------\n\nTITLE: Running dbt Test with Indirect Selection CLI Flag\nDESCRIPTION: Example of using the --indirect-selection flag with the dbt test command to run tests that only refer to selected nodes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/indirect-selection.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndbt test --indirect-selection cautious\n```\n\n----------------------------------------\n\nTITLE: Running dbt Core with Prefect 2\nDESCRIPTION: This snippet illustrates how to run dbt Core commands using Prefect 2. It utilizes the trigger_dbt_cli_command task from the prefect-dbt library.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/deployment-tools.md#2025-04-09_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom prefect import flow\nfrom prefect_dbt.cli.commands import trigger_dbt_cli_command\n\n@flow\ndef run_dbt_core():\n    result = trigger_dbt_cli_command(\n        command=\"run\",\n        project_dir=\"/path/to/dbt/project\",\n    )\n    return result\n\nif __name__ == \"__main__\":\n    run_dbt_core()\n```\n\n----------------------------------------\n\nTITLE: Configuring event_time for Snapshots\nDESCRIPTION: Examples of setting event_time configuration for snapshot resources.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/event-time.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  [resource-path:]:\n    +event_time: my_time_field\n```\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  - name: snapshot_name\n    [config]:\n      event_time: my_time_field\n```\n\n----------------------------------------\n\nTITLE: Configuring Table Partitioning in BigQuery with dbt\nDESCRIPTION: Demonstrates how to configure table partitioning in BigQuery using dbt. The example shows partitioning by a timestamp field with daily granularity.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/bigquery-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized='table',\n    partition_by={\n      \"field\": \"created_at\",\n      \"data_type\": \"timestamp\",\n      \"granularity\": \"day\"\n    }\n)}}\n\nselect\n  user_id,\n  event_name,\n  created_at\n\nfrom {{ ref('events') }}\n```\n\n----------------------------------------\n\nTITLE: Extended Incremental Model Configuration\nDESCRIPTION: Advanced configuration for incremental model including schema management and refresh controls\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-07-12-change-data-capture.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n      materialized='incremental',\n\t  full_refresh=false,\n\t  schema='history',\n\t  on_schema_change='sync_all_columns'\n    )\n}}\n```\n\n----------------------------------------\n\nTITLE: Basic Orders Snapshot Example (dbt 1.9+)\nDESCRIPTION: YAML configuration for a complete orders snapshot example showing updated_at implementation with schema and unique key settings.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/updated_at.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  - name: orders_snapshot\n    relation: source('jaffle_shop', 'orders')\n    config:\n      schema: snapshots\n      unique_key: id\n      strategy: timestamp\n      updated_at: updated_at\n```\n\n----------------------------------------\n\nTITLE: Running Tests on a Single dbt Source\nDESCRIPTION: This command runs tests on a specific source (jaffle_shop) including all of its tables. It filters tests to only those associated with the named source.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Tests/testing-sources.md#2025-04-09_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$ dbt test --select source:jaffle_shop\n```\n\n----------------------------------------\n\nTITLE: Using DATE_TRUNC in Google BigQuery and Amazon Redshift\nDESCRIPTION: Shows the syntax for using the DATE_TRUNC function in Google BigQuery and Amazon Redshift, where the date/time field is the first argument.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/date-functions/sql-date-trunc.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\ndate_trunc(<date/time field>, <date part>)\n```\n\n----------------------------------------\n\nTITLE: Implementing unique_key in Incremental Materialization using config.get\nDESCRIPTION: This snippet demonstrates how to use the config.get function to retrieve the unique_key configuration in an incremental materialization. It shows how to handle the case when the configuration is provided by the user.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/config.md#2025-04-09_snippet_0\n\nLANGUAGE: jinja\nCODE:\n```\n{% materialization incremental, default -%}\n  {%- set unique_key = config.get('unique_key') -%}\n  ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Exports in semantic_model.yml (dbt 1.8 and lower)\nDESCRIPTION: Example of configuring a saved query with an export in the semantic_model.yml file for dbt version 1.8 and lower. It includes query parameters and export configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/saved-queries.md#2025-04-09_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nsaved_queries:\n  - name: order_metrics\n    description: Relevant order metrics\n    query_params:\n      metrics:\n        - orders\n        - large_order\n        - food_orders\n        - order_total\n      group_by:\n        - Entity('order_id')\n        - TimeDimension('metric_time', 'day')\n        - Dimension('customer__customer_name')\n        - ... # Additional group_by\n      where:\n        - \"{{TimeDimension('metric_time')}} > current_timestamp - interval '1 week'\"\n         - ... # Additional where clauses\n    exports:\n      - name: order_metrics\n        config:\n          export_as: table # Options available: table, view\n          schema: my_export_schema_name # Optional - defaults to deployment schema\n          alias: my_export_alias # Optional - defaults to Export name\n```\n\n----------------------------------------\n\nTITLE: Iceberg Table Configuration with ORC Format\nDESCRIPTION: Configuration example for creating an Iceberg table with ORC file format and bucketing partitioning strategy using dbt model properties.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/watsonx-presto-config.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(\n    materialized='table',\n    properties={\n      \"format\": \"'ORC'\", -- Specifies the file format\n      \"partitioning\": \"ARRAY['bucket(id, 2)']\", -- Defines the partitioning strategy\n    }\n  )\n}}\n```\n\n----------------------------------------\n\nTITLE: Running dbt Models\nDESCRIPTION: Executes the dbt run command to run the models in the dbt project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_onrunstart-onrunend-commands.md#2025-04-09_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndbt run\n```\n\n----------------------------------------\n\nTITLE: Querying with Where Filter as List Format (Recommended)\nDESCRIPTION: Example of querying the semantic layer using where filters in list format, which offers better performance through predicate pushdown. Each condition is specified as a separate string in the list.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-jdbc.md#2025-04-09_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nselect * from {{\nsemantic_layer.query(metrics=['food_order_amount', 'order_gross_profit'],\ngroup_by=[Dimension('metric_time').grain('month'),'customer__customer_type'],\nwhere=[\"{{ Dimension('metric_time').grain('month') }} >= '2017-03-09'\", \"{{ Dimension('customer__customer_type') }} in ('new')\", \"{{ Entity('order_id') }} = 10\"])\n}}\n```\n\n----------------------------------------\n\nTITLE: Defining Source Freshness in dbt YAML Configuration\nDESCRIPTION: YAML structure for configuring source freshness checks in dbt, including options for warn_after, error_after, loaded_at_field, and loaded_at_query.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/freshness.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: <source_name>\n    freshness:\n      warn_after:\n        count: <positive_integer>\n        period: minute | hour | day\n      error_after:\n        count: <positive_integer>\n        period: minute | hour | day\n      filter: <boolean_sql_expression>\n    loaded_at_field: <column_name_or_expression>\n    loaded_at_query: <sql_expression> # v1.10 or higher. Should not be used if loaded_at_field is defined\n\n    tables:\n      - name: <table_name>\n        freshness:\n          warn_after:\n            count: <positive_integer>\n            period: minute | hour | day\n          error_after:\n            count: <positive_integer>\n            period: minute | hour | day\n          filter: <boolean_sql_expression>\n        loaded_at_field: <column_name_or_expression>\n        loaded_at_query: <sql_expression> # v1.10 or higher. Should not be used if loaded_at_field is defined\n\n        ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Table Materialization in Doris/SelectDB (Config Block)\nDESCRIPTION: Creates a Doris table with specific configuration parameters using the config block approach. Defines duplicate keys, partitioning, distribution, and custom properties directly in the model file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/doris-configs.md#2025-04-09_snippet_3\n\nLANGUAGE: jinja\nCODE:\n```\n{{ config(\n    materialized = \"table\",\n    duplicate_key = [ \"<column-name>\", ... ],\n    partition_by = [ \"<column-name>\", ... ],\n    partition_type = \"<engine-type>\",\n    partition_by_init = [\"<pertition-init>\", ... ]\n    distributed_by = [ \"<column-name>\", ... ],\n    buckets = \"int\",\n    properties = {\"<key>\":\"<value>\",...}\n      ...\n    ]\n) }}\n```\n\n----------------------------------------\n\nTITLE: Appending Custom Comment in dbt\nDESCRIPTION: Configuration to append a custom dynamic comment to SQL queries using dictionary syntax with comment and append options.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/query-comment.md#2025-04-09_snippet_12\n\nLANGUAGE: yml\nCODE:\n```\nquery-comment:\n  comment: \"run by {{ target.user }} in dbt\"\n  append: True\n```\n\n----------------------------------------\n\nTITLE: Defining Sources in dbt Project\nDESCRIPTION: Creates a sources.yml file to define and document the raw data sources used in the project. This allows referencing source tables using the source() function and enables data lineage tracking and testing.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/teradata-qs.md#2025-04-09_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n   - name: jaffle_shop\n     description: This is a replica of the Postgres database used by the app\n     database: raw\n     schema: jaffle_shop\n     tables:\n         - name: customers\n           description: One record per customer.\n         - name: orders\n           description: One record per order. Includes canceled and deleted orders.\n```\n\n----------------------------------------\n\nTITLE: Creating Staging Customer Model\nDESCRIPTION: SQL model that transforms raw customer data into a staging table with selected columns and renamed fields.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/mesh-qs.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    id as customer_id,\n    first_name,\n    last_name\n\nfrom {{ source('jaffle_shop', 'customers') }}\n```\n\n----------------------------------------\n\nTITLE: Non-boolean Config Examples\nDESCRIPTION: Examples of using non-boolean configuration flags with specific values.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/command-line-options.md#2025-04-09_snippet_3\n\nLANGUAGE: text\nCODE:\n```\ndbt run --printer-width=80 \ndbt test --indirect-selection=eager\n```\n\n----------------------------------------\n\nTITLE: Running and Previewing the Fiscal Calendar Model\nDESCRIPTION: These bash commands demonstrate how to run or preview the fiscal calendar model in dbt. The first command builds the model, while the second previews it if developing locally.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/mf-time-spine.md#2025-04-09_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --select fiscal_calendar\ndbt show --select fiscal_calendar # Use this command to preview the model if developing locally\n```\n\n----------------------------------------\n\nTITLE: Configuring Individual Snapshot in dbt_project.yml\nDESCRIPTION: This snippet shows how to configure an individual snapshot using the full resource path in the dbt_project.yml file, including strategy and unique key.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/snapshot-configs.md#2025-04-09_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  jaffle_shop:\n    postgres_app:\n      orders_snapshot:\n        +unique_key: id\n        +strategy: timestamp\n        +updated_at: updated_at\n```\n\n----------------------------------------\n\nTITLE: Any Value Selection in SQL with dbt\nDESCRIPTION: Macro to return a non-deterministic value from a group.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/cross-database-macros.md#2025-04-09_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt.any_value(\"column_name\") }}\n```\n\n----------------------------------------\n\nTITLE: Creating Snowflake Infrastructure for dbt Project\nDESCRIPTION: SQL commands to create a warehouse, databases, and schemas in Snowflake to prepare for dbt development. This creates a warehouse named 'transforming', a 'raw' database for source data, an 'analytics' database for transformed data, and schemas for jaffle_shop and stripe data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/snowflake-qs.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\ncreate warehouse transforming;\ncreate database raw;\ncreate database analytics;\ncreate schema raw.jaffle_shop;\ncreate schema raw.stripe;\n```\n\n----------------------------------------\n\nTITLE: Upgrading pip, wheel, and setuptools for dbt Core Installation\nDESCRIPTION: This command upgrades pip, wheel, and setuptools to their latest versions. This ensures better dependency resolution and faster install times using precompiled wheels when available.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Core/install-pip-best-practices.md#2025-04-09_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython -m pip install --upgrade pip wheel setuptools\n```\n\n----------------------------------------\n\nTITLE: Custom Test Directory Configuration\nDESCRIPTION: Example showing how to configure a custom directory named 'custom_tests' for storing test files instead of the default 'tests' directory.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/test-paths.md#2025-04-09_snippet_3\n\nLANGUAGE: yml\nCODE:\n```\ntest-paths: [\"custom_tests\"]\n```\n\n----------------------------------------\n\nTITLE: Using ROW_NUMBER Function in SQL\nDESCRIPTION: This SQL query demonstrates how to use the ROW_NUMBER function to assign unique row numbers for each customer based on their order date. It partitions the data by customer_id and orders the rows within each partition by order_date.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/window-functions/sql-row-number.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    customer_id,\n    order_id,\n    order_date,\n    row_number() over (partition by customer_id order by order_date) as row_n\nfrom {{ ref('orders') }}\norder by 1\n```\n\n----------------------------------------\n\nTITLE: DBT Model Reference Example in SQL\nDESCRIPTION: Example showing how DBT models can reference other models from different schemas (analytics.analytics and analytics.dbt_dconnors), demonstrating potential mixing of production and development models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-01-09-defer-in-development.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nwith \n\nmodel_b as (\n select * from analytics.analytics.model_b\n),\n\nmodel_c as (\n select * from analytics.dbt_dconnors.model_b\n),\n\n...\n```\n\n----------------------------------------\n\nTITLE: Combined Models and Seeds Configuration\nDESCRIPTION: Configuration showing how to co-locate seeds and models in the same directory, possible because they use different file extensions (.csv and .sql).\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/seed-paths.md#2025-04-09_snippet_4\n\nLANGUAGE: yml\nCODE:\n```\nseed-paths: [\"models\"]\nmodel-paths: [\"models\"]\n```\n\n----------------------------------------\n\nTITLE: Using config.get with Default Values in Materializations\nDESCRIPTION: This snippet demonstrates various ways to use config.get in materializations, including handling cases with no default, using an alternate value, and specifying a default value when the configuration doesn't exist.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/config.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{% materialization incremental, default -%}\n  -- Example w/ no default. unique_key will be None if the user does not provide this configuration\n  {%- set unique_key = config.get('unique_key') -%}\n\n  -- Example w/ alternate value. Use alternative of 'id' if 'unique_key' config is provided, but it is None\n  {%- set unique_key = config.get('unique_key') or 'id' -%}\n\n  -- Example w/ default value. Default to 'id' if the 'unique_key' config does not exist\n  {%- set unique_key = config.get('unique_key', default='id') -%}\n  ...\n```\n\n----------------------------------------\n\nTITLE: dbt Custom Constraints for Masking Policy YAML\nDESCRIPTION: This YAML configuration demonstrates how to add a masking policy without tags. It defines a model with a custom constraint that applies a masking policy to the id column.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/constraints.md#2025-04-09_snippet_21\n\nLANGUAGE: yml\nCODE:\n```\nmodels:\n  - name: my_model\n    config:\n      contract:\n        enforced: true\n      materialized: table\n    columns:\n      - name: id\n        data_type: int\n        constraints:\n          - type: custom\n            expression: \"masking policy my_policy\"\n```\n\n----------------------------------------\n\nTITLE: Feature Import Component in Markdown\nDESCRIPTION: Markdown code that imports and renders a Features component with product and plan information for the dbt Semantic Layer\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-api-overview.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nimport Features from '/snippets/_sl-plan-info.md'\n\n<Features\nproduct=\"dbt Semantic Layer\"\nplan=\"dbt Cloud Team or Enterprise\"\n/>\n```\n\n----------------------------------------\n\nTITLE: Generated Yellowbrick SQL with Distribution and Clustering\nDESCRIPTION: The SQL output generated by dbt for a model using single-column distribution and clustering on multiple columns. Shows the full Yellowbrick-specific syntax for optimized table creation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/yellowbrick-configs.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\ncreate  table if not exists marts.fact_match as (\nselect\n    hash(concat_ws('||',\n        lower(trim(s.season_name)),\n        translate(left(m.match_ts,10), '-', ''),\n        lower(trim(h.\"name\")),\n        lower(trim(a.\"name\")))) as match_key\n    , hash(lower(trim(s.season_name))) as season_key\n    , cast(translate(left(m.match_ts,10), '-', '') as integer) as match_date_key\n    , hash(lower(trim(h.\"name\"))) as home_team_key\n    , hash(lower(trim(a.\"name\"))) as away_team_key\n    , m.htscore\n    , split_part(m.htscore, '-', 1)  as home_team_goals_half_time\n    , split_part(m.htscore , '-', 2)  as away_team_goals_half_time\n    , m.ftscore\n    , split_part(m.ftscore, '-', 1)  as home_team_goals_full_time\n    , split_part(m.ftscore, '-', 2)  as away_team_goals_full_time\nfrom\n    premdb.public.match m\n        inner join premdb.public.team h on (m.htid = h.htid)\n        inner join premdb.public.team a on (m.atid = a.atid)\n        inner join premdb.public.season s on (m.seasonid = s.seasonid)\n)\ndistribute on (match_key)\ncluster on (season_key, match_date_key, home_team_key, away_team_key);\n```\n\n----------------------------------------\n\nTITLE: Common Metadata Structure in dbt Artifacts (JSON)\nDESCRIPTION: This snippet outlines the common metadata structure found in all dbt artifacts. It includes information such as dbt version, schema version, generation timestamp, adapter type, environment variables, and invocation ID.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/artifacts/dbt-artifacts.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"metadata\": {\n    \"dbt_version\": \"string\",\n    \"dbt_schema_version\": \"string\",\n    \"generated_at\": \"string\",\n    \"adapter_type\": \"string\",\n    \"env\": {},\n    \"invocation_id\": \"string\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Building Complex Customer Model in dbt\nDESCRIPTION: This SQL snippet creates a complex customer model by combining the staging models for customers and orders. It includes customer details and order statistics.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/azure-synapse-analytics-qs.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nwith customers as (\n\n    select * from {{ ref('stg_customers') }}\n\n),\n\norders as (\n\n    select * from {{ ref('stg_orders') }}\n\n),\n\ncustomer_orders as (\n\n    select\n        customer_id,\n\n        min(order_date) as first_order_date,\n        max(order_date) as most_recent_order_date,\n        count(order_id) as number_of_orders\n\n    from orders\n\n    group by customer_id\n\n),\n\nfinal as (\n\n    select\n        customers.customer_id,\n        customers.first_name,\n        customers.last_name,\n        customer_orders.first_order_date,\n        customer_orders.most_recent_order_date,\n        coalesce(customer_orders.number_of_orders, 0) as number_of_orders\n\n    from customers\n\n    left join customer_orders on customers.customer_id = customer_orders.customer_id\n\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Configuring Exports in semantic_model.yml (dbt 1.9+)\nDESCRIPTION: Example of configuring a saved query with an export in the semantic_model.yml file for dbt version 1.9 and higher. It includes query parameters and export configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/saved-queries.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nsaved_queries:\n  - name: order_metrics\n    description: Relevant order metrics\n    config:\n      tags:\n        - order_metrics\n    query_params:\n      metrics:\n        - orders\n        - large_order\n        - food_orders\n        - order_total\n      group_by:\n        - Entity('order_id')\n        - TimeDimension('metric_time', 'day')\n        - Dimension('customer__customer_name')\n        - ... # Additional group_by\n      where:\n        - \"{{TimeDimension('metric_time')}} > current_timestamp - interval '1 week'\"\n         - ... # Additional where clauses\n    exports:\n      - name: order_metrics\n        config:\n          export_as: table # Options available: table, view\n          alias: my_export_alias # Optional - defaults to Export name\n          schema: my_export_schema_name # Optional - defaults to deployment schema           \n```\n\n----------------------------------------\n\nTITLE: Configuring Warning Options in dbt_project.yml (dbt 1.8+)\nDESCRIPTION: This YAML configuration demonstrates how to promote all warnings to errors except for specific warnings using the dbt_project.yml file in dbt version 1.8 and above.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/warnings.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nflags:\n  warn_error_options:\n    error: all # Previously called \"include\"\n    warn:      # Previously called \"exclude\"\n      - NoNodesForSelectionCriteria\n    silence:   # To silence or ignore warnings\n      - NoNodesForSelectionCriteria\n```\n\n----------------------------------------\n\nTITLE: Promoting Warnings to Errors in dbt 1.7 and Earlier (Command-line)\nDESCRIPTION: These commands show how to promote all warnings to errors, exclude specific warnings, or target only certain warnings as errors using the --warn-error-options flag in dbt versions 1.7 and earlier.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/warnings.md#2025-04-09_snippet_4\n\nLANGUAGE: text\nCODE:\n```\ndbt --warn-error-options '{\"include\": \"all\"}' run\n```\n\nLANGUAGE: text\nCODE:\n```\ndbt --warn-error-options '{\"include\": \"all\", \"exclude\": [\"NoNodesForSelectionCriteria\"]}' run\n```\n\nLANGUAGE: text\nCODE:\n```\ndbt --warn-error-options '{\"include\": [\"NoNodesForSelectionCriteria\"]}' run\n```\n\nLANGUAGE: text\nCODE:\n```\nDBT_WARN_ERROR_OPTIONS='{\"include\": [\"NoNodesForSelectionCriteria\"]}' dbt run\n```\n\n----------------------------------------\n\nTITLE: Implementing Cross-Database dateadd Macro in dbt\nDESCRIPTION: This snippet demonstrates how to create a cross-database compatible dateadd macro using dbt's dispatch functionality. It includes the main macro definition and adapter-specific implementations for BigQuery, Postgres, and Redshift.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-11-23-so-you-want-to-build-a-package.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{% macro dateadd(datepart, interval, from_date_or_timestamp) %}\n\n  {{ return(adapter.dispatch('dateadd', 'dbt_utils')(datepart, interval, from_date_or_timestamp)) }}\n\n{% endmacro %}\n\n{% macro default__dateadd(datepart, interval, from_date_or_timestamp) %}\n\n    dateadd(\n\n        {{ datepart }},\n\n        {{ interval }},\n\n        {{ from_date_or_timestamp }}\n\n        )\n\n{% endmacro %}\n\n{% macro bigquery__dateadd(datepart, interval, from_date_or_timestamp) %}\n\n        datetime_add(\n\n            cast( {{ from_date_or_timestamp }} as datetime),\n\n        interval {{ interval }} {{ datepart }}\n\n        )\n\n{% endmacro %}\n\n{% macro postgres__dateadd(datepart, interval, from_date_or_timestamp) %}\n\n    {{ from_date_or_timestamp }} + ((interval '1 {{ datepart }}') * ({{ interval }}))\n\n{% endmacro %}\n\n{# redshift should use default instead of postgres #}\n\n{% macro redshift__dateadd(datepart, interval, from_date_or_timestamp) %}\n\n    {{ return(dbt_utils.default__dateadd(datepart, interval, from_date_or_timestamp)) }}\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: SQL HAVING Practical Example\nDESCRIPTION: Shows how to use HAVING clause to filter customers with more than one order using the Jaffle Shop orders table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/clauses/sql-having.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    customer_id,\n    count(order_id) as num_orders\nfrom {{ ref('orders') }}\ngroup by 1\nhaving num_orders > 1\n```\n\n----------------------------------------\n\nTITLE: Implementing Hashed Surrogate Keys using dbt_utils.generate_surrogate_key in SQL\nDESCRIPTION: This SQL model demonstrates how to implement hashed surrogate keys in a daily user orders report using the dbt_utils.generate_surrogate_key macro. The model processes order data, aggregates it by report date and user ID, and then generates a unique surrogate key based on these columns to ensure uniqueness across the dataset.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-08-17-managing-surrogate-keys-in-dbt.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n# in models/reports/daily_user_orders.sql\nwith \n\norders as (\n\tselect * from {{ ref('fct_orders') }} \n),\n\nagg as (\n\n\tselect \n\t\tdate_trunc(day, order_date) as report_date \t\n\t\tuser_id, \n\t\tcount(*) as total_orders\n  from orders\n  group by 1,2\n\n),\n\nfinal as (\n\t\n\tselect\n\t\t{{ dbt_utils.generate_surrogate_key([\n\t\t\t\t'report_date', \n\t\t\t\t'user_id'\n\t\t\t])\n\t\t}} as unique_key, \n\t\t*\n\tfrom agg\n\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Checking Python Version on MacOS for dbt Core Compatibility\nDESCRIPTION: This command checks the installed Python version on MacOS. dbt Core requires Python 3.8 or higher for successful installation and operation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Core/install-pip-os-prereqs.md#2025-04-09_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython --version\n```\n\n----------------------------------------\n\nTITLE: Default Model Path Configuration\nDESCRIPTION: Shows the recommended way to configure model paths using relative paths in dbt_project.yml.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/model-paths.md#2025-04-09_snippet_1\n\nLANGUAGE: yml\nCODE:\n```\nmodel-paths: [\"models\"]\n```\n\n----------------------------------------\n\nTITLE: DBT Node Selection Defer Reference\nDESCRIPTION: Reference link to DBT deferral documentation for node selection.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_indirect-selection-definitions.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n/reference/node-selection/defer\n```\n\n----------------------------------------\n\nTITLE: Upgrading to dbt Core v1.8+ and Installing Prereleases\nDESCRIPTION: Commands to uninstall dbt-adapters, upgrade to dbt Core v1.8 or higher, install prereleases of dbt-core, dbt-common, and dbt-adapters, and verify the dbt version.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/pip-install.md#2025-04-09_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\npython -m pip uninstall -y dbt-adapters\npython -m pip install --upgrade --pre dbt-core dbt-common dbt-adapters\ndbt --version\n```\n\n----------------------------------------\n\nTITLE: Creating Users and Assigning Roles in Snowflake\nDESCRIPTION: SQL commands to create users for different purposes (data loading tools, analysts, BI tools) and assign them appropriate roles and default warehouses.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/database-permissions/snowflake-permissions.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\ncreate user stitch_user -- or fivetran_user\n    password = '_generate_this_'\n    default_warehouse = loading\n    default_role = loader; \n\ncreate user claire -- or amy, jeremy, etc.\n    password = '_generate_this_'\n    default_warehouse = transforming\n    default_role = transformer\n    must_change_password = true;\n\ncreate user dbt_cloud_user\n    password = '_generate_this_'\n    default_warehouse = transforming\n    default_role = transformer;\n\ncreate user looker_user -- or mode_user etc.\n    password = '_generate_this_'\n    default_warehouse = reporting\n    default_role = reporter;\n\n-- then grant these roles to each user\ngrant role loader to user stitch_user; -- or fivetran_user\ngrant role transformer to user dbt_cloud_user;\ngrant role transformer to user claire; -- or amy, jeremy\ngrant role reporter to user looker_user; -- or mode_user, periscope_user\n```\n\n----------------------------------------\n\nTITLE: Querying Multiple Metrics in dbt Semantic Layer\nDESCRIPTION: Examples showing how to query multiple metrics in the dbt Semantic Layer using comma-separated values without spaces. These commands demonstrate querying multiple metrics and dimensions in a single command.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metricflow-commands.md#2025-04-09_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\ndbt sl query --metrics accounts_active,users_active\n```\n\nLANGUAGE: bash\nCODE:\n```\ndbt sl query --metrics accounts_active,users_active --group-by metric_time__week,accounts__plan_tier\n```\n\nLANGUAGE: bash\nCODE:\n```\ndbt sl query --metrics accounts_active,users_active --group-by metric_time__week,accounts__plan_tier --where \"metric_time__week >= '2024-02-01' and accounts__plan_tier = 'coco'\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Partition Groups in Vertica dbt Model\nDESCRIPTION: This example illustrates how to use the 'partition_by_string' and 'partition_by_group_by_string' config parameters to create partition groups based on a CASE statement.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/vertica-configs.md#2025-04-09_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\n{{config(materialized='table',\npartition_by_string='number_of_children', \npartition_by_group_by_string=\"\"\"\n                                  CASE WHEN number_of_children <= 2 THEN 'small_family'\n                                  ELSE 'big_family' END\"\"\")}}\nselect * from public.customer_dimension\n```\n\n----------------------------------------\n\nTITLE: Installing dbt-spark with Session Connection Support\nDESCRIPTION: Command to install dbt-spark with support for session connections. This is used for connecting to a pySpark session.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/spark-setup.md#2025-04-09_snippet_1\n\nLANGUAGE: zsh\nCODE:\n```\n# session connections\n$ python -m pip install \"dbt-spark[session]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Incremental Predicates in YAML\nDESCRIPTION: Example of using 'incremental_predicates' in a YAML configuration file. This advanced configuration limits the scan of existing data to improve performance for large datasets.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/incremental-strategy.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: my_incremental_model\n    config:\n      materialized: incremental\n      unique_key: id\n      cluster_by: ['session_start']  \n      incremental_strategy: merge\n      incremental_predicates: [\"DBT_INTERNAL_DEST.session_start > dateadd(day, -7, current_date)\"]\n```\n\n----------------------------------------\n\nTITLE: Defining Snapshot in SQL (dbt 1.8 and earlier)\nDESCRIPTION: This snippet demonstrates how to define a snapshot using SQL and Jinja in dbt versions 1.8 and earlier. It shows the basic structure of a snapshot block.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snapshot_name.md#2025-04-09_snippet_1\n\nLANGUAGE: jinja2\nCODE:\n```\n{% snapshot snapshot_name %}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Implementing Data Masking in Dynamic Views with SQL\nDESCRIPTION: A SQL code snippet demonstrating how to implement data masking in a dbt model using dynamic views. The example shows how to mask email addresses based on user group membership using Databricks' is_account_group_member function and regex pattern matching.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/productionize-your-dbt-databricks-project.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nCASE\nWHEN is_account_group_member('auditors') THEN email\nELSE regexp_extract(email, '^.*@(.*)$', 1)\nEND\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Database for Seeds in dbt\nDESCRIPTION: This snippet demonstrates how to configure a custom database for seed files in your dbt_project.yml file. The example shows how to load a seed into a 'staging' database instead of the target database.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/database.md#2025-04-09_snippet_1\n\nLANGUAGE: yml\nCODE:\n```\nseeds:\n  your_project:\n    product_categories:\n      +database: staging\n```\n\n----------------------------------------\n\nTITLE: Configuring snapshot-paths with custom directory\nDESCRIPTION: Demonstrates how to configure snapshot-paths to use a custom directory named 'archives' instead of the default 'snapshots' directory. This allows for more flexibility in organizing project files.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/snapshot-paths.md#2025-04-09_snippet_3\n\nLANGUAGE: yml\nCODE:\n```\nsnapshot-paths: [\"archives\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring View Materialization in dbt for ClickHouse\nDESCRIPTION: This snippet shows how to configure a dbt model as a ClickHouse view using either the project YAML file or a config block in the model SQL file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/clickhouse-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  <resource-path>:\n    +materialized: view\n```\n\nLANGUAGE: jinja\nCODE:\n```\n{{ config(materialized = \"view\") }}\n```\n\n----------------------------------------\n\nTITLE: Querying Customer Data in SQL\nDESCRIPTION: A simple SQL query to select all columns from the default.jaffle_shop_customers table. This is used as an example to test the connection to the data warehouse.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/databricks-qs.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nselect * from default.jaffle_shop_customers\n```\n\n----------------------------------------\n\nTITLE: Including Repository Image in Model Description\nDESCRIPTION: This example demonstrates how to include an image from the repository in a model's description field using Markdown syntax.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/description.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: customers\n    description: \"!\\[dbt Logo](assets/dbt-logo.svg)\"\n\n    columns:\n      - name: customer_id\n        description: Primary key\n```\n\n----------------------------------------\n\nTITLE: Configuring Dispatch Search Order in YAML\nDESCRIPTION: Sets up a custom search order for dispatched macros in the dbt_project.yml file. This configuration tells dbt to look in the local project for macro implementations before falling back to the package.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/dispatch.md#2025-04-09_snippet_4\n\nLANGUAGE: yml\nCODE:\n```\ndispatch:\n  - macro_namespace: dbt_utils\n    search_order: ['my_project', 'dbt_utils']\n```\n\n----------------------------------------\n\nTITLE: Handling Empty Test Results with Case Statement\nDESCRIPTION: Example of using a case statement in fail_calc to handle scenarios where test queries return no rows, preventing None/null errors by returning 0 when no records exist.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/fail_calc.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nfail_calc: \"case when count(*) > 0 then sum(n_records) else 0 end\"\n```\n\n----------------------------------------\n\nTITLE: Error Message for Invalid Access Reference\nDESCRIPTION: Shows the error message displayed when attempting to reference a model outside of its supported access level. This occurs when a model tries to reference another model that has access restrictions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/access.md#2025-04-09_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ndbt run -s marketing_model\n...\ndbt.exceptions.DbtReferenceError: Parsing Error\n  Node model.jaffle_shop.marketing_model attempted to reference node model.jaffle_shop.finance_model, \n  which is not allowed because the referenced node is private to the finance group.\n```\n\n----------------------------------------\n\nTITLE: Python Script for Triggering dbt Cloud Jobs from Databricks\nDESCRIPTION: A Python notebook that uses the requests library to trigger a dbt Cloud job via API, monitors its execution status, and handles job completion or failure. The script includes widget parameters for job ID and securely retrieves API credentials from Databricks secrets.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/how-to-use-databricks-workflows-to-run-dbt-cloud-jobs.md#2025-04-09_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport enum\nimport os\nimport time\nimport json\nimport requests\nfrom getpass import getpass\n     \ndbutils.widgets.text(\"job_id\", \"Enter the Job ID\")\njob_id = dbutils.widgets.get(\"job_id\")\n\naccount_id = <YOUR_ACCOUNT_ID>\nbase_url =  \"<YOUR_BASE_URL>\"\napi_key =  dbutils.secrets.get(scope = \"<YOUR_SECRET_SCOPE>\", key = \"<YOUR_SECRET_KEY>\")\n\n# These are documented on the dbt Cloud API docs\nclass DbtJobRunStatus(enum.IntEnum):\n    QUEUED = 1\n    STARTING = 2\n    RUNNING = 3\n    SUCCESS = 10\n    ERROR = 20\n    CANCELLED = 30\n\ndef _trigger_job() -> int:\n    res = requests.post(\n        url=f\"https://{base_url}/api/v2/accounts/{account_id}/jobs/{job_id}/run/\",\n        headers={'Authorization': f\"Token {api_key}\"},\n        json={\n            # Optionally pass a description that can be viewed within the dbt Cloud API.\n            # See the API docs for additional parameters that can be passed in,\n            # including `schema_override` \n            'cause': f\"Triggered by Databricks Workflows.\",\n        }\n    )\n\n    try:\n        res.raise_for_status()\n    except:\n        print(f\"API token (last four): ...{api_key[-4:]}\")\n        raise\n\n    response_payload = res.json()\n    return response_payload['data']['id']\n\ndef _get_job_run_status(job_run_id):\n    res = requests.get(\n        url=f\"https://{base_url}/api/v2/accounts/{account_id}/runs/{job_run_id}/\",\n        headers={'Authorization': f\"Token {api_key}\"},\n    )\n\n    res.raise_for_status()\n    response_payload = res.json()\n    return response_payload['data']['status']\n\ndef run():\n    job_run_id = _trigger_job()\n    print(f\"job_run_id = {job_run_id}\")   \n    while True:\n        time.sleep(5)\n        status = _get_job_run_status(job_run_id)\n        print(DbtJobRunStatus(status))\n        if status == DbtJobRunStatus.SUCCESS:\n            break\n        elif status == DbtJobRunStatus.ERROR or status == DbtJobRunStatus.CANCELLED:\n            raise Exception(\"Failure!\")\n\nif __name__ == '__main__':\n    run()\n```\n\n----------------------------------------\n\nTITLE: Reverting to dbt Core\nDESCRIPTION: Commands to uninstall both dbt Cloud CLI and dbt Core, then reinstall dbt Core with specific adapter.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/cloud-cli-installation.md#2025-04-09_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npip uninstall dbt-core dbt\npip install dbt-adapter_name --force-reinstall\n```\n\n----------------------------------------\n\nTITLE: Quiet Mode Configuration in profiles.yml\nDESCRIPTION: YAML configuration for enabling quiet mode to suppress non-error logs\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/logs.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nconfig:\n  quiet: true\n```\n\n----------------------------------------\n\nTITLE: Sample output of dbt environment help command\nDESCRIPTION: Example output when running the help command for dbt environment, showing available subcommands and flags.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/dbt-environment.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n dbt help environment\nInteract with dbt environments\n\nUsage:\n  dbt environment [command]\n\nAliases:\n  environment, env\n\nAvailable Commands:\n  show        Show the working environment\n\nFlags:\n  -h, --help   help for environment\n\nUse \"dbt environment [command] --help\" for more information about a command.\n```\n\n----------------------------------------\n\nTITLE: Configuring Docs for Analyses in schema.yml\nDESCRIPTION: Shows how to configure documentation visibility and node color for specific analyses in a schema.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/docs.md#2025-04-09_snippet_6\n\nLANGUAGE: yml\nCODE:\n```\nversion: 2\n\nanalyses:\n  - name: analysis_name\n    docs:\n      show: true | false\n      node_color: color_id # Use name (such as node_color: purple) or hex code with quotes (such as node_color: \"#cd7f32\")\n```\n\n----------------------------------------\n\nTITLE: Analyzing Model Performance with Python and dbt Cloud Discovery API\nDESCRIPTION: This Python script queries the dbt Cloud Discovery API to analyze model performance. It fetches the latest run metadata for all models, identifies the longest-running model, and retrieves its historical run data. The script then creates visualizations of run elapsed time and execution time trends.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/discovery-use-cases-and-examples.md#2025-04-09_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Import libraries\nimport os\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport requests\n\n# Set API key\nauth_token = *[SERVICE_TOKEN_HERE]*\n\n# Query the API\ndef query_discovery_api(auth_token, gql_query, variables):\n    response = requests.post('https://metadata.cloud.getdbt.com/graphql',\n        headers={\"authorization\": \"Bearer \"+auth_token, \"content-type\": \"application/json\"},\n        json={\"query\": gql_query, \"variables\": variables})\n    data = response.json()['data']\n\n    return data\n\n# Get the latest run metadata for all models\nmodels_latest_metadata = query_discovery_api(auth_token, query_one, variables_query_one)['environment']\n\n# Convert to dataframe\nmodels_df = pd.DataFrame([x['node'] for x in models_latest_metadata['applied']['models']['edges']])\n\n# Unnest the executionInfo column\nmodels_df = pd.concat([models_df.drop(['executionInfo'], axis=1), models_df['executionInfo'].apply(pd.Series)], axis=1)\n\n# Sort the models by execution time\nmodels_df_sorted = models_df.sort_values('executionTime', ascending=False)\n\nprint(models_df_sorted)\n\n# Get the uniqueId of the longest running model\nlongest_running_model = models_df_sorted.iloc[0]['uniqueId']\n\n# Define second query variables\nvariables_query_two = {\n    \"environmentId\": *[ENVR_ID_HERE]*\n    \"lastRunCount\": 10,\n    \"uniqueId\": longest_running_model\n}\n\n# Get the historical run metadata for the longest running model\nmodel_historical_metadata = query_discovery_api(auth_token, query_two, variables_query_two)['environment']['applied']['modelHistoricalRuns']\n\n# Convert to dataframe\nmodel_df = pd.DataFrame(model_historical_metadata)\n\n# Filter dataframe to only successful runs\nmodel_df = model_df[model_df['status'] == 'success']\n\n# Convert the runGeneratedAt, executeStartedAt, and executeCompletedAt columns to datetime\nmodel_df['runGeneratedAt'] = pd.to_datetime(model_df['runGeneratedAt'])\nmodel_df['executeStartedAt'] = pd.to_datetime(model_df['executeStartedAt'])\nmodel_df['executeCompletedAt'] = pd.to_datetime(model_df['executeCompletedAt'])\n\n# Plot the runElapsedTime over time\nplt.plot(model_df['runGeneratedAt'], model_df['runElapsedTime'])\nplt.title('Run Elapsed Time')\nplt.show()\n\n# # Plot the executionTime over time\nplt.plot(model_df['executeStartedAt'], model_df['executionTime'])\nplt.title(model_df['name'].iloc[0]+\" Execution Time\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: dbt's CREATE TABLE AS Approach\nDESCRIPTION: This snippet illustrates how dbt creates tables using a CREATE TABLE AS statement, which inherits column types from the SELECT query. This approach eliminates the need for separate type definitions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Models/specifying-column-types.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\ncreate table dbt_alice.my_table as (\n  select id, created from some_other_table\n)\n```\n\n----------------------------------------\n\nTITLE: Querying Formula 1 Circuits Data in Snowflake\nDESCRIPTION: SQL query to retrieve all data from the circuits table in the Formula 1 database. This query returns information about Formula 1 race circuits including location, country, and coordinates.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nselect * from formula1.raw.circuits\n```\n\n----------------------------------------\n\nTITLE: Configuring Z-Order for Data Skipping in Incremental Models\nDESCRIPTION: Configuration snippet for setting up Z-ordering on specific columns in an incremental dbt model. Z-ordering helps improve query performance by co-locating related data together, especially useful for high-cardinality columns frequently used in join or where conditions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-models-on-databricks.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nconfig(\n\nmaterialized='incremental',\n\nzorder=\"column_A\" | [\"column_A\", \"column_B\"]\n\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Macro to Return Payment Methods\nDESCRIPTION: A Jinja macro that returns a list of payment methods, which can be reused across multiple models in the dbt project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/using-jinja.md#2025-04-09_snippet_6\n\nLANGUAGE: jinja\nCODE:\n```\n{% macro get_payment_methods() %}\n{{ return([\"bank_transfer\", \"credit_card\", \"gift_card\"]) }}\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Snapshot Path in dbt_project.yml\nDESCRIPTION: Configuration setting in dbt_project.yml that specifies the directory path where snapshot files are stored. By default, dbt looks for snapshots in the 'snapshots' subdirectory.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Accounts/configurable-snapshot-path.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshot-paths: [\"snapshots\"]\n```\n\n----------------------------------------\n\nTITLE: Forcing Dependencies with SQL Comments\nDESCRIPTION: Shows how to force dbt to recognize dependencies when they're not automatically detected, such as when a model only references a macro or when references appear within conditional blocks.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/ref.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n -- depends_on: {{ ref('upstream_parent_model') }}\n\n {{ your_macro('variable') }}\n```\n\nLANGUAGE: sql\nCODE:\n```\n-- depends_on: {{ source('raw', 'orders') }}\n\n{% if is_incremental() %}\nselect * from {{ source('raw', 'orders') }}\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Static Parser in DBT\nDESCRIPTION: Configuration example for enabling the static parser in DBT projects through profiles.yml. The static parser provides enhanced parsing capabilities.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/parsing.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nconfig:\n  static_parser: true\n```\n\n----------------------------------------\n\nTITLE: Disabling dbt Version Check in Bash\nDESCRIPTION: This command demonstrates how to run dbt with the version check disabled using the --no-version-check flag. It shows the output of running dbt version 1.0.0 with this flag, indicating the number of models, tests, archives, analyses, macros, and operations found.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/version-compatibility.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndbt --no-version-check run\nRunning with dbt=1.0.0\nFound 13 models, 2 tests, 1 archives, 0 analyses, 204 macros, 2 operations....\n```\n\n----------------------------------------\n\nTITLE: Configuring warn-error-options in dbt_project.yml\nDESCRIPTION: Example of silencing specific warnings in dbt_project.yml file using the warn_error_options configuration with the silence parameter.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/warnings.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\n  flags:\n    warn_error_options:\n      error: # Previously called \"include\"\n      warn: # Previously called \"exclude\"\n      silence: # To silence or ignore warnings\n        - NoNodesForSelectionCriteria\n```\n\n----------------------------------------\n\nTITLE: Applying Configurations to Project-Specific Snapshots\nDESCRIPTION: This snippet shows how to apply a configuration to all snapshots in a specific project, excluding snapshots in installed packages, using the dbt_project.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/snapshot-configs.md#2025-04-09_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  jaffle_shop:\n    +unique_key: id\n```\n\n----------------------------------------\n\nTITLE: Setting Query Band in profiles.yml for dbt-teradata\nDESCRIPTION: Demonstrates how to set a query band at the profiles level in the profiles.yml file for dbt-teradata.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/teradata-configs.md#2025-04-09_snippet_20\n\nLANGUAGE: yaml\nCODE:\n```\nquery_band: 'application=dbt;'\n```\n\n----------------------------------------\n\nTITLE: Generating Sequences Macro\nDESCRIPTION: A dbt macro that creates or replaces sequences for models that require surrogate keys based on their meta configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-08-17-managing-surrogate-keys-in-dbt.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n{% macro generate_sequences() %}\n\n    {% if execute %}\n      \n    {% set models = graph.nodes.values() | selectattr('resource_type', 'eq', 'model') %}\n    {% set sk_models = [] %}\n    {% for model in models %}\n        {% if model.config.meta.surrogate_key %}\n          {% do sk_models.append(model) %}\n        {% endif %}\n    {% endfor %}\n\n    {% endif %}\n\n    {% for model in sk_models %}\n\n        {% if flags.FULL_REFRESH or model.config.materialized == 'table' %}\n        create or replace sequence {{ model.database }}.{{ model.schema }}.{{ model.name }}_seq;\n\n        {% else %}\n        create sequence if not exists {{ model.database }}.{{ model.schema }}.{{ model.name }}_seq;\n        \n        {% endif %}\n    \n    {% endfor %}\n  \n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Distribution Key in Greenplum dbt Model\nDESCRIPTION: Sets up data distribution by specifying a distribution key field for improved join performance. This configuration determines how data is distributed across segments in Greenplum.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/greenplum-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        ...\n        distributed_by='<field_name>'\n        ...\n    )\n}}\n\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Using dbt run-operation Command in Bash\nDESCRIPTION: Demonstrates the usage syntax of the dbt run-operation command. It shows how to specify a macro to invoke and provide arguments as a YAML string.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/run-operation.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ dbt run-operation {macro} --args '{args}'\n  {macro}        Specify the macro to invoke. dbt will call this macro\n                        with the supplied arguments and then exit\n  --args ARGS           Supply arguments to the macro. This dictionary will be\n                        mapped to the keyword arguments defined in the\n                        selected macro. This argument should be a YAML string,\n                        eg. '{my_variable: my_value}'\n```\n\n----------------------------------------\n\nTITLE: Configuring Microsoft Entra ID Integrated Authentication in profiles.yml for Fabric\nDESCRIPTION: Configuration for Microsoft Fabric connection using Microsoft Entra ID integrated authentication. This method uses the credentials from the current Windows machine login without requiring explicit username/password.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/fabric-setup.md#2025-04-09_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: fabric\n      driver: 'ODBC Driver 18 for SQL Server' # (The ODBC Driver installed on your system)\n      server: hostname or IP of your server\n      port: 1433\n      database: exampledb\n      schema: schema_name\n      authentication: ActiveDirectoryIntegrated\n```\n\n----------------------------------------\n\nTITLE: Granting Raw Database Permissions to Loader Role in Snowflake\nDESCRIPTION: SQL command to grant the loader role full permissions on the raw database for data ingestion purposes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/database-permissions/snowflake-permissions.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nuse role sysadmin;\ngrant all on database raw to role loader;\n```\n\n----------------------------------------\n\nTITLE: Querying Sample Data in BigQuery\nDESCRIPTION: SQL queries to select all data from sample tables in the dbt-tutorial dataset. These queries are used to verify access to the sample data in BigQuery.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/bigquery-qs.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect * from `dbt-tutorial.jaffle_shop.customers`;\nselect * from `dbt-tutorial.jaffle_shop.orders`;\nselect * from `dbt-tutorial.stripe.payment`;\n```\n\n----------------------------------------\n\nTITLE: Configuring valid_history Incremental Strategy for dbt-teradata\nDESCRIPTION: Demonstrates the configuration for the valid_history incremental materialization strategy, including required parameters for managing historical data efficiently.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/teradata-configs.md#2025-04-09_snippet_23\n\nLANGUAGE: yaml\nCODE:\n```\n  {{\n      config(\n          materialized='incremental',\n          unique_key='id',\n          on_schema_change='fail',\n          incremental_strategy='valid_history',\n          valid_period='valid_period_col',\n          use_valid_to_time='no',\n  )\n  }}\n```\n\n----------------------------------------\n\nTITLE: Initial Data Query in dbt Cloud\nDESCRIPTION: Simple SELECT query to test the connection to the Redshift jaffle_shop database\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/redshift-qs.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nselect * from jaffle_shop.customers\n```\n\n----------------------------------------\n\nTITLE: General Configurations in SQL Config Block\nDESCRIPTION: Example of using the config() macro in a SQL test file to set general configurations like enabled status, tags, meta information, and database/schema/alias properties.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/data-test-configs.md#2025-04-09_snippet_4\n\nLANGUAGE: jinja\nCODE:\n```\n{{ config(\n    [enabled]=true | false,\n    [tags]=\"<string>\" | [\"<string>\"]\n    [meta]={dictionary},\n    [database]=\"<string>\",\n    [schema]=\"<string>\",\n    [alias]=\"<string>\",\n) }}\n\n```\n\n----------------------------------------\n\nTITLE: Defining a dbt Source in a Different Database using the database property\nDESCRIPTION: YAML configuration that shows how to define a source located in a different database than your target database. The example uses the database property to specify that the jaffle_shop source is in the 'raw' database and includes the tables 'orders' and 'customers'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Project/source-in-different-database.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: jaffle_shop\n    database: raw\n    tables:\n      - name: orders\n      - name: customers\n\n```\n\n----------------------------------------\n\nTITLE: General Configurations in Project File\nDESCRIPTION: Example of general test configurations in dbt_project.yml. Shows how to set enabled status, tags, meta information, and database/schema/alias for test failure storage.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/data-test-configs.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ntests:\n  [<resource-path>]:\n    [+][enabled]: true | false\n    [+][tags]: <string> | [<string>]\n    [+][meta]: {dictionary}\n    # relevant for [store_failures] only\n    [+][database]: <string>\n    [+][schema]: <string>\n    [+][alias]: <string>\n```\n\n----------------------------------------\n\nTITLE: Printing Model Contents in dbt Cloud IDE\nDESCRIPTION: Demonstrates how to print the full contents of a model object in the dbt Cloud IDE using tojson filter.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/model.md#2025-04-09_snippet_2\n\nLANGUAGE: jinja\nCODE:\n```\n{{ model | tojson(indent = 4) }}\n```\n\n----------------------------------------\n\nTITLE: Requiring a Specific dbt Version\nDESCRIPTION: Example of pinning to an exact dbt version. This is not recommended for dbt Core v1.0.0 and higher as it limits flexibility and can cause compatibility issues.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/require-dbt-version.md#2025-04-09_snippet_5\n\nLANGUAGE: yml\nCODE:\n```\nrequire-dbt-version: \"1.5.0\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Incremental Model with Append Strategy in dbt\nDESCRIPTION: This snippet demonstrates how to configure an incremental model using the 'append' strategy in dbt. It includes a filter to only append new records based on the order date.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/firebolt-configs.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n   materialized = 'incremental',\n   incremental_strategy='append'\n) }}\n\n/* All rows returned by this query will be appended to the existing model */\n\n\nselect * from {{ ref('raw_orders') }}\n{% if is_incremental() %}\n   where order_date > (select max(order_date) from {{ this }})\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Creating a Staging Orders Model in SQL for dbt\nDESCRIPTION: This SQL query creates a staging model for orders, selecting and renaming columns from the orders table in the jaffle_shop database.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/bigquery-qs.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    id as order_id,\n    user_id as customer_id,\n    order_date,\n    status\n\nfrom `dbt-tutorial`.jaffle_shop.orders\n```\n\n----------------------------------------\n\nTITLE: Config Macro Usage (SQL)\nDESCRIPTION: Example demonstrating how to set configurations directly in SQL files using the config() macro\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/configs-and-properties.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized='table',\n    schema='my_schema'\n) }}\n\nSELECT * FROM my_table\n```\n\n----------------------------------------\n\nTITLE: Implementing merge_update_columns in Vertica Incremental Models\nDESCRIPTION: Demonstrates how to configure a Vertica incremental model with the merge strategy that updates only specific columns (names and salary) when records match on the unique_key (id).\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/vertica-configs.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n{{ config( materialized = 'incremental', incremental_strategy='merge', unique_key = 'id', merge_update_columns = [\"names\", \"salary\"] )}}\n    \n        select * from {{ref('seed_tc1')}}\n```\n\nLANGUAGE: sql\nCODE:\n```\n        merge into \"VMart\".\"public\".\"test_merge\" as DBT_INTERNAL_DEST using \"test_merge__dbt_tmp\" as DBT_INTERNAL_SOURCE on  DBT_INTERNAL_DEST.\"id\" = DBT_INTERNAL_SOURCE.\"id\"\n        \n        when matched then update set\n          \"names\" = DBT_INTERNAL_SOURCE.\"names\", \"salary\" = DBT_INTERNAL_SOURCE.\"salary\"\n        \n        when not matched then insert\n        (\"id\", \"names\", \"salary\")\n        values\n        (\n          DBT_INTERNAL_SOURCE.\"id\", DBT_INTERNAL_SOURCE.\"names\", DBT_INTERNAL_SOURCE.\"salary\"\n        )\n```\n\n----------------------------------------\n\nTITLE: Setting Default Generic Test Configuration\nDESCRIPTION: Example of setting a default fail_calc configuration within a generic test block definition.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/fail_calc.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{% test <testname>(model, column_name) %}\n\n{{ config(fail_calc = \"missing_in_a + missing_in_b\") }}\n\nselect ...\n\n{% endtest %}\n```\n\n----------------------------------------\n\nTITLE: Creating Formula 1 Sources YAML Definition\nDESCRIPTION: This YAML file defines the Formula 1 data sources including tables for circuits, constructors, drivers, lap times, pit stops, races, results, and status. It includes table descriptions, primary key tests for uniqueness and not-null constraints, and accepted value tests for certain fields.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: formula1\n    description: formula 1 datasets with normalized tables \n    database: formula1 \n    schema: raw\n    tables:\n      - name: circuits\n        description: One record per circuit, which is the specific race course. \n        columns:\n          - name: circuitid\n            tests:\n            - unique\n            - not_null\n      - name: constructors \n        description: One record per constructor. Constructors are the teams that build their formula 1 cars. \n        columns:\n          - name: constructorid\n            tests:\n            - unique\n            - not_null\n      - name: drivers\n        description: One record per driver. This table gives details about the driver. \n        columns:\n          - name: driverid\n            tests:\n            - unique\n            - not_null\n      - name: lap_times\n        description: One row per lap in each race. Lap times started being recorded in this dataset in 1984 and joined through driver_id.\n      - name: pit_stops \n        description: One row per pit stop. Pit stops do not have their own id column, the combination of the race_id and driver_id identify the pit stop.\n        columns:\n          - name: stop\n            tests:\n              - accepted_values:\n                  values: [1,2,3,4,5,6,7,8]\n                  quote: false            \n      - name: races \n        description: One race per row. Importantly this table contains the race year to understand trends. \n        columns:\n          - name: raceid\n            tests:\n            - unique\n            - not_null        \n      - name: results\n        columns:\n          - name: resultid\n            tests:\n            - unique\n            - not_null   \n        description: One row per result. The main table that we join out for grid and position variables.\n      - name: status\n        description: One status per row. The status contextualizes whether the race was finished or what issues arose e.g. collisions, engine, etc. \n        columns:\n          - name: statusid\n            tests:\n            - unique\n            - not_null\n```\n\n----------------------------------------\n\nTITLE: Debug Format Log Output Example\nDESCRIPTION: Example of debug format logging output with detailed timestamps and additional metadata\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/logs.md#2025-04-09_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n============================== 16:12:08.555032 | 9089bafa-4010-4f38-9b42-564ec9106e07 ==============================\n16:12:08.555032 [info ] [MainThread]: Running with dbt=1.8.0\n16:12:08.751069 [info ] [MainThread]: Registered adapter: postgres=1.8.0\n```\n\n----------------------------------------\n\nTITLE: Configuring Versioned Model Materializations in YAML\nDESCRIPTION: Demonstrates how to configure different materializations for different versions of the same model in dbt. Version 2 is materialized as a table while version 1 is materialized as a view.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/govern/model-versions.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nversions:\n  - v: 2\n    config:\n      materialized: table\n  - v: 1\n    config:\n      materialized: view\n```\n\n----------------------------------------\n\nTITLE: Displaying Congratulatory Message with ConfettiTrigger in Markdown\nDESCRIPTION: This snippet uses a custom ConfettiTrigger component to wrap the concluding content of the guide, likely triggering a visual confetti effect when rendered.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/mesh-qs.md#2025-04-09_snippet_14\n\nLANGUAGE: markdown\nCODE:\n```\n<ConfettiTrigger>\n\nCongratulations ! You're ready to bring the benefits of dbt Mesh to your organization. You've learned:\n\n- How to establish a foundational project \"Jaffle | Data Analytics.\"\n- Create a downstream project \"Jaffle | Finance.\"\n- Implement model access, versions, and contracts.\n- Set up dbt Cloud jobs triggered by upstream job completions.\n\nHere are some additional resources to help you continue your journey:\n\n- [How we build our dbt mesh projects](https://docs.getdbt.com/best-practices/how-we-mesh/mesh-1-intro)\n- [dbt Mesh FAQs](https://docs.getdbt.com/best-practices/how-we-mesh/mesh-5-faqs)\n- [Implement dbt Mesh with the Semantic Layer](/docs/use-dbt-semantic-layer/sl-faqs#how-can-i-implement-dbt-mesh-with-the-dbt-semantic-layer)\n- [Cross-project references](/docs/collaborate/govern/project-dependencies#how-to-write-cross-project-ref)\n- [dbt Explorer](/docs/collaborate/explore-projects)\n\n</ConfettiTrigger>\n```\n\n----------------------------------------\n\nTITLE: Enabling Statistics in SQL Model Configuration for Teradata\nDESCRIPTION: Configuring a table to copy statistics from the base table in Teradata, which can improve query performance.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/teradata-configs.md#2025-04-09_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\n{{\n  config(\n      materialized=\"table\",\n      with_statistics=\"true\"\n  )\n}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Groups in dbt Project\nDESCRIPTION: Demonstrates how to assign groups to model directories in dbt_project.yml file. Shows configuration for organizing models under customer_success and finance groups.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/govern/model-access.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  my_project_name:\n    marts:\n      customers:\n        +group: customer_success\n      finance:\n        +group: finance\n```\n\n----------------------------------------\n\nTITLE: Controlling Whitespace with Jinja Syntax\nDESCRIPTION: This snippet demonstrates Jinja syntax for whitespace control. Use minus signs at the start or end of Jinja blocks to strip whitespace before or after the block, helping to produce cleaner compiled SQL output.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Jinja/jinja-whitespace.md#2025-04-09_snippet_0\n\nLANGUAGE: jinja\nCODE:\n```\n{{- ... -}}, {%- ... %}, {#- ... -#}\n```\n\n----------------------------------------\n\nTITLE: Querying Python Model Results in SQL\nDESCRIPTION: This SQL query references the Python model to retrieve and display the results of the fastest pit stops calculation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_23\n\nLANGUAGE: sql\nCODE:\n```\nselect * from {{ ref('fastest_pit_stops_by_constructor') }}\n```\n\n----------------------------------------\n\nTITLE: Staging Formula 1 Races Data\nDESCRIPTION: Creates a staging model for Formula 1 races data, including dates and times for all race weekend sessions\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\nwith\n\nsource  as (\n\n    select * from {{ source('formula1','races') }}\n\n),\n\nrenamed as (\n    select\n        raceid as race_id,\n        year as race_year,\n        round as race_round,\n        circuitid as circuit_id,\n        name as circuit_name,\n        date as race_date,\n        to_time(time) as race_time,\n        -- omit the url\n        fp1_date as free_practice_1_date,\n        fp1_time as free_practice_1_time,\n        fp2_date as free_practice_2_date,\n        fp2_time as free_practice_2_time,\n        fp3_date as free_practice_3_date,\n        fp3_time as free_practice_3_time,\n        quali_date as qualifying_date,\n        quali_time as qualifying_time,\n        sprint_date,\n        sprint_time\n    from source\n)\n\nselect * from renamed\n```\n\n----------------------------------------\n\nTITLE: Using a Macro in the SQL Model\nDESCRIPTION: SQL model that calls the get_payment_methods macro to retrieve the list of payment methods, improving code organization and reusability.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/using-jinja.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\n{%- set payment_methods = get_payment_methods() -%}\n\nselect\norder_id,\n{%- for payment_method in payment_methods %}\nsum(case when payment_method = '{{payment_method}}' then amount end) as {{payment_method}}_amount\n{%- if not loop.last %},{% endif -%}\n{% endfor %}\nfrom {{ ref('raw_payments') }}\ngroup by 1\n\n```\n\n----------------------------------------\n\nTITLE: Results Logging Macro\nDESCRIPTION: A macro that logs the execution results of dbt operations, including node status and messages.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/on-run-end-context.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n{% macro log_results(results) %}\n\n  {% if execute %}\n  {{ log(\"========== Begin Summary ==========\", info=True) }}\n  {% for res in results -%}\n    {% set line -%}\n        node: {{ res.node.unique_id }}; status: {{ res.status }} (message: {{ res.message }})\n    {%- endset %}\n\n    {{ log(line, info=True) }}\n  {% endfor %}\n  {{ log(\"========== End Summary ==========\", info=True) }}\n  {% endif %}\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Setting Default Package Installation Path in dbt_project.yml\nDESCRIPTION: Configures the default directory where packages are installed when running 'dbt deps'. By default, dbt installs packages in the 'dbt_packages' directory if this configuration is not specified.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/packages-install-path.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\npackages-install-path: directorypath\n```\n\n----------------------------------------\n\nTITLE: Promoting All Warnings to Errors (Command-line Flags)\nDESCRIPTION: These commands demonstrate various ways to promote all warnings to errors using command-line flags in different versions of dbt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/warnings.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ndbt --warn-error run \ndbt --warn-error-options '{\"include\": \"all\"}' run \ndbt --warn-error-options '{\"include\": \"*\"}' run\n```\n\nLANGUAGE: bash\nCODE:\n```\ndbt --warn-error run \ndbt --warn-error-options '{\"error\": \"all\"}' run \ndbt --warn-error-options '{\"error\": \"*\"}' run\n```\n\n----------------------------------------\n\nTITLE: Suppressing Print Messages in dbt Run Command\nDESCRIPTION: This command demonstrates how to use the --no-print flag with dbt run to suppress print() messages from showing in stdout.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/print-output.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ndbt --no-print run\n```\n\n----------------------------------------\n\nTITLE: Implementing Print Function in dbt Macro\nDESCRIPTION: Example of using the print() function within a dbt macro to output the values of arguments. The print statement will output to both the log file and stdout, and remains visible even when QUIET config is enabled.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/print.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n  {% macro some_macro(arg1, arg2) %}\n    {{ print(\"Running some_macro: \" ~ arg1 ~ \", \" ~ arg2) }}\n  {% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Copy Grants in dbt\nDESCRIPTION: Configuration for enabling copy grants functionality in dbt models. When enabled, dbt will add the 'copy grants' DDL qualifier when rebuilding tables and views.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snowflake-configs.md#2025-04-09_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  +copy_grants: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Enabled Sources in dbt YAML\nDESCRIPTION: Example of enabling or disabling sources in the dbt_project.yml and properties.yml files. This configuration can be applied to all sources or specific source tables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/enabled.md#2025-04-09_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  [<resource-path>]:\n    +enabled: true | false\n```\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: [<source-name>]\n    config:\n      enabled: true | false\n    tables:\n      - name: [<source-table-name>]\n        config:\n          enabled: true | false\n```\n\n----------------------------------------\n\nTITLE: Setting Column Orientation in Greenplum dbt Model\nDESCRIPTION: Configures the table storage orientation to column-based storage instead of row-based. Column-oriented storage can improve query performance for analytical workloads.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/greenplum-configs.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        ...\n        orientation='column'\n        ...\n    )\n}}\n\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Defining an Exposure in dbt YAML\nDESCRIPTION: Example of defining an exposure in a dbt YAML file. This snippet demonstrates how to create an exposure for a dashboard with defined dependencies on other dbt resources and ownership information.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/exposures.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nexposures:\n\n  - name: weekly_jaffle_metrics\n    label: Jaffles by the Week\n    type: dashboard\n    maturity: high\n    url: https://bi.tool/dashboards/1\n    description: >\n      Did someone say \"exponential growth\"?\n\n    depends_on:\n      - ref('fct_orders')\n      - ref('dim_customers')\n      - source('gsheets', 'goals')\n      - metric('count_orders')\n\n    owner:\n      name: Callum McData\n      email: data@jaffleshop.com\n```\n\n----------------------------------------\n\nTITLE: Defining Versions for a dbt Model without Explicit Latest Version in YAML\nDESCRIPTION: This example shows how to define multiple versions for a dbt model without explicitly specifying the 'latest_version'. In this case, dbt will automatically set the latest version to the highest version number (3).\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/latest_version.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: model_name\n    versions:\n      - v: 3\n      - v: 2\n      - v: 1\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Generic Test Severity in SQL\nDESCRIPTION: Example of setting a default warning severity level for a custom generic test implementation. All instances of this test will default to warning status.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/severity.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{% test <testname>(model, column_name) %}\n\n{{ config(severity = 'warn') }}\n\nselect ...\n\n{% endtest %}\n```\n\n----------------------------------------\n\nTITLE: Granting Basic Redshift Database Permissions\nDESCRIPTION: SQL statements for granting essential permissions in Redshift, including schema creation, usage rights, and table/view operations. These commands allow administrators to set up proper access controls for users and roles in a Redshift database.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/database-permissions/redshift-permissions.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\ngrant create schema on database database_name to user_name;\ngrant usage on schema database.schema_name to user_name;\ngrant create table on schema database.schema_name to user_name;\ngrant create view on schema database.schema_name to user_name;\ngrant usage for schemas in database database_name to role role_name;\ngrant select on all tables in database database_name to user_name;\ngrant select on all views in database database_name to user_name;\n```\n\n----------------------------------------\n\nTITLE: Ratio Metric with Dimensional Filtering\nDESCRIPTION: An example showing how to apply dimensional filters to ratio metrics using the Dimension function. This example defines a ratio for frequent purchasers, applying a filter to the numerator while using the same base metric for both parts.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/ratio-metrics.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  - name: frequent_purchaser_ratio\n    description: Fraction of active users who qualify as frequent purchasers\n    type: ratio\n    type_params:\n      numerator:\n        name: distinct_purchasers\n        filter: |\n          {{Dimension('customer__is_frequent_purchaser')}}\n        alias: frequent_purchasers\n      denominator:\n        name: distinct_purchasers\n```\n\n----------------------------------------\n\nTITLE: Setting Primary and Shard Keys in SingleStore\nDESCRIPTION: Shows how to configure primary key and shard key for a SingleStore table model. Primary keys define unique identifiers while shard keys determine how data is distributed across shards in the database.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/singlestore-configs.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        primary_key=['id', 'user_id'],\n        shard_key=['id']\n    )\n}}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Incremental Model in dbt\nDESCRIPTION: This code block shows how to configure an incremental model in dbt using the 'config' block. It specifies the materialization type as 'incremental' and sets a 'unique_key' for handling updates to existing records.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/materializations/materializations-guide-4-incremental-models.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        materialized='incremental',\n        unique_key='order_id'\n    )\n}}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Late Binding Views in Redshift\nDESCRIPTION: How to create late binding views in Redshift using dbt, which prevents views from being dropped when upstream dependencies change. Shows single model configuration using bind=False.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/redshift-configs.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(materialized='view', bind=False) }}\n\nselect *\nfrom source.data\n```\n\n----------------------------------------\n\nTITLE: Adding Descriptions to Generic Data Tests in dbt YAML\nDESCRIPTION: This example shows how to add descriptions to generic data tests in dbt v1.9+. The description provides context about the test's purpose and what action to take if the test fails, enhancing documentation and team collaboration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/data-test-configs.md#2025-04-09_snippet_11\n\nLANGUAGE: yml\nCODE:\n```\nmodels:\n  - name: my_model\n    columns:\n      - name: delivery_status\n        tests:\n          - accepted_values:\n              values: ['delivered', 'pending', 'failed']\n              description: \"This test checks whether there are unexpected delivery statuses. If it fails, check with logistics team\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Grants for Seeds in YAML\nDESCRIPTION: Shows the configuration of grants for a seed in a YAML schema file. This example grants select privileges to 'reporter' and 'bi' roles.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/grants.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  - name: seed_name\n    config:\n      grants:\n        select: ['reporter', 'bi']\n```\n\n----------------------------------------\n\nTITLE: Logging Model Contents via CLI\nDESCRIPTION: Shows how to print the full contents of a model object using the log() function in CLI.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/model.md#2025-04-09_snippet_1\n\nLANGUAGE: jinja\nCODE:\n```\n{{ log(model, info=True) }}\n```\n\n----------------------------------------\n\nTITLE: Running dbt with Fail-Fast Option\nDESCRIPTION: This snippet demonstrates the usage of the fail-fast option in a dbt run command. It shows the output when a failure occurs in the first model, preventing other models from running.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/failing-fast.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ndbt -x run --threads 1\nRunning with dbt=1.0.0\nFound 4 models, 1 test, 1 snapshot, 2 analyses, 143 macros, 0 operations, 1 seed file, 0 sources\n\n14:47:39 | Concurrency: 1 threads (target='dev')\n14:47:39 |\n14:47:39 | 1 of 4 START table model test_schema.model_1........... [RUN]\n14:47:40 | 1 of 4 ERROR creating table model test_schema.model_1.. [ERROR in 0.06s]\n14:47:40 | 2 of 4 START view model test_schema.model_2............ [RUN]\n14:47:40 | CANCEL query model.debug.model_2....................... [CANCEL]\n14:47:40 | 2 of 4 ERROR creating view model test_schema.model_2... [ERROR in 0.05s]\n\nDatabase Error in model model_1 (models/model_1.sql)\n  division by zero\n  compiled SQL at target/run/debug/models/model_1.sql\n\nEncountered an error:\nFailFast Error in model model_1 (models/model_1.sql)\n  Failing early due to test failure or runtime error\n```\n\n----------------------------------------\n\nTITLE: Importing Explorer Course Component in Markdown\nDESCRIPTION: A React/MDX import statement for including the ExplorerCourse component from a snippets file, used to provide links to course information in the documentation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/project-recommendations.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nimport ExplorerCourse from '/snippets/_explorer-course-link.md';\n\n<ExplorerCourse />\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced Row Compression for Oracle Table Materialization in dbt\nDESCRIPTION: This snippet shows how to enable Advanced Row Compression for table materialization in Oracle using dbt. It uses the 'ROW STORE COMPRESS ADVANCED' option, which is recommended for OLTP systems.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/oracle-configs.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n-- Advanced Row compression\n{{config(materialized='table', table_compression_clause='ROW STORE COMPRESS ADVANCED')}}\nSELECT c.cust_id, c.cust_first_name, c.cust_last_name\nfrom {{ source('sh_database', 'customers') }} c\n```\n\n----------------------------------------\n\nTITLE: Using Relation Objects with Jinja Templates in dbt\nDESCRIPTION: This snippet demonstrates how to create and use Relation objects in dbt templates. It shows how to access relation properties, conditionally include parts of the relation, and test for relation types.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-classes.md#2025-04-09_snippet_1\n\nLANGUAGE: jinja2\nCODE:\n```\n{% set relation = api.Relation.create(schema='snowplow', identifier='events') %}\n\n-- Return the `database` for this relation\n{{ relation.database }}\n\n-- Return the `schema` (or dataset) for this relation\n{{ relation.schema }}\n\n-- Return the `identifier` for this relation\n{{ relation.identifier }}\n\n-- Return relation name without the database\n{{ relation.include(database=false) }}\n\n-- Return true if the relation is a table\n{{ relation.is_table }}\n\n-- Return true if the relation is a view\n{{ relation.is_view }}\n\n-- Return true if the relation is a cte\n{{ relation.is_cte }}\n```\n\n----------------------------------------\n\nTITLE: Multiple Column Check Example (v1.9+)\nDESCRIPTION: Example showing how to configure multiple columns to check for changes in a snapshot using YAML format.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/check_cols.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  - name: orders_snapshot_check\n    relation: source('jaffle_shop', 'orders')\n    config:\n      schema: snapshots\n      unique_key: id\n      strategy: check\n      check_cols:\n        - status\n        - is_cancelled\n```\n\n----------------------------------------\n\nTITLE: Project-wide Test Configuration\nDESCRIPTION: Setting default fail_calc configurations at the project level for all tests or specific packages in dbt_project.yml.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/fail_calc.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ntests:\n  +fail_calc: count(*)  # all tests\n  \n  <package_name>:\n    +fail_calc: count(distinct id) # tests in <package_name>\n```\n\n----------------------------------------\n\nTITLE: Basic query-comment Configuration in dbt\nDESCRIPTION: Simple string configuration for query comments in dbt_project.yml.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/query-comment.md#2025-04-09_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nquery-comment: string\n```\n\n----------------------------------------\n\nTITLE: Static CI Schema Generation\nDESCRIPTION: Macro for using a single schema for CI jobs while prefixing PR identifiers to table names. Includes special handling for CI environments.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/customize-schema-alias.md#2025-04-09_snippet_4\n\nLANGUAGE: jinja\nCODE:\n```\n{% macro generate_schema_name(custom_schema_name=none, node=none) -%}\n\n    {%- set default_schema = target.schema -%}\n    \n    {# If the CI Job does not exist in its own environment, use the target.name variable inside the job instead #}\n    {# {%- if target.name == 'CI' -%} #} \n    \n    {%- if env_var('DBT_ENV_TYPE','DEV') == 'CI' -%}\n        \n        ci_schema\n        \n    {%- elif custom_schema_name is none -%}\n        \n        {{ default_schema }}\n    \n    {%- else -%}\n        \n        {{ default_schema }}_{{ custom_schema_name | trim }}\n    \n    {%- endif -%}    \n\n{%- endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Fetching Granularities for All Time Dimensions with Semantic Layer JDBC API\nDESCRIPTION: SQL query to fetch available granularities for all time dimensions using the dimensions() function and selecting the granularity fields. This provides more granularity info than the queryable_granularities function.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-jdbc.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nselect NAME, QUERYABLE_GRANULARITIES from {{\n    semantic_layer.dimensions(\n        metrics=[\"order_total\"]\n    )\n}}\n```\n\n----------------------------------------\n\nTITLE: Navigating to the Jaffle Shop Directory\nDESCRIPTION: Command to change into the cloned Jaffle Shop directory to begin working with the project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/duckdb-qs.md#2025-04-09_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncd jaffle_shop_duck_db\n```\n\n----------------------------------------\n\nTITLE: Configuring Required dbt Version in Project YAML\nDESCRIPTION: Example configuration in dbt_project.yml file to specify the minimum required dbt version for the project. This prevents users from running the project with incompatible dbt versions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-11-23-how-to-upgrade-dbt-versions.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n#/dbt_project.yml\n\nname: 'your_company_project'\n\nversion: '0.1.0'\n\nrequire-dbt-version: \">=0.17.2\"\n\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Table Options in Seed Configuration for Teradata\nDESCRIPTION: Setting multiple table options (NO FALLBACK and NO JOURNAL) for a seed table in Teradata, separated by commas.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/teradata-configs.md#2025-04-09_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  <project-name>:\n    table_option:\"NO FALLBACK, NO JOURNAL\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Column Types in dbt_project.yml\nDESCRIPTION: Demonstrates how to specify custom column types for seed files in the dbt_project.yml configuration file. Shows setting varchar lengths for country codes and names.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/column_types.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  jaffle_shop:\n    country_codes:\n      +column_types:\n        country_code: varchar(2)\n        country_name: varchar(32)\n```\n\n----------------------------------------\n\nTITLE: Defining YAML Frontmatter for Documentation Page\nDESCRIPTION: This YAML snippet defines the frontmatter for the documentation page, including the title, description, ID, and sidebar configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/core-upgrade/11-Older versions/15-upgrading-to-v1.1.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: \"Upgrading to v1.1\"\ndescription: New features and changes in dbt Core v1.1\nid: \"upgrading-to-v1.1\"\ndisplayed_sidebar: \"docs\"\n---\n```\n\n----------------------------------------\n\nTITLE: Custom Schema Enforcement Implementation\nDESCRIPTION: Implementation of generate_schema_name() macro that enforces custom schema definitions by raising compilation errors when no custom schema is defined for a model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/customize-schema-alias.md#2025-04-09_snippet_7\n\nLANGUAGE: jinja\nCODE:\n```\n{% macro generate_schema_name(custom_schema_name, node) -%}\n\n    {%- set default_schema = target.schema -%}\n    {%- if custom_schema_name is none and node.resource_type == 'model' -%}\n        \n        {{ exceptions.raise_compiler_error(\"Error: No Custom Schema Defined for the model \" ~ node.name ) }}\n    \n    {%- endif -%}\n```\n\n----------------------------------------\n\nTITLE: Staging Formula 1 Constructors Data\nDESCRIPTION: Creates a staging model for Formula 1 constructors data, transforming column names and excluding URL field\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nwith\n\nsource  as (\n\n    select * from {{ source('formula1','constructors') }}\n\n),\n\nrenamed as (\n    select\n        constructorid as constructor_id,\n        constructorref as constructor_ref,\n        name as constructor_name,\n        nationality as constructor_nationality\n        -- omit the url\n    from source\n)\n\nselect * from renamed\n```\n\n----------------------------------------\n\nTITLE: Implementing get_payment_methods Macro with Jinja and SQL in dbt\nDESCRIPTION: Jinja macro that queries distinct payment methods and logs the results. It demonstrates the use of run_query and log functions in dbt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/using-jinja.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\n{% macro get_payment_methods() %}\n\n{% set payment_methods_query %}\nselect distinct\npayment_method\nfrom {{ ref('raw_payments') }}\norder by 1\n{% endset %}\n\n{% set results = run_query(payment_methods_query) %}\n\n{{ log(results, info=True) }}\n\n{{ return([]) }}\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Custom Snapshot Strategy Configuration in SQL (dbt v1.8 and earlier)\nDESCRIPTION: SQL/Jinja template for using a custom-defined snapshot strategy. The strategy must be implemented as a macro named 'snapshot_[strategy_name]_strategy'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/strategy.md#2025-04-09_snippet_11\n\nLANGUAGE: jinja2\nCODE:\n```\n{% snapshot [snapshot_name](snapshot_name) %}\n\n{{ config(\n  strategy=\"timestamp_with_deletes\",\n  updated_at=\"column_name\"\n) }}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Promoting All Warnings to Errors (Environment Variables)\nDESCRIPTION: These commands show how to promote all warnings to errors using environment variables in different versions of dbt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/warnings.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nWARN_ERROR=true dbt run\nDBT_WARN_ERROR_OPTIONS='{\"include\": \"all\"}' dbt run \nDBT_WARN_ERROR_OPTIONS='{\"include\": \"*\"}' dbt run\n```\n\nLANGUAGE: bash\nCODE:\n```\nWARN_ERROR=true dbt run \nDBT_WARN_ERROR_OPTIONS='{\"error\": \"all\"}' dbt run \nDBT_WARN_ERROR_OPTIONS='{\"error\": \"*\"}' dbt run\n```\n\n----------------------------------------\n\nTITLE: Checking Node Selection in dbt Hooks using Jinja and SQL\nDESCRIPTION: Demonstrates how to use the selected_resources variable in a dbt hook to check if a specific model is selected and log different messages based on the result. This snippet uses Jinja templating within SQL.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/selected_resources.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n/*\n  Check if a given model is selected and trigger a different action, depending on the result\n*/\n\n{% if execute %}\n  {% if 'model.my_project.model1' in selected_resources %}\n  \n    {% do log(\"model1 is included based on the current selection\", info=true) %}\n  \n  {% else %}\n\n    {% do log(\"model1 is not included based on the current selection\", info=true) %}\n\n  {% endif %}\n{% endif %}\n\n/*\n  Example output when running the code in on-run-start \n  when doing `dbt build`, including all nodels\n---------------------------------------------------------------\n  model1 is included based on the current selection\n\n\n  Example output when running the code in on-run-start \n  when doing `dbt run --select model2` \n---------------------------------------------------------------\n  model1 is not included based on the current selection\n*/\n```\n\n----------------------------------------\n\nTITLE: Complex SQL Metric Calculation\nDESCRIPTION: Traditional SQL query for calculating complex metrics involving food orders and returning customers.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/about-metricflow.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    date_trunc('day',orders.ordered_at) as day, \n    sum(case when is_food_order = true then order_total else null end) as food_order,\n    sum(orders.order_total) as sum_order_total,\n    food_order/sum_order_total\nfrom\n  orders\nleft join\n  customers\non\n  orders.customer_id = customers.customer_id\nwhere\n  case when customers.first_ordered_at is not null then true else false end = true\ngroup by 1\n```\n\n----------------------------------------\n\nTITLE: Complex Test Selection in dbt\nDESCRIPTION: These examples show more complex ways to select and run specific tests in dbt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/test-selection-examples.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndbt test --select \"assert_total_payment_amount_is_positive\" # directly select the test by name\ndbt test --select \"payments,test_type:singular\" # indirect selection, v1.2\n```\n\n----------------------------------------\n\nTITLE: Querying Latest Model States in GraphQL for dbt Cloud Discovery API\nDESCRIPTION: This GraphQL query retrieves the latest state of each model in a dbt Cloud environment. It includes detailed information such as unique ID, compiled code, database, schema, alias, materialized type, and execution information for both the most recent run and the most recent successful run.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/discovery-use-cases-and-examples.md#2025-04-09_snippet_3\n\nLANGUAGE: graphql\nCODE:\n```\nquery ($environmentId: BigInt!, $first: Int!) {\n  environment(id: $environmentId) {\n    applied {\n      models(first: $first) {\n        edges {\n          node {\n            uniqueId\n            compiledCode\n            database\n            schema\n            alias\n            materializedType\n            executionInfo {\n              executeCompletedAt\n              lastJobDefinitionId\n              lastRunGeneratedAt\n              lastRunId\n              lastRunStatus\n              lastRunError\n              lastSuccessJobDefinitionId\n              runGeneratedAt\n              lastSuccessRunId\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Unique and Sort Keys in SingleStore\nDESCRIPTION: Demonstrates how to set unique table keys and sort keys for a SingleStore table. Unique keys enforce uniqueness while sort keys determine the physical ordering of data in a columnstore table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/singlestore-configs.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        materialized='table',\n        unique_table_key=['id'],\n        sort_key=['status'],\n    )\n}}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Generated Yellowbrick SQL with Replicate Distribution and Sort\nDESCRIPTION: The SQL output generated by dbt for a model using REPLICATE distribution and a SORT column. This shows how dbt translates the configuration into native Yellowbrick syntax.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/yellowbrick-configs.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\ncreate table if not exists marts.dim_team as (\nselect\n    hash(stg.name) as team_key\n    , stg.name as team_name\n    , stg.nickname as team_nickname\n    , stg.city as home_city\n    , stg.stadium as stadium_name\n    , stg.capacity as stadium_capacity\n    , stg.avg_att as average_game_attendance\n    , current_timestamp as md_create_timestamp\nfrom\n    premdb.public.team stg\nwhere\n    stg.name is not null\n)\ndistribute REPLICATE\nsort on (stadium_capacity);\n```\n\n----------------------------------------\n\nTITLE: Installing dbt Core with Adapter\nDESCRIPTION: New installation procedure for dbt Core v1.8 requiring explicit installation of both dbt-core and the desired adapter.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/core-upgrade/07-upgrading-to-v1.8.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install dbt-core dbt-ADAPTER_NAME\n```\n\n----------------------------------------\n\nTITLE: Generated Merge SQL for Incremental Processing\nDESCRIPTION: The compiled SQL code showing how dbt transforms the incremental model into merge statements for Delta tables, including temporary view creation and merge operation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/spark-configs.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\ncreate temporary view merge_incremental__dbt_tmp as\n\n    with new_events as (\n\n        select * from analytics.events\n\n\n        where date_day >= date_add(current_date, -1)\n\n\n    )\n\n    select\n        user_id,\n        max(date_day) as last_seen\n\n    from events\n    group by 1\n\n;\n\nmerge into analytics.merge_incremental as DBT_INTERNAL_DEST\n    using merge_incremental__dbt_tmp as DBT_INTERNAL_SOURCE\n    on DBT_INTERNAL_SOURCE.user_id = DBT_INTERNAL_DEST.user_id\n    when matched then update set *\n    when not matched then insert *\n```\n\n----------------------------------------\n\nTITLE: Setting Table Kind in Seed Configuration for Teradata\nDESCRIPTION: Configuring the table_kind parameter for seeds in the dbt_project.yml file to specify SET table type in Teradata.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/teradata-configs.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  <project-name>:\n    table_kind: \"SET\"\n```\n\n----------------------------------------\n\nTITLE: Viewing dbt Profiles Configuration Location\nDESCRIPTION: Shows the configured location of the profiles.yml file using the --config-dir flag and displays the path to access it\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/debug.md#2025-04-09_snippet_2\n\nLANGUAGE: text\nCODE:\n```\ndbt debug --config-dir\nTo view your profiles.yml file, run:\n\nopen /Users/alice/.dbt\n```\n\n----------------------------------------\n\nTITLE: Creating a Seconds-based Time Spine in SQL\nDESCRIPTION: Creates a time spine table at second granularity using dbt's date_spine macro. The table generates seconds from 10 seconds before the current time up to the current timestamp.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metricflow-time-spine.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(materialized='table') }}\n\nwith seconds as (\n\n    {{\n        dbt.date_spine(\n            'second',\n            \"date_trunc('second', dateadd(second, -10, current_timestamp()))\",\n            \"date_trunc('second', current_timestamp())\"\n        )\n    }}\n\n),\n\nfinal as (\n    select cast(date_second as timestamp) as second_timestamp\n    from seconds\n)\n\nselect * from final\n\n```\n\n----------------------------------------\n\nTITLE: Basic SQL Model Definition in SingleStore\nDESCRIPTION: A simple SQL model definition with string constants for customer data. This example needs type casting to work with model contracts in SingleStore.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/singlestore-configs.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nselect\n  'abc123' as customer_id,\n  'My Best Customer' as customer_name\n```\n\n----------------------------------------\n\nTITLE: Saved Queries Configuration in dbt_project.yml\nDESCRIPTION: Example showing the correct naming convention using dashes for configuring saved queries in the dbt_project.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt_project.yml.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nsaved-queries:\n  my_saved_query:\n    +cache:\n      enabled: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Full Refresh for Individual Models in SQL\nDESCRIPTION: Sets the full_refresh config for an individual model within its SQL file. This overrides the project-level setting and determines whether the model will always, never, or conditionally perform a full refresh.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/full_refresh.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    full_refresh = false | true\n) }}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Project Dependencies\nDESCRIPTION: YAML configuration defining package and project dependencies including dbt_utils and analytics project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/mesh-qs.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\npackages:\n  - package: dbt-labs/dbt_utils\n    version: 1.1.1\n\nprojects:\n  - name: analytics\n```\n\n----------------------------------------\n\nTITLE: General Configurations in YAML Properties File\nDESCRIPTION: Example of how to add general configurations to tests in a YAML properties file. Shows configuration options for both resource-level and column-level generic tests.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/data-test-configs.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\n<resource_type>:\n  - name: <resource_name>\n    tests:\n      - <test_name>: # Actual name of the test. For example, dbt_utils.equality\n          name: # Human friendly name for the test. For example, equality_fct_test_coverage\n          [description]: \"markdown formatting\"\n          <argument_name>: <argument_value>\n          [config]:\n            [enabled]: true | false\n            [tags]: <string> | [<string>]\n            [meta]: {dictionary}\n            # relevant for [store_failures] only\n            [database]: <string>\n            [schema]: <string>\n            [alias]: <string>\n\n    [columns]:\n      - name: <column_name>\n        tests:\n          - <test_name>:\n              name: \n              [description]: \"markdown formatting\"\n              <argument_name>: <argument_value>\n              [config]:\n                [enabled]: true | false\n                [tags]: <string> | [<string>]\n                [meta]: {dictionary}\n                # relevant for [store_failures] only\n                [database]: <string>\n                [schema]: <string>\n                [alias]: <string>\n```\n\n----------------------------------------\n\nTITLE: Overriding Global dbt Macros in YAML\nDESCRIPTION: Configures dispatch to override global dbt macros by defining a custom search order. This allows organization-wide customization of dbt's built-in functionality across multiple projects.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/dispatch.md#2025-04-09_snippet_5\n\nLANGUAGE: yml\nCODE:\n```\ndispatch:\n  - macro_namespace: dbt\n    search_order: ['my_project', 'my_org_dbt_helpers', 'dbt']\n```\n\n----------------------------------------\n\nTITLE: Implementing Package Override for Built-in Materialization in Snowflake\nDESCRIPTION: Example of how to explicitly override a built-in materialization by implementing a wrapper in the root project that calls the package implementation. This approach is required when the 'require_explicit_package_overrides_for_builtin_materializations' flag is set to True.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/behavior-changes.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{% materialization view, snowflake %}\n  {{ return(my_installed_package_name.materialization_view_snowflake()) }}\n{% endmaterialization %}\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Conda Environment for dbt-oracle\nDESCRIPTION: Bash command to create a custom Conda environment with specific Python packages for use with Oracle ML.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/oracle-setup.md#2025-04-09_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n dbt_py_env -c conda-forge --override-channels --strict-channel-priority python=3.12.1 nltk gensim\n```\n\n----------------------------------------\n\nTITLE: Configuring Primary Index with Partitioning in Seed Configuration for Teradata\nDESCRIPTION: Setting up a primary index with range partitioning by date in a Teradata seed table to optimize query performance for date-based queries.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/teradata-configs.md#2025-04-09_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  <project-name>:\n    index: \"PRIMARY INDEX(id)\n      PARTITION BY RANGE_N(create_date\n                    BETWEEN DATE '2020-01-01'\n                    AND     DATE '2021-01-01'\n                    EACH INTERVAL '1' MONTH)\"\n```\n\n----------------------------------------\n\nTITLE: Adding Descriptions to Singular Data Tests in dbt YAML\nDESCRIPTION: This example demonstrates how to add descriptions to singular tests in dbt v1.9+ using a YAML file in the test directory. The description explains the test's purpose and follow-up actions if the test fails.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/data-test-configs.md#2025-04-09_snippet_12\n\nLANGUAGE: yml\nCODE:\n```\ndata_tests: \n  - name: my_custom_test\n    description: \"This test checks whether the rolling average of returns is inside of expected bounds. If it isn't, flag to customer success team\"\n```\n\n----------------------------------------\n\nTITLE: Configuring insert_overwrite Strategy for Vertica Partitioned Tables\nDESCRIPTION: Demonstrates how to use the insert_overwrite strategy which drops and recreates specific partitions of a table. This example uses partition_by_string to define partitioning by year and targets the 2023 partition.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/vertica-configs.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\n{{config(materialized = 'incremental',incremental_strategy = 'insert_overwrite',partition_by_string='YEAR(cc_open_date)',partitions=['2023'])}}\n\n\n        select * from online_sales.call_center_dimension\n```\n\nLANGUAGE: sql\nCODE:\n```\n        select PARTITION_TABLE('online_sales.update_call_center_dimension');\n\n        SELECT DROP_PARTITIONS('online_sales.update_call_center_dimension', '2023', '2023');\n      \n        SELECT PURGE_PARTITION('online_sales.update_call_center_dimension', '2023');\n      \n        insert into \"VMart\".\"online_sales\".\"update_call_center_dimension\"\n\n        (\"call_center_key\", \"cc_closed_date\", \"cc_open_date\", \"cc_name\", \"cc_class\", \"cc_employees\",\n       \n        \"cc_hours\", \"cc_manager\", \"cc_address\", \"cc_city\", \"cc_state\", \"cc_region\")\n      \n        (\n\n            select \"call_center_key\", \"cc_closed_date\", \"cc_open_date\", \"cc_name\", \"cc_class\", \"cc_employees\",\n        \n            \"cc_hours\", \"cc_manager\", \"cc_address\", \"cc_city\", \"cc_state\", \"cc_region\"\n\n            from \"update_call_center_dimension__dbt_tmp\"\n        );\n```\n\n----------------------------------------\n\nTITLE: Sample Data Transformation for valid_history Strategy in dbt-teradata\nDESCRIPTION: Provides an example of source data and its corresponding target data after applying the valid_history incremental strategy in dbt-teradata.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/teradata-configs.md#2025-04-09_snippet_24\n\nLANGUAGE: sql\nCODE:\n```\n    -- Source data\n        pk |       valid_from          | value_txt1 | value_txt2\n        ======================================================================\n        1  | 2024-03-01 00:00:00.0000  | A          | x1\n        1  | 2024-03-12 00:00:00.0000  | B          | x1\n        1  | 2024-03-12 00:00:00.0000  | B          | x2\n        1  | 2024-03-25 00:00:00.0000  | A          | x2\n        2  | 2024-03-01 00:00:00.0000  | A          | x1\n        2  | 2024-03-12 00:00:00.0000  | C          | x1\n        2  | 2024-03-12 00:00:00.0000  | D          | x1\n        2  | 2024-03-13 00:00:00.0000  | C          | x1\n        2  | 2024-03-14 00:00:00.0000  | C          | x1\n    \n    -- Target data\n        pk | valid_period                                                       | value_txt1 | value_txt2\n        ===================================================================================================\n        1  | PERIOD(TIMESTAMP)[2024-03-01 00:00:00.0, 2024-03-12 00:00:00.0]    | A          | x1\n        1  | PERIOD(TIMESTAMP)[2024-03-12 00:00:00.0, 2024-03-25 00:00:00.0]    | B          | x1\n        1  | PERIOD(TIMESTAMP)[2024-03-25 00:00:00.0, 9999-12-31 23:59:59.9999] | A          | x2\n        2  | PERIOD(TIMESTAMP)[2024-03-01 00:00:00.0, 2024-03-12 00:00:00.0]    | A          | x1\n        2  | PERIOD(TIMESTAMP)[2024-03-12 00:00:00.0, 9999-12-31 23:59:59.9999] | C          | x1\n```\n\n----------------------------------------\n\nTITLE: Markdown Configuration for dbt Documentation Page\nDESCRIPTION: Frontmatter configuration for a dbt documentation page about custom CI/CD pipelines, including metadata like title, description, tags, and search parameters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/custom-cicd-pipelines.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: Customizing CI/CD with custom pipelines\nid: custom-cicd-pipelines\ndescription: \"Learn the benefits of version-controlled analytics code and custom pipelines in dbt for enhanced code testing and workflow automation during the development process.\"\ndisplayText: Learn version-controlled code, custom pipelines, and enhanced code testing.\nhoverSnippet: Learn version-controlled code, custom pipelines, and enhanced code testing.\n# time_to_complete: '30 minutes' commenting out until we test\nicon: 'guides'\nhide_table_of_contents: true\ntags: ['dbt Cloud', 'Orchestration', 'CI']\nlevel: 'Intermediate'\nrecently_updated: true\nsearch_weight: \"heavy\"\nkeywords:\n  - bitbucket pipeline, custom pipelines, github, gitlab, azure devops, ci/cd custom pipeline\n---\n```\n\n----------------------------------------\n\nTITLE: Recording Timing Info with dbt CLI\nDESCRIPTION: This snippet demonstrates how to use the -r flag to record timing information to a file and visualize it using snakeviz. It shows the command to run dbt with timing info recording and the subsequent command to visualize the data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/record-timing-info.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n$ dbt -r timing.txt run\n...\n\n$ snakeviz timing.txt\n```\n\n----------------------------------------\n\nTITLE: GitHub Actions Workflow for Triggering dbt Cloud Job\nDESCRIPTION: This snippet shows a GitHub Actions workflow step that runs a custom script and triggers a dbt Cloud job run using dbt-cloud-cli. It demonstrates how the CLI can be integrated into CI/CD pipelines.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-05-03-making-dbt-cloud-api-calls-using-dbt-cloud-cli.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n- name: Trigger dbt Cloud job run\n  run: |\n    ./cool_script_bro.sh\n    dbt-cloud job run --job-id $DBT_CLOUD_JOB_ID\n```\n\n----------------------------------------\n\nTITLE: Setting Oracle Host and Service Environment Variables\nDESCRIPTION: Defines environment variables for Oracle host and service name.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/oracle-setup.md#2025-04-09_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nexport DBT_ORACLE_HOST=adb.example.oraclecloud.com\nexport DBT_ORACLE_SERVICE=example_high.adb.oraclecloud.com\n```\n\n----------------------------------------\n\nTITLE: Specifying File Format in dbt Model\nDESCRIPTION: Configuration example for specifying file formats (iceberg, hive, delta, or hudi) in dbt models with table materialization.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/watsonx-spark-config.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(\n    materialized='table',\n    file_format='iceberg' or 'hive' or 'delta' or 'hudi'\n  )\n}}\n```\n\n----------------------------------------\n\nTITLE: Using delete+insert Incremental Strategy in Vertica\nDESCRIPTION: Shows the configuration for the delete+insert incremental strategy which first deletes existing records and then re-inserts them. This approach requires a unique_key to identify which records to delete.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/vertica-configs.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\n{{ config( materialized = 'incremental', incremental_strategy = 'delete+insert',  unique_key='date_key'   )  }}\n\n\n          select * FROM  public.date_dimension\n```\n\nLANGUAGE: sql\nCODE:\n```\n        delete from \"VMart\".\"public\".\"samp\"\n            where (\n                date_key) in (\n                select (date_key)\n                from \"samp__dbt_tmp\"\n            );\n\n        insert into \"VMart\".\"public\".\"samp\" (\n             \"date_key\", \"date\", \"full_date_description\", \"day_of_week\", \"day_number_in_calendar_month\", \"day_number_in_calendar_year\", \"day_number_in_fiscal_month\", \"day_number_in_fiscal_year\", \"last_day_in_week_indicator\", \"last_day_in_month_indicator\", \"calendar_week_number_in_year\", \"calendar_month_name\", \"calendar_month_number_in_year\", \"calendar_year_month\", \"calendar_quarter\", \"calendar_year_quarter\", \"calendar_half_year\", \"calendar_year\", \"holiday_indicator\", \"weekday_indicator\", \"selling_season\")\n        (\n            select \"date_key\", \"date\", \"full_date_description\", \"day_of_week\", \"day_number_in_calendar_month\", \"day_number_in_calendar_year\", \"day_number_in_fiscal_month\", \"day_number_in_fiscal_year\", \"last_day_in_week_indicator\", \"last_day_in_month_indicator\", \"calendar_week_number_in_year\", \"calendar_month_name\", \"calendar_month_number_in_year\", \"calendar_year_month\", \"calendar_quarter\", \"calendar_year_quarter\", \"calendar_half_year\", \"calendar_year\", \"holiday_indicator\", \"weekday_indicator\", \"selling_season\"\n            from \"samp__dbt_tmp\"\n        );\n```\n\n----------------------------------------\n\nTITLE: Aliasing a Data Test in dbt_project.yml\nDESCRIPTION: Configures an alias for a data test at the project level in the dbt_project.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/alias.md#2025-04-09_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\ntests:\n  your_project:\n    +alias: unique_order_id_test\n```\n\n----------------------------------------\n\nTITLE: Configuring Release Track in dbt Cloud Admin API\nDESCRIPTION: This snippet demonstrates how to set the dbt_version parameter to configure an environment's release track using the dbt Cloud Admin API or Terraform.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/cloud-release-tracks.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n- `latest` (or `versionless`, the old name is still supported)\n- `compatible`\n- `extended`\n```\n\n----------------------------------------\n\nTITLE: Defining materialization settings for Python models\nDESCRIPTION: This YAML snippet specifies that all models in the marts directory should be materialized as tables, which is required for Python models. This ensures no errors will occur when running Python models in the project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nmarts:     \n  +materialized: table\n```\n\n----------------------------------------\n\nTITLE: Running Tests on a Specific Source Table\nDESCRIPTION: This command runs tests on a single table within a specific source. It uses dot notation to target the 'orders' table within the 'jaffle_shop' source.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Tests/testing-sources.md#2025-04-09_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n$ dbt test --select source:jaffle_shop.orders\n```\n\n----------------------------------------\n\nTITLE: Configuring Check Strategy in dbt_project.yml\nDESCRIPTION: YAML configuration for setting check strategy at the project level in dbt_project.yml. This applies the configuration to all snapshots in the specified resource path.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/strategy.md#2025-04-09_snippet_5\n\nLANGUAGE: yml\nCODE:\n```\nsnapshots:\n  [<resource-path>](/reference/resource-configs/resource-path):\n    +strategy: check\n    +check_cols: [column_name] | all\n```\n\n----------------------------------------\n\nTITLE: Creating a Yearly Time Spine SQL Model in dbt\nDESCRIPTION: This SQL model creates a time spine with yearly granularity, generating one row per year from 2000 to 2025. It filters to retain only years from 4 years in the past to 1 year in the future relative to the current date.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/mf-time-spine.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n{{{\n    config(\n        materialized = 'table',\n    )\n}}}\n\nwith years as (\n\n    {{\n        dbt.date_spine(\n            'year',\n            \"to_date('01/01/2000','mm/dd/yyyy')\",\n            \"to_date('01/01/2025','mm/dd/yyyy')\"\n        )\n    }}\n\n),\n\nfinal as (\n    select cast(date_year as date) as date_year\n    from years\n)\n\nselect * from final\n-- filter the time spine to a specific range\nwhere date_year >= date_trunc('year', dateadd(year, -4, current_timestamp())) \n  and date_year < date_trunc('year', dateadd(year, 1, current_timestamp()))\n```\n\n----------------------------------------\n\nTITLE: Configuring Time Dimensions with time_granularity in dbt Semantic Layer (v1.8 and earlier)\nDESCRIPTION: YAML configuration demonstrating time_granularity parameter with daily granularity level in dbt Semantic Layer v1.8 and earlier.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/dimensions.md#2025-04-09_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\ndimensions: \n  - name: created_at\n    type: time\n    label: \"Date of creation\"\n    expr: ts_created # ts_created is the underlying column name from the table \n    is_partition: True \n    type_params:\n      time_granularity: day \n  - name: deleted_at\n    type: time\n    label: \"Date of deletion\"\n    expr: ts_deleted # ts_deleted is the underlying column name from the table \n    is_partition: True \n    type_params:\n      time_granularity: day \n\nmeasures:\n  - name: users_deleted\n    expr: 1\n    agg: sum \n    agg_time_dimension: deleted_at\n  - name: users_created\n    expr: 1\n    agg: sum\n```\n\n----------------------------------------\n\nTITLE: Debugging Runtime Error: Not a dbt Project\nDESCRIPTION: Example of the error message shown when dbt cannot find a dbt_project.yml file, indicating that the current directory is not recognized as a dbt project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/debug-errors.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nRunning with dbt=1.7.1\nEncountered an error:\nRuntime Error\n  fatal: Not a dbt project (or any of the parent directories). Missing dbt_project.yml file\n```\n\n----------------------------------------\n\nTITLE: Executing dbt clean command\nDESCRIPTION: Basic usage of the dbt clean command to delete paths specified in clean-targets.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/clean.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndbt clean\n```\n\n----------------------------------------\n\nTITLE: Configuring Constant Properties for Conversion Metrics in YAML\nDESCRIPTION: YAML configuration that adds constant properties to ensure dimensions match between base and conversion events, helping track conversions where the same product appears in both steps of the conversion funnel.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/conversion-metrics.md#2025-04-09_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\n- name: view_item_detail_to_purchase_with_same_item\n  description: \"Conversion rate for users who viewed the item detail page and purchased the item\"\n  type: Conversion\n  label: View Item Detail > Purchase\n  type_params:\n    conversion_type_params:\n      calculation: conversions\n      base_measure:\n        name: view_item_detail \n      conversion_measure: purchase\n      entity: user\n      window: 1 week\n      constant_properties:\n        - base_property: product\n          conversion_property: product\n```\n\n----------------------------------------\n\nTITLE: Preferred dbt Production Commands\nDESCRIPTION: A more robust implementation for running dbt in production that first tests source data quality, then runs models, tests non-source objects, and checks source freshness. This approach prevents bad source data from affecting downstream models and allows monitoring data freshness.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-11-22-exact-commands-we-run-in-production.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndbt test -s source:* (or dbt test -m source:* if you are on a version earlier than dbt v0.21.0)\n\ndbt run\n\ndbt test --exclude source:*\n\ndbt source `freshness ` (or dbt source snapshot-freshness if you are on a version earlier to dbt v0.21.0) (this could optionally be the first step)\n```\n\n----------------------------------------\n\nTITLE: Documenting Columns and Relations in YAML\nDESCRIPTION: This snippet shows how to provide descriptions for a model and its columns in a YAML file. These descriptions can be persisted in the database when persist_docs is enabled.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/persist_docs.md#2025-04-09_snippet_6\n\nLANGUAGE: yml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: dim_customers\n    description: One record per customer\n    columns:\n      - name: customer_id\n        description: Primary key\n```\n\n----------------------------------------\n\nTITLE: Defining Constraints in Snowflake dbt Model\nDESCRIPTION: This snippet demonstrates how to define constraints in a Snowflake dbt model. It includes not_null and primary_key constraints, noting that only not_null is enforced in Snowflake.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/constraints.md#2025-04-09_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: dim_customers\n    config:\n      contract:\n        enforced: true\n    columns:\n      - name: id\n        data_type: integer\n        description: hello\n        constraints:\n          - type: not_null\n          - type: primary_key # not enforced  -- will warn & include\n          - type: check       # not supported -- will warn & skip\n            expression: \"id > 0\"\n        tests:\n          - unique            # need this test because primary_key constraint is not enforced\n      - name: customer_name\n        data_type: text\n      - name: first_transaction_date\n        data_type: date\n```\n\n----------------------------------------\n\nTITLE: Compiled SQL for Insert_Overwrite Strategy in Databricks\nDESCRIPTION: This shows the compiled SQL for an incremental model using the insert_overwrite strategy with partitioning. The code creates a temporary view with filtered data and then overwrites the specified partitions in the target table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/databricks-configs.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\ncreate temporary view databricks_incremental__dbt_tmp as\n\n    with new_events as (\n\n        select * from analytics.events\n\n\n        where date_day >= date_add(current_date, -1)\n\n\n    )\n\n    select\n        date_day,\n        count(*) as users\n\n    from events\n    group by 1\n\n;\n\ninsert overwrite table analytics.databricks_incremental\n    partition (date_day)\n    select `date_day`, `users` from databricks_incremental__dbt_tmp\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Oracle Wallet (Thin Mode)\nDESCRIPTION: Sets environment variables for wallet location, password, and TNS admin directory in thin mode.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/oracle-setup.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport WALLET_LOCATION=/path/to/directory_containing_ewallet.pem\nexport WALLET_PASSWORD=***\nexport TNS_ADMIN=/path/to/directory_containing_tnsnames.ora\n```\n\n----------------------------------------\n\nTITLE: BigQuery Copy Partitions Log Output Example\nDESCRIPTION: Shows the log output when using the copy_partitions feature with an incremental model. The log demonstrates how dbt copies the temporary table partitions to the target table using BigQuery's copy table API.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/bigquery-configs.md#2025-04-09_snippet_12\n\nLANGUAGE: plaintext\nCODE:\n```\n...\n[0m16:03:13.017641 [debug] [Thread-3 (]: BigQuery adapter: Copying table(s) \"/projects/projectname/datasets/analytics/tables/bigquery_table__dbt_tmp$20230112\" to \"/projects/projectname/datasets/analytics/tables/bigquery_table$20230112\" with disposition: \"WRITE_TRUNCATE\"\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring Seeds in dbt_project.yml for IBM Netezza\nDESCRIPTION: This snippet demonstrates how to configure column data types for seed files in the dbt_project.yml file. It's crucial for leveraging full datatype support in the dbt-ibm-netezza adapter.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/ibm-netezza-config.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  my_project:\n    my_seed:\n      +column_types:\n        user_id: integer\n        created_at: timestamp\n        email: varchar(255)\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Secrets in fly.io\nDESCRIPTION: Command to set required secret environment variables in fly.io for authentication with dbt Cloud and PagerDuty. These include the dbt Cloud service token, webhook authentication token, and PagerDuty routing key.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/serverless-pagerduty.md#2025-04-09_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nflyctl secrets set DBT_CLOUD_SERVICE_TOKEN=abc123 DBT_CLOUD_AUTH_TOKEN=def456 PD_ROUTING_KEY=ghi789\n```\n\n----------------------------------------\n\nTITLE: Configuring Lookback in SQL Model\nDESCRIPTION: Sets the lookback configuration directly in the SQL model file using a config block.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/lookback.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    lookback=2\n) }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Dynamic Tables in Project YAML\nDESCRIPTION: Configuration settings for dynamic tables in dbt_project.yml file, including materialization type, configuration change behavior, target lag, and warehouse settings.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snowflake-configs.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  [<resource-path>]:\n    [+]materialized: dynamic_table\n    [+]on_configuration_change: apply | continue | fail\n    [+]target_lag: downstream | <time-delta>\n    [+]snowflake_warehouse: <warehouse-name>\n    [+]refresh_mode: AUTO | FULL | INCREMENTAL\n    [+]initialize: ON_CREATE | ON_SCHEDULE\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Cloud Job Commands for Databricks Production\nDESCRIPTION: Set up a production job in dbt Cloud to run source freshness checks, test source data, and build models excluding sources. This job configuration ensures data quality and efficient model building in a Databricks environment.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/productionize-your-dbt-databricks-project.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- dbt source freshness\n- dbt test --models source:*\n- dbt build --exclude source:* --fail-fast\n```\n\n----------------------------------------\n\nTITLE: SQL DISTINCT with COUNT Example\nDESCRIPTION: Shows how to compare total counts versus distinct counts using the Jaffle Shop orders table, useful for analyzing customer order patterns.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/statements/sql-distinct.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\tcount(customer_id) as cnt_all_orders,\n\tcount(distinct customer_id) as cnt_distinct_customers\nfrom {{ ref('orders') }}\n```\n\n----------------------------------------\n\nTITLE: Granting Future Raw Database Access to Transformer Role in Snowflake\nDESCRIPTION: SQL commands to grant the transformer role read-only access to all future objects in the raw database, enabling data transformation processes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/database-permissions/snowflake-permissions.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\ngrant usage on database raw to role transformer;\ngrant usage on future schemas in database raw to role transformer;\ngrant select on future tables in database raw to role transformer;\ngrant select on future views in database raw to role transformer;\n```\n\n----------------------------------------\n\nTITLE: Installing dbt Core and DuckDB on Windows using Virtual Environment\nDESCRIPTION: Commands to create a Python virtual environment, activate it, upgrade pip, and install the required dependencies for dbt Core with DuckDB on Windows using Command Prompt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/duckdb-qs.md#2025-04-09_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npython -m venv venv\nvenv\\Scripts\\activate.bat\npython -m pip install --upgrade pip\npython -m pip install -r requirements.txt\nvenv\\Scripts\\activate.bat\n```\n\n----------------------------------------\n\nTITLE: Working with Column Objects in dbt Jinja Templates\nDESCRIPTION: This snippet demonstrates creating and using Column objects in dbt templates, including creating string and numeric columns, checking data types, and using static methods to generate database-specific type specifications.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-classes.md#2025-04-09_snippet_3\n\nLANGUAGE: jinja2\nCODE:\n```\n-- String column\n{%- set string_column = api.Column('name', 'varchar', char_size=255) %}\n\n-- Return true if the column is a string\n{{ string_column.is_string() }}\n\n-- Return true if the column is a numeric\n{{ string_column.is_numeric() }}\n\n-- Return true if the column is a number\n{{ string_column.is_number() }}\n\n-- Return true if the column is an integer\n{{ string_column.is_integer() }}\n\n-- Return true if the column is a float\n{{ string_column.is_float() }}\n\n-- Numeric column\n{%- set numeric_column = api.Column('distance_traveled', 'numeric', numeric_precision=12, numeric_scale=4) %}\n\n-- Return true if the column is a string\n{{ numeric_column.is_string() }}\n\n-- Return true if the column is a numeric\n{{ numeric_column.is_numeric() }}\n\n-- Return true if the column is a number\n{{ numeric_column.is_number() }}\n\n-- Return true if the column is an integer\n{{ numeric_column.is_integer() }}\n\n-- Return true if the column is a float\n{{ numeric_column.is_float() }}\n\n-- Static methods\n\n-- Return the string data type for this database adapter with a given size\n{{ api.Column.string_type(255) }}\n\n-- Return the numeric data type for this database adapter with a given precision and scale\n{{ api.Column.numeric_type('numeric', 12, 4) }}\n```\n\n----------------------------------------\n\nTITLE: Using the as_native filter for Jinja-to-Python type conversion\nDESCRIPTION: The as_native filter coerces Jinja-compiled output into native Python types (set, list, tuple, dict, etc.) using ast.literal_eval. For boolean and numeric values specifically, it's recommended to use as_bool and as_number filters instead.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/as_native.md#2025-04-09_snippet_0\n\nLANGUAGE: jinja\nCODE:\n```\n{{ jinja_output | as_native }}\n```\n\n----------------------------------------\n\nTITLE: Accessing Custom Metadata Environment Variables in SQL\nDESCRIPTION: Demonstrates how to access custom metadata environment variables using the dbt_metadata_envs context variable in SQL files.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/env_var.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n-- {{ dbt_metadata_envs }}\n\nselect 1 as id\n```\n\n----------------------------------------\n\nTITLE: Attaching Groups to Models in YAML Files for Notifications\nDESCRIPTION: This example demonstrates how to attach groups to models in a YAML file. By assigning groups to models, you specify which team should receive notifications about each model's status and test results.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/model-notifications.md#2025-04-09_snippet_1\n\nLANGUAGE: yml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: sales\n    description: \"Sales data model\"\n    config:\n      group: finance\n\n  - name: campaigns\n    description: \"Campaigns data model\"\n    config:\n      group: marketing\n```\n\n----------------------------------------\n\nTITLE: Metric-based Selection\nDESCRIPTION: Selecting resources upstream of specific metrics\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/methods.md#2025-04-09_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ndbt build --select \"+metric:weekly_active_users\"       # build all resources upstream of weekly_active_users metric\ndbt ls    --select \"+metric:*\" --resource-type source  # list all source tables upstream of all metrics\n```\n\n----------------------------------------\n\nTITLE: Querying Source Freshness with GraphQL\nDESCRIPTION: This GraphQL query checks source freshness, providing the latest metadata about source loading and information about the freshness check criteria. It's useful for ensuring that sources loaded and used in your dbt project are compliant with expectations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/discovery-use-cases-and-examples.md#2025-04-09_snippet_10\n\nLANGUAGE: graphql\nCODE:\n```\nquery ($environmentId: BigInt!, $first: Int!) {\n  environment(id: $environmentId) {\n    applied {\n      sources(\n        first: $first\n        filter: { freshnessChecked: true, database: \"production\" }\n      ) {\n        edges {\n          node {\n            sourceName\n            name\n            identifier\n            loader\n            freshness {\n              freshnessJobDefinitionId\n              freshnessRunId\n              freshnessRunGeneratedAt\n              freshnessStatus\n              freshnessChecked\n              maxLoadedAt\n              maxLoadedAtTimeAgoInS\n              snapshottedAt\n              criteria {\n                errorAfter {\n                  count\n                  period\n                }\n                warnAfter {\n                  count\n                  period\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Project-Level Cache Configuration in dbt_project.yml\nDESCRIPTION: YAML configuration for enabling declarative caching at the project level in dbt_project.yml file. This allows for global cache configuration across saved queries.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/use-dbt-semantic-layer/sl-cache.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nsaved-queries:\n  my_saved_query:\n    config:\n      +cache:\n        enabled: true\n```\n\n----------------------------------------\n\nTITLE: Semantic Layer CI Validation Command\nDESCRIPTION: Command to validate semantic nodes (metrics, models, saved queries) during CI pipeline execution. Used to test semantic nodes and verify code changes don't break existing metrics.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/2024-release-notes.md#2025-04-09_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ndbt sl validate\n```\n\n----------------------------------------\n\nTITLE: Converting UPDATE to dbt Model in SQL\nDESCRIPTION: Illustrates the process of converting an UPDATE statement to a dbt model. The example shows how to update the 'type' column in an orders table based on a condition.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/migrate-from-stored-procedures.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nUPDATE orders\n\nSET type = 'return'\n\nWHERE total < 0\n```\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT\n    CASE\n        WHEN total < 0 THEN 'return'\n        ELSE type\n    END AS type,\n\n    order_id,\n    order_date\n\nFROM {{ ref('stg_orders') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Profiles for Azure Synapse\nDESCRIPTION: Example YAML configuration for setting up dbt profiles.yml to connect to Azure Synapse Analytics. Includes essential connection parameters like server, database, authentication, and driver settings.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/azuresynapse-setup.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: synapse\n      driver: 'ODBC Driver 17 for SQL Server'\n      server: workspacename.sql.azuresynapse.net\n      port: 1433\n      database: exampledb\n      schema: schema_name\n      user: username\n      password: password\n```\n\n----------------------------------------\n\nTITLE: Creating Internal Links with Relative Paths in Markdown\nDESCRIPTION: Examples of how to create internal links using relative paths in the dbt documentation. This demonstrates the proper format for linking to other pages within the documentation site using the /docs/, /guides/, /reference/, or /community/ path prefixes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/contributing/content-style-guide.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\nFor more information about server availability, please refer to our [Regions & IP Addresses page](/docs/cloud/about-cloud/access-regions-ip-addresses)\n```\n\n----------------------------------------\n\nTITLE: Displaying Project Permissions Table using SortableTable in Markdown\nDESCRIPTION: This code snippet defines a table of project-level permissions for different roles in dbt Cloud. It uses a SortableTable component to display the data, with rows for different project aspects and columns for various roles. The table uses 'W' for Write access, 'R' for Read access, and '-' for no access.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_enterprise-permissions-table.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n<SortableTable> \n\n{`\n|Project-level permission  | Admin | Analyst | Database admin | Developer | Git Admin | Job admin | Job runner  | Job viewer  | Metadata (Discovery API only) | Semantic Layer | Stakeholder | Team admin |\n|--------------------------|:-----:|:-------:|:--------------:|:---------:|:---------:|:---------:|:-----------:|:-----------:|:---------------------------------------:|:--------------:|:-----------:|:----------:| \n| Environment credentials  |   W   |    W    |       W        |     W     |     R     |     W     |    -        |      -      |                  -                      |        -       |     R       |     R      |\n| Custom env. variables    |   W   |    W#  |       W         |     W#   |     W     |     W     |     -       |      R      |                  -                      |        -       |     R       |     W      |\n| Data platform configs    |   W   |    W    |       W        |     W     |     R     |     W     |     -       |      -      |                  -                      |       -        |     R       |     R      |\n| Develop (IDE or CLI)     |   W   |    W    |       -        |     W     |     -     |     -     |     -       |      -      |                  -                      |       -        |     -       |      -     |\n| Environments             |   W   |    R    |       R        |     R     |     R     |     W     |      -      |      R      |                  -                      |       -        |     R       |     R      |\n| Jobs                     |   W   |    R*   |       R*       |     R*    |     R*    |     W     |      R      |      R      |                  -                      |       -        |     R       |     R*     |\n| Metadata GraphQL API access| R   |    R    |       R        |     R     |     R     |     R     |      -      |      R      |                  R                      |       -        |     R       |     R      |\n| Permissions              |   W   |    -    |       R        |     R     |     R     |     -     |      -      |      -      |                  -                      |       -        |     -       |     R      |\n| Projects                 |   W   |    R    |       W        |     R     |     W     |     R     |      -      |      R      |                  -                      |       -        |     R       |     W      |\n| Repositories             |   W   |   -     |       R        |     R     |     W     |     -     |      -      |      -      |                  -                      |       -        |     R       |     R      |\n| Runs                     |   W   |    R*   |       R*       |     R*    |     R*    |     W     |      W      |      R      |                  -                      |       -        |     R       |     R*     |\n| Semantic Layer config    |   W   |    R    |       W        |     R     |     R     |     R     |      -      |      -      |                  -                      |        W       |     R       |     R      |\n`}\n\n</SortableTable>\n```\n\n----------------------------------------\n\nTITLE: Configuring Snapshots with Legacy SQL-Based Syntax\nDESCRIPTION: Example of a snapshot file using legacy SQL-based configurations with a Jinja block. This demonstrates the basic structure of a snapshot file with configuration options.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snapshots-jinja-legacy.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{ % snapshot orders_snapshot %}\n\n{{ config(\n    [target_schema](/reference/resource-configs/target_schema)=\"<string>\",\n    [target_database](/reference/resource-configs/target_database)=\"<string>\",\n    [unique_key](/reference/resource-configs/unique_key)=\"<column_name_or_expression>\",\n    [strategy](/reference/resource-configs/strategy)=\"timestamp\" | \"check\",\n    [updated_at](/reference/resource-configs/updated_at)=\"<column_name>\",\n    [check_cols](/reference/resource-configs/check_cols)=[\"<column_name>\"] | \"all\"\n    [invalidate_hard_deletes](/reference/resource-configs/invalidate_hard_deletes) : true | false\n) \n}}\n\nselect * from {{ source('jaffle_shop', 'orders') }}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Partitioning for Oracle Incremental Materialization in dbt\nDESCRIPTION: This snippet demonstrates how to configure partitioning for incremental materialization in Oracle using dbt. It includes settings for parallel execution, hash partitioning, and column store compression.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/oracle-configs.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{\n    config(\n        materialized='incremental',\n        unique_key='group_id',\n        parallel=4,\n        partition_config={\"clause\": \"PARTITION BY HASH(PROD_NAME) PARTITIONS 4\"},\n        table_compression_clause='COLUMN STORE COMPRESS FOR QUERY LOW')\n}}\nSELECT *\nFROM {{ source('sh_database', 'sales') }}\n```\n\n----------------------------------------\n\nTITLE: Job-based Query After Deprecation - GraphQL\nDESCRIPTION: Updated job-based query using BigInt data type for IDs after the September 2023 deprecation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/2023-release-notes.md#2025-04-09_snippet_2\n\nLANGUAGE: graphql\nCODE:\n```\nquery ($jobId: BigInt!) {\n    job(id: $jobId) {\n        models {\n        uniqueId\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Surrogate Key for Uniqueness Testing in SQL\nDESCRIPTION: This snippet demonstrates how to concatenate two columns to create a surrogate key in a SQL model, which can then be tested for uniqueness.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Tests/uniqueness-two-columns.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n  country_code || '-' || order_id as surrogate_key,\n  ...\n\n```\n\n----------------------------------------\n\nTITLE: Configuring persist_docs for Snapshots in YAML files\nDESCRIPTION: This snippet shows how to configure persist_docs for snapshots in a YAML file. It enables persisting documentation for both relations and columns. This configuration is available from dbt version 1.9 onwards.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/persist_docs.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsnapshots:\n  - name: snapshot_name\n    config:\n      persist_docs:\n        relation: true\n        columns: true\n```\n\n----------------------------------------\n\nTITLE: Creating External Materialization Model in dbt-duckdb SQL\nDESCRIPTION: Shows how to create a dbt model that uses external materialization in dbt-duckdb. The model writes its output to an external Parquet file and demonstrates joining with other models and sources.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/duckdb-configs.md#2025-04-09_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(materialized='external', location='local/directory/file.parquet') \n}}\n\nSELECT m.*, s.id IS NOT NULL as has_source_id\nFROM {{ ref('upstream_model') }} m\nLEFT JOIN {{ source('upstream', 'source') }} s USING (id)\n```\n\n----------------------------------------\n\nTITLE: Using dbt invocation help command in shell\nDESCRIPTION: Demonstrates how to access the help documentation for the dbt invocation command. This command displays all available subcommands and flags for the invocation feature.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/invocation.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndbt invocation help\n```\n\n----------------------------------------\n\nTITLE: Creating Virtual Environment on Unix/macOS\nDESCRIPTION: Commands to create and activate a Python virtual environment on Unix/macOS systems, including environment creation, activation, and Python path verification.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/pip-install.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython3 -m venv env\n```\n\nLANGUAGE: shell\nCODE:\n```\nsource env/bin/activate\n```\n\nLANGUAGE: shell\nCODE:\n```\nwhich python\n```\n\nLANGUAGE: shell\nCODE:\n```\nenv/bin/python\n```\n\n----------------------------------------\n\nTITLE: Configuring Project-level Saved Queries in dbt_project.yml\nDESCRIPTION: Example of enabling saved queries at the project level by setting the saved-queries configuration in the dbt_project.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/saved-queries.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nsaved-queries:\n  my_saved_query:\n    +cache:\n      enabled: true\n```\n\n----------------------------------------\n\nTITLE: Running dbt retry without fixing the error\nDESCRIPTION: This snippet shows what happens when running dbt retry without fixing the syntax error in the customers model. The retry command only attempts to run the failed model but encounters the same error.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/retry.md#2025-04-09_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nRunning with dbt=1.6.1\nRegistered adapter: duckdb=1.6.0\nFound 5 models, 3 seeds, 20 tests, 0 sources, 0 exposures, 0 metrics, 348 macros, 0 groups, 0 semantic models\n\nConcurrency: 24 threads (target='dev')\n\n1 of 1 START sql table model main.customers .................................... [RUN]\n1 of 1 ERROR creating sql table model main.customers ........................... [ERROR in 0.03s]\n\nDone. PASS=4 WARN=0 ERROR=1 SKIP=0 TOTAL=5\n```\n\n----------------------------------------\n\nTITLE: Configuring MySQL Source in YAML\nDESCRIPTION: This snippet illustrates the YAML configuration options for integrating data from a MySQL source. It covers both source and job options, including table and column filters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/upsolver-configs.md#2025-04-09_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\ntable_include_list: ('<regexFilter>', ...)\ncolumn_exclude_list: ('<regexFilter>', ...)\nexclude_columns: ('<exclude_column>', ...)\ncolumn_transformations:\n  '<column>': '<expression>'\nskip_snapshots: True/False\nend_at: '<timestamp>/NOW'\ncompute_cluster: '<compute_cluster>'\nsnapshot_parallelism: <integer>\nddl_filters: ('<filter>', ...)\ncomment: '<comment>'\n```\n\n----------------------------------------\n\nTITLE: Referencing DBT eager Mode\nDESCRIPTION: Code reference showing the usage of the eager mode parameter in DBT, which runs tests if any parent nodes are selected.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_indirect-selection-definitions.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\neager\n```\n\n----------------------------------------\n\nTITLE: Naming Images with Snake_case or Kebab-case Conventions\nDESCRIPTION: Demonstrates proper naming conventions for image files in the documentation, using either snake_case or kebab-case formatting for descriptive filenames.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/contributing/content-style-guide.md#2025-04-09_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n* Types and Examples\n  * `snake_case`\n    * *gitlab_setup.jpg*\n  * `kebab-case`\n    * *sso-setup.jpg*\n```\n\n----------------------------------------\n\nTITLE: Displaying Usage and Options for dbt debug Command\nDESCRIPTION: This code snippet shows the usage and available options for the dbt debug command. It includes various flags for configuring logging, cache behavior, state management, and other debugging options.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/debug.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nUsage: dbt debug [OPTIONS]\n\n Show information on the current dbt environment and check dependencies, then\n test the database connection. Not to be confused with the --debug option\n which increases verbosity.\n\nOptions:\n --cache-selected-only / --no-cache-selected-only\n                At start of run, populate relational cache\n                only for schemas containing selected nodes,\n                or for all schemas of interest.\n\n -d, --debug / --no-debug    \n                Display debug logging during dbt execution.\n                Useful for debugging and making bug reports.\n\n --defer / --no-defer      \n                If set, resolve unselected nodes by\n                deferring to the manifest within the --state\n                directory.\n\n --defer-state DIRECTORY     \n                Override the state directory for deferral\n                only.\n\n --deprecated-favor-state TEXT  \n                Internal flag for deprecating old env var.\n\n -x, --fail-fast / --no-fail-fast\n                 Stop execution on first failure.\n\n --favor-state / --no-favor-state\n                If set, defer to the argument provided to\n                the state flag for resolving unselected\n                nodes, even if the node(s) exist as a\n                database object in the current environment.\n\n --indirect-selection [eager|cautious|buildable|empty]\n                Choose which tests to select that are\n                adjacent to selected resources. Eager is\n                most inclusive, cautious is most exclusive,\n                and buildable is in between. Empty includes\n                no tests at all.\n\n --log-cache-events / --no-log-cache-events\n                Enable verbose logging for relational cache\n                events to help when debugging.\n\n --log-format [text|debug|json|default]\n                Specify the format of logging to the console\n                and the log file. Use --log-format-file to\n                configure the format for the log file\n                differently than the console.\n\n --log-format-file [text|debug|json|default]\n                Specify the format of logging to the log\n                file by overriding the default value and the\n                general --log-format setting.\n\n --log-level [debug|info|warn|error|none]\n                Specify the minimum severity of events that\n                are logged to the console and the log file.\n                Use --log-level-file to configure the\n                severity for the log file differently than\n                the console.\n\n --log-level-file [debug|info|warn|error|none]\n                Specify the minimum severity of events that\n                are logged to the log file by overriding the\n                default value and the general --log-level\n                setting.\n\n --log-path PATH         \n                Configure the 'log-path'. Only applies this\n                setting for the current run. Overrides the\n                'DBT_LOG_PATH' if it is set.\n\n --partial-parse / --no-partial-parse\n                Allow for partial parsing by looking for and\n                writing to a pickle file in the target\n                directory. This overrides the user\n                configuration file.\n\n --populate-cache / --no-populate-cache\n                At start of run, use `show` or\n                `information_schema` queries to populate a\n                relational cache, which can speed up\n                subsequent materializations.\n\n --print / --no-print      \n                Output all {{ print() }} macro calls.\n\n --printer-width INTEGER     \n                Sets the width of terminal output\n\n --profile TEXT         \n                Which existing profile to load. Overrides\n                setting in dbt_project.yml.\n\n -q, --quiet / --no-quiet    \n                Suppress all non-error logging to stdout.\n                Does not affect {{ print() }} macro calls.\n\n -r, --record-timing-info PATH  \n                When this option is passed, dbt will output\n                low-level timing stats to the specified\n                file. Example: `--record-timing-info\n                output.profile`\n\n --send-anonymous-usage-stats / --no-send-anonymous-usage-stats\n                Send anonymous usage stats to dbt Labs.\n\n --state DIRECTORY        \n                Unless overridden, use this state directory\n                for both state comparison and deferral.\n\n --static-parser / --no-static-parser\n                Use the static parser.\n\n -t, --target TEXT        \n                Which target to load for the given profile\n\n --use-colors / --no-use-colors \n                Specify whether log output is colorized in\n                the console and the log file. Use --use-\n                colors-file/--no-use-colors-file to colorize\n                the log file differently than the console.\n\n --use-colors-file / --no-use-colors-file\n                Specify whether log file output is colorized\n                by overriding the default value and the\n                general --use-colors/--no-use-colors\n                setting.\n\n --use-experimental-parser / --no-use-experimental-parser\n                Enable experimental parsing features.\n\n -V, -v, --version        \n                Show version information and exit\n\n --version-check / --no-version-check\n                If set, ensure the installed dbt version\n                matches the require-dbt-version specified in\n                the dbt_project.yml file (if any).\n                Otherwise, allow them to differ.\n\n --warn-error   \n                If dbt would normally warn, instead raise an\n                exception. Examples include --select that\n                selects nothing, deprecations,\n                configurations with no associated models,\n                invalid test configurations, and missing\n                sources/refs in tests.\n\n --warn-error-options WARNERROROPTIONSTYPE\n                If dbt would normally warn, instead raise an\n                exception based on include/exclude\n                configuration. Examples include --select\n                that selects nothing, deprecations,\n                configurations with no associated models,\n                invalid test configurations, and missing\n                sources/refs in tests. This argument should\n                be a YAML string, with keys 'include' or\n                'exclude'. eg. '{\"include\": \"all\",\n                \"exclude\": [\"NoNodesForSelectionCriteria\"]}'\n\n --write-json / --no-write-json \n                Whether or not to write the manifest.json\n                and run_results.json files to the target\n                directory\n\n --connection          \n                Test the connection to the target database\n                independent of dependency checks.\n                Available in dbt Cloud IDE and dbt Core CLI\n\n --config-dir          \n                Print a system-specific command to access\n                the directory that the current dbt project\n                is searching for a profiles.yml. Then, exit.\n                This flag renders other debug step flags no-\n                ops.\n\n --profiles-dir PATH       \n                Which directory to look in for the\n                profiles.yml file. If not set, dbt will look\n                in the current working directory first, then\n                HOME/.dbt/\n\n --project-dir PATH       \n                Which directory to look in for the\n                dbt_project.yml file. Default is the current\n                working directory and its parents.\n\n --vars YAML           \n                Supply variables to the project. This\n                argument overrides variables defined in your\n                dbt_project.yml file. This argument should\n                be a YAML string, eg. '{my_variable:\n                my_value}'\n\n -h, --help           \n                Show this message and exit.\n```\n\n----------------------------------------\n\nTITLE: Implementing Alias Mapping for dbt Adapter Credentials in Python\nDESCRIPTION: Demonstrates how to add an ALIASES mapping to a Credentials class, allowing users to use alternative names for database configuration parameters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass MyAdapterCredentials(Credentials):\n    host: str\n    port: int = 1337\n    username: Optional[str] = None\n    password: Optional[str] = None\n\n    ALIASES = {\n        'collection': 'database',\n    }\n```\n\n----------------------------------------\n\nTITLE: Navigating to Project Directory in Shell\nDESCRIPTION: Command to change directory to the cloned repository location\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/serverless-datadog.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncd ~/Documents/GitHub/dbt-cloud-webhooks-datadog\n```\n\n----------------------------------------\n\nTITLE: Boolean Config Examples\nDESCRIPTION: Examples of using boolean configuration flags to enable or disable features.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/command-line-options.md#2025-04-09_snippet_5\n\nLANGUAGE: text\nCODE:\n```\ndbt run --version-check\ndbt run --no-version-check \n```\n\n----------------------------------------\n\nTITLE: Configuring persist_docs for Seeds in dbt_project.yml\nDESCRIPTION: This snippet shows how to configure persist_docs for seeds in the dbt_project.yml file. It enables persisting documentation for both relations and columns.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/persist_docs.md#2025-04-09_snippet_2\n\nLANGUAGE: yml\nCODE:\n```\nseeds:\n  [<resource-path>]:\n    +persist_docs:\n      relation: true\n      columns: true\n```\n\n----------------------------------------\n\nTITLE: Using Cross-Project References in SQL Queries\nDESCRIPTION: This SQL snippet demonstrates how to use cross-project references for public models in an upstream project, including the use of versioning for versioned models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-10-04-hybrid-mesh.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect * from {{ ref('upstream_project_name', 'monthly_revenue') }}\n```\n\n----------------------------------------\n\nTITLE: Implementing Traditional Incremental Strategy with Manual Filtering in dbt\nDESCRIPTION: Example of a traditional incremental model using 'delete+insert' strategy where the user manually defines the filtering logic with a 3-day lookback window to account for late-arriving records. The model identifies new records based on date_day values.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/incremental-microbatch.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\n{{\\n    config(\\n        materialized='incremental',\\n        incremental_strategy='delete+insert',\\n        unique_key='date_day'\\n    )\\n}}\\n\\nselect * from {{ ref('stg_events') }}\\n\\n    {% if is_incremental() %}\\n        -- this filter will only be applied on an incremental run\\n        -- add a lookback window of 3 days to account for late-arriving records\\n        where date_day >= (select {{ dbt.dateadd(\"day\", -3, \"max(date_day)\") }} from {{ this }})  \\n    {% endif %}\\n\n```\n\n----------------------------------------\n\nTITLE: Documentation for Intermediate Models in Markdown\nDESCRIPTION: Provides detailed descriptions for each intermediate model using dbt's documentation syntax. These descriptions explain the purpose and relationships for each model, which will be referenced in the YAML configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_18\n\nLANGUAGE: markdown\nCODE:\n```\n# the intent of this .md is to allow for multi-line long form explanations for our intermediate transformations\n\n# below are descriptions \n{% docs int_results %} In this query we want to join out other important information about the race results to have a human readable table about results, races, drivers, constructors, and status. \nWe will have 4 left joins onto our results table. {% enddocs %}\n\n{% docs int_pit_stops %} There are many pit stops within one race, aka a M:1 relationship. \nWe want to aggregate this so we can properly join pit stop information without creating a fanout.  {% enddocs %}\n\n{% docs int_lap_times_years %} Lap times are done per lap. We need to join them out to the race year to understand yearly lap time trends. {% enddocs %}\n```\n\n----------------------------------------\n\nTITLE: Granting Future Analytics Database Access to Reporter Role in Snowflake\nDESCRIPTION: SQL commands to grant the reporter role read-only access to all future objects in the analytics database, enabling data reporting and analysis.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/database-permissions/snowflake-permissions.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\ngrant usage on database analytics to role reporter;\ngrant usage on future schemas in database analytics to role reporter;\ngrant select on future tables in database analytics to role reporter;\ngrant select on future views in database analytics to role reporter;\n```\n\n----------------------------------------\n\nTITLE: Defining Column Objects in Python for dbt\nDESCRIPTION: The Column class represents column information including name, data type and size specifications. This snippet shows how to create column objects and use their type-checking methods.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-classes.md#2025-04-09_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass Column(object):\n  def __init__(self, column, dtype, char_size=None, numeric_size=None):\n    \"\"\"\n      column: The name of the column represented by this object\n      dtype: The data type of the column (database-specific)\n      char_size: If dtype is a variable width character type, the size of the column, or else None\n      numeric_size: If dtype is a fixed precision numeric type, the size of the column, or else None\n   \"\"\"\n\n\n# Example usage:\ncol = Column('name', 'varchar', 255)\ncol.is_string() # True\ncol.is_numeric() # False\ncol.is_number() # False\ncol.is_integer() # False\ncol.is_float() # False\ncol.string_type() # character varying(255)\ncol.numeric_type('numeric', 12, 4) # numeric(12,4)\n```\n\n----------------------------------------\n\nTITLE: Single Quote Escaping in SQL with dbt\nDESCRIPTION: Macro to escape single quotes within string literals. Works on column names, not values.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/cross-database-macros.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt.escape_single_quotes(\"they're\") }}\n{{ dbt.escape_single_quotes(\"ain't ain't a word\") }}\n```\n\n----------------------------------------\n\nTITLE: Dictionary Syntax for query-comment in dbt\nDESCRIPTION: Advanced dictionary syntax for configuring query comments with options for comment content, append behavior, and BigQuery job labeling.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/query-comment.md#2025-04-09_snippet_1\n\nLANGUAGE: yml\nCODE:\n```\nmodels:\n  my_dbt_project:\n    +materialized: table\n\nquery-comment:\n  comment: string\n  append: true | false\n  job-label: true | false  # BigQuery only\n```\n\n----------------------------------------\n\nTITLE: Configuring Group Assignment at Project Level\nDESCRIPTION: Shows how to assign models to a group using project-level configuration in dbt_project.yml.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/groups.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  marts:\n    finance:\n      +group: finance\n```\n\n----------------------------------------\n\nTITLE: Test Type Selection in dbt\nDESCRIPTION: Examples of selecting tests based on their type (unit, data, generic, or singular tests).\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/methods.md#2025-04-09_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\ndbt test --select \"test_type:unit\"           # run all unit tests\ndbt test --select \"test_type:data\"           # run all data tests\ndbt test --select \"test_type:generic\"        # run all generic data tests\ndbt test --select \"test_type:singular\"       # run all singular data tests\n```\n\n----------------------------------------\n\nTITLE: Configuring Table Materialization in dbt for ClickHouse\nDESCRIPTION: This snippet demonstrates how to configure a dbt model as a ClickHouse table, including options for engine type, order_by, and partition_by using either the project YAML file or a config block in the model SQL file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/clickhouse-configs.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  <resource-path>:\n    +materialized: table\n    +order_by: [ <column-name>, ... ]\n    +engine: <engine-type>\n    +partition_by: [ <column-name>, ... ]\n```\n\nLANGUAGE: jinja\nCODE:\n```\n{{ config(\n    materialized = \"table\",\n    engine = \"<engine-type>\",\n    order_by = [ \"<column-name>\", ... ],\n    partition_by = [ \"<column-name>\", ... ],\n      ...\n    ]\n) }}\n```\n\n----------------------------------------\n\nTITLE: Path-based Selection\nDESCRIPTION: Selecting models based on their file path\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/methods.md#2025-04-09_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n# These two selectors are equivalent\ndbt run --select \"path:models/staging/github\"\ndbt run --select \"models/staging/github\"\n\n# These two selectors are equivalent\ndbt run --select \"path:models/staging/github/stg_issues.sql\"\ndbt run --select \"models/staging/github/stg_issues.sql\"\n```\n\n----------------------------------------\n\nTITLE: Environment Query After Deprecation - GraphQL\nDESCRIPTION: Updated environment query using BigInt data type for IDs after deprecation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/2023-release-notes.md#2025-04-09_snippet_6\n\nLANGUAGE: graphql\nCODE:\n```\nquery ($environmentId: BigInt!, $first: Int!) {\n    environment(id: $environmentId) {\n        applied {\n        models(first: $first) {\n            edges {\n            node {\n                uniqueId\n                executionInfo {\n                lastRunId\n                }\n            }\n            }\n        }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Test Information from a Job in GraphQL\nDESCRIPTION: This GraphQL query retrieves information about all tests in a specific job by ID, including test metadata like runId, accountId, projectId, uniqueId, name, columnName, and state.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/schema-discovery-job-tests.mdx#2025-04-09_snippet_0\n\nLANGUAGE: graphql\nCODE:\n```\n{\n  job(id: 123) {\n    tests {\n      runId\n      accountId\n      projectId\n      uniqueId\n      name\n      columnName\n      state\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Semantic Model Configuration in YAML\nDESCRIPTION: Initial configuration for a semantic model representing an orders fact table, including model name, defaults, and description.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/sl-snowflake-qs.md#2025-04-09_snippet_18\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_models:\n  - name: orders\n    defaults:\n      agg_time_dimension: order_date\n    description: |\n      Order fact table. This table's grain is one row per order.\n    model: ref('fct_orders')\n```\n\n----------------------------------------\n\nTITLE: Configuring Null-Safe Equality in dbt Snowflake Adapter\nDESCRIPTION: Configuration setting in dbt_project.yml to enable null-safe equality comparisons. When enabled, NULL = NULL evaluates to TRUE instead of UNKNOWN, affecting incremental and snapshot materializations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/snowflake-changes.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nenable_truthy_nulls_equals_macro: True\n```\n\n----------------------------------------\n\nTITLE: Adding a New Version in dbt-versions.js (JavaScript)\nDESCRIPTION: Example of how to add a new version object to the versions array in the dbt-versions.js file. This object includes the version number and its end-of-life date.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/contributing/single-sourcing-content.md#2025-04-09_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nexports.versions = [\n\t{\n\t\tversion: \"1.2\",\n\t\tEOLDate: \"2023-01-01\"\n\t}\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Materialization in dbt Property File (YAML)\nDESCRIPTION: This snippet demonstrates how to set the materialization type for a specific model in the properties.yml file. It uses the 'config' section to specify the materialization.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/materialized.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: <model_name>\n    config:\n      materialized: [<materialization_name>]\n```\n\n----------------------------------------\n\nTITLE: Writing Multi-line Descriptions Using | Symbol in dbt YAML\nDESCRIPTION: This example demonstrates writing a longer description that spans multiple lines using the '|' symbol in a dbt YAML file. Line breaks are preserved and Markdown formatting can be used. This method is recommended for more complex descriptions with structured content.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Docs/long-descriptions.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n  version: 2\n\n  models:\n  - name: customers\n    description: |\n      ### Lorem ipsum\n\n      * dolor sit amet, consectetur adipisicing elit, sed do eiusmod\n      * tempor incididunt ut labore et dolore magna aliqua.\n```\n\n----------------------------------------\n\nTITLE: Querying Model Timing Information in GraphQL\nDESCRIPTION: GraphQL query to retrieve execution timing details for a specific model, including start time, completion time, and total execution duration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/schema-discovery-job-model.mdx#2025-04-09_snippet_1\n\nLANGUAGE: graphql\nCODE:\n```\n{\n  job(id: 123) {\n    model(uniqueId: \"model.jaffle_shop.dim_user\") {\n      runId\n      projectId\n      name\n      uniqueId\n      resourceType\n      executeStartedAt\n      executeCompletedAt\n      executionTime\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating CSV Seed File with Country Codes\nDESCRIPTION: Demonstrates the structure of a CSV seed file containing country codes and names. This file can be used as a reference table in dbt models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Project/add-a-seed.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ncountry_code,country_name\nUS,United States\nCA,Canada\nGB,United Kingdom\n...\n```\n\n----------------------------------------\n\nTITLE: Example of a Firebolt Fact Table with Aggregating Index\nDESCRIPTION: Complete example of configuring a fact table in Firebolt with a primary index on 'id' and an aggregating index on 'order_id' that includes COUNT and AVG aggregations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/firebolt-configs.md#2025-04-09_snippet_4\n\nLANGUAGE: jinja\nCODE:\n```\n{{ config(\n    materialized = \"table\",\n    table_type = \"fact\",\n    primary_index = \"id\",\n    indexes = [\n      {\n        \"index_type\": \"aggregating\",\n        \"key_columns\": \"order_id\",\n        \"aggregation\": [\"COUNT(DISTINCT status)\", \"AVG(customer_id)\"]\n      }\n    ]\n) }}\n```\n\n----------------------------------------\n\nTITLE: Querying Job Information with GraphQL in dbt Cloud APIs\nDESCRIPTION: This GraphQL query demonstrates how to retrieve information about models and sources from a specific job. It includes querying for execution time of models and details about a specific source node.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/schema-discovery-job.mdx#2025-04-09_snippet_0\n\nLANGUAGE: graphql\nCODE:\n```\nquery JobQueryExample {\n  # Provide runId for looking at specific run, otherwise it defaults to latest run\n  job(id: 940) {\n    # Get all models from this job's latest run\n    models(schema: \"analytics\") {\n      uniqueId\n      executionTime\n    }\n\n    # Or query a single node\n    source(uniqueId: \"source.jaffle_shop.snowplow.event\") {\n      uniqueId\n      sourceName\n      name\n      state\n      maxLoadedAt\n      criteria {\n        warnAfter {\n          period\n          count\n        }\n        errorAfter {\n          period\n          count\n        }\n      }\n      maxLoadedAtTimeAgoInS\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Node.js with Homebrew\nDESCRIPTION: Command to install Node.js using Homebrew, which is required for running the Docusaurus site.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/README.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbrew install node\n```\n\n----------------------------------------\n\nTITLE: Configuring Fact Tables in models/properties.yml for Firebolt\nDESCRIPTION: Property file configuration to define fact tables in Firebolt with primary indexes and optional aggregating indexes. Used for setting model-specific configurations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/firebolt-configs.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: <model-name>\n    config:\n      materialized: table\n      table_type: fact\n      primary_index: [ <column-name>, ... ]\n      indexes:\n        - index_type: aggregating\n          key_columns: [ <column-name>, ... ]\n          aggregation: [ <agg-sql>, ... ]\n        ...\n```\n\n----------------------------------------\n\nTITLE: Calculating Monthly Average Order Amount Using SQL AVG\nDESCRIPTION: Query that calculates the rounded average order amount per month using the AVG function with date truncation. Excludes returned and pending return orders from the calculation. Uses the jaffle_shop sample dataset's orders table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/aggregate-functions/sql-avg.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\tdate_trunc('month', order_date) as order_month,\n\tround(avg(amount)) as avg_order_amount\nfrom {{ ref('orders') }}\nwhere status not in ('returned', 'return_pending')\ngroup by 1\n```\n\n----------------------------------------\n\nTITLE: Configuring Key-Based Distribution Style in dbt Redshift Models\nDESCRIPTION: Sets up a table materialization with key-based distribution using 'person_id' as the distribution key. This ensures related data across tables is co-located on the same node, optimizing join performance.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-05-19-redshift-configurations-dbt-model-optimizations.md#2025-04-09_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n{{ config(materialized='table', dist='person_id') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Extra JARs for Apache Hudi in YAML\nDESCRIPTION: This YAML snippet shows how to configure extra JARs in the classpath for using Apache Hudi with AWS Glue. It specifies the locations of necessary JAR files in an S3 bucket.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/glue-configs.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nextra_jars: \"s3://dbt-glue-hudi/Dependencies/hudi-spark.jar,s3://dbt-glue-hudi/Dependencies/spark-avro_2.11-2.4.4.jar\"\n```\n\n----------------------------------------\n\nTITLE: Cloning the Documentation Repository\nDESCRIPTION: Command to clone the dbt-labs documentation repository from GitHub.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/README.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/dbt-labs/docs.getdbt.com.git\n```\n\n----------------------------------------\n\nTITLE: Building and Running the Website Locally\nDESCRIPTION: Commands to build and run the website locally using either make or npm.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/README.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmake run\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm start\n```\n\n----------------------------------------\n\nTITLE: Configuring Firebolt Connection Profile in dbt\nDESCRIPTION: YAML configuration for connecting dbt to a Firebolt database. Includes required fields like client credentials, database name, engine name, account name, and optional configuration parameters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/firebolt-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\n<profile-name>:\n  target: <target-name>\n  outputs:\n    <target-name>:\n      type: firebolt\n      client_id: \"<id>\"\n      client_secret: \"<secret>\"\n      database: \"<database-name>\"\n      engine_name: \"<engine-name>\"\n      account_name: \"<account-name>\"\n      schema: <tablename-prefix>\n      threads: 1\n      #optional fields\n      host: \"<hostname>\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Response Retrieval Method for dbt Adapter in Python\nDESCRIPTION: Defines the 'get_response' classmethod for a ConnectionManager, which extracts and returns relevant information about the last executed database command.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n    @classmethod\n    def get_response(cls, cursor) -> AdapterResponse:\n        code = cursor.sqlstate or \"OK\"\n        rows = cursor.rowcount\n        status_message = f\"{code} {rows}\"\n        return AdapterResponse(\n            _message=status_message,\n            code=code,\n            rows_affected=rows\n        )\n```\n\n----------------------------------------\n\nTITLE: Configuring GitLab CI/CD Pipeline for SQLFluff Linting\nDESCRIPTION: YAML configuration for GitLab CI/CD that runs SQLFluff linting on push events to any branch except 'main'. It uses Python 3.9 to install and run SQLFluff against the models directory.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/set-up-ci.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nimage: python:3.9\n\nstages:\n  - pre-build\n\n# this job runs SQLFluff with a specific set of rules\n# note the dialect is set to Snowflake, so make that specific to your setup\n# details on linter rules: https://docs.sqlfluff.com/en/stable/rules.html\nlint-project:\n  stage: pre-build\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"push\" && $CI_COMMIT_BRANCH != 'main'\n  script:\n    - python -m pip install sqlfluff\n    - sqlfluff lint models --dialect snowflake\n```\n\n----------------------------------------\n\nTITLE: Testing a Surrogate Key for Uniqueness in dbt YAML\nDESCRIPTION: This YAML configuration defines a uniqueness test for the surrogate_key column created in the orders model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Tests/uniqueness-two-columns.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: orders\n    columns:\n      - name: surrogate_key\n        tests:\n          - unique\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Table Partitioning in Greenplum dbt Model\nDESCRIPTION: Sets up range partitioning on a timestamp field with daily partitions and a default partition. Requires separate field string definition and partition specification due to Greenplum's CREATE TABLE AS limitations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/greenplum-configs.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{% set fields_string %}\n    some_filed int4 null,\n    date_field timestamp NULL\n{% endset %}\n\n\n{% set raw_partition %}\n   PARTITION BY RANGE (date_field)\n   (\n       START ('2021-01-01'::timestamp) INCLUSIVE\n       END ('2023-01-01'::timestamp) EXCLUSIVE\n       EVERY (INTERVAL '1 day'),\n       DEFAULT PARTITION default_part\n   );\n{% endset %}\n\n{{\n   config(\n       ...\n       fields_string=fields_string,\n       raw_partition=raw_partition,\n       ...\n   )\n}}\n\nselect *\n```\n\n----------------------------------------\n\nTITLE: Querying Sales Order Data\nDESCRIPTION: Example SQL query to inspect the sales order header table data with limit of 10 records.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-04-18-building-a-kimball-dimensional-model-with-dbt.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nselect * from sales.salesorderheader limit 10;\n```\n\n----------------------------------------\n\nTITLE: Displaying Version Information in dbt Cloud CLI\nDESCRIPTION: This example shows the output of the dbt --version command in dbt Cloud CLI, which displays the CLI version number along with build information including commit hash and build timestamp.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/version.md#2025-04-09_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n$ dbt --version\ndbt Cloud CLI - 0.35.7 (fae78a6f5f6f2d7dff3cab3305fe7f99bd2a36f3 2024-01-18T22:34:52Z)\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt_project.yml for Snowflake Python Formula 1 Project\nDESCRIPTION: This YAML configuration defines the project settings including version requirements, file paths, and model configurations. It specifies materialization settings with tables for Python models in the marts directory and establishes documentation color schemes for different model types.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n# Name your project! Project names should contain only lowercase characters\n# and underscores. A good package name should reflect your organization's\n# name or the intended use of these models\nname: 'snowflake_dbt_python_formula1'\nversion: '1.3.0'\nrequire-dbt-version: '>=1.3.0'\nconfig-version: 2\n\n# This setting configures which \"profile\" dbt uses for this project.\nprofile: 'default'\n\n# These configurations specify where dbt should look for different types of files.\n# The `model-paths` config, for example, states that models in this project can be\n# found in the \"models/\" directory. You probably won't need to change these!\nmodel-paths: [\"models\"]\nanalysis-paths: [\"analyses\"]\ntest-paths: [\"tests\"]\nseed-paths: [\"seeds\"]\nmacro-paths: [\"macros\"]\nsnapshot-paths: [\"snapshots\"]\n\ntarget-path: \"target\"  # directory which will store compiled SQL files\nclean-targets:         # directories to be removed by `dbt clean`\n - \"target\"\n - \"dbt_packages\"\n\nmodels:\n snowflake_dbt_python_formula1:\n   staging:\n\n +docs:\n   node_color: \"CadetBlue\"\n marts:\n  +materialized: table\n  aggregates:\n   +docs:\n     node_color: \"Maroon\"\n   +tags: \"bi\"\n\n core:\n   +docs:\n     node_color: \"#800080\"\n intermediate:\n   +docs:\n     node_color: \"MediumSlateBlue\"\n ml:\n   prep:\n     +docs:\n       node_color: \"Indigo\"\n   train_predict:\n     +docs:\n       node_color: \"#36454f\"\n```\n\n----------------------------------------\n\nTITLE: String Replacement in SQL with dbt\nDESCRIPTION: Macro to replace all occurrences of one substring with another within a string. Behavior may vary by adapter.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/cross-database-macros.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt.replace(\"string_text_column\", \"old_chars_column\", \"new_chars_column\") }}\n{{ dbt.replace(\"string_text_column\", \"'-'\", \"'_'\") }}\n```\n\n----------------------------------------\n\nTITLE: Configuring .dbtignore File for dbt Projects\nDESCRIPTION: This snippet demonstrates various patterns for ignoring files and directories in a dbt project using a .dbtignore file. It shows how to ignore specific files, file types, and directories.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbtignore.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# .dbtignore\n\n# ignore individual .py files\nnot-a-dbt-model.py\nanother-non-dbt-model.py\n\n# ignore all .py files\n**.py\n\n# ignore all .py files with \"codegen\" in the filename\n*codegen*.py\n\n# ignore all folders in a directory\npath/to/folders/**\n\n# ignore some folders in a directory\npath/to/folders/subfolder/**\n\n```\n\n----------------------------------------\n\nTITLE: String Function Examples in dbt\nDESCRIPTION: Examples of string manipulation functions including concat, hash, and length operations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/cross-database-macros.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt.concat([\"column_1\", \"column_2\"]) }}\n{{ dbt.hash(\"column\") }}\n{{ dbt.length(\"column\") }}\n```\n\n----------------------------------------\n\nTITLE: Accessing flags in dbt Jinja templates\nDESCRIPTION: Example of accessing flag values in custom user-defined logic using Jinja templates within dbt_project.yml.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/about-global-configs.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n# dbt_project.yml\n\non-run-start:\n  - '{{ log(\"I will stop at the first sign of trouble\", info = true) if flags.FAIL_FAST }}'\n```\n\n----------------------------------------\n\nTITLE: Creating GLOBAL_FUNCTIONS Database for Hash Function in Teradata\nDESCRIPTION: SQL command to create the GLOBAL_FUNCTIONS database that will host the MD5 UDF required for the hash cross-database macro functionality. The database is allocated 60MB of permanent space and 120MB of spool space.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/teradata-setup.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nCREATE DATABASE GLOBAL_FUNCTIONS AS PERMANENT = 60e6, SPOOL = 120e6;\n```\n\n----------------------------------------\n\nTITLE: Executing Source Freshness Check with State Selection\nDESCRIPTION: Shows how to run source freshness checks and subsequent builds using source status selectors with state comparison.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/configure-state.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndbt source freshness\ndbt build --select \"source_status:fresher+\" --state path/to/prod/artifacts\n```\n\n----------------------------------------\n\nTITLE: Casting Date to Timestamp in loaded_at_field\nDESCRIPTION: Examples of casting a date field to a timestamp for use in the loaded_at_field configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/freshness.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nloaded_at_field: \"completed_date::timestamp\"\n```\n\nLANGUAGE: yaml\nCODE:\n```\nloaded_at_field: \"CAST(completed_date AS TIMESTAMP)\"\n```\n\n----------------------------------------\n\nTITLE: Config-based Node Selection\nDESCRIPTION: Selecting models based on their configuration properties like materialization type, schema, and clustering\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/methods.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --select \"config.materialized:incremental\"    # run all models that are materialized incrementally\ndbt run --select \"config.schema:audit\"              # run all models that are created in the `audit` schema\ndbt run --select \"config.cluster_by:geo_country\"      # run all models clustered by `geo_country`\n```\n\n----------------------------------------\n\nTITLE: Updated collect_freshness Return Format in SQL\nDESCRIPTION: Shows the proper way to return the collect_freshness result. In v1.5, the built-in collect_freshness macro now returns the entire response object instead of just the table result.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/core-upgrade/11-Older versions/10-upgrading-to-v1.5.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{ return(load_result('collect_freshness')) }}\n```\n\n----------------------------------------\n\nTITLE: Querying Definition vs Applied State using GraphQL in dbt Cloud Discovery API\nDESCRIPTION: GraphQL query that demonstrates how to compare the definition and applied states of a model in dbt Cloud. The query retrieves model names and raw code for both states, with execution info for applied state.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/project-state.md#2025-04-09_snippet_0\n\nLANGUAGE: graphql\nCODE:\n```\nquery Compare($environmentId: Int!, $first: Int!) {\n\tenvironment(id: $environmentId) {\n\t\tdefinition {\n\t\t\tmodels(first: $first) {\n\t\t\t\tedges {\n\t\t\t\t\tnode {\n\t\t\t\t\t\tname\n\t\t\t\t\t\trawCode\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tapplied {\n\t\t\tmodels(first: $first) {\n\t\t\t\tedges {\n\t\t\t\t\tnode {\n\t\t\t\t\t\tname\n\t\t\t\t\t\trawCode \n\t\t\t\t\t\texecutionInfo {\n\t\t\t\t\t\t\texecuteCompletedAt\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Semantic Model for Order Items in YAML\nDESCRIPTION: YAML configuration that defines a semantic model for order items, including entities (order_item, order_id, product), dimensions (ordered_at, is_food_item, is_drink_item), and measures (revenue, food_revenue, drink_revenue, median_revenue).\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-build-our-metrics/semantic-layer-8-refactor-a-rollup.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_models:\\n   #The name of the semantic model.\\n   - name: order_items\\n      defaults:\\n         agg_time_dimension: ordered_at\\n      description: |\\n         Items contatined in each order. The grain of the table is one row per order item.\\n      model: ref('order_items')\\n      entities:\\n         - name: order_item\\n           type: primary\\n           expr: order_item_id\\n         - name: order_id\\n           type: foreign\\n           expr: order_id\\n         - name: product\\n           type: foreign\\n           expr: product_id\\n      dimensions:\\n         - name: ordered_at\\n           expr: date_trunc('day', ordered_at)\\n           type: time\\n           type_params:\\n             time_granularity: day\\n         - name: is_food_item\\n           type: categorical\\n         - name: is_drink_item\\n           type: categorical\\n      measures:\\n         - name: revenue\\n           description: The revenue generated for each order item. Revenue is calculated as a sum of revenue associated with each product in an order.\\n           agg: sum\\n           expr: product_price\\n         - name: food_revenue\\n           description: The revenue generated for each order item. Revenue is calculated as a sum of revenue associated with each product in an order.\\n           agg: sum\\n           expr: case when is_food_item = 1 then product_price else 0 end\\n         - name: drink_revenue\\n           description: The revenue generated for each order item. Revenue is calculated as a sum of revenue associated with each product in an order.\\n           agg: sum\\n           expr: case when is_drink_item = 1 then product_price else 0 end\\n         - name: median_revenue\\n           description: The median revenue generated for each order item.\\n           agg: median\\n           expr: product_price\n```\n\n----------------------------------------\n\nTITLE: Implementing Incremental DBT Model for Performance Optimization\nDESCRIPTION: SQL implementation of an incremental DBT model that processes model execution events. The model includes logic for handling both full refresh and incremental processing scenarios, with optimizations for calculating first-time model runs and daily mode calculations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-08-12-how-we-shaved-90-minutes-off-long-running-model.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{config(materialized = 'incremental', unique_key = 'invocation_id')}}\n\nwith model_execution as (\n\n    select *\n    from {{ ref('stg_dbt_run_model_events') }}\n    where\n        1=1\n    {% if target.name == 'dev' %}\n\n        and collector_tstamp >= dateadd(d, -{{var('testing_days_of_data')}}, current_date)\n\n    {% elif is_incremental() %}\n\n        --incremental runs re-process a full day everytime to get an accurate mode below\n        and collector_tstamp > (select max(max_collector_tstamp)::date from {{ this }})\n\n    {% endif %}\n\n),\n\n{# When running rull refresh we have access to all records, so this logis isn't needed #}\n{% if is_incremental() %}\nnew_models as (\n\n    select\n        project_id,\n        model_id,\n        invocation_id,\n        dvce_created_tstamp,\n        true as is_new\n    from {{ ref('stg_dbt_run_model_events') }} as base_table\n    where\n        exists (\n                select 1\n                from model_execution\n                where\n                    base_table.project_id = model_execution.project_id\n                    and base_table.model_id = model_execution.model_id\n            )\n    qualify\n        row_number() over(partition by project_id, model_id order by dvce_created_tstamp) = 1\n\n\n),\n{% endif %}\n\ndiffed as (\n\n    select model_execution.*,\n\n        {% if is_incremental() %}\n\n            new_models.is_new,\n\n        {% else %}\n\n            row_number() over (\n                partition by project_id, model_id\n                order by dvce_created_tstamp\n            ) = 1 as is_new,\n\n        {% endif %}\n\n        /*\n            The `mode` window function returns the most common content hash for a\n            given model on a given day. We use this a proxy for the 'production'\n            version of the model, running in deployment. When a different hash\n            is run, it likely reflects that the model is undergoing development.\n        */\n\n        model_execution.contents != mode(model_execution.contents) over (\n            partition by model_execution.project_id, model_execution.model_id, model_execution.dvce_created_tstamp::date\n        ) as is_changed\n\n    from model_execution\n        {% if is_incremental() %}\n            left join new_models on\n                model_execution.project_id = new_models.project_id\n                and model_execution.model_id = new_models.model_id\n                and model_execution.invocation_id = new_models.invocation_id\n                and model_execution.dvce_created_tstamp = new_models.dvce_created_tstamp\n        {% endif %}\n\n),\n\nfinal as (\n\n    select\n        invocation_id,\n        max(collector_tstamp) as max_collector_tstamp,\n        max(model_complexity) as model_complexity,\n        max(model_total) as count_models,\n        sum(case when is_new or is_changed then 1 else 0 end) as count_changed,\n        sum(case when skipped = true then 1 else 0 end) as count_skip,\n        sum(case when error is null or error = 'false' then 0 else 1 end) as count_error,\n        sum(case when (error is null or error = 'false') and skipped = false then 1 else 0 end) as count_succeed\n\n    from diffed\n    group by 1\n\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Setting quote_columns in dbt_project.yml for Firebolt\nDESCRIPTION: Configuration to explicitly set quote_columns for seeds in Firebolt to prevent warnings. Set to false by default or true if CSV column headers contain spaces.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/firebolt-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  +quote_columns: false  #or `true` if you have csv column headers with spaces\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents in Markdown\nDESCRIPTION: This snippet demonstrates how to create a table of contents using Markdown syntax. It includes links to different sections of the document using anchor tags.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/contributing/content-types.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# TOC\n\n* [Conceptual](#conceptual)\n* [Referential](#referential)\n* [Procedural](#procedural)\n* [Guide](#guide)\n* [Quickstart](#quickstart-guide)\n* [Cookbook recipes](#cookbook-recipes)\n```\n\n----------------------------------------\n\nTITLE: Configuring Declarative Caching in Semantic Model YAML\nDESCRIPTION: YAML configuration for enabling declarative caching in a saved query with export definition. Sets cache configuration to true and defines an export table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/use-dbt-semantic-layer/sl-cache.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsaved_queries:\n  - name: my_saved_query\n    ... # Rest of the saved queries configuration.\n    config:\n      cache:\n        enabled: true  # Set to true to enable, defaults to false.\n    exports:\n      - name: order_data_key_metrics\n        config:\n          export_as: table\n```\n\n----------------------------------------\n\nTITLE: SQL IN Operator for Multiple Value Filtering\nDESCRIPTION: Example showing how to filter multiple order statuses using the NOT IN operator, demonstrating a more efficient way to exclude multiple values.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/operators/sql-in.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect * from {{ source('backend_db', 'orders') }}\nwhere status not in ('employee_order', 'influencer_order') --list of order statuses to filter out\n```\n\n----------------------------------------\n\nTITLE: Defining Measure Parameters in dbt (Markdown)\nDESCRIPTION: A markdown table detailing the parameters used for defining measures in dbt projects. It includes information on required and optional fields, supported aggregation methods, and configuration options for measures and metrics.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_sl-measures-parameters.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Parameter | Description | Required | Type | \n| --- | --- | --- | --- | \n| [`name`](/docs/build/measures#name) | Provide a name for the measure, which must be unique and can't be repeated across all semantic models in your dbt project. | Required | String | \n| [`description`](/docs/build/measures#description) | Describes the calculated measure. | Optional | String | \n| [`agg`](/docs/build/measures#aggregation) | dbt supports the following aggregations: `sum`, `max`, `min`, `average`, `median`, `count_distinct`, `percentile`, and `sum_boolean`. | Required | String |\n| [`expr`](/docs/build/measures#expr) | Either reference an existing column in the table or use a SQL expression to create or derive a new one. | Optional | String | \n| [`non_additive_dimension`](/docs/build/measures#non-additive-dimensions) | Non-additive dimensions can be specified for measures that cannot be aggregated over certain dimensions, such as bank account balances, to avoid producing incorrect results. | Optional | String |\n| `agg_params` | Specific aggregation properties, such as a percentile. | Optional | Dict |\n| `agg_time_dimension` | The time field. Defaults to the default agg time dimension for the semantic model.  | Optional | String |\n| `label` | String that defines the display value in downstream tools. Accepts plain text, spaces, and quotes (such as `orders_total` or \"orders_total\"). Available in dbt version 1.7 or higher. | Optional | String |\n| `create_metric` | Create a `simple` metric from a measure by setting `create_metric: True`. The `label` and `description` attributes will be automatically propagated to the created metric. Available in dbt version 1.7 or higher. | Optional | Boolean |\n| `config`  | Use the [`config`](/reference/resource-properties/config) property to specify configurations for your metric. Supports the [`meta`](/reference/resource-configs/meta) property, nested under `config`. | Optional |\n```\n\n----------------------------------------\n\nTITLE: Querying Job Run Results in GraphQL for dbt Cloud Discovery API\nDESCRIPTION: This GraphQL query fetches the results of a specific job run, including model and test statuses. It's useful for analyzing the performance and outcomes of individual job runs in dbt Cloud.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/discovery-use-cases-and-examples.md#2025-04-09_snippet_4\n\nLANGUAGE: graphql\nCODE:\n```\nquery ($jobId: BigInt!, $runId: BigInt!) {\n  job(id: $jobId, runId: $runId) {\n    models {\n      name\n      status\n      tests {\n        name\n        status\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring YAML Version in dbt Files\nDESCRIPTION: Demonstrates the version specification required at the top of dbt model and source YAML files. While version 2 is currently the only supported version, this configuration enables future extensibility.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Project/why-version-2.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n```\n\n----------------------------------------\n\nTITLE: Setting invalidate_hard_deletes in Jinja SQL Snapshot\nDESCRIPTION: SQL configuration using Jinja templating to enable invalidate_hard_deletes in a snapshot, applicable for dbt versions 1.8 and earlier.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/invalidate_hard_deletes.md#2025-04-09_snippet_1\n\nLANGUAGE: jinja2\nCODE:\n```\n{{\n  config(\n    strategy=\"timestamp\",\n    invalidate_hard_deletes=True\n  )\n}}\n```\n\n----------------------------------------\n\nTITLE: Accessing MetricFlow Tutorial\nDESCRIPTION: Command to access the dedicated MetricFlow tutorial to help users get started with MetricFlow in dbt Core.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metricflow-commands.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nmf tutorial # In dbt Core\n```\n\n----------------------------------------\n\nTITLE: Listing Tests by Tag Name in dbt\nDESCRIPTION: Example of using dbt ls to list tests with a specific tag (nightly in this case).\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/list.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ dbt ls --select tag:nightly --resource-type test\nmy_project.schema_test.not_null_orders_order_id\nmy_project.schema_test.unique_orders_order_id\nmy_project.schema_test.not_null_products_product_id\nmy_project.schema_test.unique_products_product_id\n...\n```\n\n----------------------------------------\n\nTITLE: Granting Snowflake Cortex Access in SQL\nDESCRIPTION: This SQL snippet creates a role for Cortex users and grants necessary permissions to access Snowflake Cortex. It's used to ensure that the Semantic Layer and deployment users have the required privileges for the Ask dbt chatbot functionality.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud-integrations/set-up-snowflake-native-app.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\ncreate role cortex_user_role;\ngrant database role SNOWFLAKE.CORTEX_USER to role cortex_user_role;\ngrant role cortex_user_role to user SL_USER;\ngrant role cortex_user_role to user DEPLOYMENT_USER;\n```\n\n----------------------------------------\n\nTITLE: Practical SQL LIMIT Example with Order Ranking\nDESCRIPTION: Shows a practical example using LIMIT with the Jaffle Shop orders table, including row ranking and ordering functionality.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/clauses/sql-limit.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\torder_id,\n\torder_date,\n\trank () over (order by order_date) as order_rnk\nfrom {{ ref('orders') }}\norder by 2\nlimit 5\n```\n\n----------------------------------------\n\nTITLE: Setting hard_deletes in dbt_project.yml\nDESCRIPTION: Project-level configuration for hard_deletes in the dbt_project.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/hard-deletes.md#2025-04-09_snippet_1\n\nLANGUAGE: yml\nCODE:\n```\nsnapshots:\n  [<resource-path>]:\n    +hard_deletes: \"ignore\" | \"invalidate\" | \"new_record\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Incremental Spark Model in dbt\nDESCRIPTION: This snippet demonstrates an incremental Spark model in dbt. It references another model, checks if it's running incrementally, and filters data based on the maximum run_date from the existing table. This approach allows for efficient updates to the model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/athena-configs.md#2025-04-09_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef model(dbt, spark_session):\n    dbt.config(materialized=\"incremental\")\n    df = dbt.ref(\"model\")\n\n    if dbt.is_incremental:\n        max_from_this = (\n            f\"select max(run_date) from {dbt.this.schema}.{dbt.this.identifier}\"\n        )\n        df = df.filter(df.run_date >= spark_session.sql(max_from_this).collect()[0][0])\n\n    return df\n```\n\n----------------------------------------\n\nTITLE: Retrieving Latest Run ID using dbt-cloud CLI\nDESCRIPTION: Command to fetch the most recent run ID from dbt Cloud using the run list API endpoint and jq for JSON parsing\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-05-03-making-dbt-cloud-api-calls-using-dbt-cloud-cli.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nlatest_run_id=$(dbt-cloud run list --job-id $DBT_CLOUD_JOB_ID | jq .data[0].id -r)\n```\n\n----------------------------------------\n\nTITLE: Using Sequence in Customer Dimension Model\nDESCRIPTION: Example of how to use the increment_sequence macro in a dbt model to generate surrogate keys.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-08-17-managing-surrogate-keys-in-dbt.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nwith cte_name as (\n\t\t...\n)\n...\n\nselect\n\n\t{{ increment_sequence() }} as customer_id, \n\tfirst_name, \n\tlast_name\n\nfrom cte_name\n\n...\n```\n\n----------------------------------------\n\nTITLE: MERGE Operation Example (SQL)\nDESCRIPTION: An example of a MERGE operation in SQL, used as a starting point for conversion to a dbt incremental model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/migrate-from-stored-procedures.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nMERGE INTO ride_details USING (\n    SELECT\n        ride_id,\n        subtotal,\n        tip\n\n    FROM rides_to_load AS rtl\n\n    ON ride_details.ride_id = rtl.ride_id\n\n    WHEN MATCHED THEN UPDATE\n\n    SET ride_details.tip = rtl.tip\n\n    WHEN NOT MATCHED THEN INSERT (ride_id, subtotal, tip)\n    VALUES (rtl.ride_id, rtl.subtotal, NVL(rtl.tip, 0, rtl.tip)\n);\n```\n\n----------------------------------------\n\nTITLE: Installing dbt Prerelease on Windows\nDESCRIPTION: Commands to install dbt-core and adapter prereleases, activate the virtual environment, and check dbt version on Windows systems.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/pip-install.md#2025-04-09_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\npy -m pip install --pre dbt-core dbt-adapter-name\n.venv\\Scripts\\activate\ndbt --version\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Contract in YAML\nDESCRIPTION: YAML configuration for enforcing a data contract on the fct_orders model to ensure reliability for downstream users.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/mesh-qs.md#2025-04-09_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: fct_orders\n    access: public\n    description: \"Customer and order details\"\n    config:\n      contract:\n        enforced: true\n    columns:\n      - name: order_id\n        .....\n```\n\n----------------------------------------\n\nTITLE: Using dbt_utils Package for Column Combination Uniqueness Testing\nDESCRIPTION: This approach uses the dbt_utils package's unique_combination_of_columns test, which is more performant for large datasets as it tests multiple columns for uniqueness together.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Tests/uniqueness-two-columns.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: orders\n    tests:\n      - dbt_utils.unique_combination_of_columns:\n          combination_of_columns:\n            - country_code\n            - order_id\n```\n\n----------------------------------------\n\nTITLE: Configuring Grants for an Incremental Model in SQL\nDESCRIPTION: Demonstrates setting grants for an incremental model in its SQL file. This example grants select privileges to both 'bi_user' and 'reporter'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/grants.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(materialized = 'incremental', grants = {\n    'select': ['bi_user', 'reporter']\n}) }}\n```\n\n----------------------------------------\n\nTITLE: Granting Analytics Database Permissions to Transformer Role in Snowflake\nDESCRIPTION: SQL command to grant the transformer role full permissions on the analytics database for creating transformed data objects.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/database-permissions/snowflake-permissions.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\ngrant all on database analytics to role transformer;\n```\n\n----------------------------------------\n\nTITLE: Removing Duplicates with DISTINCT and Window Functions in SQL\nDESCRIPTION: SQL query that uses DISTINCT with window functions to remove duplicate records from conversion data, connecting each conversion event to a visit by selecting the most recent visit for each user and purchase.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/conversion-metrics.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nselect distinct\n  first_value(v.ds) over (partition by b.ds, b.user_id, b.uuid order by v.ds desc) as v_ds,\n  first_value(v.user_id) over (partition by b.ds, b.user_id, b.uuid order by v.ds desc) as user_id,\n  first_value(v.referrer_id) over (partition by b.ds, b.user_id, b.uuid order by v.ds desc) as referrer_id,\n  b.ds,\n  b.uuid,\n  1 as buys\nfrom visits v\ninner join (\n    select *, uuid_string() as uuid from buys\n) b\non\nv.user_id = b.user_id and v.ds <= b.ds and v.ds > b.ds - interval '7 day';\n```\n\n----------------------------------------\n\nTITLE: Using Plus Operator for Node Selection in dbt Commands\nDESCRIPTION: Demonstrates the usage of the '+' operator in dbt commands to select models and their dependencies. The operator can be used to include descendants, ancestors, or both, depending on its placement.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/graph-operators.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --select \"my_model+\"         # select my_model and all descendants\ndbt run --select \"+my_model\"         # select my_model and all ancestors\ndbt run --select \"+my_model+\"        # select my_model, and all of its ancestors and descendants\n```\n\n----------------------------------------\n\nTITLE: Implementing Advanced dbt Selection Patterns\nDESCRIPTION: Illustrates various dbt selection patterns using operators like +, @, and dir_name for efficient model selection and execution in different scenarios.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/dbt-tips.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n-- Run a model and all its upstream dependencies\ndbt build --select +model_name\n\n-- Run a model and everything downstream that depends on it\ndbt build --select model_name+\n\n-- Run all models in a package or directory\ndbt build --select dir_name\n\n-- Test a model and its dependencies in a non-state-aware CI setup\ndbt build --select @model_name\n\n-- Exclude specific models from selection\ndbt build --select dir_name --exclude model_to_exclude\n\n-- Full refresh of an incremental model\ndbt build --select incremental_model --full-refresh\n```\n\n----------------------------------------\n\nTITLE: Setting up Project Structure with Bitbucket Pipelines\nDESCRIPTION: Shows the recommended project structure for implementing Bitbucket Pipelines with a SQLFluff linting job in a dbt project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/set-up-ci.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nmy_awesome_project\n bitbucket-pipelines.yml\n dbt_project.yml\n```\n\n----------------------------------------\n\nTITLE: Debug Output Example in Shell\nDESCRIPTION: Sample output when running dbt compile with the debug macro enabled. Shows the debugger interface and code context when execution pauses.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/debug-method.md#2025-04-09_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$ DBT_MACRO_DEBUGGING=write dbt compile\nRunning with dbt=1.0\n> /var/folders/31/mrzqbbtd3rn4hmgbhrtkfyxm0000gn/T/dbt-macro-compiled-cxvhhgu7.py(14)root()\n     13         environment.call(context, (undefined(name='debug') if l_0_debug is missing else l_0_debug)),\n---> 14         environment.call(context, (undefined(name='source') if l_0_source is missing else l_0_source), 'src', 'seedtable'),\n     15     )\n\nipdb> l 9,12\n      9     l_0_debug = resolve('debug')\n     10     l_0_source = resolve('source')\n     11     pass\n     12     yield '%s\\nselect * from %s' % (\n```\n\n----------------------------------------\n\nTITLE: Viewing dbt CLI Terminal Output\nDESCRIPTION: Example of dbt's terminal output showing a model being created as a view. The CLI output provides concise feedback during execution with timing information.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/events-logging.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n21:35:48  6 of 7 OK created view model dbt_testing.name_list......................... [CREATE VIEW in 0.17s]\n```\n\n----------------------------------------\n\nTITLE: State-based Node Selection in dbt\nDESCRIPTION: Examples of using state-based selection to run operations on new or modified nodes by comparing against a previous manifest.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/methods.md#2025-04-09_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\ndbt test --select \"state:new\" --state path/to/artifacts      # run all tests on new models + and new tests on old models\ndbt run --select \"state:modified\" --state path/to/artifacts  # run all models that have been modified\ndbt ls --select \"state:modified\" --state path/to/artifacts   # list all modified nodes (not just models)\n```\n\n----------------------------------------\n\nTITLE: Combining State and Result Selectors in dbt\nDESCRIPTION: Demonstrates how to combine state and result selectors in a single dbt command to capture both errors from previous runs and new/modified models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/configure-state.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --select \"result:<status>+\" state:modified+ --defer --state ./<dbt-artifact-path>\n```\n\n----------------------------------------\n\nTITLE: Custom SQL Query for loaded_at_query\nDESCRIPTION: Examples of using custom SQL queries in the loaded_at_query configuration to determine source freshness.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/freshness.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  - name: your_source\n    freshness:\n      error_after:\n        count: 2\n        period: hour\n    loaded_at_query: |\n      select max(_sdc_batched_at) from (\n      select * from {{ this }}\n      where _sdc_batched_at > dateadd(day, -7, current_date)\n      qualify count(*) over (partition by _sdc_batched_at::date) > 2000\n      )\n```\n\nLANGUAGE: yaml\nCODE:\n```\nsources: \n  - name: ecom\n    schema: raw\n    description: E-commerce data for the Jaffle Shop\n    freshness:\n      warn_after:\n        count: 24\n        period: hour\n    tables:\n      - name: raw_orders\n        description: One record per order\n        loaded_at_query: \"select {{ current_timestamp() }}\"\n```\n\n----------------------------------------\n\nTITLE: Result-based Selection\nDESCRIPTION: Selecting resources based on their previous execution results\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/methods.md#2025-04-09_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --select \"result:error\" --state path/to/artifacts # run all models that generated errors on the prior invocation of dbt run\ndbt test --select \"result:fail\" --state path/to/artifacts # run all tests that failed on the prior invocation of dbt test\ndbt build --select \"1+result:fail\" --state path/to/artifacts # run all the models associated with failed tests from the prior invocation of dbt build\ndbt seed --select \"result:error\" --state path/to/artifacts # run all seeds that generated errors on the prior invocation of dbt seed.\n```\n\n----------------------------------------\n\nTITLE: Querying Model Changes and Source Freshness with GraphQL\nDESCRIPTION: This GraphQL query compares the rawCode between definition and applied state, and reviews when sources were last loaded relative to model execution time. It helps determine if a model needs to be re-run based on code changes and source data updates.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/discovery-use-cases-and-examples.md#2025-04-09_snippet_5\n\nLANGUAGE: graphql\nCODE:\n```\nquery ($environmentId: BigInt!, $first: Int!) {\n  environment(id: $environmentId) {\n    applied {\n      models(\n        first: $first\n        filter: { uniqueIds: \"MODEL.PROJECT.MODEL_NAME\" }\n      ) {\n        edges {\n          node {\n            rawCode\n            ancestors(types: [Source]) {\n              ... on SourceAppliedStateNestedNode {\n                freshness {\n                  maxLoadedAt\n                }\n              }\n            }\n            executionInfo {\n              runGeneratedAt\n              executeCompletedAt\n            }\n            materializedType\n          }\n        }\n      }\n    }\n    definition {\n      models(\n        first: $first\n        filter: { uniqueIds: \"MODEL.PROJECT.MODEL_NAME\" }\n      ) {\n        edges {\n          node {\n            rawCode\n            runGeneratedAt\n            materializedType\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using SELECT TOP in SQL for dbt Cloud IDE Preview\nDESCRIPTION: This SQL snippet shows how to use the SELECT TOP clause to specify the number of records to return when previewing in the dbt Cloud IDE. This is an alternative method to control the output size.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/dbt-cloud-ide/ide-user-interface.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSELECT TOP #\n```\n\n----------------------------------------\n\nTITLE: Configuring Sources in properties.yml (v1.9+)\nDESCRIPTION: YAML configuration for sources in property files (models/properties.yml) for dbt version 1.9 and later. Shows how to define sources with config blocks that specify enabled status, event_time, and metadata.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/source-configs.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: [<source-name>]\n    [config]:\n      [enabled]: true | false\n      [event_time]: my_time_field\n      [meta]: {<dictionary>}\n\n    tables:\n      - name: [<source-table-name>]\n        [config]:\n          [enabled]: true | false\n          [event_time]: my_time_field\n          [meta]: {<dictionary>}\n\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Profiles for CI\nDESCRIPTION: Example profiles.yml configuration for PostgreSQL database connection in CI environment. Uses environment variables for sensitive credentials.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-07-26-pre-commit-dbt.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\njaffle_shop:\n  target: ci\n  outputs:\n    ci:\n      type: postgres\n      host: <your_host>\n      user: <user>\n      password: \"{{ env_var('DB_PASSWORD') }}\"\n      port: 5432\n      dbname: <database>\n      schema: ci\n      threads: 4\n```\n\n----------------------------------------\n\nTITLE: Configuring Bitbucket Pipelines for SQLFluff Linting\nDESCRIPTION: YAML configuration for Bitbucket Pipelines that runs SQLFluff linting on all branches with specific linting rules. It uses Python 3.11.1 and applies specific SQLFluff rules to the models directory.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/set-up-ci.md#2025-04-09_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nimage: python:3.11.1\n\n\npipelines:\n  branches:\n    '**': # this sets a wildcard to run on every branch\n      - step:\n          name: Lint dbt project\n          script:\n            - python -m pip install sqlfluff==0.13.1\n            - sqlfluff lint models --dialect snowflake --rules L019,L020,L021,L022\n\n    'main': # override if your default branch doesn't run on a branch named \"main\"\n      - step:\n          script:\n            - python --version\n```\n\n----------------------------------------\n\nTITLE: Configuring Auto Provisioning of Microsoft Entra ID Principals in dbt Project\nDESCRIPTION: Project configuration that enables automatic creation of Microsoft Entra ID principals in the database when using grants configuration with Azure SQL Database or Azure Synapse.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/mssql-configs.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  your_project_name:\n    auto_provision_aad_principals: true\n```\n\n----------------------------------------\n\nTITLE: Fetching Available Metrics for Dimensions with Semantic Layer JDBC API\nDESCRIPTION: SQL query to fetch available metrics given specific dimensions using the semantic_layer.metrics_for_dimensions() function. This is essentially the opposite of getting dimensions for metrics.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-jdbc.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nselect * from {{\n    semantic_layer.metrics_for_dimensions(group_by=['customer__customer_type'])\n}}\n```\n\n----------------------------------------\n\nTITLE: Querying Historical Model Runs and Test Results with GraphQL\nDESCRIPTION: This GraphQL query retrieves historical execution and test failure rates for a given model, up to 20 runs. It's useful for reviewing the performance of frequently used and important datasets over time.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/discovery-use-cases-and-examples.md#2025-04-09_snippet_7\n\nLANGUAGE: graphql\nCODE:\n```\nquery ($environmentId: BigInt!, $uniqueId: String!, $lastRunCount: Int) {\n  environment(id: $environmentId) {\n    applied {\n      modelHistoricalRuns(uniqueId: $uniqueId, lastRunCount: $lastRunCount) {\n        name\n        executeStartedAt\n        status\n        tests {\n          name\n          status\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Google Drive Access with gcloud Authentication\nDESCRIPTION: Command to log into Google Cloud with explicit Google Drive access permissions, which also updates the Application Default Credentials (ADC) file used by Google Cloud libraries.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Troubleshooting/access-gdrive-credential.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngcloud auth login --enable-gdrive-access --update-adc\n```\n\n----------------------------------------\n\nTITLE: Redshift Unload Example\nDESCRIPTION: Example showing how to use a post-hook to unload data from a model to S3 in Redshift.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/pre-hook-post-hook.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n  post_hook = \"unload ('select from {{ this }}') to 's3:/bucket_name/{{ this }}\"\n) }}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Source Definitions in YAML\nDESCRIPTION: YAML configuration defining jaffle_shop data sources including customers and orders tables with their descriptions and database locations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/mesh-qs.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: jaffle_shop\n    description: This is a replica of the Postgres database used by our app\n    database: raw\n    schema: jaffle_shop\n    tables:\n      - name: customers\n        description: One record per customer.\n      - name: orders\n        description: One record per order. Includes cancelled and deleted orders.\n```\n\n----------------------------------------\n\nTITLE: Enabling persist_docs Globally in dbt_project.yml\nDESCRIPTION: This snippet demonstrates how to enable persist_docs globally for all models in the dbt_project.yml file. It enables persisting documentation for both relations and columns.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/persist_docs.md#2025-04-09_snippet_7\n\nLANGUAGE: yml\nCODE:\n```\nmodels:\n  +persist_docs:\n    relation: true\n    columns: true\n```\n\n----------------------------------------\n\nTITLE: Setting Session Properties in dbt Model\nDESCRIPTION: Example of using dbt pre-hook to set session properties for query execution time limits.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/watsonx-spark-config.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(\n    pre_hook=\"set session query_max_run_time='10m'\"\n  )\n}}\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests with dbt CLI\nDESCRIPTION: Shell command example for running a specific unit test using the dbt CLI. The output shows a test failure due to an issue with the regex pattern in the email validation logic.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/unit-tests.md#2025-04-09_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndbt test --select test_is_valid_email_address\n16:03:49  Running with dbt=1.8.0-a1\n16:03:49  Registered adapter: postgres=1.8.0-a1\n16:03:50  Found 6 models, 5 seeds, 4 data tests, 0 sources, 0 exposures, 0 metrics, 410 macros, 0 groups, 0 semantic models, 1 unit test\n16:03:50  \n16:03:50  Concurrency: 5 threads (target='postgres')\n16:03:50  \n16:03:50  1 of 1 START unit_test dim_customers::test_is_valid_email_address ................... [RUN]\n16:03:51  1 of 1 FAIL 1 dim_customers::test_is_valid_email_address ............................ [FAIL 1 in 0.26s]\n16:03:51  \n16:03:51  Finished running 1 unit_test in 0 hours 0 minutes and 0.67 seconds (0.67s).\n16:03:51  \n16:03:51  Completed with 1 error and 0 warnings:\n16:03:51  \n16:03:51  Failure in unit_test test_is_valid_email_address (models/marts/unit_tests.yml)\n16:03:51    \n\nactual differs from expected:\n\n@@ ,email           ,is_valid_email_address\n  ,cool@example.com,TrueFalse\n   ,cool@unknown.com,False\n...,...             ,...\n\n\n16:03:51  \n16:03:51    compiled Code at models/marts/unit_tests.yml\n16:03:51  \n16:03:51  Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1\n```\n\n----------------------------------------\n\nTITLE: Pinning to a Version Range with Comma Syntax\nDESCRIPTION: Alternative syntax for setting version ranges using a comma-separated string. This achieves the same result as the array syntax example.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/require-dbt-version.md#2025-04-09_snippet_4\n\nLANGUAGE: yml\nCODE:\n```\nrequire-dbt-version: \">=1.0.0,<2.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests with dbt CLI\nDESCRIPTION: Shell command example for running a specific unit test using the dbt CLI. The output shows a test failure due to an issue with the regex pattern in the email validation logic.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/unit-tests.md#2025-04-09_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndbt test --select test_is_valid_email_address\n16:03:49  Running with dbt=1.8.0-a1\n16:03:49  Registered adapter: postgres=1.8.0-a1\n16:03:50  Found 6 models, 5 seeds, 4 data tests, 0 sources, 0 exposures, 0 metrics, 410 macros, 0 groups, 0 semantic models, 1 unit test\n16:03:50  \n16:03:50  Concurrency: 5 threads (target='postgres')\n16:03:50  \n16:03:50  1 of 1 START unit_test dim_customers::test_is_valid_email_address ................... [RUN]\n16:03:51  1 of 1 FAIL 1 dim_customers::test_is_valid_email_address ............................ [FAIL 1 in 0.26s]\n16:03:51  \n16:03:51  Finished running 1 unit_test in 0 hours 0 minutes and 0.67 seconds (0.67s).\n16:03:51  \n16:03:51  Completed with 1 error and 0 warnings:\n16:03:51  \n16:03:51  Failure in unit_test test_is_valid_email_address (models/marts/unit_tests.yml)\n16:03:51    \n\nactual differs from expected:\n\n@@ ,email           ,is_valid_email_address\n  ,cool@example.com,TrueFalse\n   ,cool@unknown.com,False\n...,...             ,...\n\n\n16:03:51  \n16:03:51    compiled Code at models/marts/unit_tests.yml\n16:03:51  \n16:03:51  Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1\n```\n\n----------------------------------------\n\nTITLE: Rounding Numeric Fields in Jaffle Shop Orders Model\nDESCRIPTION: This SQL query demonstrates how to use the ROUND function to round the 'amount' field in the Jaffle Shop's orders model to one decimal place. It creates a new column 'rounded_amount' with the rounded values.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/aggregate-functions/sql-round.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect \n\tcast(order_id as string) as order_id,\n\torder_date,\n\tamount,\n\tround(amount, 1) as rounded_amount\nfrom {{ ref('orders') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Snapshot Groups\nDESCRIPTION: Demonstrates group configuration for snapshot resources in both YAML and SQL\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/group.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  [<resource-path>]:\n    +group: GROUP_NAME\n```\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot [snapshot_name] %}\n\n{{ config(\n  group='GROUP_NAME'\n) }}\n\nselect ...\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Testing Data Platform Connection with dbt Debug\nDESCRIPTION: Tests only the connection to the data platform by using the --connection flag, skipping other debug checks\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/debug.md#2025-04-09_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndbt debug --connection\n```\n\n----------------------------------------\n\nTITLE: Setting Query Band at Model Level in SQL for dbt-teradata\nDESCRIPTION: Illustrates how to set a query band at the individual model level using a config block in the SQL file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/teradata-configs.md#2025-04-09_snippet_22\n\nLANGUAGE: sql\nCODE:\n```\n{{ config( query_band='sql={model};' ) }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Grouped Recency Test in dbt\nDESCRIPTION: Example of implementing a grouped recency test in dbt-utils to check for daily records across multiple turnstiles and stations. The test verifies data freshness by group using the recency test with specific datepart, field, and interval parameters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-01-17-grouping-data-tests.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: turnstile_entries\n    tests:\n      - dbt_utils.recency:\n          datepart: day\n          field: recorded_at\n          interval: 1\n          # Check for recency for each turnstile_id at each station_id\n          group_by_columns:\n            - station_id\n            - turnstile_id\n```\n\n----------------------------------------\n\nTITLE: Creating a Staging Model in dbt\nDESCRIPTION: This dbt SQL model creates a view from a raw source table, cleaning and transforming the data. It demonstrates the first step in the dbt approach, replacing the initial INSERT statement.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-07-19-migrating-from-stored-procs.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n orders_staging_model_a.sql\n{{\n    config(\n        materialized='view'\n    )\n}}\n \nwith raw_data as (\n    select * \n    from {{ source('raw', 'some_raw_table')}}\n    where is_test_record = false\n),\n \ncleaned as (\n    select messageid,\n           orderid::int as orderid,\n           sk_id,\n           case when client_name in ['a', 'b', 'c'] then clientid else -1 end\n    from   raw_data\n)\n \nselect * from cleaned\n```\n\n----------------------------------------\n\nTITLE: Installing dbt-upsolver Adapter using pip\nDESCRIPTION: Command to install the dbt-upsolver adapter using pip. This will also install dbt-core and any other dependencies.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/upsolver-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install dbt-upsolver\n```\n\n----------------------------------------\n\nTITLE: Defining Macro Arguments in YAML\nDESCRIPTION: This example shows the basic structure for documenting macro arguments in a YAML file, defining the name, type, and description for each argument.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/arguments.md#2025-04-09_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nversion: 2\n\nmacros:\n  - name: <macro name>\n    arguments:\n      - name: <arg name>\n        [type](#supported-types): <string>\n        description: <markdown_string>\n\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Project Settings\nDESCRIPTION: YAML configuration for the dbt project settings including project name and profile.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/manual-install-qs.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nname: jaffle_shop\n\n...\n\nprofile: jaffle_shop\n\n...\n\nmodels:\n    jaffle_shop:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Synchronous Client Implementation\nDESCRIPTION: Example of initializing and using the synchronous SemanticLayerClient to query metrics with the required session context manager.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-python-sdk.md#2025-04-09_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dbtsl import SemanticLayerClient\n\nclient = SemanticLayerClient(\n    environment_id=123,\n    auth_token=\"<your-semantic-layer-api-token>\",\n    host=\"semantic-layer.cloud.getdbt.com\",\n)\n\n# query the first metric by `metric_time`\ndef main():\n    with client.session():\n        metrics = client.metrics()\n        table = client.query(\n            metrics=[metrics[0].name],\n            group_by=[\"metric_time\"],\n        )\n        print(table)\n\nmain()\n```\n\n----------------------------------------\n\nTITLE: Setting event_time in SQL Model Config\nDESCRIPTION: Example of configuring event_time directly in a SQL model file using config block.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/event-time.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    event_time='my_time_field'\n) }}\n```\n\n----------------------------------------\n\nTITLE: Data Type Function Usage in dbt\nDESCRIPTION: Examples of using dbt's type_bigint, type_boolean, and type_float macros to get database-specific data types.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/cross-database-macros.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt.type_bigint() }}\n{{ dbt.type_boolean() }}\n{{ dbt.type_float() }}\n```\n\n----------------------------------------\n\nTITLE: Basic Model Description Configuration in YAML\nDESCRIPTION: Shows the basic structure for adding descriptions to models and their columns in a schema.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/description.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: model_name\n    description: markdown_string\n\n    columns:\n      - name: column_name\n        description: markdown_string\n```\n\n----------------------------------------\n\nTITLE: Standard Incremental Model Configuration with insert_overwrite in dbt BigQuery\nDESCRIPTION: Basic configuration for an incremental model using insert_overwrite strategy with date partitioning. This example aggregates tracking events by day and campaign_id.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-02-01-ingestion-time-partitioning-bigquery.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized = 'incremental',\n    incremental_strategy = 'insert_overwrite',\n    partition_by = {\n      \"field\": \"day\",\n      \"data_type\": \"date\"\n    }\n) }}\n\nselect\n    day,\n    campaign_id,\n    NULLIF(COUNTIF(action = 'impression'), 0) impressions_count\nfrom {{ source('logs', 'tracking_events') }}\ngroup by day, campaign_id\n```\n\n----------------------------------------\n\nTITLE: Configuring Database Overrides in a SQL Model File\nDESCRIPTION: Example showing how to override the database for a specific model using the config function in the model's SQL file. This changes the model to be built into a database called jaffle_shop.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/custom-databases.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(database=\"jaffle_shop\") }}\n\nselect * from ...\n```\n\n----------------------------------------\n\nTITLE: Locating profiles.yml with dbt debug\nDESCRIPTION: Using the dbt debug command with the --config-dir flag to find the location of the profiles.yml file, which is useful for troubleshooting profile-related errors.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/debug-errors.md#2025-04-09_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n$ dbt debug --config-dir\nRunning with dbt=1.7.1\nTo view your profiles.yml file, run:\n\nopen /Users/alice/.dbt\n```\n\n----------------------------------------\n\nTITLE: Checking Model Materialization in Jinja\nDESCRIPTION: Example showing how to check if a model is materialized as a view and log the information.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/model.md#2025-04-09_snippet_0\n\nLANGUAGE: jinja\nCODE:\n```\n{% if model.config.materialized == 'view' %}\n  {{ log(model.name ~ \" is a view.\", info=True) }}\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Setting Up Warehouses in Snowflake\nDESCRIPTION: SQL commands to create specialized warehouses for different workloads: loading (for data ingestion), transforming (for data transformation), and reporting (for data consumption).\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/database-permissions/snowflake-permissions.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\ncreate warehouse loading\n    warehouse_size = xsmall\n    auto_suspend = 3600\n    auto_resume = false\n    initially_suspended = true;\n\ncreate warehouse transforming\n    warehouse_size = xsmall\n    auto_suspend = 60\n    auto_resume = true\n    initially_suspended = true;\n\ncreate warehouse reporting\n    warehouse_size = xsmall\n    auto_suspend = 60\n    auto_resume = true\n    initially_suspended = true;\n```\n\n----------------------------------------\n\nTITLE: Group-based Selection\nDESCRIPTION: Selecting models that belong to specific groups\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/methods.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --select \"group:finance\" # run all models that belong to the finance group.\n```\n\n----------------------------------------\n\nTITLE: Granting Existing Analytics Database Access to Reporter Role in Snowflake\nDESCRIPTION: SQL commands to grant the reporter role read-only access to all existing objects in the analytics database, ensuring complete access to transformed data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/database-permissions/snowflake-permissions.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\ngrant usage on all schemas in database analytics to role reporter;\ngrant select on all tables in database analytics to role reporter;\ngrant select on all views in database analytics to role reporter;\n```\n\n----------------------------------------\n\nTITLE: Creating SQL Model in dbt\nDESCRIPTION: SQL query to create a simple model in dbt, which will be referenced by a Python model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-bigframes.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nselect \n1 as foo,\n2 as bar\n```\n\n----------------------------------------\n\nTITLE: Access-based Node Selection\nDESCRIPTION: Selecting models based on their access property (public, private, or protected)\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/methods.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndbt list --select \"access:public\"      # list all public models\ndbt list --select \"access:private\"       # list all private models\ndbt list --select \"access:protected\"       # list all protected models\n```\n\n----------------------------------------\n\nTITLE: Incremental Iceberg Table with Delete Condition\nDESCRIPTION: Configures an incremental Iceberg table using the merge strategy with a delete condition. Includes unique key definition, incremental predicates for performance, and a condition to identify records for deletion.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/athena-configs.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized='incremental',\n    table_type='iceberg',\n    incremental_strategy='merge',\n    unique_key='user_id',\n    incremental_predicates=[\"src.quantity > 1\", \"target.my_date >= now() - interval '4' year\"],\n    delete_condition=\"src.status != 'active' and target.my_date < now() - interval '2' year\",\n    format='parquet'\n) }}\n\nselect 'A' as user_id,\n       'pi' as name,\n       'active' as status,\n       17.89 as cost,\n       1 as quantity,\n       100000000 as quantity_big,\n       current_date as my_date\n```\n\n----------------------------------------\n\nTITLE: Querying Full Data Lineage with GraphQL\nDESCRIPTION: This GraphQL query retrieves lineage information for all project nodes, including sources, seeds, snapshots, models, and exposures.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/discovery-use-cases-and-examples.md#2025-04-09_snippet_14\n\nLANGUAGE: graphql\nCODE:\n```\nquery Lineage($environmentId: BigInt!, $first: Int!) {\n  environment(id: $environmentId) {\n    definition {\n      sources(first: $first) {\n        edges {\n          node {\n            uniqueId\n            name\n            resourceType\n            children {\n              uniqueId\n              name\n              resourceType\n            }\n          }\n        }\n      }\n      seeds(first: $first) {\n        edges {\n          node {\n            uniqueId\n            name\n            resourceType\n            children {\n              uniqueId\n              name\n              resourceType\n            }\n          }\n        }\n      }\n      snapshots(first: $first) {\n        edges {\n          node {\n            uniqueId\n            name\n            resourceType\n            parents {\n              uniqueId\n              name\n              resourceType\n            }\n            children {\n              uniqueId\n              name\n              resourceType\n            }\n          }\n        }\n      }\n      models(first: $first) {\n        edges {\n          node {\n            uniqueId\n            name\n            resourceType\n            parents {\n              uniqueId\n              name\n              resourceType\n            }\n            children {\n              uniqueId\n              name\n              resourceType\n            }\n          }\n        }\n      }\n      exposures(first: $first) {\n        edges {\n          node {\n            uniqueId\n            name\n            resourceType\n            parents {\n              uniqueId\n              name\n              resourceType\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Documenting dbt Models with Descriptions\nDESCRIPTION: Enhances the schema.yml file with detailed descriptions for models and columns. This documentation is used to generate rich documentation for the dbt project that can be shared with team members.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/teradata-qs.md#2025-04-09_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: bi_customers\n    description: One record per customer\n    columns:\n      - name: customer_id\n        description: Primary key\n        tests:\n          - unique\n          - not_null\n      - name: first_order_date\n        description: NULL when a customer has not yet placed an order.\n\n  - name: stg_customers\n    description: This model cleans up customer data\n    columns:\n      - name: customer_id\n        description: Primary key\n        tests:\n          - unique\n          - not_null\n\n  - name: stg_orders\n    description: This model cleans up order data\n    columns:\n      - name: order_id\n        description: Primary key\n        tests:\n          - unique\n          - not_null\n      - name: status\n        tests:\n          - accepted_values:\n              values: ['placed', 'shipped', 'completed', 'return_pending', 'returned']\n      - name: customer_id\n        tests:\n          - not_null\n          - relationships:\n              to: ref('stg_customers')\n              field: customer_id\n```\n\n----------------------------------------\n\nTITLE: Defining Legacy Foreign Key Constraints in dbt (v1.8 and earlier)\nDESCRIPTION: Example YAML configuration for defining constraints in older dbt versions (1.8 and earlier). Shows how to define foreign keys using manual schema references with target.schema to maintain compatibility across environments.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/constraints.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: <model_name>\n    \n    # required\n    config:\n      contract: {enforced: true}\n    \n    # model-level constraints\n    constraints:\n      - type: primary_key\n        columns: [first_column, second_column, ...]\n      - type: foreign_key # multi_column\n        columns: [first_column, second_column, ...]\n        expression: \"{{ target.schema }}.other_model_name (other_model_first_column, other_model_second_column, ...)\"\n      - type: check\n        columns: [first_column, second_column, ...]\n        expression: \"first_column != second_column\"\n        name: human_friendly_name\n      - type: ...\n    \n    columns:\n      - name: first_column\n        data_type: string\n        \n        # column-level constraints\n        constraints:\n          - type: not_null\n          - type: unique\n          - type: foreign_key\n            expression: \"{{ target.schema }}.other_model_name (other_model_column)\"\n          - type: ...\n```\n\n----------------------------------------\n\nTITLE: Creating a dbt Snapshot Using Check Strategy\nDESCRIPTION: A SQL snippet that demonstrates how to create a snapshot of a base_product table using dbt's snapshot functionality with a check strategy on the change_id column. This is part of step 2 in the process of joining complex snapshots.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-05-24-joining-snapshot-complexity.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot snp_product %}\n{{\n   config(\n     target_schema=generate_schema_name('snapshots'),\n     unique_key='assetid',\n     strategy='check',\n     check_cols=['change_id']\n   )\n}}\nselect * from {{ ref('base_product') }}\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: ORDER BY Example with Date Aggregation\nDESCRIPTION: Practical example showing how to use ORDER BY with date truncation and aggregation functions. The query calculates average order amounts per month and orders results in descending order.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/clauses/sql-order-by.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\tdate_trunc('month, order_date') as order_month,\n\tround(avg(amount)) as avg_order_amount\nfrom {{ ref('orders') }}\ngroup by 1\norder by 1 desc\n```\n\n----------------------------------------\n\nTITLE: Setting Global and Specific Seed Delimiters in dbt_project.yml\nDESCRIPTION: This YAML snippet demonstrates how to set a global delimiter for all seeds in a project and override it for a specific subdirectory.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/delimiter.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  <project_name>:\n     +delimiter: \"|\" # default project delimiter for seeds will be \"|\"\n    <seed_subdirectory>:\n      +delimiter: \",\" # delimiter for seeds in seed_subdirectory will be \",\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Incremental Model with Partitioning in dbt\nDESCRIPTION: This SQL snippet demonstrates how to configure an incremental model with partitioning in dbt. It sets the materialization type, format, and partitioning column.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/trino-configs.md#2025-04-09_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        materialized = 'incremental',\n        properties={\n          \"format\": \"'PARQUET'\",\n          \"partitioned_by\": \"ARRAY['day']\",\n        }\n    )\n}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Certificate Authentication for Trino in dbt profiles.yml\nDESCRIPTION: This snippet illustrates how to set up certificate-based authentication for a Trino connection in the dbt profiles.yml file. It includes paths to the client certificate, private key, and other required connection details.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/trino-setup.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ntrino:\n  target: dev\n  outputs:\n    dev:\n      type: trino\n      method: certificate \n      cert: [path/to/cert_file]\n      client_certificate: [path/to/client/cert]\n      client_private_key: [path to client key]\n      database: [database name]\n      schema: [your dbt schema]\n      port: [port number]\n      threads: [1 or more]\n```\n\n----------------------------------------\n\nTITLE: Ephemeral Model Configuration\nDESCRIPTION: Example of configuring an ephemeral model materialization in SQL.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/govern/model-access.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(materialized='ephemeral') }}\n```\n\n----------------------------------------\n\nTITLE: Conditional Grants in dbt_project.yml\nDESCRIPTION: Illustrates how to use Jinja to set conditional grants based on the target name. This example grants select to different users in prod and dev environments.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/grants.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  +grants:\n    select: \"{{ ['user_a', 'user_b'] if target.name == 'prod' else ['user_c'] }}\"\n```\n\n----------------------------------------\n\nTITLE: DATEADD Function Syntax in Databricks\nDESCRIPTION: The syntax for the date_add function in Databricks which takes startDate and numDays parameters to add days to a date.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-09-11-sql-dateadd.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\ndate_add( {{ startDate }}, {{ numDays }} )\n```\n\n----------------------------------------\n\nTITLE: Listing File Paths in dbt\nDESCRIPTION: Example of using dbt ls to list file paths of resources in a specific package.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/list.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ndbt ls --select snowplow.* --output path\nmodels/base/snowplow_base_events.sql\nmodels/base/snowplow_base_web_page_context.sql\nmodels/identification/snowplow_id_map.sql\n...\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Overview Documentation\nDESCRIPTION: Example of creating a custom overview documentation block for a dbt project focused on MRR analysis.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/documentation.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n{% docs __overview__ %}\n# Monthly Recurring Revenue (MRR) playbook.\nThis dbt project is a worked example to demonstrate how to model subscription\nrevenue. **Check out the full write-up \\[here](https://blog.getdbt.com/modeling-subscription-revenue/),\nas well as the repo for this project \\[here](https://github.com/dbt-labs/mrr-playbook/).**\n...\n\n{% enddocs %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Snowflake User/Password + DUO MFA Authentication in dbt (v1.9 and later)\nDESCRIPTION: This YAML configuration sets up user/password authentication with DUO Mobile app integration for 2-Factor authentication in Snowflake for dbt versions 1.9 and later. It includes updated default behavior for 'reuse_connections'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/snowflake-setup.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmy-snowflake-db:\n  target: dev\n  outputs:\n    dev:\n      type: snowflake\n      account: [account id]\n\n      # User/password auth\n      user: [username]\n      password: [password]\n      authenticator: username_password_mfa\n\n      role: [user role]\n      database: [database name]\n      warehouse: [warehouse name]\n      schema: [dbt schema]\n      threads: [1 or more]\n      client_session_keep_alive: False\n      query_tag: [anything]\n\n      # optional\n      connect_retries: 0 # default 0\n      connect_timeout: 10 # default: 10\n      retry_on_database_errors: False # default: false\n      retry_all: False  # default: false\n      reuse_connections: True # default: True if client_session_keep_alive is False, otherwise None\n```\n\n----------------------------------------\n\nTITLE: Merging Company Domains with Coalesce in SQL\nDESCRIPTION: SQL query that joins corporate gaggles with merged company domains, using coalesce to handle domain consolidation. This allows for merging different gaggles that share the same company email domain.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-02-07-customer-360-view-census-playbook.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nselect\n       coalesce(mcd.new_domain, corporate_gaggles.corporate_email) as corporate_email,\n       ....\n   from corporate_gaggles\n   left join {{ ref('merged_company_domain') }} mcd on corporate_gaggles.corporate_email = mcd.old_domain\n   group by 1\n```\n\n----------------------------------------\n\nTITLE: On-Run-Start Hook Configuration\nDESCRIPTION: YAML configuration for executing schema grants at the start of dbt run.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-07-26-configuring-grants.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\non-run-start:\n\t- {{ grant_usage_on_schemas_where_select() }}\n```\n\n----------------------------------------\n\nTITLE: Configuring DuckDB with AWS Credential Chain in YAML\nDESCRIPTION: Profile configuration that uses AWS credential chain provider to automatically fetch AWS credentials from the environment or instance metadata, eliminating the need to specify credentials directly.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/duckdb-configs.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ndefault:\n  outputs:\n    dev:\n      type: duckdb\n      path: /tmp/dbt.duckdb\n      extensions:\n        - httpfs\n        - parquet\n      secrets:\n        - type: s3\n          provider: credential_chain\n  target: dev\n```\n\n----------------------------------------\n\nTITLE: Installing Python and Dependencies on Ubuntu/Debian for dbt Core\nDESCRIPTION: This set of commands installs the necessary dependencies for dbt Core on Ubuntu/Debian, including git, libpq-dev, Python, and pip. It also upgrades cffi and installs a specific version of cryptography.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Core/install-pip-os-prereqs.md#2025-04-09_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nsudo apt-get install git libpq-dev python-dev python3-pip\nsudo apt-get remove python-cffi\nsudo pip install --upgrade cffi\npip install cryptography~=3.4\n```\n\n----------------------------------------\n\nTITLE: Aggregating Workspace (Gaggle) Level Metrics in dbt\nDESCRIPTION: CTE that aggregates user-level information up to the workspace (Gaggle) level, including first and most recent events, count of events, users, and orders for each workspace.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-02-07-customer-360-view-census-playbook.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\ngaggle_total_facts as (\n\n    select\n        gaggles.gaggle_id,\n        gaggles.gaggle_name,\n        gaggles.created_at,\n\n        min(users.first_event) as first_event,\n        max(users.most_recent_event) as most_recent_event,\n        sum(number_of_events) as number_of_events,\n        count(users.user_id) as number_of_users,\n\n        min(users.first_order) as first_order,\n        max(users.most_recent_order) as most_recent_order,\n        sum(users.number_of_orders) as number_of_orders\n\n    from users\n    left join gaggles on users.gaggle_id = gaggles.gaggle_id\n\n    group by 1,2,3\n\n),\n```\n\n----------------------------------------\n\nTITLE: Running the dbt Parse Command\nDESCRIPTION: Example of running the dbt parse command in the terminal, showing the output that indicates successful execution and the location of performance information.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/parse.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ dbt parse\n13:02:52  Running with dbt=1.5.0\n13:02:53  Performance info: target/perf_info.json\n```\n\n----------------------------------------\n\nTITLE: Verifying Data Load in Snowflake SQL\nDESCRIPTION: These SQL queries verify that data has been successfully loaded into the customers, orders, and payment tables by selecting all records from each table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/sl-snowflake-qs.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nselect * from raw.jaffle_shop.customers;\nselect * from raw.jaffle_shop.orders;\nselect * from raw.stripe.payment;\n```\n\n----------------------------------------\n\nTITLE: Debugging Runtime Error: Connection Failure\nDESCRIPTION: Example of an error message when dbt fails to connect to the database due to incorrect credentials. The error includes details about the specific authentication issue.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/debug-errors.md#2025-04-09_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nEncountered an error:\nRuntime Error\n  Database error while listing schemas in database \"analytics\"\n  Database Error\n    250001 (08001): Failed to connect to DB: your_db.snowflakecomputing.com:443. Incorrect username or password was specified.\n```\n\n----------------------------------------\n\nTITLE: Implementing debug() in dbt Macro\nDESCRIPTION: Example showing how to implement the debug() macro within a custom dbt macro. The macro demonstrates using debug() to inspect complex logic during development.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/debug-method.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n{% macro my_macro() %}\n\n  {% set something_complex = my_complicated_macro() %}\n  \n  {{ debug() }}\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Materialized View in dbt for Databricks\nDESCRIPTION: Basic configuration for creating a materialized view in dbt with Databricks. This is a required config block to use this materialization type.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/databricks-configs.md#2025-04-09_snippet_21\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n   materialized = 'materialized_view'\n ) }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Databricks Token Authentication in profiles.yml\nDESCRIPTION: Configuration example for token-based authentication in Databricks, including required parameters like host, schema, and personal access token.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/databricks-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: databricks\n      catalog: CATALOG_NAME #optional catalog name if you are using Unity Catalog]\n      schema: SCHEMA_NAME # Required\n      host: YOURORG.databrickshost.com # Required\n      http_path: /SQL/YOUR/HTTP/PATH # Required\n      token: dapiXXXXXXXXXXXXXXXXXXXXXXX # Required Personal Access Token (PAT) if using token-based authentication\n      threads: 1_OR_MORE  # Optional, default 1\n```\n\n----------------------------------------\n\nTITLE: Configuring Target Options for Data Lake and Materialized Views\nDESCRIPTION: This code snippet outlines the configuration syntax for target options in data lake and materialized view environments. It includes settings for data retention, compression, compaction, and column transformations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/upsolver-configs.md#2025-04-09_snippet_7\n\nLANGUAGE: markdown\nCODE:\n```\n| Option | Storage   | Editable | Optional | Config Syntax |\n| -------| --------- | -------- | -------- | ------------- |\n| globally_unique_keys | datalake | False | True | 'globally_unique_keys': True/False |\n| storage_connection | datalake | False | True | 'storage_connection': `'<storage_connection>'` |\n| storage_location | datalake | False | True | 'storage_location': `'<storage_location>'` |\n| compute_cluster | datalake | True | True | 'compute_cluster': `'<compute_cluster>'` |\n| compression | datalake | True | True | 'compression': 'SNAPPY/GZIP' |\n| compaction_processes | datalake | True | True | 'compaction_processes': `<integer>` |\n| disable_compaction | datalake | True | True | 'disable_compaction': True/False |\n| retention_date_partition | datalake | False | True | 'retention_date_partition': `'<column>'` |\n| table_data_retention | datalake | True | True | 'table_data_retention': `'<N DAYS>'` |\n| column_data_retention | datalake | True | True | 'column_data_retention': (\\{'COLUMN' : `'<column>'`,'DURATION': `'<N DAYS>'`\\}) |\n| comment | datalake | True | True | 'comment': `'<comment>'` |\n| storage_connection | materialized_view | False | True | 'storage_connection': `'<storage_connection>'` |\n| storage_location | materialized_view | False | True | 'storage_location': `'<storage_location>'` |\n| max_time_travel_duration | materialized_view | True | True | 'max_time_travel_duration': `'<N DAYS>'` |\n| compute_cluster | materialized_view | True | True | 'compute_cluster': `'<compute_cluster>'` |\n| column_transformations | snowflake | False | True | 'column_transformations': \\{`'<column>'` : `'<expression>'` , ...\\} |\n| deduplicate_with | snowflake | False | True | 'deduplicate_with': \\{'COLUMNS' : ['col1', 'col2'],'WINDOW': 'N HOURS'\\} |\n| exclude_columns | snowflake | False | True | 'exclude_columns': (`'<exclude_column>'`, ...) |\n| create_table_if_missing | snowflake | False | True | 'create_table_if_missing': True/False} |\n| run_interval | snowflake | False | True | 'run_interval': `'<N MINUTES/HOURS/DAYS>'` |\n```\n\n----------------------------------------\n\nTITLE: Creating Schema with Authorization in Azure SQL\nDESCRIPTION: This SQL snippet demonstrates how schemas are created with a specific authorization principal in Azure SQL. This is useful when authenticating with a principal who has permissions based on a group, such as a Microsoft Entra ID group.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/mssql-setup.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\nCREATE SCHEMA [schema_name] AUTHORIZATION [schema_authorization]\n```\n\n----------------------------------------\n\nTITLE: Implementing Covariate Encoding with scikit-learn in Python\nDESCRIPTION: Prepares Formula 1 race data by encoding categorical variables, filtering active participants, and creating a simplified position classification. Uses scikit-learn's LabelEncoder for categorical variables and creates a custom position index function.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder,OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\n\ndef model(dbt, session):\n    # dbt configuration\n    dbt.config(packages=[\"pandas\",\"numpy\",\"scikit-learn\"])\n\n    # get upstream data\n    data = dbt.ref(\"ml_data_prep\").to_pandas()\n\n    # list out covariates we want to use in addition to outcome variable we are modeling - position\n    covariates = data[['RACE_YEAR','CIRCUIT_NAME','GRID','CONSTRUCTOR_NAME','DRIVER','DRIVERS_AGE_YEARS','DRIVER_CONFIDENCE','CONSTRUCTOR_RELAIBLITY','TOTAL_PIT_STOPS_PER_RACE','ACTIVE_DRIVER','ACTIVE_CONSTRUCTOR', 'POSITION']]\n\n    # filter covariates on active drivers and constructors\n    # use fil_cov as short for \"filtered_covariates\"\n    fil_cov = covariates[(covariates['ACTIVE_DRIVER']==1)&(covariates['ACTIVE_CONSTRUCTOR']==1)]\n\n    # Encode categorical variables using LabelEncoder\n    # TODO: we'll update this to both ohe in the future for non-ordinal variables! \n    le = LabelEncoder()\n    fil_cov['CIRCUIT_NAME'] = le.fit_transform(fil_cov['CIRCUIT_NAME'])\n    fil_cov['CONSTRUCTOR_NAME'] = le.fit_transform(fil_cov['CONSTRUCTOR_NAME'])\n    fil_cov['DRIVER'] = le.fit_transform(fil_cov['DRIVER'])\n    fil_cov['TOTAL_PIT_STOPS_PER_RACE'] = le.fit_transform(fil_cov['TOTAL_PIT_STOPS_PER_RACE'])\n\n    # Simply target variable \"position\" to represent 3 meaningful categories in Formula1\n    # 1. Podium position 2. Points for team 3. Nothing - no podium or points!\n    def position_index(x):\n        if x<4:\n            return 1\n        if x>10:\n            return 3\n        else :\n            return 2\n\n    # we are dropping the columns that we filtered on in addition to our training variable\n    encoded_data = fil_cov.drop(['ACTIVE_DRIVER','ACTIVE_CONSTRUCTOR'],axis=1))\n    encoded_data['POSITION_LABEL']= encoded_data['POSITION'].apply(lambda x: position_index(x))\n    encoded_data_grouped_target = encoded_data.drop(['POSITION'],axis=1))\n\n    return encoded_data_grouped_target\n```\n\n----------------------------------------\n\nTITLE: Implementing Driver Position Prediction with Python and Snowflake UDFs\nDESCRIPTION: This Python script defines functions for loading a pre-trained machine learning model, creating a Snowflake UDF for prediction, and applying the model to new data. It uses libraries such as joblib, pandas, and Snowflake Snowpark to handle data processing and model execution.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport joblib\nimport pandas as pd\nimport os\nfrom snowflake.snowpark import types as T\n\nDB_STAGE = 'MODELSTAGE'\nversion = '1.0'\n# The name of the model file\nmodel_file_path = 'driver_position_'+version\nmodel_file_packaged = 'driver_position_'+version+'.joblib'\n\n# This is a local directory, used for storing the various artifacts locally\nLOCAL_TEMP_DIR = f'/tmp/driver_position'\nDOWNLOAD_DIR = os.path.join(LOCAL_TEMP_DIR, 'download')\nTARGET_MODEL_DIR_PATH = os.path.join(LOCAL_TEMP_DIR, 'ml_model')\nTARGET_LIB_PATH = os.path.join(LOCAL_TEMP_DIR, 'lib')\n\n# The feature columns that were used during model training\n# and that will be used during prediction\nFEATURE_COLS = [\n        \"RACE_YEAR\"\n        ,\"CIRCUIT_NAME\"\n        ,\"GRID\"\n        ,\"CONSTRUCTOR_NAME\"\n        ,\"DRIVER\"\n        ,\"DRIVERS_AGE_YEARS\"\n        ,\"DRIVER_CONFIDENCE\"\n        ,\"CONSTRUCTOR_RELAIBLITY\"\n        ,\"TOTAL_PIT_STOPS_PER_RACE\"]\n\ndef register_udf_for_prediction(p_predictor ,p_session ,p_dbt):\n\n    # The prediction udf\n\n    def predict_position(p_df: T.PandasDataFrame[int, int, int, int,\n                                        int, int, int, int, int]) -> T.PandasSeries[int]:\n        # Snowpark currently does not set the column name in the input dataframe\n        # The default col names are like 0,1,2,... Hence we need to reset the column\n        # names to the features that we initially used for training.\n        p_df.columns = [*FEATURE_COLS]\n    \n        # Perform prediction. this returns an array object\n        pred_array = p_predictor.predict(p_df)\n        # Convert to series\n        df_predicted = pd.Series(pred_array)\n        return df_predicted\n\n    # The list of packages that will be used by UDF\n    udf_packages = p_dbt.config.get('packages')\n\n    predict_position_udf = p_session.udf.register(\n        predict_position\n        ,name=f'predict_position'\n        ,packages = udf_packages\n    )\n    return predict_position_udf\n\ndef download_models_and_libs_from_stage(p_session):\n    p_session.file.get(f'@{DB_STAGE}/{model_file_path}/{model_file_packaged}', DOWNLOAD_DIR)\n\ndef load_model(p_session):\n    # Load the model and initialize the predictor\n    model_fl_path = os.path.join(DOWNLOAD_DIR, model_file_packaged)\n    predictor = joblib.load(model_fl_path)\n    return predictor\n\n# -------------------------------\ndef model(dbt, session):\n    dbt.config(\n        packages = ['snowflake-snowpark-python' ,'scipy','scikit-learn' ,'pandas' ,'numpy'],\n        materialized = \"table\",\n        tags = \"predict\"\n    )\n    session._use_scoped_temp_objects = False\n    download_models_and_libs_from_stage(session)\n    predictor = load_model(session)\n    predict_position_udf = register_udf_for_prediction(predictor, session ,dbt)\n\n    # Retrieve the data, and perform the prediction\n    hold_out_df = (dbt.ref(\"hold_out_dataset_for_prediction\")\n        .select(*FEATURE_COLS)\n    )\n\n    # Perform prediction.\n    new_predictions_df = hold_out_df.withColumn(\"position_predicted\"\n        ,predict_position_udf(*FEATURE_COLS)\n    )\n\n    return new_predictions_df\n```\n\n----------------------------------------\n\nTITLE: Configuring Incremental Model with 'this' in dbt SQL\nDESCRIPTION: This snippet demonstrates how to use the 'this' keyword in an incremental dbt model. It selects all columns from a raw events table and applies a slow function to a column. For incremental runs, it uses 'this' to filter for only new records based on the event time.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/this.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(materialized='incremental') }}\n\nselect\n    *,\n    my_slow_function(my_column)\n\nfrom raw_app_data.events\n\n{% if is_incremental() %}\n  where event_time > (select max(event_time) from {{ this }})\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Spark Data Types in dbt Unit Tests\nDESCRIPTION: Shows Spark-specific data type handling including arrays, maps, and named structs. Demonstrates proper syntax for complex data structures in Spark.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/data-types.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nunit_tests:\n  - name: test_my_data_types\n    model: fct_data_types\n    given:\n      - input: ref('stg_data_types')\n        rows:\n         - int_field: 1\n           float_field: 2.0\n           str_field: my_string\n           str_escaped_field: \"my,cool'string\"\n           bool_field: true\n           date_field: 2020-01-02\n           timestamp_field: 2013-11-03 00:00:00-0\n           timestamptz_field: 2013-11-03 00:00:00-0\n           int_array_field: 'array(1, 2, 3)'\n           map_field: 'map(\"10\", \"t\", \"15\", \"f\", \"20\", NULL)'\n           named_struct_field: 'named_struct(\"a\", 1, \"b\", 2, \"c\", 3)'\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive Catalog Properties for dbt\nDESCRIPTION: Recommended settings for Hive connector configurations when working with dbt. These settings ensure dbt can perform frequent DROP and RENAME operations by adjusting cache TTL and refresh interval.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/trino-configs.md#2025-04-09_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nhive.metastore-cache-ttl=0s\nhive.metastore-refresh-interval=5s\n```\n\n----------------------------------------\n\nTITLE: Example Snowflake Permission Grants for Custom Setup\nDESCRIPTION: Comprehensive example of SQL grant statements that can be used to set up custom permissions for roles in Snowflake, covering warehouses, databases, schemas, and objects.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/database-permissions/snowflake-permissions.md#2025-04-09_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\ngrant all on warehouse warehouse_name to role role_name;\ngrant usage on database database_name to role role_name;\ngrant create schema on database database_name to role role_name; \ngrant usage on schema database.an_existing_schema to role role_name;\ngrant create table on schema database.an_existing_schema to role role_name;\ngrant create view on schema database.an_existing_schema to role role_name;\ngrant usage on future schemas in database database_name to role role_name;\ngrant monitor on future schemas in database database_name to role role_name;\ngrant select on future tables in database database_name to role role_name;\ngrant select on future views in database database_name to role role_name;\ngrant usage on all schemas in database database_name to role role_name;\ngrant monitor on all schemas in database database_name to role role_name;\ngrant select on all tables in database database_name to role role_name;\ngrant select on all views in database database_name to role role_name;\n```\n\n----------------------------------------\n\nTITLE: Generating dbt Documentation\nDESCRIPTION: Bash command to generate documentation for the dbt project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_42\n\nLANGUAGE: bash\nCODE:\n```\ndbt docs generate\n```\n\n----------------------------------------\n\nTITLE: Excluding Specific Snapshot in dbt Snapshot Command\nDESCRIPTION: This snippet demonstrates how to use the --exclude flag with the dbt snapshot command to execute all snapshots except a specific one named 'snap_order_statuses'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/exclude.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndbt snapshot --exclude \"snap_order_statuses\"\n```\n\n----------------------------------------\n\nTITLE: Configuring OAuth Console Authentication for Trino in dbt profiles.yml\nDESCRIPTION: This snippet shows how to set up OAuth 2.0 console authentication for a Trino connection in the dbt profiles.yml file. It's similar to regular OAuth but prints the authentication URL to the console instead of opening it in a browser.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/trino-setup.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nsandbox-galaxy:\n  target: oauth_console\n  outputs:\n    oauth:\n      type: trino\n      method: oauth_console\n      host: bunbundersders.trino.galaxy-dev.io\n      catalog: dbt_target\n      schema: dataders\n      port: 443\n```\n\n----------------------------------------\n\nTITLE: Incorrect Configuration with Absolute Path\nDESCRIPTION: Shows an example of how not to configure analysis-paths. Using absolute paths is discouraged as it makes the configuration less portable and harder to manage across different environments.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/analysis-paths.md#2025-04-09_snippet_2\n\nLANGUAGE: yml\nCODE:\n```\nanalysis-paths: [\"/Users/username/project/analyses\"]\n```\n\n----------------------------------------\n\nTITLE: Selecting All Columns Except One Using dbt_utils.star Macro in SQL\nDESCRIPTION: Demonstrates how to use the dbt_utils.star macro to select all columns from a table except for a specified one, reducing the need to manually list all desired columns.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-07-13-star-sql-love-letter.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\t{{ dbt_utils.star(from=ref('table_a'), except=['column_56']) }}\nfrom {{ ref('table_a') }}\n```\n\n----------------------------------------\n\nTITLE: Displaying help for dbt clean command\nDESCRIPTION: Shows how to view the list of all supported flags for the dbt clean command in the terminal.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/clean.md#2025-04-09_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndbt clean --help\n```\n\n----------------------------------------\n\nTITLE: Defining Personal Email Domains with a dbt Macro\nDESCRIPTION: Jinja macro that returns a tuple of common personal email domains that will be used to flag emails for exclusion from business-to-business outreach.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-02-07-customer-360-view-census-playbook.md#2025-04-09_snippet_2\n\nLANGUAGE: jinja\nCODE:\n```\n{% macro get_personal_emails() %}\n\n\t{{ return(('gmail.com', 'outlook.com', 'yahoo.com', 'icloud.com', 'hotmail.com')) }}\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Creating Customer Table in Snowflake SQL\nDESCRIPTION: This SQL command creates a customer table in the jaffle_shop schema of the raw database. It defines columns for id, first_name, and last_name.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/sl-snowflake-qs.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\ncreate table raw.jaffle_shop.customers\n( id integer,\n  first_name varchar,\n  last_name varchar\n);\n```\n\n----------------------------------------\n\nTITLE: Not Null Test SQL Implementation\nDESCRIPTION: SQL implementations for the not_null test, showing both compiled and templated versions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/data-tests.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nselect *\nfrom analytics.orders\nwhere order_id is null\n```\n\nLANGUAGE: sql\nCODE:\n```\nselect *\nfrom {{ model }}\nwhere {{ column_name }} is null\n```\n\n----------------------------------------\n\nTITLE: Configuring Source Freshness with Table Exclusion in YAML\nDESCRIPTION: This YAML configuration demonstrates how to set up source freshness checks while excluding a specific table. It defines freshness criteria for the 'jaffle_shop' source and excludes the 'product_skus' table from these checks.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Project/exclude-table-from-freshness.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: jaffle_shop\n    database: raw\n\n    freshness:\n      warn_after: {count: 12, period: hour}\n      error_after: {count: 24, period: hour}\n\n    loaded_at_field: _etl_loaded_at\n\n    tables:\n      - name: orders\n      - name: product_skus\n        freshness: null # do not check freshness for this table\n```\n\n----------------------------------------\n\nTITLE: Referencing Models in Python with dbt-oracle\nDESCRIPTION: Example Python model that demonstrates how to reference another dbt model using dbt.ref() in Oracle ML Cloud.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/oracle-setup.md#2025-04-09_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef model(dbt, session):\n    # Must be either table or incremental (view is not currently supported)\n    dbt.config(materialized=\"table\")\n    # returns oml.core.DataFrame referring a dbt model\n    s_df = dbt.ref(\"sales_cost\")\n    return s_df\n```\n\n----------------------------------------\n\nTITLE: Creating a Latest Version View Macro in SQL\nDESCRIPTION: A custom macro that creates a view with the model's canonical name that points to the latest version. This allows users to access the latest version without specifying a version suffix in their queries.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/govern/model-versions.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\n{% macro create_latest_version_view() %}\n\n    -- this hook will run only if the model is versioned, and only if it's the latest version\n    -- otherwise, it's a no-op\n    {% if model.get('version') and model.get('version') == model.get('latest_version') %}\n\n        {% set new_relation = this.incorporate(path={\"identifier\": model['name']}) %}\n\n        {% set existing_relation = load_relation(new_relation) %}\n\n        {% if existing_relation and not existing_relation.is_view %}\n            {{ drop_relation_if_exists(existing_relation) }}\n        {% endif %}\n        \n        {% set create_view_sql -%}\n            -- this syntax may vary by data platform\n            create or replace view {{ new_relation }}\n              as select * from {{ this }}\n        {%- endset %}\n        \n        {% do log(\"Creating view \" ~ new_relation ~ \" pointing to \" ~ this, info = true) if execute %}\n        \n        {{ return(create_view_sql) }}\n        \n    {% else %}\n    \n        -- no-op\n        select 1 as id\n    \n    {% endif %}\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Enabled Snapshots in dbt YAML\nDESCRIPTION: Example of enabling or disabling snapshots in the dbt_project.yml file. This configuration applies to all snapshots in the specified resource path.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/enabled.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  [<resource-path>]:\n    +enabled: true | false\n```\n\n----------------------------------------\n\nTITLE: Configuring Kerberos Authentication for Hive in dbt Profiles\nDESCRIPTION: YAML configuration for connecting to Hive with Kerberos authentication using GSSAPI to share credentials. This is suitable for environments with Kerberos configured.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/hive-setup.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: hive\n      host: HOSTNAME\n      port: PORT # default value: 10000\n      auth_type: GSSAPI\n      kerberos_service_name: KERBEROS_SERVICE_NAME # default value: None\n      use_http_transport: BOOLEAN # default value: true\n      use_ssl: BOOLEAN # TLS should always be used to ensure secure transmission of credentials, default value: true\n      schema: SCHEMA_NAME\n```\n\n----------------------------------------\n\nTITLE: Creating Dimension Table for Customers in SQL\nDESCRIPTION: SQL query to create a dimension table for customers, joining and aggregating data from staging models for customers and the fact table for orders.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/sl-snowflake-qs.md#2025-04-09_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\nwith customers as (\n   select * from {{ ref('stg_customers')}}\n),\norders as (\n   select * from {{ ref('fct_orders')}}\n),\ncustomer_orders as (\n   select\n       customer_id,\n       min(order_date) as first_order_date,\n       max(order_date) as most_recent_order_date,\n       count(order_id) as number_of_orders,\n       sum(amount) as lifetime_value\n   from orders\n   group by 1\n),\nfinal as (\n   select\n       customers.customer_id,\n       customers.first_name,\n       customers.last_name,\n       customer_orders.first_order_date,\n       customer_orders.most_recent_order_date,\n       coalesce(customer_orders.number_of_orders, 0) as number_of_orders,\n       customer_orders.lifetime_value\n   from customers\n   left join customer_orders using (customer_id)\n)\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Adding dbt docs generate Command to Job Run Steps\nDESCRIPTION: This snippet shows how to add the 'dbt docs generate' command to the list of commands in job run steps. It's used to manually generate documentation as part of a job, but may cause job failure if the step fails.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/build-and-view-your-docs.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n`dbt docs generate`\n```\n\n----------------------------------------\n\nTITLE: Multi-Adapter Unit Testing Implementation\nDESCRIPTION: Unit tests implementation supporting multiple database adapters with separate test logic for each.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-08-22-unit-testing-dbt-package.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n{% macro test_to_literal() %}\n\n    {{ return(adapter.dispatch('test_to_literal', 'integration_tests')(text)) }}\n\n{% endmacro %}\n\n{% macro default__test_to_literal() %}\n\n    {% result = dbt_sample_package.to_literal('test string') %}\n\n    {% if result != \"'test string'\" %}\n\n        {{ exceptions.raise_compiler_error('The test is failed') }}\n\n    {% endif %}\n\n{% endmacro %}\n\n{% macro postgres__test_to_literal() %}\n\n    {% result = dbt_sample_package.to_literal('test string') %}\n\n    {% if result != \"E'test string'\" %}\n\n        {{ exceptions.raise_compiler_error('The test is failed') }}\n\n    {% endif %}\n\n{% endmacro%}\n```\n\n----------------------------------------\n\nTITLE: Configuring OAuth Token-Based Authentication with Temporary Token\nDESCRIPTION: Configuration for connecting to BigQuery using OAuth token-based authentication with a temporary access token.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/bigquery-setup.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmy-bigquery-db:\n  target: dev\n  outputs:\n    dev:\n      type: bigquery\n      method: oauth-secrets\n      project: GCP_PROJECT_ID\n      dataset: DBT_DATASET_NAME\n      threads: 4\n      token: TEMPORARY_ACCESS_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Building Empty Incremental Models with dbt CLI\nDESCRIPTION: Shell command for building empty versions of incremental models using model selection criteria to prepare for unit testing.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/unit-tests.md#2025-04-09_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ndbt run --select \"config.materialized:incremental\" --empty\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Groups in YAML\nDESCRIPTION: Shows how to assign groups to models using dbt_project.yml and schema.yml configuration files\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/group.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  [<resource-path>]:\n    +group: GROUP_NAME\n```\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: MODEL_NAME\n    group: GROUP\n```\n\n----------------------------------------\n\nTITLE: Auto-generated Index Creation SQL\nDESCRIPTION: Shows the automatically generated SQL statements for creating indexes with unique names based on hash values.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/postgres-configs.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\ncreate index if not exists\n\"3695050e025a7173586579da5b27d275\"\non \"my_target_database\".\"my_target_schema\".\"indexed_model\" \nusing hash\n(column_a);\n\ncreate unique index if not exists\n\"1bf5f4a6b48d2fd1a9b0470f754c1b0d\"\non \"my_target_database\".\"my_target_schema\".\"indexed_model\" \n(column_a, column_b);\n```\n\n----------------------------------------\n\nTITLE: Basic Debug Statement Placement After Query Execution\nDESCRIPTION: A Jinja code snippet showing how to place a debug() statement after executing a query, which in this case failed to reach the debug point.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-03-30-guide-to-debug-in-jinja.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{% set my_results = run_query(sql_statement) %}\n{{ debug() }}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Lambda View in SQL with dbt\nDESCRIPTION: A basic lambda view implementation that unions current data from a view with historical data from a table, using dbt's run_started_at variable to separate recent from historical records.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2020-07-01-how-to-create-near-real-time-models-with-just-dbt-sql.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nwith current_view as (\n\nselect * from {{ ref('current_view') }}\n\nwhere max_collector_tstamp >= '{{ run_started_at }}'\n\n),\n\nhistorical_table as (\n\nselect * from {{ ref('historical_table') }}\n\nwhere max_collector_tstamp < '{{ run_started_at }}'\n\n),\n\nunioned_tables as (\n\nselect * from current_view\n\nunion all\n\nselect * from historical_table\n\n)\n\nselect * from unioned_tables\n```\n\n----------------------------------------\n\nTITLE: Running Tests on Specific Resource Types in dbt\nDESCRIPTION: These commands demonstrate how to run tests on models with specific materializations, seeds, and snapshots in dbt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/test-selection-examples.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Run tests on all models with a particular materialization\ndbt test --select \"config.materialized:table\"\n\n# Run tests on all seeds, which use the 'seed' materialization\ndbt test --select \"config.materialized:seed\"\n\n# Run tests on all snapshots, which use the 'snapshot' materialization\ndbt test --select \"config.materialized:snapshot\"\n```\n\n----------------------------------------\n\nTITLE: Running dbt with Static Time Sample\nDESCRIPTION: Command to run dbt with a static time sample between specific dates and times, useful for validating data from a known busy period.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/sample-flag.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --sample=\"{'start': '2024-07-01', 'end': '2024-07-08 18:00:00'}\"\n```\n\n----------------------------------------\n\nTITLE: Checking Status of Airflow Containers\nDESCRIPTION: Command to check the status of Airflow containers after stopping them, verifying that all services are properly shut down.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/airflow-and-dbt-cloud.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nastrocloud dev ps\n```\n\n----------------------------------------\n\nTITLE: Removing logs along with other directories in dbt clean\nDESCRIPTION: Extends the clean-targets configuration to include the logs directory, in addition to target and dbt_packages. This setup will remove all three directories when running dbt clean.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/clean-targets.md#2025-04-09_snippet_2\n\nLANGUAGE: yml\nCODE:\n```\nclean-targets: [target, dbt_packages, logs]\n```\n\n----------------------------------------\n\nTITLE: Configuring Streaming Table in dbt for Databricks\nDESCRIPTION: Basic configuration for creating a streaming table in dbt with Databricks. This allows creating a table that continually processes new data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/databricks-configs.md#2025-04-09_snippet_22\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n   materialized = 'streaming_table'\n ) }}\n```\n\n----------------------------------------\n\nTITLE: GitHub Workflow Configuration\nDESCRIPTION: Initial configuration for GitHub Actions workflow to run dbt Cloud jobs on push to main branch\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/custom-cicd-pipelines.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nname: run dbt Cloud job on push\n\n# This filter says only run this job when there is a push to the main branch\n# This works off the assumption that you've restricted this branch to only all PRs to push to the default branch\n```\n\n----------------------------------------\n\nTITLE: Configuring MySQL Profile in profiles.yml\nDESCRIPTION: Example configuration for connecting to a MySQL database with dbt. Includes required fields like server, port, schema, username, password, and ssl settings.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/mysql-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: mysql\n      server: localhost\n      port: 3306\n      schema: analytics\n      username: your_mysql_username\n      password: your_mysql_password\n      ssl_disabled: True\n```\n\n----------------------------------------\n\nTITLE: Basic Model Version Configuration in YAML\nDESCRIPTION: Core YAML configuration for defining model versions, including version identifiers and column specifications.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/versions.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: model_name\n    versions:\n      - v: <version_identifier> # required\n        defined_in: <file_name> # optional -- default is <model_name>_v<v>\n        columns:\n          - include: <include_value>\n            exclude: <exclude_list>\n          - name: <column_name> # required\n      - v: ...\n    \n    # optional\n    latest_version: <version_identifier>\n```\n\n----------------------------------------\n\nTITLE: Deleting dbt Cloud Webhook\nDESCRIPTION: DELETE request to remove a specific webhook from the dbt Cloud account.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/webhooks.md#2025-04-09_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nDELETE https://{your access URL}/api/v3/accounts/{account_id}/webhooks/subscription/{webhook_id}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"data\": {\n        \"id\": \"wsu_12345abcde\"\n    },\n    \"status\": {\n        \"code\": 200,\n        \"is_success\": true\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration for Intermediate Models\nDESCRIPTION: Defines the dbt model configuration in YAML format. Uses the doc blocks from the Markdown file to provide descriptions for each intermediate model. This will be used for documentation generation in dbt Cloud.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_19\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n - name: int_results\n   description: '{{ doc(\"int_results\") }}'\n - name: int_pit_stops\n   description: '{{ doc(\"int_pit_stops\") }}'\n - name: int_lap_times_years\n   description: '{{ doc(\"int_lap_times_years\") }}'\n```\n\n----------------------------------------\n\nTITLE: Rendering Identifiers Without Database in dbt\nDESCRIPTION: Example showing how to modify the ref macro to render schema and object identifiers without the database reference, useful for Snowflake warehouses where database names might change post-build.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/builtins.md#2025-04-09_snippet_1\n\nLANGUAGE: jinja\nCODE:\n```\n  -- render identifiers without a database\n  {% do return(rel.include(database=false)) %}\n```\n\n----------------------------------------\n\nTITLE: Basic SQL Employee Table Structure\nDESCRIPTION: Example structure of an employee table showing primary key usage\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/entities.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nemployee_id (primary key)\nfirst_name\nlast_name\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests in dbt\nDESCRIPTION: Commands to run specific test types (unit or data) using the test_type selection method.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/core-upgrade/07-upgrading-to-v1.8.md#2025-04-09_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndbt test --select \"test_type:unit\"           # run all unit tests\ndbt test --select \"test_type:data\"           # run all data tests\n```\n\n----------------------------------------\n\nTITLE: Running SQL Queries to Check Audit Results\nDESCRIPTION: This SQL query shows how to check the results of an audit model after running it. The query simply selects all data from the audit model to examine the comparison between the original and refactored models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-03-23-audit-helper.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nselect * from <name of your audit model>\n\n-- or select * from {{ ref('your_audit_model') }} if you're in the dbt Cloud IDE\n```\n\n----------------------------------------\n\nTITLE: Basic Test Path Configuration in dbt\nDESCRIPTION: Sets the basic configuration for test file locations in dbt_project.yml. This setting tells dbt where to look for test files.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/test-paths.md#2025-04-09_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\ntest-paths: [directorypath]\n```\n\n----------------------------------------\n\nTITLE: ModelByEnvironment Query After Deprecation - GraphQL\nDESCRIPTION: Updated modelByEnvironment query moved into the environment object with BigInt ID type.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/2023-release-notes.md#2025-04-09_snippet_4\n\nLANGUAGE: graphql\nCODE:\n```\nquery ($environmentId: BigInt!, $uniqueId: String) {\n    environment(id: $environmentId) {\n        applied {\n        modelHistoricalRuns(uniqueId: $uniqueId) {\n            uniqueId\n            executionTime\n            executeCompletedAt\n        }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Dynamic Source Database in dbt Sources\nDESCRIPTION: YAML configuration for setting up a dynamic source database that resolves based on environment variables. This enables pointing to different databases in production vs non-production environments.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/deploy-environments.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  - name: sensitive_source\n    database: \"{{ env_var('SENSITIVE_SOURCE_DATABASE') }}\"\n    tables:\n      - name: table_with_pii\n```\n\n----------------------------------------\n\nTITLE: Updating deprecated surrogate key macro\nDESCRIPTION: Error occurs when using the deprecated surrogate_key() macro. Resolution requires replacing with the new generate_surrogate_key macro.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/core-upgrade/11-Older versions/upgrading-to-dbt-utils-v1.0.md#2025-04-09_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndbt_utils.surrogate_key -> dbt_utils.generate_surrogate_key\n```\n\n----------------------------------------\n\nTITLE: Creating a Lambda Filter Macro in dbt\nDESCRIPTION: A dbt macro that generates appropriate filtering logic based on materialization type. It handles view, incremental, and table materializations differently to ensure the lambda view pattern works correctly.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2020-07-01-how-to-create-near-real-time-models-with-just-dbt-sql.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{% macro lambda_filter(column_name) %}\n\n{% set materialized = config.require('materialized') %}\n\n{% set filter_time = var(lambda_timestamp, run_started_at) %}\n\n{% if materialized == 'view' %}\n\nwhere {{ column_name }} >= '{{ filter_time }}'\n\n{% elif is_incremental() %}\n\nwhere {{ column_name }} >= (select max({{ column_name }}) from {{ this }})\n\nand {{ column_name }} < '{{ filter_time }}'\n\n{% else %}\n\nwhere {{ column_name }} < '{{ filter_time }}'\n\n{% endif %}\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Running Tests on a Single Model in dbt\nDESCRIPTION: This command demonstrates how to use the --select flag to run tests on a specific model named 'customers' in dbt. The --select flag (or -s for short) allows you to target individual models for testing.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Tests/test-one-model.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndbt test --select customers\n```\n\n----------------------------------------\n\nTITLE: Running Tests on a Single Model in dbt\nDESCRIPTION: This command demonstrates how to use the --select flag to run tests on a specific model named 'customers' in dbt. The --select flag (or -s for short) allows you to target individual models for testing.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Tests/test-one-model.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndbt test --select customers\n```\n\n----------------------------------------\n\nTITLE: Schema Usage Grants with On-Run-End Hook\nDESCRIPTION: YAML configuration showing how to grant schema usage permissions using on-run-end hooks.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-07-26-configuring-grants.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\non-run-end:\n\t# better as a macro\n\t- \"{% for schema in schemas %}grant usage on schema {{ schema }} to reporter;{% endfor %}\"\n```\n\n----------------------------------------\n\nTITLE: Rendering Explorer Course Link Component in Markdown\nDESCRIPTION: This code snippet renders the imported ExplorerCourse component. It's used to display the content of the Explorer course link in the document.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/explore-multiple-projects.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n<ExplorerCourse />\n```\n\n----------------------------------------\n\nTITLE: Expanding Column Types\nDESCRIPTION: Demonstrates how to expand column types to match a template relation\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/adapter.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{% set tmp_relation = adapter.get_relation(...) %}\n{% set target_relation = adapter.get_relation(...) %}\n\n{% do adapter.expand_target_column_types(tmp_relation, target_relation) %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Post-Hook in dbt_project.yml\nDESCRIPTION: Project configuration that applies the create_latest_version_view macro as a post-hook to all models. This automatically creates the latest version view for any versioned model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/govern/model-versions.md#2025-04-09_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\n# dbt_project.yml\nmodels:\n  post-hook:\n    - \"{{ create_latest_version_view() }}\"\n```\n\n----------------------------------------\n\nTITLE: Specifying Hub Package with Version Range\nDESCRIPTION: This snippet demonstrates how to specify a hub package with a version range using semantic versioning. It pins the package to the latest patch version from a specific minor release.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/packages.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\npackages:\n  - package: dbt-labs/snowplow\n    version: [\">=0.7.0\", \"<0.8.0\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Version Selection in YAML\nDESCRIPTION: YAML configuration for selecting model versions in development environment, excluding old versions using selectors.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/govern/model-versions.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nselectors:\n  - name: exclude_old_versions\n    default: \"{{ target.name == 'dev' }}\"\n    definition:\n      method: fqn\n      value: \"*\"\n      exclude:\n        - method: version\n          value: old\n```\n\n----------------------------------------\n\nTITLE: Basic usage of + prefix in dbt_project.yml\nDESCRIPTION: This example demonstrates how to use the + prefix with materialization configurations at different levels of the project hierarchy. The root level uses +materialized: view while the marts subdirectory uses +materialized: table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/plus-prefix.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nname: jaffle_shop\nconfig-version: 2\n\n...\n\nmodels:\n  +materialized: view\n  jaffle_shop:\n    marts:\n      +materialized: table\n```\n\n----------------------------------------\n\nTITLE: Creating a Staging Customers Model in SQL\nDESCRIPTION: A SQL query that selects and renames columns from the jaffle_shop_customers table to create a staging customers model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/databricks-qs.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    id as customer_id,\n    first_name,\n    last_name\n\nfrom jaffle_shop_customers\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Cloud CLI with YAML file\nDESCRIPTION: Example structure of the dbt_cloud.yml configuration file used to set up the dbt Cloud CLI. It includes version, context, and project details.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/configure-cloud-cli.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: \"1\"\ncontext:\n  active-project: \"<project id from the list below>\"\n  active-host: \"<active host from the list>\"\n  defer-env-id: \"<optional defer environment id>\"\nprojects:\n  - project-name: \"<project-name>\"\n    project-id: \"<project-id>\"\n    account-name: \"<account-name>\"\n    account-id: \"<account-id>\"\n    account-host: \"<account-host>\" # for example, \"cloud.getdbt.com\"\n    token-name: \"<pat-or-service-token-name>\"\n    token-value: \"<pat-or-service-token-value>\"\n\n  - project-name: \"<project-name>\"\n    project-id: \"<project-id>\"\n    account-name: \"<account-name>\"\n    account-id: \"<account-id>\"\n    account-host: \"<account-host>\" # for example, \"cloud.getdbt.com\"\n    token-name: \"<pat-or-service-token-name>\"\n    token-value: \"<pat-or-service-token-value>\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Pre-commit-dbt with YAML\nDESCRIPTION: Configuration for pre-commit-dbt that enforces two rules: all models must be listed in a YAML file, and all models must have tests. This configuration file should be created at the root of the dbt project and includes hooks for generating documentation and checking model properties.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-07-26-pre-commit-dbt.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nrepos:\n- repo: https://github.com/offbi/pre-commit-dbt\n  rev: v1.0.0\n  hooks:\n  - id: dbt-docs-generate\n  - id: check-model-has-properties-file\n    name: Check that all models are listed in a YAML file\n    files: ^models/\n  - id: check-model-has-tests\n    name: Check that all models have tests\n    files: ^models/\n```\n\n----------------------------------------\n\nTITLE: Installing dbt Prerelease on Unix/macOS\nDESCRIPTION: Commands to install dbt-core and adapter prereleases, activate the virtual environment, and check dbt version on Unix/macOS systems.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/pip-install.md#2025-04-09_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\npython3 -m pip install --pre dbt-core dbt-adapter-name\nsource .venv/bin/activate\ndbt --version\n```\n\n----------------------------------------\n\nTITLE: Configuring Authorized Views in dbt_project.yml\nDESCRIPTION: Sets the grant_access_to config for models in the dbt_project.yml file. This configuration specifies which datasets the view should have access to, enabling creation of authorized views in BigQuery.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/bigquery-configs.md#2025-04-09_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  [<resource-path>]:\n    +grant_access_to:\n      - project: project_1\n        dataset: dataset_1\n      - project: project_2\n        dataset: dataset_2\n```\n\n----------------------------------------\n\nTITLE: Implementing Merge Strategy in Databricks SQL\nDESCRIPTION: Example of implementing the merge incremental strategy which updates records based on a unique key. Requires delta or hudi file format and specific Databricks runtime versions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/databricks-configs.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized='incremental',\n    file_format='delta', # or 'hudi'\n    unique_key='user_id',\n    incremental_strategy='merge'\n) }}\n\nwith new_events as (\n\n    select * from {{ ref('events') }}\n\n    {% if is_incremental() %}\n    where date_day >= date_add(current_date, -1)\n    {% endif %}\n\n)\n\nselect\n    user_id,\n    max(date_day) as last_seen\n\nfrom events\ngroup by 1\n```\n\n----------------------------------------\n\nTITLE: Creating Staging Model for Orders in SQL\nDESCRIPTION: This SQL snippet creates a staging model for order data, selecting and renaming relevant columns from the source table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/starburst-galaxy-qs.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    id as order_id,\n    user_id as customer_id,\n    order_date,\n    status\n\nfrom dbt_quickstart.jaffle_shop.jaffle_shop_orders\n```\n\n----------------------------------------\n\nTITLE: Using Relative Paths for asset-paths Configuration\nDESCRIPTION: Recommended approach for configuring asset-paths using relative paths. This is the proper way to specify directories that should be copied during documentation generation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/asset-paths.md#2025-04-09_snippet_1\n\nLANGUAGE: yml\nCODE:\n```\nasset-paths: [\"assets\"]\n```\n\n----------------------------------------\n\nTITLE: Snowflake dbt Seed Error Example\nDESCRIPTION: Example of the database error encountered in Snowflake when seed file columns have changed, showing the invalid identifier 'COUNTRY_NAME' error.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Seeds/full-refresh-seed.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ dbt seed\nRunning with dbt=1.6.0-rc2\nFound 0 models, 0 tests, 0 snapshots, 0 analyses, 130 macros, 0 operations, 1 seed file, 0 sources\n\n12:12:27 | Concurrency: 8 threads (target='dev_snowflake')\n12:12:27 |\n12:12:27 | 1 of 1 START seed file dbt_claire.country_codes...................... [RUN]\n12:12:30 | 1 of 1 ERROR loading seed file dbt_claire.country_codes.............. [ERROR in 2.78s]\n12:12:31 |\n12:12:31 | Finished running 1 seed in 10.05s.\n\nCompleted with 1 error and 0 warnings:\n\nDatabase Error in seed country_codes (seeds/country_codes.csv)\n  000904 (42000): SQL compilation error: error line 1 at position 62\n  invalid identifier 'COUNTRY_NAME'\n\nDone. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1\n```\n\n----------------------------------------\n\nTITLE: Basic SQL CROSS JOIN Syntax\nDESCRIPTION: Demonstrates the basic syntax for creating a cross join between two tables in SQL. Unlike regular joins, cross joins don't use keys to join database objects together.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/joins/sql-cross-join.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    <fields>\nfrom <table_1> as t1\ncross join <table_2> as t2\n```\n\n----------------------------------------\n\nTITLE: Customer Payment Journey Analysis with Versioned Reference\nDESCRIPTION: SQL model showing how to reference a specific version of an upstream model while analyzing customer payment patterns.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/mesh-qs.md#2025-04-09_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nwith stg_payments as (\n    select * from {{ ref('stg_payments') }}\n),\n\nfct_orders as (\n    select * from {{ ref('analytics', 'fct_orders', v=1) }}\n),\n\nfinal as (\n    select \n        days_as_customer_at_purchase,\n        {{ dbt_utils.pivot(\n            'payment_method',\n            dbt_utils.get_column_values(ref('stg_payments'), 'payment_method'),\n            agg='sum',\n            then_value='amount',\n            prefix='total_',\n            suffix='_amount'\n        ) }}, \n        sum(amount) as total_amount\n    from fct_orders\n    left join stg_payments using (order_id)\n    group by 1\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Running Multi-Profile Tests\nDESCRIPTION: Commands for executing tests with different connection profiles\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_32\n\nLANGUAGE: sh\nCODE:\n```\npython3 -m pytest tests/functional --profile apache_spark\npython3 -m pytest tests/functional --profile databricks_sql_endpoint\n```\n\n----------------------------------------\n\nTITLE: Retrieving Snowflake OAuth Client Credentials\nDESCRIPTION: SQL query to fetch the Client ID and Client Secret from a configured Snowflake security integration. These credentials are required when configuring the connection in dbt Cloud.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/manage-access/set-up-snowflake-oauth.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nwith\n\nintegration_secrets as (\n  select parse_json(system$show_oauth_client_secrets('DBT_CLOUD')) as secrets\n)\n\nselect\n  secrets:\"OAUTH_CLIENT_ID\"::string     as client_id,\n  secrets:\"OAUTH_CLIENT_SECRET\"::string as client_secret\nfrom\n  integration_secrets;\n```\n\n----------------------------------------\n\nTITLE: Retrieving Snowflake OAuth Client Credentials\nDESCRIPTION: SQL query to fetch the Client ID and Client Secret from a configured Snowflake security integration. These credentials are required when configuring the connection in dbt Cloud.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/manage-access/set-up-snowflake-oauth.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nwith\n\nintegration_secrets as (\n  select parse_json(system$show_oauth_client_secrets('DBT_CLOUD')) as secrets\n)\n\nselect\n  secrets:\"OAUTH_CLIENT_ID\"::string     as client_id,\n  secrets:\"OAUTH_CLIENT_SECRET\"::string as client_secret\nfrom\n  integration_secrets;\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Database for Test Results in dbt\nDESCRIPTION: This snippet shows how to customize the database for storing test failure results in your dbt_project.yml file. This configuration enables storing test failures and specifies 'test_results' as the storage database.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/database.md#2025-04-09_snippet_4\n\nLANGUAGE: yml\nCODE:\n```\ntests:\n  +store_failures: true\n  +database: test_results\n```\n\n----------------------------------------\n\nTITLE: Markdown Card Components for dbt Cloud Features\nDESCRIPTION: A collection of Card components written in Markdown that describe various dbt Cloud features including CLI, IDE, Visual Editor, Copilot, and other core functionalities. Each card contains a title, description, link, and icon.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/about-cloud/about-dbt-cloud.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<div className=\"grid--3-col\">\n\n<Card\n    title=\"dbt Cloud CLI\"\n    body=\"Use the dbt Cloud CLI to develop, test, run, and version control dbt projects and commands, directly from the command line.\"\n    link=\"/docs/cloud/cloud-cli-installation\"\n    icon=\"dbt-bit\"/>\n\n<Card\n    title=\"dbt Cloud IDE\"\n    body=\"The IDE is the easiest and most efficient way to develop dbt models, allowing you to build, test, run, and version control your dbt projects directly from your browser.\"\n    link=\"/docs/cloud/dbt-cloud-ide/develop-in-the-cloud\"\n    icon=\"dbt-bit\"/>\n\n/* Additional cards omitted for brevity */\n\n</div>\n```\n\n----------------------------------------\n\nTITLE: Configuring Begin Parameter with Relative Dates in SQL\nDESCRIPTION: Example demonstrating how to set a dynamic 'begin' parameter using Python datetime module for relative date calculations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/begin.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        materialized = 'incremental',\n        incremental_strategy='microbatch',\n        unique_key = 'run_id',\n        begin=(modules.datetime.datetime.now() - modules.datetime.timedelta(1)).isoformat(),\n        event_time='created_at',\n        batch_size='day',\n    )\n}}\n```\n\n----------------------------------------\n\nTITLE: Using a Single Column as unique_key in a Snapshot (dbt v1.9+)\nDESCRIPTION: Example of using a single column 'id' as the unique key for a snapshot in dbt v1.9+. This configuration uses the YAML-based snapshot definition format.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/unique_key.md#2025-04-09_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  - name: orders_snapshot\n    relation: source('jaffle_shop', 'orders')\n    config:\n      schema: snapshots\n      unique_key: id\n      strategy: timestamp\n      updated_at: updated_at\n```\n\n----------------------------------------\n\nTITLE: Setting up Project Structure with GitLab CI/CD\nDESCRIPTION: Shows the recommended project structure for implementing GitLab CI/CD with a SQLFluff linting job in a dbt project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/set-up-ci.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nmy_awesome_project\n dbt_project.yml\n .gitlab-ci.yml\n```\n\n----------------------------------------\n\nTITLE: March 2025 dbt Dependencies Configuration\nDESCRIPTION: List of dbt Core and adapter versions included in the March 2025 Compatible release, showing dependencies and their versions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/compatible-track-changelog.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndbt-core==1.9.3\n\n# shared interfaces\ndbt-adapters==1.14.1\ndbt-common==1.15.0\ndbt-semantic-interfaces==0.7.4\n\n# adapters\ndbt-athena==1.9.2\ndbt-bigquery==1.9.1\ndbt-databricks==1.9.7\ndbt-fabric==1.9.2\ndbt-postgres==1.9.0\ndbt-redshift==1.9.1\ndbt-snowflake==1.9.2\ndbt-spark==1.9.2\ndbt-synapse==1.8.2\ndbt-teradata==1.9.1\ndbt-trino==1.9.0\n```\n\n----------------------------------------\n\nTITLE: Using itertools Module for Cartesian Product\nDESCRIPTION: Shows how to use itertools.product to generate a cartesian product of two lists.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/modules.md#2025-04-09_snippet_3\n\nLANGUAGE: jinja\nCODE:\n```\n{%- set A = [1, 2] -%}\n{%- set B = ['x', 'y', 'z'] -%}\n{%- set AB_cartesian = modules.itertools.product(A, B) -%}\n\n{%- for item in AB_cartesian %}\n  {{ item }}\n{%- endfor -%}\n```\n\nLANGUAGE: text\nCODE:\n```\n  (1, 'x')\n  (1, 'y')\n  (1, 'z')\n  (2, 'x')\n  (2, 'y')\n  (2, 'z')\n```\n\n----------------------------------------\n\nTITLE: Selecting Fact Table Columns in SQL\nDESCRIPTION: SQL code that selects and creates the required columns for the fact table, including the sales surrogate key and business metrics like revenue which is calculated from unitprice and orderqty.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-04-18-building-a-kimball-dimensional-model-with-dbt.md#2025-04-09_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\n...\n\nselect\n    {{ dbt_utils.generate_surrogate_key(['stg_salesorderdetail.salesorderid', 'salesorderdetailid']) }} as sales_key,\n    stg_salesorderdetail.salesorderid,\n    stg_salesorderdetail.salesorderdetailid,\n    stg_salesorderdetail.unitprice,\n    stg_salesorderdetail.orderqty,\n    stg_salesorderdetail.revenue\nfrom stg_salesorderdetail\ninner join stg_salesorderheader on stg_salesorderdetail.salesorderid = stg_salesorderheader.salesorderid\n```\n\n----------------------------------------\n\nTITLE: Configuring BigQuery Grants in YAML Schema\nDESCRIPTION: This example demonstrates how to configure grants for a specific model in a schema.yml file for BigQuery, granting the dataViewer role to a user.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/grants.md#2025-04-09_snippet_10\n\nLANGUAGE: yml\nCODE:\n```\nmodels:\n  - name: specific_model\n    config:\n      grants:\n        roles/bigquery.dataViewer: ['user:someone@yourcompany.com']\n```\n\n----------------------------------------\n\nTITLE: Overriding Built-in Materialization in dbt Core v1.6\nDESCRIPTION: Example of using a built-in materialization package override from the root project via a wrapping materialization. This approach is still supported in dbt Core v1.6.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/core-upgrade/11-Older versions/09-upgrading-to-v1.6.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{% materialization view, default %}\n{{ return(my_cool_package.materialization_view_default()) }}\n{% endmaterialization %}\n```\n\n----------------------------------------\n\nTITLE: Selecting All Nodes in YAML Selectors\nDESCRIPTION: Definition showing how to use the * operator to select all nodes in a dbt project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/yaml-selectors.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ndefinition:\n  method: fqn\n  value: \"*\"\n```\n\n----------------------------------------\n\nTITLE: Custom generate_schema_name Macro Calling Another Macro in SQL/Jinja\nDESCRIPTION: This snippet demonstrates a custom generate_schema_name macro that calls another macro named generate_schema_name_for_env.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/debug-schema-names.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{% macro generate_schema_name(custom_schema_name, node) -%}\n    {{ generate_schema_name_for_env(custom_schema_name, node) }}\n{%- endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Defining Manual Exposure in YAML for dbt Cloud\nDESCRIPTION: Example of manually defining an exposure in a YAML file for use with data health tiles in dbt Cloud. This snippet shows the basic structure and required properties for a dashboard exposure.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/data-tile.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexposures:\n  - name: example_dashboard\n    type: dashboard\n    # Other required properties would be defined here\n```\n\n----------------------------------------\n\nTITLE: Creating a Daily Time Spine SQL Model in dbt\nDESCRIPTION: This SQL model creates a time spine with daily granularity, generating dates from 5 years in the past to 30 days in the future. It uses dbt's date_spine macro to create the date series and filters to retain only the relevant date range.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/mf-time-spine.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{{{\n    config(\n        materialized = 'table',\n    )\n}}}\n\nwith\n\nbase_dates as (\n    {{\n        dbt.date_spine(\n            'day',\n            \"DATE('2000-01-01')\",\n            \"DATE('2030-01-01')\"\n        )\n    }}\n),\n\nfinal as (\n    select\n        cast(date_day as date) as date_day\n    from base_dates\n)\n\nselect *\nfrom final\nwhere date_day > dateadd(year, -5, current_date())  -- Keep recent dates only\n  and date_day < dateadd(day, 30, current_date());\n```\n\n----------------------------------------\n\nTITLE: Using datetime Module in Jinja\nDESCRIPTION: Demonstrates how to use the datetime module to get current time and calculate dates. Shows datetime.now() and timedelta operations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/modules.md#2025-04-09_snippet_0\n\nLANGUAGE: jinja\nCODE:\n```\n{% set now = modules.datetime.datetime.now() %}\n{% set three_days_ago_iso = (now - modules.datetime.timedelta(3)).isoformat() %}\n```\n\n----------------------------------------\n\nTITLE: BigQuery Table Creation and Replacement in dbt\nDESCRIPTION: Demonstrates how dbt transforms a simple SELECT statement into BigQuery-specific SQL for creating and replacing tables. Includes a comment about the API call needed for dataset creation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Models/sql-dialect.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n-- Make an API call to create a dataset (no DDL interface for this)!!;\n\ncreate or replace table `dbt-dev-87681`.`dbt_alice`.`test_model` as (\n  select 1 as my_column\n);\n```\n\n----------------------------------------\n\nTITLE: Configuring Pagination for Object Results\nDESCRIPTION: Configuration for customizing pagination settings when dealing with large numbers of objects in Snowflake schemas. Allows setting custom page size and limit values.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snowflake-configs.md#2025-04-09_snippet_19\n\nLANGUAGE: yaml\nCODE:\n```\nflags:\n  list_relations_per_page: 10000\n  list_relations_page_limit: 100\n```\n\n----------------------------------------\n\nTITLE: GraphQL Fragment Query for Lineage\nDESCRIPTION: GraphQL query example demonstrating how to use fragments to query across lineage and retrieve results from specific node types including models, sources, seeds, and snapshots.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/discovery-querying.md#2025-04-09_snippet_2\n\nLANGUAGE: graphql\nCODE:\n```\nquery ($environmentId: BigInt!, $first: Int!) {\n  environment(id: $environmentId) {\n    applied {\n      models(first: $first, filter: { uniqueIds: \"MODEL.PROJECT.MODEL_NAME\" }) {\n        edges {\n          node {\n            name\n            ancestors(types: [Model, Source, Seed, Snapshot]) {\n              ... on ModelAppliedStateNestedNode {\n                name\n                resourceType\n                materializedType\n                executionInfo {\n                  executeCompletedAt\n                }\n              }\n              ... on SourceAppliedStateNestedNode {\n                sourceName\n                name\n                resourceType\n                freshness {\n                  maxLoadedAt\n                }\n              }\n              ... on SnapshotAppliedStateNestedNode {\n                name\n                resourceType\n                executionInfo {\n                  executeCompletedAt\n                }\n              }\n              ... on SeedAppliedStateNestedNode {\n                name\n                resourceType\n                executionInfo {\n                  executeCompletedAt\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Directory-Specific DBT Tests\nDESCRIPTION: Command pattern for running tests on specific models or directories in DBT, particularly focusing on staging models prefixed with 'stg_'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/tutorial-next-steps-tests.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndbt test --models stg_*\n```\n\n----------------------------------------\n\nTITLE: Embedding Data Health Tile in PowerBI with HTML iFrame\nDESCRIPTION: HTML iFrame code for embedding a dbt Explorer data health tile in PowerBI. This code creates a PowerBI measure that contains the iFrame with the necessary parameters including exposure information and metadata token.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/data-tile.md#2025-04-09_snippet_1\n\nLANGUAGE: html\nCODE:\n```\nWebsite =\n\"<iframe src='https://1234.metadata.ACCESS_URL/exposure-tile?uniqueId=exposure.EXPOSURE_NAME&environmentType=staging&environmentId=123456789&token=YOUR_METADATA_TOKEN' title='Exposure status tile' height='400'></iframe>\"\n```\n\n----------------------------------------\n\nTITLE: Installing ODBC Prerequisites on Debian/Ubuntu\nDESCRIPTION: Command to install ODBC header files required for the Microsoft Fabric Data Warehouse setup on Debian/Ubuntu systems.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/fabric-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install unixodbc-dev\n```\n\n----------------------------------------\n\nTITLE: Configuring Temporary Table Types in dbt Project YAML for Snowflake\nDESCRIPTION: Configuration in dbt_project.yml to set the temporary relation type (table or view) for incremental models in Snowflake. Views are the default, but tables can be used in certain situations for better performance.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snowflake-configs.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nname: my_project\n\n...\n\nmodels:\n  <resource-path>:\n    +tmp_relation_type: table | view ## If not defined, view is the default.\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Groups in SQL\nDESCRIPTION: Demonstrates how to set group configuration directly in SQL model files\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/group.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n  group='GROUP_NAME'\n) }}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Databricks Tables in dbt 1.9+\nDESCRIPTION: Configuration options for Databricks tables in dbt v1.9 and above, including support for Iceberg table format in addition to all v1.8 options.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/databricks-configs.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Option    | Description| Required?     | Model support   | Example      |\n|-------------|--------|-----------|-----------------|---------------|\n| table_format   | Whether or not to provision [Iceberg](https://docs.databricks.com/en/delta/uniform.html) compatibility for the materialization     | Optional     | SQL, Python     | `iceberg`    |\n| file_format <sup></sup>        | The file format to use when creating tables (`parquet`, `delta`, `hudi`, `csv`, `json`, `text`, `jdbc`, `orc`, `hive` or `libsvm`).   | Optional     | SQL, Python     | `delta`     |\n| location_root       | The created table uses the specified directory to store its data. The table alias is appended to it.     | Optional  | SQL, Python     | `/mnt/root`  |\n| partition_by        | Partition the created table by the specified columns. A directory is created for each partition. | Optional   | SQL, Python     | `date_day`  |\n| liquid_clustered_by | Cluster the created table by the specified columns. Clustering method is based on [Delta's Liquid Clustering feature](https://docs.databricks.com/en/delta/clustering.html). Available since dbt-databricks 1.6.2. | Optional          | SQL, Python     | `date_day` |\n| clustered_by        | Each partition in the created table will be split into a fixed number of buckets by the specified columns.      | Optional     | SQL, Python     | `country_code`           |\n| buckets    | The number of buckets to create while clustering   | Required if `clustered_by` is specified   | SQL, Python     | `8`        |\n| tblproperties   | [Tblproperties](https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-tblproperties.html) to be set on the created table   | Optional     | SQL, Python*    | `{'this.is.my.key': 12}` |\n| databricks_tags     | [Tags](https://docs.databricks.com/en/data-governance/unity-catalog/tags.html) to be set on the created table     | Optional    | SQL <sup></sup> , Python <sup></sup> | `{'my_tag': 'my_value'}` |\n| compression   | Set the compression algorithm.   | Optional    | SQL, Python     | `zstd`    |\n```\n\n----------------------------------------\n\nTITLE: Launching fly.io Application\nDESCRIPTION: Command to launch and deploy the application to fly.io, which makes it available to receive webhook events from dbt Cloud.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/serverless-pagerduty.md#2025-04-09_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nflyctl launch\n```\n\n----------------------------------------\n\nTITLE: Implementing Incremental Model Logic\nDESCRIPTION: SQL implementation for the incremental model showing the core logic with incremental conditional\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-07-12-change-data-capture.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    *\nfrom {{ ref('fct_income') }}\n{% if is_incremental() %}\n    where true\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Service Principal Authentication for Azure SQL in dbt\nDESCRIPTION: This configuration uses a service principal to authenticate with Azure SQL. It requires tenant ID, client ID (application ID), and client secret to be specified in the profiles.yml file for authentication.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/mssql-setup.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: sqlserver\n      driver: 'ODBC Driver 18 for SQL Server' # (The ODBC Driver installed on your system)\n      server: hostname or IP of your server\n      port: 1433\n      database: exampledb\n      schema: schema_name\n      authentication: ServicePrincipal\n      tenant_id: 00000000-0000-0000-0000-000000001234\n      client_id: 00000000-0000-0000-0000-000000001234\n      client_secret: S3cret!\n```\n\n----------------------------------------\n\nTITLE: Referencing Other Models in Python\nDESCRIPTION: Shows how to reference upstream models and sources in a Python model using dbt.ref() and dbt.source() methods, which return DataFrames pointing to those objects.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/python-models.md#2025-04-09_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef model(dbt, session):\n\n    # DataFrame representing an upstream model\n    upstream_model = dbt.ref(\"upstream_model_name\")\n\n    # DataFrame representing an upstream source\n    upstream_source = dbt.source(\"upstream_source_name\", \"table_name\")\n\n    ...\n```\n\n----------------------------------------\n\nTITLE: Replacing modelByEnvironment with environment Query in dbt Discovery API\nDESCRIPTION: A GraphQL query demonstrating how to use the environment object to replace the deprecated modelByEnvironment query. This example shows how to fetch historical runs for a specific model within an environment.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/schema-discovery-environment.mdx#2025-04-09_snippet_2\n\nLANGUAGE: graphql\nCODE:\n```\nquery ($environmentId: BigInt!, $uniqueId: String) {\n  environment(id: $environmentId) {\n    applied {\n      modelHistoricalRuns(uniqueId: $uniqueId) {\n        uniqueId\n        executionTime\n        executeCompletedAt\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Staging Order Model in SQL\nDESCRIPTION: This SQL code creates a staging model for order data, selecting and renaming columns from the raw orders table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/snowflake-qs.md#2025-04-09_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    id as order_id,\n    user_id as customer_id,\n    order_date,\n    status\n\nfrom raw.jaffle_shop.orders\n```\n\n----------------------------------------\n\nTITLE: Testing Connection in dbt Cloud IDE\nDESCRIPTION: Tests the connection specifically within the dbt Cloud IDE environment using the --connection flag\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/debug.md#2025-04-09_snippet_3\n\nLANGUAGE: text\nCODE:\n```\ndbt debug --connection\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Seeds Directory in dbt_project.yml\nDESCRIPTION: YAML configuration to specify a custom directory path for seed files in dbt. This setting overrides the default 'seeds' directory by updating the seed-paths configuration in the project file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Seeds/configurable-data-path.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nseed-paths: [\"custom_seeds\"]\n```\n\n----------------------------------------\n\nTITLE: Defining Documentation Navigation Cards in JSX\nDESCRIPTION: JSX/HTML markup for creating a navigation grid of documentation cards that link to different dbt output configuration topics. Uses custom Card components arranged in a two-column grid layout.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/organize-your-outputs.md#2025-04-09_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\n<div className=\"grid--2-col\">\n\n<Card\n    title=\"Custom schemas\"\n    body=\"Learn how to use the <code>schema</code> configuration key to specify a custom schema.\"\n    link=\"/docs/build/custom-schemas\"\n    icon=\"dbt-bit\"/>\n\n<Card\n    title=\"Custom databases\"\n    body=\"Learn how to use the <code>database</code> configuration key to specify a custom database.\"\n    link=\"/docs/build/custom-databases\"\n    icon=\"dbt-bit\"/>\n\n</div>\n<br />\n<div className=\"grid--2-col\">\n\n<Card\n    title=\"Custom aliases\"\n    body=\"Learn how to use the <code>alias</code> model configuration to change the name of a model's identifier in the database.\"\n    link=\"/docs/build/custom-aliases\"\n    icon=\"dbt-bit\"/>\n\n<Card\n    title=\"Custom target names\"\n    body=\"Learn how to define a custom target name for a dbt Cloud job.\"\n    link=\"/docs/build/custom-target-names\"\n    icon=\"dbt-bit\"/>\n\n</div>\n```\n\n----------------------------------------\n\nTITLE: Aliasing a Seed in dbt_project.yml\nDESCRIPTION: Configures an alias for a seed named 'product_categories' at the project level in the dbt_project.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/alias.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  your_project:\n    product_categories:\n      +alias: categories_data\n```\n\n----------------------------------------\n\nTITLE: Using Reusable Component for Setup Pages in dbt Adapter Documentation\nDESCRIPTION: This code snippet shows how to import and use a reusable component to auto-fill frontmatter content on new adapter setup pages. The SetUpPages component standardizes the introduction section across all adapter setup documentation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_34\n\nLANGUAGE: markdown\nCODE:\n```\nimport SetUpPages from '/snippets/_setup-pages-intro.md';\n\n<SetUpPages meta={frontMatter.meta} />\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Payload for Run Errored Event\nDESCRIPTION: This example demonstrates the JSON payload that dbt Cloud sends when a job run has encountered an error. It includes details about the failed job, including the error timestamp.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/webhooks.md#2025-04-09_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"accountId\": 1,\n  \"webhooksID\": \"wsu_12345abcde\",\n  \"eventId\": \"wev_2L6m5BggBw9uPNuSmtg4MUiW4Re\",\n  \"timestamp\": \"2023-01-31T21:15:20.419714619Z\",\n  \"eventType\": \"job.run.errored\",\n  \"webhookName\": \"test\",\n  \"data\": {\n    \"jobId\": \"123\",\n    \"jobName\": \"dbt Vault\",\n    \"runId\": \"12345\",\n    \"environmentId\": \"1234\",\n    \"environmentName\": \"dbt Vault Demo\",\n    \"dbtVersion\": \"1.0.0\",\n    \"projectName\": \"Snowflake Github Demo\",\n    \"projectId\": \"167194\",\n    \"runStatus\": \"Errored\",\n    \"runStatusCode\": 20,\n    \"runStatusMessage\": \"None\",\n    \"runReason\": \"Kicked off from UI by test@test.com\",\n    \"runStartedAt\": \"2023-01-31T21:14:41Z\",\n    \"runErroredAt\": \"2023-01-31T21:15:20Z\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Contract Enforcement with Data Type Aliasing in dbt YAML\nDESCRIPTION: Example of disabling data type aliasing in a dbt model contract. By default, dbt uses type aliasing to convert generic types to platform-specific ones, but this can be disabled by setting 'alias_types: false'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/contract.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: my_model\n    config:\n      contract:\n        enforced: true\n        alias_types: false  # true by default\n```\n\n----------------------------------------\n\nTITLE: Creating a Customer Model in SQL\nDESCRIPTION: A SQL query that creates a customer model by joining and transforming data from jaffle_shop_customers and jaffle_shop_orders tables. It includes customer details and order statistics.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/databricks-qs.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nwith customers as (\n\n    select\n        id as customer_id,\n        first_name,\n        last_name\n\n    from jaffle_shop_customers\n\n),\n\norders as (\n\n    select\n        id as order_id,\n        user_id as customer_id,\n        order_date,\n        status\n\n    from jaffle_shop_orders\n\n),\n\ncustomer_orders as (\n\n    select\n        customer_id,\n\n        min(order_date) as first_order_date,\n        max(order_date) as most_recent_order_date,\n        count(order_id) as number_of_orders\n\n    from orders\n\n    group by 1\n\n),\n\nfinal as (\n\n    select\n        customers.customer_id,\n        customers.first_name,\n        customers.last_name,\n        customer_orders.first_order_date,\n        customer_orders.most_recent_order_date,\n        coalesce(customer_orders.number_of_orders, 0) as number_of_orders\n\n    from customers\n\n    left join customer_orders using (customer_id)\n\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Creating a Staging Orders Model in dbt\nDESCRIPTION: Creates a staging model for order data that selects and transforms fields from the raw orders table. This model renames fields and will be referenced by other models in the project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/teradata-qs.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nselect\n   id as order_id,\n   user_id as customer_id,\n   order_date,\n   status\n\nfrom jaffle_shop.orders\n```\n\n----------------------------------------\n\nTITLE: Using project_name to Conditionally Call Root Project Macros in dbt\nDESCRIPTION: This macro demonstrates how to use the project_name context variable to check if a specific macro exists in the root project and call it if available, otherwise falling back to a default implementation. Specifically, it handles vacuuming tables in a Redshift database.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/project_name.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n/*\n  This macro vacuums tables in a Redshift database. If a macro exists in the\n  root-level project called `get_tables_to_vacuum`, this macro will call _that_\n  macro to find the tables to vacuum. If the macro is not defined in the root\n  project, this macro will use a default implementation instead.\n*/\n\n{% macro vacuum_tables() %}\n\n  {% set root_project = context[project_name] %}\n  {% if root_project.get_tables_to_vacuum %}\n    {% set tables = root_project.get_tables_to_vacuum() %}\n  {% else %}\n    {% set tables = redshift.get_tables_to_vacuum() %}\n  {% endif %}\n\n  {% for table in tables %}\n    {% do redshift.vacuum_table(table) %}\n  {% endfor %}\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Snowflake User/Password + DUO MFA Authentication in dbt (v1.8 and earlier)\nDESCRIPTION: This YAML configuration sets up user/password authentication with DUO Mobile app integration for 2-Factor authentication in Snowflake for dbt versions 1.8 and earlier. It includes the 'authenticator' parameter set to 'username_password_mfa'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/snowflake-setup.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmy-snowflake-db:\n  target: dev\n  outputs:\n    dev:\n      type: snowflake\n      account: [account id]\n\n      # User/password auth\n      user: [username]\n      password: [password]\n      authenticator: username_password_mfa\n\n      role: [user role]\n      database: [database name]\n      warehouse: [warehouse name]\n      schema: [dbt schema]\n      threads: [1 or more]\n      client_session_keep_alive: False\n      query_tag: [anything]\n\n      # optional\n      connect_retries: 0 # default 0\n      connect_timeout: 10 # default: 10\n      retry_on_database_errors: False # default: false\n      retry_all: False  # default: false\n      reuse_connections: False\n```\n\n----------------------------------------\n\nTITLE: Using Environment Variables in dbt profiles.yml Configuration (YAML)\nDESCRIPTION: Example of configuring a Redshift connection in profiles.yml using environment variables for sensitive information like host, user, and password. This approach keeps credentials secure by storing them as environment variables rather than hardcoding them in the configuration file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/profiles-yml-context.md#2025-04-09_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\njaffle_shop:\n  target: dev\n  outputs:\n    dev:\n      type: redshift\n      host: \"{{ env_var('DBT_HOST') }}\"\n      user: \"{{ env_var('DBT_USER') }}\"\n      password: \"{{ env_var('DBT_PASS') }}\"\n      port: 5439\n      dbname: analytics\n      schema: dbt_dbanin\n      threads: 4\n```\n\n----------------------------------------\n\nTITLE: Excluding Specific Resources in dbt Test Command\nDESCRIPTION: These examples show how to exclude specific tests or models associated with tests using the dbt test command. It demonstrates excluding a specific test by name and excluding all tests associated with a particular model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/exclude.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndbt test --exclude \"not_null_orders_order_id\"\n```\n\nLANGUAGE: bash\nCODE:\n```\ndbt test --exclude \"orders\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Table Options in SQL Model for Teradata\nDESCRIPTION: Setting multiple table options (NO FALLBACK and NO JOURNAL) for a table in Teradata, separated by commas.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/teradata-configs.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n{{\n  config(\n      materialized=\"table\",\n      table_option=\"NO FALLBACK, NO JOURNAL\"\n  )\n}}\n```\n\n----------------------------------------\n\nTITLE: Adding insert_by_period Materialization Package in dbt\nDESCRIPTION: This YAML snippet shows how to add the insert_by_period materialization, which has been moved to the experimental-features repo, to your packages.yml file. It includes an optional but recommended revision parameter for version control.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/core-upgrade/11-Older versions/upgrading-to-dbt-utils-v1.0.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\npackages:\n  - git: https://github.com/dbt-labs/dbt-labs-experimental-features\n    subdirectory: insert_by_period\n    revision: XXXX #optional but highly recommended. Provide a full git sha hash, e.g. 1c0bfacc49551b2e67d8579cf8ed459d68546e00. If not provided, uses the current HEAD.\n```\n\n----------------------------------------\n\nTITLE: SQL Model with Default Variable Value\nDESCRIPTION: Example showing how to use the var() function with a default value. This demonstrates the optional second argument that provides a fallback value if the variable is not defined.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/var.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n-- Use 'activation' as the event_type if the variable is not defined.\nselect * from events where event_type = '{{ var(\"event_type\", \"activation\") }}'\n```\n\n----------------------------------------\n\nTITLE: generate_schema_name_for_env Macro Implementation in SQL/Jinja\nDESCRIPTION: This macro generates schema names based on the target environment. It uses the custom schema name for production and the default schema for other environments.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/debug-schema-names.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{% macro generate_schema_name_for_env(custom_schema_name, node) -%}\n\n    {%- set default_schema = target.schema -%}\n    {%- if target.name == 'prod' and custom_schema_name is not none -%}\n\n        {{ custom_schema_name | trim }}\n\n    {%- else -%}\n\n        {{ default_schema }}\n\n    {%- endif -%}\n\n{%- endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Configuring a Single Model in properties.yml\nDESCRIPTION: This snippet demonstrates how to configure a specific dbt model using a properties.yml file. It defines the same configuration as the SQL example, setting materialization type as table with sort and distribution keys.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/model-configs.md#2025-04-09_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: base_events\n    config:\n      materialized: table\n      sort: event_time\n      dist: event_id\n```\n\n----------------------------------------\n\nTITLE: dbt Project Configuration Extension\nDESCRIPTION: The file extension used for dbt project configuration files.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/models.md#2025-04-09_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n.yml\n```\n\n----------------------------------------\n\nTITLE: DBT Snapshot with Modern Configuration (1.9+)\nDESCRIPTION: Updated snapshot configuration example using the newer schema-based approach for version 1.9 and later.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/snapshots.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot orders_snapshot %}\n\n{{\n    config(\n      schema='snapshots',\n      unique_key='id',\n      strategy='timestamp',\n      updated_at='updated_at',\n    )\n}}\n\nselect * from {{ source('jaffle_shop', 'orders') }}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Setting Schema for Saved Queries\nDESCRIPTION: Configuration to specify a custom schema for saved queries in dbt_project.yml.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/schema.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nsaved-queries:\n  +schema: metrics\n```\n\n----------------------------------------\n\nTITLE: Configuring Indirect Selection in dbt Project YAML\nDESCRIPTION: Example of setting the indirect_selection flag in the dbt_project.yml file to configure project-level flags.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/indirect-selection.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nflags:\n  indirect_selection: cautious\n```\n\n----------------------------------------\n\nTITLE: Defining a Source in Raw Database for dbt\nDESCRIPTION: This YAML configuration example shows how to define a source that is stored in the 'raw' database. It specifies the source name as 'jaffle_shop' and includes two tables: 'orders' and 'customers'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/database.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: jaffle_shop\n    database: raw\n    tables:\n      - name: orders\n      - name: customers\n```\n\n----------------------------------------\n\nTITLE: Handling Invalid Iterables with the set() Method in Jinja\nDESCRIPTION: Shows how the set() method handles invalid iterables (like integers) by returning None instead of raising an error. This demonstrates the forgiving nature of the standard set() method.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/set.md#2025-04-09_snippet_1\n\nLANGUAGE: jinja\nCODE:\n```\n{% set my_invalid_iterable = 1234 %}\n{% set my_set = set(my_invalid_iterable) %}\n{% do log(my_set) %}  {# None #}\n```\n\n----------------------------------------\n\nTITLE: Testing Webhook Endpoint Authorization Header Support with cURL\nDESCRIPTION: This shell command uses cURL to test if your webhook endpoint supports Authorization headers. It sends a POST request with a sample Authorization header to your webhook endpoint URL.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/webhooks.md#2025-04-09_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncurl -H 'Authorization: 123' -X POST https://<your-webhook-endpoint>\n```\n\n----------------------------------------\n\nTITLE: Limiting Data Scan in Incremental Model SQL\nDESCRIPTION: Example of limiting the data scan of upstream tables within an incremental model SQL. This technique reduces the amount of new data processed in each incremental run.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/incremental-strategy.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nwith large_source_table as (\n\n    select * from {{ ref('large_source_table') }}\n    {% if is_incremental() %}\n        where session_start >= dateadd(day, -3, current_date)\n    {% endif %}\n\n),\n\n...\n```\n\n----------------------------------------\n\nTITLE: Training AutoML Model with Layer in SQL\nDESCRIPTION: Example of training an AutoML model using Layer's integration with dbt, demonstrating regression model training with multiple features.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/layer-setup.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSELECT order_id,\n       layer.automl(\n           -- This is a regression problem\n           'regressor',\n           -- Data (input features) to train our model\n           ARRAY[\n           days_between_purchase_and_delivery, order_approved_late,\n           actual_delivery_vs_expectation_bucket, total_order_price, total_order_freight, is_multiItems_order,seller_shipped_late],\n           -- Target column we want to predict\n           review_score\n       )\nFROM {{ ref('training_data') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 Source in YAML\nDESCRIPTION: This snippet demonstrates the YAML configuration for integrating data from an S3 source. It includes options for file patterns, deduplication, and various job settings.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/upsolver-configs.md#2025-04-09_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nlocation: '<location>'\ndate_pattern: '<date_pattern>'\nfile_pattern: '<file_pattern>'\ninitial_load_pattern: '<initial_load_pattern>'\ninitial_load_prefix: '<initial_load_prefix>'\ndelete_files_after_load: True/False\ndeduplicate_with:\n  COLUMNS: ['col1', 'col2']\n  WINDOW: 'N HOURS'\nend_at: '<timestamp>/NOW'\nstart_from: '<timestamp>/NOW/BEGINNING'\ncompute_cluster: '<compute_cluster>'\nrun_parallelism: <integer>\ncontent_type: 'AUTO/CSV...'\ncompression: 'AUTO/GZIP...'\ncomment: '<comment>'\ncolumn_transformations:\n  '<column>': '<expression>'\ncommit_interval: '<N MINUTE[S]/HOUR[S]/DAY[S]>'\nskip_validations: ('EMPTY_PATH')\nskip_all_validations: True/False\nexclude_columns: ('<exclude_column>', ...)\n```\n\n----------------------------------------\n\nTITLE: Setting Log Format in dbt CLI\nDESCRIPTION: Example of using the log-format flag to output JSON formatted logs in dbt\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/logs.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ndbt --log-format json run\n```\n\n----------------------------------------\n\nTITLE: Formatting Proper Hyperlinks in Documentation\nDESCRIPTION: Examples showing incorrect and correct ways to format hyperlinks in documentation. This demonstrates best practices for creating descriptive, text-based links rather than using raw URLs or generic text.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/contributing/content-style-guide.md#2025-04-09_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n  :x: For more information, visit https://docs.getdbt.com\n\n  :x: For more information, [_Click Here_](https://docs.getdbt.com/)\n\n   For more information, visit the [_dbt Labs doc site_](https://docs.getdbt.com/).\n  \n   For more information, read the [_dbt Labs doc site_](https://docs.getdbt.com/).\n  \n   For more information, refer to the [_dbt Labs doc site_](https://docs.getdbt.com/).\n```\n\n----------------------------------------\n\nTITLE: Joining Multiple Tables with Different Distribution Keys in Redshift\nDESCRIPTION: Example query showing a common scenario where distribution key conflicts can occur when joining multiple tables with different join conditions (person_id and mask_id).\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-05-19-redshift-configurations-dbt-model-optimizations.md#2025-04-09_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nselect <your_list_of_columns>\nfrom visitors\nleft join known_visitor_profiles\n\ton visitors.person_id = known_visitor_profiles.person_id\nleft join unknown_visitor_profiles\n\ton visitors.mask_id = anonymous_visitor_profiles.mask_id\n```\n\n----------------------------------------\n\nTITLE: Creating Network Rules for dbt Cloud Access in Snowflake\nDESCRIPTION: SQL command to create a network rule in Snowflake that allows access from dbt Cloud through PrivateLink. This rule specifies the AWS VPC endpoint ID that should be permitted to connect to Snowflake.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/secure/snowflake-privatelink.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nCREATE NETWORK RULE allow_dbt_cloud_access\n  MODE = INGRESS\n  TYPE = AWSVPCEID\n  VALUE_LIST = ('<VPCE_ID>'); -- Replace '<VPCE_ID>' with the actual ID provided\n```\n\n----------------------------------------\n\nTITLE: Configuring Full Refresh for Models in dbt_project.yml\nDESCRIPTION: Sets the full_refresh config for models in the dbt_project.yml file. This determines whether models will always, never, or conditionally perform a full refresh based on the --full-refresh flag.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/full_refresh.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  [<resource-path>]:\n    +full_refresh: false | true \n```\n\n----------------------------------------\n\nTITLE: Configuring Simple and Derived Metrics with Null Filling in YAML\nDESCRIPTION: Example configuration showing how to set up website visits and leads metrics with null value handling. Demonstrates the use of fill_nulls_with parameter to replace null values with zero.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/fill-nulls-advanced.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  - name: website_visits\n    type: simple\n    type_params:\n      measure:\n        name: bookings\n  - name: leads\n    type: simple\n    type_params:\n      measure:\n        name: bookings\n        fill_nulls_with: 0 # This fills null values with zero\n  - name: leads_to_website_visit\n    type: derived\n    type_params:\n      expr: leads/website_visits\n      metrics:\n        - name: leads\n        - name: website_visits\n```\n\n----------------------------------------\n\nTITLE: Configuring SSO Authentication with External Browser in Snowflake for dbt\nDESCRIPTION: YAML configuration for Snowflake with external browser SSO authentication. This configuration omits password and instead uses the 'externalbrowser' authenticator to trigger browser-based authentication flow.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/snowflake-setup.md#2025-04-09_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nmy-snowflake-db:\n  target: dev\n  outputs:\n    dev:\n      type: snowflake\n      account: [account id] # Snowflake <account_name>\n      user: [username] # Snowflake username\n      role: [user role] # Snowflake user role\n\n      # SSO config\n      authenticator: externalbrowser\n\n      database: [database name] # Snowflake database name\n      warehouse: [warehouse name] # Snowflake warehouse name\n      schema: [dbt schema]\n      threads: [between 1 and 8]\n      client_session_keep_alive: False\n      query_tag: [anything]\n\n      # optional\n      connect_retries: 0 # default 0\n      connect_timeout: 10 # default: 10\n      retry_on_database_errors: False # default: false\n      retry_all: False  # default: false\n      reuse_connections: True # default: True if client_session_keep_alive is False, otherwise None\n```\n\n----------------------------------------\n\nTITLE: Sample output of dbt invocation list command in bash\nDESCRIPTION: Shows an example output of the dbt invocation list command, displaying details of an active session including its ID, status, type, arguments, and start time.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/invocation.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndbt invocation list\n\nActive Invocations:\n  ID                             6dcf4723-e057-48b5-946f-a4d87e1d117a\n  Status                         running\n  Type                           cli\n  Args                           [run --select test.sql]\n  Started At                     2025-01-24 11:03:19\n\n  jaffle-shop git:(test-cli)  \n```\n\n----------------------------------------\n\nTITLE: SQL Condition Example - Poor Documentation\nDESCRIPTION: Example of insufficiently documented SQL logic that doesn't provide context about the business purpose of the transformation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-09-28-analyst-to-ae.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nCase when Zone = 1 and Level like 'A%' then 'True' else 'False' end as GroupB\n```\n\n----------------------------------------\n\nTITLE: Configuring Replicate Distribution with Sort Column in Yellowbrick dbt Model\nDESCRIPTION: This example demonstrates how to configure a dbt model with REPLICATE distribution and a SORT column for the stadium_capacity field. The model transforms team data and applies Yellowbrick-specific optimizations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/yellowbrick-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{{{\n  config(\n    materialized = \"table\",\n    dist = \"replicate\",\n    sort_col = \"stadium_capacity\"\n  )\n}}}\n\nselect\n    hash(stg.name) as team_key\n    , stg.name as team_name\n    , stg.nickname as team_nickname\n    , stg.city as home_city\n    , stg.stadium as stadium_name\n    , stg.capacity as stadium_capacity\n    , stg.avg_att as average_game_attendance\n    , current_timestamp as md_create_timestamp\nfrom\n    {{ source('premdb_public','team') }} stg\nwhere\n    stg.name is not null\n```\n\n----------------------------------------\n\nTITLE: Database Schema Grant Usage\nDESCRIPTION: Configuration showing how to use the database schema grant macro in dbt_project.yml.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/on-run-end-context.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\non-run-end:\n - \"{{ grant_usage_to_schemas(database_schemas, user) }}\"\n```\n\n----------------------------------------\n\nTITLE: Defining Fixture Data for dbt Tests in Python\nDESCRIPTION: Python code defining fixture data for dbt tests, including a CSV seed file, SQL model, and YAML model configuration. These fixtures can be reused across different test cases.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# seeds/my_seed.csv\nmy_seed_csv = \"\"\"\nid,name,some_date\n1,Easton,1981-05-20T06:46:51\n2,Lillian,1978-09-03T18:10:33\n3,Jeremiah,1982-03-11T03:59:51\n4,Nolan,1976-05-06T20:21:35\n\"\"\".lstrip()\n\n# models/my_model.sql\nmy_model_sql = \"\"\"\nselect * from {{ ref('my_seed') }}\nunion all\nselect null as id, null as name, null as some_date\n\"\"\"\n\n# models/my_model.yml\nmy_model_yml = \"\"\"\nversion: 2\nmodels:\n  - name: my_model\n    columns:\n      - name: id\n        tests:\n          - unique\n          - not_null  # this test will fail\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining Metrics Configuration in YAML\nDESCRIPTION: This snippet shows the complete structure for defining metrics in a YAML file, including all possible parameters and their descriptions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metrics-overview.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  - name: metric name                     ## Required\n    description: description               ## Optional\n    type: the type of the metric          ## Required\n    type_params:                          ## Required\n      - specific properties for the metric type\n    config:                               ## Optional\n      meta:\n        my_meta_config:  'config'         ## Optional\n    label: The display name for your metric. This value will be shown in downstream tools. ## Required\n    filter: |\n      {{  Dimension('entity__name') }} > 0 and {{ Dimension(' entity__another_name') }} is not\n      null and {{ Metric('metric_name', group_by=['entity_name']) }} > 5\n```\n\n----------------------------------------\n\nTITLE: Defining Metrics Configuration in YAML\nDESCRIPTION: This snippet shows the complete structure for defining metrics in a YAML file, including all possible parameters and their descriptions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metrics-overview.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  - name: metric name                     ## Required\n    description: description               ## Optional\n    type: the type of the metric          ## Required\n    type_params:                          ## Required\n      - specific properties for the metric type\n    config:                               ## Optional\n      meta:\n        my_meta_config:  'config'         ## Optional\n    label: The display name for your metric. This value will be shown in downstream tools. ## Required\n    filter: |\n      {{  Dimension('entity__name') }} > 0 and {{ Dimension(' entity__another_name') }} is not\n      null and {{ Metric('metric_name', group_by=['entity_name']) }} > 5\n```\n\n----------------------------------------\n\nTITLE: Setting Project-Level Test Failure Storage\nDESCRIPTION: Shows how to configure store_failures defaults for all tests in a dbt project or specific package using dbt_project.yml.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/store_failures.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ntests:\n  +store_failures: true  # all tests\n  \n  <package_name>:\n    +store_failures: false # tests in <package_name>\n```\n\n----------------------------------------\n\nTITLE: Seeding dbt Data\nDESCRIPTION: Runs the dbt seed command to load CSV files from the seeds directory into the data warehouse.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_onrunstart-onrunend-commands.md#2025-04-09_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ndbt seed\n```\n\n----------------------------------------\n\nTITLE: Testing Generated Documentation with dbt Docs\nDESCRIPTION: Commands to verify that the documentation is properly generated and displayed. This helps catch any syntax errors or other issues before deploying.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-04-generating-dynamic-docs.md#2025-04-09_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ndbt docs generate\n```\n\nLANGUAGE: bash\nCODE:\n```\ndbt docs serve\n```\n\n----------------------------------------\n\nTITLE: Running Initial dbt Models Command\nDESCRIPTION: Command to execute the example dbt models that come with a new project initialization. This creates a table MY_FIRST_DBT_MODEL and a view MY_SECOND_DBT_MODEL in your development schema.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndbt run\n```\n\n----------------------------------------\n\nTITLE: Aliasing a Data Test in SQL File\nDESCRIPTION: Assigns an alias directly in the tests/unique_order_id_test.sql file using a config block.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/alias.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    alias=\"unique_order_id_test\",\n    severity=\"error\"\n) }}\n```\n\n----------------------------------------\n\nTITLE: Saving Custom Conda Environment in Oracle ML\nDESCRIPTION: Bash command to save the custom Conda environment for use with Oracle ML Python (OML4PY).\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/oracle-setup.md#2025-04-09_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\nconda upload --overwrite dbt_py_env -t application OML4PY\n```\n\n----------------------------------------\n\nTITLE: Configuring Directory-Specific Quote Columns in DBT Project\nDESCRIPTION: Applies quote_columns configuration specifically to seeds in the 'mappings' directory within the jaffle_shop project. This allows for targeted column quoting behavior.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/quote_columns.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  jaffle_shop:\n    mappings:\n      +quote_columns: true\n```\n\n----------------------------------------\n\nTITLE: Executing Incremental Model with Append Strategy in Firebolt\nDESCRIPTION: This SQL snippet shows how Firebolt executes the incremental model with the 'append' strategy. It creates a temporary dimension table and then inserts new records into the main table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/firebolt-configs.md#2025-04-09_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\nCREATE DIMENSION TABLE IF NOT EXISTS orders__dbt_tmp AS\nSELECT * FROM raw_orders\nWHERE order_date > (SELECT MAX(order_date) FROM orders);\n\nINSERT INTO orders VALUES ([columns])\nSELECT ([columns])\nFROM orders__dbt_tmp;\n```\n\n----------------------------------------\n\nTITLE: Installing dbt-core in Editable Mode\nDESCRIPTION: Command to install dbt-core in editable mode, which allows local changes to be reflected in the installation without reinstalling. This is particularly useful for contributors.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/source-install.md#2025-04-09_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython -m pip install -e editable-requirements.txt` \n```\n\n----------------------------------------\n\nTITLE: Environment Query Before Deprecation - GraphQL\nDESCRIPTION: Environment query using Int data type for IDs before deprecation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/2023-release-notes.md#2025-04-09_snippet_5\n\nLANGUAGE: graphql\nCODE:\n```\nquery ($environmentId: Int!, $first: Int!) {\n    environment(id: $environmentId) {\n        applied {\n        models(first: $first) {\n            edges {\n            node {\n                uniqueId\n                executionInfo {\n                lastRunId\n                }\n            }\n            }\n        }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Standard Run Compiled SQL\nDESCRIPTION: Compiled SQL for model_b without using defer, referencing development schema.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/defer.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\ncreate or replace view dev_me.model_b as (\n\n    select\n\n        id,\n        count(*)\n\n    from dev_alice.model_a\n    group by 1\n\n)\n```\n\n----------------------------------------\n\nTITLE: Customizing Schema Names in dbt with generate_schema_name() Macro\nDESCRIPTION: A Jinja macro that overrides dbt's default schema naming behavior. It uses custom schemas directly in production without concatenating them with the target schema, while maintaining the default concatenation behavior in development environments. This helps maintain unique model locations across different environments.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/customize-schema-alias.md#2025-04-09_snippet_0\n\nLANGUAGE: jinja\nCODE:\n```\n{% macro generate_schema_name(custom_schema_name, node) -%}\n\n    {%- set default_schema = target.schema -%}\n    {%- if custom_schema_name is none -%}\n\n        {{ default_schema }}\n\n    {%- elif  env_var('DBT_ENV_TYPE','DEV') == 'PROD' -%}\n        \n        {{ custom_schema_name | trim }}\n\n    {%- else -%}\n\n        {{ default_schema }}_{{ custom_schema_name | trim }}\n\n    {%- endif -%}\n\n{%- endmacro %}\n\n\n```\n\n----------------------------------------\n\nTITLE: Running dbt with empty flag for all models\nDESCRIPTION: This command runs all models in a project while building only the schemas in the development environment. It uses the --empty flag to limit refs and sources to zero rows.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/empty-flag.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --empty\n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Header for dbt Models\nDESCRIPTION: This snippet shows how to set the sql_header configuration for a dbt model using the config block. It allows injecting SQL statements above the create table/view statements.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/sql_header.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n  sql_header=\"<sql-statement>\"\n) }}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Serving dbt Documentation on Custom Port\nDESCRIPTION: This command serves the dbt documentation on a specified port. It uses the '--port' flag to set a custom port number for the webserver.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/cmd-docs.md#2025-04-09_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ndbt docs serve --port 8001\n```\n\n----------------------------------------\n\nTITLE: Setup Python Environment in Windows cmd.exe (Shell)\nDESCRIPTION: Shell commands for creating and activating a Python virtual environment, updating pip, and installing dependencies in Windows command prompt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-10-19-polyglot-dbt-python-dataframes-and-sql.md#2025-04-09_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\npython -m venv env\nenv\\Scripts\\activate.bat\npython -m pip install --upgrade pip\npython -m pip install -r requirements.txt\nenv\\Scripts\\activate.bat\n```\n\n----------------------------------------\n\nTITLE: Defining Constraints in BigQuery dbt Model\nDESCRIPTION: This snippet demonstrates how to define constraints in a BigQuery dbt model. It includes not_null and primary_key constraints, noting that only not_null is enforced in BigQuery.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/constraints.md#2025-04-09_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: dim_customers\n    config:\n      contract:\n        enforced: true\n    columns:\n      - name: id\n        data_type: int\n        constraints:\n          - type: not_null\n          - type: primary_key # not enforced  -- will warn & include\n          - type: check       # not supported -- will warn & skip\n            expression: \"id > 0\"\n        tests:\n          - unique            # primary_key constraint is not enforced\n      - name: customer_name\n        data_type: string\n      - name: first_transaction_date\n        data_type: date\n```\n\n----------------------------------------\n\nTITLE: Defining Primary Entity for Data Source without Primary Entity Column\nDESCRIPTION: This snippet shows how to define a primary entity for a data source that doesn't have a primary entity column, which is required by MetricFlow.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/dimensions.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_model:\n  name: bookings_monthly_source\n  description: bookings_monthly_source\n  defaults:\n    agg_time_dimension: ds\n  model: ref('bookings_monthly_source')\n  measures:\n    - name: bookings_monthly\n      agg: sum\n      create_metric: true\n  primary_entity: booking_id\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into Temporary Table in SQL Server\nDESCRIPTION: This SQL snippet inserts additional data into the temporary table from another raw table. It shows the second step in the stored procedure approach.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-07-19-migrating-from-stored-procs.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n   INSERT INTO #temp_orders(messageid,orderid,sk_id, client)\n   SELECT   messageid\n           ,orderid\n   FROM    another_raw_table\n   WHERE   . . .\n   INTO    #temp_orders\n```\n\n----------------------------------------\n\nTITLE: Running dbt with empty flag for a specific model\nDESCRIPTION: This command runs a specific model using the --select option along with the --empty flag. It builds and executes the SQL, resulting in an empty schema in the data warehouse for the selected model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/empty-flag.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --select path/to/your_model --empty\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Model Submission in dbt-databricks\nDESCRIPTION: This YAML configuration demonstrates how to set up a Python model submission using the 'workflow_job' method. It includes settings for job cluster configuration, Python job configuration, and additional workflow settings such as email notifications, retries, and post-hook tasks.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/databricks-configs.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: my_model\n    config:\n      submission_method: workflow_job\n\n      # Define a job cluster to create for running this workflow\n      # Alternately, could specify cluster_id to use an existing cluster, or provide neither to use a serverless cluster\n      job_cluster_config:\n        spark_version: \"15.3.x-scala2.12\"\n        node_type_id: \"rd-fleet.2xlarge\"\n        runtime_engine: \"{{ var('job_cluster_defaults.runtime_engine') }}\"\n        data_security_mode: \"{{ var('job_cluster_defaults.data_security_mode') }}\"\n        autoscale: { \"min_workers\": 1, \"max_workers\": 4 }\n\n      python_job_config:\n        # These settings are passed in, as is, to the request\n        email_notifications: { on_failure: [\"me@example.com\"] }\n        max_retries: 2\n\n        name: my_workflow_name\n\n        # Override settings for your model's dbt task. For instance, you can\n        # change the task key\n        additional_task_settings: { \"task_key\": \"my_dbt_task\" }\n\n        # Define tasks to run before/after the model\n        # This example assumes you have already uploaded a notebook to /my_notebook_path to perform optimize and vacuum\n        post_hook_tasks:\n          [\n            {\n              \"depends_on\": [{ \"task_key\": \"my_dbt_task\" }],\n              \"task_key\": \"OPTIMIZE_AND_VACUUM\",\n              \"notebook_task\":\n                { \"notebook_path\": \"/my_notebook_path\", \"source\": \"WORKSPACE\" },\n            },\n          ]\n\n        # Simplified structure, rather than having to specify permission separately for each user\n        grants:\n          view: [{ \"group_name\": \"marketing-team\" }]\n          run: [{ \"user_name\": \"other_user@example.com\" }]\n          manage: []\n```\n\n----------------------------------------\n\nTITLE: Configuring Dremio Software Profile with Username/Password in YAML\nDESCRIPTION: YAML configuration for connecting to Dremio Software in dbt using username and password authentication. Includes settings for host, port, object storage, and SSL usage.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/dremio-setup.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n[project name]:\n  outputs:\n    dev:\n      password: [password]\n      port: [port]\n      software_host: [hostname or IP address]\n      object_storage_source: [name\n      object_storage_path: [path]\n      dremio_space: [name]\n      dremio_space_folder: [path]\n      threads: [integer >= 1]\n      type: dremio\n      use_ssl: [true|false]\n      user: [username]\n  target: dev\n```\n\n----------------------------------------\n\nTITLE: CSV File Reference Unit Test Configuration in YAML\nDESCRIPTION: Shows how to reference an external CSV file for mock data in dbt unit tests. The fixture file should be located in the tests/fixtures directory.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/data-formats.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nunit_tests:\n  - name: test_my_model\n    model: my_model\n    given:\n      - input: ref('my_model_a')\n        format: csv\n        fixture: my_model_a_fixture\n```\n\n----------------------------------------\n\nTITLE: Calculating Cumulative Metrics with Time Range Joins in SQL\nDESCRIPTION: SQL implementation for calculating weekly active users as a cumulative metric. The query performs a time range join to a timespine table using the primary time dimension as the join key, with an accumulation window of 7 days.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/cumulative-metrics.md#2025-04-09_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\nselect\n  count(distinct distinct_users) as weekly_active_users,\n  metric_time\nfrom (\n  select\n    subq_3.distinct_users as distinct_users,\n    subq_3.metric_time as metric_time\n  from (\n    select\n      subq_2.distinct_users as distinct_users,\n      subq_1.metric_time as metric_time\n    from (\n      select\n        metric_time\n      from transform_prod_schema.mf_time_spine subq_1356\n      where (\n        metric_time >= cast('2000-01-01' as timestamp)\n      ) and (\n        metric_time <= cast('2040-12-31' as timestamp)\n      )\n    ) subq_1\n    inner join (\n      select\n        distinct_users as distinct_users,\n        date_trunc('day', ds) as metric_time\n      from demo_schema.transactions transactions_src_426\n      where (\n        (date_trunc('day', ds)) >= cast('1999-12-26' as timestamp)\n      ) AND (\n        (date_trunc('day', ds)) <= cast('2040-12-31' as timestamp)\n      )\n    ) subq_2\n    on\n      (\n        subq_2.metric_time <= subq_1.metric_time\n      ) and (\n        subq_2.metric_time > dateadd(day, -7, subq_1.metric_time)\n      )\n  ) subq_3\n)\ngroup by\n  metric_time,\nlimit 100;\n```\n\n----------------------------------------\n\nTITLE: Environment Type Variable with Default\nDESCRIPTION: Usage of DBT_CLOUD_ENVIRONMENT_TYPE variable with a default empty value for General deployment environments.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/environment-variables.md#2025-04-09_snippet_2\n\nLANGUAGE: jinja\nCODE:\n```\n{{ env_var('DBT_CLOUD_ENVIRONMENT_TYPE', '') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Warning Options in profiles.yml (dbt 1.7 and Earlier)\nDESCRIPTION: This YAML configuration shows how to exclude specific warnings using the profiles.yml file in dbt versions 1.7 and earlier.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/warnings.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nconfig:\n  warn_error_options:\n    include: all\n    exclude: \n      - NoNodesForSelectionCriteria\n```\n\n----------------------------------------\n\nTITLE: Filtering Real Differences in SQL\nDESCRIPTION: SQL code to filter out rows that are not marked as real differences, effectively removing duplicates from the dataset.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-04-19-complex-deduplication.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nfilter_real_diffs as (\n\n    select *\n  \n    from mark_real_diffs\n  \n    where is_real_diff = true\n\n)\n\nselect * from filter_real_diffs\n```\n\n----------------------------------------\n\nTITLE: Defining YAML Frontmatter for dbt Cloud Account Integrations Documentation\nDESCRIPTION: This YAML frontmatter defines metadata for the documentation page, including the title, sidebar label, and description of the content.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/account-integrations.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: \"Account integrations in dbt Cloud\"\nsidebar_label: \"Account integrations\" \ndescription: \"Learn how to configure account integrations for your dbt Cloud account.\"\n---\n```\n\n----------------------------------------\n\nTITLE: Compiled SQL Output for stg_orders\nDESCRIPTION: This is the compiled SQL output for the 'stg_orders' node, showing the transformation from raw_orders to a staged orders table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/compile.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nwith source as (\n    select * from \"jaffle_shop\".\"main\".\"raw_orders\"\n\n),\n\nrenamed as (\n\n    select\n        id as order_id,\n        user_id as customer_id,\n        order_date,\n        status\n\n    from source\n\n)\n\nselect * from renamed\n```\n\n----------------------------------------\n\nTITLE: Unit Test Example with Dictionary Input and Output\nDESCRIPTION: This example demonstrates a unit test for validating email addresses. It uses dictionary format for both input and expected output data, testing the 'dim_customers' model with mock data for 'stg_customers' and 'top_level_email_domains'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/unit-tests.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nunit_tests:\n  - name: test_is_valid_email_address # this is the unique name of the test\n    model: dim_customers # name of the model I'm unit testing\n    given: # the mock data for your inputs\n      - input: ref('stg_customers')\n        rows:\n         - {email: cool@example.com,     email_top_level_domain: example.com}\n         - {email: cool@unknown.com,     email_top_level_domain: unknown.com}\n         - {email: badgmail.com,         email_top_level_domain: gmail.com}\n         - {email: missingdot@gmailcom,  email_top_level_domain: gmail.com}\n      - input: ref('top_level_email_domains')\n        rows:\n         - {tld: example.com}\n         - {tld: gmail.com}\n    expect: # the expected output given the inputs above\n      rows:\n        - {email: cool@example.com,    is_valid_email_address: true}\n        - {email: cool@unknown.com,    is_valid_email_address: false}\n        - {email: badgmail.com,        is_valid_email_address: false}\n        - {email: missingdot@gmailcom, is_valid_email_address: false}\n```\n\n----------------------------------------\n\nTITLE: Creating Customers Table in Snowflake\nDESCRIPTION: SQL command to create the customers table in the jaffle_shop schema with columns for id, first_name, and last_name. This prepares the table structure for loading sample customer data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/snowflake-qs.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\ncreate table raw.jaffle_shop.customers \n( id integer,\n  first_name varchar,\n  last_name varchar\n);\n```\n\n----------------------------------------\n\nTITLE: Running Adapter-Specific Tests\nDESCRIPTION: Shell commands to run unit tests for specific database adapters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-08-22-unit-testing-dbt-package.md#2025-04-09_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n# Run unit tests on BigQuery\ndbt run-operation run_unit_tests --profile bigquery\n# `default__test_to_literal` is internally called.\n\n# Run unit tests on postgres\ndbt run-operation run_unit_tests --profile postgres\n# `postgres__test_to_literal` is internally called.\n```\n\n----------------------------------------\n\nTITLE: Setting Project-Wide Pre-Hook in dbt_project.yml\nDESCRIPTION: Configures a project-wide pre-hook in the dbt_project.yml file, which will apply to all models in the specified project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/hooks-operations.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  <project_name>:\n    +pre-hook:\n      - \"{{ some_macro() }}\"\n```\n\n----------------------------------------\n\nTITLE: DBT Project Configuration for Check Columns\nDESCRIPTION: Project-level configuration for check columns in DBT snapshots, showing how to set default strategy and columns to check.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/check_cols.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  [<resource-path>]:\n    +strategy: check\n    +check_cols: [column_name] | all\n```\n\n----------------------------------------\n\nTITLE: Using No-Warning Behavior Flags in Python\nDESCRIPTION: Shows how to access behavior flags without triggering warning messages by using the no_warn property, which is useful for checking flags in frequently executed code.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nclass ABCAdapter(BaseAdapter):\n    ...\n    def some_method(self, *args, **kwargs):\n        if self.behavior.enable_new_functionality_requiring_higher_permissions.no_warn:\n            # do the new thing\n        else:\n            # do the old thing\n```\n\n----------------------------------------\n\nTITLE: Example Output of Successful dbt Build Command\nDESCRIPTION: Shows the expected terminal output when running the 'dbt build' command successfully on the Jaffle Shop project, demonstrating seeds loading, models building, and tests passing.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/duckdb-qs.md#2025-04-09_snippet_6\n\nLANGUAGE: jinja\nCODE:\n```\n(venv)   jaffle_shop_duckdb git:(duckdb) dbt build\n15:10:12  Running with dbt=1.8.1\n15:10:13  Registered adapter: duckdb=1.8.1\n15:10:13  Found 5 models, 3 seeds, 20 data tests, 416 macros\n15:10:13  \n15:10:14  Concurrency: 24 threads (target='dev')\n15:10:14  \n15:10:14  1 of 28 START seed file main.raw_customers ..................................... [RUN]\n15:10:14  2 of 28 START seed file main.raw_orders ........................................ [RUN]\n15:10:14  3 of 28 START seed file main.raw_payments ...................................... [RUN]\n....\n\n15:10:15  27 of 28 PASS relationships_orders_customer_id__customer_id__ref_customers_ .... [PASS in 0.32s]\n15:10:15  \n15:10:15  Finished running 3 seeds, 3 view models, 20 data tests, 2 table models in 0 hours 0 minutes and 1.52 seconds (1.52s).\n15:10:15  \n15:10:15  Completed successfully\n15:10:15  \n15:10:15  Done. PASS=28 WARN=0 ERROR=0 SKIP=0 TOTAL=28\n```\n\n----------------------------------------\n\nTITLE: Configuring Project-wide Test Failure Limits in YAML\nDESCRIPTION: This example illustrates how to set default failure limits for all tests in a package or project using the dbt_project.yml file. It sets a global limit of 1000 and a specific limit of 50 for tests in a named package.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/limit.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ntests:\n  +limit: 1000  # all tests\n  \n  <package_name>:\n    +limit: 50 # tests in <package_name>\n```\n\n----------------------------------------\n\nTITLE: Processing Arrow Results with Python\nDESCRIPTION: Python script demonstrating how to query the GraphQL API and process Arrow format results, including authentication, polling for results, and conversion to pandas DataFrame.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-graphql.md#2025-04-09_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport base64\nimport pyarrow as pa\nimport time\n\nheaders = {\"Authorization\":\"Bearer <token>\"}\nquery_result_request = \"\"\"\n{\n  query(environmentId: 70, queryId: \"12345678\") {\n    sql\n    status\n    error\n    arrowResult\n  }\n}\n\"\"\"\n\nwhile True:\n  gql_response = requests.post(\n    \"https://semantic-layer.cloud.getdbt.com/api/graphql\",\n    json={\"query\": query_result_request},\n    headers=headers,\n  )\n  if gql_response.json()[\"data\"][\"status\"] in [\"FAILED\", \"SUCCESSFUL\"]:\n    break\n  # Set an appropriate interval between polling requests\n  time.sleep(1)\n\ndef to_arrow_table(byte_string: str) -> pa.Table:\n  \"\"\"Get a raw base64 string and convert to an Arrow Table.\"\"\"\n  with pa.ipc.open_stream(base64.b64decode(byte_string)) as reader:\n    return pa.Table.from_batches(reader, reader.schema)\n\narrow_table = to_arrow_table(gql_response.json()[\"data\"][\"query\"][\"arrowResult\"])\n\n# Perform whatever functionality is available, like convert to a pandas table.\nprint(arrow_table.to_pandas())\n```\n\n----------------------------------------\n\nTITLE: Uninstalling dbt Core using pip\nDESCRIPTION: Command to uninstall dbt Core before installing the dbt Cloud CLI to prevent conflicts.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/cloud-cli-installation.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip uninstall dbt\n```\n\n----------------------------------------\n\nTITLE: Databricks Model Definition with Constraints\nDESCRIPTION: This SQL model defines a simple table with three columns in Databricks. The model is configured to be materialized as a table and includes sample data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/constraints.md#2025-04-09_snippet_16\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(\n    materialized = \"table\"\n  )\n}}\n\nselect \n  1 as id, \n  'My Favorite Customer' as customer_name, \n  cast('2019-01-01' as date) as first_transaction_date\n```\n\n----------------------------------------\n\nTITLE: Adding Description to Singular Test in YAML\nDESCRIPTION: This YAML snippet shows how to add a description to a singular test in the project's schema file. It provides context for the 'assert_total_payment_amount_is_positive' test.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/data-tests.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\ndata_tests:\n  - name: assert_total_payment_amount_is_positive\n    description: >\n      Refunds have a negative amount, so the total amount should always be >= 0.\n      Therefore return records where total amount < 0 to make the test fail.\n```\n\n----------------------------------------\n\nTITLE: Setting Oracle TNS Name Environment Variable\nDESCRIPTION: Sets the environment variable for the Oracle TNS alias.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/oracle-setup.md#2025-04-09_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nexport DBT_ORACLE_TNS_NAME=db2022adb_high\n```\n\n----------------------------------------\n\nTITLE: Configuring Quote Columns in Seeds Properties\nDESCRIPTION: Alternative approach to configure quote_columns using a seeds/properties.yml file. This method uses version 2 of the configuration schema to set quoting behavior for specific seeds.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/quote_columns.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nseeds:\n  - name: mappings\n    config:\n      quote_columns: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Cumulative Metrics in dbt Semantic Layer (YAML)\nDESCRIPTION: This YAML snippet demonstrates the complete specification for defining cumulative metrics in dbt's Semantic Layer. It includes all possible parameters and their descriptions, showing how to configure the metric name, type, label, and various type-specific parameters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/cumulative-metrics.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  - name: The metric name # Required\n    description: The metric description # Optional\n    type: cumulative # Required\n    label: The value that will be displayed in downstream tools # Required\n    type_params: # Required\n      cumulative_type_params:\n        period_agg: first # Optional. Defaults to first. Accepted values: first|last|average\n        window: The accumulation window, such as 1 month, 7 days, 1 year. # Optional. It cannot be used with grain_to_date.\n        grain_to_date: Sets the accumulation grain, such as month will accumulate data for one month, then restart at the beginning of the next.  # Optional. It cannot be used with window.\n      measure: \n        name: The measure you are referencing. # Required\n        fill_nulls_with: Set the value in your metric definition instead of null (such as zero). # Optional\n        join_to_timespine: true/false # Boolean that indicates if the aggregated measure should be joined to the time spine table to fill in missing dates. Default `false`. # Optional\n```\n\n----------------------------------------\n\nTITLE: Example of Unique Test with Configuration\nDESCRIPTION: YAML configuration demonstrating a unique test implementation with additional where clause configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/data-tests.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: orders\n    columns:\n      - name: order_id\n        tests:\n          - unique:\n              config:\n                where: \"order_id > 21\"\n```\n\n----------------------------------------\n\nTITLE: IDE Keyboard Shortcuts in Markdown\nDESCRIPTION: Keyboard shortcuts for the dbt Cloud IDE including toggle history drawer, block comments, command palette, and tab closing commands.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/2023-release-notes.md#2025-04-09_snippet_7\n\nLANGUAGE: markdown\nCODE:\n```\nCtrl + ` (toggle history drawer)\nCMD + Option + / (toggle block comment)\nCMD + Shift + P (open command palette)\nOption + W (close editor tab)\n```\n\n----------------------------------------\n\nTITLE: Raising Warnings in dbt with exceptions.warn\nDESCRIPTION: This snippet shows how to use exceptions.warn to generate warnings without causing model failures. It validates if a number is within range (0-100) and issues a warning if not. Warnings can be promoted to errors using the --warn-error flag or with specific configuration options.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/exceptions.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{% if number < 0 or number > 100 %}\n  {% do exceptions.warn(\"Invalid `number`. Got: \" ~ number) %}\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Custom Alias Generation Macro\nDESCRIPTION: Implementation of the generate_alias_name macro that controls how model aliases are generated, with support for versioned models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/custom-aliases.md#2025-04-09_snippet_3\n\nLANGUAGE: jinja2\nCODE:\n```\n{% macro generate_alias_name(custom_alias_name=none, node=none) -%}\n\n    {%- if custom_alias_name -%}\n\n        {{ custom_alias_name | trim }}\n\n    {%- elif node.version -%}\n\n        {{ return(node.name ~ \"_v\" ~ (node.version | replace(\".\", \"_\"))) }}\n\n    {%- else -%}\n\n        {{ node.name }}\n\n    {%- endif -%}\n\n{%- endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Configuring GitLab PR Template URL in dbt Cloud\nDESCRIPTION: This snippet provides the PR template URL format for GitLab repositories in dbt Cloud. It includes parameters for source and target branches.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/git/pr-template.md#2025-04-09_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://gitlab.com/<org>/<repo>/-/merge_requests/new?merge_request[source_branch]={{source}}&merge_request[target_branch]={{destination}}\n```\n\n----------------------------------------\n\nTITLE: Unique Test SQL Implementation\nDESCRIPTION: SQL implementations for the unique test, showing both compiled and templated versions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/data-tests.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nselect *\nfrom (\n\n    select\n        order_id\n\n    from analytics.orders\n    where order_id is not null\n    group by order_id\n    having count(*) > 1\n\n) validation_errors\n```\n\nLANGUAGE: sql\nCODE:\n```\nselect *\nfrom (\n\n    select\n        {{ column_name }}\n\n    from {{ model }}\n    where {{ column_name }} is not null\n    group by {{ column_name }}\n    having count(*) > 1\n\n) validation_errors\n```\n\n----------------------------------------\n\nTITLE: Processing dbt Cloud Webhook in Python\nDESCRIPTION: This Python script validates the webhook authenticity, extracts run logs from the Admin API, and builds a summary message for Microsoft Teams. It handles webhook validation, API requests, and log parsing to create a formatted message.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/zapier-ms-teams.md#2025-04-09_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport hashlib\nimport hmac\nimport json\nimport re\n\n\nauth_header = input_data['auth_header']\nraw_body = input_data['raw_body']\n\n# Access secret credentials\nsecret_store = StoreClient('YOUR_SECRET_HERE')\nhook_secret = secret_store.get('DBT_WEBHOOK_KEY')\napi_token = secret_store.get('DBT_CLOUD_SERVICE_TOKEN')\n\n# Validate the webhook came from dbt Cloud\nsignature = hmac.new(hook_secret.encode('utf-8'), raw_body.encode('utf-8'), hashlib.sha256).hexdigest()\n\nif signature != auth_header:\n  raise Exception(\"Calculated signature doesn't match contents of the Authorization header. This webhook may not have been sent from dbt Cloud.\")\n\nfull_body = json.loads(raw_body)\nhook_data = full_body['data'] \n\n# Steps derived from these commands won't have their error details shown inline, as they're messy\ncommands_to_skip_logs = ['dbt source', 'dbt docs']\n\n# When testing, you will want to hardcode run_id and account_id to IDs that exist; the sample webhook won't work. \nrun_id = hook_data['runId']\naccount_id = full_body['accountId']\n\n# Fetch run info from the dbt Cloud Admin API\nurl = f'https://YOUR_ACCESS_URL/api/v2/accounts/{account_id}/runs/{run_id}/?include_related=[\"run_steps\"]'\nheaders = {'Authorization': f'Token {api_token}'}\nrun_data_response = requests.get(url, headers=headers)\nrun_data_response.raise_for_status()\nrun_data_results = run_data_response.json()['data']\n\n# Overall run summary\noutcome_message = f\"\"\"\n**\\[{hook_data['runStatus']} for Run #{run_id} on Job \\\"{hook_data['jobName']}\\\"]({run_data_results['href']})**\n\n\n**Environment:** {hook_data['environmentName']} | **Trigger:** {hook_data['runReason']} | **Duration:** {run_data_results['duration_humanized']}\n\n\"\"\"\n\n# Step-specific summaries\nfor step in run_data_results['run_steps']:\n  if step['status_humanized'] == 'Success':\n    outcome_message += f\"\"\"\n {step['name']} ({step['status_humanized']} in {step['duration_humanized']})\n\"\"\"\n  else:\n    outcome_message += f\"\"\"\n {step['name']} ({step['status_humanized']} in {step['duration_humanized']})\n\"\"\"\n    show_logs = not any(cmd in step['name'] for cmd in commands_to_skip_logs)\n    if show_logs:\n      full_log = step['logs']\n      # Remove timestamp and any colour tags\n      full_log = re.sub('\\x1b?\\[[0-9]+m[0-9:]*', '', full_log)\n    \n      summary_start = re.search('(?:Completed with \\d+ error.* and \\d+ warnings?:|Database Error|Compilation Error|Runtime Error)', full_log)\n    \n      line_items = re.findall('(^.*(?:Failure|Error) in .*\\n.*\\n.*)', full_log, re.MULTILINE)\n    \n      if len(line_items) == 0:\n        relevant_log = f'```{full_log[summary_start.start() if summary_start else 0:]}```'\n      else:\n        relevant_log = summary_start[0]\n        for item in line_items:\n          relevant_log += f'\\n```\\n{item.strip()}\\n```\\n'\n      outcome_message += f\"\"\"\n{relevant_log}\n\"\"\"\n\n# Zapier looks for the `output` dictionary for use in subsequent steps\noutput = {'outcome_message': outcome_message}\n```\n\n----------------------------------------\n\nTITLE: Preserving Leading Zeros in Zipcodes Example\nDESCRIPTION: Example configuration showing how to preserve leading zeros in zipcode columns by specifying a varchar data type.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/column_types.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  jaffle_shop: # you must include the project name\n    warehouse_locations:\n      +column_types:\n        zipcode: varchar(5)\n```\n\n----------------------------------------\n\nTITLE: Creating Unit Test Macro for String Literal\nDESCRIPTION: Unit test implementation to verify the to_literal macro functionality using error handling.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-08-22-unit-testing-dbt-package.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{% macro test_to_literal() %}\n\n    {% = dbt_sample_package.to_literal('test string') %}\n\n    {% if result != \"'test string'\" %}\n\n        {{ exceptions.raise_compiler_error('The test is failed') }}\n\n    {% endif %}\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: All Columns Check Example (v1.8)\nDESCRIPTION: Example showing how to configure snapshot to check all columns for changes using Jinja/SQL format.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/check_cols.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot orders_snapshot_check %}\n\n    {{\n        config(\n          strategy='check',\n          unique_key='id',\n          check_cols='all',\n        )\n    }}\n\n    select * from {{ source('jaffle_shop', 'orders') }}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Querying with Default Ascending Order By\nDESCRIPTION: Example of ordering query results using the order_by parameter with default ascending order. This orders the results by the order_gross_profit metric in ascending order.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-jdbc.md#2025-04-09_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\nselect * from {{\nsemantic_layer.query(metrics=['food_order_amount', 'order_gross_profit'],\n  group_by=[Dimension('metric_time')],\n  limit=10,\n  order_by=['order_gross_profit'])\n  }}\n```\n\n----------------------------------------\n\nTITLE: SQL IN with Subquery\nDESCRIPTION: Example showing the syntax for using the IN operator with a subquery for dynamic filtering.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/operators/sql-in.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nwhere status in (select )\n```\n\n----------------------------------------\n\nTITLE: Example of Ambiguous Alias Configuration\nDESCRIPTION: Example showing how ambiguous aliases can cause conflicts, where two models attempt to create views with the same name.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/custom-aliases.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(alias='sessions') }}\n\nselect * from ...\n```\n\n----------------------------------------\n\nTITLE: Validating Semantic Model Configurations\nDESCRIPTION: Commands to validate the defined semantic model configurations in dbt Cloud and dbt Core. Includes various options for timeout, skipping data warehouse validations, and controlling validation output details.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metricflow-commands.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndbt sl validate # For dbt Cloud users\nmf validate-configs # For dbt Core users\n\nOptions:\n  --timeout                       # dbt Cloud only\n                                  Optional timeout for data warehouse validation in dbt Cloud.\n  --dw-timeout INTEGER            # dbt Core only\n                                  Optional timeout for data warehouse\n                                  validation steps. Default None.\n  --skip-dw                       # dbt Core only\n                                  Skips the data warehouse validations.\n  --show-all                      # dbt Core only\n                                  Prints warnings and future errors.\n  --verbose-issues                # dbt Core only\n                                  Prints extra details about issues.\n  --semantic-validation-workers INTEGER  # dbt Core only\n                                  Uses specified number of workers for large configs.\n  --help                          Show this message and exit.\n```\n\n----------------------------------------\n\nTITLE: Snowflake Data Types in dbt Unit Tests\nDESCRIPTION: Demonstrates how to specify various data types in Snowflake including numeric, string, date/time, variant, geometry, geography, object, and array types. Shows proper formatting for complex data types and escaped strings.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/data-types.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nunit_tests:\n  - name: test_my_data_types\n    model: fct_data_types\n    given:\n      - input: ref('stg_data_types')\n        rows:\n         - int_field: 1\n           float_field: 2.0\n           str_field: my_string\n           str_escaped_field: \"my,cool'string\"\n           date_field: 2020-01-02\n           timestamp_field: 2013-11-03 00:00:00-0\n           timestamptz_field: 2013-11-03 00:00:00-0\n           number_field: 3\n           variant_field: 3\n           geometry_field: POINT(1820.12 890.56)\n           geography_field: POINT(-122.35 37.55)\n           object_field: {'Alberta':'Edmonton','Manitoba':'Winnipeg'}\n           str_array_field: ['a','b','c']\n           int_array_field: [1, 2, 3]\n```\n\n----------------------------------------\n\nTITLE: Fixing expression_is_true test syntax\nDESCRIPTION: Error occurs when using the deprecated 'condition' parameter in expression_is_true test. Resolution involves replacing 'condition' with 'where' parameter.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/core-upgrade/11-Older versions/upgrading-to-dbt-utils-v1.0.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\ncondition: ... -> where: ...\n```\n\n----------------------------------------\n\nTITLE: Querying Metrics with dbt Semantic Layer CLI\nDESCRIPTION: Examples of using the dbt Semantic Layer CLI to query metrics, demonstrating how different queries are counted as queried metrics for billing purposes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/billing.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndbt sl query --metrics revenue --group-by metric_time\n```\n\nLANGUAGE: shell\nCODE:\n```\ndbt sl query --metrics revenue --group-by metric_time,user__country\n```\n\nLANGUAGE: shell\nCODE:\n```\ndbt sl query --metrics revenue,gross_sales --group-by metric_time,user__country\n```\n\nLANGUAGE: shell\nCODE:\n```\ndbt sl query --metrics revenue --group-by metric_time --compile\n```\n\nLANGUAGE: shell\nCODE:\n```\ndbt sl query --metrics revenue,gross_sales --group-by metric_time --compile\n```\n\n----------------------------------------\n\nTITLE: Using No-Warning Behavior Flags in Jinja2\nDESCRIPTION: Demonstrates how to access behavior flags without triggering warning messages in Jinja2 macros by using the no_warn property.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_18\n\nLANGUAGE: sql\nCODE:\n```\n{% macro some_macro(**kwargs) %}\n    {% if adapter.behavior.enable_new_functionality_requiring_higher_permissions.no_warn %}\n        {# do the new thing #}\n    {% else %}\n        {# do the old thing #}\n    {% endif %}\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Slim CI Pipeline for dbt in Bitbucket YAML\nDESCRIPTION: This YAML configuration sets up a Slim CI pipeline for dbt in Bitbucket. It includes steps for environment setup, artifact retrieval, and conditional dbt command execution based on state deferral availability.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-04-14-add-ci-cd-to-bitbucket.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\npipelines:\n  pull-requests:\n    '**':  # run on any branch that's referenced by a pull request\n      - step:\n          name: Set up and build\n          caches:\n            - pip\n          script:\n            # Set up dbt environment + dbt packages. Rather than passing\n            # profiles.yml to dbt commands explicitly, we'll store it where dbt\n            # expects it:\n            - python -m pip install -r requirements.txt\n            - mkdir ~/.dbt\n            - cp .ci/profiles.yml ~/.dbt/profiles.yml\n            - dbt deps\n \n            # The following step downloads dbt artifacts from the Bitbucket\n            # Downloads, if available. (They are uploaded there by the CD\n            # process -- see \"Upload artifacts for slim CI runs\" step above.)\n            #\n            # curl loop ends with \"|| true\" because we want downstream steps to\n            # always run, even if the download fails. Running with \"-L\" to\n            # follow the redirect to S3, -s to suppress output, --fail to avoid\n            # outputting files if curl for whatever reason fails and confusing\n            # the downstream conditions.\n            #\n            # \">-\" converts newlines into spaces in a multiline YAML entry. This\n            # does mean that individual bash commands have to be terminated with\n            # a semicolon in order not to conflict with flow keywords (like\n            # for-do-done or if-else-fi).\n            - >-\n              export API_ROOT=\"https://api.bitbucket.org/2.0/repositories/$BITBUCKET_REPO_FULL_NAME/downloads\";\n              mkdir target-deferred/;\n              for file in manifest.json run_results.json; do\n                curl -s -L --request GET \\\n                  -u \"$BITBUCKET_USERNAME:$BITBUCKET_APP_PASSWORD\" \\\n                  --url \"$API_ROOT/$file\" \\\n                  --fail --output target-deferred/$file;\n              done || true\n            - >-\n              if [ -f target-deferred/manifest.json ]; then\n                export DBT_FLAGS=\"--defer --state target-deferred/ --select +state:modified\";\n              else\n                export DBT_FLAGS=\"\";\n              fi\n \n            # Finally, run dbt commands with the appropriate flag that depends\n            # on whether state deferral is available. (We're skipping `dbt\n            # snapshot` because only production role can write to it and it's\n            # not set up otherwise.)\n            - dbt seed\n            - dbt run $DBT_FLAGS\n            - dbt test $DBT_FLAGS\n```\n\n----------------------------------------\n\nTITLE: Configuring Stripe Payment Source\nDESCRIPTION: YAML configuration defining the stripe payment source with database and schema details.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/mesh-qs.md#2025-04-09_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: stripe\n    database: raw\n    schema: stripe \n    tables:\n      - name: payment\n```\n\n----------------------------------------\n\nTITLE: Unioning dbt Models\nDESCRIPTION: This dbt SQL model combines multiple staging models using a UNION operation. It demonstrates how to create a final consolidated model in the dbt approach.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-07-19-migrating-from-stored-procs.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        materialized='table'\n    )\n}}\n \nwith a as ( select * from {{ ref('stg_orders_a') }} ),\nb as (select * from {{ ref('stg_orders_b') }} ),\n \nunioned as (\n    select * from a \n    union all\n    select * from b\n)\n \nselect * from unioned\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Domain URL for Snowflake Connection\nDESCRIPTION: YAML configuration for connecting to Snowflake through a custom domain (vanity URL) instead of the standard account locator. This extended attribute overrides the default connection URL.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/connect-data-platform/connect-snowflake.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nhost: https://custom_domain_to_snowflake.com\n```\n\n----------------------------------------\n\nTITLE: Listing Schema Tests of Incremental Models in dbt\nDESCRIPTION: Example of using dbt ls to list schema tests for incremental models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/list.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ dbt ls --select config.materialized:incremental,test_type:schema\nmodel.my_project.logs_parsed\nmodel.my_project.events_categorized\n```\n\n----------------------------------------\n\nTITLE: Custom Singular Test for Lap Times\nDESCRIPTION: SQL-based custom test to validate that moving averages of lap times are positive or null values.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_40\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        enabled=true,\n        severity='error',\n        tags = ['bi']\n    )\n}}\n\nwith lap_times_moving_avg as ( select * from {{ ref('lap_times_moving_avg') }} )\n\nselect *\nfrom lap_times_moving_avg \nwhere lap_moving_avg_5_years < 0 and lap_moving_avg_5_years is not null\n```\n\n----------------------------------------\n\nTITLE: Original dbt Command for Complex Selection\nDESCRIPTION: The original dbt command that is being represented in YAML selector examples.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/yaml-selectors.md#2025-04-09_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\n$ dbt run --select @source:snowplow,tag:nightly models/export --exclude package:snowplow,config.materialized:incremental export_performance_timing\n```\n\n----------------------------------------\n\nTITLE: Metric Reference Syntax in YAML\nDESCRIPTION: Basic syntax for referencing a metric in a where filter using the Metric() object. Requires a metric name and exactly one entity.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/ref-metrics-in-filters.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{{ Metric('metric_name', group_by=['entity_name']) }}\n```\n\n----------------------------------------\n\nTITLE: Installing dbt Adapter Scaffold with Cookiecutter\nDESCRIPTION: Command to generate a new dbt adapter plugin scaffolding using the dbt-database-adapter-scaffold template via cookiecutter.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ cookiecutter gh:dbt-labs/dbt-database-adapter-scaffold\n```\n\n----------------------------------------\n\nTITLE: Error Handling with set_strict() Method in Jinja\nDESCRIPTION: Demonstrates how the set_strict() method raises an error when given an invalid iterable. Unlike the standard set() method, set_strict() will throw a TypeError when it can't convert the input to a set.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/set.md#2025-04-09_snippet_4\n\nLANGUAGE: jinja\nCODE:\n```\n{% set my_invalid_iterable = 1234 %}\n{% set my_set = set_strict(my_invalid_iterable) %}\n{% do log(my_set) %}\n\nCompilation Error in ... (...)\n  'int' object is not iterable\n```\n\n----------------------------------------\n\nTITLE: Configuring Target Schema in dbt Project Configuration\nDESCRIPTION: Example showing how to set the target schema for snapshots in dbt_project.yml using the resource-path configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/target_schema.md#2025-04-09_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nsnapshots:\n  [<resource-path>]:\n    +target_schema: string\n```\n\n----------------------------------------\n\nTITLE: Defining Tagged Columns for Testing in dbt\nDESCRIPTION: This YAML configuration shows how to tag a column in a dbt model and define a test for it.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/test-selection-examples.md#2025-04-09_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: orders\n    columns:\n      - name: order_id\n        tags: [my_column_tag]\n        tests:\n          - unique\n```\n\n----------------------------------------\n\nTITLE: Implementing Orders Mart Model in SQL\nDESCRIPTION: Demonstrates the SQL implementation of an orders mart model, joining order data with payment information to create a denormalized view of orders.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-structure/4-marts.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n-- orders.sql\n\nwith\n\norders as  (\n\n    select * from {{ ref('stg_jaffle_shop__orders' )}}\n\n),\n\norder_payments as (\n\n    select * from {{ ref('int_payments_pivoted_to_orders') }}\n\n),\n\norders_and_order_payments_joined as (\n\n    select\n        orders.order_id,\n        orders.customer_id,\n        orders.order_date,\n        coalesce(order_payments.total_amount, 0) as amount,\n        coalesce(order_payments.gift_card_amount, 0) as gift_card_amount\n\n    from orders\n\n    left join order_payments on orders.order_id = order_payments.order_id\n\n)\n\nselect * from orders_and_payments_joined\n```\n\n----------------------------------------\n\nTITLE: Using Inline SQL Comments in dbt Models\nDESCRIPTION: Example showing both multi-line and single-line SQL comments in a dbt model. This snippet demonstrates commenting out specific lines of code and using multi-line comments to document complex logic.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/other/sql-comments.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n/* these lines form a multi-line SQL comment; if it's uncommented, \nit will make this query error out */\nselect\n\tcustomer_id,\n\t-- order_id, this row is commented out\n\torder_date\nfrom {{ ref ('orders') }}\n```\n\n----------------------------------------\n\nTITLE: Cleanup Phase in dbt Materializations\nDESCRIPTION: Example of the cleanup phase in a dbt materialization, demonstrating how to drop backup relations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/create-new-materializations.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{{ drop_relation_if_exists(backup_relation) }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Automatic Authentication for Azure SQL in dbt\nDESCRIPTION: This configuration automatically tries multiple authentication methods in sequence, including environment-based, managed identity, Visual Studio, VS Code, Azure CLI, and Azure PowerShell module authentication.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/mssql-setup.md#2025-04-09_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: sqlserver\n      driver: 'ODBC Driver 18 for SQL Server' # (The ODBC Driver installed on your system)\n      server: hostname or IP of your server\n      port: 1433\n      database: exampledb\n      schema: schema_name\n      authentication: auto\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Delimiter for a Specific Seed in properties.yml\nDESCRIPTION: This YAML configuration sets a custom delimiter (semicolon) for a specific seed named 'country_codes'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/delimiter.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nseeds:\n  - name: country_codes\n    config:\n      delimiter: \";\"\n```\n\n----------------------------------------\n\nTITLE: Dynamic Schema Naming with Git Branch\nDESCRIPTION: Example of using the DBT_CLOUD_GIT_BRANCH environment variable to dynamically set schema names based on the current Git branch.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/environment-variables.md#2025-04-09_snippet_1\n\nLANGUAGE: jinja\nCODE:\n```\n{{ env_var('DBT_CLOUD_GIT_BRANCH') }}\n```\n\n----------------------------------------\n\nTITLE: Source Reference Usage in SQL Model\nDESCRIPTION: Demonstrates how to reference sources with different quoting configurations in a SQL model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/quoting.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nselect\n  ...\n\n-- this should be quoted\nfrom {{ source('jaffle_shop', 'orders') }}\n\n-- here, the identifier should be unquoted\nleft join {{ source('jaffle_shop', 'customers') }} using (order_id)\n```\n\n----------------------------------------\n\nTITLE: Configuring Group Assignment in Model YAML\nDESCRIPTION: Demonstrates how to assign a model to a group using model-level YAML configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/groups.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: model_name\n    config:\n      group: finance\n```\n\n----------------------------------------\n\nTITLE: Configuration Options for dbt Materializations\nDESCRIPTION: Shows how to define optional and required configuration options for custom materializations using config.get and config.require functions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/create-new-materializations.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\n# optional\nconfig.get('optional_config_name', default=\"the default\") \n# required\nconfig.require('required_config_name')\n```\n\n----------------------------------------\n\nTITLE: Configuring Pre-Hook in YAML Properties File (dbt)\nDESCRIPTION: Shows how to set a pre-hook that calls a macro in a YAML properties file for a dbt model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/hooks-operations.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: <model_name>\n    config:\n      pre_hook:\n        - \"{{ some_macro() }}\"\n```\n\n----------------------------------------\n\nTITLE: Rendering Integration Cards in JSX\nDESCRIPTION: This code snippet demonstrates how to render a grid of integration cards using JSX. It uses a custom Card component to display information about each tool that integrates with the dbt Semantic Layer. The grid is responsive with a 3-column layout.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_sl-partner-links.md#2025-04-09_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\n<div className=\"grid--3-col\">\n\n<Card\n    title=\"Power BI\"\n    link=\"/docs/cloud-integrations/semantic-layer/power-bi\"\n    body=\"Use reports to query the dbt Semantic Layer with Power BI and produce dashboards with trusted data.\"\n    icon=\"pbi\"/>\n\n <Card\n    title=\"Tableau\"\n    link=\"/docs/cloud-integrations/semantic-layer/tableau\"\n    body=\"Learn how to connect to Tableau for querying metrics and collaborating with your team.\"\n    icon=\"tableau-software\"/>\n  \n  <Card\n    title=\"Google Sheets\"\n    link=\"/docs/cloud-integrations/semantic-layer/gsheets\"\n    body=\"Discover how to connect to Google Sheets for querying metrics and collaborating with your team.\"\n    icon=\"google-sheets-logo-icon\"/>\n\n  <Card\n    title=\"Microsoft Excel\"\n    link=\"/docs/cloud-integrations/semantic-layer/excel\"\n    body=\"Connect to Microsoft Excel to query metrics and collaborate with your team. Available for Excel Desktop or Excel Online.\"\n    icon=\"excel\"/>\n\n  <div className=\"card-container\">\n    <Card\n      title=\"Dot\"\n      link=\"https://docs.getdot.ai/dot/integrations/dbt-semantic-layer\"\n      body=\"Enable everyone to analyze data with AI in Slack or Teams.\"\n      icon=\"dot-ai\"/>\n      <a href=\"https://docs.getdot.ai/dot/integrations/dbt-semantic-layer\"\n      className=\"external-link\"\n      target=\"_blank\"\n      rel=\"noopener noreferrer\">\n      <Icon name='fa-external-link' />\n    </a>\n  </div>\n\n  <div className=\"card-container\">\n    <Card\n      title=\"Hex\"\n      link=\"https://learn.hex.tech/docs/connect-to-data/data-connections/dbt-integration#dbt-semantic-layer-integration\"\n      body=\"Check out how to connect, analyze metrics, collaborate, and discover more data possibilities.\"\n      icon=\"hex\"/>\n      <a href=\"https://learn.hex.tech/docs/connect-to-data/data-connections/dbt-integration#dbt-semantic-layer-integration\"\n      className=\"external-link\"\n      target=\"_blank\"\n      rel=\"noopener noreferrer\">\n      <Icon name='fa-external-link' />\n    </a>\n  </div>\n\n<div className=\"card-container\">\n  <Card\n    title=\"Klipfolio PowerMetrics\"\n    body=\"Learn how to connect to a streamlined metrics catalog and deliver metric-centric analytics to business users.\"\n    icon=\"klipfolio\"\n    link=\"https://support.klipfolio.com/hc/en-us/articles/18164546900759-PowerMetrics-Adding-dbt-Semantic-Layer-metrics\"/>\n    <a href=\"https://support.klipfolio.com/hc/en-us/articles/18164546900759-PowerMetrics-Adding-dbt-Semantic-Layer-metrics\"\n    className=\"external-link\"\n      target=\"_blank\"\n      rel=\"noopener noreferrer\">\n      <Icon name='fa-external-link' />\n    </a>\n</div>\n\n<div className=\"card-container\">\n  <Card\n    title=\"Lightdash\"\n    body=\"Check out how to connect, query, and consume reliable dbt metrics in real time \"\n    link=\"https://docs.lightdash.com/references/dbt-semantic-layer\"\n    icon=\"lightdash\"/>\n    <a href=\"https://docs.lightdash.com/references/dbt-semantic-layer\"\n    className=\"external-link\"\n      target=\"_blank\"\n      rel=\"noopener noreferrer\">\n      <Icon name='fa-external-link' />\n    </a>\n</div>\n\n<div className=\"card-container\">\n  <Card\n    title=\"Mode\"\n    body=\"Discover how to connect, access, and get trustworthy metrics and insights.\"\n    link=\"https://mode.com/help/articles/supported-databases#dbt-semantic-layer\"\n    icon=\"mode\"/>\n    <a href=\"https://mode.com/help/articles/supported-databases#dbt-semantic-layer\"\n    className=\"external-link\"\n      target=\"_blank\"\n      rel=\"noopener noreferrer\">\n      <Icon name='fa-external-link' />\n    </a>\n</div>\n\n<div className=\"card-container\">\n  <Card\n    title=\"Push.ai\"\n    body=\"Explore how to connect and use metrics to power reports and insights that drive change.\"\n    link=\"https://docs.push.ai/data-sources/semantic-layers/dbt\"\n    icon=\"push\"/>\n    <a href=\"https://docs.push.ai/data-sources/semantic-layers/dbt\"\n    className=\"external-link\"\n      target=\"_blank\"\n      rel=\"noopener noreferrer\">\n      <Icon name='fa-external-link' />\n    </a>\n</div>\n\n<div className=\"card-container\">\n  <Card\n    title=\"Sigma (Preview)\"\n    body=\"Connect Sigma to the dbt Semantic Layer to allow you to leverage your predefined dbt metrics in Sigma workbooks.\"\n    link=\"https://help.sigmacomputing.com/docs/configure-a-dbt-semantic-layer-integration\"\n    icon=\"sigma\"/>\n    <a href=\"https://help.sigmacomputing.com/docs/configure-a-dbt-semantic-layer-integration\"\n    className=\"external-link\"\n      target=\"_blank\"\n      rel=\"noopener noreferrer\">\n      <Icon name='fa-external-link' />\n    </a>\n</div>\n\n\n<div className=\"card-container\">\n  <Card\n    title=\"Steep\"\n    body=\"Connect Steep to the dbt Semantic Layer for centralized, scalable analytics.\"\n    link=\"https://help.steep.app/integrations/dbt-cloud\"\n    icon=\"steep\"/>\n    <a href=\"https://help.steep.app/integrations/dbt-cloud\"\n    className=\"external-link\"\n      target=\"_blank\"\n      rel=\"noopener noreferrer\">\n      <Icon name='fa-external-link' />\n    </a>\n</div>\n\n</div>\n```\n\n----------------------------------------\n\nTITLE: Revealing dbt Functions in Editor with Underscores\nDESCRIPTION: Type two consecutive underscores in the dbt Cloud IDE to reveal a list of available dbt functions in the editor. This shortcut works the same way on both macOS and Windows.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/dbt-cloud-ide/keyboard-shortcuts.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n__\n```\n\n----------------------------------------\n\nTITLE: Importing and Using a Partial File in Documentation (Markdown)\nDESCRIPTION: Example of how to import and use a partial file in a documentation page. This shows the import statement and component usage for inserting reusable content.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/contributing/single-sourcing-content.md#2025-04-09_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\nDocs content here.\n\nimport SetUpPages from '/snippets/_partial-name.md';\n<!-- It's important to leave a blank line or a comment between import and usage, otherwise it won't work -->\n<SetUpPages />\n\nDocs content here.\n```\n\n----------------------------------------\n\nTITLE: Creating a Test Macro for Value Validation\nDESCRIPTION: SQL macro definition for testing if values are greater than or equal to zero, reusable across multiple models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_38\n\nLANGUAGE: sql\nCODE:\n```\n{% macro test_all_values_gte_zero(table, column) %}\n\nselect * from {{ ref(table) }} where {{ column }} < 0\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Implementing Behavior Change Flags in dbt Adapter\nDESCRIPTION: Demonstrates how to implement behavior change flags in a custom adapter, which allows for gradual transitions between different implementations. These flags help manage breaking changes in adapter behavior.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass ABCAdapter(BaseAdapter):\n    ...\n    @property\n    def _behavior_flags(self) -> List[BehaviorFlag]:\n        return [\n            {\n                \"name\": \"enable_new_functionality_requiring_higher_permissions\",\n                \"default\": False,\n                \"source\": \"dbt-abc\",\n                \"description\": (\n                    \"The dbt-abc adapter is implementing a new method for sourcing metadata. \"\n                    \"This is a more performant way for dbt to source metadata but requires higher permissions on the platform. \"\n                    \"Enabling this without granting the requisite permissions will result in an error. \"\n                    \"This feature is expected to be required by Spring 2025.\"\n                ),\n                \"docs_url\": \"https://docs.getdbt.com/reference/global-configs/behavior-changes#abc-enable_new_functionality_requiring_higher_permissions\",\n            }\n        ]\n```\n\n----------------------------------------\n\nTITLE: Example dbt YAML File Extensions\nDESCRIPTION: Demonstrates valid file naming patterns for dbt YAML configuration files that contain tests and descriptions. The file must have a .yml extension and be located in the appropriate directory (models/, seeds/, snapshots/, or macros/).\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Project/schema-yml-name.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nwhatever_you_want.yml\n```\n\n----------------------------------------\n\nTITLE: Complete Semantic Model Configuration Example\nDESCRIPTION: A comprehensive example of a complete semantic model configuration for an orders table, including all necessary components: model reference, entities, measures, and dimensions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_sl-create-semanticmodel.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_models:\n  #The name of the semantic model.\n  - name: orders\n    defaults:\n      agg_time_dimension: ordered_at\n    description: |\n      Order fact table. This table is at the order grain with one row per order. \n    #The name of the dbt model and schema\n    model: ref('orders')\n    #Entities. These usually corespond to keys in the table.\n    entities:\n      - name: order_id\n        type: primary\n      - name: location\n        type: foreign\n        expr: location_id\n      - name: customer\n        type: foreign\n        expr: customer_id\n    #Measures. These are the aggregations on the columns in the table.\n    measures: \n      - name: order_total\n        description: The total revenue for each order.\n        agg: sum\n      - name: order_count\n        expr: 1\n        agg: sum\n      - name: tax_paid\n        description: The total tax paid on each order. \n        agg: sum\n      - name: customers_with_orders\n        description: Distinct count of customers placing orders\n        agg: count_distinct\n        expr: customer_id\n      - name: locations_with_orders\n        description: Distinct count of locations with order\n        expr: location_id\n        agg: count_distinct\n      - name: order_cost\n        description: The cost for each order item. Cost is calculated as a sum of the supply cost for each order item. \n        agg: sum\n    #Dimensions. Either categorical or time. These add additional context to metrics. The typical querying pattern is Metric by Dimension.  \n    dimensions:\n      - name: ordered_at\n        type: time\n        type_params:\n          time_granularity: day \n      - name: order_total_dim\n        type: categorical\n        expr: order_total\n      - name: is_food_order\n        type: categorical\n      - name: is_drink_order\n        type: categorical\n```\n\n----------------------------------------\n\nTITLE: SQL for Joining VISITS and BUYS Tables\nDESCRIPTION: This SQL query joins the VISITS and BUYS tables to get all combinations of visits-buys events that match the join condition where buys occur within 7 days of the visit. It's the first step in calculating the conversion metric.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/conversion-metrics.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nselect\n  v.ds,\n  v.user_id,\n  v.referrer_id,\n  b.ds,\n  b.uuid,\n  1 as buys\nfrom visits v\ninner join (\n    select *, uuid_string() as uuid from buys -- Adds a uuid column to uniquely identify the different rows\n) b\non\nv.user_id = b.user_id and v.ds <= b.ds and v.ds > b.ds - interval '7 days'\n```\n\n----------------------------------------\n\nTITLE: Documenting Custom Materializations in dbt YAML Schema\nDESCRIPTION: This snippet shows how to document custom materializations in dbt using a schema file. It includes examples for default and adapter-specific materializations, demonstrating the naming convention and description format.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Docs/documenting-macros.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmacros:\n  - name: materialization_my_materialization_name_default\n    description: A custom materialization to insert records into an append-only table and track when they were added.\n  - name: materialization_my_materialization_name_xyz\n    description: A custom materialization to insert records into an append-only table and track when they were added.\n```\n\n----------------------------------------\n\nTITLE: Defining Tagged Tests in dbt\nDESCRIPTION: This YAML configuration shows how to tag a specific test in a dbt model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/test-selection-examples.md#2025-04-09_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: orders\n    columns:\n      - name: order_id\n        tests:\n          - unique:\n              tags: [my_test_tag]\n```\n\n----------------------------------------\n\nTITLE: Soft Delete Implementation in dbt (SQL)\nDESCRIPTION: This code shows how to implement a soft delete in dbt by marking records for deletion and then filtering them out.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/migrate-from-stored-procedures.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nWITH\n\nsoft_deletes AS (\n\n    SELECT\n        *,\n        CASE\n            WHEN order_status IS NULL THEN true\n            ELSE false\n        END AS to_delete\n\n    FROM {{ ref('stg_orders') }}\n\n)\n\nSELECT * FROM soft_deletes WHERE to_delete = false\n```\n\n----------------------------------------\n\nTITLE: Configuring Partial Parsing in DBT\nDESCRIPTION: Configuration example for enabling partial parsing in DBT projects through profiles.yml. Partial parsing can improve performance by only parsing changed files.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/parsing.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nconfig:\n  partial_parse: true\n```\n\n----------------------------------------\n\nTITLE: Calculating Time on Task with Subquery in SQL\nDESCRIPTION: This SQL snippet demonstrates how to calculate the Time on Task (business hours between dates) using a subquery approach. It accounts for partial hours at the start and end, and counts full business hours in between using a custom calendar table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-01-12-time-on-task-calculation.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    (60 - extract(minute from start_time) +\n    ( select count_if(is_business_hour) * 60 from all_business_hours where date_hour > start_date  and date_hour < end_date ) +\n   ( extract(minute  from end_time)\nfrom table\n```\n\n----------------------------------------\n\nTITLE: Setting Up dbt Profile Target for Testing in Python\nDESCRIPTION: This pytest fixture defines the dbt profile target configuration for testing. It demonstrates how to set up connection details for the adapter, using environment variables for sensitive information.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\nimport os\n\n# Import the standard functional fixtures as a plugin\n# Note: fixtures with session scope need to be local\npytest_plugins = [\"dbt.tests.fixtures.project\"]\n\n# The profile dictionary, used to write out profiles.yml\n# dbt will supply a unique schema per test, so we do not specify 'schema' here\n@pytest.fixture(scope=\"class\")\ndef dbt_profile_target():\n    return {\n        'type': '<myadapter>',\n        'threads': 1,\n        'host': os.getenv('HOST_ENV_VAR_NAME'),\n        'user': os.getenv('USER_ENV_VAR_NAME'),\n        ...\n    }\n```\n\n----------------------------------------\n\nTITLE: Basic Seed Path Configuration in dbt\nDESCRIPTION: Basic configuration for specifying the seed-paths parameter in dbt_project.yml. This allows customization of where dbt looks for seed files.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/seed-paths.md#2025-04-09_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nseed-paths: [directorypath]\n```\n\n----------------------------------------\n\nTITLE: Mentioning SQL DDL and DML in dbt Context\nDESCRIPTION: References DDL and DML in the context of dbt transformations, highlighting the complexities and potential issues when writing these directly.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Project/why-not-write-dml.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nddl\n```\n\nLANGUAGE: sql\nCODE:\n```\ndml\n```\n\n----------------------------------------\n\nTITLE: Defining Cumulative Metric for All-Time Revenue in dbt YAML\nDESCRIPTION: This snippet creates a cumulative metric to calculate all-time revenue. It uses the 'revenue' measure and doesn't specify a window, indicating it should accumulate over the entire time period selected by users.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-build-our-metrics/semantic-layer-5-advanced-metrics.md#2025-04-09_snippet_3\n\nLANGUAGE: yml\nCODE:\n```\n- name: cumulative_revenue\n  description: The cumulative revenue for all orders.\n  label: Cumulative Revenue (All Time)\n  type: cumulative\n  type_params:\n    measure: revenue\n```\n\n----------------------------------------\n\nTITLE: Querying Joined Data in MetricFlow using CLI\nDESCRIPTION: These CLI commands demonstrate how to query data from joined semantic models in MetricFlow. They show how to use the '--metrics' and '--group-by' flags to specify metrics and dimensions from different models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/join-logic.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndbt sl query --metrics average_purchase_price --group-by metric_time,user_id__type # In dbt Cloud\n```\n\nLANGUAGE: yaml\nCODE:\n```\nmf query --metrics average_purchase_price --group-by metric_time,user_id__type # In dbt Core\n```\n\n----------------------------------------\n\nTITLE: Promoting Warnings to Errors in dbt 1.8+ (Command-line)\nDESCRIPTION: These commands demonstrate how to promote all warnings to errors, exclude specific warnings, or target only certain warnings as errors using the --warn-error-options flag in dbt version 1.8 and above.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/warnings.md#2025-04-09_snippet_3\n\nLANGUAGE: text\nCODE:\n```\ndbt --warn-error-options '{\"error\": \"all\"}' run\n```\n\nLANGUAGE: text\nCODE:\n```\ndbt --warn-error-options '{\"error\": \"all\", \"warn\": [\"NoNodesForSelectionCriteria\"]}' run\n```\n\nLANGUAGE: text\nCODE:\n```\ndbt --warn-error-options '{\"error\": [\"NoNodesForSelectionCriteria\"]}' run\n```\n\nLANGUAGE: text\nCODE:\n```\nDBT_WARN_ERROR_OPTIONS='{\"error\": [\"NoNodesForSelectionCriteria\"]}' dbt run\n```\n\n----------------------------------------\n\nTITLE: Cancelling Active Session in dbt Cloud CLI\nDESCRIPTION: To cancel an active session in the dbt Cloud CLI, use the Ctrl + Z keyboard shortcut.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Troubleshooting/long-sessions-cloud-cli.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nCtrl + Z\n```\n\n----------------------------------------\n\nTITLE: Querying Column-Level Model Information in GraphQL\nDESCRIPTION: GraphQL query to fetch detailed column information for a model, including names, types, comments, descriptions, tags, and metadata. Requires prior execution of 'dbt docs generate'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/schema-discovery-job-model.mdx#2025-04-09_snippet_2\n\nLANGUAGE: graphql\nCODE:\n```\n{\n  job(id: 123) {\n    model(uniqueId: \"model.jaffle_shop.dim_user\") {\n      columns {\n        name\n        index\n        type\n        comment\n        description\n        tags\n        meta\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Resource Path Structure in YAML\nDESCRIPTION: Demonstrates the basic nested dictionary structure used for resource paths in dbt configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/resource-path.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nresource_type:\n  project_name:\n    directory_name:\n      subdirectory_name:\n        instance_of_resource_type (by name):\n          ...\n```\n\n----------------------------------------\n\nTITLE: Reinstalling dbt Core with Adapter using pip\nDESCRIPTION: Command to reinstall dbt Core with a specific adapter after uninstalling the dbt Cloud CLI, used when switching back from dbt Cloud CLI to dbt Core.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/cloud-cli-installation.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install dbt-adapter_name --force-reinstall\n```\n\n----------------------------------------\n\nTITLE: Building a Customer Model in dbt Cloud\nDESCRIPTION: SQL query for creating a dbt model that combines and transforms data from customers and orders tables to create a comprehensive customer view.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/starburst-galaxy-qs.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nwith customers as (\n\n    select\n        id as customer_id,\n        first_name,\n        last_name\n\n    from dbt_quickstart.jaffle_shop.jaffle_shop_customers\n),\n\norders as (\n\n    select\n        id as order_id,\n        user_id as customer_id,\n        order_date,\n        status\n\n    from dbt_quickstart.jaffle_shop.jaffle_shop_orders\n),\n\n\ncustomer_orders as (\n\n    select\n        customer_id,\n        min(order_date) as first_order_date,\n        max(order_date) as most_recent_order_date,\n        count(order_id) as number_of_orders\n\n    from orders\n    group by 1\n),\n\nfinal as (\n\n    select\n        customers.customer_id,\n        customers.first_name,\n        customers.last_name,\n        customer_orders.first_order_date,\n        customer_orders.most_recent_order_date,\n        coalesce(customer_orders.number_of_orders, 0) as number_of_orders\n\n    from customers\n    left join customer_orders on customers.customer_id = customer_orders.customer_id\n)\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Database Preparation Pseudocode for Table Materialization\nDESCRIPTION: Pseudocode example showing how to check for existing relations and prepare the database before creating a new table materialization.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/create-new-materializations.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n-- Refer to the table materialization (linked above) for an example of real syntax\n-- This code will not work and is only intended for demonstration purposes\n{% set existing = adapter.get_relation(this) %}\n{% if existing and existing.is_view  %}\n  {% do adapter.drop_relation(existing) %}\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Event Details Structure in Markdown\nDESCRIPTION: Table showing the structure and description of event detail fields in the audit log\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/manage-access/audit-log.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Name                 | Description                                   |\n| -------------------- | --------------------------------------------- |\n| account_id           | Account ID of where the event occurred        |\n| actor                | Actor that carried out the event - User or Service    |\n| actor_id             | Unique ID of the actor                        |\n| actor_ip             | IP address of the actor                       |\n| actor_name           | Identifying name of the actor                 |\n| actor_type           | Whether the action was done by a user or an API request |\n| created_at           | UTC timestamp of when the event occurred      |\n| event_type           | Unique key identifying the event              |\n| event_context        | This key will be different for each event and will match the event_type. This data will include all the details about the object(s) that was changed. |\n| id                   | Unique ID of the event                        |\n| service              | Service that carried out the action           |\n| source               | Source of the event - dbt Cloud UI or API     |\n```\n\n----------------------------------------\n\nTITLE: Configuring Service Account Impersonation for BigQuery\nDESCRIPTION: Configuration for service account impersonation, allowing users authenticating via local OAuth to access BigQuery resources with the permissions of a specified service account.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/bigquery-setup.md#2025-04-09_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nmy-profile:\n  target: dev\n  outputs:\n    dev:\n      type: bigquery\n      method: oauth\n      project: abc-123\n      dataset: my_dataset\n      impersonate_service_account: dbt-runner@yourproject.iam.gserviceaccount.com\n```\n\n----------------------------------------\n\nTITLE: Tag-based Selection in dbt\nDESCRIPTION: Demonstrates how to select models based on specified tags.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/methods.md#2025-04-09_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --select \"tag:nightly\"    # run all models with the `nightly` tag\n```\n\n----------------------------------------\n\nTITLE: Using At Operator for Comprehensive Node Selection in dbt Commands\nDESCRIPTION: Illustrates the use of the '@' operator in dbt commands. This operator selects a model, its descendants, and all ancestors of those descendants, which is useful in CI environments where some ancestors might not exist in the schema yet.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/graph-operators.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --select \"@my_model\"         # select my_model, its descendants, and the ancestors of its descendants\n```\n\n----------------------------------------\n\nTITLE: Overriding Package Macros in SQL\nDESCRIPTION: Shows how to override a package's macro implementation while still leveraging the original package for other adapters. This example provides a custom implementation for Redshift while falling back to dbt_utils for other adapters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/dispatch.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{% macro concat(fields) -%}\n    {{ return(adapter.dispatch('concat')(fields)) }}\n{%- endmacro %}\n\n{% macro default__concat(fields) -%}\n    {{ return(dbt_utils.concat(fields)) }}\n{%- endmacro %}\n\n{% macro redshift__concat(fields) %}\n    {% for field in fields %}\n        nullif({{ field }},'') {{ ' || ' if not loop.last }}\n    {% endfor %}\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Creating an Ephemeral Model for Snapshot Source Data\nDESCRIPTION: An ephemeral SQL model that selects all data from the 'orders' source table in the 'jaffle_shop' schema. This model can be used as a transformation layer before snapshotting.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/snapshots.md#2025-04-09_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\n{{ config(materialized='ephemeral') }}\n\nselect * from {{ source('jaffle_shop', 'orders') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Partitioned Table in Apache Impala\nDESCRIPTION: Example showing how to create a partitioned table in Impala using dbt. The configuration includes setting materialization type as table, specifying a unique key, and partitioning by city. Note that the partition column must be the last column in the select statement.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/impala-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        materialized='table',\n        unique_key='id',\n        partition_by=['city'],\n    )\n}}\n\nwith source_data as (\n     select 1 as id, \"Name 1\" as name, \"City 1\" as city,\n     union all\n     select 2 as id, \"Name 2\" as name, \"City 2\" as city,\n     union all\n     select 3 as id, \"Name 3\" as name, \"City 2\" as city,\n     union all\n     select 4 as id, \"Name 4\" as name, \"City 1\" as city,\n)\n\nselect * from source_data\n```\n\n----------------------------------------\n\nTITLE: Creating a Singular Data Test in SQL\nDESCRIPTION: This SQL snippet defines a singular data test that checks if the total payment amount is positive for each order. It selects records where the total amount is negative, which would cause the test to fail.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/data-tests.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n-- Refunds have a negative amount, so the total amount should always be >= 0.\n-- Therefore return records where total_amount < 0 to make the test fail.\nselect\n    order_id,\n    sum(amount) as total_amount\nfrom {{ ref('fct_payments') }}\ngroup by 1\nhaving total_amount < 0\n```\n\n----------------------------------------\n\nTITLE: dbt Command Example\nDESCRIPTION: Example command showing how to list metrics related to a semantic model using dbt's selection syntax. The + operator is used to select downstream nodes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_metrics-dependencies.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndbt list --select my_semantic_model+\n```\n\n----------------------------------------\n\nTITLE: Configuring Yearly Time Spine in YAML\nDESCRIPTION: This YAML configuration defines a yearly time spine model for MetricFlow. It specifies the standard granularity column as date_year and configures the column with year granularity, enabling yearly aggregations in the Semantic Layer.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/mf-time-spine.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: time_spine_daily\n    ... rest of the daily time spine config ...\n\n  - name: time_spine_yearly\n    description: time spine one row per house\n    time_spine:\n      standard_granularity_column: date_year\n    columns:\n      - name: date_year\n        granularity: year\n```\n\n----------------------------------------\n\nTITLE: Creating Base Customer Model in dbt for Staging Layer\nDESCRIPTION: Creates a base model for customers from the jaffle_shop source. This model extracts and renames fields from the raw customer data before it can be joined with deletion information in the staging model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-structure/2-staging.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n-- base_jaffle_shop__customers.sql\n\nwith\n\nsource as (\n\n    select * from {{ source('jaffle_shop','customers') }}\n\n),\n\ncustomers as (\n\n    select\n        id as customer_id,\n        first_name,\n        last_name\n\n    from source\n\n)\n\nselect * from customers\n```\n\n----------------------------------------\n\nTITLE: Creating Extended Attributes for BigQuery via API in dbt Cloud\nDESCRIPTION: This API request creates the extended attributes payload for a project, defining the service account JSON structure with environment variable references for sensitive data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/connect-data-platform/connnect-bigquery.md#2025-04-09_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ncurl --request POST \\\n--url https://cloud.getdbt.com/api/v3/accounts/XXXXX/projects/YYYYY/extended-attributes/ \\\n--header 'Accept: application/json' \\\n--header 'Authorization: Bearer ZZZZZ' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"id\": null,\n\"extended_attributes\": {\"type\":\"service_account\",\"project_id\":\"xxx\",\"private_key_id\":\"xxx\",\"private_key\":\"{{ env_var(\\'DBT_ENV_SECRET_PROJECTXXX_PRIVATE_KEY\\')    }}\",\"client_email\":\"xxx\",\"client_id\":xxx,\"auth_uri\":\"https://accounts.google.com/o/oauth2/auth\",\"token_uri\":\"https://oauth2.googleapis.com/token\",\"auth_provider_x509_cert_url\":\"https://www.googleapis.com/oauth2/v1/certs\",\"client_x509_cert_url\":\"xxx\"},\n\"state\": 1\n}'\n```\n\n----------------------------------------\n\nTITLE: Python Data Loading Examples\nDESCRIPTION: Examples showing different approaches to loading data from CSV files into databases using Python client libraries.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ngoogle.cloud.bigquery.Client.load_table_from_file(...)\n```\n\n----------------------------------------\n\nTITLE: Using Order and Limit Functions in Queries\nDESCRIPTION: Example showing how to add order and limit functions to filter results and improve readability. Demonstrates sorting in descending order with the - prefix and limiting results to 10 records.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metricflow-commands.md#2025-04-09_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n# In dbt Cloud \ndbt sl query --metrics order_total --group-by order_id__is_food_order --limit 10 --order-by -metric_time \n\n# In dbt Core\nmf query --metrics order_total --group-by order_id__is_food_order --limit 10 --order-by -metric_time \n```\n\nLANGUAGE: bash\nCODE:\n```\n Success  - query completed after 1.41 seconds\n| METRIC_TIME   | IS_FOOD_ORDER   |   ORDER_TOTAL |\n|:--------------|:----------------|---------------:|\n| 2017-08-31    | True            |         459.90 |\n| 2017-08-31    | False           |         327.08 |\n| 2017-08-30    | False           |         348.90 |\n| 2017-08-30    | True            |         448.18 |\n| 2017-08-29    | True            |         479.94 |\n| 2017-08-29    | False           |         333.65 |\n| 2017-08-28    | False           |         334.73 |\n```\n\n----------------------------------------\n\nTITLE: Using config() Macro for Resource-Specific Configurations in dbt\nDESCRIPTION: This snippet illustrates how to use the {{ config() }} macro within a resource's file to set resource-specific configurations directly in the file where the resource is defined.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_config-description-resource.md#2025-04-09_snippet_2\n\nLANGUAGE: jinja\nCODE:\n```\n{{ config() }}\n```\n\n----------------------------------------\n\nTITLE: Multi-Adapter String Literal Implementation\nDESCRIPTION: Enhanced version of to_literal macro supporting multiple database adapters including Postgres.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-08-22-unit-testing-dbt-package.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{% macro to_literal(text) %}\n\n    {{ return(adapter.dispatch('to_literal', 'dbt_sample_package')(text)) }}\n\n{% endmacro %}\n\n{% macro default__to_literal(text) %}\n\n    '{{- text -}}'\n\n{% endmacro %}\n\n{% macro postgres__to_literal(text) %}\n\n    E'{{- text -}}'\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Bitbucket Pipelines Configuration\nDESCRIPTION: Bitbucket Pipelines configuration for running a dbt Cloud job on main branch pushes. Uses Python 3.11.1 as base image and exports required environment variables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/custom-cicd-pipelines.md#2025-04-09_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nimage: python:3.11.1\n\npipelines:\n  branches:\n    'main':\n      - step:\n          name: 'Run dbt Cloud Job'\n          script:\n            - export DBT_URL=\"https://cloud.getdbt.com\"\n            - export DBT_JOB_CAUSE=\"Bitbucket Pipeline CI Job\"\n            - export DBT_ACCOUNT_ID=00000\n            - export DBT_PROJECT_ID=00000\n            - export DBT_PR_JOB_ID=00000\n            - python python/run_and_monitor_dbt_job.py\n```\n\n----------------------------------------\n\nTITLE: Docker Hub Pulls Loader Script\nDESCRIPTION: Python script to fetch Docker Hub image pull counts and load them into BigQuery, similar to the GitHub stars loader.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-11-29-open-source-community-growth.md#2025-04-09_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfor image in images:\n  url = 'https://hub.docker.com/v2/repositories/' + image\n  response = requests.get(url)\n  pull_count = response.json()['pull_count']\n  client.insert_rows(table, [(now,image,pull_count)])\n```\n\n----------------------------------------\n\nTITLE: EXTRACT Function Example with Jaffle Shop Data in SQL\nDESCRIPTION: Illustrates the use of the EXTRACT function to extract week, month, and year from an order_date field in the jaffle shop orders table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-06-30-extract-sql-function.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect \n\torder_id,\n\torder_date,\n\textract(week from order_date) as order_week,\n\textract(month from order_date) as order_month,\n\textract(year from order_date) as order_year\nfrom {{ ref('orders') }}\n```\n\n----------------------------------------\n\nTITLE: Field Separator Logic in generate_surrogate_key Macro\nDESCRIPTION: This snippet demonstrates the conditional logic used by the generate_surrogate_key macro to add separators between fields when constructing a surrogate key. It appends a hyphen between fields except for the last one.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-11-22-sql-surrogate-keys.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\n  {%- if not loop.last %}\n    {%- set _ = fields.append(\"'-'\") -%}\n  {%- endif -%}\n```\n\n----------------------------------------\n\nTITLE: Using Behavior Flags in Python Adapter Methods\nDESCRIPTION: Shows how to use behavior flags in adapter methods to conditionally execute different code paths based on whether a new behavior is enabled or not.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nclass ABCAdapter(BaseAdapter):\n    ...\n    def some_method(self, *args, **kwargs):\n        if self.behavior.enable_new_functionality_requiring_higher_permissions:\n            # do the new thing\n        else:\n            # do the old thing\n```\n\n----------------------------------------\n\nTITLE: Configuring Lookback in dbt_project.yml\nDESCRIPTION: Sets the lookback value to 2 for the user_sessions model in the project configuration file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/lookback.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  my_project:\n    user_sessions:\n      +lookback: 2\n```\n\n----------------------------------------\n\nTITLE: Loading Jaffle Shop Sample Data into Microsoft Fabric\nDESCRIPTION: SQL script to create and populate three tables (customers, orders, and payments) in Microsoft Fabric using sample Jaffle Shop data from parquet files stored in Azure Blob Storage. The script drops any existing tables before creating new ones with the appropriate schema.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/microsoft-fabric-qs.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nDROP TABLE dbo.customers;\n\nCREATE TABLE dbo.customers\n(\n    [ID] [int],\n    \\[FIRST_NAME] [varchar](8000),\n    \\[LAST_NAME] [varchar](8000)\n);\n\nCOPY INTO [dbo].[customers]\nFROM 'https://dbtlabsynapsedatalake.blob.core.windows.net/dbt-quickstart-public/jaffle_shop_customers.parquet'\nWITH (\n    FILE_TYPE = 'PARQUET'\n);\n\nDROP TABLE dbo.orders;\n\nCREATE TABLE dbo.orders\n(\n    [ID] [int],\n    [USER_ID] [int],\n    -- [ORDER_DATE] [int],\n    [ORDER_DATE] [date],\n    \\[STATUS] [varchar](8000)\n);\n\nCOPY INTO [dbo].[orders]\nFROM 'https://dbtlabsynapsedatalake.blob.core.windows.net/dbt-quickstart-public/jaffle_shop_orders.parquet'\nWITH (\n    FILE_TYPE = 'PARQUET'\n);\n\nDROP TABLE dbo.payments;\n\nCREATE TABLE dbo.payments\n(\n    [ID] [int],\n    [ORDERID] [int],\n    \\[PAYMENTMETHOD] [varchar](8000),\n    \\[STATUS] [varchar](8000),\n    [AMOUNT] [int],\n    [CREATED] [date]\n);\n\nCOPY INTO [dbo].[payments]\nFROM 'https://dbtlabsynapsedatalake.blob.core.windows.net/dbt-quickstart-public/stripe_payments.parquet'\nWITH (\n    FILE_TYPE = 'PARQUET'\n);\n```\n\n----------------------------------------\n\nTITLE: Defining Simple Metric for Food Revenue in dbt YAML\nDESCRIPTION: This snippet defines a simple metric for food revenue in the dbt Semantic Layer. It uses the 'food_revenue' measure as the basis for the metric.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-build-our-metrics/semantic-layer-5-advanced-metrics.md#2025-04-09_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\n- name: food_revenue\n  description: The revenue from food in each order.\n  label: Food Revenue\n  type: simple\n  type_params:\n    measure: food_revenue\n```\n\n----------------------------------------\n\nTITLE: Setting Up External Table Options in YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure the et_options.yml file for setting up external table options in dbt for IBM Netezza. It includes options for skipping rows, specifying delimiters, and setting maximum errors.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/ibmnetezza-setup.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- !ETOptions\n    SkipRows: \"1\"\n    Delimiter: \"','\"\n    DateDelim: \"'-'\"\n    MaxErrors: \" 0 \"\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Models in Properties YAML File (Pre-1.9)\nDESCRIPTION: Example of configuring dbt models using a properties.yml file. This shows how to set configurations for specific models by name, with all available configuration options for versions before 1.9.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/model-configs.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: [<model-name>] # Must match the filename of a model -- including case sensitivity.\n    config:\n      [enabled]: true | false\n      [tags]: <string> | [<string>]\n      [pre_hook]: <sql-statement> | [<sql-statement>]\n      [post_hook]: <sql-statement> | [<sql-statement>]\n      [database]: <string>\n      [schema]: <string>\n      [alias]: <string>\n      [persist_docs]: <dict>\n      [full_refresh]: <boolean>\n      [meta]: {<dictionary>}\n      [grants]: {<dictionary>}\n      [contract]: {<dictionary>}\n```\n\n----------------------------------------\n\nTITLE: Compiled SQL for Append Strategy in Databricks\nDESCRIPTION: This shows the compiled SQL that dbt generates when executing an incremental model with the append strategy. It creates a temporary view with the filtered data and then inserts those records into the target table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/databricks-configs.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\ncreate temporary view databricks_incremental__dbt_tmp as\n\n    select * from analytics.events\n\n    where event_ts >= (select max(event_ts) from {{ this }})\n\n;\n\ninsert into table analytics.databricks_incremental\n    select `date_day`, `users` from databricks_incremental__dbt_tmp\n```\n\n----------------------------------------\n\nTITLE: Displaying Incorrect Username/Password Error in Snowflake Connection\nDESCRIPTION: This code snippet shows the error message displayed when there's an issue with the username or password in the Snowflake connection. It indicates that the provided credentials are incorrect.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Troubleshooting/failed-snowflake-oauth-connection.md#2025-04-09_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nFailed to connect to DB: xxxxxxx.snowflakecomputing.com:443. Incorrect username or password was specified.\n```\n\n----------------------------------------\n\nTITLE: Running dbt Build Command Selecting Only Specific Project Tags\nDESCRIPTION: A dbt CLI command that builds only models tagged with 'smaller_subset_project'. This enables running only the models from the consolidated project separately.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Project/consolidate-projects.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndbt build --select tag:smaller_subset_project\n```\n\n----------------------------------------\n\nTITLE: Configuring Snapshot Meta Column Names in dbt_project.yml\nDESCRIPTION: Example of setting project-wide or path-specific snapshot metadata column names in the dbt_project.yml file. This approach allows applying consistent naming conventions across multiple snapshots.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snapshot_meta_column_names.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  [<resource-path>]:\n    +snapshot_meta_column_names:\n      dbt_valid_from: <string>\n      dbt_valid_to: <string>\n      dbt_scd_id: <string>\n      dbt_updated_at: <string>\n      dbt_is_deleted: <string>\n```\n\n----------------------------------------\n\nTITLE: Generating and Visualizing a dbt Lineage Graph with Python\nDESCRIPTION: This code snippet shows how to query the Discovery API, extract node definitions, create a lineage graph, and visualize it using the previously defined plotting function.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/discovery-use-cases-and-examples.md#2025-04-09_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nquery_response = query_discovery_api(auth_token, gql_query, variables)\n\nnodes_df = extract_node_definitions(query_response)\n\nG = create_generic_lineage_graph(nodes_df)\n\nplot_generic_graph(G)\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Schema in dbt Project File\nDESCRIPTION: Sets the schema configuration for all models in the marketing directory through the dbt_project.yml file. This applies the 'marketing' schema to all models in that directory.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/custom-schemas.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# models in `models/marketing/ will be built in the \"*_marketing\" schema\nmodels:\n  my_project:\n    marketing:\n      +schema: marketing\n```\n\n----------------------------------------\n\nTITLE: Model Configuration Example in SQL\nDESCRIPTION: Shows the recommended format for specifying model-specific configurations in dbt, including materialization, sort keys, and distribution keys.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-style/2-how-we-style-our-sql.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n      materialized = 'table',\n      sort = 'id',\n      dist = 'id'\n    )\n}}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Concat Macro with Multiple Adapter Support in SQL\nDESCRIPTION: Creates a concat macro with adapter-specific implementations for Redshift and Snowflake, plus a default implementation for other adapters. This demonstrates how to handle different SQL syntax requirements across platforms.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/dispatch.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{% macro concat(fields) -%}\n    {{ return(adapter.dispatch('concat')(fields)) }}\n{%- endmacro %}\n\n\n{% macro default__concat(fields) -%}\n    concat({{ fields|join(', ') }})\n{%- endmacro %}\n\n\n{% macro redshift__concat(fields) %}\n    {{ fields|join(' || ') }}\n{% endmacro %}\n\n\n{% macro snowflake__concat(fields) %}\n    {{ fields|join(' || ') }}\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Importing MDX Components for PrivateLink Documentation\nDESCRIPTION: MDX import statements for reusable documentation components related to PrivateLink configuration and features.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/secure/about-privatelink.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nimport SetUpPages from '/snippets/_available-tiers-privatelink.md';\nimport PrivateLinkHostnameWarning from '/snippets/_privatelink-hostname-restriction.md';\nimport CloudProviders from '/snippets/_privatelink-across-providers.md';\n```\n\n----------------------------------------\n\nTITLE: Resource Type Selection\nDESCRIPTION: Selecting nodes based on their resource type\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/methods.md#2025-04-09_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ndbt build --select \"resource_type:exposure\"    # build all resources upstream of exposures\ndbt list --select \"resource_type:test\"         # list all tests in your project\ndbt list --select \"resource_type:source\"       # list all sources in your project\n```\n\n----------------------------------------\n\nTITLE: Configuring Unit Tests in dbt_project.yml with YAML\nDESCRIPTION: YAML configuration for a unit test that verifies email validation logic in the dim_customers model. The test provides mock data for upstream dependencies and defines expected outputs for different test cases.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/unit-tests.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nunit_tests:\n  - name: test_is_valid_email_address\n    description: \"Check my is_valid_email_address logic captures all known edge cases - emails without ., emails without @, and emails from invalid domains.\"\n    model: dim_customers\n    given:\n      - input: ref('stg_customers')\n        rows:\n          - {email: cool@example.com,    email_top_level_domain: example.com}\n          - {email: cool@unknown.com,    email_top_level_domain: unknown.com}\n          - {email: badgmail.com,        email_top_level_domain: gmail.com}\n          - {email: missingdot@gmailcom, email_top_level_domain: gmail.com}\n      - input: ref('top_level_email_domains')\n        rows:\n          - {tld: example.com}\n          - {tld: gmail.com}\n    expect:\n      rows:\n        - {email: cool@example.com,    is_valid_email_address: true}\n        - {email: cool@unknown.com,    is_valid_email_address: false}\n        - {email: badgmail.com,        is_valid_email_address: false}\n        - {email: missingdot@gmailcom, is_valid_email_address: false}\n```\n\n----------------------------------------\n\nTITLE: Configuring Check Columns in DBT Snapshot SQL (v1.8)\nDESCRIPTION: Example of configuring check columns in a DBT snapshot using Jinja templating in SQL. Demonstrates basic configuration setup.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/check_cols.md#2025-04-09_snippet_1\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ config(\n  strategy=\"check\",\n  check_cols=[\"column_name\"]\n) }}\n```\n\n----------------------------------------\n\nTITLE: Implementing Constant Properties in SQL Conversion Queries\nDESCRIPTION: SQL query that includes a constant property condition in the join criteria to track conversions where the same product appears in both the view and purchase events, ensuring accurate product-specific conversion tracking.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/conversion-metrics.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\nselect distinct\n  first_value(v.ds) over (partition by buy_source.ds, buy_source.user_id, buy_source.session_id order by v.ds desc rows between unbounded preceding and unbounded following) as ds,\n  first_value(v.user_id) over (partition by buy_source.ds, buy_source.user_id, buy_source.session_id order by v.ds desc rows between unbounded preceding and unbounded following) as user_id,\n  first_value(v.referrer_id) over (partition by buy_source.ds, buy_source.user_id, buy_source.session_id order by v.ds desc rows between unbounded preceding and unbounded following) as referrer_id,\n  buy_source.uuid,\n  1 as buys\nfrom {{ source_schema }}.fct_view_item_details v\ninner join\n  (\n    select *, {{ generate_random_uuid() }} as uuid from {{ source_schema }}.fct_purchases\n  ) buy_source\non\n  v.user_id = buy_source.user_id\n  and v.ds <= buy_source.ds\n  and v.ds > buy_source.ds - interval '7 day'\n  and buy_source.product_id = v.product_id --Joining on the constant property product_id\n```\n\n----------------------------------------\n\nTITLE: Configuring Dispatch Search Order in dbt\nDESCRIPTION: This YAML snippet shows how to configure the dispatch search order for macros in the dbt_utils namespace, allowing for custom implementations and compatibility packages.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/dispatch.md#2025-04-09_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\ndispatch:\n  - macro_namespace: dbt_utils\n    search_order: ['my_project', 'spark_utils', 'dbt_utils']\n```\n\n----------------------------------------\n\nTITLE: Running dbt Test Command\nDESCRIPTION: Example output of running dbt test command showing test execution and results.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/data-tests.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ dbt test\n\nFound 3 models, 2 tests, 0 snapshots, 0 analyses, 130 macros, 0 operations, 0 seed files, 0 sources\n\n17:31:05 | Concurrency: 1 threads (target='learn')\n17:31:05 |\n17:31:05 | 1 of 2 START test not_null_order_order_id..................... [RUN]\n17:31:06 | 1 of 2 PASS not_null_order_order_id........................... [PASS in 0.99s]\n17:31:06 | 2 of 2 START test unique_order_order_id....................... [RUN]\n17:31:07 | 2 of 2 PASS unique_order_order_id............................. [PASS in 0.79s]\n17:31:07 |\n17:31:07 | Finished running 2 tests in 7.17s.\n\nCompleted successfully\n\nDone. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2\n```\n\n----------------------------------------\n\nTITLE: Basic SQL TRIM Function Syntax\nDESCRIPTION: The basic syntax for the TRIM function, which takes a field name and optional characters to remove. By default, it removes blank spaces from the beginning and end of a string.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/string-functions/sql-trim.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\ntrim(<field_name> [, <characters_to_remove>])\n```\n\n----------------------------------------\n\nTITLE: Creating Training Dataset with Temporal Split in Python\nDESCRIPTION: Creates a training dataset by filtering Formula 1 race data for years 2010-2019. Uses dbt Python model with pandas to create a temporal split for model training.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndef model(dbt, session):\n\n    # dbt configuration\n    dbt.config(packages=[\"pandas\"], tags=\"train\")\n\n    # get upstream data\n    encoding = dbt.ref(\"covariate_encoding\").to_pandas()\n\n    # provide years so we do not hardcode dates in filter command\n    start_year=2010\n    end_year=2019\n\n    # describe the data for a full decade\n    train_test_dataset =  encoding.loc[encoding['RACE_YEAR'].between(start_year, end_year)]\n\n    return train_test_dataset\n```\n\n----------------------------------------\n\nTITLE: Seeding Historical Data with dbt\nDESCRIPTION: Using 'dbt seed' to load historical data into the database from CSV files located in the data directory, creating tables for GitHub and Slack daily summary history.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-11-29-open-source-community-growth.md#2025-04-09_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n% dbt seed\nRunning with dbt=1.9.0\nFound 7 models, 0 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 2 seed files, 4 sources, 0 exposures\n\n18:40:45 | Concurrency: 1 threads (target='dev')\n18:40:45 | \n18:40:45 | 1 of 2 START seed file metrics.github_daily_summary_history.......... [RUN]\n18:40:49 | 1 of 2 OK loaded seed file metrics.github_daily_summary_history...... [INSERT 2000 in 4.60s]\n18:40:49 | 2 of 2 START seed file metrics.slack_daily_summary_history........... [RUN]\n18:40:53 | 2 of 2 OK loaded seed file metrics.slack_daily_summary_history....... [INSERT 316 in 4.07s]\n18:40:53 | \n18:40:53 | Finished running 2 seeds in 9.48s.\n\nCompleted successfully\n\nDone. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2\n```\n\n----------------------------------------\n\nTITLE: Examining Performance Information JSON Structure\nDESCRIPTION: The structure of the perf_info.json file generated by dbt parse, which contains detailed timing information about the parsing process for each project, parser type, and path count.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/parse.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"path_count\": 7,\n    \"is_partial_parse_enabled\": false,\n    \"parse_project_elapsed\": 0.20151838900000008,\n    \"patch_sources_elapsed\": 0.00039490800000008264,\n    \"process_manifest_elapsed\": 0.029363873999999957,\n    \"load_all_elapsed\": 0.240095269,\n    \"projects\": [\n        {\n            \"project_name\": \"my_project\",\n            \"elapsed\": 0.07518750299999999,\n            \"parsers\": [\n                {\n                    \"parser\": \"model\",\n                    \"elapsed\": 0.04545303199999995,\n                    \"path_count\": 1\n                },\n                {\n                    \"parser\": \"operation\",\n                    \"elapsed\": 0.0006415469999998535,\n                    \"path_count\": 1\n                },\n                {\n                    \"parser\": \"seed\",\n                    \"elapsed\": 0.026538173000000054,\n                    \"path_count\": 2\n                }\n            ],\n            \"path_count\": 4\n        },\n        {\n            \"project_name\": \"dbt_postgres\",\n            \"elapsed\": 0.0016448299999998195,\n            \"parsers\": [\n                {\n                    \"parser\": \"operation\",\n                    \"elapsed\": 0.00021672399999994596,\n                    \"path_count\": 1\n                }\n            ],\n            \"path_count\": 1\n        },\n        {\n            \"project_name\": \"dbt\",\n            \"elapsed\": 0.006580432000000025,\n            \"parsers\": [\n                {\n                    \"parser\": \"operation\",\n                    \"elapsed\": 0.0002488560000000195,\n                    \"path_count\": 1\n                },\n                {\n                    \"parser\": \"docs\",\n                    \"elapsed\": 0.002500640000000054,\n                    \"path_count\": 1\n                }\n            ],\n            \"path_count\": 2\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Compiled Reference in Production Environment\nDESCRIPTION: Shows how the ref function compiles in a production environment\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-01-09-defer-in-development.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n-- in target/compiled/models/my_model.sql\nselect * from analytics.analytics.model_a\n```\n\n----------------------------------------\n\nTITLE: Basic SQL DISTINCT Query Structure\nDESCRIPTION: Demonstrates the basic syntax for using DISTINCT in a SQL SELECT statement to remove duplicate rows from the result set.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/statements/sql-distinct.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\tdistinct\n\trow_1,\n\trow_2\nfrom my_data_source\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Indexes in SingleStore Tables\nDESCRIPTION: Shows how to define custom indexes for a SingleStore table, including specifying columns, index types (hash or btree), and shard keys. This example creates two indexes: a composite index and a hash index.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/singlestore-configs.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        materialized='table',\n        shard_key=['id'],\n        indexes=[{'columns': ['order_date', 'id']}, {'columns': ['status'], 'type': 'hash'}]\n    )\n}}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Increment Sequence Macro\nDESCRIPTION: A simple dbt macro that returns the next value from a sequence for a given model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-08-17-managing-surrogate-keys-in-dbt.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n{%- macro increment_sequence() -%}\n  \n  {{ this.name }}_seq.nextval\n\n{%- endmacro -%}\n```\n\n----------------------------------------\n\nTITLE: Specifying Model Names in dbt Schema Files\nDESCRIPTION: This snippet demonstrates how to use the 'name' field in a dbt schema.yml file to specify a model name. When used for models, dbt will look for a corresponding SQL file with the same name.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/name.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nname: events.sql\n```\n\n----------------------------------------\n\nTITLE: Configuring Compute in dbt Cloud with Extended Attributes\nDESCRIPTION: Shows how to configure compute resources in dbt Cloud environments using the extended attributes feature. This defines named compute resources with their corresponding HTTP paths.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/databricks-configs.md#2025-04-09_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\ncompute:\n  Compute1:\n    http_path: /SOME/OTHER/PATH\n  Compute2:\n    http_path: /SOME/OTHER/PATH\n```\n\n----------------------------------------\n\nTITLE: SQL CASCADE Example in dbt Transformation Challenges\nDESCRIPTION: Mentions the CASCADE operation as an example of potential complexities when writing DML directly in dbt transformations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Project/why-not-write-dml.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\ncascade\n```\n\n----------------------------------------\n\nTITLE: Implementing GitHub Pull Request Template for dbt Projects\nDESCRIPTION: This markdown code snippet contains the full GitHub pull request template used by dbt Labs. It includes sections for description, to-do items, screenshots, model validation, changes to existing models, and a checklist for quality assurance.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-11-22-dbt-labs-pr-template.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!---\n\nProvide a short summary in the Title above. Examples of good PR titles:\n\n* \"Feature: add so-and-so models\"\n\n* \"Fix: deduplicate such-and-such\"\n\n* \"Update: dbt version 0.13.0\"\n\n-->\n\n## Description & motivation\n\n<!---\n\nDescribe your changes, and why you're making them. Is this linked to an open\n\nissue, a Trello card, or another pull request? Link it here.\n\n-->\n\n## To-do before merge\n\n<!---\n\n(Optional -- remove this section if not needed)\n\nInclude any notes about things that need to happen before this PR is merged, e.g.:\n\n- [ ] Change the base branch\n\n- [ ] Update dbt Cloud jobs\n\n- [ ] Ensure PR #56 is merged\n\n-->\n\n## Screenshots:\n\n<!---\n\nInclude a screenshot of the relevant section of the updated DAG. You can access\n\nyour version of the DAG by running `dbt docs generate && dbt docs serve`.\n\n-->\n\n## Validation of models:\n\n<!---\n\nInclude any output that confirms that the models do what is expected. This might\n\nbe a link to an in-development dashboard in your BI tool, or a query that\n\ncompares an existing model with a new one.\n\n-->\n\n## Changes to existing models:\n\n<!---\n\nInclude this section if you are changing any existing models. Link any related\n\npull requests on your BI tool, or instructions for merge (e.g. whether old\n\nmodels should be dropped after merge, or whether a full-refresh run is required)\n\n-->\n```\n\n----------------------------------------\n\nTITLE: Overriding Default Materialization for a Specific Model\nDESCRIPTION: Demonstrates how to override the project-level materialization setting for a specific model by using the config() function. This example changes the bi_customers model to be materialized as a view instead of a table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/teradata-qs.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(\n    materialized='view'\n  )\n}}\n\nwith customers as (\n\n    select\n        id as customer_id\n        ...\n\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Date Function for dbt Adapter in Python\nDESCRIPTION: Defines the 'date_function' classmethod for an Adapter, which returns the database-specific function for getting the current date.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n    @classmethod\n    def date_function(cls):\n        return 'datenow()'\n```\n\n----------------------------------------\n\nTITLE: Configuring General Snapshot Settings\nDESCRIPTION: Example of general configuration options for snapshots that apply across multiple resource types. These settings control operational aspects like enabling/disabling the snapshot, tagging, and hooks.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snapshots-jinja-legacy.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    [enabled](/reference/resource-configs/enabled)=true | false,\n    [tags](/reference/resource-configs/tags)=\"<string>\" | [\"<string>\"],\n    [alias](/reference/resource-configs/alias)=\"<string>\", \n    [pre_hook](/reference/resource-configs/pre-hook-post-hook)=\"<sql-statement>\" | [\"<sql-statement>\"],\n    [post_hook](/reference/resource-configs/pre-hook-post-hook)=\"<sql-statement>\" | [\"<sql-statement>\"]\n    [persist_docs](/reference/resource-configs/persist_docs)={<dict>}\n    [grants](/reference/resource-configs/grants)={<dict>}\n) }}\n```\n\n----------------------------------------\n\nTITLE: Data Engineering Output Format\nDESCRIPTION: Markdown table showing how data engineers might structure the output with columns for different user groups, demonstrating potential misalignment with BI tool requirements.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-09-28-analyst-to-ae.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Date       | Active Users Group A | Active Users Group B | Active Users Group C |\n|------------|----------------------|----------------------|----------------------|\n| 2022-08-01 | 34                   | 60                   | 61                   |\n| 2022-08-02 | 77                   | 86                   | 37                   |\n| 2022-08-03 | 71                   | 9                    | 6                    |\n| 2022-08-04 | 63                   | 87                   | 10                   |\n```\n\n----------------------------------------\n\nTITLE: Sample Query Results from dbt Semantic Layer\nDESCRIPTION: Output from the dbt Semantic Layer CLI query showing monthly revenue data, displaying the results of the semantic model and metric definitions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-build-our-metrics/semantic-layer-8-refactor-a-rollup.md#2025-04-09_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n Success  - query completed after 1.02 seconds\\n| METRIC_TIME__MONTH   |   REVENUE |\\n|:---------------------|----------:|\\n| 2016-09-01 00:00:00  |  17032.00 |\\n| 2016-10-01 00:00:00  |  20684.00 |\\n| 2016-11-01 00:00:00  |  26338.00 |\\n| 2016-12-01 00:00:00  |  10685.00 |\n```\n\n----------------------------------------\n\nTITLE: Cloning Repository Using GitHub CLI (Shell)\nDESCRIPTION: Shell commands to clone the demo Python project repository using GitHub CLI and navigate to the project directory.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-10-19-polyglot-dbt-python-dataframes-and-sql.md#2025-04-09_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ngh repo clone dbt-labs/demo-python-blog\ncd demo-python-blog\n```\n\n----------------------------------------\n\nTITLE: Updating Dremio Adapter Pattern\nDESCRIPTION: Python code modification for the Dremio adapter to support schema names containing dots and spaces.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dremio-lakehouse.md#2025-04-09_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nPATTERN = re.compile(r\"\"\"((?:[^.\"']|\"[^\"]*\"|'[^']*')+)\"\"\")\nreturn \".\".join(PATTERN.split(identifier)[1::2])\n```\n\nLANGUAGE: python\nCODE:\n```\ndef quoted_by_component(self, identifier, componentName):\n        if componentName == ComponentName.Schema:\n            PATTERN = re.compile(r\"\"\"((?:[^.\"']|\"[^\"]*\"|'[^']*')+)\"\"\")\n            return \".\".join(PATTERN.split(identifier)[1::2])\n        else:\n            return self.quoted(identifier)\n```\n\n----------------------------------------\n\nTITLE: Interpreting dbt Core CLI Output for Model Execution\nDESCRIPTION: This snippet demonstrates how to read the dbt Core CLI output to understand model execution details, including start time, completion status, model type, and execution time.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/materializations/materializations-guide-6-examining-builds.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n20:24:51  5 of 10 START sql view model main.stg_products ......... [RUN]\n20:24:51  5 of 10 OK created sql view model main.stg_products .... [OK in 0.13s]\n```\n\n----------------------------------------\n\nTITLE: Running a Single dbt Model using Shell Command\nDESCRIPTION: This command demonstrates how to use the --select flag in dbt to run a specific model named 'customers'. The --select flag (or -s for short) allows users to specify which model(s) to run, providing granular control over the execution process.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Runs/run-one-model.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ dbt run --select customers\n```\n\n----------------------------------------\n\nTITLE: Configuring Lake Formation Data Cell Filters in Python\nDESCRIPTION: Example of configuring Lake Formation data cell filters for row-level security. This configuration enables row filtering based on specified conditions for different principals, allowing fine-grained access control within Athena tables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/athena-configs.md#2025-04-09_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nlf_grants={\n        'data_cell_filters': {\n            'enabled': True | False,\n            'filters': {\n                'filter_name': {\n                    'row_filter': '<filter_condition>',\n                    'principals': ['principal_arn1', 'principal_arn2']\n                }\n            }\n        }\n    }\n```\n\n----------------------------------------\n\nTITLE: Complete Example of Snapshot Meta Column Names Configuration\nDESCRIPTION: A comprehensive example showing how to configure custom metadata column names for a snapshot with hard delete tracking. This demonstrates renaming all standard tracking fields for a snapshot using the check strategy.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snapshot_meta_column_names.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  - name: orders_snapshot\n    relation: ref(\"orders\")\n    config:\n      unique_key: id\n      strategy: check\n      check_cols: all\n      hard_deletes: new_record\n      snapshot_meta_column_names:\n        dbt_valid_from: start_date\n        dbt_valid_to: end_date\n        dbt_scd_id: scd_id\n        dbt_updated_at: modified_date\n        dbt_is_deleted: is_deleted\n```\n\n----------------------------------------\n\nTITLE: Implementing External API Access in dbt Python Model\nDESCRIPTION: This Python model uses the configured external access integration to make an HTTP request to the Carbon Intensity API. It demonstrates how to use the external_access_integrations config and install required packages.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-06-12-putting-your-dag-on-the-internet.md#2025-04-09_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport snowflake.snowpark as snowpark\ndef model(dbt, session: snowpark.Session):\n    dbt.config(\n        materialized=\"table\",\n        external_access_integrations=[\"test_external_access_integration\"],\n        packages=[\"httpx==0.26.0\"]\n    )\n    import httpx\n    return session.create_dataframe(\n            [{\"carbon_intensity\": httpx.get(url=\"https://api.carbonintensity.org.uk/intensity\").text}]\n    )\n```\n\n----------------------------------------\n\nTITLE: Retrieving dbt Version Using Jinja Macro\nDESCRIPTION: A dbt macro that logs the currently installed version of dbt using the dbt_version variable. The macro can be executed using the dbt run-operation command to display version information.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/dbt_version.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{% macro get_version() %}\n\n  {% do log(\"The installed version of dbt is: \" ~ dbt_version, info=true) %}\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Overriding Default Materialization for a Specific Model\nDESCRIPTION: Demonstrates how to override the project-level materialization setting for a specific model by using the config() function. This example changes the bi_customers model to be materialized as a view instead of a table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/teradata-qs.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(\n    materialized='view'\n  )\n}}\n\nwith customers as (\n\n    select\n        id as customer_id\n        ...\n\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring PullRequestContribute Permission in Azure DevOps via CLI\nDESCRIPTION: CLI command to grant the PullRequestContribute permission to a service account in Azure DevOps. This permission allows posting Pull Request statuses to Azure DevOps, which is required for dbt Cloud integration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/git/setup-azure-service-user.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\naz devops security permission update --organization https://dev.azure.com/<org_name> --namespace-id 2e9eb7ed-3c0a-47d4-87c1-0ffdd275fd87 --subject <service_account>@xxxxxx.onmicrosoft.com --token repoV2/<azure_devops_project_object_id>/<azure_devops_repository_object_id> --allow-bit 16384\n```\n\n----------------------------------------\n\nTITLE: Permission Set Reference - Account Level\nDESCRIPTION: Reference to account-level permission identifier used in dbt Cloud configuration\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/manage-access/enterprise-permissions.md#2025-04-09_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\naccount-level\n```\n\n----------------------------------------\n\nTITLE: Test Name Selection in dbt\nDESCRIPTION: Shows how to select and run specific tests based on their names, including both built-in and custom tests.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/methods.md#2025-04-09_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\ndbt test --select \"test_name:unique\"            # run all instances of the `unique` test\ndbt test --select \"test_name:equality\"          # run all instances of the `dbt_utils.equality` test\ndbt test --select \"test_name:range_min_max\"     # run all instances of a custom schema test defined in the local project, `range_min_max`\n```\n\n----------------------------------------\n\nTITLE: Configuring OAuth Authorization for Snowflake in dbt\nDESCRIPTION: YAML configuration for Snowflake with OAuth authentication. This setup requires OAuth client ID, client secret, and refresh token that should be retrieved from the Snowflake OAuth configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/snowflake-setup.md#2025-04-09_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nmy-snowflake-db:\n  target: dev\n  outputs:\n    dev:\n      type: snowflake\n      account: [account id]\n      \n      # The following fields are retrieved from the Snowflake configuration\n      authenticator: oauth\n      oauth_client_id: [OAuth client id]\n      oauth_client_secret: [OAuth client secret]\n      token: [OAuth refresh token]\n```\n\n----------------------------------------\n\nTITLE: Referencing a Doc Block in YAML Schema\nDESCRIPTION: This YAML snippet demonstrates how to reference a documentation block in a schema.yml file. It uses the `doc` function to pull in the content of the 'orders' doc block for the description of an 'orders' model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/doc.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\nmodels:\n  - name: orders\n    description: \"{{ doc('orders') }}\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Schema Tests in YAML\nDESCRIPTION: YAML configuration for adding generic schema tests (unique and not_null) to a dbt model's columns.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/data-tests.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: orders\n    columns:\n      - name: order_id\n        tests:\n          - unique\n          - not_null\n```\n\n----------------------------------------\n\nTITLE: String Literal Creation in SQL with dbt\nDESCRIPTION: Macro to convert a Jinja string into a SQL string literal.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/cross-database-macros.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\nselect {{ dbt.string_literal(\"Pennsylvania\") }}\n```\n\n----------------------------------------\n\nTITLE: Creating an Incremental Model in SQL\nDESCRIPTION: SQL model with incremental materialization that selects data from an events table. When run incrementally, it only processes new records based on event_time.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/unit-tests.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        materialized='incremental'\n    )\n}}\n\nselect * from {{ ref('events') }}\n{% if is_incremental() %}\nwhere event_time > (select max(event_time) from {{ this }})\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Using CONCAT to create a full_name field in SQL\nDESCRIPTION: This example demonstrates using the CONCAT function to combine first_name and last_name columns with a space between them to create a full_name column. The query references a dbt model 'customers' and limits results to 3 rows.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/string-functions/sql-concat.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\tuser_id,\n\tfirst_name,\n\tlast_name,\n\tconcat(first_name, ' ', last_name) as full_name\nfrom {{ ref('customers') }}\nlimit 3\n```\n\n----------------------------------------\n\nTITLE: Installing Python and Dependencies on CentOS for dbt Core\nDESCRIPTION: This command installs the required dependencies for dbt Core on CentOS, including Python, gcc, and development libraries for libffi, Python, and OpenSSL.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Core/install-pip-os-prereqs.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nsudo yum install redhat-rpm-config gcc libffi-devel \\\n  python-devel openssl-devel\n```\n\n----------------------------------------\n\nTITLE: Mapping Account Classifications in dbt_quickbooks\nDESCRIPTION: This SQL snippet from the dbt_quickbooks package shows how to map account types to classifications.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-09-07-leverage-accounting-principles-when-finacial-modeling.md#2025-04-09_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nmodels/intermediate/int_quickbooks__account_classifications.sql#L23-L35\n```\n\n----------------------------------------\n\nTITLE: Listing active dbt invocations in shell\nDESCRIPTION: Shows how to use the list subcommand to display all currently active dbt invocations. This helps in monitoring and debugging long-running or hanging sessions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/invocation.md#2025-04-09_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndbt invocation list\n```\n\n----------------------------------------\n\nTITLE: Legacy Job-Based Data Health Tile iFrame for BI Tools\nDESCRIPTION: HTML iFrame for embedding legacy job-based data health tiles that use the dbt Cloud Discovery API. This iFrame requires the exposure name, job ID, and metadata-only token.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/data-tile.md#2025-04-09_snippet_4\n\nLANGUAGE: html\nCODE:\n```\n<iframe src='https://metadata.YOUR_ACCESS_URL/exposure-tile?name=<exposure_name>&jobId=<job_id>&token=<metadata_only_token>' title='Exposure Status Tile'></iframe>\n```\n\n----------------------------------------\n\nTITLE: Configuring Grants Using New dbt v1.2 Syntax\nDESCRIPTION: Example of using the new grants config feature to manage select permissions on an incremental model in dbt Core v1.2.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-07-26-configuring-grants.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n\tmaterialized = 'incremental',\n\tgrants = {'select': ['another_user']}\n) }}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Analysis Properties in dbt YAML\nDESCRIPTION: This snippet demonstrates the YAML structure for defining analysis properties in dbt. It includes required fields like name, and optional configurations such as description, documentation settings, tags, and column definitions. This file would typically be placed in the analyses/ directory of a dbt project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/analysis-properties.md#2025-04-09_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nversion: 2\n\nanalyses:\n  - name: <analysis_name> # required\n    [description](/reference/resource-properties/description): <markdown_string>\n    [docs](/reference/resource-configs/docs):\n      show: true | false\n      node_color: <color_id> # Use name (such as node_color: purple) or hex code with quotes (such as node_color: \"#cd7f32\")\n    config:\n      [tags](/reference/resource-configs/tags): <string> | [<string>]\n    columns:\n      - name: <column_name>\n        [description](/reference/resource-properties/description): <markdown_string>\n      - name: ... # declare properties of additional columns\n\n  - name: ... # declare properties of additional analyses\n```\n\n----------------------------------------\n\nTITLE: Configuring warn-error-options in profiles.yml\nDESCRIPTION: Example configuration in profiles.yml that promotes all warnings to errors except for the NoNodesForSelectionCriteria warning.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/warnings.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nconfig:\n  warn_error_options:\n    include: all\n    exclude: \n      - NoNodesForSelectionCriteria\n```\n\n----------------------------------------\n\nTITLE: Creating a Database in Amazon Athena\nDESCRIPTION: SQL command to create a new database in Amazon Athena, which will be used as the target database for dbt models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/athena-qs.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\ncreate database YOUR_DATABASE_NAME\n```\n\n----------------------------------------\n\nTITLE: DATEADD Function Syntax in Postgres\nDESCRIPTION: The syntax for adding dates in Postgres which doesn't have a dateadd function. Instead, it uses the interval operator with a similar syntax to BigQuery.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-09-11-sql-dateadd.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{{ from_date }} + (interval '{{ interval }} {{ datepart }}')\n```\n\n----------------------------------------\n\nTITLE: Implementing Grants Standardization Method for dbt Adapter in Python\nDESCRIPTION: Defines the 'standardize_grants_dict' method for a ConnectionManager, which converts database-specific grant information into a standardized dictionary format used by dbt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n    @available\n    def standardize_grants_dict(self, grants_table: agate.Table) -> dict:\n        \"\"\"\n        :param grants_table: An agate table containing the query result of\n            the SQL returned by get_show_grant_sql\n        :return: A standardized dictionary matching the `grants` config\n        :rtype: dict\n        \"\"\"\n        grants_dict: Dict[str, List[str]] = {}\n        for row in grants_table:\n            grantee = row[\"grantee\"]\n            privilege = row[\"privilege_type\"]\n            if privilege in grants_dict.keys():\n                grants_dict[privilege].append(grantee)\n            else:\n                grants_dict.update({privilege: [grantee]})\n        return grants_dict\n```\n\n----------------------------------------\n\nTITLE: Sample output of dbt environment show command\nDESCRIPTION: Example output when running the dbt environment show command, displaying both local configuration (account ID, project details, file paths) and cloud configuration (connection details, environment information).\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/dbt-environment.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n dbt env show\nLocal Configuration:\n  Active account ID              185854\n  Active project ID              271692\n  Active host name               cloud.getdbt.com\n  dbt_cloud.yml file path        /Users/cesar/.dbt/dbt_cloud.yml\n  dbt_project.yml file path      /Users/cesar/git/cloud-cli-test-project/dbt_project.yml\n  dbt Cloud CLI version          0.35.7\n  OS info                        darwin arm64\n\nCloud Configuration:\n  Account ID                     185854\n  Project ID                     271692\n  Project name                   Snowflake\n  Environment ID                 243762\n  Environment name               Development\n  Defer environment ID           [N/A]\n  dbt version                    1.6.0-latest\n  Target name                    default\n  Connection type                snowflake\n\nSnowflake Connection Details:\n  Account                        ska67070\n  Warehouse                      DBT_TESTING_ALT\n  Database                       DBT_TEST\n  Schema                         CLOUD_CLI_TESTING\n  Role                           SYSADMIN\n  User                           dbt_cloud_user\n  Client session keep alive      false \n```\n\n----------------------------------------\n\nTITLE: Basic SQL LIMIT Clause Usage\nDESCRIPTION: Demonstrates the basic syntax for using LIMIT clause in SQL queries to restrict the number of returned rows.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/clauses/sql-limit.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\tsome_rows\nfrom my_data_source\nlimit <integer>\n```\n\n----------------------------------------\n\nTITLE: Defining Jinja Macro with Proper Spacing\nDESCRIPTION: Example of a Jinja macro definition showing proper spacing within delimiters and block indentation. The macro takes an uncool_id parameter and processes it with a do_cool_thing function.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-style/4-how-we-style-our-jinja.md#2025-04-09_snippet_0\n\nLANGUAGE: jinja\nCODE:\n```\n{% macro make_cool(uncool_id) %}\n\n    do_cool_thing({{ uncool_id }})\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Disabling Telemetry Configuration\nDESCRIPTION: Example of how to disable telemetry data collection in the SDK by setting PLATFORM.anonymous to True.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-python-sdk.md#2025-04-09_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom dbtsl.env import PLATFORM\nPLATFORM.anonymous = True\n\n# ... initialize client\n```\n\n----------------------------------------\n\nTITLE: Executing Query in dbt Semantic Layer using CLI\nDESCRIPTION: This command demonstrates how to execute a query against the semantic layer using the dbt Cloud CLI. It calculates monthly revenue using the 'revenue' metric.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-build-our-metrics/semantic-layer-2-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndbt sl query --metrics revenue --group-by metric_time__month\n```\n\n----------------------------------------\n\nTITLE: Querying Measures from Metrics\nDESCRIPTION: GraphQL query to access measures through the metrics endpoint.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-graphql.md#2025-04-09_snippet_8\n\nLANGUAGE: graphql\nCODE:\n```\n{\n  metrics(environmentId: BigInt!) {\n    measures {\n      name\n      aggTimeDimension\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying dbt Source in SQL\nDESCRIPTION: This SQL snippet shows how to reference the dbt source in a downstream model. It uses the source() function with the source and table names defined in the YAML configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Project/source-has-bad-name.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect * from {{ source('jaffle_shop', 'orders') }}\n```\n\n----------------------------------------\n\nTITLE: Deferred Test Compiled SQL\nDESCRIPTION: Compiled SQL for relationship test using defer, referencing production schema for model_a.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/defer.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nselect count(*) as validation_errors\nfrom (\n    select id as id from dev_alice.model_b\n) as child\nleft join (\n    select id as id from prod.model_a\n) as parent on parent.id = child.id\nwhere child.id is not null\n  and parent.id is null\n```\n\n----------------------------------------\n\nTITLE: Coalescing Columns for Snapshot Timestamp Tracking (1.9+)\nDESCRIPTION: Creates a reliable 'updated_at' column for snapshot tracking by coalescing 'updated_at' and 'created_at' columns. Used when the source only has an 'updated_at' column filled when records are updated.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snapshots-jinja-legacy.md#2025-04-09_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot orders_snapshot %}\n\n{{\n    config(\n      schema='snapshots',\n      unique_key='id',\n\n      strategy='timestamp',\n      updated_at='updated_at_for_snapshot'\n    )\n}}\n\nselect\n    *,\n    coalesce(updated_at, created_at) as updated_at_for_snapshot\n\nfrom {{ source('jaffle_shop', 'orders') }}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Displaying NoneType Error Message in Shell\nDESCRIPTION: Example of the error message displayed when encountering a NoneType attribute error in the dbt Cloud IDE, typically related to SSH tunnel configuration issues.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Troubleshooting/nonetype-ide-error.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nNoneType object has no attribute \nenumerate_fields'\n```\n\n----------------------------------------\n\nTITLE: Creating a Customer Model in SQL\nDESCRIPTION: This SQL code creates a customer model by joining customer and order data, calculating order statistics, and producing a final view of customer information with their order history.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/snowflake-qs.md#2025-04-09_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\nwith customers as (\n\n    select\n        id as customer_id,\n        first_name,\n        last_name\n\n    from raw.jaffle_shop.customers\n\n),\n\norders as (\n\n    select\n        id as order_id,\n        user_id as customer_id,\n        order_date,\n        status\n\n    from raw.jaffle_shop.orders\n\n),\n\ncustomer_orders as (\n\n    select\n        customer_id,\n\n        min(order_date) as first_order_date,\n        max(order_date) as most_recent_order_date,\n        count(order_id) as number_of_orders\n\n    from orders\n\n    group by 1\n\n),\n\nfinal as (\n\n    select\n        customers.customer_id,\n        customers.first_name,\n        customers.last_name,\n        customer_orders.first_order_date,\n        customer_orders.most_recent_order_date,\n        coalesce(customer_orders.number_of_orders, 0) as number_of_orders\n\n    from customers\n\n    left join customer_orders using (customer_id)\n\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt_project.yml for Cloud CLI\nDESCRIPTION: Example of how to include the dbt-cloud section with project-id in the dbt_project.yml file for use with the dbt Cloud CLI.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/configure-cloud-cli.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# dbt_project.yml\nname:\nversion:\n# Your project configs...\n\ndbt-cloud: \n    project-id: PROJECT_ID\n```\n\n----------------------------------------\n\nTITLE: Azure Private Link Request Template for Databricks\nDESCRIPTION: Email template to submit an Azure Private Link request for Databricks to dbt Support. It includes fields for Databricks instance details, resource ID, environment, and Azure region.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/secure/databricks-privatelink.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\nSubject: New Azure Multi-Tenant Private Link Request\n- Type: Databricks\n- Databricks instance name:\n- Azure Databricks Workspace URL (e.g. adb-################.##.azuredatabricks.net)\n- Databricks Azure resource ID:\n- dbt Cloud multi-tenant environment: EMEA\n- Azure region: Region that hosts your Databricks workspace (like, WestEurope, NorthEurope)\n```\n\n----------------------------------------\n\nTITLE: Code Formatting Workflow with sqlfmt\nDESCRIPTION: Step-by-step process to format SQL code using sqlfmt in the dbt Cloud IDE. The formatter provides standardized SQL and Jinja formatting rules.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/dbt-cloud-ide/lint-format.md#2025-04-09_snippet_1\n\n\n\n----------------------------------------\n\nTITLE: FQN-based Selection\nDESCRIPTION: Selecting nodes based on their fully qualified names\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/methods.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --select \"fqn:some_model\"\ndbt run --select \"fqn:your_project.some_model\"\ndbt run --select \"fqn:some_package.some_other_model\"\ndbt run --select \"fqn:some_path.some_model\"\ndbt run --select \"fqn:your_project.some_path.some_model\"\n```\n\n----------------------------------------\n\nTITLE: Default Schema Name Generation in dbt\nDESCRIPTION: The default macro that dbt uses to generate schema names. It combines the target schema with any custom schema name provided. If no custom schema is provided, it uses only the target schema.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/custom-schemas.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{% macro generate_schema_name(custom_schema_name, node) -%}\n\n    {%- set default_schema = target.schema -%}\n    {%- if custom_schema_name is none -%}\n\n        {{ default_schema }}\n\n    {%- else -%}\n\n        {{ default_schema }}_{{ custom_schema_name | trim }}\n\n    {%- endif -%}\n\n{%- endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Conditional Logic Using flags in SQL (dbt)\nDESCRIPTION: This snippet demonstrates how to use the `flags` variable in a dbt SQL file to conditionally execute different SQL statements based on the FULL_REFRESH flag.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/flags.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{% if flags.FULL_REFRESH %}\ndrop table ...\n{% else %}\n-- no-op\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Basic dbt Project Configuration in YAML\nDESCRIPTION: Example of a minimal dbt_project.yml file showing project name and profile configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/connection-profiles.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# Example dbt_project.yml file\nname: 'jaffle_shop'\nprofile: 'jaffle_shop'\n...\n```\n\n----------------------------------------\n\nTITLE: Collecting Statistics with Post-hooks in dbt-teradata\nDESCRIPTION: Shows how to use dbt's post-hooks to collect statistics on a table after creation or significant modification, which is important for Teradata's query optimizer.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/teradata-configs.md#2025-04-09_snippet_25\n\nLANGUAGE: yaml\nCODE:\n```\n{{ config(\n  post_hook=[\n    \"COLLECT STATISTICS ON  {{ this }} COLUMN (column_1,  column_2  ...);\"\n    ]\n)}}\n```\n\n----------------------------------------\n\nTITLE: Querying SCD Transactions by Tier and Month\nDESCRIPTION: Command line examples for querying transaction counts grouped by sales tier and month, showing syntax for both dbt Cloud and Core users.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/dimensions.md#2025-04-09_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n# dbt Cloud users\ndbt sl query --metrics transactions --group-by metric_time__month,sales_person__tier --order-by metric_time__month,sales_person__tier\n\n# dbt Core users\nmf query --metrics transactions --group-by metric_time__month,sales_person__tier --order-by metric_time__month,sales_person__tier\n```\n\n----------------------------------------\n\nTITLE: Configuring Snapshots in Config Block (up to v1.8)\nDESCRIPTION: Jinja configuration block for snapshots that can be used directly in snapshot SQL files for versions up to 1.8. Includes configuration options like target_schema, target_database, unique_key, strategy, updated_at, check_cols, and invalidate_hard_deletes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/snapshot-configs.md#2025-04-09_snippet_3\n\nLANGUAGE: jinja\nCODE:\n```\n{{ config(\n    [target_schema](/reference/resource-configs/target_schema)=\"<string>\",\n    [target_database](/reference/resource-configs/target_database)=\"<string>\",\n    [unique_key](/reference/resource-configs/unique_key)=\"<column_name_or_expression>\",\n    [strategy](/reference/resource-configs/strategy)=\"timestamp\" | \"check\",\n    [updated_at](/reference/resource-configs/updated_at)=\"<column_name>\",\n    [check_cols](/reference/resource-configs/check_cols)=[\"<column_name>\"] | \"all\"\n    [invalidate_hard_deletes](/reference/resource-configs/invalidate_hard_deletes) : true | false\n) }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Time Spine Models in YAML\nDESCRIPTION: YAML configuration for hourly and daily time spine models in dbt. Defines the standard granularity column and custom granularities for each model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metricflow-time-spine.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels: \n  - name: time_spine_hourly \n    description: my favorite time spine\n    time_spine:\n      standard_granularity_column: date_hour \n      custom_granularities:\n        - name: fiscal_year\n          column_name: fiscal_year_column\n    columns:\n      - name: date_hour\n        granularity: hour \n\n  - name: time_spine_daily\n    time_spine:\n      standard_granularity_column: date_day \n    columns:\n      - name: date_day\n        granularity: day\n```\n\n----------------------------------------\n\nTITLE: Configuring Materializations in properties.yml Files\nDESCRIPTION: This example shows how to configure materializations using a properties.yml file. The configuration sets the 'events' model to use the table materialization type.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/materializations.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: events\n    config:\n      materialized: table\n```\n\n----------------------------------------\n\nTITLE: Configuring BigQuery Adapter Test Cases\nDESCRIPTION: Example of modifying test configuration for BigQuery-specific requirements\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\nfrom dbt.tests.adapter.basic.test_base import BaseSimpleMaterializations\n\nclass TestSimpleMaterializationsBigQuery(BaseSimpleMaterializations):\n    @pytest.fixture(scope=\"class\")\n    def test_config(self):\n        # effect: add '--full-refresh' flag in requisite 'dbt run' step\n        return {\"require_full_refresh\": True}\n```\n\n----------------------------------------\n\nTITLE: Defining Container Model Schema in YAML\nDESCRIPTION: This YAML file defines the schema for the src_container model, including column names, descriptions, and tests.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-10-24-demystifying-event-streams.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n---\nversion: 2\n\nmodels:\n  - name: src_container\n    description: pass the OMG model variable to generate the data\n    columns:\n      - name: templateName\n        description: STRING Specifies the templateName\n        tests:\n          - not_null\n      - name: complete\n        description: STRING Specifies the complete\n      - name: aggregateID\n        description: STRING Specifies the aggregateID\n      - name: recipientID\n        description: STRING Specifies the recipientID\n      - name: templateID\n        description: STRING Specifies the templateID\n      - name: templateType\n        description: STRING Specifies the templateType\n      - name: state\n        description: STRING Specifies the state\n      - name: id\n        description: STRING Specifies the id\n      - name: orgID\n```\n\n----------------------------------------\n\nTITLE: Creating Temporary Table in SQL Server\nDESCRIPTION: This SQL snippet defines a temporary table by selecting data from a raw table and inserting it. It demonstrates the first step in a stored procedure approach.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-07-19-migrating-from-stored-procs.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nIF OBJECT_ID('tempdb..#temp_orders') IS NOT NULL DROP TABLE #temp_orders\n   SELECT  messageid\n           ,orderid\n           ,sk_id\n           ,client\n   FROM    some_raw_table\n   WHERE   . . .\n   INTO   #temp_orders\n```\n\n----------------------------------------\n\nTITLE: Example DBT Resource File Names in YAML\nDESCRIPTION: Common naming patterns for DBT resource files including schema.yml and model-specific files. Shows different approaches like using directory names or model names.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Project/resource-yml-name.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nschema.yml\n```\n\n----------------------------------------\n\nTITLE: Using DATEDIFF in Google BigQuery\nDESCRIPTION: Shows the syntax for using the DATEDIFF function in Google BigQuery. The function is called DATETIME_DIFF and takes three arguments in a different order: start date/time, end date/time, and date part.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-07-05-datediff-sql-love-letter.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\ndatetime_diff(<start date/time>, <end date/time>, <date part>)\n```\n\n----------------------------------------\n\nTITLE: Running dbt Tests with Command Line\nDESCRIPTION: Shell command to run tests on a specific model. The -s flag selects only the tests associated with 'my_model' for execution.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/artifacts/run-results-json.md#2025-04-09_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ndbt test -s my_model\n```\n\n----------------------------------------\n\nTITLE: Creating External Catalog in Starrocks SQL\nDESCRIPTION: This SQL snippet demonstrates how to create an external catalog in Starrocks, specifically for a Hive metastore. It sets up the connection to the Hive metastore for data integration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/starrocks-configs.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nCREATE EXTERNAL CATALOG `hive_catalog`\nPROPERTIES (\n    \"hive.metastore.uris\"  =  \"thrift://127.0.0.1:8087\",\n    \"type\"=\"hive\"\n);\n```\n\n----------------------------------------\n\nTITLE: Verifying dbt Cloud Webhook and Triggering Tableau Workbook Refresh\nDESCRIPTION: This Python code authenticates an incoming webhook from dbt Cloud, verifies the run was successful, then connects to the Tableau API to trigger a workbook refresh. It handles webhook verification, Tableau authentication, workbook ID retrieval, and finally sends the refresh request.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/zapier-refresh-tableau-workbook.md#2025-04-09_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nimport hashlib\nimport json\nimport hmac\n\n# Access secret credentials\nsecret_store = StoreClient('YOUR_STORAGE_SECRET_HERE')\nhook_secret = secret_store.get('DBT_WEBHOOK_KEY')\nserver_url = secret_store.get('TABLEAU_SITE_URL')\nserver_name = secret_store.get('TABLEAU_SITE_NAME')\npat_name = secret_store.get('TABLEAU_API_TOKEN_NAME')\npat_secret = secret_store.get('TABLEAU_API_TOKEN_SECRET')\n\n#Enter the name of the workbook to refresh\nworkbook_name = \"YOUR_WORKBOOK_NAME\"\napi_version = \"ENTER_COMPATIBLE_VERSION\"\n\n#Validate authenticity of webhook coming from dbt Cloud\nauth_header = input_data['auth_header']\nraw_body = input_data['raw_body']\n\nsignature = hmac.new(hook_secret.encode('utf-8'), raw_body.encode('utf-8'), hashlib.sha256).hexdigest()\n\nif signature != auth_header:\nraise Exception(\"Calculated signature doesn't match contents of the Authorization header. This webhook may not have been sent from dbt Cloud.\")\n\nfull_body = json.loads(raw_body)\nhook_data = full_body['data'] \n\nif hook_data['runStatus'] == \"Success\":\n\n#Authenticate with Tableau Server to get an authentication token\nauth_url = f\"{server_url}/api/{api_version}/auth/signin\"\nauth_data = {\n    \"credentials\": {\n        \"personalAccessTokenName\": pat_name,\n        \"personalAccessTokenSecret\": pat_secret,\n        \"site\": {\n            \"contentUrl\": server_name\n        }\n    }\n}\nauth_headers = {\n    \"Accept\": \"application/json\",\n    \"Content-Type\": \"application/json\"\n}\nauth_response = requests.post(auth_url, data=json.dumps(auth_data), headers=auth_headers)\n\n#Extract token to use for subsequent calls\nauth_token = auth_response.json()[\"credentials\"][\"token\"]\nsite_id = auth_response.json()[\"credentials\"][\"site\"][\"id\"]\n\n#Extract the workbook ID\nworkbooks_url = f\"{server_url}/api/{api_version}/sites/{site_id}/workbooks\"\nworkbooks_headers = {\n    \"Accept\": \"application/json\",\n    \"Content-Type\": \"application/json\",\n    \"X-Tableau-Auth\": auth_token\n}\nworkbooks_params = {\n    \"filter\": f\"name:eq:{workbook_name}\"\n}\nworkbooks_response = requests.get(workbooks_url, headers=workbooks_headers, params=workbooks_params)\n\n#Assign workbook ID\nworkbooks_data = workbooks_response.json()\nworkbook_id = workbooks_data[\"workbooks\"][\"workbook\"][0][\"id\"]\n\n# Refresh the workbook\nrefresh_url = f\"{server_url}/api/{api_version}/sites/{site_id}/workbooks/{workbook_id}/refresh\"\nrefresh_data = {}\nrefresh_headers = {\n    \"Accept\": \"application/json\",\n    \"Content-Type\": \"application/json\",\n    \"X-Tableau-Auth\": auth_token\n}\n\nrefresh_trigger = requests.post(refresh_url, data=json.dumps(refresh_data), headers=refresh_headers)\nreturn {\"message\": \"Workbook refresh has been queued\"}\n```\n\n----------------------------------------\n\nTITLE: Implementing Querystring for Tabbed Content in SQL\nDESCRIPTION: Demonstrates how to use the queryString prop in the Tabs component to create unique hyperlinks for each tab. This allows for sharing links to specific tabs within the documentation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/contributing/adding-page-components.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n<Tabs queryString=\"current-os\">\n  <TabItem value=\"android\" label=\"Android\">\n    Android\n  </TabItem>\n  <TabItem value=\"ios\" label=\"iOS\">\n    iOS\n  </TabItem>\n</Tabs>\n```\n\n----------------------------------------\n\nTITLE: Example SQL Output with Dynamic Comment\nDESCRIPTION: SQL query showing how a dynamic comment with interpolated values appears in the executed query.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/query-comment.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\n/* run by drew in dbt */\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Parallel Batch Execution in dbt_project.yml\nDESCRIPTION: Example of configuring parallel batch execution at the project level in the dbt_project.yml file. Setting concurrent_batches to true enables parallel processing of microbatches for all models in the project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/parallel-batch-execution.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  +concurrent_batches: true # value set to true to run batches in parallel\n```\n\n----------------------------------------\n\nTITLE: Configuring View Materialization in SQL\nDESCRIPTION: Demonstrates how to configure a SQL model to materialize as a view using a Jinja config block. The materialized argument is set to 'view' to specify the desired outcome.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/materializations/materializations-guide-3-configuring-materializations.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n    {{\n        config(\n            materialized='view'\n        )\n    }}\n\n    select ...\n```\n\n----------------------------------------\n\nTITLE: Implementing Query Cancellation Method for dbt Adapter in Python\nDESCRIPTION: Defines the 'cancel' method for a ConnectionManager, which attempts to cancel an ongoing query for databases that support cancellation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n    def cancel(self, connection):\n        tid = connection.handle.transaction_id()\n        sql = 'select cancel_transaction({})'.format(tid)\n        logger.debug(\"Cancelling query '{}' ({})\".format(connection_name, pid))\n        _, cursor = self.add_query(sql, 'master')\n        res = cursor.fetchone()\n        logger.debug(\"Canceled query '{}': {}\".format(connection_name, res))\n```\n\n----------------------------------------\n\nTITLE: Referencing DBT cautious Mode\nDESCRIPTION: Code reference showing the usage of the cautious mode parameter in DBT, which ensures all dependencies are met before running tests.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_indirect-selection-definitions.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\ncautious\n```\n\n----------------------------------------\n\nTITLE: Help command output for dbt invocation in bash\nDESCRIPTION: Displays the complete help output for the dbt invocation command, showing all available subcommands, flags, and usage information.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/invocation.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndbt invocation help\nManage invocations\n\nUsage:\n  dbt invocation [command]\n\nAvailable Commands:\n  list        List active invocations\n\nFlags:\n  -h, --help   help for invocation\n\nGlobal Flags:\n      --log-format LogFormat   The log format, either json or plain. (default plain)\n      --log-level LogLevel     The log level, one of debug, info, warning, error or fatal. (default info)\n      --no-color               Disables colorization of the output.\n  -q, --quiet                  Suppress all non-error logging to stdout.\n\nUse \"dbt invocation [command] --help\" for more information about a command.\n```\n\n----------------------------------------\n\nTITLE: Redshift Table Creation and Replacement in dbt\nDESCRIPTION: Shows how dbt transforms a simple SELECT statement into Redshift-specific SQL for creating and replacing tables. Uses transactions to ensure atomic operations with backup table creation and cleanup.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Models/sql-dialect.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n-- you can't create or replace on redshift, so use a transaction to do this in an atomic way\n\nbegin;\n\ncreate table \"dbt_alice\".\"test_model__dbt_tmp\" as (\n    select 1 as my_column\n);\n\nalter table \"dbt_alice\".\"test_model\" rename to \"test_model__dbt_backup\";\n\nalter table \"dbt_alice\".\"test_model__dbt_tmp\" rename to \"test_model\"\n\ncommit;\n\nbegin;\n\ndrop table if exists \"dbt_alice\".\"test_model__dbt_backup\" cascade;\n\ncommit;\n```\n\n----------------------------------------\n\nTITLE: Performing Temporal Range Join on Component Hierarchies\nDESCRIPTION: This SQL query performs a temporal range join between the top assembly and all of its descendants, splitting time-varying component changes into distinct time ranges. It uses a custom dbt macro (trange_join) to handle the temporal relationships and create surrogate keys for each unique combination of components and time periods.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-02-modeling-ragged-time-varying-hierarchies.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\n-- Start with all of the assemblies at the top (hierarchy depth = 0)\nwith l0_assemblies as (\n    select\n        top_assembly_id,\n        component_id,\n        -- Prep fields required for temporal range join\n        {{ dbt_utils.surrogate_key(['component_id', 'valid_from_at']) }} as dbt_scd_id,\n        valid_from_at as dbt_valid_from,\n        valid_to_at as dbt_valid_to\n    from\n        component_traversal\n    where\n        component_hierarchy_depth = 0\n),\n\ncomponents as (\n    select\n        top_assembly_id,\n        component_hierarchy_depth,\n        component_trace,\n        assembly_id,\n        component_id,\n        installed_at,\n        removed_at,\n        -- Prep fields required for temporal range join\n        {{ dbt_utils.surrogate_key(['component_trace', 'valid_from_at'])}} as dbt_scd_id,\n        valid_from_at as dbt_valid_from,\n        valid_to_at as dbt_valid_to\n    from\n        component_traversal\n),\n\n-- Perform temporal range join\n{{\n    trange_join(\n      left_model='l0_assemblies',\n      left_fields=[\n        'top_assembly_id',\n      ],\n      left_primary_key='top_assembly_id',\n      right_models={\n        'components': {\n          'fields': [\n              'component_hierarchy_depth',\n              'component_trace',\n              'assembly_id',\n              'component_id',\n              'installed_at',\n              'removed_at',\n          ],\n          'left_on': 'component_id',\n          'right_on': 'top_assembly_id',\n        }\n      }\n    )\n}}\n\nselect\n    surrogate_key,\n    top_assembly_id,\n    component_hierarchy_depth,\n    component_trace,\n    assembly_id,\n    component_id,\n    installed_at,\n    removed_at,\n    valid_from_at,\n    valid_to_at\nfrom\n    trange_final\norder by\n    top_assembly_id,\n    valid_from_at,\n    component_hierarchy_depth\n```\n\n----------------------------------------\n\nTITLE: PostgreSQL dbt Model SQL\nDESCRIPTION: This SQL snippet defines a simple dbt model for PostgreSQL, creating a table with an id, customer_name, and first_transaction_date.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/constraints.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(\n    materialized = \"table\"\n  )\n}}\n\nselect \n  1 as id, \n  'My Favorite Customer' as customer_name, \n  cast('2019-01-01' as date) as first_transaction_date\n```\n\n----------------------------------------\n\nTITLE: Granting Existing Raw Database Access to Transformer Role in Snowflake\nDESCRIPTION: SQL commands to grant the transformer role read-only access to all existing objects in the raw database, ensuring complete access to data for transformation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/database-permissions/snowflake-permissions.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\ngrant usage on all schemas in database raw to role transformer;\ngrant select on all tables in database raw to role transformer;\ngrant select on all views in database raw to role transformer;\n```\n\n----------------------------------------\n\nTITLE: Implementing Check Strategy for Snapshot Change Detection\nDESCRIPTION: Example of a snapshot using the check strategy to detect changes. This approach is useful for tables without a reliable updated_at column, checking specified columns for changes between snapshots.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snapshots-jinja-legacy.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot orders_snapshot_check %}\n\n    {{\n        config(\n          strategy='check',\n          unique_key='id',\n          check_cols=['status', 'is_cancelled'],\n        )\n    }}\n\n    select * from {{ source('jaffle_shop', 'orders') }}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Defining docs-paths in dbt_project.yml\nDESCRIPTION: This snippet shows the basic structure for specifying custom docs-paths in the dbt_project.yml file. It allows users to define a list of directory paths where dbt should look for documentation blocks.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/docs-paths.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndocs-paths: [directorypath]\n```\n\n----------------------------------------\n\nTITLE: Querying DBT Snapshot Results\nDESCRIPTION: Shows how to reference and query snapshot results in downstream models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/snapshots.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nselect * from {{ ref('orders_snapshot') }}\n```\n\n----------------------------------------\n\nTITLE: Package-based Selection\nDESCRIPTION: Selecting models from specific packages\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/methods.md#2025-04-09_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n# These three selectors are equivalent\ndbt run --select \"package:snowplow\"\ndbt run --select \"snowplow\"\ndbt run --select \"snowplow.*\"\n```\n\n----------------------------------------\n\nTITLE: Incremental Model with Iceberg in dbt-glue\nDESCRIPTION: SQL model configuration for an incremental table using Iceberg format in dbt-glue. This includes merge strategy, unique key definition, partitioning by status column, and Iceberg-specific table properties.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/glue-setup.md#2025-04-09_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized='incremental',\n    incremental_strategy='merge',\n    unique_key=['user_id'],\n    file_format='iceberg',\n    iceberg_expire_snapshots='False', \n    partition_by=['status']\n    table_properties={'write.target-file-size-bytes': '268435456'}\n) }}\n\nwith new_events as (\n\n    select * from {{ ref('events') }}\n\n    {% if is_incremental() %}\n    where date_day >= date_add(current_date, -1)\n    {% endif %}\n\n)\n\nselect\n    user_id,\n    max(date_day) as last_seen\n\nfrom events\ngroup by 1\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Dispatch Macro in SQL\nDESCRIPTION: Shows the basic syntax for creating a dispatched macro that returns the appropriate implementation for the current adapter. This pattern allows defining adapter-specific behavior while maintaining a single entry point.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/dispatch.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{% macro my_macro(arg1, arg2) -%}\n  {{ return(adapter.dispatch('my_macro')(arg1, arg2)) }}\n{%- endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Vertica Connection Profile in YAML\nDESCRIPTION: YAML configuration for setting up a dbt profile to connect to Vertica database. Includes all possible connection parameters like host, port, authentication credentials, schema settings, and connection behavior options.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/vertica-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nyour-profile:\n  outputs:\n    dev:\n      type: vertica # Don't change this!\n      host: [hostname]\n      port: [port] # or your custom port (optional)\n      username: [your username]\n      password: [your password]\n      database: [database name]\n      oauth_access_token: [access token]\n      schema: [dbt schema]\n      connection_load_balance: True\n      backup_server_node: [list of backup hostnames or IPs]\n      retries: [1 or more]\n      autocommit: False\n      \n      threads: [1 or more]\n  target: dev\n```\n\n----------------------------------------\n\nTITLE: Configuring Grants for Snapshots in YAML\nDESCRIPTION: Illustrates how to set grants for a snapshot in a YAML schema file. This configuration grants select privileges to 'reporter' and 'bi' roles.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/grants.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  - name: snapshot_name\n    config:  \n      grants:\n        select: ['reporter', 'bi']\n```\n\n----------------------------------------\n\nTITLE: Customer Retention Metric with Offset Window\nDESCRIPTION: Example of calculating customer retention using derived metrics with a 1-month offset window to compare current and previous month values.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/derived-metrics.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n- name: customer_retention\n  description: Percentage of customers that are active now and those active 1 month ago\n  label: customer_retention\n  type_params:\n    expr: (active_customers/ active_customers_prev_month)\n    metrics:\n      - name: active_customers\n        alias: current_active_customers\n      - name: active_customers\n        offset_window: 1 month\n        alias: active_customers_prev_month\n```\n\n----------------------------------------\n\nTITLE: Creating Holdout Dataset for Prediction in Python\nDESCRIPTION: Prepares a holdout dataset using 2020 Formula 1 race data for model validation. Uses dbt Python model with pandas to filter data for prediction testing.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndef model(dbt, session):\n    # dbt configuration\n    dbt.config(packages=[\"pandas\"], tags=\"predict\")\n\n    # get upstream data\n    encoding = dbt.ref(\"covariate_encoding\").to_pandas()\n    \n    # variable for year instead of hardcoding it \n    year=2020\n\n    # filter the data based on the specified year\n    hold_out_dataset =  encoding.loc[encoding['RACE_YEAR'] == year]\n    \n    return hold_out_dataset\n```\n\n----------------------------------------\n\nTITLE: CI-Specific Alias Generation\nDESCRIPTION: Macro for generating table aliases in CI environments with PR and schema information included in the name.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/customize-schema-alias.md#2025-04-09_snippet_5\n\nLANGUAGE: jinja\nCODE:\n```\n{% macro generate_alias_name(custom_alias_name=none, node=none) -%}\n\n    {# If the CI Job does not exist in its own environment, use the target.name variable inside the job instead #}\n    {# {%- if target.name == 'CI' -%} #}   \n    {%- if  env_var('DBT_ENV_TYPE','DEV') == 'CI' -%}\n\n        {%- if custom_alias_name -%}\n\n            {{ target.schema }}__{{ node.config.schema }}__{{ custom_alias_name | trim }}\n\n        {%- elif node.version -%}\n\n            {{ target.schema }}__{{ node.config.schema }}__{{ node.name ~ \"_v\" ~ (node.version | replace(\".\", \"_\")) }}\n\n        {%- else -%}\n\n            {{ target.schema }}__{{ node.config.schema }}__{{ node.name }}\n\n        {%- endif -%}\n    \n    {%- else -%}\n\n        {%- if custom_alias_name -%}\n\n            {{ custom_alias_name | trim }}\n\n        {%- elif node.version -%}\n\n            {{ return(node.name ~ \"_v\" ~ (node.version | replace(\".\", \"_\"))) }}\n\n        {%- else -%}\n\n            {{ node.name }}\n\n        {%- endif -%}\n\n    {%- endif -%}\n\n{%- endmacro %}\n```\n\n----------------------------------------\n\nTITLE: YAML/Markdown/JSON Formatting with Prettier\nDESCRIPTION: Instructions for formatting YAML, Markdown, and JSON files using Prettier in the dbt Cloud IDE. Includes information about configuration file options and precedence rules.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/dbt-cloud-ide/lint-format.md#2025-04-09_snippet_2\n\n\n\n----------------------------------------\n\nTITLE: Deactivating Python Virtual Environment (Shell)\nDESCRIPTION: Shell command to deactivate the Python virtual environment after completing the tutorial.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-10-19-polyglot-dbt-python-dataframes-and-sql.md#2025-04-09_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\ndeactivate\n```\n\n----------------------------------------\n\nTITLE: Implementing Current Records Only Filter with Jinja\nDESCRIPTION: SQL snippet with Jinja conditional logic that filters for only current records when the current_records_only variable is set to true, providing an efficient way to generate present-day-only reports.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-05-24-joining-snapshot-complexity.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\n{% if var(\"current_records_only\") %}\n\nwhere valid_to = cast('{{ var(\"future_proof_date\") }}' as timestamp)\n\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Setting Profile in dbt Project Configuration\nDESCRIPTION: Configuration snippet showing how to specify the profile name in dbt_project.yml file. The profile setting determines which connection profile dbt should use to connect to your data warehouse.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/profile.md#2025-04-09_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nprofile: string\n```\n\n----------------------------------------\n\nTITLE: Querying Distinct Payment Methods with SQL in dbt\nDESCRIPTION: SQL query to select distinct payment methods from a raw payments table, referenced using dbt's ref function.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/using-jinja.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nselect distinct\npayment_method\nfrom {{ ref('raw_payments') }}\norder by 1\n```\n\n----------------------------------------\n\nTITLE: Implementing Warehouse-Specific Code Snippets in SQL\nDESCRIPTION: Demonstrates how to use the WHCode component to provide code snippets for different warehouses. This allows for warehouse-specific examples to be displayed in the documentation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/contributing/adding-page-components.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n<WHCode>\n\n<div warehouse=\"warehouse#1\">\n\n```\nselect * from `dbt-tutorial.jaffle_shop.customers`\n```\n\n</div>\n\n<div warehouse=\"warehouse#2\">\n\n```\nselect * from default.jaffle_shop_customers\n```\n\n</div>\n\n</WHCode>\n```\n\n----------------------------------------\n\nTITLE: Using zip_strict Context Method with Lists in Jinja\nDESCRIPTION: Example of using the zip_strict context method to combine elements from two lists into tuples. This works similarly to the regular zip method when all inputs are valid iterables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/zip.md#2025-04-09_snippet_2\n\nLANGUAGE: jinja\nCODE:\n```\n{% set my_list_a = [1, 2] %}\n{% set my_list_b = ['alice', 'bob'] %}\n{% set my_zip = zip_strict(my_list_a, my_list_b) | list %}\n{% do log(my_zip) %}  {# [(1, 'alice'), (2, 'bob')] #}\n```\n\n----------------------------------------\n\nTITLE: Configuring batch_size in properties YAML file\nDESCRIPTION: Sets the batch_size configuration to 'day' for the user_sessions model in a properties.yml file. This approach uses the model properties file to configure how large batches are when running a microbatch incremental model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/batch_size.md#2025-04-09_snippet_1\n\nLANGUAGE: yml\nCODE:\n```\nmodels:\n  - name: user_sessions\n    config:\n      batch_size: day\n```\n\n----------------------------------------\n\nTITLE: Configuring Incremental Model in dbt\nDESCRIPTION: Basic configuration block for an incremental model without unique key to capture historical data changes\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-07-12-change-data-capture.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n      materialized='incremental'\n    )\n}}\n```\n\n----------------------------------------\n\nTITLE: Package Configuration in Python Model\nDESCRIPTION: Examples of configuring required Python packages in both Python and YAML formats.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/python-models.md#2025-04-09_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef model(dbt, session):\n    dbt.config(\n        packages = [\"numpy==1.23.1\", \"scikit-learn\"]\n    )\n```\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: my_python_model\n    config:\n      packages:\n        - \"numpy==1.23.1\"\n        - scikit-learn\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Data Error Detection with SQL Date Casting\nDESCRIPTION: This SQL snippet attempts to cast an invalid date string ('2025-01-32') to a date type. This demonstrates a data error that parsers and compilers cannot detect, but requires executor-level validation to identify that January 32nd is not a valid date.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2025-01-23-levels-of-sql-comprehension.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nselect cast('2025-01-32' as date) as tomorrow\n```\n\n----------------------------------------\n\nTITLE: Semantic Model Selection\nDESCRIPTION: Selecting semantic models and their upstream resources\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/methods.md#2025-04-09_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ndbt list --select \"semantic_model:*\"        # list all semantic models \ndbt list --select \"+semantic_model:orders\"  # list your semantic model named \"orders\" and all upstream resources\n```\n\n----------------------------------------\n\nTITLE: Using ref Function in dbt SQL Model\nDESCRIPTION: This SQL snippet demonstrates how to use the ref function in a dbt model to reference another model (stg_orders). When this code is executed, dbt automatically infers that stg_orders must be built before the customer_orders model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Models/create-dependencies.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    customer_id,\n    min(order_date) as first_order_date,\n    max(order_date) as most_recent_order_date,\n    count(order_id) as number_of_orders\nfrom {{ ref('stg_orders') }}\ngroup by 1\n\n```\n\n----------------------------------------\n\nTITLE: Building Feature Tables Using dbt SQL with Window Aggregations\nDESCRIPTION: This SQL query creates a feature table by applying the rolling_agg macro to compute various aggregations of customer transactions over different time intervals (1, 7, and 30 days). It demonstrates how to generate multiple time-based features from transaction data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-10-05-snowflake-feature-store.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect\n   tx_datetime,\n   customer_id,\n   tx_amount,\n   {{ rolling_agg(\"TX_AMOUNT\", \"CUSTOMER_ID\", \"TX_DATETIME\", \"1 days\", \"sum\") }}\n   as tx_amount_1d,\n   {{ rolling_agg(\"TX_AMOUNT\", \"CUSTOMER_ID\", \"TX_DATETIME\", \"7 days\", \"sum\") }}\n   as tx_amount_7d,\n   {{ rolling_agg(\"TX_AMOUNT\", \"CUSTOMER_ID\", \"TX_DATETIME\", \"30 days\", \"sum\") }}\n   as tx_amount_30d,\n   {{ rolling_agg(\"TX_AMOUNT\", \"CUSTOMER_ID\", \"TX_DATETIME\", \"1 days\", \"avg\") }}\n   as tx_amount_avg_1d,\n   {{ rolling_agg(\"TX_AMOUNT\", \"CUSTOMER_ID\", \"TX_DATETIME\", \"7 days\", \"avg\") }}\n   as tx_amount_avg_7d,\n   {{ rolling_agg(\"TX_AMOUNT\", \"CUSTOMER_ID\", \"TX_DATETIME\", \"30 days\", \"avg\") }}\n   as tx_amount_avg_30d,\n   {{ rolling_agg(\"*\", \"CUSTOMER_ID\", \"TX_DATETIME\", \"1 days\", \"count\") }}\n   as tx_cnt_1d,\n   {{ rolling_agg(\"*\", \"CUSTOMER_ID\", \"TX_DATETIME\", \"7 days\", \"count\") }}\n   as tx_cnt_7d,\n   {{ rolling_agg(\"*\", \"CUSTOMER_ID\", \"TX_DATETIME\", \"30 days\", \"count\") }}\n   as tx_cnt_30d\nfrom {{ ref(\"stg_transactions\") }}\n\n```\n\n----------------------------------------\n\nTITLE: Expected Redshift DDL for Constraint Enforcement\nDESCRIPTION: This SQL snippet shows the expected DDL generated by dbt to enforce the defined constraints in Redshift, including the creation of the table with constraints and the insert statement.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/constraints.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\ncreate table \"database_name\".\"schema_name\".\"constraints_example__dbt_tmp\"\n    \n(\n    id integer not null,\n    customer_name varchar,\n    first_transaction_date date,\n    primary key(id)\n)    \n;\n\ninsert into \"database_name\".\"schema_name\".\"constraints_example__dbt_tmp\"\n(   \nselect\n    1 as id,\n    'My Favorite Customer' as customer_name,\n    cast('2019-01-01' as date) as first_transaction_date\n); \n```\n\n----------------------------------------\n\nTITLE: BigQuery dbt Model SQL\nDESCRIPTION: This SQL snippet defines a simple dbt model for BigQuery, creating a table with an id, customer_name, and first_transaction_date.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/constraints.md#2025-04-09_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(\n    materialized = \"table\"\n  )\n}}\n\nselect \n  1 as id, \n  'My Favorite Customer' as customer_name, \n  cast('2019-01-01' as date) as first_transaction_date\n```\n\n----------------------------------------\n\nTITLE: Configuring Service Account JSON Authentication\nDESCRIPTION: Configuration for connecting to BigQuery using service account JSON credentials directly in the profile.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/bigquery-setup.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmy-bigquery-db:\n  target: dev\n  outputs:\n    dev:\n      type: bigquery\n      method: service-account-json\n      project: GCP_PROJECT_ID\n      dataset: DBT_DATASET_NAME\n      threads: 4\n      keyfile_json:\n        type: xxx\n        project_id: xxx\n        private_key_id: xxx\n        private_key: xxx\n        client_email: xxx\n        client_id: xxx\n        auth_uri: xxx\n        token_uri: xxx\n        auth_provider_x509_cert_url: xxx\n        client_x509_cert_url: xxx\n```\n\n----------------------------------------\n\nTITLE: Accessing Sources in dbt Graph\nDESCRIPTION: Example demonstrating how to access and union all Snowplow sources that begin with 'event_' using the graph context variable.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/graph.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{% set sources = [] -%}\n{% for node in graph.sources.values() -%}\n  {%- if node.name.startswith('event_') and node.source_name == 'snowplow' -%}\n    {%- do sources.append(source(node.source_name, node.name)) -%}\n  {%- endif -%}\n{%- endfor %}\n\nselect * from (\n  {%- for source in sources %}\n    select * from {{ source }} {% if not loop.last %} union all {% endif %}\n  {% endfor %}\n)\n```\n\n----------------------------------------\n\nTITLE: Using Environment Variable for Account in Extended Attributes (Jinja)\nDESCRIPTION: This example shows how to use an environment variable to set the account value in Extended Attributes. It uses the Jinja templating syntax to reference the 'DBT_ACCOUNT' environment variable.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_sl-env-vars.md#2025-04-09_snippet_1\n\nLANGUAGE: jinja\nCODE:\n```\n{{env_var('DBT_ACCOUNT')}}\n```\n\n----------------------------------------\n\nTITLE: Defining Simple Metric in YAML for dbt Semantic Layer\nDESCRIPTION: This snippet illustrates the configuration of a simple metric named 'cancellations'. It specifies the measure it's based on, includes options for filling null values, and applies a filter using dimensions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metrics-overview.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  - name: cancellations\n    description: The number of cancellations\n    type: simple\n    label: Cancellations\n    type_params:\n      measure:\n        name: cancellations_usd  # Specify the measure you are creating a proxy for.\n        fill_nulls_with: 0\n        join_to_timespine: true\n    filter: |\n      {{ Dimension('order__value')}} > 100 and {{Dimension('user__acquisition')}} is not null\n```\n\n----------------------------------------\n\nTITLE: Listing dbt Models in Activity Based Interest Folder\nDESCRIPTION: This bash command lists all dbt models in the activity_based_interest directory, showing the folder structure.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-04-generating-dynamic-docs.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmodels/core/activity_based_interest\n events\n    activity_based_interest_activated.sql\n    activity_based_interest_deactivated.sql\n    activity_based_interest_updated.sql\n    downgrade_interest_level_for_user.sql\n    set_inactive_interest_rate_after_july_1st_in_bec_for_user.sql\n    set_inactive_interest_rate_from_july_1st_in_bec_for_user.sql\n    set_interest_levels_from_june_1st_in_bec_for_user.sql\n models\n     f_activity_based_interest.sql\n```\n\n----------------------------------------\n\nTITLE: Running dbt Docker Container with Project Mount\nDESCRIPTION: This command runs a dbt Docker container, mounting the local project directory and profiles.yml file. It uses host networking and executes the 'ls' command in the container.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/docker-install.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run \\\n--network=host \\\n--mount type=bind,source=path/to/project,target=/usr/app \\\n--mount type=bind,source=path/to/profiles.yml,target=/root/.dbt/profiles.yml \\\n<dbt_image_name> \\\nls\n```\n\n----------------------------------------\n\nTITLE: Iceberg Snapshot Configuration for dbt v1.9+\nDESCRIPTION: SQL snapshot configuration for dbt version 1.9 and newer using Iceberg format. Updated to use the schema parameter instead of target_schema to align with newer dbt versions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/glue-setup.md#2025-04-09_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot demosnapshot %}\n\n{{\n    config(\n        strategy='timestamp',\n        schema='jaffle_db',\n        updated_at='dt',\n        file_format='iceberg'\n) }}\n\nselect * from {{ ref('customers') }}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Executing dbt Run with Result Status Selection\nDESCRIPTION: Example showing how to run dbt with a result status selector and state flag for deferred execution. This allows selecting nodes based on their previous execution status.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/configure-state.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --select \"result:<status>\" --defer --state path/to/prod/artifacts\n```\n\n----------------------------------------\n\nTITLE: Defining a SQL Model in dbt\nDESCRIPTION: Example of a SQL model in dbt with a select statement returning customer data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/govern/model-contracts.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n-- lots of SQL\n\nfinal as (\n\n    select\n        customer_id,\n        customer_name,\n        -- ... many more ...\n    from ...\n\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Creating Results Intermediate Model in SQL\nDESCRIPTION: Combines data from multiple tables (races, drivers, constructors, and status) to enrich the results table. Calculates new features like driver's age at race time and a flag to indicate if a driver did not finish (DNF) the race.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_17\n\nLANGUAGE: sql\nCODE:\n```\nwith results as (\n\n    select * from {{ ref('stg_f1_results') }}\n\n),\n\nraces as (\n\n    select * from {{ ref('stg_f1_races') }}\n\n),\n\ndrivers as (\n\n    select * from {{ ref('stg_f1_drivers') }}\n\n),\n\nconstructors as (\n\n    select * from {{ ref('stg_f1_constructors') }}\n),\n\nstatus as (\n\n    select * from {{ ref('stg_f1_status') }}\n),\n\nint_results as (\n    select\n        result_id,\n        results.race_id,\n        race_year,\n        race_round,\n        circuit_id,\n        circuit_name,\n        race_date,\n        race_time,\n        results.driver_id,\n        results.driver_number,\n        forename ||' '|| surname as driver,\n        cast(datediff('year', date_of_birth, race_date) as int) as drivers_age_years,\n        driver_nationality,\n        results.constructor_id,\n        constructor_name,\n        constructor_nationality,\n        grid,\n        position,\n        position_text,\n        position_order,\n        points,\n        laps,\n        results_time_formatted,\n        results_milliseconds,\n        fastest_lap,\n        results_rank,\n        fastest_lap_time_formatted,\n        fastest_lap_speed,\n        results.status_id,\n        status,\n        case when position is null then 1 else 0 end as dnf_flag\n    from results\n    left join races\n        on results.race_id=races.race_id\n    left join drivers\n        on results.driver_id = drivers.driver_id\n    left join constructors\n        on results.constructor_id = constructors.constructor_id\n    left join status\n        on results.status_id = status.status_id\n)\n\nselect * from int_results\n```\n\n----------------------------------------\n\nTITLE: Saved Query Selection\nDESCRIPTION: Selecting saved queries and their upstream resources\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/methods.md#2025-04-09_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ndbt list --select \"saved_query:*\"                    # list all saved queries \ndbt list --select \"+saved_query:orders_saved_query\"  # list your saved query named \"orders_saved_query\" and all upstream resources\n```\n\n----------------------------------------\n\nTITLE: Creating Orders Table in Snowflake SQL\nDESCRIPTION: This SQL command creates an orders table in the jaffle_shop schema of the raw database. It defines columns for id, user_id, order_date, status, and a timestamp for ETL loading.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/sl-snowflake-qs.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\ncreate table raw.jaffle_shop.orders\n( id integer,\n  user_id integer,\n  order_date date,\n  status varchar,\n  _etl_loaded_at timestamp default current_timestamp\n);\n```\n\n----------------------------------------\n\nTITLE: Defining Future-Proof Date Variable in dbt_project.yml\nDESCRIPTION: Sets a global variable in the dbt_project.yml file that defines a far-future date to replace NULL values in valid_to columns of snapshot tables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-05-24-joining-snapshot-complexity.md#2025-04-09_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\nvars:\n future_proof_date: '9999-12-31'\n```\n\n----------------------------------------\n\nTITLE: Dynamic Comment Using Jinja in dbt\nDESCRIPTION: Configuration to create a dynamic comment that includes the target user from the dbt context.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/query-comment.md#2025-04-09_snippet_7\n\nLANGUAGE: yml\nCODE:\n```\nquery-comment: \"run by {{ target.user }} in dbt\"\n\n```\n\n----------------------------------------\n\nTITLE: Example PEM Certificate Format for SAML 2.0 Integration\nDESCRIPTION: This snippet shows the correct format for an X.509 certificate in PEM format, which is required for the SAML 2.0 integration with dbt Cloud.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/manage-access/set-up-sso-saml-2.0.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n-----BEGIN CERTIFICATE-----\nMIIC8DCCAdigAwIBAgIQSANTIKwxA1221kqhkiG9w0dbtLabsBAQsFADA0MTIwMAYDVQQD\nEylNaWNyb3NvZnQgQXp1cmUgRmVkZXJhdGVkIFNTTyBDZXJ0aWZpY2F0ZTAeFw0yMzEyMjIwMDU1\nMDNaFw0yNjEyMjIwMDU1MDNaMDQxMjAwBgNVBAMTKU1pY3Jvc29mdCBBenVyZSBGZWRlcmF0ZWQg\nU1NPIENlcnRpZmljYXRlMIIBIjANBgkqhkiG9w0BAEFAAFRANKIEMIIBCgKCAQEAqfXQGc/D8ofK\naXbPXftPotqYLEQtvqMymgvhFuUm+bQ9YSpS1zwNQ9D9hWVmcqis6gO/VFw61e0lFnsOuyx+XMKL\nrJjAIsuWORavFqzKFnAz7hsPrDw5lkNZaO4T7tKs+E8N/Qm4kUp5omZv/UjRxN0XaD+o5iJJKPSZ\nPBUDo22m+306DE6ZE8wqxT4jTq4g0uXEitD2ZyKaD6WoPRETZELSl5oiCB47Pgn/mpqae9o0Q2aQ\nLP9zosNZ07IjKkIfyFKMP7xHwzrl5a60y0rSIYS/edqwEhkpzaz0f8QW5pws668CpZ1AVgfP9TtD\nY1EuxBSDQoY5TLR8++2eH4te0QIDAQABMA0GCSqGSIb3DmAKINgAA4IBAQCEts9ujwaokRGfdtgH\n76kGrRHiFVWTyWdcpl1dNDvGhUtCRsTC76qwvCcPnDEFBebVimE0ik4oSwwQJALExriSvxtcNW1b\nqvnY52duXeZ1CSfwHkHkQLyWBANv8ZCkgtcSWnoHELLOWORLD4aSrAAY2s5hP3ukWdV9zQscUw2b\nGwN0/bTxxQgA2NLZzFuHSnkuRX5dbtrun21USPTHMGmFFYBqZqwePZXTcyxp64f3Mtj3g327r/qZ\nsquyPSq5BrF4ivguYoTcGg4SCP7qfiNRFyBUTTERFLYU0n46MuPmVC7vXTsPRQtNRTpJj/b2gGLk\n1RcPb1JosS1ct5Mtjs41\n-----END CERTIFICATE-----\n```\n\n----------------------------------------\n\nTITLE: Defining Conversion Metrics in YAML\nDESCRIPTION: This snippet demonstrates how to define conversion metrics in a YAML file, including parameters specific to conversion type metrics.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metrics-overview.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  - name: The metric name \n    description: The metric description \n    type: conversion \n    label: YOUR_LABEL \n    type_params: #\n      conversion_type_params: \n        entity: ENTITY\n        calculation: CALCULATION_TYPE \n        base_measure: \n          name: The name of the measure \n          fill_nulls_with: Set the value in your metric definition instead of null (such as zero) \n          join_to_timespine: true/false\n        conversion_measure:\n          name: The name of the measure \n          fill_nulls_with: Set the value in your metric definition instead of null (such as zero) \n          join_to_timespine: true/false\n        window: TIME_WINDOW\n        constant_properties:\n          - base_property: DIMENSION or ENTITY \n            conversion_property: DIMENSION or ENTITY \n```\n\n----------------------------------------\n\nTITLE: Creating Teradata Database Connection in JupyterLab\nDESCRIPTION: Connects to a local Teradata database instance using JupyterLab's magic command.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/teradata-qs.md#2025-04-09_snippet_0\n\nLANGUAGE: ipynb\nCODE:\n```\n%connect local\n```\n\n----------------------------------------\n\nTITLE: Correct and Incorrect Screenshot File Naming Examples\nDESCRIPTION: Examples showing the wrong and right way to name screenshot files in the documentation, emphasizing descriptive and sequential naming conventions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/contributing/content-style-guide.md#2025-04-09_snippet_7\n\nLANGUAGE: markdown\nCODE:\n```\n  :x: screenshot-august0822.jpg\n\n  :white_check_mark: viewing-admins-01.jpg\n```\n\n----------------------------------------\n\nTITLE: Managing dbt Package Lock File\nDESCRIPTION: Shell commands for managing package lock file and package installations, including upgrading packages and adding new dependencies.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/deps.md#2025-04-09_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndbt deps --lock\n\ndbt deps --upgrade\n\ndbt deps --add-package dbt-labs/dbt_utils@1.0.0\n\n# with semantic version range\ndbt deps --add-package dbt-labs/snowplow@\">=0.7.0,<0.8.0\"\n\n# Git package\ndbt deps --add-package https://github.com/fivetran/dbt_amplitude@v0.3.0 --source git\n\n# Local package\ndbt deps --add-package /opt/dbt/redshift --source local\n```\n\n----------------------------------------\n\nTITLE: Running dbt Adapter Tests with PyTest\nDESCRIPTION: Command to execute functional tests using pytest\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_27\n\nLANGUAGE: sh\nCODE:\n```\npython3 -m pytest tests/functional\n```\n\n----------------------------------------\n\nTITLE: Creating a Snapshot of fct_income Model in dbt\nDESCRIPTION: dbt code to create a snapshot of the fct_income model. This snapshot configuration uses the 'check' strategy and tracks changes in the 'income' column. Note that this approach is not recommended as a best practice.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-07-12-change-data-capture.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot snapshot_fct_income %}\n\n{{\n    config(\n      target_database='analytics',\n      target_schema='snapshots',\n      unique_key='id',\n      strategy='check',\n      check_cols=['income']\n    )\n}}\n\nselect\n    month_year || ' - ' || product_category as id,    \n    *\nfrom {{ ref('fct_income') }}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Documenting Macro Arguments in YAML\nDESCRIPTION: YAML configuration for documenting macro arguments. When the 'validate_macro_args' flag is set to True, dbt will validate that argument names match the macro definition and that types follow the supported format.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/behavior-changes.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmacros:\n  - name: <macro name>\n    arguments:\n      - name: <arg name>\n        type: <string>\n```\n\n----------------------------------------\n\nTITLE: Configuring Databricks Tables in dbt 1.8\nDESCRIPTION: Configuration options for Databricks tables in dbt v1.8, including all v1.7 options plus support for Databricks Tags at the table level.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/databricks-configs.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Option    | Description   | Required?| Model support | Example  |\n|-----------|---------------|----------|---------------|----------|\n| file_format     | The file format to use when creating tables (`parquet`, `delta`, `hudi`, `csv`, `json`, `text`, `jdbc`, `orc`, `hive` or `libsvm`).  | Optional  | SQL, Python     | `delta`     |\n| location_root | The created table uses the specified directory to store its data. The table alias is appended to it.  | Optional | SQL, Python   | `/mnt/root`  |\n| partition_by    | Partition the created table by the specified columns. A directory is created for each partition.  | Optional   | SQL, Python   | `date_day`   |\n| liquid_clustered_by | Cluster the created table by the specified columns. Clustering method is based on [Delta's Liquid Clustering feature](https://docs.databricks.com/en/delta/clustering.html). Available since dbt-databricks 1.6.2. | Optional    | SQL, Python   | `date_day` |\n| clustered_by  | Each partition in the created table will be split into a fixed number of buckets by the specified columns. | Optional  | SQL, Python   | `country_code`   |\n| buckets    | The number of buckets to create while clustering  | Required if `clustered_by` is specified   | SQL, Python   | `8`  |\n| tblproperties  | [Tblproperties](https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-tblproperties.html) to be set on the created table   | Optional   | SQL, Python<sup>*</sup>  | `{'this.is.my.key': 12}` |\n| databricks_tags     | [Tags](https://docs.databricks.com/en/data-governance/unity-catalog/tags.html) to be set on the created table    | Optional    |  SQL<sup></sup>, Python<sup></sup> | `{'my_tag': 'my_value'}`  |\n| compression   | Set the compression algorithm.  | Optional     | SQL, Python   | `zstd`   |\n```\n\n----------------------------------------\n\nTITLE: Running dbt Tests\nDESCRIPTION: Runs the dbt test command to execute tests defined in the dbt project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_onrunstart-onrunend-commands.md#2025-04-09_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ndbt test\n```\n\n----------------------------------------\n\nTITLE: Running dbt with Artifact Upload\nDESCRIPTION: Commands to run dbt Core with artifact upload enabled, including an example of overriding environment variables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/hybrid-setup.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndbt run\n```\n\nLANGUAGE: bash\nCODE:\n```\nDBT_CLOUD_ACCOUNT_ID=1 DBT_CLOUD_ENVIRONMENT_ID=123 dbt run\n```\n\n----------------------------------------\n\nTITLE: Referencing SQL SELECT Statement in dbt Transformations\nDESCRIPTION: Demonstrates the use of a SELECT statement in dbt transformations, which is more accessible and easier to write than DML statements.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Project/why-not-write-dml.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n```\n\n----------------------------------------\n\nTITLE: Source Description Configuration in YAML\nDESCRIPTION: Demonstrates how to add descriptions to sources, their tables and columns in a schema.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/description.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: source_name\n    description: markdown_string\n\n    tables:\n      - name: table_name\n        description: markdown_string\n\n        columns:\n          - name: column_name\n            description: markdown_string\n```\n\n----------------------------------------\n\nTITLE: Implementing Conditional Target Lag in Snowflake Dynamic Table\nDESCRIPTION: This configuration for a Snowflake dynamic table uses a macro to set different target lags based on the environment.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-08-01-announcing-materialized-views.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\n{{\\nconfig(\\n    materialized = 'dynamic_table',\\n    snowflake_warehouse = 'transforming',\\n    target_lag = target_lag_environment(),\\n    on_configuration_change = 'apply',\\n)\\n}}\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Schema in Individual dbt Model\nDESCRIPTION: Model-level configuration using a config block to set the schema to 'core' for a specific model\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Models/model-custom-schemas.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(\n    schema='core'\n  )\n}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Updated_at in YAML (dbt 1.9+)\nDESCRIPTION: YAML configuration for setting up updated_at parameter in a snapshot using the timestamp strategy. Defines snapshot name, source relation, and configuration options.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/updated_at.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  - name: snapshot\n    relation: source('my_source', 'my_table')\n    config:\n      strategy: timestamp\n      updated_at: column_name\n```\n\n----------------------------------------\n\nTITLE: Basic SQL Self Join Structure\nDESCRIPTION: Demonstrates the basic syntax structure for creating a self join in SQL. Shows how to join a table to itself using aliases and a common join condition.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/joins/sql-self-join.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\t<fields>\nfrom <table_1> as t1\n[<join_type>] join <table_2> as t2\non t1.id = t2.id\n```\n\n----------------------------------------\n\nTITLE: Defining Dimensions in YAML for dbt Semantic Layer\nDESCRIPTION: This snippet defines dimensions for a transaction-based semantic model. It includes a time dimension and a categorical dimension for bulk transactions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/measures.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n# --- dimensions ---\n    dimensions:\n      - name: transaction_date\n        type: time\n        expr: date_trunc('day', ts) # expr refers to underlying column ts\n        type_params:\n          time_granularity: day\n      - name: is_bulk_transaction\n        type: categorical\n        expr: case when quantity > 10 then true else false end\n```\n\n----------------------------------------\n\nTITLE: Including Modified and Downstream Models in SQL\nDESCRIPTION: This SQL command includes models that were directly modified and also those one step downstream using the 'modified+1' selector.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/job-commands.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n--select state:modified+1\n```\n\n----------------------------------------\n\nTITLE: Configuring Snowflake User/Password Authentication in dbt (v1.8 and earlier)\nDESCRIPTION: This YAML configuration sets up basic user/password authentication for Snowflake in dbt versions 1.8 and earlier. It includes essential parameters like account ID, user credentials, role, database, warehouse, and schema.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/snowflake-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmy-snowflake-db:\n  target: dev\n  outputs:\n    dev:\n      type: snowflake\n      account: [account id]\n\n      # User/password auth\n      user: [username]\n      password: [password]\n\n      role: [user role]\n      database: [database name]\n      warehouse: [warehouse name]\n      schema: [dbt schema]\n      threads: [1 or more]\n      client_session_keep_alive: False\n      query_tag: [anything]\n\n      # optional\n      connect_retries: 0 # default 0\n      connect_timeout: 10 # default: 10\n      retry_on_database_errors: False # default: false\n      retry_all: False  # default: false\n      reuse_connections: False\n```\n\n----------------------------------------\n\nTITLE: Basic dispatch configuration in YAML\nDESCRIPTION: This snippet shows the basic structure of the dispatch configuration in dbt_project.yml. It defines the search order for macros in specific namespaces.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/dispatch-config.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndispatch:\n  - macro_namespace: packagename\n    search_order: [packagename]\n  - macro_namespace: packagename\n    search_order: [packagename]\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Profile for Exasol Connection\nDESCRIPTION: YAML configuration for connecting dbt to an Exasol database. This profile configuration specifies the connection parameters including DSN, credentials, and target schema. Required fields include type, threads, dsn, user, password, dbname, and schema.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/exasol-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndbt-exasol:\n  target: dev\n  outputs:\n    dev:\n      type: exasol\n      threads: 1\n      dsn: HOST:PORT\n      user: USERNAME\n      password: PASSWORD\n      dbname: db\n      schema: SCHEMA\n```\n\n----------------------------------------\n\nTITLE: Static Comment Prepending in dbt\nDESCRIPTION: Configuration to prepend a static comment to all SQL queries executed by dbt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/query-comment.md#2025-04-09_snippet_3\n\nLANGUAGE: yml\nCODE:\n```\nquery-comment: \"executed by dbt\"\n\n```\n\n----------------------------------------\n\nTITLE: Optimizing Model Versions with SQL References\nDESCRIPTION: Demonstrates how to optimize versioned models by defining newer versions in terms of previous versions. This example creates a view that selects from version 1 but excludes a specific column.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/govern/model-versions.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(materialized = 'view') }}\n\n{% set dim_customers_v1 = ref('dim_customers', v=1) %}\n\nselect\n{{ dbt_utils.star(from=dim_customers_v1, except=[\"country_name\"]) }}\nfrom {{ dim_customers_v1 }}\n```\n\n----------------------------------------\n\nTITLE: Defining a Basic Measure in YAML\nDESCRIPTION: This snippet shows how to define a basic measure named 'customers' that counts unique customer IDs.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/cumulative-metrics.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nmeasures:\n  - name: customers\n    expr: customer_id\n    agg: count_distinct\n```\n\n----------------------------------------\n\nTITLE: Executing dbt Build Command\nDESCRIPTION: Runs the dbt build command to execute the entire DAG of a dbt project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_onrunstart-onrunend-commands.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndbt build\n```\n\n----------------------------------------\n\nTITLE: Invalid SQL Query - Syntax Error Example\nDESCRIPTION: Demonstrates a SQL query with a syntax error (misspelled keyword) that can be caught by a Level 1 Parser.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2025-01-23-levels-of-sql-comprehension.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselecte dateadd('day', 1, getdate()) as tomorrow\n```\n\n----------------------------------------\n\nTITLE: Defining Bitbucket Pipeline for dbt Continuous Deployment in YAML\nDESCRIPTION: YAML configuration for a Bitbucket Pipeline that performs continuous deployment for a dbt project. It includes steps for setting up the environment, running dbt commands, and uploading artifacts for slim CI runs.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-04-14-add-ci-cd-to-bitbucket.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nimage: python:3.8\n \npipelines:\n  branches:\n    main:\n      - step:\n          name: Deploy to production\n          caches:\n            - pip\n          artifacts:  # Save the dbt run artifacts for the next step (upload)\n            - target/*.json\n          script:\n            - python -m pip install -r requirements.txt\n            - mkdir ~/.dbt\n            - cp .ci/profiles.yml ~/.dbt/profiles.yml\n            - dbt deps\n            - dbt seed --target prod\n            - dbt run --target prod\n            - dbt snapshot --target prod\n      - step:\n          name: Upload artifacts for slim CI runs\n          script:\n            - pipe: atlassian/bitbucket-upload-file:0.3.2\n              variables:\n                BITBUCKET_USERNAME: $BITBUCKET_USERNAME\n                BITBUCKET_APP_PASSWORD: $BITBUCKET_APP_PASSWORD\n                FILENAME: 'target/*.json'\n```\n\n----------------------------------------\n\nTITLE: Creating a Query Comment Macro in dbt\nDESCRIPTION: A simple Jinja macro that generates a custom comment string with dbt version, node ID, and target information.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/query-comment.md#2025-04-09_snippet_14\n\nLANGUAGE: jinja2\nCODE:\n```\n{% macro query_comment() %}\n\n  dbt {{ dbt_version }}: running {{ node.unique_id }} for target {{ target.name }}\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Installing MetricFlow CLI for Snowflake in dbt Core\nDESCRIPTION: Command to install the MetricFlow CLI package with Snowflake adapter support for dbt Core users who need to run MetricFlow commands and define semantic model configurations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/sl-migration.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install \"dbt-metricflow[snowflake]\"\n```\n\n----------------------------------------\n\nTITLE: Installing Xcode Command Line Tools on Mac Terminal\nDESCRIPTION: Command to install Xcode Command Line Tools required for setting up the local development environment.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/README.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nxcode-select --install\n```\n\n----------------------------------------\n\nTITLE: Enabling Fastload for Seeds in Teradata\nDESCRIPTION: Configuring seeds to use Teradata's fastload functionality to improve performance when loading large seed files.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/teradata-configs.md#2025-04-09_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  <project-name>:\n    +use_fastload: true\n```\n\n----------------------------------------\n\nTITLE: Using DATEDIFF in Snowflake, Amazon Redshift, and Databricks\nDESCRIPTION: Demonstrates the syntax for using the DATEDIFF function in Snowflake, Amazon Redshift, and Databricks. The function takes three arguments: date part, start date/time, and end date/time.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-07-05-datediff-sql-love-letter.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\ndatediff(<date part>, <start date/time>, <end date/time>)\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Schema for Marketing Models\nDESCRIPTION: Configuration in dbt_project.yml to set a custom schema 'marketing' for a group of marketing-related models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/schema.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  your_project:\n    marketing: #  Grouping or folder for set of models\n      +schema: marketing\n```\n\n----------------------------------------\n\nTITLE: Defining a Staging Model in SQL for dbt\nDESCRIPTION: This SQL snippet defines a staging model for orders, selecting and transforming fields from a source table. It includes entity IDs, numeric measures, and timestamp dimensions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-build-our-metrics/semantic-layer-3-build-semantic-models.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\n    ----------  ids -> entities\n    id as order_id,\n    store_id as location_id,\n    customer as customer_id,\n\n    ---------- numerics -> measures\n    (order_total / 100.0) as order_total,\n    (tax_paid / 100.0) as tax_paid,\n\n    ---------- timestamps -> dimensions\n    ordered_at\n\nfrom source\n```\n\n----------------------------------------\n\nTITLE: Fetching Primary Time Dimension Names with Semantic Layer JDBC API\nDESCRIPTION: SQL query to fetch primary time dimension names that represent metric_time using the semantic_layer.measures() function. This helps identify the common time thread across all metrics.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-jdbc.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nselect * from {{\n    semantic_layer.measures(metrics=['orders'])\n}}\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt_project.yml for Cloud Integration\nDESCRIPTION: Example of adding the tenant_hostname configuration to the dbt_project.yml file for dbt Cloud integration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/hybrid-setup.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nname: \"jaffle_shop\"\nversion: \"3.0.0\"\nrequire-dbt-version: \">=1.5.0\"\n....rest of dbt_project.yml configuration...\n\ndbt-cloud:\n  tenant_hostname: cloud.getdbt.com # Replace with your Tenant URL\n```\n\n----------------------------------------\n\nTITLE: Configuring Complete Unit Test Overrides in dbt\nDESCRIPTION: Example showing how to configure a unit test with overrides for macros, variables, and environment variables. Demonstrates setting up test inputs and expected outputs with various override types.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/unit-test-overrides.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n - name: test_my_model_overrides\n    model: my_model\n    given:\n      - input: ref('my_model_a')\n        rows:\n          - {id: 1, a: 1}\n      - input: ref('my_model_b')\n        rows:\n          - {id: 1, b: 2}\n          - {id: 2, b: 2}\n    overrides:\n      macros:\n        type_numeric: override\n        invocation_id: 123\n      vars:\n        my_test: var_override\n      env_vars:\n        MY_TEST: env_var_override\n    expect:\n      rows:\n        - {macro_call: override, var_call: var_override, env_var_call: env_var_override, invocation_id: 123}\n```\n\n----------------------------------------\n\nTITLE: Converting Non-UTC Timestamp to UTC in loaded_at_field\nDESCRIPTION: Example of converting a non-UTC timestamp to UTC for use in the loaded_at_field configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/freshness.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nloaded_at_field: \"convert_timezone('Australia/Sydney', 'UTC', created_at_local)\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Alias in SQL\nDESCRIPTION: Example of setting a custom alias for a model directly in the SQL file using config block. The model will be created with the identifier 'sessions' in the 'google_analytics' schema.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/custom-aliases.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n-- This model will be created in the database with the identifier `sessions`\n-- Note that in this example, `alias` is used along with a custom schema\n{{ config(alias='sessions', schema='google_analytics') }}\n\nselect * from ...\n```\n\n----------------------------------------\n\nTITLE: Disabling a Nested Source Table in dbt_project.yml (v1.9+)\nDESCRIPTION: Example showing how to disable a source table nested in a YAML file in a subfolder and configure event_time for dbt version 1.9 and later using the resource path syntax.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/source-configs.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  your_project_name:\n    subdirectory_name:\n      source_name:\n        source_table_name:\n          +enabled: false\n          +event_time: my_time_field\n```\n\n----------------------------------------\n\nTITLE: Configuring DuckDB with Multiple Attached Databases in YAML\nDESCRIPTION: Profile configuration that attaches multiple databases to a primary DuckDB instance. This allows querying across multiple files including local DuckDB files, S3 stored databases, and even SQLite databases.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/duckdb-configs.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ndefault:\n  outputs:\n    dev:\n      type: duckdb\n      path: /tmp/dbt.duckdb\n      attach:\n        - path: /tmp/other.duckdb\n        - path: ./yet/another.duckdb\n          alias: yet_another\n        - path: s3://yep/even/this/works.duckdb\n          read_only: true\n        - path: sqlite.db\n          type: sqlite\n```\n\n----------------------------------------\n\nTITLE: Export Commands\nDESCRIPTION: Commands for running exports in dbt Cloud for single and multiple saved queries.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metricflow-commands.md#2025-04-09_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\ndbt sl export\ndbt sl export-all\n```\n\n----------------------------------------\n\nTITLE: Defining Owner Groups in YAML for dbt Cloud Notifications\nDESCRIPTION: This snippet shows how to define groups with owner information in a YAML file. Each group must have a single email address specified for receiving model-level notifications. The owner key can include arbitrary fields beyond the required email field.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/model-notifications.md#2025-04-09_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nversion: 2\n\ngroups:\n  - name: finance\n    owner:\n      # Email is required to receive model-level notifications, additional properties are also allowed.\n      name: \"Finance team\"\n      email: finance@dbtlabs.com\n      favorite_food: donuts\n\n  - name: marketing\n    owner:\n      name: \"Marketing team\"\n      email: marketing@dbtlabs.com\n      favorite_food: jaffles\n\n  - name: docs\n    owner:\n      name: \"Documentation team\"\n      email: docs@dbtlabs.com\n      favorite_food: pizza\n```\n\n----------------------------------------\n\nTITLE: Defining Users Model Schema in YAML\nDESCRIPTION: This YAML file defines the schema for the users model, including column names, descriptions, and tests such as not_null, unique, and accepted_values.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-10-24-demystifying-event-streams.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n---\nversion: 2\n\nmodels:\n  - name: users\n    description: Lovely humans that use our app\n    columns:\n      - name: id\n        description: INT The id of this user\n        tests:\n          - not_null\n          - unique\n      - name: email\n        description: STRING User's contact email\n        tests:\n          - not_null\n      - name: state\n        description: STRING The current state of the user\n        tests:\n          - accepted_values:\n             values:\n               - \"active\"\n               - \"invited\"\n          - not_null\n```\n\n----------------------------------------\n\nTITLE: GitLab CI Single Job Configuration\nDESCRIPTION: GitLab CI pipeline configuration for running a dbt Cloud job on main branch pushes. Includes variable definitions and job configuration using Python 3.9 as the base image.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/custom-cicd-pipelines.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nimage: python:3.9\n\nvariables:\n  DBT_ACCOUNT_ID: 00000\n  DBT_PROJECT_ID: 00000\n  DBT_PR_JOB_ID:  00000\n  DBT_API_KEY: $DBT_API_KEY\n  DBT_URL: https://cloud.getdbt.com\n  DBT_JOB_CAUSE: 'GitLab Pipeline CI Job'\n  DBT_JOB_BRANCH: $CI_COMMIT_BRANCH\n\nstages:\n  - build\n\nrun-dbt-cloud-job:\n  stage: build\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"push\" && $CI_COMMIT_BRANCH == 'main'\n  script:\n    - python python/run_and_monitor_dbt_job.py\n```\n\n----------------------------------------\n\nTITLE: Installing ODBC Dependencies on Ubuntu/Debian\nDESCRIPTION: Command to install ODBC header files required for Azure Synapse setup on Debian/Ubuntu systems.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/azuresynapse-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install unixodbc-dev\n```\n\n----------------------------------------\n\nTITLE: Using generate_database_name macro for snapshots\nDESCRIPTION: Shows how to use the generate_database_name macro to set the target_database for a snapshot, ensuring consistent database naming with models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/target_database.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n      target_database=generate_database_name('snapshots')\n    )\n}}\n```\n\n----------------------------------------\n\nTITLE: SQL Compilation Query\nDESCRIPTION: Commands to view the generated SQL for a MetricFlow query using compile/explain flags.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metricflow-commands.md#2025-04-09_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\n# In dbt Cloud\ndbt sl query --metrics order_total --group-by metric_time,is_food_order --limit 10 --order-by -metric_time --where \"is_food_order = True\" --start-time '2017-08-22' --end-time '2017-08-27' --compile\n\n# In dbt Core\nmf query --metrics order_total --group-by metric_time,is_food_order --limit 10 --order-by -metric_time --where \"is_food_order = True\" --start-time '2017-08-22' --end-time '2017-08-27' --explain\n```\n\n----------------------------------------\n\nTITLE: Processing dbt Cloud Webhook and Generating Slack Messages in Python\nDESCRIPTION: This Python script validates the webhook authenticity, fetches run data from the dbt Cloud API, and constructs summary and error messages for Slack. It handles webhook validation, API requests, log parsing, and message formatting.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/zapier-slack.md#2025-04-09_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport hashlib\nimport hmac\nimport json\nimport re\n\n\nauth_header = input_data['auth_header']\nraw_body = input_data['raw_body']\n\n# Access secret credentials\nsecret_store = StoreClient('YOUR_SECRET_HERE')\nhook_secret = secret_store.get('DBT_WEBHOOK_KEY')\napi_token = secret_store.get('DBT_CLOUD_SERVICE_TOKEN')\n\n# Validate the webhook came from dbt Cloud\nsignature = hmac.new(hook_secret.encode('utf-8'), raw_body.encode('utf-8'), hashlib.sha256).hexdigest()\n\nif signature != auth_header:\n  raise Exception(\"Calculated signature doesn't match contents of the Authorization header. This webhook may not have been sent from dbt Cloud.\")\n\nfull_body = json.loads(raw_body)\nhook_data = full_body['data'] \n\n# Steps derived from these commands won't have their error details shown inline, as they're messy\ncommands_to_skip_logs = ['dbt source', 'dbt docs']\n\n# When testing, you will want to hardcode run_id and account_id to IDs that exist; the sample webhook won't work. \nrun_id = hook_data['runId']\naccount_id = full_body['accountId']\n\n# Fetch run info from the dbt Cloud Admin API\nurl = f'https://YOUR_ACCESS_URL/api/v2/accounts/{account_id}/runs/{run_id}/?include_related=[\"run_steps\"]'\nheaders = {'Authorization': f'Token {api_token}'}\nrun_data_response = requests.get(url, headers=headers)\nrun_data_response.raise_for_status()\nrun_data_results = run_data_response.json()['data']\n\n# Overall run summary\nstep_summary_post = f\"\"\"\n*<{run_data_results['href']}|{hook_data['runStatus']} for Run #{run_id} on Job \"{hook_data['jobName']}\">*\n\n*Environment:* {hook_data['environmentName']} | *Trigger:* {hook_data['runReason']} | *Duration:* {run_data_results['duration_humanized']}\n\n\"\"\"\n\nthreaded_errors_post = \"\"\n\n# Step-specific summaries\nfor step in run_data_results['run_steps']:\n  if step['status_humanized'] == 'Success':\n    step_summary_post += f\"\"\"\n {step['name']} ({step['status_humanized']} in {step['duration_humanized']})\n\"\"\"\n  else:\n    step_summary_post += f\"\"\"\n {step['name']} ({step['status_humanized']} in {step['duration_humanized']})\n\"\"\"\n\n    # Don't try to extract info from steps that don't have well-formed logs\n    show_logs = not any(cmd in step['name'] for cmd in commands_to_skip_logs)\n    if show_logs:\n      full_log = step['logs']\n      # Remove timestamp and any colour tags\n      full_log = re.sub('\\x1b?\\[[0-9]+m[0-9:]*', '', full_log)\n    \n      summary_start = re.search('(?:Completed with \\d+ error.* and \\d+ warnings?:|Database Error|Compilation Error|Runtime Error)', full_log)\n    \n      line_items = re.findall('(^.*(?:Failure|Error) in .*\\n.*\\n.*)', full_log, re.MULTILINE)\n\n      if not summary_start:\n        continue\n      \n      threaded_errors_post += f\"\"\"\n*{step['name']}*\n\"\"\"    \n      # If there are no line items, the failure wasn't related to dbt nodes, and we want the whole rest of the message. \n      # If there are, then we just want the summary line and then to log out each individual node's error.\n      if len(line_items) == 0:\n        relevant_log = f'```{full_log[summary_start.start():]}```'\n      else:\n        relevant_log = summary_start[0]\n        for item in line_items:\n          relevant_log += f'\\n```\\n{item.strip()}\\n```\\n'\n      threaded_errors_post += f\"\"\"\n{relevant_log}\n\"\"\"\n\nsend_error_thread = len(threaded_errors_post) > 0\n\n# Zapier looks for the `output` dictionary for use in subsequent steps\noutput = {'step_summary_post': step_summary_post, 'send_error_thread': send_error_thread, 'threaded_errors_post': threaded_errors_post}\n```\n\n----------------------------------------\n\nTITLE: Setting Default Grants in dbt_project.yml\nDESCRIPTION: Demonstrates how to set default grants for all models in the dbt_project.yml file. This configuration grants select privileges to 'user_a' and 'user_b'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/grants.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  +grants:  # In this case the + is not optional, you must include it for your project to parse.\n    select: ['user_a', 'user_b']\n```\n\n----------------------------------------\n\nTITLE: Reviewing dbt Logs for Debugging\nDESCRIPTION: Examine the dbt log file located at logs/dbt.log to understand what operations dbt is performing behind the scenes. This can help identify issues in Jinja template processing or SQL compilation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Project/debugging-jinja.md#2025-04-09_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nlogs/dbt.log\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Profile for Oracle (Host and Service)\nDESCRIPTION: Sets up the dbt profile using host and service for Oracle connection.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/oracle-setup.md#2025-04-09_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\ndbt_test:\n   target: \"{{ env_var('DBT_TARGET', 'dev') }}\"\n   outputs:\n      dev:\n         type: oracle\n         user: \"{{ env_var('DBT_ORACLE_USER') }}\"\n         pass: \"{{ env_var('DBT_ORACLE_PASSWORD') }}\"\n         protocol: \"tcps\"\n         host: \"{{ env_var('DBT_ORACLE_HOST') }}\"\n         port: 1522\n         service: \"{{ env_var('DBT_ORACLE_SERVICE') }}\"\n         database: \"{{ env_var('DBT_ORACLE_DATABASE') }}\"\n         schema: \"{{ env_var('DBT_ORACLE_SCHEMA') }}\"\n         retry_count: 1\n         retry_delay: 3\n         threads: 4\n```\n\n----------------------------------------\n\nTITLE: Merging Duplicate Contacts with Left Join in dbt\nDESCRIPTION: SQL snippet showing a left join to a merged_user seed file that maps old email addresses to new ones, enabling identity resolution for users who have changed email addresses.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-02-07-customer-360-view-census-playbook.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nleft join {{ ref('merged_user') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Tags in Resource YAML Files\nDESCRIPTION: Examples of adding tags to dbt resources in YAML property files, including support for column-level and test-level tags.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/tags.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nresource_type:\n  - name: resource_name\n    config:\n      tags: <string> | [<string>]\n    columns:\n      - name: column_name\n        tags: <string> | [<string>]\n        tests:\n          test-name:\n            config:\n              tags: \"single-string\"\n              tags: [\"string-1\", \"string-2\"]\n```\n\n----------------------------------------\n\nTITLE: DeleteSubscriptions Permission JSON Structure\nDESCRIPTION: JSON representation of the DeleteSubscriptions permission in Azure DevOps, showing the bit value, display name, and permission name used in the ServiceHooks namespace.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/git/setup-azure-service-user.md#2025-04-09_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"bit\": 4,\n    \"displayName\": \"Delete Subscriptions\",\n    \"name\": \"DeleteSubscriptions\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Databricks Catalogs for Development and Production\nDESCRIPTION: SQL commands to create separate catalogs for development and production environments in Databricks Unity Catalog. These catalogs serve as top-level containers for schemas, tables, and views.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/set-up-your-databricks-dbt-project.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\ncreate catalog if not exists dev;\ncreate catalog if not exists prod;\n```\n\n----------------------------------------\n\nTITLE: Calculating General Ledger Balances in dbt_quickbooks\nDESCRIPTION: This SQL model from dbt_quickbooks demonstrates how to calculate beginning, ending, and net change balances for the general ledger.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-09-07-leverage-accounting-principles-when-finacial-modeling.md#2025-04-09_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nmodels/intermediate/int_quickbooks__general_ledger_balances.sql\n```\n\n----------------------------------------\n\nTITLE: Schema Authorization in Redshift\nDESCRIPTION: SQL command to create and set proper authorization for test audit schema in Redshift.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/schema.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\ncreate schema if not exists dev_username_dbt_test__audit authorization username;\n```\n\n----------------------------------------\n\nTITLE: Invoking clean_stale_models Macro with dbt run-operation\nDESCRIPTION: Example of using dbt run-operation to invoke the clean_stale_models macro with 'days' and 'dry_run' arguments.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/run-operation.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ dbt run-operation clean_stale_models --args '{days: 7, dry_run: True}'\n```\n\n----------------------------------------\n\nTITLE: Configuring Session Connection for pySpark in dbt Profile\nDESCRIPTION: YAML configuration for connecting to a pySpark session. This method is intended for advanced users and experimental dbt development.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/spark-setup.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: spark\n      method: session\n      schema: [database/schema name]\n      host: NA                           # not used, but required by `dbt-core`\n      server_side_parameters:\n        \"spark.driver.memory\": \"4g\" \n```\n\n----------------------------------------\n\nTITLE: Configuring Event Time in dbt YAML for Microbatch Strategy\nDESCRIPTION: YAML configuration that sets the event_time field for a staging model. This configuration is required for parent models when using the microbatch incremental strategy to identify time-based records.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/incremental-microbatch.md#2025-04-09_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\\n  - name: stg_events\\n    config:\\n      event_time: my_time_field\\n\n```\n\n----------------------------------------\n\nTITLE: Signing Up for fly.io Authentication\nDESCRIPTION: Command to sign up for fly.io service, which is necessary before deploying the serverless application.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/serverless-pagerduty.md#2025-04-09_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nflyctl auth signup\n```\n\n----------------------------------------\n\nTITLE: Installing Multiple dbt Packages\nDESCRIPTION: Command to install multiple dbt packages simultaneously, including core and various adapters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/pip-install.md#2025-04-09_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npython -m pip install \\\n  dbt-core \\\n  dbt-postgres \\\n  dbt-redshift \\\n  dbt-snowflake \\\n  dbt-bigquery \\\n  dbt-trino\n```\n\n----------------------------------------\n\nTITLE: Configuring hard_deletes in snapshot schema.yml\nDESCRIPTION: YAML configuration for setting hard_deletes option in a snapshot's schema file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/hard-deletes.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  - name: <snapshot_name>\n    config:\n      hard_deletes: 'ignore' | 'invalidate' | 'new_record'\n```\n\n----------------------------------------\n\nTITLE: Complete Source Freshness Configuration Example\nDESCRIPTION: A comprehensive example of source freshness configuration in dbt, including multiple tables with different freshness settings.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/freshness.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: jaffle_shop\n    database: raw\n\n    freshness: # default freshness\n      warn_after: {count: 12, period: hour}\n      error_after: {count: 24, period: hour}\n\n    loaded_at_field: _etl_loaded_at\n\n    tables:\n      - name: customers # this will use the freshness defined above\n\n      - name: orders\n        freshness: # make this a little more strict\n          warn_after: {count: 6, period: hour}\n          error_after: {count: 12, period: hour}\n          # Apply a where clause in the freshness query\n          filter: datediff('day', _etl_loaded_at, current_timestamp) < 2\n\n\n      - name: product_skus\n        freshness: # do not check freshness for this table\n```\n\n----------------------------------------\n\nTITLE: Overriding is_incremental Macro in dbt Tests\nDESCRIPTION: Example of overriding the is_incremental macro when unit testing an incremental model. Shows how to set the macro to false for testing in full refresh mode.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/unit-test-overrides.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nunit_tests:\n    - name: my_unit_test\n      model: my_incremental_model\n      overrides:\n        macros:\n          # unit test this model in \"full refresh\" mode\n          is_incremental: false \n      ...\n```\n\n----------------------------------------\n\nTITLE: Service Principal Authentication Profile\nDESCRIPTION: YAML configuration for dbt profiles.yml using service principal authentication with tenant ID, client ID, and secret.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/fabric-setup.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: fabric\n      driver: 'ODBC Driver 18 for SQL Server'\n      server: hostname or IP of your server\n      port: 1433\n      database: exampledb\n      schema: schema_name\n      authentication: ServicePrincipal\n      tenant_id: 00000000-0000-0000-0000-000000001234\n      client_id: 00000000-0000-0000-0000-000000001234\n      client_secret: S3cret!\n```\n\n----------------------------------------\n\nTITLE: Referencing Docs Blocks in YAML Configuration\nDESCRIPTION: Shows how to reference docs blocks in the YAML configuration file for dbt models. Uses the doc() Jinja function to pull in documentation from the markdown file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-04-generating-dynamic-docs.md#2025-04-09_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: activity_based_interest_activated\n    description: \"\"\n    columns:\n      - name: id\n        description: \"{{ doc('activity_based_interest__id') }}\"\n\n      - name: user_id\n        description: \"{{ doc('activity_based_interest__user_id') }}\"\n\n... (truncated for example purposes)\n\n  - name: set_inactive_interest_rate_after_july_1st_in_bec_for_user\n    description: \"\"\n    columns:\n      - name: id\n        description: \"{{ doc('activity_based_interest__id') }}\"\n\n      - name: user_id\n        description: \"{{ doc('activity_based_interest__user_id') }}\"\n```\n\n----------------------------------------\n\nTITLE: Compiled SQL for Cumulative Revenue Metric with Granularity\nDESCRIPTION: This SQL snippet shows how the cumulative revenue metric is compiled into SQL. It demonstrates the use of window functions to select the first value for the selected granularity window, and how the metric is re-aggregated based on the specified time granularity.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/cumulative-metrics.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n-- re-aggregate metric via the group by\nselect\n  metric_time__week,\n  metric_time__quarter,\n  revenue_all_time\nfrom (\n  -- window function for metric re-aggregation\n  select\n    metric_time__week,\n    metric_time__quarter,\n    first_value(revenue_all_time) over (\n      partition by\n        metric_time__week,\n        metric_time__quarter\n      order by metric_time__day\n      rows between unbounded preceding and unbounded following\n    ) as revenue_all_time\n  from (\n    -- join self over time range\n    -- pass only elements: ['txn_revenue', 'metric_time__week', 'metric_time__quarter', 'metric_time__day']\n    -- aggregate measures\n    -- compute metrics via expressions\n    select\n      subq_11.metric_time__day as metric_time__day,\n      subq_11.metric_time__week as metric_time__week,\n      subq_11.metric_time__quarter as metric_time__quarter,\n      sum(revenue_src_28000.revenue) as revenue_all_time\n    from (\n      -- time spine\n      select\n        ds as metric_time__day,\n        date_trunc('week', ds) as metric_time__week,\n        date_trunc('quarter', ds) as metric_time__quarter\n      from mf_time_spine subq_12\n      group by\n        ds,\n        date_trunc('week', ds),\n        date_trunc('quarter', ds)\n    ) subq_11\n    inner join fct_revenue revenue_src_28000\n    on (\n      date_trunc('day', revenue_src_28000.created_at) <= subq_11.metric_time__day\n    )\n    group by\n      subq_11.metric_time__day,\n      subq_11.metric_time__week,\n      subq_11.metric_time__quarter\n  ) subq_16\n) subq_17\ngroup by\n  metric_time__week,\n  metric_time__quarter,\n  revenue_all_time\n```\n\n----------------------------------------\n\nTITLE: Compiled SQL for Cumulative Revenue Metric with Granularity\nDESCRIPTION: This SQL snippet shows how the cumulative revenue metric is compiled into SQL. It demonstrates the use of window functions to select the first value for the selected granularity window, and how the metric is re-aggregated based on the specified time granularity.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/cumulative-metrics.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n-- re-aggregate metric via the group by\nselect\n  metric_time__week,\n  metric_time__quarter,\n  revenue_all_time\nfrom (\n  -- window function for metric re-aggregation\n  select\n    metric_time__week,\n    metric_time__quarter,\n    first_value(revenue_all_time) over (\n      partition by\n        metric_time__week,\n        metric_time__quarter\n      order by metric_time__day\n      rows between unbounded preceding and unbounded following\n    ) as revenue_all_time\n  from (\n    -- join self over time range\n    -- pass only elements: ['txn_revenue', 'metric_time__week', 'metric_time__quarter', 'metric_time__day']\n    -- aggregate measures\n    -- compute metrics via expressions\n    select\n      subq_11.metric_time__day as metric_time__day,\n      subq_11.metric_time__week as metric_time__week,\n      subq_11.metric_time__quarter as metric_time__quarter,\n      sum(revenue_src_28000.revenue) as revenue_all_time\n    from (\n      -- time spine\n      select\n        ds as metric_time__day,\n        date_trunc('week', ds) as metric_time__week,\n        date_trunc('quarter', ds) as metric_time__quarter\n      from mf_time_spine subq_12\n      group by\n        ds,\n        date_trunc('week', ds),\n        date_trunc('quarter', ds)\n    ) subq_11\n    inner join fct_revenue revenue_src_28000\n    on (\n      date_trunc('day', revenue_src_28000.created_at) <= subq_11.metric_time__day\n    )\n    group by\n      subq_11.metric_time__day,\n      subq_11.metric_time__week,\n      subq_11.metric_time__quarter\n  ) subq_16\n) subq_17\ngroup by\n  metric_time__week,\n  metric_time__quarter,\n  revenue_all_time\n```\n\n----------------------------------------\n\nTITLE: Branch-Based Schema Generation\nDESCRIPTION: Macro that uses Git branch names as schema prefixes, allowing for branch-based isolation of work. Includes regex cleaning of branch names.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/customize-schema-alias.md#2025-04-09_snippet_3\n\nLANGUAGE: jinja\nCODE:\n```\n{% macro generate_schema_name(custom_schema_name, node) -%}\n\n    {%- set default_schema = target.schema -%}\n    {%- if  env_var('DBT_ENV_TYPE','DEV') == 'DEV' -%}\n    \n        {#- we replace characters not allowed in the schema names by \"_\" -#}\n        {%- set re = modules.re -%}\n        {%- set cleaned_branch = re.sub(\"\\W\", \"_\", env_var('DBT_CLOUD_GIT_BRANCH')) -%}\n        \n        {%- if custom_schema_name is none -%}\n\n            {{ cleaned_branch }}\n\n        {%- else -%}\n\n             {{ cleaned_branch }}_{{ custom_schema_name | trim }}\n\n        {%- endif -%}\n        \n    {%- else -%}\n\n        {{ default_schema }}_{{ custom_schema_name | trim }}\n\n    {%- endif -%}\n\n{%- endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Getting Missing Columns Example\nDESCRIPTION: Shows how to detect and add missing columns by comparing two relations\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/adapter.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{%- set target_relation = api.Relation.create(\n      database='database_name',\n      schema='schema_name',\n      identifier='table_name') -%}\n\n{% for col in adapter.get_missing_columns(target_relation, this) %}\n  alter table {{this}} add column \"{{col.name}}\" {{col.data_type}};\n{% endfor %}\n```\n\n----------------------------------------\n\nTITLE: Validating Python Version\nDESCRIPTION: Command to verify Python 3 installation and version.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dremio-lakehouse.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ python3 --version\nPython 3.11.4 # Must be Python 3\n```\n\n----------------------------------------\n\nTITLE: Configuring ODBC Connection for Databricks in dbt Profile\nDESCRIPTION: YAML configuration for connecting to Databricks SQL endpoint or interactive cluster via ODBC driver. This includes settings for host, organization, token, and optional parameters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/spark-setup.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: spark\n      method: odbc\n      driver: [path/to/driver]\n      schema: [database/schema name]\n      host: [yourorg.sparkhost.com]\n      organization: [org id]    # Azure Databricks only\n      token: [abc123]\n      \n      # one of:\n      endpoint: [endpoint id]\n      cluster: [cluster id]\n      \n      # optional\n      port: [port]              # default 443\n      user: [user]\n      server_side_parameters:\n        \"spark.driver.memory\": \"4g\" \n```\n\n----------------------------------------\n\nTITLE: Configuring Expectations/Constraints in dbt for Upsolver\nDESCRIPTION: This snippet shows how to set up expectations/constraints in dbt for Upsolver using YAML configuration. It demonstrates both model-level and column-level constraints.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/upsolver-configs.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: <model name>\n    # required\n    config:\n      contract:\n        enforced: true\n    # model-level constraints\n    constraints:\n      - type: check\n        columns: ['<column1>', '<column2>']\n        expression: \"column1 <= column2\"\n        name: <constraint_name>\n      - type: not_null\n        columns: ['column1', 'column2']\n        name: <constraint_name>\n\n    columns:\n      - name: <column3>\n        data_type: string\n\n        # column-level constraints\n        constraints:\n          - type: not_null\n          - type: check\n            expression: \"REGEXP_LIKE(<column3>, '^[0-9]{4}[a-z]{5}$')\"\n            name: <constraint_name>\n```\n\n----------------------------------------\n\nTITLE: Querying Models by Schema in GraphQL\nDESCRIPTION: This query finds all models in a specific schema and returns their uniqueId and executionTime. It demonstrates how to retrieve multiple models that match a schema filter.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/schema-discovery-job-models.mdx#2025-04-09_snippet_1\n\nLANGUAGE: graphql\nCODE:\n```\n{\n  job(id: 123) {\n    models(schema: \"analytics\") {\n      uniqueId\n      executionTime\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running dbt Build Command (Shell)\nDESCRIPTION: Shell command to build the dbt project, which will run the Python model and execute the tests.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-10-19-polyglot-dbt-python-dataframes-and-sql.md#2025-04-09_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\ndbt build\n```\n\n----------------------------------------\n\nTITLE: Defining Latest Version for a dbt Model in YAML\nDESCRIPTION: This snippet shows how to specify the 'latest_version' property for a dbt model in a YAML configuration file. It demonstrates setting the latest version to 2 and defining multiple versions of the model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/latest_version.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: model_name\n    latest_version: 2\n    versions:\n      - v: 2\n      - v: 1\n```\n\n----------------------------------------\n\nTITLE: Executing dbt Model for Driver Position Prediction\nDESCRIPTION: This bash command runs the dbt model 'predict_position' to apply the machine learning model and generate predictions for driver positions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_35\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --select predict_position\n```\n\n----------------------------------------\n\nTITLE: Color Configuration in profiles.yml\nDESCRIPTION: YAML configuration for controlling color output in file logs\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/logs.md#2025-04-09_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nconfig:\n  use_colors_file: False\n```\n\n----------------------------------------\n\nTITLE: Generating dbt Documentation for OpenLineage Integration\nDESCRIPTION: Running the 'dbt docs generate' command to create the catalog.json file needed by OpenLineage to extract metadata. This command analyzes the project structure and outputs a catalog file to the target directory.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-11-29-open-source-community-growth.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n% dbt docs generate\nRunning with dbt=1.9.0\nFound 7 models, 0 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 2 seed files, 4 sources, 0 exposures\n\n18:41:20 | Concurrency: 1 threads (target='dev')\n18:41:20 | \n18:41:20 | Done.\n18:41:20 | Building catalog\n18:41:31 | Catalog written to /Users/rturk/projects/metrics/target/catalog.json\n```\n\n----------------------------------------\n\nTITLE: Selector Inheritance in dbt\nDESCRIPTION: Using the selector method to reuse and extend definitions from other selectors.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/yaml-selectors.md#2025-04-09_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\nselectors:\n  - name: foo_and_bar\n    definition:\n      intersection:\n        - tag: foo\n        - tag: bar\n\n  - name: foo_bar_less_buzz\n    definition:\n      intersection:\n        # reuse the definition from above\n        - method: selector\n          value: foo_and_bar\n        # with a modification!\n        - exclude:\n            - method: tag\n              value: buzz\n```\n\n----------------------------------------\n\nTITLE: Staging Customer Model\nDESCRIPTION: Basic staging model that extracts and renames customer fields from the source table\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/redshift-qs.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    id as customer_id,\n    first_name,\n    last_name\n\nfrom jaffle_shop.customers\n```\n\n----------------------------------------\n\nTITLE: Aliasing All Fields Using dbt_utils.star Macro in SQL\nDESCRIPTION: Shows how to use the dbt_utils.star macro with the relation_alias argument to alias all fields in a model without explicitly renaming each one.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-07-13-star-sql-love-letter.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\t{{ dbt_utils.star(from=ref('table_a'), relation_alias='my_new_alias') }}\nfrom {{ ref('table_a') }}\n```\n\n----------------------------------------\n\nTITLE: Specifying Prerelease Package Version\nDESCRIPTION: This snippet shows two ways to specify a prerelease version of a package: explicitly specifying the prerelease version, and using a version range with the install_prerelease flag.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/packages.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\npackages:\n  - package: brooklyn-data/dbt_artifacts\n    version: 0.4.5-a2\n```\n\nLANGUAGE: yaml\nCODE:\n```\npackages:\n  - package: brooklyn-data/dbt_artifacts\n    version: [\">=0.4.4\", \"<0.4.6\"]\n    install_prerelease: true\n```\n\n----------------------------------------\n\nTITLE: Running dbt retry after fixing the error\nDESCRIPTION: This snippet demonstrates a successful dbt retry execution after fixing the syntax error in the customers model. The command only runs the previously failed model without re-running the successful models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/retry.md#2025-04-09_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nRunning with dbt=1.6.1\nRegistered adapter: duckdb=1.6.0\nFound 5 models, 3 seeds, 20 tests, 0 sources, 0 exposures, 0 metrics, 348 macros, 0 groups, 0 semantic models\n \nConcurrency: 24 threads (target='dev')\n\n1 of 1 START sql table model main.customers .................................... [RUN]\n1 of 1 OK created sql table model main.customers ............................... [OK in 0.05s]\n\nFinished running 1 table model in 0 hours 0 minutes and 0.09 seconds (0.09s).\n \nCompleted successfully\n  \nDone. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1\n```\n\n----------------------------------------\n\nTITLE: Categorical Dimension Grouping Query\nDESCRIPTION: Query showing how to group metrics by both time grain and categorical dimension.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-jdbc.md#2025-04-09_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nselect * from {{\n\tsemantic_layer.query(metrics=['food_order_amount', 'order_gross_profit'], \n\tgroup_by=[Dimension('metric_time').grain('month'), 'customer__customer_type'])\n\t}}\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration for dbt Documentation Categories\nDESCRIPTION: YAML front matter configuration that defines the structure and organization of dbt documentation guides. It organizes guides into three main categories: most popular integrations, troubleshooting resources, and advanced use cases.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/_config.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Browse our guides\ndescription: dbt Cloud is the fastest and most reliable way to deploy dbt for scalable data transformation, while dbt Core powers open-source transformation workflows. Together, they provide a seamless analytics engineering experience. Explore our step-by-step guides, quickstart tutorials, and troubleshooting resources to get started with dbt and your data platform.\ncategories:\n  - title: Most popular\n    guides:\n      - snowflake\n      - databricks\n      - bigquery\n      - redshift\n      - microsoft-fabric\n      - athena\n      - manual-install\n      - azure-synapse-analytics\n      - starburst-galaxy\n      - teradata\n\n  - title: Troubleshooting\n    guides:\n      - debug-schema-names\n      - using-jinja\n      - debug-errors\n\n  - title: Advanced use cases\n    guides:\n      - airflow-and-dbt-cloud\n      - adapter-creation\n      - core-to-cloud-1\n      - create-new-materializations\n      - customize-schema-alias\n      - refactoring-legacy-sql\n---\n```\n\n----------------------------------------\n\nTITLE: Configuring Resource Property File without Version Specified\nDESCRIPTION: Example of a resource property file without a version tag. Starting from dbt version 1.5, the version tag is no longer required in resource property files.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/version.md#2025-04-09_snippet_2\n\nLANGUAGE: yml\nCODE:\n```\n\nmodels: \n    ...\n```\n\n----------------------------------------\n\nTITLE: Simplified profiles.yml structure for dbt Core v1.8+\nDESCRIPTION: This YAML snippet shows the simplified structure of a profiles.yml file for dbt Core version 1.8 and later. It removes the global configurations section, focusing solely on profile names, targets, and database-specific connection details.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/profiles.yml.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n<profile-name>:\n  target: <target-name> # this is the default target\n  outputs:\n    <target-name>:\n      type: <bigquery | postgres | redshift | snowflake | other>\n      schema: <schema_identifier>\n      threads: <natural_number>\n\n      ### database-specific connection details\n      ...\n\n    <target-name>: # additional targets\n      ...\n\n<profile-name>: # additional profiles\n  ...\n```\n\n----------------------------------------\n\nTITLE: Configuring RisingWave Profile in dbt\nDESCRIPTION: Example YAML configuration for setting up a RisingWave profile in the dbt profiles.yml file. It includes fields for host, user, password, database name, port, and schema.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/risingwave-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndefault:\n  outputs:\n    dev:\n      type: risingwave\n      host: [host name] \n      user: [user name]\n      pass: [password]\n      dbname: [database name]\n      port: [port]\n      schema: [dbt schema]\n  target: dev\n```\n\n----------------------------------------\n\nTITLE: Running dbt Test with Custom Name via CLI\nDESCRIPTION: This shell command demonstrates how to run a specific dbt test using its custom name. The output shows the test execution and results.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/data-tests.md#2025-04-09_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\n$ dbt test --select unexpected_order_status_today\n12:43:41  Running with dbt=1.1.0\n12:43:41  Found 1 model, 1 test, 0 snapshots, 0 analyses, 167 macros, 0 operations, 1 seed file, 0 sources, 0 exposures, 0 metrics\n12:43:41\n12:43:41  Concurrency: 5 threads (target='dev')\n12:43:41\n12:43:41  1 of 1 START test unexpected_order_status_today ................................ [RUN]\n12:43:41  1 of 1 PASS unexpected_order_status_today ...................................... [PASS in 0.03s]\n12:43:41\n12:43:41  Finished running 1 test in 0.13s.\n12:43:41\n12:43:41  Completed successfully\n12:43:41\n12:43:41  Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1\n```\n\n----------------------------------------\n\nTITLE: Querying Data with GraphQL Parameters\nDESCRIPTION: Basic GraphQL query structure for retrieving data with support for Arrow and JSON output formats. Includes parameters for environment ID, query ID, and pagination.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-graphql.md#2025-04-09_snippet_10\n\nLANGUAGE: graphql\nCODE:\n```\n{\n  query(environmentId: BigInt!, queryId: Int!, pageNum: Int! = 1) {\n    sql\n    status\n    error\n    totalPages\n    arrowResult\n    jsonResult(orient: PandasJsonOrient! = TABLE, encoded: Boolean! = true)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running and Testing dbt Models via Command Line\nDESCRIPTION: A bash command that executes dbt run to build models and dbt test to run the tests. This validates that the dimension tables were created correctly according to the defined tests.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-04-18-building-a-kimball-dimensional-model-with-dbt.md#2025-04-09_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ndbt run && dbt test \n```\n\n----------------------------------------\n\nTITLE: Configuring Teradata Connection Profile in YAML\nDESCRIPTION: This snippet shows how to set up a Teradata connection profile in your profiles.yml file. It specifies required parameters including username, password, schema, transaction mode, and optional threading configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/teradata-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n<profile-name>:\n  target: <target-name>\n  outputs:\n    <target-name>:\n      type: teradata\n      user: <username>\n      password: <password>\n      schema: <database-name>\n      tmode: ANSI\n      threads: [optional, 1 or more]\n      #optional fields\n      <field-name: <field-value>\n```\n\n----------------------------------------\n\nTITLE: Retrieving Feature Values for ML Inference using Snowflake Feature Store in Python\nDESCRIPTION: This code snippet demonstrates how to retrieve feature values for machine learning inference using Snowflake Feature Store. It creates an inference spine dataframe, retrieves feature values using the retrieve_feature_values method, and converts the result to a pandas dataframe for further processing.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-10-05-snowflake-feature-store.md#2025-04-09_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ninfernce_spine = session.create_dataframe(\n    [\n        ('1', '3937', \"2019-07-01 00:00\"), \n        ('2', '2', \"2019-07-01 00:00\"),\n        ('3', '927', \"2019-07-01 00:00\"),\n    ], \n    schema=[\"INSTANCE_ID\", \"CUSTOMER_ID\", \"EVENT_TIMESTAMP\"])\n\ninference_dataset = fs.retrieve_feature_values(\n    spine_df=infernce_spine,\n    features=[customer_transactions_fv],\n    spine_timestamp_col=\"EVENT_TIMESTAMP\",\n)\n\ninference_dataset.to_pandas()\n```\n\n----------------------------------------\n\nTITLE: Deactivating Virtual Environment\nDESCRIPTION: Command to deactivate the current Python virtual environment.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/pip-install.md#2025-04-09_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndeactivate\n```\n\n----------------------------------------\n\nTITLE: Configuring Materialize Connection in dbt profiles.yml\nDESCRIPTION: This YAML configuration sets up the connection details for Materialize in the dbt profiles.yml file. It includes parameters for host, port, user credentials, database name, cluster, schema, and SSL settings.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/materialize-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmaterialize:\n  target: dev\n  outputs:\n    dev:\n      type: materialize\n      host: [host]\n      port: [port]\n      user: [user@domain.com]\n      pass: [password]\n      dbname: [database]\n      cluster: [cluster] # default 'default'\n      schema: [dbt schema]\n      sslmode: require\n      keepalives_idle: 0 # default: 0, indicating the system default\n      connect_timeout: 10 # default: 10 seconds\n      retries: 1 # default: 1, retry on error/timeout when opening connections\n```\n\n----------------------------------------\n\nTITLE: Multiple Seed Directories Configuration\nDESCRIPTION: Example of configuring multiple directories for seed files, though using subdirectories within a single seeds directory is recommended instead.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/seed-paths.md#2025-04-09_snippet_5\n\nLANGUAGE: yml\nCODE:\n```\nseed-paths: [\"seeds\", \"custom_seeds\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Oracle Instant Client on Windows (Bash)\nDESCRIPTION: Instructions for setting up Oracle Instant Client on Windows, including adding the client directory to the system PATH.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/oracle-setup.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nSET PATH=C:\\oracle\\instantclient_19_9;%PATH%\n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Merge Job in dbt for Upsolver\nDESCRIPTION: This snippet shows how to set up a SQL merge job in dbt for Upsolver. It uses the 'incremental' materialization with 'merge' strategy and allows specifying various options.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/upsolver-configs.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(  materialized='incremental',\n            sync=True|False,\n            map_columns_by_name=True|False,\n            incremental_strategy='merge',\n            options={\n              'option_name': 'option_value'\n            },\n            primary_key=[{}]\n          )\n}}\nSELECT ...\nFROM {{ ref(<model>) }}\nWHERE ...\nGROUP BY ...\nHAVING COUNT ...\n```\n\n----------------------------------------\n\nTITLE: Plotting a Lineage Graph with Python and NetworkX\nDESCRIPTION: This function creates a visual representation of a dbt project's lineage using NetworkX and matplotlib. It assigns nodes to layers, colors them by resource type, and creates a directed graph showing relationships between dbt resources.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/discovery-use-cases-and-examples.md#2025-04-09_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Plot the lineage graph\ndef plot_generic_graph(G):\n    plt.figure(figsize=(10, 6.5))\n\n    # Assign layers to the nodes\n    layer_counts = assign_layers(G)\n\n    # Use the multipartite_layout to create a layered layout\n    pos = nx.multipartite_layout(G, subset_key=\"layer\", align='vertical', scale=2)\n\n    # Adjust the y-coordinate of nodes to spread them out\n    y_offset = 1.0\n    for node, coords in pos.items():\n        layer = G.nodes[node][\"layer\"]\n        coords[1] = (coords[1] - 0.5) * (y_offset * layer_counts[layer])\n\n    # Define a color mapping for node types\n    type_color_map = {\n        \"models\": \"blue\",\n        \"sources\": \"green\",\n        \"seeds\": \"lightgreen\",\n        \"snapshots\": \"lightblue\",\n        \"metrics\": \"red\",\n        \"exposures\": \"orange\"\n    }\n\n    node_colors = [type_color_map[G.nodes[n].get(\"type\")] for n in G.nodes()]\n    nx.draw(G, pos, node_color=node_colors, node_shape=\"s\", node_size=3000, bbox=dict(facecolor=\"white\", edgecolor='gray', boxstyle='round,pad=0.1'), edgecolors='Gray', alpha=0.8, with_labels=True, labels={n: G.nodes[n].get('name') for n in G.nodes()}, font_size=11, font_weight=\"bold\")\n    plt.axis(\"off\")\n    plt.show()\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Profile Test Configuration\nDESCRIPTION: Setup for running tests with multiple database connection profiles\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndef pytest_addoption(parser):\n    parser.addoption(\"--profile\", action=\"store\", default=\"apache_spark\", type=str)\n\n\n# Using @pytest.mark.skip_profile('apache_spark') uses the 'skip_by_profile_type'\n# autouse fixture below\ndef pytest_configure(config):\n    config.addinivalue_line(\n        \"markers\",\n        \"skip_profile(profile): skip test for the given profile\",\n    )\n\n@pytest.fixture(scope=\"session\")\ndef dbt_profile_target(request):\n    profile_type = request.config.getoption(\"--profile\")\n    elif profile_type == \"databricks_sql_endpoint\":\n        target = databricks_sql_endpoint_target()\n    elif profile_type == \"apache_spark\":\n        target = apache_spark_target()\n    else:\n        raise ValueError(f\"Invalid profile type '{profile_type}'\")\n    return target\n\ndef apache_spark_target():\n    return {\n        \"type\": \"spark\",\n        \"host\": \"localhost\",\n        ...\n    }\n\ndef databricks_sql_endpoint_target():\n    return {\n        \"type\": \"spark\",\n        \"host\": os.getenv(\"DBT_DATABRICKS_HOST_NAME\"),\n        ...\n    }\n\n@pytest.fixture(autouse=True)\ndef skip_by_profile_type(request):\n    profile_type = request.config.getoption(\"--profile\")\n    if request.node.get_closest_marker(\"skip_profile\"):\n        for skip_profile_type in request.node.get_closest_marker(\"skip_profile\").args:\n            if skip_profile_type == profile_type:\n                pytest.skip(\"skipped on '{profile_type}' profile\")\n```\n\n----------------------------------------\n\nTITLE: Redshift dbt Seed Error Example\nDESCRIPTION: Example of the database error encountered in Redshift when seed file columns have changed, showing the column does not exist error.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Seeds/full-refresh-seed.md#2025-04-09_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$ dbt seed\nRunning with dbt=1.6.0-rc2\nFound 0 models, 0 tests, 0 snapshots, 0 analyses, 149 macros, 0 operations, 1 seed file, 0 sources\n\n12:14:46 | Concurrency: 1 threads (target='dev_redshift')\n12:14:46 |\n12:14:46 | 1 of 1 START seed file dbt_claire.country_codes...................... [RUN]\n12:14:46 | 1 of 1 ERROR loading seed file dbt_claire.country_codes.............. [ERROR in 0.23s]\n12:14:46 |\n12:14:46 | Finished running 1 seed in 1.75s.\n\nCompleted with 1 error and 0 warnings:\n\nDatabase Error in seed country_codes (seeds/country_codes.csv)\n  column \"country_name\" of relation \"country_codes\" does not exist\n\nDone. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1\n```\n\n----------------------------------------\n\nTITLE: dbt Command Table Structure\nDESCRIPTION: Markdown table showing dbt commands with their descriptions, parallel execution status, and tool/version compatibility\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-commands.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Command | Description | Parallel execution | <div style={{width:'250px'}}>Caveats</div> |\n|---------|-------------| :-----------------:| ------------------------------------------ |\n| [build](/reference/commands/build) | Builds and tests all selected resources (models, seeds, snapshots, tests) |   | All tools <br /> All [supported versions](/docs/dbt-versions/core) |\n```\n\n----------------------------------------\n\nTITLE: Docker Hub Pulls Table Schema\nDESCRIPTION: SQL schema definition for dockerhub_pulls table in BigQuery to store Docker image pull statistics.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-11-29-open-source-community-growth.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE `openlineage.metrics.dockerhub_pulls`\n(\n  timestamp TIMESTAMP,\n  image STRING,\n  pull_count INT64\n)\n```\n\n----------------------------------------\n\nTITLE: Using ref function in dbt to reference resources\nDESCRIPTION: The ref function is used to build dependencies between resources such as models, seeds, and snapshots by passing the resource name as an argument. When resource names are duplicated across different projects, you can use the two-argument ref syntax to specify the namespace.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Project/unique-resource-names.md#2025-04-09_snippet_0\n\nLANGUAGE: jinja\nCODE:\n```\nref\n```\n\nLANGUAGE: jinja\nCODE:\n```\nref('resource_name')\n```\n\n----------------------------------------\n\nTITLE: Legacy Job-Based Data Health Tile URL for Tableau\nDESCRIPTION: URL format for embedding legacy job-based data health tiles in Tableau using a Web Page object. This URL requires the exposure name, job ID, and metadata-only token.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/data-tile.md#2025-04-09_snippet_5\n\nLANGUAGE: html\nCODE:\n```\nhttps://metadata.YOUR_ACCESS_URL/exposure-tile?name=<exposure_name>&jobId=<job_id>&token=<metadata_only_token>\n```\n\n----------------------------------------\n\nTITLE: Configuring a Fiscal Calendar Time Spine in YAML\nDESCRIPTION: This YAML configuration defines a fiscal calendar model with custom granularities for fiscal year and fiscal week. It includes column descriptions and specified the standard granularity column.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/mf-time-spine.md#2025-04-09_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: time_spine_yearly\n    ... rest of the yearly time spine config ...  \n    \n  - name: fiscal_calendar\n    description: A custom fiscal calendar with fiscal year and fiscal week granularities.\n    time_spine:\n      standard_granularity_column: date_day\n      custom_granularities:\n        - name: fiscal_year\n          column_name: fiscal_year\n        - name: fiscal_week\n          column_name: fiscal_week\n    columns:\n      - name: date_day\n        granularity: day\n      - name: fiscal_year\n        description: \"Custom fiscal year starting in October\"\n      - name: fiscal_week\n        description: \"Fiscal week, shifted by 1 week from standard calendar\"\n```\n\n----------------------------------------\n\nTITLE: Snapshot Join Macro for Historical Data\nDESCRIPTION: A dbt macro that standardizes the process of joining snapshot tables while preserving proper time validity. It calculates the overlapping valid timeframes between tables and handles the complex join logic.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-05-24-joining-snapshot-complexity.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n-- requires any extra columns from table_join_on to be listed prior to using this macro.\n-- assumes we have replaced instances of valid_to = null with a future_proof_date = '9999-12-31'.\n \n{% macro join_snapshots(cte_join, cte_join_on, cte_join_valid_to,\n   cte_join_valid_from, cte_join_on_valid_to, cte_join_on_valid_from,\n   cte_join_id, cte_join_on_id) %}\n \n \n       {{cte_join}}.*,\n       greatest({{cte_join}}.{{cte_join_valid_from}},\n               coalesce( {{cte_join_on}}.{{cte_join_on_valid_from}}, {{cte_join}}.{{cte_join_valid_from}}))\n           as add_{{cte_join_on}}_valid_from,\n       least({{cte_join}}.{{cte_join_valid_to}},\n           coalesce({{cte_join_on}}.{{cte_join_on_valid_to}}, {{cte_join}}.{{cte_join_valid_to}})) as add_{{cte_join_on}}_valid_to\n  \n   from {{cte_join}}\n   left join {{cte_join_on}} on {{cte_join}}.{{cte_join_id}} = {{cte_join_on}}.{{cte_join_on_id}}\n      and ({{cte_join_on}}.{{cte_join_on_valid_from}} <= {{cte_join}}.{{cte_join_valid_to}}\n      and {{cte_join_on}}.{{cte_join_on_valid_to}} >= {{cte_join}}.{{cte_join_valid_from}})\n      \n  \n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Singular Test Severity in SQL\nDESCRIPTION: Example of setting an error threshold for a singular test using SQL configuration. Triggers an error if the test returns more than 50 failures.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/severity.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(error_if = '>50') }}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Running Specific Seeds with dbt Seed Command\nDESCRIPTION: This example demonstrates how to run a specific seed file named 'country_codes' using the --select flag with the dbt seed command. The output shows the execution process and results, including the number of records inserted and execution time.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/seed.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ dbt seed --select \"country_codes\"\nFound 2 models, 3 tests, 0 archives, 0 analyses, 53 macros, 0 operations, 2 seed files\n\n14:46:15 | Concurrency: 1 threads (target='dev')\n14:46:15 |\n14:46:15 | 1 of 1 START seed file analytics.country_codes........................... [RUN]\n14:46:15 | 1 of 1 OK loaded seed file analytics.country_codes....................... [INSERT 3 in 0.01s]\n14:46:16 |\n14:46:16 | Finished running 1 seed in 0.14s.\n```\n\n----------------------------------------\n\nTITLE: Configuring unique_key in dbt_project.yml for Snapshots\nDESCRIPTION: Example of setting unique_key globally for snapshots in dbt_project.yml. This approach applies the same unique key configuration to multiple snapshots defined in a specific resource path.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/unique_key.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  [<resource-path>](/reference/resource-configs/resource-path):\n    +unique_key: column_name_or_expression\n```\n\n----------------------------------------\n\nTITLE: Setting Table Kind in SQL Model Configuration for Teradata\nDESCRIPTION: Configuring the table_kind parameter in a dbt model to define whether the table should be MULTISET (default) or SET type in Teradata.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/teradata-configs.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n{{\n  config(\n      materialized=\"table\",\n      table_kind=\"SET\"\n  )\n}}\n```\n\n----------------------------------------\n\nTITLE: dbt Project YAML Parsing Error Message\nDESCRIPTION: Example of the error message displayed when dbt Cloud encounters issues parsing the dbt_project.yml file. This typically occurs due to tab indentation, Unicode characters, or missing required fields.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Troubleshooting/could-not-parse-project.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nCould not parse dbt_project.yml: while scanning for...\n```\n\n----------------------------------------\n\nTITLE: Configuring Managed Identity Authentication for Azure SQL in dbt\nDESCRIPTION: This configuration uses managed identity authentication for Azure SQL, which works with both system-assigned and user-assigned managed identities. It only requires basic connection details in the profiles.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/mssql-setup.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: sqlserver\n      driver: 'ODBC Driver 18 for SQL Server' # (The ODBC Driver installed on your system)\n      server: hostname or IP of your server\n      port: 1433\n      database: exampledb\n      schema: schema_name\n      authentication: ActiveDirectoryMsi\n```\n\n----------------------------------------\n\nTITLE: Configuring Tags in SQL Models\nDESCRIPTION: Example of applying tags directly in SQL model files using the config block.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/tags.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    tags=\"<string>\" | [\"<string>\"]\n) }}\n```\n\n----------------------------------------\n\nTITLE: Model Configuration Example\nDESCRIPTION: Example model configuration showing various config properties that can be used for selection\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/methods.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n{{ config(\n  materialized = 'incremental',\n  unique_key = ['column_a', 'column_b'],\n  grants = {'select': ['reporter', 'analysts']},\n  meta = {\"contains_pii\": true},\n  transient = true\n) }}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Referencing WHERE Filter in Saved Queries Documentation\nDESCRIPTION: This code snippet demonstrates how to reference the WHERE clause in saved queries. When a saved query uses a WHERE filter, the add-on will display the advanced syntax for this filter.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_sl-excel-gsheets.md#2025-04-09_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<code>WHERE</code>\n```\n\n----------------------------------------\n\nTITLE: Creating a Materialized View in Trino using dbt SQL Model\nDESCRIPTION: This SQL configuration creates a materialized view in Parquet format using dbt. It sets the materialization type and format property.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/trino-configs.md#2025-04-09_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(\n    materialized = 'materialized_view',\n    properties = {\n      'format': \"'PARQUET'\"\n    },\n  )\n}}\n```\n\n----------------------------------------\n\nTITLE: Compiling dbt Project\nDESCRIPTION: Executes the dbt compile command to generate executable SQL from project files without running them.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_onrunstart-onrunend-commands.md#2025-04-09_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndbt compile\n```\n\n----------------------------------------\n\nTITLE: Configuring Network Rules and External Access Integration in dbt YAML\nDESCRIPTION: This YAML configuration sets up a network rule and external access integration for accessing the Carbon Intensity API. It uses pre-hooks to create the necessary Snowflake objects.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-06-12-putting-your-dag-on-the-internet.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: external_access_sample\n    config:\n      pre_hook: \n        - \"create or replace network rule test_network_rule type = host_port mode = egress value_list= ('api.carbonintensity.org.uk:443');\"\n        - \"create or replace external access integration test_external_access_integration allowed_network_rules = (test_network_rule) enabled = true;\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Dataproc for Python Models in BigQuery\nDESCRIPTION: Configuration to run dbt Python models on a Dataproc cluster. This requires specifying a GCS bucket, cluster name, and region to execute Python transformations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/bigquery-setup.md#2025-04-09_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\nmy-profile:\n  target: dev\n  outputs:\n    dev:\n      type: bigquery\n      method: oauth\n      project: abc-123\n      dataset: my_dataset\n      \n      # for dbt Python models to be run on a Dataproc cluster\n      gcs_bucket: dbt-python\n      dataproc_cluster_name: dbt-python\n      dataproc_region: us-central1\n```\n\n----------------------------------------\n\nTITLE: Configuring LDAP Authentication for Starburst/Trino in dbt\nDESCRIPTION: Example YAML configuration in profiles.yml for setting up LDAP authentication with Starburst/Trino in dbt. Includes essential parameters like host, database, schema, and authentication details.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/trino-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntrino:\n  target: dev\n  outputs:\n    dev:\n      type: trino\n      method: ldap \n      user: [user]\n      password: [password]\n      host: [hostname]\n      database: [database name]\n      schema: [your dbt schema]\n      port: [port number]\n      threads: [1 or more]\n```\n\n----------------------------------------\n\nTITLE: Setting Target Schema in Individual Snapshot File\nDESCRIPTION: Example demonstrating how to configure target schema directly within a snapshot SQL file using the config block.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/target_schema.md#2025-04-09_snippet_1\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ config(\n      target_schema=\"string\"\n) }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Quoting in dbt Project\nDESCRIPTION: Sets the default quoting configuration for dbt-oracle in the dbt_project.yaml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/oracle-setup.md#2025-04-09_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\nquoting:\n  database: false\n  identifier: false\n  schema: false\n```\n\n----------------------------------------\n\nTITLE: Non-boolean Config Flag Structure\nDESCRIPTION: Template showing the structure for using non-boolean configuration flags with dbt commands.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/command-line-options.md#2025-04-09_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n<SUBCOMMAND> --<THIS-CONFIG>=<SETTING> \n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Server Profile with Standard Authentication\nDESCRIPTION: YAML configuration for a dbt profile using standard SQL Server authentication. This includes server details, database name, schema, and user credentials.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/mssql-setup.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: sqlserver\n      driver: 'ODBC Driver 18 for SQL Server' # (The ODBC Driver installed on your system)\n      server: hostname or IP of your server\n      port: 1433\n      database: database\n      schema: schema_name\n      user: username\n      password: password\n```\n\n----------------------------------------\n\nTITLE: Configuring the 'append_new_columns' Parameter for Incremental Models in Vertica\nDESCRIPTION: Example of using the 'append_new_columns' parameter, which automatically adds new columns from the source to the target table while preserving existing data. This is useful for schema evolution.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/vertica-configs.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{ config( materialized='incremental', on_schema_change='append_new_columns') }}\n\n\n\n    select * from  public.seed_added\n```\n\nLANGUAGE: sql\nCODE:\n```\n          insert into \"VMart\".\"public\".\"over\" (\"id\", \"name\", \"some_date\", \"w\", \"w1\", \"t1\", \"t2\", \"t3\")\n          (\n                select \"id\", \"name\", \"some_date\", \"w\", \"w1\", \"t1\", \"t2\", \"t3\"\n                from \"over__dbt_tmp\"\n          )\n```\n\n----------------------------------------\n\nTITLE: Generating Surrogate Keys in Postgres SQL\nDESCRIPTION: This SQL snippet shows how to create a surrogate key in Postgres by concatenating multiple columns without explicit null handling, as Postgres' CONCAT function ignores nulls.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-11-22-sql-surrogate-keys.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nmd5 ( concat (column1, column2) )\n```\n\n----------------------------------------\n\nTITLE: Implementing Slack Notifications for Pipeline Execution in Python\nDESCRIPTION: This code snippet defines a decorator function that sends notifications to Slack upon completion or failure of a pipeline function. It uses dlt's Slack integration to post messages about successful runs or errors encountered during execution.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-12-15-serverless-free-tier-data-stack-with-dlt-and-dbt-core.md#2025-04-09_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dlt.common.runtime.slack import send_slack_message\n\ndef notify_on_completion(hook):\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            try:\n                load_info = func(*args, **kwargs)\n                message = f\"Function {func.__name__} completed successfully. Load info: {load_info}\"\n                send_slack_message(hook, message)\n                return load_info\n            except Exception as e:\n                message = f\"Function {func.__name__} failed. Error: {str(e)}\"\n                send_slack_message(hook, message)\n                raise\n        return wrapper\n    return decorator\n```\n\n----------------------------------------\n\nTITLE: Loading GitHub Stars Data to BigQuery\nDESCRIPTION: Python script to fetch current GitHub star counts from GitHub API and load them into BigQuery. The script runs before each pipeline execution to ensure latest data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-11-29-open-source-community-growth.md#2025-04-09_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndataset_ref = client.dataset('metrics')\ntable_ref = dataset_ref.table('github_stars')\ntable = client.get_table(table_ref)\n\nnow = int(time.time())\n\nfor project in projects:\n  url = 'https://api.github.com/repos/' + project\n  response = requests.get(url)\n  watchers = response.json()['watchers']\n  client.insert_rows(table, [(now,project,watchers)])\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt-databricks Authentication Profile\nDESCRIPTION: Example of simplified authentication configuration in profiles.yml using the new dbt-databricks adapter.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/migrate-from-spark-to-databricks.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: databricks\n      schema: my_schema\n      host:  dbc-l33t-nwb.cloud.databricks.com\n      http_path: /sql/1.0/endpoints/8657cad335ae63e3\n      token: [my_secret_token]\n```\n\n----------------------------------------\n\nTITLE: Example SQL Join Query for Redshift\nDESCRIPTION: A basic SQL join query that demonstrates the relationship between two tables that would benefit from proper distribution configuration. This query joins visitor data with known visitor profiles using person_id.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-05-19-redshift-configurations-dbt-model-optimizations.md#2025-04-09_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nselect <your_list_of_columns>\nfrom visitors\nleft join known_visitor_profiles\non visitors.person_id = known_visitor_profiles.person_id\n```\n\n----------------------------------------\n\nTITLE: Running dbt Seed Command to Load CSV Files\nDESCRIPTION: Example of running the dbt seed command which loads all CSV files from the seeds directory into the data warehouse as tables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/seeds.md#2025-04-09_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n$ dbt seed\n\nFound 2 models, 3 tests, 0 archives, 0 analyses, 53 macros, 0 operations, 1 seed file\n\n14:46:15 | Concurrency: 1 threads (target='dev')\n14:46:15 |\n14:46:15 | 1 of 1 START seed file analytics.country_codes........................... [RUN]\n14:46:15 | 1 of 1 OK loaded seed file analytics.country_codes....................... [INSERT 3 in 0.01s]\n14:46:16 |\n14:46:16 | Finished running 1 seed in 0.14s.\n\nCompleted successfully\n\nDone. PASS=1 ERROR=0 SKIP=0 TOTAL=1\n```\n\n----------------------------------------\n\nTITLE: Source Freshness JSON Output Format\nDESCRIPTION: Example of the JSON output format generated by the dbt source freshness command. It shows the structure of the sources.json file, including metadata, freshness status, and evaluation criteria for each source.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/source.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"meta\": {\n        \"generated_at\": \"2019-02-15T00:53:03.971126Z\",\n        \"elapsed_time\": 0.21452808380126953\n    },\n    \"sources\": {\n        \"source.project_name.source_name.table_name\": {\n            \"max_loaded_at\": \"2019-02-15T00:45:13.572836+00:00Z\",\n            \"snapshotted_at\": \"2019-02-15T00:53:03.880509+00:00Z\",\n            \"max_loaded_at_time_ago_in_s\": 481.307673,\n            \"state\": \"pass\",\n            \"criteria\": {\n                \"warn_after\": {\n                    \"count\": 12,\n                    \"period\": \"hour\"\n                },\n                \"error_after\": {\n                    \"count\": 1,\n                    \"period\": \"day\"\n                }\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Project Name in dbt\nDESCRIPTION: Basic project configuration in dbt_project.yml file defining the project name.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/govern/project-dependencies.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nname: jaffle_marketing\n```\n\n----------------------------------------\n\nTITLE: Configuring Transformation Options for Multiple Storage Systems\nDESCRIPTION: This markdown table outlines various configuration options for data transformations across different storage systems. It includes details on run intervals, time ranges, compute clusters, validation settings, parallelism, and storage-specific options.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/upsolver-configs.md#2025-04-09_snippet_8\n\nLANGUAGE: markdown\nCODE:\n```\n| Option | Storage   | Editable | Optional | Config Syntax |\n| -------| --------- | -------- | -------- | ------------- |\n| run_interval | s3 | False | True | 'run_interval': `'<N MINUTES/HOURS/DAYS>'` |\n| start_from | s3 | False | True | 'start_from': `'<timestamp>/NOW/BEGINNING'` |\n| end_at | s3 | True | True | 'end_at': `'<timestamp>/NOW'` |\n| compute_cluster | s3 | True | True | 'compute_cluster': `'<compute_cluster>'` |\n| comment | s3 | True | True | 'comment': `'<comment>'` |\n| skip_validations | s3 | False | True | 'skip_validations': ('ALLOW_CARTESIAN_PRODUCT', ...) |\n| skip_all_validations | s3 | False | True | 'skip_all_validations': True/False |\n| aggregation_parallelism | s3 | True | True | 'aggregation_parallelism': `<integer>` |\n| run_parallelism | s3 | True | True | 'run_parallelism': `<integer>` |\n| file_format | s3 | False | False | 'file_format': '(type = `<file_format>`)' |\n| compression | s3 | False | True | 'compression': 'SNAPPY/GZIP ...' |\n| date_pattern | s3 | False | True | 'date_pattern': `'<date_pattern>'` |\n| output_offset | s3 | False | True | 'output_offset': `'<N MINUTES/HOURS/DAYS>'` |\n| run_interval | elasticsearch | False | True | 'run_interval': `'<N MINUTES/HOURS/DAYS>'` |\n| routing_field_name | elasticsearch | True | True | 'routing_field_name': `'<routing_field_name>'` |\n| start_from | elasticsearch | False | True | 'start_from': `'<timestamp>/NOW/BEGINNING'` |\n| end_at | elasticsearch | True | True | 'end_at': `'<timestamp>/NOW'` |\n| compute_cluster | elasticsearch | True | True | 'compute_cluster': `'<compute_cluster>'` |\n| skip_validations | elasticsearch | False | True | 'skip_validations': ('ALLOW_CARTESIAN_PRODUCT', ...) |\n| skip_all_validations | elasticsearch | False | True | 'skip_all_validations': True/False |\n| aggregation_parallelism | elasticsearch | True | True | 'aggregation_parallelism': `<integer>` |\n| run_parallelism | elasticsearch | True | True | 'run_parallelism': `<integer>` |\n| bulk_max_size_bytes | elasticsearch | True | True | 'bulk_max_size_bytes': `<integer>` |\n| index_partition_size | elasticsearch | True | True | 'index_partition_size': 'HOURLY/DAILY ...' |\n| comment | elasticsearch | True | True | 'comment': `'<comment>'` |\n| custom_insert_expressions | snowflake | True | True | 'custom_insert_expressions': {'INSERT_TIME' : 'CURRENT_TIMESTAMP()','MY_VALUE': `'<value>'`} |\n| custom_update_expressions | snowflake | True | True | 'custom_update_expressions': {'UPDATE_TIME' : 'CURRENT_TIMESTAMP()','MY_VALUE': `'<value>'`} |\n| keep_existing_values_when_null | snowflake | True | True | 'keep_existing_values_when_null': True/False |\n| add_missing_columns | snowflake | False | True | 'add_missing_columns': True/False |\n| run_interval | snowflake | False | True | 'run_interval': `'<N MINUTES/HOURS/DAYS>'` |\n| commit_interval | snowflake | True | True | 'commit_interval': `'<N MINUTE[S]/HOUR[S]/DAY[S]>'` |\n| start_from | snowflake | False | True | 'start_from': `'<timestamp>/NOW/BEGINNING'` |\n| end_at | snowflake | True | True | 'end_at': `'<timestamp>/NOW'` |\n| compute_cluster | snowflake | True | True | 'compute_cluster': `'<compute_cluster>'` |\n| skip_validations | snowflake | False | True | 'skip_validations': ('ALLOW_CARTESIAN_PRODUCT', ...) |\n| skip_all_validations | snowflake | False | True | 'skip_all_validations': True/False |\n| aggregation_parallelism | snowflake | True | True | 'aggregation_parallelism': `<integer>` |\n| run_parallelism | snowflake | True | True | 'run_parallelism': `<integer>` |\n| comment | snowflake | True | True | 'comment': `'<comment>'` |\n| add_missing_columns | datalake | False | True | 'add_missing_columns': True/False |\n| run_interval | datalake | False | True | 'run_interval': `'<N MINUTES/HOURS/DAYS>'` |\n| start_from | datalake | False | True | 'start_from': `'<timestamp>/NOW/BEGINNING'` |\n| end_at | datalake | True | True | 'end_at': `'<timestamp>/NOW'` |\n| compute_cluster | datalake | True | True | 'compute_cluster': `'<compute_cluster>'` |\n| skip_validations | datalake | False | True | 'skip_validations': ('ALLOW_CARTESIAN_PRODUCT', ...) |\n| skip_all_validations | datalake | False | True | 'skip_all_validations': True/False |\n| aggregation_parallelism | datalake | True | True | 'aggregation_parallelism': `<integer>` |\n| run_parallelism | datalake | True | True | 'run_parallelism': `<integer>` |\n| comment | datalake | True | True | 'comment': `'<comment>'` |\n| run_interval | redshift | False | True | 'run_interval': `'<N MINUTES/HOURS/DAYS>'` |\n| start_from | redshift | False | True | 'start_from': `'<timestamp>/NOW/BEGINNING'` |\n| end_at | redshift | True | True | 'end_at': `'<timestamp>/NOW'` |\n| compute_cluster | redshift | True | True | 'compute_cluster': `'<compute_cluster>'` |\n| skip_validations | redshift | False | True | 'skip_validations': ('ALLOW_CARTESIAN_PRODUCT', ...) |\n| skip_all_validations | redshift | False | True | 'skip_all_validations': True/False |\n| aggregation_parallelism | redshift | True | True | 'aggregation_parallelism': `<integer>` |\n| run_parallelism | redshift | True | True | 'run_parallelism': `<integer>` |\n| skip_failed_files | redshift | False | True | 'skip_failed_files': True/False |\n| fail_on_write_error | redshift | False | True | 'fail_on_write_error': True/False |\n| comment | redshift | True | True | 'comment': `'<comment>'` |\n| run_interval | postgres | False | True | 'run_interval': `'<N MINUTES/HOURS/DAYS>'` |\n| start_from | postgres | False | True | 'start_from': `'<timestamp>/NOW/BEGINNING'` |\n| end_at | postgres | True | True | 'end_at': `'<timestamp>/NOW'` |\n| compute_cluster | postgres | True | True | 'compute_cluster': `'<compute_cluster>'` |\n| skip_validations | postgres | False | True | 'skip_validations': ('ALLOW_CARTESIAN_PRODUCT', ...) |\n| skip_all_validations | postgres | False | True | 'skip_all_validations': True/False |\n| aggregation_parallelism | postgres | True | True | 'aggregation_parallelism': `<integer>` |\n| run_parallelism | postgres | True | True | 'run_parallelism': `<integer>` |\n| comment | postgres | True | True | 'comment': `'<comment>'` |\n```\n\n----------------------------------------\n\nTITLE: SQL Query Analogy for Dimensions and Measures\nDESCRIPTION: An example SQL query demonstrating how dimensions relate to GROUP BY columns and measures relate to aggregated columns in SQL, providing a conceptual analogy for semantic model components.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_sl-create-semanticmodel.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nselect\n  metric_time_day,  -- time\n  country,  -- categorical dimension\n  sum(revenue_usd) -- measure\nfrom\n  snowflake.fact_transactions  -- sql table\ngroup by metric_time_day, country  -- dimensions\n```\n\n----------------------------------------\n\nTITLE: IAM Policy for Iceberg Commit Locking on AWS DynamoDB\nDESCRIPTION: JSON IAM policy that grants necessary permissions for Iceberg's commit locking mechanism using DynamoDB. Required for merge operations with Iceberg tables to ensure atomic transactions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/glue-setup.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"CommitLockTable\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"dynamodb:CreateTable\",\n                \"dynamodb:BatchGetItem\",\n                \"dynamodb:BatchWriteItem\",\n                \"dynamodb:ConditionCheckItem\",\n                \"dynamodb:PutItem\",\n                \"dynamodb:DescribeTable\",\n                \"dynamodb:DeleteItem\",\n                \"dynamodb:GetItem\",\n                \"dynamodb:Scan\",\n                \"dynamodb:Query\",\n                \"dynamodb:UpdateItem\"\n            ],\n            \"Resource\": \"arn:aws:dynamodb:<AWS_REGION>:<AWS_ACCOUNT_ID>:table/myGlueLockTable\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Aliasing a Model in SQL File\nDESCRIPTION: Assigns an alias directly in the models/sales_total.sql file using a config block.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/alias.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    alias=\"sales_dashboard\"\n) }}\n```\n\n----------------------------------------\n\nTITLE: Version Selection in dbt\nDESCRIPTION: Shows how to select versioned models based on their version identifiers and latest version status.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/methods.md#2025-04-09_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\ndbt list --select \"version:latest\"      # only 'latest' versions\ndbt list --select \"version:prerelease\"  # versions newer than the 'latest' version\ndbt list --select \"version:old\"         # versions older than the 'latest' version\n\ndbt list --select \"version:none\"        # models that are *not* versioned\n```\n\n----------------------------------------\n\nTITLE: Querying Total Mileage by Bike\nDESCRIPTION: This SQL query joins the fact table (fct_daily_mileage) with the dimension table (dim_bikes) to calculate the total mileage accumulated for each bike in the fleet. It demonstrates a simple many-to-one relationship between the fact and dimension tables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-02-modeling-ragged-time-varying-hierarchies.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    dim_bikes.bike_id,\n    sum(fct_daily_mileage.miles) as miles\nfrom\n    fct_daily_mileage\ninner join\n    dim_bikes\n    on\n        fct_daily_mileage.bike_sk = dim_bikes.bike_sk\ngroup by\n    1\n```\n\n----------------------------------------\n\nTITLE: Filtering Models and Tests with GraphQL\nDESCRIPTION: Query example demonstrating how to filter models with error status and failed tests in dbt Cloud. Uses environment ID and pagination parameters to fetch specific results based on execution status.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/discovery-querying.md#2025-04-09_snippet_4\n\nLANGUAGE: graphql\nCODE:\n```\nquery ModelsAndTests($environmentId: BigInt!, $first: Int!) {\n  environment(id: $environmentId) {\n    applied {\n      models(first: $first, filter: { lastRunStatus: error }) {\n        edges {\n          node {\n            name\n            executionInfo {\n              lastRunId\n            }\n          }\n        }\n      }\n      tests(first: $first, filter: { status: \"fail\" }) {\n        edges {\n          node {\n            name\n            executionInfo {\n              lastRunId\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Source Information with GraphQL in dbt\nDESCRIPTION: This GraphQL query retrieves information about a specific source in a dbt job, including its unique ID, source and object names, state, load time, and freshness criteria. It demonstrates how to access source details through the job endpoint using a source's unique ID.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/schema-discovery-job-source.mdx#2025-04-09_snippet_0\n\nLANGUAGE: graphql\nCODE:\n```\n{\n  job(id: 123) {\n    source(uniqueId: \"source.jaffle_shop.snowplow.event\") {\n      uniqueId\n      sourceName\n      name\n      state\n      maxLoadedAt\n      criteria {\n        warnAfter {\n          period\n          count\n        }\n        errorAfter {\n          period\n          count\n        }\n      }\n      maxLoadedAtTimeAgoInS\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Source Information with GraphQL in dbt\nDESCRIPTION: This GraphQL query retrieves information about a specific source in a dbt job, including its unique ID, source and object names, state, load time, and freshness criteria. It demonstrates how to access source details through the job endpoint using a source's unique ID.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/schema-discovery-job-source.mdx#2025-04-09_snippet_0\n\nLANGUAGE: graphql\nCODE:\n```\n{\n  job(id: 123) {\n    source(uniqueId: \"source.jaffle_shop.snowplow.event\") {\n      uniqueId\n      sourceName\n      name\n      state\n      maxLoadedAt\n      criteria {\n        warnAfter {\n          period\n          count\n        }\n        errorAfter {\n          period\n          count\n        }\n      }\n      maxLoadedAtTimeAgoInS\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Metadata Retrieval UDF in Snowflake\nDESCRIPTION: Snowflake Python UDF that retrieves metrics and dimensions metadata from dbt Cloud's Semantic Layer API. Requires external access integration and service token configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-05-02-semantic-layer-llm.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\ncreate or replace function retrieve_sl_metadata()\n    returns object\n    language python\n    runtime_version = 3.9\n    handler = 'main'\n    external_access_integrations = (dbt_cloud_semantic_layer_integration)\n    packages = ('requests')\n    secrets = ('cred' = dbt_cloud_service_token)\nas\n$$\nfrom typing import Dict\nimport _snowflake\nimport requests\n\nquery = \"\"\"\nquery GetMetrics($environmentId: BigInt!) {\n  metrics(environmentId: $environmentId) {\n    description\n    name\n    queryableGranularities\n    type\n    dimensions {\n      description\n      name\n      type\n    }\n  }\n}\n\"\"\"\n\ndef main():\n    session = requests.Session()\n    token = _snowflake.get_generic_secret_string('cred')\n    session.headers = {'Authorization': f'Bearer {token}'}\n\n    # TODO: Update for your environment ID\n    payload = {\"query\": query, \"variables\": {\"environmentId\": 1}}\n\n    # TODO: Update for your deployment type\n    response = session.post(\"https://semantic-layer.cloud.getdbt.com/api/graphql\", json=payload)\n    response.raise_for_status()\n    return response.json()\n\n$$;\n\ngrant usage on function retrieve_sl_metadata() to role public;\n```\n\n----------------------------------------\n\nTITLE: Configuring ClickHouse Profile in YAML for dbt\nDESCRIPTION: This YAML snippet shows how to configure a ClickHouse profile in the profiles.yml file for dbt. It includes all possible configuration options with their default values and descriptions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/clickhouse-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n<profile-name>:\n  target: <target-name>\n  outputs:\n    <target-name>:\n      type: clickhouse\n      schema: [ default ] # ClickHouse database for dbt models\n\n      # optional\n      driver: [ http ] # http or native.  If not configured, this will be auto-determined based on the port setting\n      host: [ localhost ]\n      port: [ 8123 ]  # Defaults to 8123, 8443, 9000, 9440 depending on the secure and driver settings \n      user: [ default ] # User for all database operations\n      password: [ <empty string> ] # Password for the user\n      cluster: [ <empty string> ] # If configured, certain DDL/table operations will be executed with the `ON CLUSTER` clause using this cluster. Distributed materializations require this setting to work. See the following ClickHouse Cluster section for more details.\n      verify: [ True ] # Validate TLS certificate if using TLS/SSL\n      secure: [ False ] # Use TLS (native protocol) or HTTPS (http protocol)\n      retries: [ 1 ] # Number of times to retry a \"retriable\" database exception (such as a 503 'Service Unavailable' error)\n      compression: [ <empty string> ] # Use gzip compression if truthy (http), or compression type for a native connection\n      connect_timeout: [ 10 ] # Timeout in seconds to establish a connection to ClickHouse\n      send_receive_timeout: [ 300 ] # Timeout in seconds to receive data from the ClickHouse server\n      cluster_mode: [ False ] # Use specific settings designed to improve operation on Replicated databases (recommended for ClickHouse Cloud)\n      use_lw_deletes: [ False ] # Use the strategy `delete+insert` as the default incremental strategy.\n      check_exchange: [ True ] # Validate that clickhouse support the atomic EXCHANGE TABLES command.  (Not needed for most ClickHouse versions)\n      local_suffix: [ _local ] # Table suffix of local tables on shards for distributed materializations.\n      local_db_prefix: [ <empty string> ] # Database prefix of local tables on shards for distributed materializations. If empty, it uses the same database as the distributed table.\n      allow_automatic_deduplication: [ False ] # Enable ClickHouse automatic deduplication for Replicated tables\n      tcp_keepalive: [ False ] # Native client only, specify TCP keepalive configuration. Specify custom keepalive settings as [idle_time_sec, interval_sec, probes].\n      custom_settings: [ { } ] # A dictionary/mapping of custom ClickHouse settings for the connection - default is empty.\n\n      # Native (clickhouse-driver) connection settings\n      sync_request_timeout: [ 5 ] # Timeout for server ping\n      compress_block_size: [ 1048576 ] # Compression block size if compression is enabled\n```\n\n----------------------------------------\n\nTITLE: Successful Unit Test Execution in Shell\nDESCRIPTION: Shell command output showing a successful unit test run after fixing the regex pattern issue in the email validation logic.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/unit-tests.md#2025-04-09_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndbt test --select test_is_valid_email_address\n16:09:11  Running with dbt=1.8.0-a1\n16:09:12  Registered adapter: postgres=1.8.0-a1\n16:09:12  Found 6 models, 5 seeds, 4 data tests, 0 sources, 0 exposures, 0 metrics, 410 macros, 0 groups, 0 semantic models, 1 unit test\n16:09:12  \n16:09:13  Concurrency: 5 threads (target='postgres')\n16:09:13  \n16:09:13  1 of 1 START unit_test dim_customers::test_is_valid_email_address ................... [RUN]\n16:09:13  1 of 1 PASS dim_customers::test_is_valid_email_address .............................. [PASS in 0.26s]\n16:09:13  \n16:09:13  Finished running 1 unit_test in 0 hours 0 minutes and 0.75 seconds (0.75s).\n16:09:13  \n16:09:13  Completed successfully\n16:09:13  \n16:09:13  Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1\n```\n\n----------------------------------------\n\nTITLE: Configuring Quoting for Approximate Match Error\nDESCRIPTION: Adjusts quoting configuration to resolve approximate relation match errors in dbt_project.yaml.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/oracle-setup.md#2025-04-09_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\nquoting:\n  database: true\n```\n\n----------------------------------------\n\nTITLE: Installing dbt-core from GitHub Source\nDESCRIPTION: Commands to clone the dbt-core repository from GitHub and install it using pip. These commands download the source code and install the required dependencies listed in requirements.txt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/source-install.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/dbt-labs/dbt-core.git\ncd dbt-core\npython -m pip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Boolean Config Flag Structure\nDESCRIPTION: Template showing the structure for using boolean configuration flags with dbt commands.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/command-line-options.md#2025-04-09_snippet_4\n\nLANGUAGE: text\nCODE:\n```\ndbt <SUBCOMMAND> --<THIS-CONFIG> \ndbt <SUBCOMMAND> --no-<THIS-CONFIG> \n```\n\n----------------------------------------\n\nTITLE: Configuring Lakeformation Tags at Project Level in YAML for dbt-glue\nDESCRIPTION: Example of configuring Lakeformation tags at the dbt project level, specifying tags for seeds and models separately.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/glue-setup.md#2025-04-09_snippet_22\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  +lf_tags_config:\n    enabled: true\n    tags_table: \n      name_of_my_table_tag: 'value_of_my_table_tag'  \n    tags_database: \n      name_of_my_database_tag: 'value_of_my_database_tag'\nmodels:\n  +lf_tags_config:\n    enabled: true\n    drop_existing: True\n    tags_database: \n      name_of_my_database_tag: 'value_of_my_database_tag'\n    tags_table: \n      name_of_my_table_tag: 'value_of_my_table_tag'\n```\n\n----------------------------------------\n\nTITLE: Compiled SQL with Excessive Whitespace\nDESCRIPTION: Example of how the compiled SQL looks with excessive whitespace when Jinja whitespace control is not used.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/using-jinja.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n\n\nselect\norder_id,\n\nsum(case when payment_method = 'bank_transfer' then amount end) as bank_transfer_amount\n,\n\nsum(case when payment_method = 'credit_card' then amount end) as credit_card_amount\n,\n\nsum(case when payment_method = 'gift_card' then amount end) as gift_card_amount\n\n\nfrom raw_jaffle_shop.payments\ngroup by 1\n```\n\n----------------------------------------\n\nTITLE: Generating SQL from MetricFlow Query\nDESCRIPTION: Command for open-source users to generate SQL code from MetricFlow queries without using dbt Cloud services. This command explains the query and outputs SQL that can be used in other systems.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_sl-faqs.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmf query --explain\n```\n\n----------------------------------------\n\nTITLE: Creating Staging Model for Customers in SQL\nDESCRIPTION: This SQL snippet creates a staging model for customer data, selecting and renaming relevant columns from the source table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/starburst-galaxy-qs.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    id as customer_id,\n    first_name,\n    last_name\n\nfrom dbt_quickstart.jaffle_shop.jaffle_shop_customers\n```\n\n----------------------------------------\n\nTITLE: Running dbt Models with OpenLineage Integration\nDESCRIPTION: Executing 'dbt-ol run' to run all dbt models while capturing lineage metadata and sending it to a Marquez instance. This process creates views and tables for analyzing community metrics across different platforms.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-11-29-open-source-community-growth.md#2025-04-09_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n% dbt-ol run\nRunning OpenLineage dbt wrapper version 0.3.1\nThis wrapper will send OpenLineage events at the end of dbt execution.\nRunning with dbt=1.0.0\nFound 7 models, 0 tests, 0 snapshots, 0 analyses, 184 macros, 0 operations, 2 seed files, 4 sources, 0 exposures\n\n18:44:15 | Concurrency: 1 threads (target='dev')\n18:44:15 | \n18:44:15 | 1 of 7 START view model metrics.dockerhub_daily_summary.............. [RUN]\n18:44:16 | 1 of 7 OK created view model metrics.dockerhub_daily_summary......... [OK in 0.98s]\n18:44:16 | 2 of 7 START table model metrics.github_daily_summary................ [RUN]\n18:44:19 | 2 of 7 OK created table model metrics.github_daily_summary........... [CREATE TABLE (1.6k rows, 79.1 KB processed) in 3.03s]\n18:44:19 | 3 of 7 START incremental model metrics.pypi_downloads................ [RUN]\n18:44:44 | 3 of 7 OK created incremental model metrics.pypi_downloads........... [MERGE (1.7k rows, 7.2 GB processed) in 25.10s]\n18:44:44 | 4 of 7 START view model metrics.slack_daily_summary.................. [RUN]\n18:44:45 | 4 of 7 OK created view model metrics.slack_daily_summary............. [OK in 0.81s]\n18:44:45 | 5 of 7 START view model metrics.slack_daily_summary_by_user.......... [RUN]\n18:44:46 | 5 of 7 OK created view model metrics.slack_daily_summary_by_user..... [OK in 0.87s]\n18:44:46 | 6 of 7 START table model metrics.pypi_daily_summary.................. [RUN]\n18:44:49 | 6 of 7 OK created table model metrics.pypi_daily_summary............. [CREATE TABLE (35.9k rows, 4.7 MB processed) in 3.04s]\n18:44:49 | 7 of 7 START view model metrics.daily_summary........................ [RUN]\n18:44:50 | 7 of 7 OK created view model metrics.daily_summary................... [OK in 0.87s]\n18:44:50 | \n18:44:50 | Finished running 4 view models, 2 table models, 1 incremental model in 35.55s.\n\nCompleted successfully\n\nDone. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7\nEmitting OpenLineage events: 100%|| 14/14 [00:01<00:00,  7.89it/s]\nEmitted 16 openlineage events\n```\n\n----------------------------------------\n\nTITLE: Versioned ref Function Usage\nDESCRIPTION: Shows how to use the ref function with versioned models by specifying the version parameter. This allows referencing specific versions of models that may contain breaking changes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/ref.md#2025-04-09_snippet_2\n\nLANGUAGE: yml\nCODE:\n```\nmodels:\n  - name: model_name\n    latest_version: 2\n    versions:\n      - v: 2\n      - v: 1\n```\n\nLANGUAGE: sql\nCODE:\n```\n -- returns the `Relation` object corresponding to version 1 of model_name\nselect * from {{ ref('model_name', version=1) }}\n```\n\nLANGUAGE: sql\nCODE:\n```\n -- returns the `Relation` object corresponding to version 2 (the latest version) of model_name\nselect * from {{ ref('model_name') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring SSO Credentials in dbt Cloud for Okta Integration\nDESCRIPTION: This code snippet shows the necessary fields and values to be configured in dbt Cloud's Enterprise > Single Sign On settings page to complete the Okta SSO integration. It includes important notes about login slugs and certificate expiration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/manage-access/set-up-sso-okta.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Field | Value |\n| ----- | ----- |\n| **Log&nbsp;in&nbsp;with** | Okta |\n| **Identity&nbsp;Provider&nbsp;SSO&nbsp;Url** | Paste the **Identity Provider Single Sign-On URL** shown in the Okta setup instructions |\n| **Identity&nbsp;Provider&nbsp;Issuer** | Paste the **Identity Provider Issuer** shown in the Okta setup instructions |\n| **X.509&nbsp;Certificate** | Paste the **X.509 Certificate** shown in the Okta setup instructions; <br />**Note:** When the certificate expires, an Okta admin will have to generate a new one to be pasted into dbt Cloud for uninterrupted application access. |\n| **Slug** | Enter your desired login slug. Users will be able to log into dbt Cloud by navigating to `https://YOUR_ACCESS_URL/enterprise-login/LOGIN-SLUG`, replacing `YOUR_ACCESS_URL` with the [appropriate Access URL](/docs/cloud/about-cloud/access-regions-ip-addresses) for your region and plan. Login slugs must be unique across all dbt Cloud accounts, so pick a slug that uniquely identifies your company. |\n```\n\n----------------------------------------\n\nTITLE: Using Behavior Flags in Jinja2 Macros\nDESCRIPTION: Demonstrates how to access and use behavior flags within Jinja2 macros to conditionally execute different SQL code paths based on enabled features.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_16\n\nLANGUAGE: sql\nCODE:\n```\n{% macro some_macro(**kwargs) %}\n    {% if adapter.behavior.enable_new_functionality_requiring_higher_permissions %}\n        {# do the new thing #}\n    {% else %}\n        {# do the old thing #}\n    {% endif %}\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Marking Real Differences in Data Using SQL Window Functions\nDESCRIPTION: SQL code to identify real differences in data by comparing current and previous grain_id values using window functions. This helps in identifying unique rows for deduplication.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-04-19-complex-deduplication.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nmark_real_diffs as (\n\n  select\n      *,\n      coalesce(\n          lag(grain_id) over (partition by entity_id order by updated_at_date),\n          'first_record'\n      ) as previous_grain_id,\n      case\n          when grain_id != previous_grain_id then true \n          else false\n      end as is_real_diff\n\n  from base_product\n\n),\n```\n\n----------------------------------------\n\nTITLE: Querying Metrics with the MetricFlow CLI\nDESCRIPTION: Example command showing how to query a metric with a dimension using the MetricFlow CLI. This is used to test metric queries after migrating to the new Semantic Layer.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/sl-migration.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmf query --metrics revenue --group-by metric_time\n```\n\n----------------------------------------\n\nTITLE: Compiled SQL Output for Inline Query\nDESCRIPTION: This is the compiled SQL output for the inline query, showing a simple select statement from the raw_orders table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/compile.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nselect * from \"jaffle_shop\".\"main\".\"raw_orders\"\n```\n\n----------------------------------------\n\nTITLE: Explicitly Setting Latest Version for a dbt Model with Multiple Versions in YAML\nDESCRIPTION: This snippet demonstrates how to explicitly set the 'latest_version' property for a dbt model with multiple versions. It shows setting the latest version to 2, even though version 3 exists, which makes version 3 a 'prerelease' version.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/latest_version.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: model_name\n    latest_version: 2\n    versions:\n      - v: 3\n      - v: 2\n      - v: 1\n```\n\n----------------------------------------\n\nTITLE: CSV Export Query\nDESCRIPTION: Command to export MetricFlow query results to a CSV file in dbt Core.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metricflow-commands.md#2025-04-09_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nmf query --metrics order_total --group-by metric_time,is_food_order --limit 10 --order-by -metric_time --where \"is_food_order = True\" --start-time '2017-08-22' --end-time '2017-08-27' --csv query_example.csv\n```\n\n----------------------------------------\n\nTITLE: Incorrect Schema Name Generation Example\nDESCRIPTION: An example of what not to do when customizing the schema generation macro. This implementation would cause conflicts as it drops the target schema prefix, causing multiple developers to write to the same schema.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/custom-schemas.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{% macro generate_schema_name(custom_schema_name, node) -%}\n\n    {%- set default_schema = target.schema -%}\n    {%- if custom_schema_name is none -%}\n\n        {{ default_schema }}\n\n    {%- else -%}\n    # The following is incorrect as it omits {{ default_schema }} before {{ custom_schema_name | trim }}. \n        {{ custom_schema_name | trim }} \n\n    {%- endif -%}\n\n{%- endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Environment-Based Authentication Profile\nDESCRIPTION: YAML configuration for dbt profiles.yml using environment-based authentication method that dynamically selects authentication based on available environment variables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/fabric-setup.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: fabric\n      driver: 'ODBC Driver 18 for SQL Server'\n      server: hostname or IP of your server\n      port: 1433\n      database: exampledb\n      schema: schema_name\n      authentication: environment\n```\n\n----------------------------------------\n\nTITLE: Using pytz Module for Timezone Operations\nDESCRIPTION: Shows how to work with timezones using pytz module, including creating a datetime object and localizing it to US/Eastern timezone.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/modules.md#2025-04-09_snippet_1\n\nLANGUAGE: jinja\nCODE:\n```\n{% set dt = modules.datetime.datetime(2002, 10, 27, 6, 0, 0) %}\n{% set dt_local = modules.pytz.timezone('US/Eastern').localize(dt) %}\n{{ dt_local }}\n```\n\n----------------------------------------\n\nTITLE: Variable Definition in dbt Project Configuration\nDESCRIPTION: Configuration example showing how to define variables in dbt_project.yml file. This demonstrates the proper YAML structure for declaring project variables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/var.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nname: my_dbt_project\nversion: 1.0.0\n\nconfig-version: 2\n\n# Define variables here\nvars:\n  event_type: activation\n```\n\n----------------------------------------\n\nTITLE: Applying Configurations to All Snapshots\nDESCRIPTION: This snippet demonstrates how to apply a configuration to all snapshots, including those in installed packages, using the dbt_project.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/snapshot-configs.md#2025-04-09_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  +unique_key: id\n```\n\n----------------------------------------\n\nTITLE: New Data Tests Syntax in YML\nDESCRIPTION: Updated syntax for defining data tests in YML configuration using the new data_tests keyword.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/core-upgrade/07-upgrading-to-v1.8.md#2025-04-09_snippet_3\n\nLANGUAGE: yml\nCODE:\n```\nmodels:\n  - name: orders\n    columns:\n      - name: order_id\n        data_tests:\n          - unique\n          - not_null\n```\n\n----------------------------------------\n\nTITLE: Setting up dbt Cloud Semantic Layer Integration\nDESCRIPTION: SQL commands to configure network rules, external access integration, and secrets for connecting to dbt Cloud Semantic Layer.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-05-02-semantic-layer-llm.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\ngrant create network rule on schema <database_name>.<schema_name> to role public;\ngrant create secret on schema <database_name>.<schema_name> to role public;\n\nuse database <database_name>;\nuse schema <schema_name>;\n\ncreate or replace network rule dbt_cloud_semantic_layer_rule\n    mode = egress\n    type = host_port\n    value_list = (\n        'semantic-layer.cloud.getdbt.com',\n        'semantic-layer.emea.dbt.com',\n        'semantic-layer.au.dbt.com'\n    );\n\ncreate or replace secret dbt_cloud_service_token\n    type = generic_string\n    secret_string = '<service_token>';\n\ncreate or replace external access integration dbt_cloud_semantic_layer_integration\n    allowed_network_rules = (dbt_cloud_semantic_layer_rule)\n    allowed_authentication_secrets = (dbt_cloud_service_token)\n    enabled = true;\n\ngrant usage on integration dbt_cloud_semantic_layer_integration to role public;\ngrant ownership on secret dbt_cloud_service_token to role public;\ngrant usage on secret dbt_cloud_service_token to role public;\n```\n\n----------------------------------------\n\nTITLE: AWS IAM Policy Configuration for Glue Integration\nDESCRIPTION: IAM policy template defining permissions for dbt-glue adapter including access to Glue databases, Lake Formation, and S3 buckets. Contains specific permissions for read/write operations on output databases and read-only access to source databases.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/glue-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Read_and_write_databases\",\n            \"Action\": [\n                \"glue:SearchTables\",\n                \"glue:BatchCreatePartition\",\n                \"glue:CreatePartitionIndex\",\n                \"glue:DeleteDatabase\",\n                \"glue:GetTableVersions\",\n                \"glue:GetPartitions\",\n                \"glue:DeleteTableVersion\",\n                \"glue:UpdateTable\",\n                \"glue:DeleteTable\",\n                \"glue:DeletePartitionIndex\",\n                \"glue:GetTableVersion\",\n                \"glue:UpdateColumnStatisticsForTable\",\n                \"glue:CreatePartition\",\n                \"glue:UpdateDatabase\",\n                \"glue:CreateTable\",\n                \"glue:GetTables\",\n                \"glue:GetDatabases\",\n                \"glue:GetTable\",\n                \"glue:GetDatabase\",\n                \"glue:GetPartition\",\n                \"glue:UpdateColumnStatisticsForPartition\",\n                \"glue:CreateDatabase\",\n                \"glue:BatchDeleteTableVersion\",\n                \"glue:BatchDeleteTable\",\n                \"glue:DeletePartition\",\n                \"glue:GetUserDefinedFunctions\",\n                \"lakeformation:ListResources\",\n                \"lakeformation:BatchGrantPermissions\",\n                \"lakeformation:ListPermissions\", \n                \"lakeformation:GetDataAccess\",\n                \"lakeformation:GrantPermissions\",\n                \"lakeformation:RevokePermissions\",\n                \"lakeformation:BatchRevokePermissions\",\n                \"lakeformation:AddLFTagsToResource\",\n                \"lakeformation:RemoveLFTagsFromResource\",\n                \"lakeformation:GetResourceLFTags\",\n                \"lakeformation:ListLFTags\",\n                \"lakeformation:GetLFTag\"\n            ],\n            \"Resource\": [\n                \"arn:aws:glue:<region>:<AWS Account>:catalog\",\n                \"arn:aws:glue:<region>:<AWS Account>:table/<dbt output database>/*\",\n                \"arn:aws:glue:<region>:<AWS Account>:database/<dbt output database>\"\n            ],\n            \"Effect\": \"Allow\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Example of dbt Structured JSON Event Format\nDESCRIPTION: An example of dbt's structured JSON event format showing a model execution start event. The JSON structure includes common metadata in the 'info' section and event-specific data in the 'data' section, including node information and execution context.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/events-logging.md#2025-04-09_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"data\": {\n    \"description\": \"sql view model dbt_jcohen.my_model\",\n    \"index\": 1,\n    \"node_info\": {\n      \"materialized\": \"view\",\n      \"meta\": {\n        \"first\": \"some_value\",\n        \"second\": \"1234\"\n      },\n      \"node_finished_at\": \"\",\n      \"node_name\": \"my_model\",\n      \"node_path\": \"my_model.sql\",\n      \"node_relation\": {\n        \"alias\": \"my_model\",\n        \"database\": \"my_database\",\n        \"relation_name\": \"\\\"my_database\\\".\\\"my_schema\\\".\\\"my_model\\\"\",\n        \"schema\": \"my_schema\"\n      },\n      \"node_started_at\": \"2023-04-12T19:27:27.435364\",\n      \"node_status\": \"started\",\n      \"resource_type\": \"model\",\n      \"unique_id\": \"model.my_dbt_project.my_model\"\n    },\n    \"total\": 1\n  },\n  \"info\": {\n    \"category\": \"\",\n    \"code\": \"Q011\",\n    \"extra\": {\n      \"my_custom_env_var\": \"my_custom_value\"\n    },\n    \"invocation_id\": \"206b4e61-8447-4af7-8035-b174ab3ac991\",\n    \"level\": \"info\",\n    \"msg\": \"1 of 1 START sql view model my_database.my_model ................................ [RUN]\",\n    \"name\": \"LogStartLine\",\n    \"pid\": 95894,\n    \"thread\": \"Thread-1\",\n    \"ts\": \"2023-04-12T19:27:27.436283Z\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Incorrect Absolute Path Configuration\nDESCRIPTION: Demonstrates an anti-pattern of using absolute paths in model-paths configuration, which should be avoided.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/model-paths.md#2025-04-09_snippet_2\n\nLANGUAGE: yml\nCODE:\n```\nmodel-paths: [\"/Users/username/project/models\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring User Attribute Statements in Okta for dbt Cloud SSO\nDESCRIPTION: This snippet shows the expected User Attribute Statements to be configured in Okta for proper integration with dbt Cloud. It maps Okta user attributes to the format expected by dbt Cloud.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/manage-access/set-up-sso-okta.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Name           | Name format | Value                | Description                |\n| -------------- | ----------- | -------------------- | -------------------------- |\n| `email`        | Unspecified | `user.email`      | _The user's email address_ |\n| `first_name`   | Unspecified | `user.firstName`  | _The user's first name_    |\n| `last_name`    | Unspecified | `user.lastName`   | _The user's last name_     |\n```\n\n----------------------------------------\n\nTITLE: Configuring Fact Tables in dbt_project.yml for Firebolt\nDESCRIPTION: Project-level configuration to define fact tables in Firebolt with primary indexes and optional aggregating indexes. Specifies the table materialization type and indexing strategy.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/firebolt-configs.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  <resource-path>:\n    +materialized: table\n    +table_type: fact\n    +primary_index: [ <column-name>, ... ]\n    +indexes:\n      - index_type: aggregating\n        key_columns: [ <column-name>, ... ]\n        aggregation: [ <agg-sql>, ... ]\n      ...\n```\n\n----------------------------------------\n\nTITLE: Configuring LDAP Authentication for Impala in dbt\nDESCRIPTION: Configuration for connecting to Impala using LDAP authentication. This is the recommended method for Cloudera Data Platform (CDP) and supports both Binary and HTTP connection mechanisms.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/impala-setup.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n     type: impala\n     host: [host name]\n     http_path: [optional, http path to Impala]\n     port: [port] # default value: 21050\n     auth_type: ldap\n     use_http_transport: [true / false] # default value: true\n     use_ssl: [true / false] # TLS should always be used with LDAP to ensure secure transmission of credentials, default value: true\n     username: [username]\n     password: [password]\n     dbname: [db name]  # this should be same as schema name provided below, starting with 1.1.2 this parameter is optional\n     schema: [schema name]\n     retries: [retries] # number of times impyla attempts retry conneciton to warehouse, default value: 3\n  \n```\n\n----------------------------------------\n\nTITLE: Implementing generate_database_name Macro in Jinja\nDESCRIPTION: The default implementation of the generate_database_name macro that controls how dbt generates database names. This macro accepts a custom database name and node, returning either the custom name if provided or the target database.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/custom-databases.md#2025-04-09_snippet_2\n\nLANGUAGE: jinja2\nCODE:\n```\n{% macro generate_database_name(custom_database_name=none, node=none) -%}\n\n    {%- set default_database = target.database -%}\n    {%- if custom_database_name is none -%}\n\n        {{ default_database }}\n\n    {%- else -%}\n\n        {{ custom_database_name | trim }}\n\n    {%- endif -%}\n\n{%- endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Creating Customer Payment Journey Analysis Model in SQL\nDESCRIPTION: SQL model that analyzes payment patterns across customer journey by joining staging payments with fact orders and using dbt_utils pivot macro for payment method analysis.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/mesh-qs.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\nwith stg_payments as (\n    select * from {{ ref('stg_payments') }}\n),\n\nfct_orders as (\n    select * from {{ ref('analytics', 'fct_orders') }}\n),\n\nfinal as (\n    select \n        days_as_customer_at_purchase,\n        {{ dbt_utils.pivot(\n            'payment_method',\n            dbt_utils.get_column_values(ref('stg_payments'), 'payment_method'),\n            agg='sum',\n            then_value='amount',\n            prefix='total_',\n            suffix='_amount'\n        ) }}, \n        sum(amount) as total_amount\n    from fct_orders\n    left join stg_payments using (order_id)\n    group by 1\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Creating Customer Payment Journey Analysis Model in SQL\nDESCRIPTION: SQL model that analyzes payment patterns across customer journey by joining staging payments with fact orders and using dbt_utils pivot macro for payment method analysis.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/mesh-qs.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\nwith stg_payments as (\n    select * from {{ ref('stg_payments') }}\n),\n\nfct_orders as (\n    select * from {{ ref('analytics', 'fct_orders') }}\n),\n\nfinal as (\n    select \n        days_as_customer_at_purchase,\n        {{ dbt_utils.pivot(\n            'payment_method',\n            dbt_utils.get_column_values(ref('stg_payments'), 'payment_method'),\n            agg='sum',\n            then_value='amount',\n            prefix='total_',\n            suffix='_amount'\n        ) }}, \n        sum(amount) as total_amount\n    from fct_orders\n    left join stg_payments using (order_id)\n    group by 1\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Null Value Problem in Surrogate Key Generation\nDESCRIPTION: This SQL snippet illustrates the issue with null values when creating surrogate keys using simple concatenation. It shows how null values in any column can result in a null surrogate key.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-11-22-sql-surrogate-keys.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nwith \n\nexample_ids as (\n  select\n  123 as user_id,\n  123 as product_id\n\n  union all\n\n  select\n  123 as user_id,\n  null as product_id\n\n  union all\n\n  select\n  null as user_id,\n  123 as product_id\n\n)\n\nselect\n  *,\n  concat(user_id, product_id) as _surrogate_key\nfrom example_ids\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment ID for Defer in dbt Cloud CLI using dbt_cloud.yml\nDESCRIPTION: Configure the defer-env-id in the dbt_cloud.yml file to specify which environment should be used for deferral artifacts. This allows manual control over which environment's metadata should be used when resolving references.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/about-cloud-develop-defer.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  active-host: ...\n  active-project: ...\n  defer-env-id: '123456'\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Alias in YAML\nDESCRIPTION: Example of setting a custom alias for a model using a schema.yml configuration file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/custom-aliases.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: ga_sessions\n    config:\n      alias: sessions\n```\n\n----------------------------------------\n\nTITLE: Querying Customer Orders in Databricks\nDESCRIPTION: This SQL query analyzes customer ordering patterns in Databricks by joining customer and order data. It calculates metrics such as first order date, most recent order date, and total number of orders. The query references tables directly by name without project or schema prefixes as per Databricks conventions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/tutorial-sql-query.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nwith customers as (\n\n    select\n        id as customer_id,\n        first_name,\n        last_name\n\n    from jaffle_shop_customers\n\n),\n\norders as (\n\n    select\n        id as order_id,\n        user_id as customer_id,\n        order_date,\n        status\n\n    from jaffle_shop_orders\n\n),\n\ncustomer_orders as (\n\n    select\n        customer_id,\n\n        min(order_date) as first_order_date,\n        max(order_date) as most_recent_order_date,\n        count(order_id) as number_of_orders\n\n    from orders\n\n    group by 1\n\n),\n\nfinal as (\n\n    select\n        customers.customer_id,\n        customers.first_name,\n        customers.last_name,\n        customer_orders.first_order_date,\n        customer_orders.most_recent_order_date,\n        coalesce(customer_orders.number_of_orders, 0) as number_of_orders\n\n    from customers\n\n    left join customer_orders using (customer_id)\n\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Using the source() Function to Declare Raw Data Dependencies\nDESCRIPTION: The source() function is used to reference raw data sources in dbt models. This is introduced in the toddlerhood stage to properly define and document data sources.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-12-05-how-to-build-a-mature-dbt-project-from-scratch.md#2025-04-09_snippet_1\n\nLANGUAGE: jinja-sql\nCODE:\n```\n{{ source() }}\n```\n\n----------------------------------------\n\nTITLE: Running dbt with Profile Override\nDESCRIPTION: Command line example showing how to run dbt with specific profile and target overrides.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/connection-profiles.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --profile my-profile-name --target dev\n```\n\n----------------------------------------\n\nTITLE: Alternative syntax for dbt invocation help in shell\nDESCRIPTION: Shows the alternative method to access help documentation for the invocation command using the global help command followed by the specific command name.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/invocation.md#2025-04-09_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndbt help invocation\n```\n\n----------------------------------------\n\nTITLE: Using Custom Identifier for Source Table in dbt\nDESCRIPTION: This example shows how to use a simpler name for a source table in dbt than the one in the database. It defines a source 'jaffle_shop' with a table 'orders' that has the identifier 'api_orders' in the database.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/identifier.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: jaffle_shop\n    tables:\n      - name: orders\n        identifier: api_orders\n```\n\n----------------------------------------\n\nTITLE: Configuring Quoting for dbt Sources in YAML\nDESCRIPTION: This YAML snippet demonstrates how to use the 'quoting' property in a dbt source configuration. It shows how to enable quoting for database, schema, and identifier at the source level, and how to override quoting for a specific table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Models/source-quotes.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: jaffle_shop\n    database: raw\n    quoting:\n      database: true\n      schema: true\n      identifier: true\n\n    tables:\n      - name: order_items\n      - name: orders\n        # This overrides the `jaffle_shop` quoting config\n        quoting:\n          identifier: false\n```\n\n----------------------------------------\n\nTITLE: Traversing Component Hierarchies Using Recursive SQL\nDESCRIPTION: This SQL query traverses a component hierarchy using a recursive CTE to find all components belonging to the same top assembly. It handles installation and removal timestamps, tracks hierarchy depth, detects circular dependencies, and ensures valid time ranges where both parent and child components are installed.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-02-modeling-ragged-time-varying-hierarchies.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nwith recursive\n-- Contains our source data with records that link a child to a parent\ncomponents as (\n    select\n        *,\n        -- Valid dates start as installed/removed, but may be modified as we traverse the hierarchy below\n        installed_at as valid_from_at,\n        removed_at as valid_to_at\n    from\n        erp_components\n),\n\n-- Get all the source records that are at the top of hierarchy\ntop_assemblies as (\n    select * from components where assembly_id is null\n),\n\n-- This is where the recursion happens that traverses the hierarchy\ntraversal as (\n    -- Start at the top of hierarchy\n    select\n        -- Keep track of the depth as we traverse down\n        0 as component_hierarchy_depth,\n        -- Flag to determine if we've entered a circular relationship\n        false as is_circular,\n        -- Define an array that will keep track of all of the ancestors of a component\n        [component_id] as component_trace,\n        -- At the top of the hierarchy, the component is the top assembly\n        component_id as top_assembly_id,\n\n        assembly_id,\n        component_id,\n\n        installed_at,\n        removed_at,\n        valid_from_at,\n        valid_to_at\n    from\n        top_assemblies\n\n    union all\n\n    -- Join the current layer of the hierarchy with the next layer down by linking\n    -- the current component id to the assembly id of the child\n    select\n        traversal.component_hierarchy_depth + 1 as component_hierarchy_depth,\n        -- Check for any circular dependencies\n        array_contains(components.component_id::variant, traversal.component_trace) as is_circular,\n        -- Append trace array\n        array_append(traversal.component_trace, components.component_id) as component_trace,\n        -- Keep track of the top of the assembly\n        traversal.top_assembly_id,\n\n        components.assembly_id,\n        components.component_id,\n\n        components.installed_at,\n        components.removed_at,\n        -- As we recurse down the hierarchy, only want to consider time ranges where both\n        -- parent and child are installed; so choose the latest \"from\" timestamp and the earliest \"to\".\n        greatest(traversal.valid_from_at, components.valid_from_at) as valid_from_at,\n        least(traversal.valid_to_at, components.valid_to_at) as valid_to_at\n    from\n        traversal\n    inner join\n        components\n        on\n            traversal.component_id = components.assembly_id\n            and\n            -- Exclude component assemblies that weren't installed at the same time\n            -- This may happen due to source data quality issues\n            (\n                traversal.valid_from_at < components.valid_to_at\n                and\n                traversal.valid_to_at >= components.valid_from_at\n            )\n    where\n        -- Stop if a circular hierarchy is detected\n        not array_contains(components.component_id::variant, traversal.component_trace)\n        -- There can be some bad data that might end up in hierarchies that are artificially extremely deep\n        and traversal.component_hierarchy_depth < 20\n),\n\nfinal as (\n    -- Note that there may be duplicates at this point (thus \"distinct\").\n    -- Duplicates can happen when a component's parent is moved from one grandparent to another.\n    -- At this point, we only traced the ancestry of a component, and fixed the valid/from dates\n    -- so that all child ranges are contained in parent ranges.\n\n    select distinct *\n    from\n        traversal\n    where\n        -- Prevent zero-time (or less) associations from showing up\n        valid_from_at < valid_to_at\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Referencing Aliased Models\nDESCRIPTION: Example showing how to reference models using ref() function, using the model's filename regardless of any aliasing configurations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/custom-aliases.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n-- Use the model's filename in ref's, regardless of any aliasing configs\n\nselect * from {{ ref('ga_sessions') }}\nunion all\nselect * from {{ ref('snowplow_sessions') }}\n```\n\n----------------------------------------\n\nTITLE: Raising Compiler Errors in dbt with exceptions.raise_compiler_error\nDESCRIPTION: This snippet demonstrates how to use exceptions.raise_compiler_error to halt execution when input validation fails. In this example, it checks if a number is within a valid range (0-100) and raises an error with a descriptive message if the validation fails.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/exceptions.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{% if number < 0 or number > 100 %}\n  {{ exceptions.raise_compiler_error(\"Invalid `number`. Got: \" ~ number) }}\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Updating ERP Component Data for Wheel Replacement\nDESCRIPTION: This SQL-like table structure demonstrates how the ERP system is updated when a major component (wheel) is replaced, affecting multiple sub-components. It shows the removal of old components and installation of new ones.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-02-modeling-ragged-time-varying-hierarchies.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n| assembly_id | component_id | installed_at | removed_at |\n| -             | -              | -              | -            |\n| Bike-1        | Wheel-1        | 2023-01-01     | 2023-08-01   |\n| Wheel-1       | Rim-1          | 2023-01-01     | 2023-08-01   |\n| Wheel-1       | Tire-1         | 2023-01-01     | 2023-08-01   |\n| Tire-1        | Tube-2         | 2023-06-01     | 2023-08-01   |\n| Bike-1        | Wheel-2        | 2023-08-01     |              |\n| Wheel-2       | Rim-2          | 2023-08-01     |              |\n| Wheel-2       | Tire-2         | 2023-08-01     |              |\n| Tire-2        | Tube-3         | 2023-08-01     |              |\n```\n\n----------------------------------------\n\nTITLE: Custom Materialization Override\nDESCRIPTION: Example of overriding a built-in materialization from the root project using a wrapping materialization.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/core-upgrade/07-upgrading-to-v1.8.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{% materialization view, default %}\n{{ return(my_cool_package.materialization_view_default()) }}\n{% endmaterialization %}\n```\n\n----------------------------------------\n\nTITLE: Updating dbt Cloud Webhook Configuration\nDESCRIPTION: PUT request to update an existing webhook's configuration including event types, name, URL, and job IDs.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/webhooks.md#2025-04-09_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nPUT https://{your access URL}/api/v3/accounts/{account_id}/webhooks/subscription/{webhook_id}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n\t\"event_types\": [\n\t\t\t\"job.run.started\"\n\t],\n\t\"name\": \"Webhook for jobs\",\n\t\"client_url\": \"https://test.com\",\n\t\"active\": true,\n\t\"description\": \"A webhook for when jobs are started\",\n\t\"job_ids\": [\n\t\t\t123,\n\t\t\t321\n\t]\n}\n```\n\n----------------------------------------\n\nTITLE: Month Selection CTE for Historical Tracking\nDESCRIPTION: Creates a CTE that combines current date with historical month values from a calendar dimension table. This enables tracking of customer segments across different time periods.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-08-building-a-historical-user-segmentation-model-with-dbt.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nmonths AS(\n\tSELECT NOW() AS date_month\n    UNION ALL\n    SELECT DISTINCT date_month AS date_month\n    FROM ref {{'dim_calendar'}}\n),\n```\n\n----------------------------------------\n\nTITLE: DBT Profile Configuration for Hudi\nDESCRIPTION: Example of DBT profile configuration for using Hudi with Glue, including necessary Spark configurations and datalake formats.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/glue-setup.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\ntest_project:\n  target: dev\n  outputs:\n    dev:\n      type: glue\n      query-comment: my comment\n      role_arn: arn:aws:iam::1234567890:role/GlueInteractiveSessionRole\n      region: eu-west-1\n      glue_version: \"4.0\"\n      workers: 2\n      worker_type: G.1X\n      schema: \"dbt_test_project\"\n      session_provisioning_timeout_in_seconds: 120\n      location: \"s3://aws-dbt-glue-datalake-1234567890-eu-west-1/\"\n      conf: spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.sql.hive.convertMetastoreParquet=false\n      datalake_formats: hudi\n```\n\n----------------------------------------\n\nTITLE: Selecting Models with --select in dbt CLI\nDESCRIPTION: This example demonstrates how to use the --select option in the dbt CLI to run a specific model and its downstream dependencies.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/yaml-selectors.md#2025-04-09_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\ndbt run --select my_model+\n```\n\n----------------------------------------\n\nTITLE: Setting TNS_ADMIN Environment Variable (Thick Mode)\nDESCRIPTION: Sets the TNS_ADMIN environment variable for thick mode Oracle connections.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/oracle-setup.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport TNS_ADMIN=/path/to/directory_containing_tnsnames.ora\n```\n\n----------------------------------------\n\nTITLE: EditSubscriptions Permission JSON Structure\nDESCRIPTION: JSON representation of the EditSubscriptions permission in Azure DevOps, showing the bit value, display name, and permission name used in the ServiceHooks namespace.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/git/setup-azure-service-user.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"bit\": 2,\n    \"displayName\": \"Edit Subscription\",\n    \"name\": \"EditSubscriptions\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Audit Helper Package in dbt\nDESCRIPTION: Configuration for installing the audit_helper package in a dbt project via packages.yml. Requires dbt version >= 1.2.0 and < 2.0.0.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-03-23-audit-helper.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\npackages:\n- package: dbt-labs/audit_helper\n  version: 0.7.0\n```\n\n----------------------------------------\n\nTITLE: Querying Oracle Database Name\nDESCRIPTION: SQL query to retrieve the Oracle Database name.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/oracle-setup.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nSELECT SYS_CONTEXT('userenv', 'DB_NAME') FROM DUAL\n```\n\n----------------------------------------\n\nTITLE: Creating Staging Model for Customers in SQL\nDESCRIPTION: SQL query to create a staging model for customers, selecting and renaming relevant columns from the source table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/sl-snowflake-qs.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\n  select\n   id as customer_id,\n   first_name,\n   last_name\nfrom {{ source('jaffle_shop', 'customers') }}\n```\n\n----------------------------------------\n\nTITLE: December 2024 dbt Dependencies Configuration\nDESCRIPTION: List of dbt Core and adapter versions included in the December 2024 Compatible release, showing dependencies and their versions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/compatible-track-changelog.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ndbt-core==1.9.0\n\n# shared interfaces\ndbt-adapters==1.10.4\ndbt-common==1.14.0\ndbt-semantic-interfaces==0.7.4\n\n# adapters\ndbt-athena==1.9.0\ndbt-bigquery==1.9.0\ndbt-databricks==1.9.0\ndbt-fabric==1.8.8\ndbt-postgres==1.9.0\ndbt-redshift==1.9.0\ndbt-snowflake==1.9.0\ndbt-spark==1.9.0\ndbt-synapse==1.8.2\ndbt-teradata==1.8.2\ndbt-trino==1.8.5\n```\n\n----------------------------------------\n\nTITLE: Basic dbt Production Commands\nDESCRIPTION: The simplest version of scheduled dbt commands for production environments. This approach runs models first and then tests them afterward, assuming the organization can tolerate a brief period where models may be incorrect before testing occurs.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-11-22-exact-commands-we-run-in-production.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndbt run\n\ndbt test\n```\n\n----------------------------------------\n\nTITLE: Creating Alias for Virtual Environment Activation\nDESCRIPTION: Shell command to create an alias for activating the dbt virtual environment, to be added to shell configuration file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/cloud-cli-installation.md#2025-04-09_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nalias env_dbt='source <PATH_TO_VIRTUAL_ENV_CONFIG>/bin/activate'\n```\n\n----------------------------------------\n\nTITLE: Using dbt_utils.dateadd Macro\nDESCRIPTION: The standard syntax for using the dbt_utils.dateadd macro, which abstracts away the differences between warehouse platforms.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-09-11-sql-dateadd.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_utils.dateadd(datepart, interval, from_date_or_timestamp) }}\n```\n\n----------------------------------------\n\nTITLE: Array Data Type Example\nDESCRIPTION: Example demonstrating an array data type containing multiple string elements.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/data-type/sql-data-types.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n[\"cheesecake\", \"cupcake\", \"brownie\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Post-hooks with On-Run-End in dbt Project YAML\nDESCRIPTION: Example of using post-hooks in dbt_project.yml to grant permissions to other transformers and BI users. This ensures that any changes made will be accessible to collaborators and utilized in the BI layer.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-02-05-dbt-project-checklist.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\non-run-end:\n  - \"grant select on all tables in schema {{ target.schema }} to role analyst\"\n  - \"grant select on all tables in schema {{ target.schema }} to role reporter\"\n```\n\n----------------------------------------\n\nTITLE: Initialize Basic DBT Snapshot Block\nDESCRIPTION: Basic structure for creating a snapshot block in a .sql file within the snapshots directory.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snapshots-jinja-legacy.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot orders_snapshot %}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Bundling Multiple Unit Tests\nDESCRIPTION: Macro that bundles multiple unit tests to run them together.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-08-22-unit-testing-dbt-package.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{% macro run_unit_tests() %}\n\n    {% do test_to_literal() %}\n\n    {% do another_test() %}\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Example Seed File with Comma Delimiter\nDESCRIPTION: This text snippet demonstrates the format of a seed file using a comma (,) as the delimiter.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/delimiter.md#2025-04-09_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nname,id\nluna,1\ndoug,2\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Model Properties in BigQuery using Python\nDESCRIPTION: Example of setting configuration parameters for a Python model in BigQuery, specifying submission method and Dataproc cluster name directly in the model code.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/python-models.md#2025-04-09_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef model(dbt, session):\n    dbt.config(\n        submission_method=\"cluster\",\n        dataproc_cluster_name=\"my-favorite-cluster\"\n    )\n    ...\n```\n\n----------------------------------------\n\nTITLE: Creating a Rolling Aggregation Macro in dbt SQL\nDESCRIPTION: This SQL macro creates reusable window aggregation logic for feature engineering, enabling consistent calculation of features over various time intervals. It uses Snowflake's range between syntax to compute aggregations over specified time windows.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-10-05-snowflake-feature-store.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{% macro rolling_agg(column, partition_by, order_by, interval='30 days', agg_function='sum') %}\n   {{ agg_function }}({{ column }}) over (\n       partition by {{ partition_by }}\n       order by {{ order_by }}\n       range between interval '{{ interval }}' preceding and current row\n   )\n{% endmacro %}\n\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS CodeCommit PR Template URL in dbt Cloud\nDESCRIPTION: This snippet provides the PR template URL format for AWS CodeCommit repositories in dbt Cloud. It includes the necessary path and branch reference parameters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/git/pr-template.md#2025-04-09_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://console.aws.amazon.com/codesuite/codecommit/repositories/<repo>/pull-requests/new/refs/heads/{{destination}}/.../refs/heads/{{source}}\n```\n\n----------------------------------------\n\nTITLE: Example SQL Output with Appended Custom Comment\nDESCRIPTION: SQL query showing how a custom comment appears when appended to the end of a query.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/query-comment.md#2025-04-09_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nselect ...\n/* run by drew in dbt */\n;\n```\n\n----------------------------------------\n\nTITLE: Configuring Package Dependencies in dbt\nDESCRIPTION: This YAML snippet demonstrates how to specify package dependencies in a dbt project, including both dbt_utils and spark_utils packages.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/dispatch.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\npackages:\n  - package: dbt-labs/dbt_utils\n    version: ...\n  - package: dbt-labs/spark_utils\n    version: ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Script File Structure\nDESCRIPTION: Shows the file structure for implementing a dbt Cloud job runner script\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/custom-cicd-pipelines.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmy_awesome_project\n python\n    run_and_monitor_dbt_job.py\n```\n\n----------------------------------------\n\nTITLE: Configuring ViewSubscriptions Permission in Azure DevOps via CLI\nDESCRIPTION: CLI command to grant the ViewSubscriptions permission to a service account in Azure DevOps. This permission allows viewing existing Azure DevOps service hooks subscriptions, which is required for dbt Cloud integration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/git/setup-azure-service-user.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\naz devops security permission update --organization https://dev.azure.com/<org_name> --namespace-id cb594ebe-87dd-4fc9-ac2c-6a10a4c92046 --subject <service_account>@xxxxxx.onmicrosoft.com --token PublisherSecurity/<azure_devops_project_object_id> --allow-bit 1\n```\n\n----------------------------------------\n\nTITLE: Configuring ViewSubscriptions Permission in Azure DevOps via CLI\nDESCRIPTION: CLI command to grant the ViewSubscriptions permission to a service account in Azure DevOps. This permission allows viewing existing Azure DevOps service hooks subscriptions, which is required for dbt Cloud integration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/git/setup-azure-service-user.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\naz devops security permission update --organization https://dev.azure.com/<org_name> --namespace-id cb594ebe-87dd-4fc9-ac2c-6a10a4c92046 --subject <service_account>@xxxxxx.onmicrosoft.com --token PublisherSecurity/<azure_devops_project_object_id> --allow-bit 1\n```\n\n----------------------------------------\n\nTITLE: Generating and Serving dbt Documentation\nDESCRIPTION: Command to generate and serve dbt documentation to verify that the docs blocks are working correctly. This launches a local web server for viewing the documentation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-04-generating-dynamic-docs.md#2025-04-09_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n$ dbt docs && dbt docs serve\n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Server Profile with Windows Authentication\nDESCRIPTION: YAML configuration for a dbt profile using Windows authentication for SQL Server. This includes server details, database name, schema, and enables Windows login.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/mssql-setup.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: sqlserver\n      driver: 'ODBC Driver 18 for SQL Server' # (The ODBC Driver installed on your system)\n      server: hostname or IP of your server\n      port: 1433\n      database: exampledb\n      schema: schema_name\n      windows_login: True\n```\n\n----------------------------------------\n\nTITLE: Configuring Parallel Batch Execution in a SQL Model File\nDESCRIPTION: Example of configuring a specific SQL model to use microbatch incremental strategy with parallel batch execution. The configuration includes setting materialized as incremental, defining the time-based parameters, and enabling concurrent batches.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/parallel-batch-execution.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(\n    materialized='incremental',\n    incremental_strategy='microbatch',\n    event_time='session_start',\n    begin='2020-01-01',\n    batch_size='day',\n    concurrent_batches=true, # value set to true to run batches in parallel\n    ...\n  )\n}}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: AWS PrivateLink Request Template for Databricks\nDESCRIPTION: Email template to submit an AWS PrivateLink request for Databricks to dbt Support. It includes fields for Databricks instance name, AWS region, and dbt Cloud environment.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/secure/databricks-privatelink.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nSubject: New AWS Multi-Tenant PrivateLink Request\n- Type: Databricks\n- Databricks instance name:\n- Databricks cluster AWS Region (e.g., us-east-1, eu-west-2):\n- dbt Cloud multi-tenant environment (US, EMEA, AU):\n```\n\n----------------------------------------\n\nTITLE: ViewSubscriptions Permission JSON Structure\nDESCRIPTION: JSON representation of the ViewSubscriptions permission in Azure DevOps, showing the bit value, display name, and permission name used in the ServiceHooks namespace.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/git/setup-azure-service-user.md#2025-04-09_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"bit\": 1,\n    \"displayName\": \"View Subscriptions\",\n    \"name\": \"ViewSubscriptions\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet File Format with Partitioning\nDESCRIPTION: Configuration for materializing a table as partitioned Parquet files when using file-based connectors like Hive. This specifies the format as Parquet and applies bucket partitioning.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/trino-configs.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(\n    materialized='table',\n    properties= {\n      \"format\": \"'PARQUET'\",\n      \"partitioning\": \"ARRAY['bucket(id, 2)']\",\n    }\n  )\n}}\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for dbt Cloud Integration\nDESCRIPTION: Commands to set environment variables required for connecting dbt Core to dbt Cloud and enabling artifact upload.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/hybrid-setup.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport DBT_CLOUD_ACCOUNT_ID=your_account_id\nexport DBT_CLOUD_ENVIRONMENT_ID=your_environment_id\nexport DBT_CLOUD_TOKEN=your_token\nexport DBT_UPLOAD_TO_ARTIFACTS_INGEST_API=True\n```\n\n----------------------------------------\n\nTITLE: Configuring Delta Tables with Universal Format for Iceberg Compatibility\nDESCRIPTION: Configuration example showing how to make Delta tables compatible with Iceberg readers using Databricks Universal Format. This enables cross-engine interoperability.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/databricks-configs.md#2025-04-09_snippet_25\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    tblproperties={\n      'delta.enableIcebergCompatV2' = 'true'\n      'delta.universalFormat.enabledFormats' = 'iceberg'\n    }\n ) }}\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for psycopg2 on Ubuntu\nDESCRIPTION: Bash commands to install the required system dependencies (libpq-dev and python-dev) for building psycopg2 from source on Ubuntu systems.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/postgres-setup.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get update\nsudo apt-get install libpq-dev python-dev\n```\n\n----------------------------------------\n\nTITLE: SQL Student Table Structure\nDESCRIPTION: Example structure of a student table showing both primary and unique key usage\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/entities.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nstudent_id (primary key)\nemail (unique key)\nfirst_name\nlast_name\n```\n\n----------------------------------------\n\nTITLE: Creating a GitHub Pull Request Template for dbt Projects\nDESCRIPTION: This snippet shows how to create a pull request template for GitHub repositories containing dbt projects. The template includes a checklist of items to verify before submitting a PR, with conditional checks for Redshift-specific optimizations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-11-22-dbt-labs-pr-template.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n## Checklist:\n\n&lt;!---\n\nThis checklist is mostly useful as a reminder of small things that can easily be\n\nforgotten  it is meant as a helpful tool rather than hoops to jump through.\n\nPut an `x` in all the items that apply, make notes next to any that haven't been\n\naddressed, and remove any items that are not relevant to this PR.\n\n-->\n\n- [ ] My pull request represents one logical piece of work.\n\n- [ ] My commits are related to the pull request and look clean.\n\n- [ ] My SQL follows the style guide.\n\n- [ ] I have materialized my models appropriately.\n\n- [ ] I have added appropriate tests and documentation to any new models.\n\n- [ ] I have updated the README file.\n\n{%- if project.warehouse == 'redshift' %}\n\n- [ ] I have added sort and dist keys to models materialized as tables.\n\n- [ ] I have validated the SQL in any late-binding views.\n\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Extended Attributes YAML with Additional BigQuery Connection Options\nDESCRIPTION: This extended YAML configuration includes additional BigQuery connection options like priority and execution project alongside the service account details.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/connect-data-platform/connnect-bigquery.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\npriority: interactive\nkeyfile_json:\n  type: xxx\n  project_id: xxx\n  private_key_id: xxx\n  private_key: '{{ env_var(''DBT_ENV_SECRET_PROJECTXXX_PRIVATE_KEY'') }}'\n  client_email: xxx\n  client_id: xxx\n  auth_uri: xxx\n  token_uri: xxx\n  auth_provider_x509_cert_url: xxx\n  client_x509_cert_url: xxx\nexecution_project: buck-stops-here-456\n```\n\n----------------------------------------\n\nTITLE: DATEADD Function in Snowflake SQL\nDESCRIPTION: Demonstrates the syntax for the DATEADD function in Snowflake. It accepts datepart, interval, and from_date as parameters. Supports hour, minute, and second as dateparts.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/date-functions/sql-dateadd.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\ndateadd( {{ datepart }}, {{ interval }}, {{ from_date }} )\n```\n\n----------------------------------------\n\nTITLE: SQL Query with Embedded Jinja Conditionals\nDESCRIPTION: Example of SQL query incorporating Jinja conditionals and macro usage, demonstrating proper spacing and indentation patterns for mixed SQL and Jinja code.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-style/4-how-we-style-our-jinja.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    entity_id,\n    entity_type,\n    {% if this %}\n\n        {{ that }},\n\n    {% else %}\n\n        {{ the_other_thing }},\n\n    {% endif %}\n    {{ make_cool('uncool_id') }} as cool_id\n```\n\n----------------------------------------\n\nTITLE: Running dbt show Command for Model Preview\nDESCRIPTION: Examples of using the dbt show command to preview a model or run an inline query. The command compiles and runs the query, displaying the first 5 rows of the result in the terminal.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/show.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndbt show --select \"model_name.sql\"\n```\n\nLANGUAGE: bash\nCODE:\n```\ndbt show --inline \"select * from {{ ref('model_name') }}\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic String Literal Macro in dbt\nDESCRIPTION: Simple macro that creates a string literal from input text in dbt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-08-22-unit-testing-dbt-package.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{% macro to_literal(text) %}\n\n    '{{- text -}}'\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Debugging Compilation Error: Invalid Jinja\nDESCRIPTION: Error message shown when there is a syntax error in a Jinja template, such as a missing closing tag. In this example, the error is due to a missing {% endmacro %} tag.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/debug-errors.md#2025-04-09_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\n$ dbt run\nRunning with dbt=1.7.1\nCompilation Error in macro (macros/cents_to_dollars.sql)\n  Reached EOF without finding a close tag for macro (searched from line 1)\n```\n\n----------------------------------------\n\nTITLE: Payment History by Month CTE\nDESCRIPTION: Creates a CTE that joins payment data with months to track payment history up to each month end. This enables historical analysis of customer payment behavior.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-08-building-a-historical-user-segmentation-model-with-dbt.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\npayments_with_months AS(\n    SELECT  user_id,\n            date_month,\n            payment_date,\n            payment_id,\n            payment_amount\n    FROM months\n        JOIN payments ON payment_date <= date_month \n),\n```\n\n----------------------------------------\n\nTITLE: Importing Explorer Course Link Component in Markdown\nDESCRIPTION: This code snippet imports a custom component called ExplorerCourse from a markdown file. It's likely used to include a reusable section of content related to dbt Explorer courses.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/explore-multiple-projects.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nimport ExplorerCourse from '/snippets/_explorer-course-link.md';\n```\n\n----------------------------------------\n\nTITLE: Registering Entities in Snowflake Feature Store with Python\nDESCRIPTION: This Python code creates and registers entities (Customer and Transaction) in the Snowflake Feature Store. Entities represent the objects that features are associated with and serve as join keys for feature lookups.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-10-05-snowflake-feature-store.md#2025-04-09_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncustomer = Entity(name=\"CUSTOMER\", join_keys=[\"CUSTOMER_ID\"])\ntransaction = Entity(name=\"TRANSACTION\", join_keys=[\"TRANSACTION_ID\"])\nfs.register_entity(customer)\nfs.register_entity(transaction)\n\n```\n\n----------------------------------------\n\nTITLE: Basic Schema Grant Usage Example\nDESCRIPTION: A simple example showing how to grant usage on schemas to a database reader using the schemas context variable in on-run-end hook.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/on-run-end-context.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\non-run-end:\n - \"{% for schema in schemas %}grant usage on schema {{ schema }} to db_reader;{% endfor %}\"\n```\n\n----------------------------------------\n\nTITLE: BigQuery Optional Configurations Table\nDESCRIPTION: A markdown table listing optional BigQuery configurations with their types and examples.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/connect-data-platform/connnect-bigquery.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Configuration    | <div style={{width:'250'}}>Information</div>   | Type    | <div style={{width:'150'}}>Example</div>             |\n|---------------------------|-----------------------------------------|---------|--------------------|\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Glue Profile for dbt with Iceberg Support\nDESCRIPTION: YAML configuration for a dbt profile targeting AWS Glue with Iceberg format support. This includes role configuration, region, worker settings, schema, and necessary Spark configuration parameters for Iceberg integration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/glue-setup.md#2025-04-09_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\ntest_project:\n  target: dev\n  outputs:\n    dev:\n      type: glue\n      query-comment: my comment\n      role_arn: arn:aws:iam::1234567890:role/GlueInteractiveSessionRole\n      region: eu-west-1\n      glue_version: \"4.0\"\n      workers: 2\n      worker_type: G.1X\n      schema: \"dbt_test_project\"\n      session_provisioning_timeout_in_seconds: 120\n      location: \"s3://aws-dbt-glue-datalake-1234567890-eu-west-1/\"\n      datalake_formats: iceberg\n      conf: --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.sql.warehouse=s3://aws-dbt-glue-datalake-1234567890-eu-west-1/dbt_test_project --conf spark.sql.catalog.glue_catalog=org.apache.iceberg.spark.SparkCatalog --conf spark.sql.catalog.glue_catalog.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog --conf spark.sql.catalog.glue_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO --conf spark.sql.catalog.glue_catalog.lock-impl=org.apache.iceberg.aws.dynamodb.DynamoDbLockManager --conf spark.sql.catalog.glue_catalog.lock.table=myGlueLockTable  --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \n```\n\n----------------------------------------\n\nTITLE: Generated YAML Output for Multiple dbt Models\nDESCRIPTION: This YAML snippet shows a subset of the generated documentation structure for multiple models in the activity_based_interest folder.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-04-generating-dynamic-docs.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: activity_based_interest_activated\n    description: \"\"\n    columns:\n      - name: id\n        description: \"\"\n\n      - name: user_id\n        description: \"\"\n\n... (truncated for example purposes)\n\n  - name: set_inactive_interest_rate_after_july_1st_in_bec_for_user\n    description: \"\"\n    columns:\n      - name: id\n        description: \"\"\n\n      - name: user_id\n        description: \"\"\n\n      - name: start_date\n        description: \"\"\n\n      - name: event_time\n        description: \"\"\n\n      - name: event_day\n        description: \"\"\n\n  - name: set_inactive_interest_rate_from_july_1st_in_bec_for_user\n    description: \"\"\n    columns:\n      - name: id\n        description: \"\"\n\n      - name: user_id\n        description: \"\"\n\n      - name: event_time\n        description: \"\"\n\n      - name: event_day\n        description: \"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring sqlnet.ora for Wallet Location\nDESCRIPTION: Edits the sqlnet.ora file to specify the wallet directory location for thick mode connections.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/oracle-setup.md#2025-04-09_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nWALLET_LOCATION = (SOURCE = (METHOD = file) (METHOD_DATA = (DIRECTORY=\"/path/to/wallet/directory\")))\nSSL_SERVER_DN_MATCH=yes\n```\n\n----------------------------------------\n\nTITLE: Using --selector with Pre-defined Selector in dbt CLI\nDESCRIPTION: This example shows how to use the --selector option in the dbt CLI to run models defined by a pre-configured selector in the selectors.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/yaml-selectors.md#2025-04-09_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\ndbt run --selector nightly_diet_snowplow\n```\n\n----------------------------------------\n\nTITLE: PrivateLink VCS Request Template\nDESCRIPTION: Template for submitting a new Multi-Tenant PrivateLink request for VCS integration to dbt Support. Includes required fields for service configuration including endpoint service name, custom DNS options, and environment details.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/secure/vcs-privatelink.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nSubject: New Multi-Tenant PrivateLink Request\n- Type: VCS Interface-type\n- VPC Endpoint Service Name:\n- Custom DNS (optional)\n    - Private hosted zone:\n    - DNS record:\n- VCS install AWS Region (e.g., us-east-1, eu-west-2):\n- dbt Cloud multi-tenant environment (US, EMEA, AU):\n```\n\n----------------------------------------\n\nTITLE: Cron Schedule Examples for dbt Cloud Jobs\nDESCRIPTION: Examples of cron syntax for scheduling dbt Cloud jobs with different frequencies and conditions, including hourly, daily, and monthly executions. These examples demonstrate how to use cron expressions for precise scheduling control in dbt Cloud.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/deploy-jobs.md#2025-04-09_snippet_0\n\nLANGUAGE: cron\nCODE:\n```\n0 * * * *\n```\n\nLANGUAGE: cron\nCODE:\n```\n*/5 * * * *\n```\n\nLANGUAGE: cron\nCODE:\n```\n5 4 * * *\n```\n\nLANGUAGE: cron\nCODE:\n```\n30 */4 * * *\n```\n\nLANGUAGE: cron\nCODE:\n```\n0 0 */2 * *\n```\n\nLANGUAGE: cron\nCODE:\n```\n0 0 * * 1\n```\n\nLANGUAGE: cron\nCODE:\n```\n0 0 L * *\n```\n\nLANGUAGE: cron\nCODE:\n```\n0 0 L 1,2,3,4,5,6,8,9,10,11,12 *\n```\n\nLANGUAGE: cron\nCODE:\n```\n0 0 L 7 *\n```\n\nLANGUAGE: cron\nCODE:\n```\n0 0 L * FRI,SAT\n```\n\nLANGUAGE: cron\nCODE:\n```\n0 12 L * *\n```\n\nLANGUAGE: cron\nCODE:\n```\n0 7 L * 5\n```\n\nLANGUAGE: cron\nCODE:\n```\n30 14 L * *\n```\n\n----------------------------------------\n\nTITLE: Setting dbt Cloud Secrets with Zapier Storage Python Client\nDESCRIPTION: Python code snippet that uses Zapier's StoreClient to securely save dbt Cloud service token and webhook key. The code requires a Zapier Storage UUID and needs to be run as a Python step in Zapier. The stored secrets remain valid for 3 months if accessed at least once during that period.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/webhook_guide_zapier_secret_store.md#2025-04-09_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nstore = StoreClient('abc123') #replace with your UUID secret\nstore.set('DBT_WEBHOOK_KEY', 'abc123') #replace with webhook secret\nstore.set('DBT_CLOUD_SERVICE_TOKEN', 'abc123') #replace with your dbt Cloud API token\n```\n\n----------------------------------------\n\nTITLE: Redshift Serverless PrivateLink Request Template\nDESCRIPTION: Template for submitting a Redshift Serverless PrivateLink configuration request to dbt Support. Includes fields for workgroup details and environment specification.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/secure/redshift-privatelink.md#2025-04-09_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nSubject: New Multi-Tenant PrivateLink Request\n- Type: Redshift-managed - Serverless\n- Redshift workgroup name:\n- Redshift workgroup AWS account ID:\n- Redshift workgroup AWS Region (e.g., us-east-1, eu-west-2):\n- dbt Cloud multi-tenant environment (US, EMEA, AU):\n```\n\n----------------------------------------\n\nTITLE: Coalescing NULL Valid-To Values with Future-Proof Date\nDESCRIPTION: SQL snippet that replaces NULL values in dbt_valid_to columns with the future-proof date defined in the global variables, ensuring all records have a proper timestamp for joining.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-05-24-joining-snapshot-complexity.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\ncoalesce(dbt_valid_to, cast('{{ var(\"future_proof_date\") }}' as timestamp)) as valid_to\n```\n\n----------------------------------------\n\nTITLE: DBT Hook SQL Statement Format Examples\nDESCRIPTION: Examples showing two formats for SQL statements in pre-hooks and post-hooks: single statement as a string (without brackets) and multiple statements as an array (with brackets).\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/post-and-pre-hooks-sql-statement.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n\"SQL-STATEMENT\" | [\"SQL-STATEMENT\", \"SQL-STATEMENT\"]\n```\n\n----------------------------------------\n\nTITLE: Loading Sample Data into Azure Synapse Analytics with SQL\nDESCRIPTION: SQL script to create and populate three tables (customers, orders, payments) in Azure Synapse Analytics using the COPY INTO command from external Parquet files stored in Azure Blob Storage.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/azure-synapse-analytics-qs.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE dbo.customers\n(\n    [ID] [bigint],\n    \\[FIRST_NAME] [varchar](8000),\n    \\[LAST_NAME] [varchar](8000)\n);\n\nCOPY INTO [dbo].[customers]\nFROM 'https://dbtlabsynapsedatalake.blob.core.windows.net/dbt-quickstart-public/jaffle_shop_customers.parquet'\nWITH (\n    FILE_TYPE = 'PARQUET'\n);\n\nCREATE TABLE dbo.orders\n(\n    [ID] [bigint],\n    [USER_ID] [bigint],\n    [ORDER_DATE] [date],\n    \\[STATUS] [varchar](8000)\n);\n\nCOPY INTO [dbo].[orders]\nFROM 'https://dbtlabsynapsedatalake.blob.core.windows.net/dbt-quickstart-public/jaffle_shop_orders.parquet'\nWITH (\n    FILE_TYPE = 'PARQUET'\n);\n\nCREATE TABLE dbo.payments\n(\n    [ID] [bigint],\n    [ORDERID] [bigint],\n    \\[PAYMENTMETHOD] [varchar](8000),\n    \\[STATUS] [varchar](8000),\n    [AMOUNT] [bigint],\n    [CREATED] [date]\n);\n\nCOPY INTO [dbo].[payments]\nFROM 'https://dbtlabsynapsedatalake.blob.core.windows.net/dbt-quickstart-public/stripe_payments.parquet'\nWITH (\n    FILE_TYPE = 'PARQUET'\n);\n\n```\n\n----------------------------------------\n\nTITLE: Traditional SQL UNION ALL Implementation\nDESCRIPTION: Example showing how to manually combine sessions and sales tables using UNION ALL with null column propagation for mismatched columns.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-09-11-union-all.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\tdate,\n\tlanding_page_url,\n\tcampaign,\n\tchannel,\n\tsessions,\n\tpageviews,\n\ttime_on_site,\n\tnull as orders,\n\tnull as customers,\n\tnull as revenue\nfrom sessions\n\nunion all \n\nselect\n\tdate,\n\tlanding_page_url,\n\tcampaign,\n\tchannel,\n\tnull as sessions,\n\tnull as pageviews,\n\tnull as time_on_site,\n\torders,\n\tcustomers,\n\trevenue\nfrom sales\n```\n\n----------------------------------------\n\nTITLE: Dynamic Schema Usage Grants Macro\nDESCRIPTION: Complex macro for automatically granting schema usage based on object-level select permissions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-07-26-configuring-grants.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{% macro grant_usage_on_schemas_where_select() %}\n    /*\n      Note: This is pseudo code only, for demonstration purposes\n      For every role that can access at least one object in a schema,\n      grant 'usage' on that schema to the role.\n      That way, users with the role can run metadata queries showing objects\n      in that schema (a common need for BI tools)\n    */\n    {% set schema_grants = {} %}\n    {% if execute %}\n      {% for node in graph.nodes.values() %}\n        {% set grants = node.config.get('grants') %}\n        {% set select_roles = grants['select'] if grants else [] %}\n        {% if select_roles %}\n          {% set database_schema = node.database ~ \".\" ~ node.schema %}\n          {% if database_schema in database_schemas %}\n            {% do schema_grants[database_schema].add(select_roles) %}\n          {% else %}\n            {% do schema_grants.update({database_schema: set(select_roles)}) %}\n          {% endif %}\n        {% endif %}\n      {% endfor %}\n    {% endif %}\n    {% set grant_list %}\n      {% for schema in schema_grants %}\n        {% for role in schema_grants[schema] %}\n          grant usage on schema {{ schema }} to {{ role }};\n        {% endfor %}\n      {% endfor %}\n    {% endset %}\n    {{ return(grant_list) }}\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Importing dbt Copilot Components\nDESCRIPTION: Import statements for dbt Copilot related MDX components used in the documentation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/use-dbt-copilot.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nimport CopilotResources from '/snippets/_use-copilot-resources.md';\nimport CopilotEditCode from '/snippets/_use-copilot-edit-code.md';\nimport CopilotVE from '/snippets/_use-copilot-ve.md';\n```\n\n----------------------------------------\n\nTITLE: Removing dbt-labs tap from Homebrew\nDESCRIPTION: Command to remove the dbt-labs tap from Homebrew to prevent conflicts when installing the dbt Cloud CLI.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/cloud-cli-installation.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbrew untap dbt-labs/dbt\n```\n\n----------------------------------------\n\nTITLE: Querying with Descending Order Using Object Notation\nDESCRIPTION: Example of ordering query results in descending order using the full object notation with the descending method. This approach is required when ordering by objects with operations like granularity changes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-jdbc.md#2025-04-09_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\nselect * from {{\nsemantic_layer.query(metrics=['food_order_amount', 'order_gross_profit'],\n  group_by=[Dimension('metric_time').grain('week')],\n  limit=10,\n  order_by=[Metric('order_gross_profit').descending(True), Dimension('metric_time').grain('week').descending(True) ])\n  }}\n```\n\n----------------------------------------\n\nTITLE: Disabling All Sources from a Package in dbt_project.yml\nDESCRIPTION: Example showing how to disable all sources included from a package by applying configuration under the project name in the sources config.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/source-configs.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  events:\n    +enabled: false\n```\n\n----------------------------------------\n\nTITLE: Implementing Ingestion-Time Partitioning with dbt\nDESCRIPTION: Enhanced SQL model configuration that enables ingestion-time partitioning by adding the time_ingestion_partitioning flag to the configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-02-01-ingestion-time-partitioning-bigquery.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized = 'incremental',\n    incremental_strategy = 'insert_overwrite',\n    partition_by = {\n      \"field\": \"day\",\n      \"data_type\": \"date\",\n      \"time_ingestion_partitioning\": true\n    }\n) }}\n\nselect\n    day,\n    campaign_id,\n    NULLIF(COUNTIF(action = 'impression'), 0) impressions_count\nfrom {{ source('logs', 'tracking_events') }}\ngroup by day, campaign_id\n```\n\n----------------------------------------\n\nTITLE: MDX Component Usage for PrivateLink Documentation\nDESCRIPTION: Implementation of imported MDX components to display PrivateLink-related content.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/secure/about-privatelink.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n<SetUpPages features={\"/snippets/_available-tiers-privatelink.md\"}/>\n\n<CloudProviders type='a data platform' />\n\n<PrivateLinkHostnameWarning features={\"/snippets/_privatelink-hostname-restriction.md\"}/>\n```\n\n----------------------------------------\n\nTITLE: Configuring Dependencies for Downstream dbt Cloud Project\nDESCRIPTION: This YAML snippet shows how to set up the dependencies.yml file in a downstream dbt Cloud project to enable cross-project references with an upstream project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-10-04-hybrid-mesh.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# dependencies.yml file in dbt Cloud downstream project\nprojects:\n- name: upstream_project_name\n```\n\n----------------------------------------\n\nTITLE: Listing Models by Package in dbt\nDESCRIPTION: Example of using dbt ls to list models from a specific package (snowplow in this case).\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/list.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ dbt ls --select snowplow.*\nsnowplow.snowplow_base_events\nsnowplow.snowplow_base_web_page_context\nsnowplow.snowplow_id_map\nsnowplow.snowplow_page_views\nsnowplow.snowplow_sessions\n...\n```\n\n----------------------------------------\n\nTITLE: Adding Dispatch Configuration for External Tables in dbt_project.yml\nDESCRIPTION: Configuration to add to dbt_project.yml to properly set up the search order for macros when using the dbt-external-tables package with Firebolt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/firebolt-configs.md#2025-04-09_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\ndispatch:\n  - macro_namespace: dbt_external_tables\n    search_order: ['dbt', 'dbt_external_tables']\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt-spark Authentication Profile\nDESCRIPTION: Example of authentication configuration in profiles.yml using the dbt-spark adapter with ODBC connection method.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/migrate-from-spark-to-databricks.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: spark\n      method: odbc\n      driver: '/opt/simba/spark/lib/64/libsparkodbc_sb64.so'\n      schema: my_schema\n      host: dbc-l33t-nwb.cloud.databricks.com\n      endpoint: 8657cad335ae63e3\n      token: [my_secret_token]\n```\n\n----------------------------------------\n\nTITLE: Initial Markdown File with Extracted Column Names\nDESCRIPTION: Shows the extracted column names pasted into a markdown file as a starting point for creating documentation blocks for each column.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-04-generating-dynamic-docs.md#2025-04-09_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n$ cat models/core/activity_based_interest/_activity_based_interest_docs.md\nend_date\nevent_day\nevent_time\nid\nis_active\nlast_updated_date\nstart_date\ntier_interest_percentage\ntier_threshold_amount\nuser_id\n```\n\n----------------------------------------\n\nTITLE: Request Template for PrivateLink Configuration\nDESCRIPTION: Template format for submitting a new Multi-Tenant PrivateLink request to dbt Support, including required fields for Postgres Interface-type configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/secure/postgres-privatelink.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nSubject: New Multi-Tenant PrivateLink Request\n- Type: Postgres Interface-type\n- VPC Endpoint Service Name:\n- Postgres server AWS Region (e.g., us-east-1, eu-west-2):\n- dbt Cloud multi-tenant environment (US, EMEA, AU):\n```\n\n----------------------------------------\n\nTITLE: Model Access Error When Running Unauthorized References\nDESCRIPTION: Example of the error message shown when running a model that attempts to reference a model it doesn't have access to. Shows how dbt enforces the access controls defined in the configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-01-evolving-data-engineer-craft.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ dbt run -s marketing_model\n...\ndbt.exceptions.DbtReferenceError: Parsing Error\n  Node model.jaffle_shop.marketing_model attempted to reference node model.jaffle_shop.finance_model, \n  which is not allowed because the referenced node is private to the finance group.\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Blog Post\nDESCRIPTION: YAML configuration block defining metadata for the blog post including title, description, authors, tags, and other publishing parameters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-11-30-dbt-project-evaluator.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: \"Introducing the dbt_project_evaluator: Automatically evaluate your dbt project for alignment with best practices \"\ndescription: \"The dbt_project_evaluator is a dbt package created by the Professional Services team at dbt Labs to help analytics engineers automatically audit their dbt projects for bad practices. Goodbye auditing nightmares, hello beautiful DAG.\"\nslug: align-with-dbt-project-evaluator\n\nauthors: [grace_goheen]\n\ntags: [analytics craft]\nhide_table_of_contents: false\n\ndate: 2022-11-30\nis_featured: true\n---\n```\n\n----------------------------------------\n\nTITLE: YAML Frontmatter Configuration\nDESCRIPTION: YAML configuration block defining metadata for the documentation page including title, ID, description, sidebar label and tags.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/use-dbt-semantic-layer/sl-architecture.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: \"dbt Semantic Layer architecture\"\nid: sl-architecture\ndescription: \"dbt Semantic Layer product architecture and related questions.\"\nsidebar_label: \"Semantic Layer architecture\"\ntags: [Semantic Layer]\n---\n```\n\n----------------------------------------\n\nTITLE: Excluding Models by Tags in SQL\nDESCRIPTION: This SQL command excludes models based on tags for scenarios like when models share a common feature or function.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/job-commands.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n--select state modified --exclude tag:tagname_a tag:tagname_b\n```\n\n----------------------------------------\n\nTITLE: Authorizing Stage Access for Azure Private Link in Snowflake\nDESCRIPTION: SQL command to authorize access to Snowflake internal stages through an Azure private endpoint. This function must be executed by a Snowflake administrator to enable private connectivity for internal stages.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/secure/snowflake-privatelink.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nUSE ROLE ACCOUNTADMIN;\n\n-- Azure Private Link\nSELECT SYSTEMS$AUTHORIZE_STAGE_PRIVATELINK_ACCESS ( `AZURE PRIVATE ENDPOINT RESOURCE ID` );\n```\n\n----------------------------------------\n\nTITLE: Example Sigma Embedded UI Element URL for APAC Region\nDESCRIPTION: This code snippet provides an example of the URL format for adding a dbt Cloud status tile in Sigma for users in the APAC region. It demonstrates how to replace the access URL placeholder with the specific regional URL.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/data-tile.md#2025-04-09_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://metadata.au.dbt.com/exposure-tile?name=<exposure_name>&jobId=<job_id>&token=<metadata_only_token>\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Profiles for IBM watsonx.data Presto\nDESCRIPTION: YAML configuration example showing how to set up profiles.yml for connecting to IBM watsonx.data Presto in both software and SaaS environments. Includes essential parameters like authentication method, credentials, host details, and SSL verification settings.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/watsonx-presto-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmy_project:\n  outputs:\n    software:\n      type: presto\n      method: BasicAuth\n      user: [user]\n      password: [password]\n      host: [hostname]\n      database: [catalog name]\n      schema: [your dbt schema]\n      port: [port number]\n      threads: [1 or more]\n      ssl_verify: path/to/certificate\n\n    saas:\n      type: presto\n      method: BasicAuth\n      user: [user]\n      password: [api_key]\n      host: [hostname]\n      database: [catalog name]\n      schema: [your dbt schema]\n      port: [port number]\n      threads: [1 or more]\n\n  target: software\n```\n\n----------------------------------------\n\nTITLE: Simplified UNION ALL Using dbt Utils Macro\nDESCRIPTION: Demonstrates how to use the union_relations macro from dbt_utils package to automatically handle column mismatches across multiple tables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-09-11-union-all.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_utils.union_relations(\n\n    relations=[ref('sessions'), ref('sales'), ref('ads')\n\n) }}\n```\n\n----------------------------------------\n\nTITLE: Troubleshooting DuckDB File Lock Error in Jinja\nDESCRIPTION: An example of a common DuckDB error message when the database file is locked by another process, preventing dbt from accessing it.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/duckdb-qs.md#2025-04-09_snippet_7\n\nLANGUAGE: Jinja\nCODE:\n```\nIO Error: Could not set lock on file \"jaffle_shop.duckdb\": Resource temporarily unavailable\n```\n\n----------------------------------------\n\nTITLE: dbt Seed Full Refresh Solution\nDESCRIPTION: Command to resolve seed column changes by performing a full refresh, which drops and rebuilds the table structure.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Seeds/full-refresh-seed.md#2025-04-09_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndbt seed --full-refresh\n```\n\n----------------------------------------\n\nTITLE: Navigating to dbt project directory in Bash\nDESCRIPTION: Command to change the current directory to a specific dbt project folder using the cd command in Bash.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/configure-cloud-cli.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd ~/dbt-projects/jaffle_shop\n```\n\n----------------------------------------\n\nTITLE: Defining Analysis Paths in dbt_project.yml\nDESCRIPTION: Specifies the basic configuration for analysis-paths in the dbt_project.yml file. This setting allows you to define custom directories where analyses are located.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/analysis-paths.md#2025-04-09_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nanalysis-paths: [directorypath]\n```\n\n----------------------------------------\n\nTITLE: Using Dynamic Configuration in Python Models\nDESCRIPTION: Example of using dynamic configurations within Python f-strings in dbt 1.8+, allowing access to configuration values directly in Python code.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/python-models.md#2025-04-09_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Previously, attempting to access a configuration value like this would result in None\nprint(f\"{dbt.config.get('my_var')}\")  # Output before change: None\n\n# Now you can access the actual configuration value\n# Assuming 'my_var' is configured to 5 for the current model\nprint(f\"{dbt.config.get('my_var')}\")  # Output after change: 5\n```\n\n----------------------------------------\n\nTITLE: Documenting the Cents to Dollars Macro\nDESCRIPTION: This YAML configuration documents the cents_to_dollars macro, specifying that it accepts a column name and an optional scale parameter with their respective types and descriptions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/arguments.md#2025-04-09_snippet_3\n\nLANGUAGE: yml\nCODE:\n```\nversion: 2\n\nmacros:\n  - name: cents_to_dollars\n    arguments:\n      - name: column_name\n        type: column\n        description: \"The name of a column\"\n      - name: scale\n        type: integer\n        description: \"The number of decimal places to round to. Default is 2.\"\n\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Cloud Environments for Direct Promotion Strategy\nDESCRIPTION: A table showing the recommended configuration for dbt Cloud environments when implementing a Direct Promotion git branching strategy. It defines three environment types (Development, Continuous Integration, and Production) with their respective settings and purposes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2025-01-28-git-branching-strategies-and-dbt.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Environment Name | [Environment Type](https://docs.getdbt.com/docs/dbt-cloud-environments#types-of-environments) | [Deployment Type](https://docs.getdbt.com/docs/deploy/deploy-environments#staging-environment) | Base Branch | Will handle |\n| --- | --- | --- | --- | --- |\n| Development | development | - | `main` | Operations done in the IDE (including creating feature branches) |\n| Continuous Integration | deployment | General | `main` | A continuous integration job |\n| Production | deployment | Production | `main` | A deployment job |\n```\n\n----------------------------------------\n\nTITLE: Creating Snowflake Security Integration for Entra ID OAuth Authentication\nDESCRIPTION: SQL command template for creating a security integration in Snowflake that enables external OAuth authentication with Microsoft Entra ID (formerly Azure AD). This integration maps Entra ID user principal names to Snowflake login names for authentication.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/manage-access/external-oauth.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\ncreate or replace security integration <whatever you want to name it>\n   type = external_oauth\n   enabled = true\n   external_oauth_type = azure\n   external_oauth_issuer = '<AZURE_AD_ISSUER>'\n   external_oauth_jws_keys_url = '<AZURE_AD_JWS_KEY_ENDPOINT>'\n   external_oauth_audience_list = ('<SNOWFLAKE_APPLICATION_ID_URI>')\n   external_oauth_token_user_mapping_claim = 'upn'\n   external_oauth_any_role_mode = 'ENABLE'\n   external_oauth_snowflake_user_mapping_attribute = 'login_name';\n```\n\n----------------------------------------\n\nTITLE: Triggering dbt Cloud Job with Airflow DbtCloudRunJobOperator\nDESCRIPTION: This code snippet demonstrates how to use the DbtCloudRunJobOperator in Airflow to trigger a dbt Cloud job. It includes setting up the necessary parameters and creating a DAG task.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/deployment-tools.md#2025-04-09_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow import DAG\nfrom airflow.operators.dummy import DummyOperator\nfrom airflow.utils.dates import days_ago\nfrom cosmos.providers.dbt.cloud.operators import DbtCloudRunJobOperator\n\nwith DAG(\n    dag_id=\"example_dbt_cloud\",\n    default_args={\"owner\": \"astronomer\"},\n    start_date=days_ago(1),\n    schedule_interval=None,\n) as dag:\n    begin = DummyOperator(task_id=\"begin\")\n    dbt_cloud_run_job = DbtCloudRunJobOperator(\n        task_id=\"dbt_cloud_run_job\",\n        job_id=12345,\n        account_id=12345,\n        trigger_reason=\"Astronomer Test\",\n    )\n    end = DummyOperator(task_id=\"end\")\n\n    begin >> dbt_cloud_run_job >> end\n```\n\n----------------------------------------\n\nTITLE: Properties File Structure Example (YAML)\nDESCRIPTION: Example showing how properties and configs are defined in properties.yml files. Properties are declared directly while configs are nested under a config property.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/configs-and-properties.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nproperties:\n  model_name:\n    description: \"Model description\"\n    columns:\n      - name: id\n        description: \"Column description\"\n    config:\n      materialized: table\n      schema: my_schema\n```\n\n----------------------------------------\n\nTITLE: Querying with MetricFlow\nDESCRIPTION: Commands to create and execute queries with MetricFlow against your data platform. Includes comprehensive options for metrics, group-by, filtering by time, limiting results, ordering, and output formatting.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metricflow-commands.md#2025-04-09_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ndbt sl query --metrics <metric_name> --group-by <dimension_name> # In dbt Cloud \ndbt sl query --saved-query <name> # In dbt Cloud\n\nmf query --metrics <metric_name> --group-by <dimension_name> # In dbt Core\n\nOptions:\n\n  --metrics SEQUENCE       Syntax to query single metrics: --metrics metric_name\n                           For example, --metrics bookings\n                           To query multiple metrics, use --metrics followed by the metric names, separated by commas without spaces.\n                           For example,  --metrics bookings,messages\n\n  --group-by SEQUENCE      Syntax to group by single dimension/entity: --group-by dimension_name\n                           For example, --group-by ds\n                           For multiple dimensions/entities, use --group-by followed by the dimension/entity names, separated by commas without spaces.\n                           For example, --group-by ds,org\n                           \n\n  --end-time TEXT          Optional iso8601 timestamp to constraint the end\n                           time of the data (inclusive).\n                           *Not available in dbt Cloud yet \n\n  --start-time TEXT        Optional iso8601 timestamp to constraint the start\n                           time of the data (inclusive)\n                           *Not available in dbt Cloud yet\n\n  --where TEXT             SQL-like where statement provided as a string and wrapped in quotes.\n                           All filter items must explicitly reference fields or dimensions that are part of your model.\n                           To query a single statement: ---where \"{{ Dimension('order_id__revenue') }} > 100\"\n                           To query multiple statements: --where \"{{ Dimension('order_id__revenue') }} > 100 and {{ Dimension('user_count') }} < 1000\"\n                           To add a dimension filter, use the `Dimension()` template wrapper to indicate that the filter item is part of your model. \n                           Refer to the [FAQ](#faqs) for more info on how to do this using a template wrapper.\n\n  --limit TEXT             Limit the number of rows out using an int or leave\n                           blank for no limit. For example: --limit 100\n\n  --order-by SEQUENCE     Specify metrics, dimension, or group bys to order by.\n                          Add the `-` prefix to sort query in descending (DESC) order. \n                          Leave blank for ascending (ASC) order.\n                          For example, to sort metric_time in DESC order: --order-by -metric_time \n                          To sort metric_time in ASC order and revenue in DESC order:  --order-by metric_time,-revenue\n\n  --csv FILENAME           Provide filepath for data frame output to csv\n\n --compile (dbt Cloud)    In the query output, show the query that was\n --explain (dbt Core)     executed against the data warehouse         \n                           \n\n  --show-dataflow-plan     Display dataflow plan in explain output\n\n  --display-plans          Display plans (such as metric dataflow) in the browser\n\n  --decimals INTEGER       Choose the number of decimal places to round for\n                           the numerical values\n\n  --show-sql-descriptions  Shows inline descriptions of nodes in displayed SQL\n\n  --help                   Show this message and exit.\n```\n\n----------------------------------------\n\nTITLE: Setting Default Severity for a Generic Test Definition\nDESCRIPTION: Example showing how to set a default severity level for all instances of a custom generic test by adding a config block to the test definition macro.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/data-test-configs.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\n{% test my_test() %}\n\n    {{ config(severity = 'warn') }}\n\n    select ...\n\n{% endtest %}\n```\n\n----------------------------------------\n\nTITLE: Defining Model Access Control with Groups\nDESCRIPTION: YAML configuration for implementing model access control using groups and privacy levels. It defines a private finance model and a marketing model, demonstrating how to restrict model access between teams.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-01-evolving-data-engineer-craft.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: finance_model\n    access: private\n    group: finance\n  - name: marketing_model\n    group: marketing\n```\n\n----------------------------------------\n\nTITLE: Using Dynamic Configuration in Python Models\nDESCRIPTION: Example of using dynamic configurations within Python f-strings in dbt 1.8+, allowing access to configuration values directly in Python code.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/python-models.md#2025-04-09_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Previously, attempting to access a configuration value like this would result in None\nprint(f\"{dbt.config.get('my_var')}\")  # Output before change: None\n\n# Now you can access the actual configuration value\n# Assuming 'my_var' is configured to 5 for the current model\nprint(f\"{dbt.config.get('my_var')}\")  # Output after change: 5\n```\n\n----------------------------------------\n\nTITLE: Setting Failure Limit for One-off Test in SQL\nDESCRIPTION: This example demonstrates how to configure a failure limit for a one-off (data) test in a SQL file. It sets the limit to 1000 using the config function.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/limit.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(limit = 1000) }}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Creating a Snowflake Table for Staff Members\nDESCRIPTION: This SQL snippet creates a transient table in Snowflake to store staff member information. It defines various fields such as ID, name components, job title, and status indicators.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-07-17-GPT-and-dbt-test.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\ncreate or replace TRANSIENT TABLE STAGING.BASE.STG_STAFF_MEMBER (\n      ID NUMBER(38,0),\n      CREATEDATETIME TIMESTAMP_NTZ(9),\n      UPDATEDATETIME TIMESTAMP_NTZ(9),\n      VERSION NUMBER(38,0),\n      FIRSTNAME VARCHAR(16777216),\n      JOBTITLE VARCHAR(16777216),\n      LASTNAME VARCHAR(16777216),\n      MIDDLENAME VARCHAR(16777216),\n      ISCAREADMIN BOOLEAN,\n      ISARCHIVED BOOLEAN,\n      ADDRESSID VARCHAR(16777216),\n      ENTERPRISEID VARCHAR(16777216),\n      ISDELETED BOOLEAN\n);\n```\n\n----------------------------------------\n\nTITLE: Querying Sources by Database, Schema, and Identifier in GraphQL\nDESCRIPTION: This GraphQL query finds a specific source by its unique database, schema, and identifier. It queries the job with ID 123 and returns the uniqueId of the matching source.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/schema-discovery-job-sources.mdx#2025-04-09_snippet_0\n\nLANGUAGE: graphql\nCODE:\n```\n{\n  job(id: 123) {\n    sources(\n      database: \"analytics\"\n      schema: \"analytics\"\n      identifier: \"dim_customers\"\n    ) {\n      uniqueId\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Staging Orders Model\nDESCRIPTION: Basic staging model that extracts and renames order fields from the source table\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/redshift-qs.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    id as order_id,\n    user_id as customer_id,\n    order_date,\n    status\n\nfrom jaffle_shop.orders\n```\n\n----------------------------------------\n\nTITLE: Importing Trusted Adapters Component in Markdown\nDESCRIPTION: Markdown import statement that includes a component listing trusted adapters from an external file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/trusted-adapters.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nimport AdaptersTrusted from '/snippets/_adapters-trusted.md';\n\n<AdaptersTrusted />\n```\n\n----------------------------------------\n\nTITLE: Corrected Project Name in dbt_project.yml\nDESCRIPTION: The corrected version of the project name, using snake_case as recommended. This configuration will be valid and not cause runtime errors.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/name.md#2025-04-09_snippet_2\n\nLANGUAGE: yml\nCODE:\n```\nname: jaffle_shop\n```\n\n----------------------------------------\n\nTITLE: Granting Snowflake Governance Viewer Access\nDESCRIPTION: SQL command to grant the necessary GOVERNANCE_VIEWER database role to the dbt Cloud deployment role in Snowflake. This permission is required to access query history metadata tables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/model-query-history.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nGRANT DATABASE ROLE SNOWFLAKE.GOVERNANCE_VIEWER TO ROLE <YOUR_DBT_CLOUD_DEPLOYMENT_ROLE>;\n```\n\n----------------------------------------\n\nTITLE: Implementation of dbt_utils.dateadd Macro\nDESCRIPTION: The complete implementation of the dbt_utils.dateadd macro which handles different warehouse adapters by using adapter-specific implementations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-09-11-sql-dateadd.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\n{% macro dateadd(datepart, interval, from_date_or_timestamp) %}\n  {{ adapter_macro('dbt_utils.dateadd', datepart, interval, from_date_or_timestamp) }}\n{% endmacro %}\n\n\n{% macro default__dateadd(datepart, interval, from_date_or_timestamp) %}\n\n    dateadd(\n        {{ datepart }},\n        {{ interval }},\n        {{ from_date_or_timestamp }}\n        )\n\n{% endmacro %}\n\n\n{% macro bigquery__dateadd(datepart, interval, from_date_or_timestamp) %}\n\n        datetime_add(\n            cast( {{ from_date_or_timestamp }} as datetime),\n        interval {{ interval }} {{ datepart }}\n        )\n\n{% endmacro %}\n\n\n{% macro postgres__dateadd(datepart, interval, from_date_or_timestamp) %}\n\n    {{ from_date_or_timestamp }} + ((interval '1 {{ datepart }}') * ({{ interval }}))\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Incorrect Schema Name Generation Implementation\nDESCRIPTION: Example of an incorrect implementation of generate_schema_name() macro that can cause conflicts in shared database environments by omitting the default schema.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/customize-schema-alias.md#2025-04-09_snippet_6\n\nLANGUAGE: jinja\nCODE:\n```\n{% macro generate_schema_name(custom_schema_name, node) -%}\n\n    {%- set default_schema = target.schema -%}\n    {%- if custom_schema_name is none -%}\n\n        {{ default_schema }}\n\n    {%- else -%}\n    # The following is incorrect as it omits {{ default_schema }} before {{ custom_schema_name | trim }}. \n        {{ custom_schema_name | trim }} \n\n    {%- endif -%}\n\n{%- endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Generating dbt Project Documentation\nDESCRIPTION: This command generates the documentation for a dbt project. It compiles resources, runs queries against database metadata, and creates necessary JSON files.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/cmd-docs.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndbt docs generate\n```\n\n----------------------------------------\n\nTITLE: Using dbt Snapshot Command\nDESCRIPTION: The dbt snapshot command is used for batch-based change data capture and should be scheduled to run between hourly and daily intervals to effectively track table changes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Runs/snapshot-frequency.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndbt snapshot\n```\n\n----------------------------------------\n\nTITLE: Specifying Custom Output Location for Source Freshness Results\nDESCRIPTION: Bash command demonstrating how to specify a custom output location for the source freshness JSON results using the --output flag.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/source.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Output source freshness info to a different path\n$ dbt source freshness --output target/source_freshness.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Tags in dbt_project.yml\nDESCRIPTION: Configuration options for applying tags to models, snapshots, seeds, and saved queries in the project configuration file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/tags.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n[models]:\n  [<resource-path>]:\n    +tags: <string> | [<string>] # Supports single strings or list of strings\n\n[snapshots]:\n  [<resource-path>]:\n    +tags: <string> | [<string>]\n\n[seeds]:\n  [<resource-path>]:\n    +tags: <string> | [<string>]\n\n[saved-queries]:\n  [<resource-path>]:\n    +tags: <string> | [<string>]\n```\n\n----------------------------------------\n\nTITLE: Creating Latest Snapshot of Event Streams in SQL\nDESCRIPTION: This macro includes logic to identify the latest state of every domain model object in the table, applying deletes when found. It uses window functions to determine the latest row for each object.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-10-24-demystifying-event-streams.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{%- macro stream_model_latest_snapshot_macro(streams_var, streams_schema='streams') -%}\n{%- set identityFields = var(\"overriddenIdentityFields\") -%}\n\nWITH changeStream AS (\n{% for stream in streams_var %}\n    SELECT\n        '{{ stream }}' as streamName,\n        -- Need to alias ID column here to custom column if its overwritten in the variable\n        RECORD_CONTENT:data.{{ identityFields.get(stream,'id') }} AS idCol,\n        RECORD_METADATA:CreateTime AS createTime,\n        RECORD_CONTENT:changeType::STRING AS changeType,\n        RECORD_CONTENT:data AS data,\n        GET(RECORD_CONTENT,'deletedID') AS deletedID\n    FROM {{ source(streams_schema, stream ) }}\n    {%- if not loop.last %} UNION ALL{% endif -%}\n{% endfor %}\n),\n\norderedStream AS (\n    SELECT\n        cs.*\n        , cs.deletedID IN (SELECT deletedID FROM changeStream WHERE changeType = 'DELETE') AS isDeleted\n        , ROW_NUMBER() OVER (PARTITION BY cs.idCol ORDER BY cs.createTime DESC, cs.changeType DESC) AS LatestRow\n    FROM changeStream AS cs\n    WHERE changeType IN ('INSERT', 'UPDATE')\n),\nselectedStream AS (\n    SELECT\n        *\n    FROM orderedStream\n    WHERE LatestRow = 1\n)\n\n{%- endmacro -%}\n```\n\n----------------------------------------\n\nTITLE: Configuring DeleteSubscriptions Permission in Azure DevOps via CLI\nDESCRIPTION: CLI command to grant the DeleteSubscriptions permission to a service account in Azure DevOps. This permission allows deleting redundant Azure DevOps service hooks subscriptions, though it may be included in EditSubscriptions in recent Azure versions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/git/setup-azure-service-user.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\naz devops security permission update --organization https://dev.azure.com/<org_name> --namespace-id cb594ebe-87dd-4fc9-ac2c-6a10a4c92046 --subject <service_account>@xxxxxx.onmicrosoft.com --token PublisherSecurity/<azure_devops_project_object_id> --allow-bit 4\n```\n\n----------------------------------------\n\nTITLE: Updating ERP Component Data for Tube Replacement\nDESCRIPTION: This SQL-like table structure shows how the ERP system is updated when a component (in this case, a tube) is replaced. It includes the removal date for the old component and the installation of the new one.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-02-modeling-ragged-time-varying-hierarchies.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n| assembly_id | component_id | installed_at | removed_at |\n| -             | -              | -              | -            |\n| ...           | ...            | ...            | ...          |\n| Tire-1        | Tube-1         | 2023-01-01     | 2023-06-01   |\n| Tire-1        | Tube-2         | 2023-06-01     |              |\n| ...           | ...            | ...            | ...          |\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Test Directory in dbt_project.yml\nDESCRIPTION: This YAML snippet demonstrates how to modify the test-paths configuration in the dbt_project.yml file to specify a custom directory for storing tests. It allows users to define a different location for singular and generic tests.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Tests/configurable-data-test-path.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntest-paths: [\"my_cool_tests\"]\n```\n\n----------------------------------------\n\nTITLE: BigQuery Connection Profile Configuration\nDESCRIPTION: YAML configuration for connecting to BigQuery including authentication and project settings.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/manual-install-qs.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\njaffle_shop:\n    target: dev\n    outputs:\n        dev:\n            type: bigquery\n            method: service-account\n            keyfile: /Users/BBaggins/.dbt/dbt-tutorial-project-331118.json\n            project: grand-highway-265418\n            dataset: dbt_bbagins\n            threads: 1\n            timeout_seconds: 300\n            location: US\n            priority: interactive\n```\n\n----------------------------------------\n\nTITLE: Creating Documentation and Tests for a Dimension Model in YAML\nDESCRIPTION: A YAML file that defines documentation and tests for the dim_product model. It sets up not_null and unique tests for key columns in the dimension table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-04-18-building-a-kimball-dimensional-model-with-dbt.md#2025-04-09_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: dim_product\n    columns:\n      - name: product_key \n        description: The surrogate key of the product\n        tests:\n          - not_null\n          - unique\n      - name: productid \n        description: The natural key of the product\n        tests:\n          - not_null\n          - unique\n      - name: product_name \n        description: The product name\n        tests:\n          - not_null\n```\n\n----------------------------------------\n\nTITLE: Creating a Minutes-based Time Spine in SQL\nDESCRIPTION: Creates a time spine table at minute granularity using dbt's date_spine macro. The table generates minutes from 5 minutes before the current time up to the current timestamp.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metricflow-time-spine.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(materialized='table') }}\n\nwith minutes as (\n\n    {{\n        dbt.date_spine(\n            'minute',\n            \"date_trunc('minute', dateadd(minute, -5, current_timestamp()))\",\n            \"date_trunc('minute', current_timestamp())\"\n        )\n    }}\n\n),\n\nfinal as (\n    select cast(date_minute as timestamp) as minute_timestamp\n    from minutes\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Creating Tables in Starburst Galaxy for Jaffle Shop Data\nDESCRIPTION: SQL queries to create a schema and tables for Jaffle Shop data in Starburst Galaxy. These queries create a 'jaffle_shop' schema and tables for customers, orders, and payments using data from an S3 bucket.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/starburst-galaxy-qs.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nCREATE SCHEMA jaffle_shop WITH (location='s3://YOUR_S3_BUCKET_NAME/dbt-quickstart/');\n\nCREATE TABLE jaffle_shop.jaffle_shop_customers (\n    id VARCHAR,\n    first_name VARCHAR,\n    last_name VARCHAR\n)\n\nWITH (\n    external_location = 's3://YOUR_S3_BUCKET_NAME/dbt-quickstart/jaffle-shop-customers/',\n    format = 'csv',\n    type = 'hive',\n    skip_header_line_count=1\n\n);\n\nCREATE TABLE jaffle_shop.jaffle_shop_orders (\n\n    id VARCHAR,\n    user_id VARCHAR,\n    order_date VARCHAR,\n    status VARCHAR\n\n)\n\nWITH (\n    external_location = 's3://YOUR_S3_BUCKET_NAME/dbt-quickstart/jaffle-shop-orders/',\n    format = 'csv',\n    type = 'hive',\n    skip_header_line_count=1\n);\n\nCREATE TABLE jaffle_shop.stripe_payments (\n\n    id VARCHAR,\n    order_id VARCHAR,\n    paymentmethod VARCHAR,\n    status VARCHAR,\n    amount VARCHAR,\n    created VARCHAR\n)\n\nWITH (\n\n    external_location = 's3://YOUR_S3_BUCKET_NAME/dbt-quickstart/stripe-payments/',\n    format = 'csv',\n    type = 'hive',\n    skip_header_line_count=1\n\n);\n```\n\n----------------------------------------\n\nTITLE: Installing dbt Core with Snowflake Adapter\nDESCRIPTION: Example of installing dbt Core with Snowflake adapter using the new installation procedure.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/core-upgrade/07-upgrading-to-v1.8.md#2025-04-09_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install dbt-core dbt-snowflake\n```\n\n----------------------------------------\n\nTITLE: DBT Audit Query Materialization Pattern\nDESCRIPTION: Code pattern for materializing audit results in the data warehouse instead of printing to terminal. Demonstrates how to modify the comparison macro to save results rather than display them.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-03-23-audit-helper.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{{ audit_query }}\n```\n\n----------------------------------------\n\nTITLE: Configuring an Existing Date Dimension as a Time Spine\nDESCRIPTION: This YAML configuration shows how to use an existing date dimension model as a time spine for MetricFlow. It specifies the standard granularity column and defines multiple date-related columns with their respective granularities.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/mf-time-spine.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: dim_date\n    description: An existing date dimension model used as a time spine.\n    time_spine:\n      standard_granularity_column: date_day\n    columns:\n      - name: date_day\n        granularity: day\n      - name: day_of_week\n        granularity: day\n      - name: full_date\n        granularity: day\n```\n\n----------------------------------------\n\nTITLE: Querying with Where Filter as String Format\nDESCRIPTION: Example of querying the semantic layer using a where filter in string format. This approach filters metrics based on time dimension, customer type, and order ID using a single filter string.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-jdbc.md#2025-04-09_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nselect * from {{\nsemantic_layer.query(metrics=['food_order_amount', 'order_gross_profit'],\ngroup_by=[Dimension('metric_time').grain('month'),'customer__customer_type'],\nwhere=\"{{ Dimension('metric_time').grain('month')  }} >= '2017-03-09' AND {{ Dimension('customer__customer_type' }} in ('new') AND {{ Entity('order_id') }} = 10\")\n}}\n```\n\n----------------------------------------\n\nTITLE: Defining Project Name in dbt_project.yml\nDESCRIPTION: The basic configuration for setting the project name in dbt. The name must be a string consisting of letters, digits, and underscores only, and cannot start with a digit.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/name.md#2025-04-09_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nname: string\n```\n\n----------------------------------------\n\nTITLE: DATEADD Function Syntax in Snowflake\nDESCRIPTION: The syntax for the DATEADD function in Snowflake requires datepart, interval, and from_date parameters in that specific order. Supports hour, minute, and second intervals.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-09-11-sql-dateadd.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\ndateadd( {{ datepart }}, {{ interval }}, {{ from_date }} )\n```\n\n----------------------------------------\n\nTITLE: Example Redshift Query Optimization Workflow\nDESCRIPTION: A structured workflow for identifying and optimizing problematic joins in Redshift queries. Details a step-by-step process for benchmarking and improving query performance through join optimization, distribution strategies, and sort key configurations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-05-19-redshift-configurations-dbt-model-optimizations.md#2025-04-09_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n1. Run the problematic model (I do this a couple of times to get a baseline average on runtime). Notate the build time.\n2. Comment out joins and one by one, run the model. Keep doing this until you find which join is causing unideal run times.\n3. Decide on how best to optimize the join:\n    - Optimize the logic or flow, such as moving the calculation on a key to a prior CTE or upstream model before the join.\n    - Optimizing the distribution, such as doing the join in an upstream model so you can facilitate co-location of the data.\n    - Optimizing the sort, such as identifying and assigning a frequently filtered column so that finding data is faster in downstream processing.\n```\n\n----------------------------------------\n\nTITLE: Displaying OAuth Role Error Message in Snowflake Connection\nDESCRIPTION: This code snippet shows the error message displayed when there's an issue with the role requested in the OAuth connection to Snowflake. It indicates that the role is not listed in the Access Token or was filtered.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Troubleshooting/failed-snowflake-oauth-connection.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nFailed to connect to DB: xxxxxxx.snowflakecomputing.com:443. The role requested in the connection, or the default role if none was requested in the connection ('xxxxx'), is not listed in the Access Token or was filtered. \nPlease specify another role, or contact your OAuth Authorization server administrator.\n```\n\n----------------------------------------\n\nTITLE: Disabling Partial Parsing via Command Line\nDESCRIPTION: Command line example showing how to disable partial parsing when running DBT commands.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/parsing.md#2025-04-09_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ndbt --no-partial-parse run\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Exposures in YAML\nDESCRIPTION: This snippet illustrates how to configure dbt exposures using the 'config' property in a YAML file. It allows setting configurations such as 'enabled' and 'meta'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/config.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nexposures:\n  - name: <exposure_name>\n    config:\n      [enabled](/reference/resource-configs/enabled): true | false\n      [meta](/reference/resource-configs/meta): {dictionary}\n```\n\n----------------------------------------\n\nTITLE: Database Query Execution Tip\nDESCRIPTION: Markdown tip block explaining the basic concept of query engine execution of physical plans to return tabular data\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2025-01-24-sql-comprehension-technologies.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n:::tip\nA query engine can **execute** a *physical plan* and return tabular data\n:::\n```\n\n----------------------------------------\n\nTITLE: Setting RSA Public Key for Snowflake User Authentication\nDESCRIPTION: SQL command to configure key pair authentication for a Snowflake user. This associates the public key with the user account to enable key pair authentication from dbt Cloud.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/connect-data-platform/connect-snowflake.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nalter user jsmith set rsa_public_key='MIIBIjANBgkqh...';\n```\n\n----------------------------------------\n\nTITLE: Using + Prefix in dbt Configuration\nDESCRIPTION: Example demonstrating the use of the + prefix in dbt_project.yml to differentiate between resource properties and configuration parameters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/define-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  jaffle_shop:  # this is the project name\n    marts:\n      +materialized: table  # this configures all models in the marts/ directory\n      marketing:  # this is the subdirectory name\n        +materialized: view  # this configures all models in the marts/marketing/ directory\n```\n\n----------------------------------------\n\nTITLE: Incorrectly Configured Snapshot with Table Materialization in dbt\nDESCRIPTION: An example of a snapshot incorrectly configured with 'table' materialization which causes issues. This was silently accepted in dbt versions before 1.4 but now raises an error.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Snapshots/snapshot-target-is-not-a-snapshot-table.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot snappy %}\n  {{ config(materialized = 'table', ...) }}\n  ...\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Default Quoting Configuration for Snowflake\nDESCRIPTION: Default quoting configuration specific to Snowflake where all quoting options are set to false. This prevents identifiers from becoming case-sensitive and makes them easier to reference.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/quoting.md#2025-04-09_snippet_2\n\nLANGUAGE: yml\nCODE:\n```\nquoting:\n  database: false\n  schema: false\n  identifier: false\n\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Profile for Oracle ML Cloud\nDESCRIPTION: YAML configuration for setting up a dbt profile with Oracle ML Cloud, including database connection details and OML cloud service URL.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/oracle-setup.md#2025-04-09_snippet_19\n\nLANGUAGE: yaml\nCODE:\n```\ndbt_test:\n   target: dev\n   outputs:\n      dev:\n         type: oracle\n         user: \"{{ env_var('DBT_ORACLE_USER') }}\"\n         pass: \"{{ env_var('DBT_ORACLE_PASSWORD') }}\"\n         database: \"{{ env_var('DBT_ORACLE_DATABASE') }}\"\n         tns_name: \"{{ env_var('DBT_ORACLE_TNS_NAME') }}\"\n         schema: \"{{ env_var('DBT_ORACLE_SCHEMA') }}\"\n         oml_cloud_service_url: \"https://tenant1-dbt.adb.us-sanjose-1.oraclecloudapps.com\"\n```\n\n----------------------------------------\n\nTITLE: Partition Copy Configuration in dbt BigQuery\nDESCRIPTION: Enhanced configuration that enables partition copy by adding the copy_partitions flag. This configuration improves performance when updating partitioned data in BigQuery.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-02-01-ingestion-time-partitioning-bigquery.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized = 'incremental',\n    incremental_strategy = 'insert_overwrite',\n    partition_by = {\n      \"field\": \"day\",\n      \"data_type\": \"date\",\n      \"copy_partitions\": true\n    }\n) }}\n\nselect\n    day,\n    campaign_id,\n    NULLIF(COUNTIF(action = 'impression'), 0) impressions_count\nfrom {{ source('logs', 'tracking_events') }}\ngroup by day, campaign_id\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Calendar Granularities in YAML\nDESCRIPTION: YAML configuration for defining custom time granularities in MetricFlow. This example shows how to map a standard granularity column to a custom fiscal year column for specialized time-based querying.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metricflow-time-spine.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n - name: my_time_spine\n   description: my favorite time spine\n   time_spine:\n      standard_granularity_column: date_day\n      custom_granularities:\n        - name: fiscal_year\n          column_name: fiscal_year_column\n```\n\n----------------------------------------\n\nTITLE: Configuring PostgreSQL Source in YAML\nDESCRIPTION: This snippet shows the YAML configuration for integrating data from a PostgreSQL source. It includes options for table selection, column exclusion, and various job settings.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/upsolver-configs.md#2025-04-09_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\ntable_include_list: ('<regexFilter>', ...)\ncolumn_exclude_list: ('<regexFilter>', ...)\nheartbeat_table: '<heartbeat_table>'\nskip_snapshots: True/False\npublication_name: '<publication_name>'\nend_at: '<timestamp>/NOW'\ncompute_cluster: '<compute_cluster>'\ncomment: '<comment>'\nparse_json_columns: True/False\ncolumn_transformations:\n  '<column>': '<expression>'\nsnapshot_parallelism: <integer>\nexclude_columns: ('<exclude_column>', ...)\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment ID for Defer in dbt Cloud CLI using dbt_project.yml\nDESCRIPTION: Configure the defer-env-id in the dbt_project.yml file as an alternative method to specify which environment should be used for deferral artifacts when using the dbt Cloud CLI.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/about-cloud-develop-defer.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndbt-cloud:\n  defer-env-id: '123456'\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment ID for Defer in dbt Cloud CLI using dbt_project.yml\nDESCRIPTION: Configure the defer-env-id in the dbt_project.yml file as an alternative method to specify which environment should be used for deferral artifacts when using the dbt Cloud CLI.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/about-cloud-develop-defer.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndbt-cloud:\n  defer-env-id: '123456'\n```\n\n----------------------------------------\n\nTITLE: Refactored Customer Model with Staging References\nDESCRIPTION: Updated customer transformation model that references staging models using dbt ref functions\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/redshift-qs.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nwith customers as (\n\n    select * from {{ ref('stg_customers') }}\n\n),\n\norders as (\n\n    select * from {{ ref('stg_orders') }}\n\n),\n\ncustomer_orders as (\n\n    select\n        customer_id,\n\n        min(order_date) as first_order_date,\n        max(order_date) as most_recent_order_date,\n        count(order_id) as number_of_orders\n\n    from orders\n\n    group by 1\n\n),\n\nfinal as (\n\n    select\n        customers.customer_id,\n        customers.first_name,\n        customers.last_name,\n        customer_orders.first_order_date,\n        customer_orders.most_recent_order_date,\n        coalesce(customer_orders.number_of_orders, 0) as number_of_orders\n\n    from customers\n\n    left join customer_orders using (customer_id)\n\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Creating Relations with the Relation Class in Python\nDESCRIPTION: The Relation class provides a method to safely create database object references with appropriate quoting. The create method accepts database, schema, identifier, and type parameters to construct a properly formatted relation reference.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-classes.md#2025-04-09_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass Relation:\n  def create(database=None, schema=None, identifier=None,\n             type=None):\n  \"\"\"\n    database (optional): The name of the database for this relation\n    schema (optional): The name of the schema (or dataset, if on BigQuery) for this relation\n    identifier (optional): The name of the identifier for this relation\n    type (optional): Metadata about this relation, eg: \"table\", \"view\", \"cte\"\n  \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Refactoring Customer Model to Use Staging Models in SQL for dbt\nDESCRIPTION: This SQL query refactors the customer model to use the staging models (stg_customers and stg_orders) created earlier, using dbt's ref function to reference these models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/bigquery-qs.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nwith customers as (\n\n    select * from {{ ref('stg_customers') }}\n\n),\n\norders as (\n\n    select * from {{ ref('stg_orders') }}\n\n),\n\ncustomer_orders as (\n\n    select\n        customer_id,\n\n        min(order_date) as first_order_date,\n        max(order_date) as most_recent_order_date,\n        count(order_id) as number_of_orders\n\n    from orders\n\n    group by 1\n\n),\n\nfinal as (\n\n    select\n        customers.customer_id,\n        customers.first_name,\n        customers.last_name,\n        customer_orders.first_order_date,\n        customer_orders.most_recent_order_date,\n        coalesce(customer_orders.number_of_orders, 0) as number_of_orders\n\n    from customers\n\n    left join customer_orders using (customer_id)\n\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Using Metric Aliases with Semantic Layer JDBC API\nDESCRIPTION: SQL query demonstrating how to query metrics using aliases for simpler or more intuitive column names in the result set. The query returns the alias as the metric name even if not defined in the metric configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-jdbc.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\nselect * from {{\n    semantic_layer.query(metrics=[Metric(\"metric_name\", alias=\"metric_alias\")])\n}}\n```\n\n----------------------------------------\n\nTITLE: Compile-only dbt Command\nDESCRIPTION: Command to generate a stable manifest for state comparison purposes without executing models. Used in scheduled jobs to maintain consistent comparison state.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/set-up-ci.md#2025-04-09_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ndbt compile\n```\n\n----------------------------------------\n\nTITLE: JSON Format Log Output Example\nDESCRIPTION: Example of structured JSON format logging output\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/logs.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\"data\": {\"log_version\": 3, \"version\": \"=1.8.0\"}, \"info\": {\"category\": \"\", \"code\": \"A001\", \"extra\": {}, \"invocation_id\": \"82131fa0-d2b4-4a77-9436-019834e22746\", \"level\": \"info\", \"msg\": \"Running with dbt=1.8.0\", \"name\": \"MainReportVersion\", \"pid\": 7875, \"thread\": \"MainThread\", \"ts\": \"2024-05-29T23:32:54.993336Z\"}}\n{\"data\": {\"adapter_name\": \"postgres\", \"adapter_version\": \"=1.8.0\"}, \"info\": {\"category\": \"\", \"code\": \"E034\", \"extra\": {}, \"invocation_id\": \"82131fa0-d2b4-4a77-9436-019834e22746\", \"level\": \"info\", \"msg\": \"Registered adapter: postgres=1.8.0\", \"name\": \"AdapterRegistered\", \"pid\": 7875, \"thread\": \"MainThread\", \"ts\": \"2024-05-29T23:32:56.437986Z\"}}\n```\n\n----------------------------------------\n\nTITLE: Aggregating Order Events with CTE in dbt\nDESCRIPTION: Common Table Expression (CTE) that aggregates order_placed events by user_id, calculating the first order date, most recent order date, and total count of orders.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-02-07-customer-360-view-census-playbook.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\norder_events as (\n\n    select\n        user_id,\n\n        min(timestamp) as first_order,\n        max(timestamp) as most_recent_order,\n        count(event_id) as number_of_orders\n\n    from events\n    where event_name = 'order_placed'\n    group by 1\n\n),\n```\n\n----------------------------------------\n\nTITLE: Handling Null Values in Surrogate Key Generation\nDESCRIPTION: This SQL snippet demonstrates how to handle null values when generating surrogate keys by using COALESCE and CAST functions. It ensures that null values are replaced with a default string.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-11-22-sql-surrogate-keys.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nselect\n  *,\n  concat(\n    coalesce(cast(user_id as string), '_this_used_to_be_null_'),\n    coalesce(cast(product_id as string), '_this_used_to_be_null_')\n    ) as _surrogate_key\nfrom example_ids\n```\n\n----------------------------------------\n\nTITLE: Querying invocation_id in SQL\nDESCRIPTION: Example SQL query demonstrating how to use the invocation_id macro in a SELECT statement. This query selects the invocation_id as a test_id column from a specified table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/invocation_id.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect '{{ invocation_id }}' as test_id from TABLE_NAME\n```\n\n----------------------------------------\n\nTITLE: Querying Test Coverage and Status with GraphQL\nDESCRIPTION: This GraphQL query retrieves test results for a given environment, including test names, parent nodes, and execution information.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/discovery-use-cases-and-examples.md#2025-04-09_snippet_11\n\nLANGUAGE: graphql\nCODE:\n```\nquery ($environmentId: BigInt!, $first: Int!) {\n  environment(id: $environmentId) {\n    applied {\n      tests(first: $first) {\n        edges {\n          node {\n            name\n            columnName\n            parents {\n              name\n              resourceType\n            }\n            executionInfo {\n              lastRunStatus\n              lastRunError\n              executeCompletedAt\n              executionTime\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying dbt Project Folder Structure in Shell\nDESCRIPTION: This code snippet shows the folder structure of a typical dbt project named 'jaffle_shop'. It includes folders for analyses, seeds, macros, snapshots, and tests, demonstrating the standard organization of a dbt project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-structure/5-the-rest-of-the-project.md#2025-04-09_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\njaffle_shop\n analyses\n seeds\n    employees.csv\n macros\n    _macros.yml\n    cents_to_dollars.sql\n snapshots\n tests\n assert_positive_value_for_total_amount.sql\n```\n\n----------------------------------------\n\nTITLE: Aliasing a Snapshot in dbt_project.yml\nDESCRIPTION: Configures an alias for a snapshot named 'your_snapshot' at the project level in the dbt_project.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/alias.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  your_project:\n    your_snapshot:\n      +alias: the_best_snapshot\n```\n\n----------------------------------------\n\nTITLE: Creating a Logical Mart Table for Order Items in SQL\nDESCRIPTION: SQL code for building a logical mart table at the order_items grain that joins order items with orders, products, and supplies to create a consolidated dataset for semantic modeling. The query handles aggregating supply costs and mapping fields from various tables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-build-our-metrics/semantic-layer-8-refactor-a-rollup.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{\\n   config(\\n      materialized = 'table',\\n   )\\n}}\\n\\nwith\\n\\norder_items as (\\n\\n   select * from {{ ref('stg_order_items') }}\\n\\n),\\n\\norders as (\\n\\n   select * from {{ ref('stg_orders')}}\\n\\n),\\n\\nproducts as (\\n\\n   select * from {{ ref('stg_products') }}\\n\\n),\\n\\nsupplies as (\\n\\n   select * from {{ ref('stg_supplies') }}\\n\\n),\\n\\norder_supplies_summary as (\\n\\n   select\\n      product_id,\\n      sum(supply_cost) as supply_cost\\n\\n   from supplies\\n\\n   group by 1\\n),\\n\\njoined as (\\n\\n   select\\n      order_items.*,\\n      products.product_price,\\n      order_supplies_summary.supply_cost,\\n      products.is_food_item,\\n      products.is_drink_item,\\n      orders.ordered_at\\n\\n   from order_items\\n\\n   left join orders on order_items.order_id  = orders.order_id\\n\\n   left join products on order_items.product_id = products.product_id\\n\\n   left join order_supplies_summary on order_items.product_id = order_supplies_summary.product_id\\n\\n)\\n\\nselect * from joined\n```\n\n----------------------------------------\n\nTITLE: Compiled Reference in Development Environment\nDESCRIPTION: Shows how the ref function compiles in a development environment\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-01-09-defer-in-development.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n-- in target/compiled/models/my_model.sql\nselect * from analytics.dbt_dconnors.model_a\n```\n\n----------------------------------------\n\nTITLE: Running models downstream of a specific source table in dbt\nDESCRIPTION: This command runs models downstream of a specific table ('orders') within the 'jaffle_shop' source in dbt. It shows how to target a particular source table using the dot notation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Runs/running-models-downstream-of-source.md#2025-04-09_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$ dbt run --select source:jaffle_shop.orders+\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Profile for Oracle (Connect String)\nDESCRIPTION: Sets up the dbt profile using connect string for Oracle connection.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/oracle-setup.md#2025-04-09_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\ndbt_test:\n   target: \"{{ env_var('DBT_TARGET', 'dev') }}\"\n   outputs:\n      dev:\n         type: oracle\n         user: \"{{ env_var('DBT_ORACLE_USER') }}\"\n         pass: \"{{ env_var('DBT_ORACLE_PASSWORD') }}\"\n         database: \"{{ env_var('DBT_ORACLE_DATABASE') }}\"\n         schema: \"{{ env_var('DBT_ORACLE_SCHEMA') }}\"\n         connection_string: \"{{ env_var('DBT_ORACLE_CONNECT_STRING') }}\"\n```\n\n----------------------------------------\n\nTITLE: Running dbt Project Evaluator Package in Bash\nDESCRIPTION: This command runs and tests all models in the dbt Project Evaluator package. It uses the dbt build command with a select option to focus on the package models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-11-30-dbt-project-evaluator.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndbt build --select package:dbt_project_evaluator\n```\n\n----------------------------------------\n\nTITLE: Defining Cumulative Metrics in dbt YAML (version 1.9+)\nDESCRIPTION: This YAML snippet defines three cumulative metrics: all-time cumulative order total, trailing 1-month cumulative order total, and month-to-date cumulative order total. It uses 'type_params' and 'cumulative_type_params' to specify measure, window, and grain_to_date parameters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/cumulative-metrics.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  - name: cumulative_order_total\n    label: Cumulative order total (All-Time)    \n    description: The cumulative value of all orders\n    type: cumulative\n    type_params:\n      measure: \n        name: order_total\n  \n  - name: cumulative_order_total_l1m\n    label: Cumulative order total (L1M)   \n    description: Trailing 1-month cumulative order total\n    type: cumulative\n    type_params:\n      measure: \n        name: order_total\n      cumulative_type_params:\n        window: 1 month\n  \n  - name: cumulative_order_total_mtd\n    label: Cumulative order total (MTD)\n    description: The month-to-date value of all orders\n    type: cumulative\n    type_params:\n      measure: \n        name: order_total\n      cumulative_type_params:\n        grain_to_date: month\n```\n\n----------------------------------------\n\nTITLE: Querying Customer Data in dbt Cloud\nDESCRIPTION: SQL query to select all data from the customers table in the jaffle_shop dataset. This query is used as an example of directly querying data from the warehouse in dbt Cloud.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/bigquery-qs.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect * from `dbt-tutorial.jaffle_shop.customers`\n```\n\n----------------------------------------\n\nTITLE: Upgrading dbt Core\nDESCRIPTION: Command to upgrade dbt Core to the latest version using pip.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/hybrid-setup.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install --upgrade dbt-core\n```\n\n----------------------------------------\n\nTITLE: Creating Staging Orders Model\nDESCRIPTION: SQL model that transforms raw order data into a staging table with selected columns and renamed fields.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/mesh-qs.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    id as order_id,\n    user_id as customer_id,\n    order_date,\n    status\n\nfrom {{ source('jaffle_shop', 'orders') }}\n```\n\n----------------------------------------\n\nTITLE: ModelByEnvironment Query Before Deprecation - GraphQL\nDESCRIPTION: Example of modelByEnvironment query before being moved into the environment object.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/2023-release-notes.md#2025-04-09_snippet_3\n\nLANGUAGE: graphql\nCODE:\n```\nquery ($environmentId: Int!, $uniqueId: String) {\n    modelByEnvironment(environmentId: $environmentId, uniqueId: $uniqueId) {\n        uniqueId\n        executionTime\n        executeCompletedAt\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Granting Privileges in Databricks SQL\nDESCRIPTION: SQL command to grant all privileges on the default schema to users, ensuring access for potential collaborators on the dbt project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/databricks-qs.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\ngrant all privileges on schema default to users;\n```\n\n----------------------------------------\n\nTITLE: Setting project flags in dbt_project.yml\nDESCRIPTION: Demonstrates how to set global configurations in the dbt_project.yml file using the flags dictionary. This is the primary method for configuring project-wide settings and opting out of behavior changes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/project-flags.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nflags:\n  <global_config>: <value>\n```\n\n----------------------------------------\n\nTITLE: Querying Transformed Data\nDESCRIPTION: SQL query to analyze average tip amounts by vendor from the transformed dataset.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dremio-lakehouse.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nSELECT vendor_id,\n       AVG(tip_amount)\nFROM dev.application.\"nyc_treips_with_weather\"\nGROUP BY vendor_id\n```\n\n----------------------------------------\n\nTITLE: Running dbt Source Freshness Command with Custom Output\nDESCRIPTION: This command runs the dbt source freshness check and specifies a custom output file using the -o flag.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Project/dbt-source-freshness.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndbt source freshness -o custom_output.json\n```\n\n----------------------------------------\n\nTITLE: Compiling SQL from a Saved Query\nDESCRIPTION: Example showing how to compile SQL using a saved query. This is useful for frequently used queries that you want to inspect without execution.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-jdbc.md#2025-04-09_snippet_28\n\nLANGUAGE: sql\nCODE:\n```\nselect * from {{ semantic_layer.query(saved_query=\"new_customer_orders\", limit=5, compile=True}}\n```\n\n----------------------------------------\n\nTITLE: Creating Jaffle Shop Database in Teradata\nDESCRIPTION: Creates a new database named jaffle_shop with specified permanent space allocation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/teradata-qs.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nCREATE DATABASE jaffle_shop AS PERM = 1e9;\n```\n\n----------------------------------------\n\nTITLE: Project-Level Schema Configuration\nDESCRIPTION: Project-wide configuration examples for different resource types in dbt_project.yml.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/schema.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  jaffle_shop: # the name of a project\n    marketing:\n      +schema: marketing\n```\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  +schema: mappings\n```\n\nLANGUAGE: yaml\nCODE:\n```\ntests:\n  +store_failures: true\n  +schema: _sad_test_failures  # Will write tables to my_database.my_schema__sad_test_failures\n```\n\n----------------------------------------\n\nTITLE: Creating a Composite unique_key Expression with String Concatenation\nDESCRIPTION: Shows how to create a composite unique key by concatenating multiple columns with a separator. This technique allows tracking changes in records that are uniquely identified by multiple columns.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snapshots-jinja-legacy.md#2025-04-09_snippet_19\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot transaction_items_snapshot %}\n\n    {{\n        config(\n          unique_key=\"transaction_id||'-'||line_item_id\",\n          ...\n        )\n    }}\n\nselect\n    transaction_id||'-'||line_item_id as id,\n    *\nfrom {{ source('erp', 'transactions') }}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Measures in YAML (dbt v1.9+)\nDESCRIPTION: Complete YAML specification for defining measures within semantic models. This example shows the structure with all possible parameters including name, description, aggregation type, expression, aggregation parameters, time dimension, and configuration options.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/measures.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_models:\n  - name: semantic_model_name\n   ..rest of the semantic model config\n    measures:\n      - name: The name of the measure\n        description: 'same as always' ## Optional\n        agg: the aggregation type.\n        expr: the field\n        agg_params: 'specific aggregation properties such as a percentile'  ## Optional\n        agg_time_dimension: The time field. Defaults to the default agg time dimension for the semantic model. ##  Optional\n        non_additive_dimension: 'Use these configs when you need non-additive dimensions.' ## Optional\n        [config](/reference/resource-properties/config): Use the config property to specify configurations for your measure.  ## Optional\n          [meta](/reference/resource-configs/meta):  {<dictionary>} Set metadata for a resource and organize resources. Accepts plain text, spaces, and quotes. ## Optional\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Source with Custom Schema and Identifier in YAML\nDESCRIPTION: This YAML configuration demonstrates how to set up a dbt source with custom schema and identifier properties. It allows mapping a well-named source to a poorly named database schema and table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Project/source-has-bad-name.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: jaffle_shop\n    schema: postgres_backend_public_schema\n    database: raw\n    tables:\n      - name: orders\n        identifier: api_orders\n```\n\n----------------------------------------\n\nTITLE: Granting Schema Privileges in Databricks SQL\nDESCRIPTION: SQL commands demonstrating how to grant different levels of permissions on schema objects to principals (users, service principals, or groups). Shows examples of granting all privileges, table creation rights, and view creation permissions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/database-permissions/databricks-permissions.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\ngrant all privileges on schema schema_name to principal;\ngrant create table on schema schema_name to principal;\ngrant create view on schema schema_name to principal;\n```\n\n----------------------------------------\n\nTITLE: Setting Oracle Connection Environment Variables\nDESCRIPTION: Defines mandatory environment variables for Oracle Database connection in dbt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/oracle-setup.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexport DBT_ORACLE_USER=<username>\nexport DBT_ORACLE_PASSWORD=***\nexport DBT_ORACLE_SCHEMA=<username>\nexport DBT_ORACLE_DATABASE=example_db2022adb\n```\n\n----------------------------------------\n\nTITLE: Blog Post Front Matter Configuration in Markdown\nDESCRIPTION: YAML front matter configuration for a blog post, specifying metadata like title, description, authors, tags, and publication details.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2025-01-21-wish-i-had-a-control-plane-for-my-renovation.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: \"Why I wish I had a control plane for my renovation\"\ndescription: \"When I think back to my renovation, I realize how much smoother it would've been if I'd had a control plane for the entire process.\"\nslug: wish-i-had-a-control-plane-for-my-renovation\n\nauthors: [mark_wan]\n\ntags: [analytics craft, data_ecosystem]\nhide_table_of_contents: false\n\ndate: 2025-01-21\nis_featured: true\n---\n```\n\n----------------------------------------\n\nTITLE: Compiled SQL for Cumulative Metric with Grain-to-Date\nDESCRIPTION: This SQL snippet demonstrates how the cumulative metric with grain-to-date is compiled in the database, including the use of window functions and date truncation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/cumulative-metrics.md#2025-04-09_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nwith staging as (\n    select\n        subq_3.date_day as metric_time__day,\n        date_trunc('week', subq_3.date_day) as metric_time__week,\n        sum(subq_1.order_count) as orders_last_month_to_date\n    from dbt_jstein.metricflow_time_spine subq_3\n    inner join (\n        select\n            date_trunc('day', ordered_at) as metric_time__day,\n            1 as order_count\n        from analytics.dbt_jstein.orders orders_src_10000\n    ) subq_1\n    on (\n        subq_1.metric_time__day <= subq_3.date_day\n    ) and (\n        subq_1.metric_time__day >= date_trunc('month', subq_3.date_day)\n    )\n    group by\n        subq_3.date_day,\n        date_trunc('week', subq_3.date_day)\n)\n\nselect\n    *\nfrom (\n    select\n        metric_time__week,\n        first_value(orders_last_month_to_date) over (partition by date_trunc('week', metric_time__day) order by metric_time__day) as cumulative_revenue\n    from\n        staging\n)\ngroup by\n    metric_time__week,\n    cumulative_revenue\norder by\n    metric_time__week\n    1\n```\n\n----------------------------------------\n\nTITLE: Basic Defer Command Usage in Shell\nDESCRIPTION: Shows the basic shell commands for using defer with dbt run and test operations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/defer.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndbt run --select [...] --defer --state path/to/artifacts\ndbt test --select [...] --defer --state path/to/artifacts\n```\n\n----------------------------------------\n\nTITLE: Running and Previewing the Time Spine Model with dbt Commands\nDESCRIPTION: These dbt CLI commands build and preview the time spine model. The first command runs the model to materialize it in the database, while the second displays a preview of the data, which is useful for validating the time spine's contents.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/mf-time-spine.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --select time_spine_daily\ndbt show --select time_spine_daily # Use this command to preview the model if developing locally\n```\n\n----------------------------------------\n\nTITLE: Creating Fact Orders Model\nDESCRIPTION: SQL model that builds a fact table combining customer and order information with calculated fields using CTEs and joins.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/mesh-qs.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nwith customers as (\n    select * \n    from {{ ref('stg_customers') }}\n),\n\norders as (\n    select * \n    from {{ ref('stg_orders') }}\n),\n\ncustomer_orders as (\n    select\n        customer_id,\n        min(order_date) as first_order_date\n    from orders\n    group by customer_id\n),\n\nfinal as (\n    select\n        o.order_id,\n        o.order_date,\n        o.status,\n        c.customer_id,\n        c.first_name,\n        c.last_name,\n        co.first_order_date,\n        {{ datediff('first_order_date', 'order_date', 'day') }} as days_as_customer_at_purchase\n    from orders o\n    left join customers c using (customer_id)\n    left join customer_orders co using (customer_id)\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Configuring Connection Options for Various Storage Systems\nDESCRIPTION: This code snippet shows the configuration syntax for various connection options across different storage systems and databases. It includes parameters such as AWS credentials, host information, SSL settings, and authentication details.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/upsolver-configs.md#2025-04-09_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n| Option | Storage   | Editable | Optional | Config Syntax |\n| -------| --------- | -------- | -------- | ------------- |\n| aws_role | s3 | True | True | 'aws_role': `'<aws_role>'` |\n| external_id | s3 | True | True | 'external_id': `'<external_id>'` |\n| aws_access_key_id | s3 | True | True | 'aws_access_key_id': `'<aws_access_key_id>'` |\n| aws_secret_access_key | s3 | True | True | 'aws_secret_access_key_id': `'<aws_secret_access_key_id>'` |\n| path_display_filter | s3 | True | True | 'path_display_filter': `'<path_display_filter>'` |\n| path_display_filters | s3 | True | True | 'path_display_filters': (`'<filter>'`, ...) |\n| read_only | s3 | True | True | 'read_only': True/False |\n| encryption_kms_key | s3 | True | True | 'encryption_kms_key': `'<encryption_kms_key>'` |\n| encryption_customer_managed_key | s3 | True | True | 'encryption_customer_kms_key': `'<encryption_customer_kms_key>'` |\n| comment | s3 | True | True | 'comment': `'<comment>'` |\n| host | kafka | False | False | 'host': `'<host>'` |\n| hosts | kafka | False | False | 'hosts': (`'<host>'`, ...) |\n| consumer_properties | kafka | True | True | 'consumer_properties': `'<consumer_properties>'` |\n| version | kafka | False | True | 'version': `'<value>'` |\n| require_static_ip | kafka | True | True | 'require_static_ip': True/False |\n| ssl | kafka | True | True | 'ssl': True/False |\n| topic_display_filter | kafka | True | True | 'topic_display_filter': `'<topic_display_filter>'` |\n| topic_display_filters | kafka | True | True | 'topic_display_filter': (`'<filter>'`, ...) |\n| comment | kafka | True | True | 'comment': `'<comment>'` |\n| aws_role | glue_catalog | True | True | 'aws_role': `'<aws_role>'` |\n| external_id | glue_catalog | True | True | 'external_id': `'<external_id>'` |\n| aws_access_key_id | glue_catalog | True | True | 'aws_access_key_id': `'<aws_access_key_id>'` |\n| aws_secret_access_key | glue_catalog | True | True | 'aws_secret_access_key': `'<aws_secret_access_key>'` |\n| default_storage_connection | glue_catalog | False | False | 'default_storage_connection': `'<default_storage_connection>'` |\n| default_storage_location | glue_catalog | False | False | 'default_storage_location': `'<default_storage_location>'` |\n| region | glue_catalog | False | True | 'region': `'<region>'` |\n| database_display_filter | glue_catalog | True | True | 'database_display_filter': `'<database_display_filter>'` |\n| database_display_filters | glue_catalog | True | True | 'database_display_filters': (`'<filter>'`, ...) |\n| comment | glue_catalog | True | True | 'comment': `'<comment>'` |\n| aws_role | kinesis | True | True | 'aws_role': `'<aws_role>'` |\n| external_id | kinesis | True | True | 'external_id': `'<external_id>'` |\n| aws_access_key_id | kinesis | True | True | 'aws_access_key_id': `'<aws_access_key_id>'` |\n| aws_secret_access_key | kinesis | True | True | 'aws_secret_access_key': `'<aws_secret_access_key>'` |\n| region | kinesis | False | False | 'region': `'<region>'` |\n| read_only | kinesis | False | True | 'read_only': True/False |\n| max_writers | kinesis | True | True | 'max_writers': `<integer>` |\n| stream_display_filter | kinesis | True | True | 'stream_display_filter': `'<stream_display_filter>'` |\n| stream_display_filters | kinesis | True | True | 'stream_display_filters': (`'<filter>'`, ...) |\n| comment | kinesis | True | True | 'comment': `'<comment>'` |\n| connection_string | snowflake | True | False | 'connection_string': `'<connection_string>'` |\n| user_name | snowflake | True | False | 'user_name': `'<user_name>'` |\n| password | snowflake | True | False | 'password': `'<password>'` |\n| max_concurrent_connections | snowflake | True | True | 'max_concurrent_connections': `<integer>` |\n| comment | snowflake | True | True | 'comment': `'<comment>'` |\n| connection_string | redshift | True | False | 'connection_string': `'<connection_string>'` |\n| user_name | redshift | True | False | 'user_name': `'<user_name>'` |\n| password | redshift | True | False | 'password': `'<password>'` |\n| max_concurrent_connections | redshift | True | True | 'max_concurrent_connections': `<integer>` |\n| comment | redshift | True | True | 'comment': `'<comment>'` |\n| connection_string | mysql | True | False | 'connection_string': `'<connection_string>'` |\n| user_name | mysql | True | False | 'user_name': `'<user_name>'` |\n| password | mysql | True | False | 'password': `'<password>'` |\n| comment | mysql | True | True | 'comment': `'<comment>'` |\n| connection_string | postgres | True | False | 'connection_string': `'<connection_string>'` |\n| user_name | postgres | True | False | 'user_name': `'<user_name>'` |\n| password | postgres | True | False | 'password': `'<password>'` |\n| comment | postgres | True | True | 'comment': `'<comment>'` |\n| connection_string | elasticsearch | True | False | 'connection_string': `'<connection_string>'` |\n| user_name | elasticsearch | True | False | 'user_name': `'<user_name>'` |\n| password | elasticsearch | True | False | 'password': `'<password>'` |\n| comment | elasticsearch | True | True | 'comment': `'<comment>'` |\n| connection_string | mongodb | True | False | 'connection_string': `'<connection_string>'` |\n| user_name | mongodb | True | False | 'user_name': `'<user_name>'` |\n| password | mongodb | True | False | 'password': `'<password>'` |\n| timeout | mongodb | True | True | 'timeout': \"INTERVAL 'N' SECONDS\" |\n| comment | mongodb | True | True | 'comment': `'<comment>'` |\n| connection_string | mssql | True | False | 'connection_string': `'<connection_string>'` |\n| user_name | mssql | True | False | 'user_name': `'<user_name>'` |\n| password | mssql | True | False | 'password': `'<password>'` |\n| comment | mssql | True | True | 'comment': `'<comment>'` |\n```\n\n----------------------------------------\n\nTITLE: Installing Jafgen Python Package for Data Generation\nDESCRIPTION: Command to install the jafgen Python package, which is used to generate synthetic Jaffle Shop data for testing.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/duckdb-qs.md#2025-04-09_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\npython -m pip install jafgen\n```\n\n----------------------------------------\n\nTITLE: Creating Redshift Tables\nDESCRIPTION: SQL commands to create customer, order, and payment tables with their respective columns and data types.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/redshift-qs.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\ncreate table jaffle_shop.customers(\n    id integer,\n    first_name varchar(50),\n    last_name varchar(50)\n);\n\ncreate table jaffle_shop.orders(\n    id integer,\n    user_id integer,\n    order_date date,\n    status varchar(50)\n);\n\ncreate table stripe.payment(\n    id integer,\n    orderid integer,\n    paymentmethod varchar(50),\n    status varchar(50),\n    amount integer,\n    created date\n);\n```\n\n----------------------------------------\n\nTITLE: Configuring Where Clause in Saved Queries (dbt 1.8+)\nDESCRIPTION: Examples of syntax for referencing entities, dimensions, time dimensions, or metrics in filters for saved queries in dbt version 1.8 and higher.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/saved-queries.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nfilter: | \n  {{ Entity('entity_name') }}\n\nfilter: |  \n  {{ Dimension('primary_entity__dimension_name') }}\n\nfilter: |  \n  {{ TimeDimension('time_dimension', 'granularity') }}\n\nfilter: |  \n  {{ Metric('metric_name', group_by=['entity_name']) }}\n```\n\n----------------------------------------\n\nTITLE: Example of dbt Cloud Job URL Structure\nDESCRIPTION: Example URL format for a dbt Cloud job, showing the structure with account_id, project_id, and job_id parameters that will be needed for configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/airflow-and-dbt-cloud.md#2025-04-09_snippet_3\n\nLANGUAGE: html\nCODE:\n```\nhttps://YOUR_ACCESS_URL/#/accounts/{account_id}/projects/{project_id}/jobs/{job_id}/\n```\n\n----------------------------------------\n\nTITLE: Schema Grant Macro Usage\nDESCRIPTION: Example of how to use the schema grant macro in the dbt_project.yml configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/on-run-end-context.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\non-run-end:\n - \"{{ grant_usage_to_schemas(schemas, 'user') }}\"\n```\n\n----------------------------------------\n\nTITLE: Listing Metrics Command in dbt Cloud and Core\nDESCRIPTION: Command syntax for listing metrics with their available dimensions in both dbt Cloud and dbt Core environments. Includes available command options.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metricflow-commands.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndbt sl list metrics <metric_name> # In dbt Cloud\n\nmf list metrics <metric_name> # In dbt Core\n\nOptions:\n  --search TEXT          Filter available metrics by this search term\n  --show-all-dimensions  Show all dimensions associated with a metric.\n  --help                 Show this message and exit.\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Model with Advanced Settings in dbt\nDESCRIPTION: This example shows how to configure a Spark model with advanced settings in dbt. It sets various engine configurations, enables Spark encryption and cross-account catalog access, configures requester pays, and sets polling interval and timeout. The model creates a simple DataFrame with one column.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/athena-configs.md#2025-04-09_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef model(dbt, spark_session):\n    dbt.config(\n        materialized=\"table\",\n        engine_config={\n            \"CoordinatorDpuSize\": 1,\n            \"MaxConcurrentDpus\": 3,\n            \"DefaultExecutorDpuSize\": 1\n        },\n        spark_encryption=True,\n        spark_cross_account_catalog=True,\n        spark_requester_pays=True\n        polling_interval=15,\n        timeout=120,\n    )\n\n    data = [(1,), (2,), (3,), (4,)]\n\n    df = spark_session.createDataFrame(data, [\"A\"])\n\n    return df\n```\n\n----------------------------------------\n\nTITLE: Configuring JDBC Connection String for dbt Semantic Layer\nDESCRIPTION: Example of a JDBC connection string for the dbt Semantic Layer. This includes the protocol, host, port, environment ID, and service token parameters needed to establish a connection.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-jdbc.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\njdbc:arrow-flight-sql://semantic-layer.cloud.getdbt.com:443?&environmentId=202339&token=SERVICE_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Recommended snapshot-paths configuration using relative path\nDESCRIPTION: Demonstrates the recommended way to configure snapshot-paths using a relative path. This approach is more portable and easier to manage across different environments.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/snapshot-paths.md#2025-04-09_snippet_1\n\nLANGUAGE: yml\nCODE:\n```\nsnapshot-paths: [\"snapshots\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Enabled Snapshots in dbt SQL (Legacy)\nDESCRIPTION: Example of enabling or disabling a specific snapshot in its SQL file. This is a legacy method and not recommended. Use the YAML file instead.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/enabled.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot [snapshot_name] %}\n\n{{ config(\n  enabled=true | false\n) }}\n\nselect ...\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Defining Group Access in YAML Schema\nDESCRIPTION: Example of configuring private access for grouped models in schema.yml and demonstrating group assignments.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/groups.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: finance_private_model\n    access: private\n    config:\n      group: finance\n\n  # in a different group!\n  - name: marketing_model\n    config:\n      group: marketing\n```\n\n----------------------------------------\n\nTITLE: Directory Structure for Intermediate dbt Models\nDESCRIPTION: Shows the recommended folder structure for intermediate dbt models, demonstrating how to organize files under business groupings like finance, with appropriate naming conventions for model files and YAML configurations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-structure/3-intermediate.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nmodels/intermediate\n finance\n     _int_finance__models.yml\n     int_payments_pivoted_to_orders.sql\n```\n\n----------------------------------------\n\nTITLE: Configuring unique_key Parameter in dbt Snapshots\nDESCRIPTION: Basic example of setting the unique_key configuration for a snapshot. This parameter defines a column or expression that uniquely identifies records for change tracking.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snapshots-jinja-legacy.md#2025-04-09_snippet_17\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n  unique_key=\"column_name\"\n) }}\n```\n\n----------------------------------------\n\nTITLE: Importing Google Source Repository SSH URL in dbt Cloud\nDESCRIPTION: Example of the SSH URL format provided by Google Cloud Platform for importing a repository into dbt Cloud. This URL includes the user email, project ID, and repository name.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Git/google-cloud-repo.md#2025-04-09_snippet_0\n\nLANGUAGE: plain\nCODE:\n```\nssh://drew@fishtownanalytics.com@source.developers.google.com:2022/p/dbt-integration-tests/r/drew-debug\n```\n\n----------------------------------------\n\nTITLE: Configuring Time Dimensions with time_granularity in dbt Semantic Layer (v1.9+)\nDESCRIPTION: YAML configuration demonstrating time_granularity parameter with hourly and daily granularity levels in dbt Semantic Layer v1.9+.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/dimensions.md#2025-04-09_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\ndimensions: \n  - name: created_at\n    type: time\n    label: \"Date of creation\"\n    expr: ts_created # ts_created is the underlying column name from the table \n    is_partition: True \n    type_params:\n      time_granularity: hour \n  - name: deleted_at\n    type: time\n    label: \"Date of deletion\"\n    expr: ts_deleted # ts_deleted is the underlying column name from the table \n    is_partition: True \n    type_params:\n      time_granularity: day \n\nmeasures:\n  - name: users_deleted\n    expr: 1\n    agg: sum \n    agg_time_dimension: deleted_at\n  - name: users_created\n    expr: 1\n    agg: sum\n```\n\n----------------------------------------\n\nTITLE: Overriding Parameters in dbtRunner\nDESCRIPTION: Demonstrates how to pass parameters as keyword arguments instead of CLI-style strings, showing equivalent invocation methods for the same dbt command.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/programmatic-invocations.md#2025-04-09_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dbt.cli.main import dbtRunner\ndbt = dbtRunner()\n\n# these are equivalent\ndbt.invoke([\"--fail-fast\", \"run\", \"--select\", \"tag:my_tag\"])\ndbt.invoke([\"run\"], select=[\"tag:my_tag\"], fail_fast=True)\n```\n\n----------------------------------------\n\nTITLE: Configuring No Fallback Protection in SQL Model for Teradata\nDESCRIPTION: Setting the table_option parameter to disable fallback protection for a table in Teradata, which affects data redundancy.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/teradata-configs.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n{{\n  config(\n      materialized=\"table\",\n      table_option=\"NO FALLBACK\"\n  )\n}}\n```\n\n----------------------------------------\n\nTITLE: Initializing a New dbt Package Project\nDESCRIPTION: Use the dbt init command to create a new dbt project that will serve as your package. This command sets up the initial project structure.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/building-packages.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ dbt init [package_name]\n```\n\n----------------------------------------\n\nTITLE: Basic Incremental Model Configuration\nDESCRIPTION: Example of an incremental model configuration with conditional logic for selecting only new data. This demonstrates how to implement incremental processing that only processes new records since the last run.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/trino-configs.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n      materialized = 'incremental', \n      unique_key='<optional>',\n      incremental_strategy='<optional>',)\n}}\nselect * from {{ ref('events') }}\n{% if is_incremental() %}\n  where event_ts > (select max(event_ts) from {{ this }})\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Running dbt Pipeline\nDESCRIPTION: Shell command and expected output for executing the dbt transformation pipeline.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dremio-lakehouse.md#2025-04-09_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n$ dbt run -t cloud_dev\n```\n\n----------------------------------------\n\nTITLE: Implementing Cost Snapshot Model\nDESCRIPTION: Snapshot configuration and implementation for tracking changes in source cost data\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-07-12-change-data-capture.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot costs_snapshot %}\n\n{{\n    config(\n      target_database='analytics',\n      target_schema='snapshots',\n      unique_key='cost_id',\n      strategy='timestamp',\n      updated_at='updated_at'\n    )\n}}\n\nselect * from {{ source('source', 'costs') }}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Schema Change Behavior in dbt Project\nDESCRIPTION: YAML configuration in dbt_project.yml to set the default schema change behavior for all models using the on_schema_change parameter.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/incremental-models.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  +on_schema_change: \"sync_all_columns\"\n```\n\n----------------------------------------\n\nTITLE: Reference to Target Variable in dbt\nDESCRIPTION: Shows the Jinja syntax for accessing the target variable in dbt configurations, which is used for conditional configs and determining the SQL flavor to use.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Warehouse/db-connection-dbt-compile.md#2025-04-09_snippet_0\n\nLANGUAGE: jinja\nCODE:\n```\n{{target}}\n```\n\n----------------------------------------\n\nTITLE: Selecting Common Descendants Using Intersection\nDESCRIPTION: Demonstrates how to select common descendants of two staging models using comma-separated selectors. This example finds common descendants between stg_invoices and stg_accounts.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/set-operators.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --select \"stg_invoices+,stg_accounts+\"\n```\n\n----------------------------------------\n\nTITLE: Configuring watsonx.data Spark Connection Profile\nDESCRIPTION: YAML configuration example for setting up connection profiles with watsonx.data Spark, including authentication and catalog settings.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/watsonx-spark-config.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nproject_name:\n  target: \"dev\"\n  outputs:\n    dev:\n      type: watsonx_spark\n      method: http\n      schema: [schema name]\n      host: [hostname]\n      uri: [uri]\n      catalog: [catalog name]\n      use_ssl: false\n      auth:\n        instance: [Watsonx.data Instance ID]\n        user: [username]\n        apikey: [apikey]\n```\n\n----------------------------------------\n\nTITLE: Configuring watsonx.data Spark Connection Profile\nDESCRIPTION: YAML configuration example for setting up connection profiles with watsonx.data Spark, including authentication and catalog settings.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/watsonx-spark-config.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nproject_name:\n  target: \"dev\"\n  outputs:\n    dev:\n      type: watsonx_spark\n      method: http\n      schema: [schema name]\n      host: [hostname]\n      uri: [uri]\n      catalog: [catalog name]\n      use_ssl: false\n      auth:\n        instance: [Watsonx.data Instance ID]\n        user: [username]\n        apikey: [apikey]\n```\n\n----------------------------------------\n\nTITLE: Running dbt Docker Container with Alternate Profiles Mount\nDESCRIPTION: This command runs a dbt Docker container, mounting the local project directory and profiles.yml file in an alternate location. It uses host networking and executes the 'ls' command in the container.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/docker-install.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run \\\n--network=host \\\n--mount type=bind,source=path/to/project,target=/usr/app \\\n--mount type=bind,source=path/to/profiles.yml.dbt,target=/root/.dbt/ \\\n<dbt_image_name> \\\nls\n```\n\n----------------------------------------\n\nTITLE: Installing dbt Adapter in Python\nDESCRIPTION: Command for installing a dbt adapter from PyPI using pip. The example shows installation for the Snowflake adapter, which will also include dbt-core and other required dependencies.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/connect-adapters.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install dbt-snowflake\n```\n\n----------------------------------------\n\nTITLE: Example: Hiding Subfolders and Packages in dbt_project.yml\nDESCRIPTION: Shows how to hide all models within a specific subfolder or an entire dbt package from documentation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/docs.md#2025-04-09_snippet_9\n\nLANGUAGE: yml\nCODE:\n```\nmodels:\n  # hiding models within the staging subfolder\n  tpch:\n    staging:\n      +materialized: view\n      +docs:\n        show: false\n  \n  # hiding a dbt package\n  dbt_artifacts:\n    +docs:\n      show: false\n```\n\n----------------------------------------\n\nTITLE: Querying Metrics with Time Grouping in dbt Semantic Layer\nDESCRIPTION: Command for querying user metrics grouped and ordered by year using the mf CLI tool.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/dimensions.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmf query --metrics users_created,users_deleted --group-by metric_time__year --order-by metric_time__year\n```\n\n----------------------------------------\n\nTITLE: Importing Cycle Detection Snippet in Markdown\nDESCRIPTION: This code snippet imports a markdown file containing information about cycle detection in dbt Mesh projects. It uses a custom import syntax specific to the documentation platform.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-mesh/mesh-3-structures.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nimport CycleDetection from '/snippets/_mesh-cycle-detection.md';\n\n<CycleDetection />\n```\n\n----------------------------------------\n\nTITLE: Configuring No Fallback Protection in Seed Configuration for Teradata\nDESCRIPTION: Setting the table_option parameter in a seed configuration to disable fallback protection for a table in Teradata.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/teradata-configs.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  <project-name>:\n    table_option:\"NO FALLBACK\"\n```\n\n----------------------------------------\n\nTITLE: Using a Generic Test with Default and Override Configuration in YAML\nDESCRIPTION: Shows how to use a test with default configuration and how to override those defaults in specific instances. One column uses the default 'warn' severity while another overrides it to 'error'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/custom-generic-tests.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: users\n    columns:\n      - name: favorite_number\n        description: \"Test favorite_number\"\n        tests:\n      \t  - warn_if_odd         # default 'warn'\n      - name: other_number\n        description: \"Test other_number\"\n        tests:\n          - warn_if_odd:\n              severity: error   # overrides\n```\n\n----------------------------------------\n\nTITLE: Basic SQL LOWER Function Syntax\nDESCRIPTION: The basic syntax for using the LOWER function to convert a string column to lowercase. This syntax is consistent across major database platforms including Snowflake, Databricks, BigQuery, Redshift, and Postgres.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/string-functions/sql-lower.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nlower(<string_column>)\n```\n\n----------------------------------------\n\nTITLE: Extended Dimensions Definition\nDESCRIPTION: Shows how to define multiple dimensions including both time and categorical types with custom expressions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-build-our-metrics/semantic-layer-3-build-semantic-models.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ndimensions:\n  - name: ordered_at\n    expr: date_trunc('day', ordered_at)\n    type: time\n    type_params:\n      time_granularity: day\n  - name: is_large_order\n    type: categorical\n    expr: case when order_total > 50 then true else false end\n```\n\n----------------------------------------\n\nTITLE: BigQuery DDL for Table Creation with Constraints\nDESCRIPTION: This SQL snippet shows the DDL that BigQuery generates to create a table with constraints. It creates a table with three columns: id (with a not null constraint), customer_name, and first_transaction_date.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/constraints.md#2025-04-09_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\ncreate or replace table `<project>`.`<dataset>`.`constraints_model`        \n(\n    id integer not null,\n    customer_name string,\n    first_transaction_date date  \n)\nas\n(\nselect \n  1 as id, \n  'My Favorite Customer' as customer_name, \n  cast('2019-01-01' as date) as first_transaction_date\n);\n```\n\n----------------------------------------\n\nTITLE: Setting Session Properties in dbt Model\nDESCRIPTION: Example of using dbt hooks to set session properties for a specific model, demonstrating how to set query maximum run time.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/watsonx-presto-config.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(\n    pre_hook=\"set session query_max_run_time='10m'\"\n  )\n}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Package Installation in YAML\nDESCRIPTION: This YAML snippet demonstrates how to install a local package in the integration_tests subdirectory using the 'local' syntax in the packages.yml file. It specifies the package location as one directory above the current directory.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/building-packages.md#2025-04-09_snippet_2\n\nLANGUAGE: yml\nCODE:\n```\npackages:\n    - local: ../ # this means \"one directory above the current directory\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Enabled Exposures in dbt YAML\nDESCRIPTION: Example of enabling or disabling exposures in the dbt_project.yml and exposures.yml files. This configuration can be applied to all exposures or specific exposure definitions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/enabled.md#2025-04-09_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nexposures:\n  [<resource-path>]:\n    +enabled: true | false\n```\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nexposures:\n  - name: [<exposure-name>]\n    config:\n      enabled: true | false\n```\n\n----------------------------------------\n\nTITLE: Setting Up Formula 1 Data in Snowflake from S3\nDESCRIPTION: Complete setup script that creates a Formula 1 database, defines a CSV file format, creates a stage pointing to an S3 bucket, and loads eight Formula 1 tables from CSV files.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n-- create and define our formula1 database\ncreate or replace database formula1;\nuse database formula1; \ncreate or replace schema raw; \nuse schema raw; \n\n-- define our file format for reading in the csvs \ncreate or replace file format csvformat\ntype = csv\nfield_delimiter =','\nfield_optionally_enclosed_by = '\"', \nskip_header=1; \n\n--\ncreate or replace stage formula1_stage\nfile_format = csvformat \nurl = 's3://formula1-dbt-cloud-python-demo/formula1-kaggle-data/';\n\n-- load in the 8 tables we need for our demo \n-- we are first creating the table then copying our data in from s3\n-- think of this as an empty container or shell that we are then filling\ncreate or replace table formula1.raw.circuits (\n    CIRCUITID NUMBER(38,0),\n    CIRCUITREF VARCHAR(16777216),\n    NAME VARCHAR(16777216),\n    LOCATION VARCHAR(16777216),\n    COUNTRY VARCHAR(16777216),\n    LAT FLOAT,\n    LNG FLOAT,\n    ALT NUMBER(38,0),\n    URL VARCHAR(16777216)\n);\n-- copy our data from public s3 bucket into our tables \ncopy into circuits \nfrom @formula1_stage/circuits.csv\non_error='continue';\n\ncreate or replace table formula1.raw.constructors (\n    CONSTRUCTORID NUMBER(38,0),\n    CONSTRUCTORREF VARCHAR(16777216),\n    NAME VARCHAR(16777216),\n    NATIONALITY VARCHAR(16777216),\n    URL VARCHAR(16777216)\n);\ncopy into constructors \nfrom @formula1_stage/constructors.csv\non_error='continue';\n\ncreate or replace table formula1.raw.drivers (\n    DRIVERID NUMBER(38,0),\n    DRIVERREF VARCHAR(16777216),\n    NUMBER VARCHAR(16777216),\n    CODE VARCHAR(16777216),\n    FORENAME VARCHAR(16777216),\n    SURNAME VARCHAR(16777216),\n    DOB DATE,\n    NATIONALITY VARCHAR(16777216),\n    URL VARCHAR(16777216)\n);\ncopy into drivers \nfrom @formula1_stage/drivers.csv\non_error='continue';\n\ncreate or replace table formula1.raw.lap_times (\n    RACEID NUMBER(38,0),\n    DRIVERID NUMBER(38,0),\n    LAP NUMBER(38,0),\n    POSITION FLOAT,\n    TIME VARCHAR(16777216),\n    MILLISECONDS NUMBER(38,0)\n);\ncopy into lap_times \nfrom @formula1_stage/lap_times.csv\non_error='continue';\n\ncreate or replace table formula1.raw.pit_stops (\n    RACEID NUMBER(38,0),\n    DRIVERID NUMBER(38,0),\n    STOP NUMBER(38,0),\n    LAP NUMBER(38,0),\n    TIME VARCHAR(16777216),\n    DURATION VARCHAR(16777216),\n    MILLISECONDS NUMBER(38,0)\n);\ncopy into pit_stops \nfrom @formula1_stage/pit_stops.csv\non_error='continue';\n\ncreate or replace table formula1.raw.races (\n    RACEID NUMBER(38,0),\n    YEAR NUMBER(38,0),\n    ROUND NUMBER(38,0),\n    CIRCUITID NUMBER(38,0),\n    NAME VARCHAR(16777216),\n    DATE DATE,\n    TIME VARCHAR(16777216),\n    URL VARCHAR(16777216),\n    FP1_DATE VARCHAR(16777216),\n    FP1_TIME VARCHAR(16777216),\n    FP2_DATE VARCHAR(16777216),\n    FP2_TIME VARCHAR(16777216),\n    FP3_DATE VARCHAR(16777216),\n    FP3_TIME VARCHAR(16777216),\n    QUALI_DATE VARCHAR(16777216),\n    QUALI_TIME VARCHAR(16777216),\n    SPRINT_DATE VARCHAR(16777216),\n    SPRINT_TIME VARCHAR(16777216)\n);\ncopy into races \nfrom @formula1_stage/races.csv\non_error='continue';\n\ncreate or replace table formula1.raw.results (\n    RESULTID NUMBER(38,0),\n    RACEID NUMBER(38,0),\n    DRIVERID NUMBER(38,0),\n    CONSTRUCTORID NUMBER(38,0),\n    NUMBER NUMBER(38,0),\n    GRID NUMBER(38,0),\n    POSITION FLOAT,\n    POSITIONTEXT VARCHAR(16777216),\n    POSITIONORDER NUMBER(38,0),\n    POINTS NUMBER(38,0),\n    LAPS NUMBER(38,0),\n    TIME VARCHAR(16777216),\n    MILLISECONDS NUMBER(38,0),\n    FASTESTLAP NUMBER(38,0),\n    RANK NUMBER(38,0),\n    FASTESTLAPTIME VARCHAR(16777216),\n    FASTESTLAPSPEED FLOAT,\n    STATUSID NUMBER(38,0)\n);\ncopy into results \nfrom @formula1_stage/results.csv\non_error='continue';\n\ncreate or replace table formula1.raw.status (\n    STATUSID NUMBER(38,0),\n    STATUS VARCHAR(16777216)\n);\ncopy into status \nfrom @formula1_stage/status.csv\non_error='continue';\n```\n\n----------------------------------------\n\nTITLE: Testing an Expression for Uniqueness in dbt YAML\nDESCRIPTION: This approach tests the uniqueness of a combination of columns using an expression directly in the test definition without creating a separate column in the model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Tests/uniqueness-two-columns.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: orders\n    tests:\n      - unique:\n          column_name: \"(country_code || '-' || order_id)\"\n```\n\n----------------------------------------\n\nTITLE: Testing an Expression for Uniqueness in dbt YAML\nDESCRIPTION: This approach tests the uniqueness of a combination of columns using an expression directly in the test definition without creating a separate column in the model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Tests/uniqueness-two-columns.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: orders\n    tests:\n      - unique:\n          column_name: \"(country_code || '-' || order_id)\"\n```\n\n----------------------------------------\n\nTITLE: Test Selection Syntax Examples in dbt\nDESCRIPTION: These examples show various ways to select and run tests on specific models, directories, and resource types in dbt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/test-selection-examples.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Run tests on a model (indirect selection)\ndbt test --select \"customers\"\n\n# Run tests on two or more specific models (indirect selection)\ndbt test --select \"customers orders\"\n\n# Run tests on all models in the models/staging/jaffle_shop directory (indirect selection)\ndbt test --select \"staging.jaffle_shop\"\n\n# Run tests downstream of a model (note this will select those tests directly!)\ndbt test --select \"stg_customers+\"\n\n# Run tests upstream of a model (indirect selection)\ndbt test --select \"+stg_customers\"\n\n# Run tests on all models with a particular tag (direct + indirect)\ndbt test --select \"tag:my_model_tag\"\n\n# Run tests on all models with a particular materialization (indirect selection)\ndbt test --select \"config.materialized:table\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Macro-based Test\nDESCRIPTION: SQL implementation of the gte_zero macro test with configuration options for severity and tags.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_39\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        enabled=true,\n        severity='warn',\n        tags = ['bi']\n    )\n}}\n\n{{ test_all_values_gte_zero('fastest_pit_stops_by_constructor', 'mean') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Materialized Views in dbt for Upsolver\nDESCRIPTION: This snippet demonstrates how to configure SQL materialized views in dbt for Upsolver. It uses the 'materializedview' materialization and allows specifying options like sync.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/upsolver-configs.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(  materialized='materializedview',\n            sync=True|False,\n            options={'option_name': 'option_value'}\n        )\n}}\nSELECT ...\nFROM {{ ref(<model>) }}\nWHERE ...\nGROUP BY ...\n```\n\n----------------------------------------\n\nTITLE: Running dbt and Displaying Macro Count\nDESCRIPTION: This shell command runs dbt and displays the project statistics, including the number of macros. It shows that the project contains 138 macros, which includes both user-defined and dbt-provided macros.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Project/why-so-many-macros.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ dbt run\nRunning with dbt=1.7.0\nFound 1 model, 0 tests, 0 snapshots, 0 analyses, 138 macros, 0 operations, 0 seed files, 0 sources\n```\n\n----------------------------------------\n\nTITLE: Example Seed File with Semicolon Delimiter\nDESCRIPTION: This text snippet shows the format of a seed file using a semicolon (;) as the delimiter.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/delimiter.md#2025-04-09_snippet_6\n\nLANGUAGE: text\nCODE:\n```\ncountry_code;country_name\nUS;United States\nCA;Canada\nGB;United Kingdom\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring target_database in dbt_project.yml\nDESCRIPTION: Demonstrates how to set the target_database configuration for snapshots in the dbt_project.yml file. This configuration applies to all snapshots unless overridden.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/target_database.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  [<resource-path>]:\n    +target_database: string\n```\n\n----------------------------------------\n\nTITLE: Defining a Basic Measure for Order Total in YAML\nDESCRIPTION: This snippet shows how to define a basic measure 'order_total' that sums the order amounts.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/cumulative-metrics.md#2025-04-09_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\n    measures:\n      - name: order_total\n        agg: sum\n```\n\n----------------------------------------\n\nTITLE: Applying Whitespace Control to dbt Macro Definition in Jinja\nDESCRIPTION: Demonstrates how to use Jinja's whitespace control syntax to remove extra whitespace in dbt macro definitions. This helps clean up the compiled output in the target/compiled folder.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_whitespace-control.md#2025-04-09_snippet_0\n\nLANGUAGE: jinja\nCODE:\n```\n{%- macro generate_schema_name(...) -%} ... {%- endmacro -%}\n```\n\n----------------------------------------\n\nTITLE: Incorrect Full Refresh Command for Microbatch\nDESCRIPTION: Shows the incorrect way to perform a full refresh on a microbatch model without specifying time range parameters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/incremental-microbatch.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --full-refresh\n```\n\n----------------------------------------\n\nTITLE: SQL RIGHT JOIN Example with dbt References\nDESCRIPTION: A complete example of a RIGHT JOIN query using dbt's ref() function to reference tables. It demonstrates joining car_type and car_color tables on user_id.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/joins/sql-right-join.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect\n   car_type.user_id as user_id,\n   car_type.car_type as type,\n   car_color.car_color as color\nfrom {{ ref('car_type') }} as car_type\nright join {{ ref('car_color') }} as car_color\non car_type.user_id = car_color.user_id\n```\n\n----------------------------------------\n\nTITLE: Adding Dimensions to Semantic Model\nDESCRIPTION: Configuration adding time-based dimension for order dates with daily granularity to the semantic model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/sl-snowflake-qs.md#2025-04-09_snippet_20\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_models:\n  - name: orders\n    defaults:\n      agg_time_dimension: order_date\n    description: |\n      Order fact table. This table's grain is one row per order.\n    model: ref('fct_orders')\n    entities:\n      - name: order_id\n        type: primary\n      - name: customer\n        expr: customer_id\n        type: foreign\n    dimensions:   \n      - name: order_date\n        type: time\n        type_params:\n          time_granularity: day\n```\n\n----------------------------------------\n\nTITLE: Configuring Sources in dbt_project.yml (v1.8 and earlier)\nDESCRIPTION: YAML configuration for sources in dbt_project.yml for dbt version 1.8 and earlier. This snippet shows how to configure enabled status and metadata for sources using the resource path syntax.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/source-configs.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  [<resource-path>]:\n    [+][enabled]: true | false\n    [+][meta]:\n      key: value\n```\n\n----------------------------------------\n\nTITLE: Indirect Promotion Strategy Setup Steps\nDESCRIPTION: Checklist for transitioning from direct to indirect promotion strategy, including git platform configuration and dbt Cloud environment setup.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2025-01-28-git-branching-strategies-and-dbt.md#2025-04-09_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n- git Platform\n    - Create a new branch derived from `main` for your middle branch.\n    - Protect the branch with branch protection rules\n- dbt Cloud\n    - Development: Switch your environment to use the **custom branch** option and specify your new middle branch's name. This will base developers off of the middle branch.\n    - Continous Integration: If you have an existing environment for this, ensure the **custom branch** is also changed to the middle branch's name. This will change the CI job's trigger to occur on pull requests to your middle branch.\n```\n\n----------------------------------------\n\nTITLE: Running Hooks in dbt Materializations\nDESCRIPTION: Shows how to execute pre-hooks and post-hooks in a dbt materialization using the run_hooks function.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/create-new-materializations.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n...\n{{ run_hooks(pre_hooks) }}\n....\n```\n\n----------------------------------------\n\nTITLE: Configuring GitHub Actions Workflow for SQLFluff Linting\nDESCRIPTION: YAML configuration for GitHub Actions that runs SQLFluff linting whenever code is pushed to any branch except 'main'. It installs Python, SQLFluff and runs linting with Snowflake dialect.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/set-up-ci.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nname: lint dbt project on push\n\non:\n  push:\n    branches-ignore:\n      - 'main'\n\njobs:\n  # this job runs SQLFluff with a specific set of rules\n  # note the dialect is set to Snowflake, so make that specific to your setup\n  # details on linter rules: https://docs.sqlfluff.com/en/stable/rules.html\n  lint_project:\n    name: Run SQLFluff linter\n    runs-on: ubuntu-latest\n  \n    steps:\n      - uses: \"actions/checkout@v3\"\n      - uses: \"actions/setup-python@v4\"\n        with:\n          python-version: \"3.9\"\n      - name: Install SQLFluff\n        run: \"python -m pip install sqlfluff\"\n      - name: Lint project\n        run: \"sqlfluff lint models --dialect snowflake\"\n```\n\n----------------------------------------\n\nTITLE: Debugging Compilation Error: Invalid ref Function\nDESCRIPTION: Error message shown when a model references another model that doesn't exist. In this example, the 'customers' model references 'stg_customer' which cannot be found.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/debug-errors.md#2025-04-09_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n$ dbt run -s customers\nRunning with dbt=1.1.0\n\nEncountered an error:\nCompilation Error in model customers (models/customers.sql)\n  Model 'model.jaffle_shop.customers' (models/customers.sql) depends on a node named 'stg_customer' which was not found\n```\n\n----------------------------------------\n\nTITLE: Dictionary Format Unit Test Configuration in YAML\nDESCRIPTION: Example of using the default dictionary format for specifying mock data in dbt unit tests. Shows how to define inline dictionary values for test rows.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/data-formats.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nunit_tests:\n  - name: test_my_model\n    model: my_model\n    given:\n      - input: ref('my_model_a')\n        format: dict\n        rows:\n          - {id: 1, name: gerda}\n          - {id: 2, b: michelle}\n```\n\n----------------------------------------\n\nTITLE: Executing dbt Source Freshness Commands for Specific Sources\nDESCRIPTION: This snippet demonstrates how to use the --select flag with the dbt source freshness command to check freshness for specific sources and tables. It includes examples for checking all tables in a source, a specific table, or multiple specific tables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Snapshots/snapshotting-freshness-for-one-source.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# Snapshot freshness for all Jaffle Shop tables:\n$ dbt source freshness --select source:jaffle_shop\n\n# Snapshot freshness for a particular source table:\n$ dbt source freshness --select source:jaffle_shop.orders\n\n# Snapshot freshness for multiple particular source tables:\n$ dbt source freshness --select source:jaffle_shop.orders source:jaffle_shop.customers\n```\n\n----------------------------------------\n\nTITLE: Including Snapshots in DBT Build\nDESCRIPTION: Command to include all snapshots in the DBT build process using the --resource-type flag.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/resource-type.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ndbt build --resource-type snapshot\n```\n\n----------------------------------------\n\nTITLE: Setting target_database for all snapshots\nDESCRIPTION: Example of setting the target_database configuration in dbt_project.yml to build all snapshots in a specific database named 'snapshots'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/target_database.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  +target_database: snapshots\n```\n\n----------------------------------------\n\nTITLE: Building Final Customer Model with Dependencies\nDESCRIPTION: SQL model that combines staged customer and order data to create a final customer view with order metrics. Uses dbt ref function to manage dependencies between models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/manual-install-qs.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nwith customers as (\n\n    select * from {{ ref('stg_customers') }}\n\n),\n\norders as (\n\n    select * from {{ ref('stg_orders') }}\n\n),\n\ncustomer_orders as (\n\n    select\n        customer_id,\n\n        min(order_date) as first_order_date,\n        max(order_date) as most_recent_order_date,\n        count(order_id) as number_of_orders\n\n    from orders\n\n    group by 1\n\n),\n\nfinal as (\n\n    select\n        customers.customer_id,\n        customers.first_name,\n        customers.last_name,\n        customer_orders.first_order_date,\n        customer_orders.most_recent_order_date,\n        coalesce(customer_orders.number_of_orders, 0) as number_of_orders\n\n    from customers\n\n    left join customer_orders using (customer_id)\n\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Oracle Driver in Thick Mode (Bash)\nDESCRIPTION: Sets the ORA_PYTHON_DRIVER_TYPE environment variable to 'thick' for the Python Oracle driver. This mode requires Oracle Client libraries and enables advanced functionalities.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/oracle-setup.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport ORA_PYTHON_DRIVER_TYPE=thick\n```\n\n----------------------------------------\n\nTITLE: Specifying config-version in dbt_project.yml\nDESCRIPTION: This snippet shows how to set the config-version to 2 in the dbt_project.yml file. It's used to explicitly specify that the project is using the version 2 structure of dbt_project.yml.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/config-version.md#2025-04-09_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nconfig-version: 2\n```\n\n----------------------------------------\n\nTITLE: Creating a Customer Dimension Model with Email Validation in SQL\nDESCRIPTION: SQL model that creates a customer dimension with email validation logic. The model joins customer data with accepted email domains and uses regex pattern matching to validate email addresses.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/unit-tests.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nwith customers as (\n\n    select * from {{ ref('stg_customers') }}\n\n),\n\naccepted_email_domains as (\n\n    select * from {{ ref('top_level_email_domains') }}\n\n),\n\t\ncheck_valid_emails as (\n\n    select\n        customers.customer_id,\n        customers.first_name,\n        customers.last_name,\n        customers.email,\n\t      coalesce (regexp_like(\n            customers.email, '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{2,}$'\n        )\n        = true\n        and accepted_email_domains.tld is not null,\n        false) as is_valid_email_address\n    from customers\n\t\tleft join accepted_email_domains\n        on customers.email_top_level_domain = lower(accepted_email_domains.tld)\n\n)\n\nselect * from check_valid_emails\n```\n\n----------------------------------------\n\nTITLE: Setting Query Row Limit in dbt Cloud CLI\nDESCRIPTION: Command to modify the default query row limit when querying the dbt Semantic Layer through the dbt Cloud CLI. The example shows how to increase the limit to 1000 rows.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_sl-faqs.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndbt sl list metrics --limit 1000\n```\n\n----------------------------------------\n\nTITLE: Creating Staging Model for Payments in SQL\nDESCRIPTION: SQL query to create a staging model for payments, selecting, renaming, and transforming relevant columns from the source table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/sl-snowflake-qs.md#2025-04-09_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nselect\n   id as payment_id,\n   orderid as order_id,\n   paymentmethod as payment_method,\n   status,\n   -- amount is stored in cents, convert it to dollars\n   amount / 100 as amount,\n   created as created_at\n\n\nfrom {{ source('stripe', 'payment') }}\n```\n\n----------------------------------------\n\nTITLE: Generating Training Datasets with Snowflake Feature Store in Python\nDESCRIPTION: This Python code generates a training dataset using the Snowflake Feature Store. It creates a spine dataframe with entity IDs and timestamps, then uses the feature store to fetch point-in-time correct feature values for these entities at the specified timestamps.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-10-05-snowflake-feature-store.md#2025-04-09_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nspine_df = session.create_dataframe(\n    [\n        ('1', '3937', \"2019-05-01 00:00\"), \n        ('2', '2', \"2019-05-01 00:00\"),\n        ('3', '927', \"2019-05-01 00:00\"),\n    ], \n    schema=[\"INSTANCE_ID\", \"CUSTOMER_ID\", \"EVENT_TIMESTAMP\"])\n\ntrain_dataset = fs.generate_dataset(\n    name= \"customers_fv\",\n    version= \"1_0\",\n    spine_df=spine_df,\n    features=[customer_transactions_fv],\n    spine_timestamp_col= \"EVENT_TIMESTAMP\",\n    spine_label_cols = []\n)\n\n```\n\n----------------------------------------\n\nTITLE: Filtering Non-Card Payments Using SQL NOT with LIKE\nDESCRIPTION: A query that demonstrates using the NOT operator with LIKE to filter out payment methods containing 'card' from a payments table. This example uses the Jaffle Shop sample dataset to show payments made through non-card methods.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/operators/sql-not.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n   payment_id,\n   order_id,\n   payment_method\nfrom {{ ref('payments') }}\nwhere payment_method not like '%card'\n```\n\n----------------------------------------\n\nTITLE: Querying with Descending Order Using Shorthand Notation\nDESCRIPTION: Example showing how to order query results in descending order using the shorthand notation (prefixing with a minus sign). This only works with simple references that don't use object notation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-jdbc.md#2025-04-09_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\nselect * from {{\nsemantic_layer.query(metrics=['food_order_amount', 'order_gross_profit'],\n  group_by=[Dimension('metric_time')],\n  limit=10,\n  order_by=['-order_gross_profit'])\n  }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Dimensions and Measures in a Semantic Model\nDESCRIPTION: Configuration for defining dimensions (categorical and time-based properties) and measures (aggregatable numerical values) in a semantic model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_sl-create-semanticmodel.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n    #Measures. These are the aggregations on the columns in the table.\n    measures: \n      - name: order_total\n        description: The total revenue for each order.\n        agg: sum\n      - name: order_count\n        expr: 1\n        agg: sum\n      - name: tax_paid\n        description: The total tax paid on each order. \n        agg: sum\n      - name: customers_with_orders\n        description: Distinct count of customers placing orders\n        agg: count_distinct\n        expr: customer_id\n      - name: locations_with_orders\n        description: Distinct count of locations with order\n        expr: location_id\n        agg: count_distinct\n      - name: order_cost\n        description: The cost for each order item. Cost is calculated as a sum of the supply cost for each order item. \n        agg: sum\n  #Dimensions. Either categorical or time. These add additional context to metrics. The typical querying pattern is Metric by Dimension.  \n    dimensions:\n      - name: ordered_at\n        type: time\n        type_params:\n          time_granularity: day \n      - name: order_total_dim\n        type: categorical\n        expr: order_total\n      - name: is_food_order\n        type: categorical\n      - name: is_drink_order\n        type: categorical\n```\n\n----------------------------------------\n\nTITLE: Building Modified Models and Dependencies in dbt CI\nDESCRIPTION: This command builds all modified models and their downstream dependencies. It's used as the second step in a CI job, after cloning incremental models, to test changes in a PR-specific schema.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/clone-incremental-models.md#2025-04-09_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndbt build --select state:modified+\n```\n\n----------------------------------------\n\nTITLE: Defining dbt Sources in YAML Configuration\nDESCRIPTION: A YAML configuration example that defines sources for a dbt project, allowing users to reference raw database tables using the src() function rather than direct database references.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/refactoring-legacy-sql.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  - name: jaffle_shop\n    tables:\n      - name: orders\n      - name: customers\n```\n\n----------------------------------------\n\nTITLE: Creating Payment Table in Snowflake SQL\nDESCRIPTION: This SQL command creates a payment table in the stripe schema of the raw database. It defines columns for payment details including id, orderid, paymentmethod, status, amount, created date, and a batched timestamp.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/sl-snowflake-qs.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\ncreate table raw.stripe.payment\n( id integer,\n  orderid integer,\n  paymentmethod varchar,\n  status varchar,\n  amount integer,\n  created date,\n  _batched_at timestamp default current_timestamp\n);\n```\n\n----------------------------------------\n\nTITLE: Limiting Data in Development Environment for dbt\nDESCRIPTION: SQL pattern that limits the amount of data processed in development environments by using time-based filtering. This improves iteration speed by reducing the dataset size when not in production.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/best-practice-workflows.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nselect\n*\nfrom event_tracking.events\n{% if target.name == 'dev' %}\nwhere created_at >= dateadd('day', -3, current_date)\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Apache License Boilerplate Notice Template\nDESCRIPTION: Standard template for applying Apache License 2.0 to a project, including copyright notice and license terms reference. The template uses placeholder values for year and copyright owner that should be replaced with actual values.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/License.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nCopyright {yyyy} {name of copyright owner}\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n\n----------------------------------------\n\nTITLE: Iceberg Snapshot Configuration for dbt Prior to v1.9\nDESCRIPTION: SQL snapshot configuration for dbt versions up to 1.8 using Iceberg format. This demonstrates the timestamp strategy with a target_schema configuration for older dbt versions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/glue-setup.md#2025-04-09_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot demosnapshot %}\n\n{{\n    config(\n        strategy='timestamp',\n        target_schema='jaffle_db',\n        updated_at='dt',\n        file_format='iceberg'\n) }}\n\nselect * from {{ ref('customers') }}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Group Attribute Statements in Okta for dbt Cloud SSO\nDESCRIPTION: This snippet demonstrates the expected Group Attribute Statement configuration in Okta for dbt Cloud integration. It shows how to map Okta groups to dbt Cloud, with a note about potential limitations for users belonging to many groups.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/manage-access/set-up-sso-okta.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Name     | Name format | Filter        | Value | Description                           |\n| -------- | ----------- | ------------- | ----- | ------------------------------------- |\n| `groups` | Unspecified | Matches regex | `.*`  | _The groups that the user belongs to_ |\n```\n\n----------------------------------------\n\nTITLE: SQL Operations Reference in dbt Models\nDESCRIPTION: Reference to SQL operations in dbt context, showing how SELECT statements are used instead of direct INSERT statements, and how they're wrapped in CREATE TABLE AS statements by dbt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Models/insert-records.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\ninsert\nupdate\ncreate table as\n```\n\n----------------------------------------\n\nTITLE: Querying Parsed Date Results in DuckDB (Shell)\nDESCRIPTION: Shell command to query the output of the date parsing transformation using duckcli to display the results in a table format.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-10-19-polyglot-dbt-python-dataframes-and-sql.md#2025-04-09_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nduckcli demo.duckdb --table --execute \"select id, transaction_time, parsed_transaction_time from parse_datetimes order by id\"\n```\n\n----------------------------------------\n\nTITLE: Text Format Log Output Example\nDESCRIPTION: Example of default text format logging output with simple timestamps\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/logs.md#2025-04-09_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n23:30:16  Running with dbt=1.8.0\n23:30:17  Registered adapter: postgres=1.8.0\n```\n\n----------------------------------------\n\nTITLE: Avoiding Absolute Paths in asset-paths Configuration\nDESCRIPTION: Example of a discouraged approach using absolute paths in asset-paths configuration. Absolute paths should be avoided for better portability of dbt projects.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/asset-paths.md#2025-04-09_snippet_2\n\nLANGUAGE: yml\nCODE:\n```\nasset-paths: [\"/Users/username/project/assets\"]\n```\n\n----------------------------------------\n\nTITLE: Markdown Case Study - Customer Conversions ML Project\nDESCRIPTION: Structured case study documenting a machine learning project at Better.com focusing on customer conversion predictions using dbt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/community/resources/speaking-at-a-meetup.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n### Predicting customer conversions with dbt + machine learning  Kenny Ning, Better.com\n\n*[Video](https://www.youtube.com/watch?v=BH7HH8JDUS0), [slides](https://docs.google.com/presentation/d/1iqVjzxxRggMnRoI40ku88miDKw795djpKV_v4bbLpPE/).*\n```\n\n----------------------------------------\n\nTITLE: Generating dbt Documentation\nDESCRIPTION: Runs the dbt docs generate command to create documentation for the dbt project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_onrunstart-onrunend-commands.md#2025-04-09_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndbt docs generate\n```\n\n----------------------------------------\n\nTITLE: Documentation Hosting Security Warning\nDESCRIPTION: Security warning about using dbt docs serve command for local development only\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/view-documentation.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n:::caution Security\n\nThe `dbt docs serve` command is only intended for local/development hosting of the documentation site. Please use one of the methods listed in the next section (or similar) to ensure that your documentation site is hosted securely!\n\n:::\n```\n\n----------------------------------------\n\nTITLE: Column Include/Exclude Configuration in YAML\nDESCRIPTION: Detailed YAML configuration showing how to specify column inclusion and exclusion in versioned models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/versions.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  # top-level model properties\n  - name: <model_name>\n    columns:\n      - name: <column_name> # required\n    \n    # versions of this model\n    versions:\n      - v: <version_identifier> # required\n        columns:\n          - include: '*' | 'all' | [<column_name>, ...]\n            exclude:\n              - <column_name>\n              - ... # declare additional column names to exclude\n          \n          # declare more columns -- can be overrides from top-level, or in addition\n          - name: <column_name>\n```\n\n----------------------------------------\n\nTITLE: Querying Metrics with Aliases\nDESCRIPTION: Example of using aliases for metrics to provide more intuitive column names in the result set. This can make queries more readable and results more user-friendly.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-jdbc.md#2025-04-09_snippet_30\n\nLANGUAGE: sql\nCODE:\n```\nselect * from {{\n    semantic_layer.query(metrics=[Metric(\"revenue\", alias=\"metric_alias\")])\n}}\n```\n\n----------------------------------------\n\nTITLE: Basic dbt Model SELECT Statement\nDESCRIPTION: A simple dbt model that selects a single column with the value 1. This basic SQL is compatible across different data warehouses.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Models/sql-dialect.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect 1 as my_column\n\n```\n\n----------------------------------------\n\nTITLE: Basic SQL CASE WHEN Syntax\nDESCRIPTION: Demonstrates the general syntax structure for SQL CASE WHEN statements. Shows how to create conditional logic with multiple scenarios and an optional else clause.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/statements/sql-case-statement.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\ncase when [scenario 1] then [result 1]\n     when [scenario 2] then [result 2]\n    -- as many scenarios as you want\n     when [scenario n] then [result n]\n     else [fallback result] -- this else is optional\nend as <new_field_name>\n```\n\n----------------------------------------\n\nTITLE: Implementing CTE Structure in dbt SQL Model\nDESCRIPTION: A SQL example showing the recommended 4-part CTE structure for dbt models: import CTEs to reference source data, logical CTEs for transformations, a final CTE to combine results, and a simple SELECT statement to output the data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/refactoring-legacy-sql.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nwith\n\nimport_orders as (\n\n    -- query only non-test orders\n    select * from {{ source('jaffle_shop', 'orders') }}\n    where amount > 0\n),\n\nimport_customers as (\n    select * from {{ source('jaffle_shop', 'customers') }}\n),\n\nlogical_cte_1 as (\n\n    -- perform some math on import_orders\n\n),\n\nlogical_cte_2 as (\n\n    -- perform some math on import_customers\n),\n\nfinal_cte as (\n\n    -- join together logical_cte_1 and logical_cte_2\n)\n\nselect * from final_cte\n```\n\n----------------------------------------\n\nTITLE: Examining dbt Debug Log File Format\nDESCRIPTION: Example of dbt's detailed debug log file (logs/dbt.log) showing initialization information, command arguments, and project resource discovery. These logs contain DEBUG-level events and contextual information for troubleshooting.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/events-logging.md#2025-04-09_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n============================== 21:21:15.272780 | 48cef052-3819-4550-a83a-4a648aef5a31 ==============================\n21:21:15.272780 [info ] [MainThread]: Running with dbt=1.5.0-b5\n21:21:15.273802 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/jerco/dev/scratch/testy/logs', 'profiles_dir': '/Users/jerco/.dbt', 'version_check': 'False', 'use_colors': 'False', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}\n21:21:16.190990 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.\n21:21:16.191404 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing\n21:21:16.207330 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 1 analysis, 535 macros, 0 operations, 1 seed file, 0 sources, 0 exposures, 0 metrics, 0 groups\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Error and Failure Resolution in dbt\nDESCRIPTION: Command to address errored models, failed tests, and related modifications in a single operation. This is useful for complex scenarios where you need to fix multiple types of issues concurrently.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/best-practice-workflows.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndbt build --select state:modified+ result:error+ result:fail+ --defer --state path/to/prod/artifacts\n```\n\n----------------------------------------\n\nTITLE: Example of Private Key Format for Snowflake Key Pair Authentication\nDESCRIPTION: Format example of an encrypted private key for Snowflake key pair authentication. The private key must include the BEGIN and END comments when pasted into dbt Cloud.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/connect-data-platform/connect-snowflake.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n-----BEGIN ENCRYPTED PRIVATE KEY-----\n< encrypted private key contents here - line 1 >\n< encrypted private key contents here - line 2 >\n< ... >\n-----END ENCRYPTED PRIVATE KEY-----\n```\n\n----------------------------------------\n\nTITLE: Generating dbt Documentation with Empty Catalog\nDESCRIPTION: This command generates dbt documentation without running database queries to populate catalog.json. It uses the '--empty-catalog' flag to skip this step.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/cmd-docs.md#2025-04-09_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndbt docs generate --empty-catalog\n```\n\n----------------------------------------\n\nTITLE: Configuring Asset Paths for Image Inclusion in dbt Core\nDESCRIPTION: This snippet shows how to set the 'asset-paths' configuration in the dbt_project.yml file to copy image directories to the target/ directory during 'dbt docs generate'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/description.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nasset-paths: [\"assets\"]\n```\n\n----------------------------------------\n\nTITLE: Cloning Repository Using SSH (Shell)\nDESCRIPTION: Shell commands to clone the demo Python project repository using SSH and navigate to the project directory.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-10-19-polyglot-dbt-python-dataframes-and-sql.md#2025-04-09_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ngit clone git@github.com:dbt-labs/demo-python-blog.git\ncd demo-python-blog\n```\n\n----------------------------------------\n\nTITLE: Documenting Filter Options in Markdown\nDESCRIPTION: Markdown documentation listing available filter options in dbt Explorer, including resource type, model access, model layer, model materialization, and tags.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/explore-projects.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n- [Resource type](/docs/build/projects) (like models, sources, and so on)\n- [Model access](/docs/collaborate/govern/model-access) (like public, private)\n- [Model layer](/best-practices/how-we-structure/1-guide-overview) (like marts, staging)\n- [Model materialization](/docs/build/materializations) (like view, table)\n- [Tags](/reference/resource-configs/tags) (supports multi-select)\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Materialization in dbt Project File\nDESCRIPTION: Sets the project name and configures all models in the jaffle_shop project to be materialized as tables by default. This is done in the dbt_project.yml configuration file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/teradata-qs.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nname: 'jaffle_shop'\n```\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  jaffle_shop:\n    +materialized: table\n```\n\n----------------------------------------\n\nTITLE: Directory Structure for dbt Staging Models\nDESCRIPTION: Example directory structure showing the organization of staging models in a dbt project. Demonstrates the recommended approach of organizing by source system (jaffle_shop and stripe) and includes documentation, schema, and source files alongside the SQL models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-structure/2-staging.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nmodels/staging\n jaffle_shop\n    _jaffle_shop__docs.md\n    _jaffle_shop__models.yml\n    _jaffle_shop__sources.yml\n    base\n       base_jaffle_shop__customers.sql\n       base_jaffle_shop__deleted_customers.sql\n    stg_jaffle_shop__customers.sql\n    stg_jaffle_shop__orders.sql\n stripe\n     _stripe__models.yml\n     _stripe__sources.yml\n     stg_stripe__payments.sql\n```\n\n----------------------------------------\n\nTITLE: Configuring DBT Sources and Models in YAML\nDESCRIPTION: Example YAML configuration defining sources and models for a jaffle_shop project. Includes table definitions, column specifications, tests, and model configurations with various properties like materialization and tags.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_configs-properties.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: raw_jaffle_shop\n    description: A replica of the postgres database used to power the jaffle_shop app.\n    tables:\n      - name: customers\n        columns:\n          - name: id\n            description: Primary key of the table\n            tests:\n              - unique\n              - not_null\n\n      - name: orders\n        columns:\n          - name: id\n            description: Primary key of the table\n            tests:\n              - unique\n              - not_null\n\n          - name: user_id\n            description: Foreign key to customers\n\n          - name: status\n            tests:\n              - accepted_values:\n                  values: ['placed', 'shipped', 'completed', 'return_pending', 'returned']\n\n\nmodels:\n  - name: stg_jaffle_shop__customers #  Must match the filename of a model -- including case sensitivity.\n    config:\n      tags: ['pii']\n    columns:\n      - name: customer_id\n        tests:\n          - unique\n          - not_null\n\n  - name: stg_jaffle_shop__orders\n    config:\n      materialized: view\n    columns:\n      - name: order_id\n        tests:\n          - unique\n          - not_null\n      - name: status\n        tests:\n          - accepted_values:\n              values: ['placed', 'shipped', 'completed', 'return_pending', 'returned']\n              config:\n                severity: warn\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Freshness Settings for Package Sources in dbt\nDESCRIPTION: Example showing how to override source freshness settings at both the source and table level for a package source. This allows customizing data freshness checks for sources defined in packages to match project-specific requirements.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/overrides.md#2025-04-09_snippet_2\n\nLANGUAGE: yml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: github\n    overrides: github_source\n\n    freshness:\n      warn_after:\n        count: 1\n        period: day\n      error_after:\n        count: 2\n        period: day\n\n    tables:\n      - name: issue_assignee\n        freshness:\n          warn_after:\n            count: 2\n            period: day\n          error_after:\n            count: 4\n            period: day\n```\n\n----------------------------------------\n\nTITLE: General Seed Configurations Pre-1.9\nDESCRIPTION: Shows general configuration options available for seeds in versions before 1.9, including enabled, tags, hooks, and other common settings.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/seed-configs.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  [<resource-path>]:\n    [+]enabled: true | false\n    [+]tags: <string> | [<string>]\n    [+]pre-hook: <sql-statement> | [<sql-statement>]\n    [+]post-hook: <sql-statement> | [<sql-statement>]\n    [+]database: <string>\n    [+]schema: <string>\n    [+]alias: <string>\n    [+]persist_docs: <dict>\n    [+]full_refresh: <boolean>\n    [+]meta: {<dictionary>}\n    [+]grants: {<dictionary>}\n```\n\n----------------------------------------\n\nTITLE: Querying Sharded BigQuery Tables in dbt SQL\nDESCRIPTION: This SQL snippet shows how to query sharded tables in BigQuery using dbt. It includes both the dbt SQL and the compiled SQL, demonstrating how to filter on shards using the _table_suffix.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/identifier.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nselect * from {{ source('ga', 'events') }}\n\n-- filter on shards by suffix\nwhere _table_suffix > '20200101'\n```\n\nLANGUAGE: sql\nCODE:\n```\nselect * from `my_project`.`ga`.`events_*`\n\n-- filter on shards by suffix\nwhere _table_suffix > '20200101'\n```\n\n----------------------------------------\n\nTITLE: Fetching Dimension Granularities\nDESCRIPTION: GraphQL query to retrieve queryable granularities for specific dimensions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-graphql.md#2025-04-09_snippet_5\n\nLANGUAGE: graphql\nCODE:\n```\n{\n  dimensions(environmentId: BigInt!, metrics:[{name:\"order_total\"}]) {\n    name\n    queryableGranularities\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example of YAML with Incorrect Indentation\nDESCRIPTION: Example of a YAML file with incorrect indentation that causes a compilation error. The 'columns' key is indented too far, creating invalid YAML syntax.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/debug-errors.md#2025-04-09_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: customers\n      columns: # this is indented too far!\n      - name: customer_id\n        tests:\n          - unique\n          - not_null\n```\n\n----------------------------------------\n\nTITLE: Joining Dimensional Tables in dbt\nDESCRIPTION: SQL code showing how to join CTE tables using appropriate join keys.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-04-18-building-a-kimball-dimensional-model-with-dbt.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n...\n\nselect\n    ... \nfrom stg_product\nleft join stg_product_subcategory on stg_product.productsubcategoryid = stg_product_subcategory.productsubcategoryid\nleft join stg_product_category on stg_product_subcategory.productcategoryid = stg_product_category.productcategoryid\n```\n\n----------------------------------------\n\nTITLE: Creating a PR Schema Cleanup Macro in dbt\nDESCRIPTION: SQL macro that identifies and drops temporary PR schemas that haven't been altered for a specified number of days. This helps manage database clutter by removing unused PR schemas created during CI/CD processes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/custom-cicd-pipelines.md#2025-04-09_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\n{# \n    This macro finds PR schemas older than a set date and drops them \n    The macro defaults to 10 days old, but can be configured with the input argument age_in_days\n    Sample usage with different date:\n        dbt run-operation pr_schema_cleanup --args \"{'database_to_clean': 'analytics','age_in_days':'15'}\"\n#}\n{% macro pr_schema_cleanup(database_to_clean, age_in_days=10) %}\n\n    {% set find_old_schemas %}\n        select \n            'drop schema {{ database_to_clean }}.'||schema_name||';'\n        from {{ database_to_clean }}.information_schema.schemata\n        where\n            catalog_name = '{{ database_to_clean | upper }}'\n            and schema_name ilike 'DBT_CLOUD_PR%'\n            and last_altered <= (current_date() - interval '{{ age_in_days }} days')\n    {% endset %}\n\n    {% if execute %}\n\n        {{ log('Schema drop statements:' ,True) }}\n\n        {% set schema_drop_list = run_query(find_old_schemas).columns[0].values() %}\n\n        {% for schema_to_drop in schema_drop_list %}\n            {% do run_query(schema_to_drop) %}\n            {{ log(schema_to_drop ,True) }}\n        {% endfor %}\n\n    {% endif %}\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kinesis Source in YAML\nDESCRIPTION: This snippet shows the YAML configuration for integrating data from a Kinesis source. It includes options for stream settings, data processing, and job control.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/upsolver-configs.md#2025-04-09_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\nstream: '<stream>'\nreader_shards: <integer>\nstore_raw_data: True/False\nstart_from: '<timestamp>/NOW/BEGINNING'\nend_at: '<timestamp>/NOW'\ncompute_cluster: '<compute_cluster>'\nrun_parallelism: <integer>\ncontent_type: 'AUTO/CSV...'\ncompression: 'AUTO/GZIP...'\ncomment: '<comment>'\ncolumn_transformations:\n  '<column>': '<expression>'\ndeduplicate_with:\n  COLUMNS: ['col1', 'col2']\n  WINDOW: 'N HOURS'\ncommit_interval: '<N MINUTE[S]/HOUR[S]/DAY[S]>'\nskip_validations: ('MISSING_STREAM')\nskip_all_validations: True/False\nexclude_columns: ('<exclude_column>', ...)\n```\n\n----------------------------------------\n\nTITLE: Source Freshness Check in dbt\nDESCRIPTION: Command to establish a baseline for source freshness checks. This must be run to generate the artifact needed for comparison in subsequent freshness-based operations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/best-practice-workflows.md#2025-04-09_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# job 1\ndbt source freshness # must be run to get previous state\n```\n\n----------------------------------------\n\nTITLE: Implementing Incremental Model with Cutoff Logic in SQL\nDESCRIPTION: This SQL snippet demonstrates how to create an incremental model in dbt. It uses the 'is_incremental()' macro to apply a cutoff filter only when specific conditions are met, such as when the model is configured as incremental and an existing table is present.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/materializations/materializations-guide-4-incremental-models.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{\\n    config(\\n        materialized='incremental',\\n        unique_key='order_id'\\n    )\\n}}\\n\\nselect * from orders\\n\\n{% if is_incremental() %}\\n\\nwhere\\n  updated_at > (select max(updated_at) from {{ this }})\\n\\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Generating Static dbt Documentation\nDESCRIPTION: This command generates dbt documentation as a static page for hosting on a cloud storage provider. It uses the '--static' flag to create a single page with embedded JSON files.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/cmd-docs.md#2025-04-09_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ndbt docs generate --static\n```\n\n----------------------------------------\n\nTITLE: Error Message When Incorrectly Configuring a Snapshot in dbt v1.4+\nDESCRIPTION: The parsing error message displayed in dbt versions 1.4 and higher when a snapshot is incorrectly configured with a materialization other than 'snapshot'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Snapshots/snapshot-target-is-not-a-snapshot-table.md#2025-04-09_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nA snapshot must have a materialized value of 'snapshot'\n```\n\n----------------------------------------\n\nTITLE: Querying Customer Data in Athena\nDESCRIPTION: Simple SQL query to select all records from the customers table in the Jaffle Shop dataset, used for testing the connection to Athena from dbt Cloud.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/athena-qs.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nselect * from jaffle_shop.customers\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Revenue Metric in YAML\nDESCRIPTION: This YAML snippet defines a simple revenue metric with basic properties including name, description, label, type, and measure parameter. The metric tracks the sum of order totals.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-build-our-metrics/semantic-layer-4-build-metrics.md#2025-04-09_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nmetrics:\n  - name: revenue\n    description: Sum of the order total.\n    label: Revenue\n    type: simple\n    type_params:\n      measure: order_total\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Packages with Relative Paths in packages.yml (Unsupported in dbt Cloud CLI)\nDESCRIPTION: This example shows a packages.yml configuration with a relative path to a local package that won't work with the dbt Cloud CLI. The configuration references a shared_macros directory located one level up from the current project directory.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_cloud-cli-relative-path.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# repository_root/my_dbt_project_in_a_subdirectory/packages.yml\n\npackages:\n  - local: ../shared_macros\n```\n\n----------------------------------------\n\nTITLE: Creating Environment Variables with API for BigQuery Credentials in dbt Cloud\nDESCRIPTION: This API request creates a new secret environment variable to store a BigQuery private key. It supports setting different values for different environments within the same project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/connect-data-platform/connnect-bigquery.md#2025-04-09_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncurl --request POST \\\n--url https://cloud.getdbt.com/api/v3/accounts/XXXXX/projects/YYYYY/environment-variables/bulk/ \\\n--header 'Accept: application/json' \\\n--header 'Authorization: Bearer ZZZZZ' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"env_var\": [\n{\n    \"new_name\": \"DBT_ENV_SECRET_PROJECTXXX_PRIVATE_KEY\",\n    \"project\": \"Value by default for the entire project\",\n    \"ENVIRONMENT_NAME_1\": \"Optional, if wanted, value for environment name 1\",\n    \"ENVIRONMENT_NAME_2\": \"Optional, if wanted, value for environment name 2\"\n}\n]\n}'\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation Block Example\nDESCRIPTION: Shows how to create a markdown documentation block for detailed resource descriptions with formatted tables and lists.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/description.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n{% docs orders_status %}\n\nOrders can be one of the following statuses:\n\n| status         | description                                                               |\n|----------------|-------------------------------------------------------------------|\n| placed         | The order has been placed but has not yet left the warehouse              |\n| shipped        | The order has been shipped to the customer and is currently in transit     |\n| completed      | The order has been received by the customer                               |\n| returned       | The order has been returned by the customer and received at the warehouse |\n\n{% enddocs %}\n```\n\n----------------------------------------\n\nTITLE: Defining Private GitHub Package with Access Token in YAML\nDESCRIPTION: This YAML snippet demonstrates how to define a private GitHub package in the packages.yml file using an environment variable to pass an access token into the repository URL. This is the supported method for cloning private GitHub packages in dbt Cloud.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/2022-release-notes.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\npackages:\n- git: \"https://{{env_var('DBT_ENV_SECRET_GIT_CREDENTIAL')}}@github.com/dbt-labs/awesome_repo.git\"\n```\n\n----------------------------------------\n\nTITLE: Configuring persist_docs for Snapshots in dbt_project.yml\nDESCRIPTION: This snippet demonstrates how to configure persist_docs for snapshots in the dbt_project.yml file. It enables persisting documentation for both relations and columns.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/persist_docs.md#2025-04-09_snippet_3\n\nLANGUAGE: yml\nCODE:\n```\nsnapshots:\n  [<resource-path>]:\n    +persist_docs:\n      relation: true\n      columns: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Null Conversion Values in YAML\nDESCRIPTION: YAML configuration that sets null conversion events to zero using the fill_nulls_with parameter in a conversion metric definition, ensuring all metrics have values in the final dataset.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/conversion-metrics.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n- name: visit_to_buy_conversion_rate_7_day_window\n  description: \"Conversion rate from viewing a page to making a purchase\"\n  type: conversion\n  label: Visit to Seller Conversion Rate (7 day window)\n  type_params:\n    conversion_type_params:\n      calculation: conversions\n      base_measure:\n        name: visits\n      conversion_measure: \n        name: buys\n        fill_nulls_with: 0\n      entity: user\n      window: 7 days \n```\n\n----------------------------------------\n\nTITLE: Creating MetricFlow Time Spine in SQL\nDESCRIPTION: SQL query to create a time spine table for MetricFlow, using the dbt_utils package to generate a date range.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/sl-snowflake-qs.md#2025-04-09_snippet_17\n\nLANGUAGE: sql\nCODE:\n```\n{{\n   config(\n       materialized = 'table',\n   )\n}}\nwith days as (\n   {{\n       dbt_utils.date_spine(\n           'day',\n           \"to_date('01/01/2000','mm/dd/yyyy')\",\n           \"to_date('01/01/2027','mm/dd/yyyy')\"\n       )\n   }}\n),\nfinal as (\n   select cast(date_day as date) as date_day\n   from days\n)\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Configuring Grants for a Table Model in SQL\nDESCRIPTION: Shows how to set grants for a table model within its SQL file. This configuration grants select privileges to 'bi_user'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/grants.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(materialized = 'table', grants = {\n    'select': 'bi_user'\n}) }}\n```\n\n----------------------------------------\n\nTITLE: Building Models Based on Fresher Sources in dbt\nDESCRIPTION: Commands to identify sources with fresher data and rebuild their downstream models. This allows for selective rebuilding of models only when source data has been updated.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/best-practice-workflows.md#2025-04-09_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# job 2\ndbt source freshness # must be run again to compare current to previous state\ndbt build --select source_status:fresher+ --state path/to/prod/artifacts\n```\n\n----------------------------------------\n\nTITLE: Complete Semantic Model with Measures\nDESCRIPTION: Full semantic model configuration including measures for order totals, counts, customer metrics, and percentile calculations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/sl-snowflake-qs.md#2025-04-09_snippet_21\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_models:\n  - name: orders\n    defaults:\n      agg_time_dimension: order_date\n    description: |\n      Order fact table. This table's grain is one row per order\n    model: ref('fct_orders')\n    entities:\n      - name: order_id\n        type: primary\n      - name: customer\n        expr: customer_id\n        type: foreign\n    dimensions:\n      - name: order_date\n        type: time\n        type_params:\n          time_granularity: day\n    measures:\n      - name: order_total\n        description: The total amount for each order including taxes.\n        agg: sum\n        expr: amount\n      - name: order_count\n        expr: 1\n        agg: sum\n      - name: customers_with_orders\n        description: Distinct count of customers placing orders\n        agg: count_distinct\n        expr: customer_id\n      - name: order_value_p99\n        expr: amount\n        agg: percentile\n        agg_params:\n          percentile: 0.99\n          use_discrete_percentile: True\n          use_approximate_percentile: False\n```\n\n----------------------------------------\n\nTITLE: Markdown Content Structure\nDESCRIPTION: Structured markdown content defining a guide for dbt materializations best practices, including frontmatter metadata, learning goals, prerequisites, and guiding principles.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/materializations/materializations-guide-1-guide-overview.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: \"Materializations best practices\"\nid: materializations-guide-1-guide-overview\nslug: 1-guide-overview\ndescription: Read this guide to understand how using materializations in dbt is a crucial skill for effective analytics engineering.\ndisplayText: Materializations best practices\nhoverSnippet: Read this guide to understand how using materializations in dbt is a crucial skill for effective analytics engineering.\n---\n```\n\n----------------------------------------\n\nTITLE: Generating dbt Documentation Without Recompilation\nDESCRIPTION: This command generates dbt documentation without recompiling the project. It uses the '--no-compile' flag to skip the compilation step.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/commands/cmd-docs.md#2025-04-09_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndbt docs generate --no-compile\n```\n\n----------------------------------------\n\nTITLE: Example Raw Login Data Table\nDESCRIPTION: Markdown table showing sample login data structure with multiple date fields and user IDs to demonstrate data complexity analysts must handle.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-09-28-analyst-to-ae.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Login_Date | Session_Date | User_Id |\n|------------|--------------|----------|\n| 2022-08-01 | 2022-08-01   | 123     |\n| 2022-08-01 | 2022-08-03   | 123     |\n| 2022-08-04 | 2022-08-04   | 975     |\n| 2022-08-04 | 2022-08-04   | NULL    |\n```\n\n----------------------------------------\n\nTITLE: Cloning the Jaffle Shop DuckDB Repository\nDESCRIPTION: Command to clone the Jaffle Shop git repository which contains the sample dbt project configured for DuckDB.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/duckdb-qs.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/dbt-labs/jaffle_shop_duckdb.git\n```\n\n----------------------------------------\n\nTITLE: Enterprise Login URL Format for dbt Cloud\nDESCRIPTION: URL pattern used to access dbt Cloud enterprise login. Requires replacing the placeholder LOGIN-SLUG with configured value and YOUR_ACCESS_URL with appropriate regional access URL.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/login_url_note.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://YOUR_ACCESS_URL/enterprise-login/LOGIN-SLUG\n```\n\n----------------------------------------\n\nTITLE: Defining Inputs for dbt Unit Tests in YAML\nDESCRIPTION: This example demonstrates how to set up a unit test for a customer dimension model that validates email addresses. It shows how to provide mock data for two inputs: a staging customers model and a reference table of valid top-level domains.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/unit-test-input.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nunit_tests:\n  - name: test_is_valid_email_address # this is the unique name of the test\n    model: dim_customers # name of the model I'm unit testing\n    given: # the mock data for your inputs\n      - input: ref('stg_customers')\n        rows:\n         - {email: cool@example.com,     email_top_level_domain: example.com}\n         - {email: cool@unknown.com,     email_top_level_domain: unknown.com}\n         - {email: badgmail.com,         email_top_level_domain: gmail.com}\n         - {email: missingdot@gmailcom,  email_top_level_domain: gmail.com}\n      - input: ref('top_level_email_domains')\n        rows:\n         - {tld: example.com}\n         - {tld: gmail.com}\n...\n```\n\n----------------------------------------\n\nTITLE: Creating BigQuery Dataset and GCS Buckets\nDESCRIPTION: Commands to create a BigQuery dataset and GCS buckets for staging Python code and storing logs.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-bigframes.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbq mk --location=${REGION} echo \"${GOOGLE_CLOUD_PROJECT}\" | tr '-' '_'_dataset\n\ngcloud storage buckets create gs://${GOOGLE_CLOUD_PROJECT}-bucket --location=${REGION}\ngcloud storage buckets add-iam-policy-binding gs://${GOOGLE_CLOUD_PROJECT}-bucket --member=serviceAccount:dbt-bigframes-sa@${GOOGLE_CLOUD_PROJECT}.iam.gserviceaccount.com --role=roles/storage.admin\n\ngcloud storage buckets create gs://${GOOGLE_CLOUD_PROJECT}-bucket-logs --location=${REGION}\ngcloud storage buckets add-iam-policy-binding gs://${GOOGLE_CLOUD_PROJECT}-bucket-logs --member=serviceAccount:dbt-bigframes-sa@${GOOGLE_CLOUD_PROJECT}.iam.gserviceaccount.com --role=roles/storage.admin\n```\n\n----------------------------------------\n\nTITLE: Disabling concurrent batches in a model config block\nDESCRIPTION: Configures an incremental model to use microbatch strategy with concurrent batches disabled in the model's SQL file, ensuring sequential batch processing.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/concurrent_batches.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(\n    materialized='incremental',\n    incremental_strategy='microbatch'\n    concurrent_batches=false\n  )\n}}\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Checking dbt Version\nDESCRIPTION: Command to verify dbt Core and Dremio plugin versions are compatible (requires dbt Core 1.5+ and Dremio adapter 1.5+).\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dremio-lakehouse.md#2025-04-09_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$ dbt --version\nCore:\n  - installed: 1.5.0 # Must be 1.5 or newer\n  - latest:    1.6.3 - Update available!\n\n  Your version of dbt-core is out of date!\n  You can find instructions for upgrading here:\n  https://docs.getdbt.com/docs/installation\n\nPlugins:\n  - dremio: 1.5.0 - Up to date! # Must be 1.5 or newer\n```\n\n----------------------------------------\n\nTITLE: Environment Configuration for Postgres\nDESCRIPTION: Deployment credentials configuration for Postgres, including username, password and schema settings.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/deploy-environments.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n#### Editable fields\n\n- **Username**: Postgres username to use (most likely a service account)\n- **Password**: Postgres password for the listed user\n- **Schema**: Target schema\n```\n\n----------------------------------------\n\nTITLE: Querying Data with SQL BETWEEN Operator\nDESCRIPTION: This SQL query demonstrates how to use the BETWEEN operator to filter orders from the 'orders' table within a specific date range. It selects customer_id, order_id, and order_date for orders placed in January 2018.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/operators/sql-between.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    customer_id,\n    order_id,\n    order_date\nfrom {{ ref('orders') }}\nwhere order_date between '2018-01-01' and '2018-01-31'\n```\n\n----------------------------------------\n\nTITLE: Creating Customer Model in dbt Cloud\nDESCRIPTION: SQL query for a dbt model that transforms raw customer and order data into a combined analytical dataset. The model joins customer information with their order history, calculating metrics like first order date, most recent order date, and total number of orders.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/microsoft-fabric-qs.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nwith customers as (\n\nselect\n    ID as customer_id,\n    FIRST_NAME as first_name,\n    LAST_NAME as last_name\n\nfrom dbo.customers\n),\n\norders as (\n\n    select\n        ID as order_id,\n        USER_ID as customer_id,\n        ORDER_DATE as order_date,\n        STATUS as status\n\n    from dbo.orders\n),\n\ncustomer_orders as (\n\n    select\n        customer_id,\n\n        min(order_date) as first_order_date,\n        max(order_date) as most_recent_order_date,\n        count(order_id) as number_of_orders\n\n    from orders\n\n    group by customer_id\n),\n\nfinal as (\n\n    select\n        customers.customer_id,\n        customers.first_name,\n        customers.last_name,\n        customer_orders.first_order_date,\n        customer_orders.most_recent_order_date,\n        coalesce(customer_orders.number_of_orders, 0) as number_of_orders\n\n    from customers\n\n    left join customer_orders on customers.customer_id = customer_orders.customer_id\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Delta Lake Incremental Model with Merge Strategy in dbt\nDESCRIPTION: SQL model example for Delta Lake that uses incremental materialization with merge strategy. Includes configuration for unique key, partitioning, and an incremental filter to process only recent data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/glue-setup.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized='incremental',\n    incremental_strategy='merge',\n    unique_key='user_id',\n    partition_by=['dt'],\n    file_format='delta'\n) }}\n\nwith new_events as (\n\n    select * from {{ ref('events') }}\n\n    {% if is_incremental() %}\n    where date_day >= date_add(current_date, -1)\n    {% endif %}\n\n)\n\nselect\n    user_id,\n    max(date_day) as last_seen,\n    current_date() as dt\n\nfrom events\ngroup by 1\n```\n\n----------------------------------------\n\nTITLE: Staging Orders SQL Model\nDESCRIPTION: SQL model showing the structure of a staging orders table with IDs, properties, and timestamps.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-build-our-metrics/semantic-layer-3-build-semantic-models.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nrenamed as (\n\n    select\n\n        ----------  ids\n        id as order_id,\n        store_id as location_id,\n        customer as customer_id,\n\n        ---------- properties\n        (order_total / 100.0) as order_total,\n        (tax_paid / 100.0) as tax_paid,\n\n        ---------- timestamps\n        ordered_at\n\n    from source\n```\n\n----------------------------------------\n\nTITLE: Controlling Color Output via Command Line Flags\nDESCRIPTION: These commands show how to enable or disable color output for dbt using command-line flags.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/print-output.md#2025-04-09_snippet_3\n\nLANGUAGE: text\nCODE:\n```\ndbt --use-colors run\ndbt --no-use-colors run\n```\n\n----------------------------------------\n\nTITLE: Displaying Marts Folder Structure in dbt\nDESCRIPTION: Shows the recommended folder structure for organizing marts models in a dbt project, grouped by department (finance and marketing).\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-structure/4-marts.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nmodels/marts\n finance\n    _finance__models.yml\n    orders.sql\n    payments.sql\n marketing\n     _marketing__models.yml\n     customers.sql\n```\n\n----------------------------------------\n\nTITLE: Executing dbt Build Command in Deployment Job\nDESCRIPTION: Command used to build and update dbt models in a production environment. This command runs all model transformations, tests, and documentation generation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/teradata-qs.md#2025-04-09_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ndbt build\n```\n\n----------------------------------------\n\nTITLE: Retrieving Webhook Details with dbt Cloud API\nDESCRIPTION: This GET request fetches detailed information about a specific webhook. It requires the account's access URL, account ID, and webhook ID as path parameters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/webhooks.md#2025-04-09_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nGET https://{your access URL}/api/v3/accounts/{account_id}/webhooks/subscription/{webhook_id}\n```\n\n----------------------------------------\n\nTITLE: Loading Data from S3 to Redshift\nDESCRIPTION: SQL COPY commands to load data from S3 bucket into Redshift tables using IAM role authentication.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/redshift-qs.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\ncopy jaffle_shop.customers( id, first_name, last_name)\nfrom 's3://dbt-data-lake-xxxx/jaffle_shop_customers.csv'\niam_role 'arn:aws:iam::XXXXXXXXXX:role/RoleName'\nregion 'us-east-1'\ndelimiter ','\nignoreheader 1\nacceptinvchars;\n   \ncopy jaffle_shop.orders(id, user_id, order_date, status)\nfrom 's3://dbt-data-lake-xxxx/jaffle_shop_orders.csv'\niam_role 'arn:aws:iam::XXXXXXXXXX:role/RoleName'\nregion 'us-east-1'\ndelimiter ','\nignoreheader 1\nacceptinvchars;\n\ncopy stripe.payment(id, orderid, paymentmethod, status, amount, created)\nfrom 's3://dbt-data-lake-xxxx/stripe_payments.csv'\niam_role 'arn:aws:iam::XXXXXXXXXX:role/RoleName'\nregion 'us-east-1'\ndelimiter ','\nignoreheader 1\nAcceptinvchars;\n```\n\n----------------------------------------\n\nTITLE: BI Tool Required Format\nDESCRIPTION: Markdown table showing the preferred data structure for BI tools with normalized group values, highlighting the importance of understanding output requirements.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-09-28-analyst-to-ae.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Date       | Group   | Active Users       |\n|------------|---------|--------------------||\n| 2022-08-01 | Group A | 34                 |\n| 2022-08-01 | Group B | 60                 |\n| 2022-08-01 | Group C | 61                 |\n| 2022-08-02 | Group A | 77                 |\n| 2022-08-02 | Group B | 86                 |\n| 2022-08-02 | Group C | 37                 |\n```\n\n----------------------------------------\n\nTITLE: Modifying Timezone of run_started_at Variable in SQL\nDESCRIPTION: This snippet shows how to convert the run_started_at variable to a different timezone (America/New_York) using the pytz module within a SQL query.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/run_started_at.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\t'{{ run_started_at.astimezone(modules.pytz.timezone(\"America/New_York\")) }}' as run_started_est\n\nfrom ...\n```\n\n----------------------------------------\n\nTITLE: Configuring unique_key in SQL Model File\nDESCRIPTION: Example of configuring unique_key in the config block of an incremental model's SQL file. This setup identifies the 'id' column as the unique identifier for upsert operations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/unique_key.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        materialized='incremental',\n        unique_key='id'\n    )\n}}\n```\n\n----------------------------------------\n\nTITLE: Refactoring Staging Orders Model to Use Source Function\nDESCRIPTION: Updates the stg_orders model to use the source() function to reference the raw orders table defined in the sources.yml file. This enables better data lineage tracking and source freshness monitoring.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/teradata-qs.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\nselect\n   id as order_id,\n   user_id as customer_id,\n   order_date,\n   status\n\nfrom {{ source('jaffle_shop', 'orders') }}\n```\n\n----------------------------------------\n\nTITLE: Refactoring Staging Customer Model to Use Source in SQL\nDESCRIPTION: This SQL code refactors the staging customer model to use the source() function, referencing the jaffle_shop source defined in the YAML file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/snowflake-qs.md#2025-04-09_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    id as customer_id,\n    first_name,\n    last_name\n\nfrom {{ source('jaffle_shop', 'customers') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring a Cumulative Metric with Window in YAML (v1.8 and earlier)\nDESCRIPTION: This snippet shows how to define a cumulative metric 'weekly_customers' with a 7-day window in MetricFlow versions 1.8 and earlier.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/cumulative-metrics.md#2025-04-09_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics: \n  - name: weekly_customers\n  type: cumulative\n  type_params:\n    measure: customers\n    window: 7 days\n```\n\n----------------------------------------\n\nTITLE: Time Dimension Definition\nDESCRIPTION: Demonstrates how to define a time dimension with day-level granularity in a semantic model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-build-our-metrics/semantic-layer-3-build-semantic-models.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ndimensions:\n  - name: ordered_at\n    expr: date_trunc('day', ordered_at)\n    type: time\n    type_params:\n      time_granularity: day\n```\n\n----------------------------------------\n\nTITLE: Testing Webhook Endpoint Authorization\nDESCRIPTION: Shell command to test if an endpoint accepts Authorization headers using curl.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/webhooks.md#2025-04-09_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\ncurl -H 'Authorization: 123' -X POST https://<your-webhook-endpoint>\n```\n\n----------------------------------------\n\nTITLE: Basic Semantic Model Structure in YAML\nDESCRIPTION: Shows the basic structure of a semantic model definition with placeholder sections for entities, dimensions, and measures.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-build-our-metrics/semantic-layer-3-build-semantic-models.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_models:\n  - name: orders\n    entities: ... # we'll define these later\n    dimensions: ... # we'll define these later\n    measures: ... # we'll define these later\n```\n\n----------------------------------------\n\nTITLE: Creating Ratio Metric for Food Revenue Percentage in dbt YAML\nDESCRIPTION: This snippet defines a ratio metric to calculate the percentage of order revenue from food. It uses the 'food_revenue' metric as the numerator and 'revenue' as the denominator.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-build-our-metrics/semantic-layer-5-advanced-metrics.md#2025-04-09_snippet_1\n\nLANGUAGE: yml\nCODE:\n```\n- name: food_revenue_pct\n  description: The % of order revenue from food.\n  label: Food Revenue %\n  type: ratio\n  type_params:\n    numerator: food_revenue\n    denominator: revenue\n```\n\n----------------------------------------\n\nTITLE: Creating a Staging Customers Model in SQL for dbt\nDESCRIPTION: This SQL query creates a staging model for customers, selecting and renaming columns from the customers table in the jaffle_shop database.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/bigquery-qs.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    id as customer_id,\n    first_name,\n    last_name\n\nfrom `dbt-tutorial`.jaffle_shop.customers\n```\n\n----------------------------------------\n\nTITLE: Specifying Quota Project for BigQuery API Usage\nDESCRIPTION: Configuration to override the default quota project for BigQuery API usage. This is useful when impersonating service accounts that do not have the BigQuery API enabled in their own project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/bigquery-setup.md#2025-04-09_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nmy-profile:\n  target: dev\n  outputs:\n    dev:\n      type: bigquery\n      method: oauth\n      project: abc-123\n      dataset: my_dataset\n      quota_project: my-bq-quota-project\n```\n\n----------------------------------------\n\nTITLE: Cloning and Navigating to the Airflow-dbt-Cloud Repository\nDESCRIPTION: Commands to clone the airflow-dbt-cloud repository from GitHub and navigate to the project directory. This repository contains example Airflow DAGs for orchestrating dbt Cloud jobs.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/airflow-and-dbt-cloud.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/dbt-labs/airflow-dbt-cloud.git\ncd airflow-dbt-cloud\n```\n\n----------------------------------------\n\nTITLE: Explicitly rendering a source reference in dbt with the render method\nDESCRIPTION: This snippet demonstrates how to use the .render() method on a source() call within a pre-hook configuration. This ensures dbt processes the source relation even when the --empty flag is used for optimization.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_render-method.md#2025-04-09_snippet_0\n\nLANGUAGE: Jinja\nCODE:\n```\n{{ config(\n    pre_hook = [\n        \"alter external table {{ source('sys', 'customers').render() }} refresh\"\n    ]\n) }}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Unit Testing with dbt-unittest Package\nDESCRIPTION: Simplified unit test implementation using the dbt-unittest package's assertion macros.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-08-22-unit-testing-dbt-package.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\n{% macro test_to_literal() %}\n\n    {{ return(adapter.dispatch('test_to_literal', 'integration_tests')(text)) }}\n\n{% endmacro %}\n\n{% macro default__test_to_literal() %}\n\n    {% result = dbt_sample_package.to_literal('test string') %}\n\n    {{ dbt_unittest.assert_equals(result, \"'test string'\") }}\n\n{% endmacro %}\n\n{% macro postgres__test_to_literal() %}\n\n    {% result = dbt_sample_package.to_literal('test string') %}\n\n    {{ dbt_unittest.assert_equals(result, \"E'test string'\") }}\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Adding Cron Schedule to Databricks Materialized View\nDESCRIPTION: This post-hook demonstrates how to add a cron schedule to a materialized view in Databricks for automatic refreshing.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-08-01-announcing-materialized-views.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\npost_hook = 'ALTER MATERIALIZED VIEW {{this}} ADD SCHEDULE CRON \"0 0 0 * * ? *\" AT TIME ZONE \"America/Los_Angeles\";'\n```\n\n----------------------------------------\n\nTITLE: Configuring Models in Subdirectory\nDESCRIPTION: Shows how to apply configuration to all models within a specific subdirectory of a project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/resource-path.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nname: jaffle_shop\n\nmodels:\n  jaffle_shop:\n    staging:\n      +enabled: false # this will apply to all models in the `staging/` directory of your project\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Grants in dbt YAML for Teradata\nDESCRIPTION: Setting multiple grant types (select and insert) for different users on a dbt model in Teradata, demonstrating how to configure different permission types.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/teradata-configs.md#2025-04-09_snippet_19\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n- name: model_name\n  config:\n    materialized: table\n    grants:\n      select: [\"user_b\"]\n      insert: [\"user_c\"]\n```\n\n----------------------------------------\n\nTITLE: Using Environment Variables in Databricks Connection Settings\nDESCRIPTION: Example of using dbt Cloud's environment variable syntax for dynamically configuring Databricks workspace connections across different environments. This approach allows referencing multiple workspaces from a single dbt Cloud project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/dbt-unity-catalog-best-practices.md#2025-04-09_snippet_0\n\nLANGUAGE: dbt\nCODE:\n```\n{{env_var('DBT_HOSTNAME')}}.cloud.databricks.com\n```\n\nLANGUAGE: dbt\nCODE:\n```\n/sql/1.0/warehouses/{{env_var('DBT_HTTP_PATH')}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Redshift Profiles with Different Autocommit Settings in YAML\nDESCRIPTION: This YAML configuration demonstrates how to set up multiple Redshift profiles with different autocommit settings. It's useful for running certain macros that require autocommit to be enabled.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/redshift-setup.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nprofile-to-my-RS-target:\n  target: dev\n  outputs:\n    dev:\n      type: redshift\n      ...\n      autocommit: False\n      \n  \nprofile-to-my-RS-target-with-autocommit-enabled:\n  target: dev\n  outputs:\n    dev:\n      type: redshift\n      ...\n      autocommit: True\n```\n\n----------------------------------------\n\nTITLE: Accessing Models in dbt Graph\nDESCRIPTION: Example showing how to access and print information about models in the Snowplow package using the graph context variable.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/graph.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{% if execute %}\n  {% for node in graph.nodes.values()\n     | selectattr(\"resource_type\", \"equalto\", \"model\")\n     | selectattr(\"package_name\", \"equalto\", \"snowplow\") %}\n  \n    {% do log(node.unique_id ~ \", materialized: \" ~ node.config.materialized, info=true) %}\n  \n  {% endfor %}\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Basic Jinja Template Structure in dbt\nDESCRIPTION: Example showing the basic structure for using Jinja templates with dbt_utils.date_spine function.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/dont-nest-your-curlies.md#2025-04-09_snippet_0\n\nLANGUAGE: jinja\nCODE:\n```\n  {{ dbt_utils.date_spine(\n      datepart=\"day\",\n      start_date=[ USE JINJA HERE ]\n      )\n  }}\n```\n\n----------------------------------------\n\nTITLE: Running Tagged Tests in dbt\nDESCRIPTION: This command selects and runs tests tagged with 'my_test_tag' in dbt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/test-selection-examples.md#2025-04-09_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ndbt test --select \"tag:my_test_tag\"\n```\n\n----------------------------------------\n\nTITLE: Semantic Model with Entities\nDESCRIPTION: Shows how to define entities in a semantic model, including primary and foreign entity types with expressions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-build-our-metrics/semantic-layer-3-build-semantic-models.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_models:\n  - name: orders\n    ...\n    entities:\n      # we use the column for the name here because order is a reserved word in SQL\n      - name: order_id\n        type: primary\n      - name: location\n        type: foreign\n        expr: location_id\n      - name: customer\n        type: foreign\n        expr: customer_id\n\n    dimensions:\n      ...\n    measures:\n      ...\n```\n\n----------------------------------------\n\nTITLE: Specifying Git Package with Commit Hash\nDESCRIPTION: This snippet demonstrates how to specify a git package using a specific commit hash as the revision.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/packages.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\npackages:\n  - git: \"https://github.com/dbt-labs/dbt-utils.git\"\n    revision: 4e28d6da126e2940d17f697de783a717f2503188\n```\n\n----------------------------------------\n\nTITLE: Disabling a Nested Source Table in dbt_project.yml (v1.8 and earlier)\nDESCRIPTION: Example showing how to disable a source table nested in a YAML file in a subfolder for dbt version 1.8 and earlier using the resource path syntax.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/source-configs.md#2025-04-09_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  your_project_name:\n    subdirectory_name:\n      source_name:\n        source_table_name:\n          +enabled: false\n```\n\n----------------------------------------\n\nTITLE: Ensuring Uniqueness in Surrogate Key Generation\nDESCRIPTION: This SQL snippet shows how to ensure uniqueness in surrogate keys by adding a separator between concatenated fields. It addresses the issue of potentially identical keys when concatenating different combinations of values.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-11-22-sql-surrogate-keys.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nselect\n  *,\n  concat(\n    coalesce(cast(user_id as string), ''),\n    '|',\n    coalesce(cast(product_id as string), '')\n    ) as _surrogate_key\nfrom example_ids\n```\n\n----------------------------------------\n\nTITLE: Displaying git rev-list master error in Shell\nDESCRIPTION: This code snippet shows the error message encountered when trying to access the dbt Cloud IDE. The error indicates an issue with identifying the primary branch, possibly due to a branch name change from 'master' to 'main'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Troubleshooting/git-revlist-error.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit rev-list master..origin/main --count\nfatal: ambiguous argument 'master..origin/main': unknown revision or path not in the working tree.\nUse '--' to separate paths from revisions, like this:\n'git <command> [<revision>...] -- [<file>...]'\n```\n\n----------------------------------------\n\nTITLE: SQL Permissioning Examples\nDESCRIPTION: Examples showing different ways to handle grant statements across databases - some accept multiple grantees while others require separate statements.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\ngrant SELECT on table dinner.corn to corn_kid, everyone;\ngrant SELECT on table dinner.corn to corn_kid; grant SELECT on table dinner.corn to everyone\n```\n\n----------------------------------------\n\nTITLE: Creating dbt Snapshots\nDESCRIPTION: Executes the dbt snapshot command to capture changes in mutable tables over time.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_onrunstart-onrunend-commands.md#2025-04-09_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ndbt snapshot\n```\n\n----------------------------------------\n\nTITLE: Configuring Surrogate Keys in YAML\nDESCRIPTION: YAML configuration for enabling surrogate keys on a dbt model using meta config.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-08-17-managing-surrogate-keys-in-dbt.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: dim_customers\n\tdescription: all customers\n    config:\n      meta:\n        surrogate_key: true\n```\n\n----------------------------------------\n\nTITLE: Using a Macro for Query Comments in dbt_project.yml\nDESCRIPTION: Configuration to use a custom macro for generating query comments, ensuring the macro call is properly quoted for YAML parsing.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/query-comment.md#2025-04-09_snippet_15\n\nLANGUAGE: yml\nCODE:\n```\nquery-comment: \"{{ query_comment() }}\"\n\n```\n\n----------------------------------------\n\nTITLE: Setting Character Set and Collation in SingleStore\nDESCRIPTION: Demonstrates how to specify the character set and collation for a SingleStore table using the charset and collation configuration options. This example uses UTF-8 multibyte encoding with general case-insensitive collation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/singlestore-configs.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        charset='utf8mb4',\n        collation='utf8mb4_general_ci'\n    )\n}}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: YAML Frontmatter Configuration for Documentation Page\nDESCRIPTION: YAML configuration block defining metadata for the documentation page including title, ID, description, and various display settings.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/sl-partner-integration-guide.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: \"Integrate with dbt Semantic Layer using best practices\" \nid: \"sl-partner-integration-guide\"\ndescription: Learn about partner integration guidelines, roadmap, and connectivity. \nhoverSnippet: Learn how to integrate with the Semantic Layer using best practices\n# time_to_complete: '30 minutes' commenting out until we test\nicon: 'guides'\nhide_table_of_contents: true\ntags: ['Semantic Layer','Best practices']\nlevel: 'Advanced'\nrecently_updated: true\n---\n```\n\n----------------------------------------\n\nTITLE: Using a Custom Generic Test in YAML Configuration\nDESCRIPTION: Example of how to apply the custom 'is_even' test to a column in a model's YAML configuration file. This shows how to reference the test by name in the tests property of a column.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/custom-generic-tests.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: users\n    columns:\n      - name: favorite_number\n        tests:\n      \t  - is_even\n            [description](/reference/resource-properties/description): \"This is a test\"\n```\n\n----------------------------------------\n\nTITLE: Derived Metrics Implementation Examples\nDESCRIPTION: Practical examples of derived metrics including gross profit calculations and growth metrics with filters and offset windows.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/derived-metrics.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  - name: order_gross_profit\n    description: Gross profit from each order.\n    type: derived\n    label: Order gross profit\n    type_params:\n      expr: revenue - cost\n      metrics:\n        - name: order_total\n          alias: revenue\n        - name: order_cost\n          alias: cost\n  - name: food_order_gross_profit\n    label: Food order gross profit\n    description: \"The gross profit for each food order.\"\n    type: derived\n    type_params:\n      expr: revenue - cost\n      metrics:\n        - name: order_total\n          alias: revenue\n          filter: |\n            {{ Dimension('order__is_food_order') }} = True\n        - name: order_cost\n          alias: cost\n          filter: |\n            {{ Dimension('order__is_food_order') }} = True\n  - name: order_total_growth_mom\n    description: \"Percentage growth of orders total completed to 1 month ago\"\n    type: derived\n    label: Order total growth % M/M\n    type_params:\n      expr: (order_total - order_total_prev_month)*100/order_total_prev_month\n      metrics: \n        - name: order_total\n        - name: order_total\n          offset_window: 1 month\n          alias: order_total_prev_month\n```\n\n----------------------------------------\n\nTITLE: Using a Relationships Test with Custom Arguments in YAML\nDESCRIPTION: Shows how to implement the 'relationships' test in a YAML file, providing values for the additional required arguments 'to' and 'field' as a dictionary.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/custom-generic-tests.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: people\n    columns:\n      - name: account_id\n        tests:\n          - relationships:\n            [description](/reference/resource-properties/description): \"This is a test\"\n              to: ref('accounts')\n              field: id\n```\n\n----------------------------------------\n\nTITLE: Generating Alias Names for Concurrent Development in SingleStore\nDESCRIPTION: SQL macro to support concurrent development by prefixing table names with the schema specified in the profiles.yml file. This macro allows for namespace-like functionality in SingleStore, which doesn't natively support schemas like other databases.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/singlestore-setup.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{% macro generate_alias_name(custom_alias_name=none, node=none) -%}\n    {%- if custom_alias_name is none -%}\n        {{ node.schema }}__{{ node.name }}\n    {%- else -%}\n        {{ node.schema }}__{{ custom_alias_name | trim }}\n    {%- endif -%}\n{%- endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Documenting Generic Test Macros in YAML\nDESCRIPTION: Example of how to document a custom generic test macro in a schema.yml file, including descriptions for the test itself and its arguments. Note the use of the 'test_' prefix in the macro name.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/custom-generic-tests.md#2025-04-09_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nmacros:\n  - name: test_not_empty_string\n    description: Complementary test to default `not_null` test as it checks that there is not an empty string. It only accepts columns of type string.\n    arguments:\n      - name: model \n        type: string\n        description: Model Name\n      - name: column_name\n        type: string\n        description: Column name that should not be an empty string\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Model with Special Characters (Invalid Syntax)\nDESCRIPTION: This snippet demonstrates an invalid configuration attempt using the standard config block syntax. It fails due to the presence of a dash in the 'post-hook' parameter.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/advanced-config-usage.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    post-hook=\"grant select on {{ this }} to role reporter\",\n    materialized='table'\n) }}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Implementing PySpark UDF with External Python Files in dbt\nDESCRIPTION: This snippet demonstrates how to use PySpark User-Defined Functions (UDFs) with external Python files in a dbt model. It configures an incremental model with merge strategy, adds external Python files to the Spark context, defines a UDF using an imported function, and applies it to a DataFrame column.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/athena-configs.md#2025-04-09_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef model(dbt, spark_session):\n    dbt.config(\n        materialized=\"incremental\",\n        incremental_strategy=\"merge\",\n        unique_key=\"num\",\n    )\n    sc = spark_session.sparkContext\n    sc.addPyFile(\"s3://athena-dbt/test/file1.py\")\n    sc.addPyFile(\"s3://athena-dbt/test/file2.py\")\n\n    def func(iterator):\n        from file2 import transform\n\n        return [transform(i) for i in iterator]\n\n    from pyspark.sql.functions import udf\n    from pyspark.sql.functions import col\n\n    udf_with_import = udf(func)\n\n    data = [(1, \"a\"), (2, \"b\"), (3, \"c\")]\n    cols = [\"num\", \"alpha\"]\n    df = spark_session.createDataFrame(data, cols)\n\n    return df.withColumn(\"udf_test_col\", udf_with_import(col(\"alpha\")))\n```\n\n----------------------------------------\n\nTITLE: Configuring a Basic Timestamp-based Snapshot in dbt (Pre-1.9)\nDESCRIPTION: A snapshot configuration using the timestamp strategy with a simple 'updated_at' column for change tracking. Uses target_schema parameter which was deprecated in dbt 1.9.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snapshots-jinja-legacy.md#2025-04-09_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot orders_snapshot %}\n\n{{\n    config(\n      target_schema='snapshots',\n      unique_key='id',\n\n      strategy='timestamp',\n      updated_at='updated_at'\n    )\n}}\n\nselect * from {{ source('jaffle_shop', 'orders') }}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Response Schema for Listing Webhook Subscriptions\nDESCRIPTION: This JSON response shows the structure of the data returned when listing webhook subscriptions. It includes details such as webhook IDs, names, event types, and status codes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/webhooks.md#2025-04-09_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"data\": [\n        {\n            \"id\": \"wsu_12345abcde\",\n            \"account_identifier\": \"act_12345abcde\",\n            \"name\": \"Webhook for jobs\",\n            \"description\": \"A webhook for when jobs are started\",\n            \"job_ids\": [\n                \"123\",\n                \"321\"\n            ],\n            \"event_types\": [\n                \"job.run.started\"\n            ],\n            \"client_url\": \"https://test.com\",\n            \"active\": true,\n            \"created_at\": \"1675735768491774\",\n            \"updated_at\": \"1675787482826757\",\n            \"account_id\": \"123\",\n            \"http_status_code\": \"0\"\n        },\n        {\n            \"id\": \"wsu_12345abcde\",\n            \"account_identifier\": \"act_12345abcde\",\n            \"name\": \"Notification Webhook\",\n            \"description\": \"Webhook used to trigger notifications in Slack\",\n            \"job_ids\": [],\n            \"event_types\": [\n                \"job.run.completed\",\n                \"job.run.started\",\n                \"job.run.errored\"\n            ],\n            \"client_url\": \"https://test.com\",\n            \"active\": true,\n            \"created_at\": \"1674645300282836\",\n            \"updated_at\": \"1675786085557224\",\n            \"http_status_code\": \"410\",\n            \"dispatched_at\": \"1675786085548538\",\n            \"account_id\": \"123\"\n        }\n    ],\n    \"status\": {\n        \"code\": 200\n    },\n    \"extra\": {\n        \"pagination\": {\n            \"total_count\": 2,\n            \"count\": 2\n        },\n        \"filters\": {\n            \"offset\": 0,\n            \"limit\": 10\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: BigQuery Configuration Parameters Example\nDESCRIPTION: Demonstrates the syntax for two distinct BigQuery access control parameters: grant_access_to for creating authorized views and grants for managing dataset permissions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/grants-vs-access-to.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ngrants_access_to:\ngrants:\n```\n\n----------------------------------------\n\nTITLE: Displaying Markdown Image with Caption\nDESCRIPTION: This snippet demonstrates how to include an image with a caption in Markdown format. It uses the standard Markdown image syntax followed by italicized text for the caption.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-04-05-when-backend-devs-spark-joy.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n![A screenshot of the conversation that started the thread](/img/blog/2022-04-05-when-backend-devs-spark-joy/slack_thread_screenshot.png)\n*A screenshot of the conversation that started the thread*\n```\n\n----------------------------------------\n\nTITLE: Querying Applied Models in GraphQL for dbt Cloud Discovery API\nDESCRIPTION: This GraphQL query fetches a list of all executed models and their execution time from the latest state environment-level API. It includes model details such as name, unique ID, materialized type, and execution information.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/discovery-use-cases-and-examples.md#2025-04-09_snippet_0\n\nLANGUAGE: graphql\nCODE:\n```\nquery AppliedModels($environmentId: BigInt!, $first: Int!) {\n  environment(id: $environmentId) {\n    applied {\n      models(first: $first) {\n        edges {\n          node {\n            name\n            uniqueId\n            materializedType\n            executionInfo {\n              lastSuccessRunId\n              executionTime\n              executeStartedAt\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Macro Argument Types\nDESCRIPTION: This snippet demonstrates how to specify the type for macro arguments in a YAML configuration file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/arguments.md#2025-04-09_snippet_1\n\nLANGUAGE: yml\nCODE:\n```\nversion: 2\n\nmacros:\n  - name: <macro name>\n    arguments:\n      - name: <arg name>\n        type: <string>\n\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Schema for Seeds\nDESCRIPTION: Configuration to place seeds in a separate 'mappings' schema using dbt_project.yml.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/schema.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  your_project:\n    product_mappings:\n      +schema: mappings\n```\n\n----------------------------------------\n\nTITLE: Configuring Extended Attributes for dbt Cloud Environment\nDESCRIPTION: This YAML snippet demonstrates how to set extended attributes in a dbt Cloud environment. It includes database connection details and shows how to use environment variables for sensitive information.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_cloud-environments-info.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndbname: jaffle_shop      \nschema: dbt_alice      \nthreads: 4\nusername: alice\npassword: '{{ env_var(''DBT_ENV_SECRET_PASSWORD'') }}'\n```\n\n----------------------------------------\n\nTITLE: Creating Lap Times by Year Intermediate Model in SQL\nDESCRIPTION: Joins lap time data with race information to enable analysis of lap times across different years. The model filters out records with null lap times, which were common in earlier Formula 1 eras when lap timing wasn't recorded.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\nwith lap_times as (\n\n    select * from {{ ref('stg_f1_lap_times') }}\n\n),\n\nraces as (\n\n    select * from {{ ref('stg_f1_races') }}\n\n),\n\nexpanded_lap_times_by_year as (\n    select\n        lap_times.race_id,\n        driver_id,\n        race_year,\n        lap,\n        lap_time_milliseconds\n    from lap_times\n    left join races\n        on lap_times.race_id = races.race_id\n    where lap_time_milliseconds is not null\n)\n\nselect * from expanded_lap_times_by_year\n```\n\n----------------------------------------\n\nTITLE: Creating Database Object-Level Comments\nDESCRIPTION: SQL syntax for adding comments to database objects like tables and views. This approach adds metadata directly to the database objects for better context and documentation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/other/sql-comments.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\ncomment on [database object type] <database object name> is 'comment text here';\n```\n\n----------------------------------------\n\nTITLE: Configuring Directory-Level Materializations in dbt\nDESCRIPTION: Example of setting materializations for different folders in dbt_project.yml, demonstrating cascading configurations where marketing and paid_ads folders are set as views while the google subfolder is configured as tables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/materializations/materializations-guide-5-best-practices.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  jaffle_shop:\n    marketing:\n      +materialized: view\n      paid_ads:\n        google:\n          +materialized: table\n```\n\n----------------------------------------\n\nTITLE: Loading Customer Data into Snowflake SQL\nDESCRIPTION: This SQL command loads customer data from an S3 bucket CSV file into the previously created customers table. It specifies the CSV format and skips the header row.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/sl-snowflake-qs.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\ncopy into raw.jaffle_shop.customers (id, first_name, last_name)\nfrom 's3://dbt-tutorial-public/jaffle_shop_customers.csv'\nfile_format = (\n    type = 'CSV'\n    field_delimiter = ','\n    skip_header = 1\n    );\n```\n\n----------------------------------------\n\nTITLE: Generic Test with Default Configuration in SQL\nDESCRIPTION: Example of a generic test that includes default configuration settings. This 'warn_if_odd' test sets a default severity level of 'warn' using the config() function.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/custom-generic-tests.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{% test warn_if_odd(model, column_name) %}\n\n    {{ config(severity = 'warn') }}\n\n    select *\n    from {{ model }}\n    where ({{ column_name }} % 2) = 1\n\n{% endtest %}\n```\n\n----------------------------------------\n\nTITLE: Configuring dispatch for compatibility package in YAML\nDESCRIPTION: This example demonstrates how to configure dispatch to use a compatibility package (spark_utils) as a shim for dbt_utils. It sets the search order to look in spark_utils before dbt_utils.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/dispatch-config.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndispatch:\n  - macro_namespace: dbt_utils\n    search_order: ['spark_utils', 'dbt_utils']\n```\n\n----------------------------------------\n\nTITLE: DBT Snapshot with Full Configuration (Pre-1.9)\nDESCRIPTION: Complete snapshot configuration using timestamp strategy with target database and schema specifications for versions before 1.9.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snapshots-jinja-legacy.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot orders_snapshot %}\n\n{{\n    config(\n      target_database='analytics',\n      target_schema='snapshots',\n      unique_key='id',\n\n      strategy='timestamp',\n      updated_at='updated_at',\n    )\nThe following table outlines the configurations available for snapshots:\n\nselect * from {{ source('jaffle_shop', 'orders') }}\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Testing RisingWave Connection in dbt\nDESCRIPTION: Command to test the connection to RisingWave using dbt debug.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/risingwave-setup.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndbt debug\n```\n\n----------------------------------------\n\nTITLE: Implementing Test Methods for dbt Commands in Python\nDESCRIPTION: This code defines test methods that execute dbt commands and perform assertions. It includes examples of running dbt seed, run, and test commands, as well as handling expected test failures using pytest's xfail marker.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n    # The actual sequence of dbt commands and assertions\n    # pytest will take care of all \"setup\" + \"teardown\"\n    def test_run_seed_test(self, project):\n        \"\"\"\n        Seed, then run, then test. We expect one of the tests to fail\n        An alternative pattern is to use pytest \"xfail\" (see below)\n        \"\"\"\n        # seed seeds\n        results = run_dbt([\"seed\"])\n        assert len(results) == 1\n        # run models\n        results = run_dbt([\"run\"])\n        assert len(results) == 1\n        # test tests\n        results = run_dbt([\"test\"], expect_pass = False) # expect failing test\n        assert len(results) == 2\n        # validate that the results include one pass and one failure\n        result_statuses = sorted(r.status for r in results)\n        assert result_statuses == [\"fail\", \"pass\"]\n\n    @pytest.mark.xfail\n    def test_build(self, project):\n        \"\"\"Expect a failing test\"\"\"\n        # do it all\n        results = run_dbt([\"build\"])\n```\n\n----------------------------------------\n\nTITLE: Installing dbt Adapter Package using pip (version 1.7 and earlier)\nDESCRIPTION: Command to install the specific adapter package for dbt versions 1.7 and earlier. This installation method automatically includes dbt-core and any additional dependencies.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_setup-pages-intro.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install {props.meta.pypi_package}\n```\n\n----------------------------------------\n\nTITLE: Implementing Email Domain Extraction Macro in dbt\nDESCRIPTION: Jinja macro definition for extract_email_domain that uses a regular expression to capture the domain part of an email address. It converts the email to lowercase before extraction to ensure case insensitivity.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-02-07-customer-360-view-census-playbook.md#2025-04-09_snippet_1\n\nLANGUAGE: jinja\nCODE:\n```\n{% macro extract_email_domain(email) %}\n\n{# This is the SQL to extract the email domain in the Snowflake Flavor of SQL #}\n\n\tregexp_substr(lower({{ email }}), '@(.*)', 1, 1, 'e',1)\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: January 2025 dbt Dependencies Configuration\nDESCRIPTION: List of dbt Core and adapter versions included in the January 2025 Compatible release, showing dependencies and their versions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/compatible-track-changelog.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ndbt-core==1.9.1\n\n# shared interfaces\ndbt-adapters==1.13.1\ndbt-common==1.14.0\ndbt-semantic-interfaces==0.7.4\n\n# adapters\ndbt-athena==1.9.0\ndbt-bigquery==1.9.1\ndbt-databricks==1.9.1\ndbt-fabric==1.9.0\ndbt-postgres==1.9.0\ndbt-redshift==1.9.0\ndbt-snowflake==1.9.0\ndbt-spark==1.9.0\ndbt-synapse==1.8.2\ndbt-teradata==1.9.0\ndbt-trino==1.9.0\n```\n\n----------------------------------------\n\nTITLE: Configuring Docs for Models in schema.yml\nDESCRIPTION: Shows how to configure documentation visibility and node color for specific models in a schema.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/docs.md#2025-04-09_snippet_1\n\nLANGUAGE: yml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: model_name\n    docs:\n      show: true | false\n      node_color: color_id # Use name (such as node_color: purple) or hex code with quotes (such as node_color: \"#cd7f32\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Extended Attributes for Staging Environment in YAML\nDESCRIPTION: This YAML snippet shows the configuration of extended attributes for a staging environment in dbt Cloud. It sets up a connection to a BigQuery account with specific project, dataset, and authentication settings for staging work.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-04-22-extended-attributes.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\naccount: 123dev\nproject: staging\ndataset: main\nmethod: service-account-json\nthreads: 16\n```\n\n----------------------------------------\n\nTITLE: Configuring Dynamic Table in Snowflake with dbt\nDESCRIPTION: This configuration sets up a dynamic table (Snowflake's equivalent to materialized view) using dbt. It specifies the warehouse, target lag, and behavior on configuration changes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-08-01-announcing-materialized-views.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\n{{\\nconfig(\\n    materialized = 'dynamic_table',\\n    snowflake_warehouse = '<warehouse>',\\n    target_lag = '<desired_lag>',\\n    on_configuration_change = 'apply',\\n)\\n}}\n```\n\n----------------------------------------\n\nTITLE: Duplicate Test Definition in dbt YAML Configuration\nDESCRIPTION: This YAML snippet shows an example of defining duplicate tests with different configurations, which can lead to compilation errors in dbt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/data-tests.md#2025-04-09_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: orders\n    columns:\n      - name: status\n        tests:\n          - accepted_values:\n              values: ['placed', 'shipped', 'completed', 'returned']\n              config:\n                where: \"order_date = current_date\"\n          - accepted_values:\n              values: ['placed', 'shipped', 'completed', 'returned']\n              config:\n                # only difference is in the 'where' config\n                where: \"order_date = (current_date - interval '1 day')\" # PostgreSQL syntax\n```\n\n----------------------------------------\n\nTITLE: Defining Frontmatter for Stacy Lo's Community Spotlight in Markdown\nDESCRIPTION: This YAML frontmatter block defines metadata for Stacy Lo's community spotlight page, including her personal information, social links, and community award status.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/community/spotlight/stacy-lo.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nid: stacy-lo\ntitle: Stacy Lo\ndescription: |\n  I began my career as a data analyst, then transitioned to a few different roles in data and software development. Analytics Engineer is the best title to describe my expertise in data.\n\n  I've been in the dbt Community for almost a year. In April, I shared my experience adopting dbt at the <a href=\"https://www.meetup.com/taipei-dbt-meetup/\" rel=\"noopener noreferrer\" target=\"_blank\">Taipei dbt Meetup</a>, which inspired me to write technical articles.\n\n  In Taiwan, the annual \"iThome Iron Man Contest\" happens in September, where participants post a technical article written in Mandarin every day for 30 consecutive days. Since no one has ever written about dbt in the contest, I'd like to be the first person, and that's what I have been busy with for in the past couple of months.\nimage: /img/community/spotlight/stacy.jpg\npronouns: she/her\nlocation: Taipei, Taiwan\njobTitle: Senior IT Developer\ncompanyName: Teamson\nsocialLinks:\n  - name: LinkedIn\n    link: https://www.linkedin.com/in/olycats/\ndateCreated: 2023-11-01\nhide_table_of_contents: true\ncommunityAward: true\ncommunityAwardYear: 2023\n---\n```\n\n----------------------------------------\n\nTITLE: Defining a Cumulative Metric with Grain-to-Date in YAML (v1.8 and earlier)\nDESCRIPTION: This snippet shows how to define a cumulative metric 'orders_last_month_to_date' using a monthly grain-to-date approach in MetricFlow versions 1.8 and earlier.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/cumulative-metrics.md#2025-04-09_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\n- name: orders_last_month_to_date\n  label: Orders month to date\n  type: cumulative\n  type_params:\n    measure: order_count\n    grain_to_date: month\n```\n\n----------------------------------------\n\nTITLE: Incorrect Nested Curly Braces Usage\nDESCRIPTION: Shows an incorrect implementation where Jinja expressions are unnecessarily nested, resulting in literal string interpretation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/dont-nest-your-curlies.md#2025-04-09_snippet_2\n\nLANGUAGE: jinja\nCODE:\n```\n  {{ dbt_utils.date_spine(\n      datepart=\"day\",\n      start_date=\"{{ var('start_date') }}\"\n      )\n  }}\n```\n\n----------------------------------------\n\nTITLE: Results Logging Usage\nDESCRIPTION: Configuration showing how to use the results logging macro in dbt_project.yml.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/on-run-end-context.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\non-run-end: \"{{ log_results(results) }}\"\n```\n\n----------------------------------------\n\nTITLE: Querying Seeds Data in GraphQL\nDESCRIPTION: Example GraphQL query demonstrating how to fetch information about seeds in a dbt job, including unique ID, name, execution time, and status.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/schema-discovery-job-seeds.mdx#2025-04-09_snippet_0\n\nLANGUAGE: graphql\nCODE:\n```\n{\n  job(id: 123) {\n    seeds {\n      uniqueId\n      name\n      executionTime\n      status\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Sigma Embedded UI Element URL for dbt Cloud Status Tiles\nDESCRIPTION: This code snippet shows the URL format for adding a dbt Cloud status tile as an embedded UI element in a Sigma workbook. It includes placeholders for the exposure name, job ID, and metadata token.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/data-tile.md#2025-04-09_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://metadata.YOUR_ACCESS_URL/exposure-tile?name=<exposure_name>&jobId=<job_id>&token=<metadata_only_token>\n```\n\n----------------------------------------\n\nTITLE: Running dbt Core Pipeline with dlt in Python\nDESCRIPTION: This code snippet demonstrates how to set up and run a dbt Core pipeline using dlt in a Google Cloud Function environment. It initializes a dlt pipeline, creates a virtual environment, packages the dbt project, and executes the models, printing the results for each model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-12-15-serverless-free-tier-data-stack-with-dlt-and-dbt-core.md#2025-04-09_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef dbt_run():\n  # make an authenticated connection with dlt to the dwh\n    pipeline = dlt.pipeline(\n        pipeline_name='dbt_pipeline',\n        destination='bigquery', # credentials read from env\n        dataset_name='dbt'\n    )\n  # make a venv in case we have lib conflicts between dlt and current env\n    venv = dlt.dbt.get_venv(pipeline)\n  # package the pipeline, dbt package and env\n    dbt = dlt.dbt.package(pipeline, \"dbt/property_analytics\", venv=venv)\n  # and run it\n    models = dbt.run_all()\n    # show outcome\n    for m in models:\n        print(f\"Model {m.model_name} materialized in {m.time} with status {m.status} and message {m.message}\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Timestamp Strategy for dbt Snapshots in SQL (dbt v1.8 and earlier)\nDESCRIPTION: SQL/Jinja template for configuring a snapshot with the timestamp strategy. The config block specifies strategy and the updated_at column for change detection.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/strategy.md#2025-04-09_snippet_1\n\nLANGUAGE: jinja2\nCODE:\n```\n{% snapshot [snapshot_name](snapshot_name) %}\n\n{{ config(\n  strategy=\"timestamp\",\n  updated_at=\"column_name\"\n) }}\n\nselect ...\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Insecure Connection for Impala in dbt\nDESCRIPTION: Basic configuration for connecting to Impala without authentication. This method is only recommended for testing with a local Impala installation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/impala-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: impala\n      host: [host] # default value: localhost\n      port: [port] # default value: 21050\n      dbname: [db name]  # this should be same as schema name provided below, starting with 1.1.2 this parameter is optional\n      schema: [schema name]\n      \n```\n\n----------------------------------------\n\nTITLE: Configuring Materialized View in Databricks\nDESCRIPTION: Shows how to configure a model as a materialized view in Databricks. Materialized views are an alternative to incremental tables powered by Delta Live Tables and require Unity Catalog and serverless SQL Warehouses.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/databricks-configs.md#2025-04-09_snippet_19\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n   materialized = 'materialized_view'\n ) }}\n```\n\n----------------------------------------\n\nTITLE: Command Comparison Table in Markdown\nDESCRIPTION: A markdown table comparing the basic characteristics of defer and clone commands in dbt, showing their usage patterns and outputs.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-10-31-to-defer-or-to-clone.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|                           | defer                                                                                                                                                              | clone                                                                                                                                    |\n|---------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| **How do I use it?**      | Implicit via the `--defer` flag                                                                                                                                    | Explicit via the `dbt clone` command                                                                                                     |\n| **What are its outputs?** | Doesn't create any objects itself, but dbt might create objects in the target schema if they've changed from those in the source schema.                           | Copies objects from source schema to target schema in the data warehouse, which are persisted after operation is finished.               |\n| **How does it work?**     | Compares manifests between source and target dbt runs and overrides ref to resolve models not built in the target run to point to objects built in the source run. | Uses zero-copy cloning if available to copy objects from source to target schemas, else creates pointer views (`select * from my_model`) |\n```\n\n----------------------------------------\n\nTITLE: Embedding Data Health Tile URL in Sigma\nDESCRIPTION: URL format for embedding a dbt Explorer data health tile in Sigma using an embedded UI element. The URL includes parameters for the exposure identifier, environment type, environment ID, and authentication token.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/data-tile.md#2025-04-09_snippet_3\n\nLANGUAGE: html\nCODE:\n```\nhttps://metadata.ACCESS_URL/exposure-tile?uniqueId=exposure.EXPOSURE_NAME&environmentType=production&environmentId=ENV_ID_NUMBER&token=<YOUR_METADATA_TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Markdown Frontmatter Configuration for Community Profile\nDESCRIPTION: YAML frontmatter configuration for Josh Devlin's community profile page, including metadata like social links, job details, and community award information.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/community/spotlight/josh-devlin.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nid: josh-devlin\ntitle: Josh Devlin\ndescription: |\n  Josh Devlin has a rich history of community involvement and technical expertise in both the dbt and wider analytics communities.\n\n  Discovering dbt in early 2020, he quickly became an integral member of its <a href=\"https://www.getdbt.com/community/join-the-community\" rel=\"noopener noreferrer\" target=\"_blank\">community</a>, leveraging the platform as a learning tool and aiding others along their dbt journey. Josh has helped thousands of dbt users with his advice and near-encyclopaedic knowledge of dbt.\n\n  Beyond the online community, he transitioned from being an attendee at the first virtual Coalesce conference in December 2020 to a <a href=\"https://coalesce.getdbt.com/blog/babies-and-bathwater-is-kimball-still-relevant\" title=\"at the first in-person Coalesce\" rel=\"noopener noreferrer\" target=\"_blank\">presenter at the first in-person Coalesce event</a> in New Orleans in 2022.  He has also contributed to the dbt-core and dbt-snowflake codebases, helping improve the product in the most direct way.\n\n  His continuous contributions echo his philosophy of learning through teaching, a principle that has not only enriched the dbt community but also significantly bolstered his proficiency with the tool, making him a valuable community member.\n\n  Aside from his technical endeavors, Josh carries a heart for communal growth and an individual's ability to contribute to a larger whole, a trait mirrored in his earlier pursuits as an orchestral musician. His story is a blend of technical acumen, communal involvement, and a nuanced appreciation for the symbiotic relationship between teaching and learning, making him a notable figure in the analytics engineering space.\nimage: /img/community/spotlight/josh-devlin.jpg\npronouns: he/him\nlocation: Melbourne, Australia (but spent most of the last decade in Houston, USA)\njobTitle: Senior Analytics Engineer\ncompanyName: Canva\nsocialLinks:\n  - name: Twitter\n    link: https://twitter.com/JayPeeDevlin\n  - name: LinkedIn\n    link: https://www.linkedin.com/in/josh-devlin/\ndateCreated: 2023-11-10\nhide_table_of_contents: true\ncommunityAward: true\ncommunityAwardYear: 2023\n---\n```\n\n----------------------------------------\n\nTITLE: Basic SQL Filtering Example\nDESCRIPTION: Simple example of filtering out employee orders from an orders table using a direct comparison operator.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/operators/sql-in.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect * from {{ source('backend_db', 'orders') }}\nwhere status != 'employee_order'\n```\n\n----------------------------------------\n\nTITLE: Specifying Custom Config Keys for Generic Data Tests in dbt YAML\nDESCRIPTION: This example demonstrates how to use custom configuration keys (like 'snowflake_warehouse') for generic data tests in dbt v1.9+. It allows specifying both standard configs like severity and custom configs that can control test execution environments.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/data-test-configs.md#2025-04-09_snippet_10\n\nLANGUAGE: yml\nCODE:\n```\nmodels:\n  - name: my_model\n    columns:\n      - name: color\n        tests:\n          - accepted_values:\n              values: ['blue', 'red']\n              config:\n                severity: warn\n                snowflake_warehouse: my_warehouse\n```\n\n----------------------------------------\n\nTITLE: Correct Jinja Variable Usage\nDESCRIPTION: Demonstrates the proper way to use variables within a Jinja expression using the var() context method.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/dont-nest-your-curlies.md#2025-04-09_snippet_1\n\nLANGUAGE: jinja\nCODE:\n```\n  {{ dbt_utils.date_spine(\n      datepart=\"day\",\n      start_date=var('start_date')\n      )\n  }}\n```\n\n----------------------------------------\n\nTITLE: Configuring MindsDB Profile in YAML for dbt\nDESCRIPTION: A basic profile.yml configuration for connecting dbt to MindsDB. It specifies connection details such as host, port, database, schema, username, and password.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/mindsdb-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmindsdb:\n  outputs:\n    dev:\n      database: 'mindsdb'\n      host: '127.0.0.1'\n      password: ''\n      port: 47335\n      schema: 'mindsdb'\n      type: mindsdb\n      username: 'mindsdb'\n  target: dev\n```\n\n----------------------------------------\n\nTITLE: Getting Relation Example\nDESCRIPTION: Shows how to fetch a relation object using database, schema, and identifier\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/adapter.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{%- set source_relation = adapter.get_relation(\n      database=\"analytics\",\n      schema=\"dbt_drew\",\n      identifier=\"orders\") -%}\n\n{{ log(\"Source Relation: \" ~ source_relation, info=true) }}\n```\n\n----------------------------------------\n\nTITLE: SQL Query with Quoted Column Example\nDESCRIPTION: Example of SQL query using quoted column names to handle reserved words.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/columns.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nselect user_group as \"group\"\n```\n\n----------------------------------------\n\nTITLE: Response Schema for Creating a New Webhook Subscription\nDESCRIPTION: This JSON response shows the structure of the data returned when creating a new webhook subscription. It includes details of the newly created webhook, such as its ID, name, and configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/webhooks.md#2025-04-09_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"data\": {\n        \"id\": \"wsu_12345abcde\",\n        \"account_identifier\": \"act_12345abcde\",\n        \"name\": \"Webhook for jobs\",\n        \"description\": \"A webhook for when jobs are started\",\n        \"job_ids\": [\n            \"123\",\n\t\t\t\t\t\t\"321\"\n        ],\n        \"event_types\": [\n            \"job.run.started\"\n        ],\n        \"client_url\": \"https://test.com\",\n        \"hmac_secret\": \"12345abcde\",\n        \"active\": true,\n        \"created_at\": \"1675795644808877\",\n        \"updated_at\": \"1675795644808877\",\n        \"account_id\": \"123\",\n        \"http_status_code\": \"0\"\n    },\n    \"status\": {\n        \"code\": 201\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Materialized Views in Postgres/Redshift/Databricks/Bigquery\nDESCRIPTION: SQL configuration for implementing materialized views in dbt for Postgres, Redshift, Databricks, and Bigquery adapters. This snippet demonstrates how to specify the 'materialized_view' materialization type in a dbt model configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-08-01-announcing-materialized-views.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{{\nconfig(\n    materialized = 'materialized_view',\n)\n}}\n```\n\n----------------------------------------\n\nTITLE: Configuring IBM watsonx.data Spark Profile in YAML\nDESCRIPTION: Example YAML configuration for connecting dbt to IBM watsonx.data Spark. This profile should be added to the profiles.yml file in the .dbt/ directory of the user's home folder.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/watsonx-spark-setup.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nproject_name:\n  target: \"dev\"\n  outputs:\n    dev:\n      type: watsonx_spark\n      method: http\n      schema: [schema name]\n      host: [hostname]\n      uri: [uri]\n      catalog: [catalog name]\n      use_ssl: false\n      auth:\n        instance: [Watsonx.data Instance ID]\n        user: [username]\n        apikey: [apikey]\n```\n\n----------------------------------------\n\nTITLE: Incorrect snapshot-paths configuration using absolute path\nDESCRIPTION: Shows an example of how not to configure snapshot-paths using an absolute path. This approach is discouraged as it's less portable and can cause issues when moving the project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/snapshot-paths.md#2025-04-09_snippet_2\n\nLANGUAGE: yml\nCODE:\n```\nsnapshot-paths: [\"/Users/username/project/snapshots\"]\n```\n\n----------------------------------------\n\nTITLE: Effects Comparison Table in Markdown\nDESCRIPTION: A markdown table showing the second-order effects that distinguish clone and defer commands from each other.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-10-31-to-defer-or-to-clone.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n|                                                                          | defer                                                                                                                              | clone                                                                                  |\n|--------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------|\n| **Where can I use objects built in the target schema?**                  | Only within the context of dbt                                                                                                     | Any downstream tool (e.g. BI)                                                          |\n| **Can I safely modify objects built in the target schema?**              | No, since this would modify production data                                                                                        | Yes, cloning is a cheap way to create a sandbox of production data for experimentation |\n| **Will data in the target schema drift from data in the source schema?** | No, since deferral will always point to the latest version of the source schema                                                    | Yes, since clone is a point-in-time operation                                          |\n| **Can I use multiple source schemas at once?**                           | Yes, defer can dynamically switch between source schemas e.g. ref unchanged models from production and changed models from staging | No, clone copies objects from one source schema to one target schema                   |\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM Classification with Snowflake Cortex in dbt SQL\nDESCRIPTION: This snippet demonstrates how to use Snowflake's Cortex LLM function to classify text into predefined product segments. It includes prompt engineering techniques, handling non-deterministic responses, and using Jinja variables to maintain consistency between the prompt and result processing.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-02-29-cortex-slack.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n-- a cut down list of segments for the sake of readability\n{% set segments = ['Warehouse configuration', 'dbt Cloud IDE', 'dbt Core', 'SQL', 'dbt Orchestration', 'dbt Explorer', 'Unknown'] %}\n\n    select trim(\n        snowflake.cortex.complete(\n            'llama2-70b-chat',\n            concat(\n                'Identify the dbt product segment that this message relates to, out of [{{ segments | join (\"|') }}]. Your response should be only the segment with no explanation. <message>',\n                text,\n                '</message>'\n            )\n        )\n    ) as product_segment_raw,\n\n    -- reusing the segments Jinja variable here\n    coalesce(regexp_substr(product_segment_raw, '{{ segments | join (\"|') }}'), 'Unknown') as product_segment\n```\n\n----------------------------------------\n\nTITLE: Configuring Web Crawler Access with Robots.txt\nDESCRIPTION: Basic robots.txt configuration that specifies crawler permissions. It uses a wildcard user-agent to apply rules to all web crawlers and specifically disallows access to the /learn directory.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/static/robots.txt#2025-04-09_snippet_0\n\nLANGUAGE: robots.txt\nCODE:\n```\nUser-agent: *\nDisallow: /learn\n```\n\n----------------------------------------\n\nTITLE: Converting Order and Customer IDs to Strings\nDESCRIPTION: Example showing how to cast numeric order_id and customer_id fields to strings in a SELECT statement using the Jaffle Shop's orders model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/other/sql-cast.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect \n\tcast(order_id as string) as order_id,\n\tcast(customer_id as string) as customer_id,\n\torder_date,\n\tstatus\nfrom {{ ref('orders') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Microsoft Entra ID Interactive Authentication for Azure SQL in dbt (Windows Only)\nDESCRIPTION: This Windows-specific configuration enables interactive login with Microsoft Entra ID, which can include Multi-Factor Authentication prompts. It requires specifying the user email in the profiles.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/mssql-setup.md#2025-04-09_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: sqlserver\n      driver: 'ODBC Driver 18 for SQL Server' # (The ODBC Driver installed on your system)\n      server: hostname or IP of your server\n      port: 1433\n      database: exampledb\n      schema: schema_name\n      authentication: ActiveDirectoryInteractive\n      user: bill.gates@microsoft.com\n```\n\n----------------------------------------\n\nTITLE: Hook Configuration Exception\nDESCRIPTION: Demonstrates the special case where nested curly braces are allowed in dbt hooks for post-execution configurations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/dont-nest-your-curlies.md#2025-04-09_snippet_4\n\nLANGUAGE: jinja\nCODE:\n```\n{{ config(post_hook=\"grant select on {{ this }} to role bi_role\") }}\n```\n\n----------------------------------------\n\nTITLE: Processing dbt Cloud Run Logs with Python in Zapier\nDESCRIPTION: Python script that retrieves and processes dbt Cloud run logs using the Admin API. Extracts error messages from run steps and formats them for posting in Slack threads. Includes error message filtering and formatting logic.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/zapier-slack.md#2025-04-09_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport re\n\n# Access secret credentials\nsecret_store = StoreClient('YOUR_SECRET_HERE')\napi_token = secret_store.get('DBT_CLOUD_SERVICE_TOKEN')\n\n# Steps derived from these commands won't have their error details shown inline, as they're messy\ncommands_to_skip_logs = ['dbt source', 'dbt docs']\nrun_id = input_data['run_id']\naccount_id = input_data['account_id']\nurl = f'https://YOUR_ACCESS_URL/api/v2/accounts/{account_id}/runs/{run_id}/?include_related=[\"run_steps\"]'\nheaders = {'Authorization': f'Token {api_token}'}\n\nresponse = requests.get(url, headers=headers)\nresponse.raise_for_status()\nresults = response.json()['data']\n\nthreaded_errors_post = \"\"\nfor step in results['run_steps']:\n  show_logs = not any(cmd in step['name'] for cmd in commands_to_skip_logs)\n  if not show_logs:\n    continue\n  if step['status_humanized'] != 'Success':\n    full_log = step['logs']\n    # Remove timestamp and any colour tags\n    full_log = re.sub('\\x1b?\\[[0-9]+m[0-9:]*', '', full_log)\n    \n    summary_start = re.search('(?:Completed with \\d+ error.* and \\d+ warnings?:|Database Error|Compilation Error|Runtime Error)', full_log)\n    \n    line_items = re.findall('(^.*(?:Failure|Error) in .*\\n.*\\n.*)', full_log, re.MULTILINE)\n    if not summary_start:\n      continue\n      \n    threaded_errors_post += f\"\"\"\n*{step['name']}*\n\"\"\"    \n    # If there are no line items, the failure wasn't related to dbt nodes, and we want the whole rest of the message. \n    # If there are, then we just want the summary line and then to log out each individual node's error.\n    if len(line_items) == 0:\n      relevant_log = f'```{full_log[summary_start.start():]}```'\n    else:\n      relevant_log = summary_start[0]\n      for item in line_items:\n        relevant_log += f'\\n```\\n{item.strip()}\\n```\\n'\n    threaded_errors_post += f\"\"\"\n{relevant_log}\n\"\"\"\n\noutput = {'threaded_errors_post': threaded_errors_post}\n```\n\n----------------------------------------\n\nTITLE: Documenting dbt Adapters in Markdown\nDESCRIPTION: Example of required markdown header format for adapter documentation\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_33\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: \"Documenting a new adapter\"\nid: \"documenting-a-new-adapter\"\n---\n```\n\n----------------------------------------\n\nTITLE: Example SQL Output with Appended Default Comment\nDESCRIPTION: SQL query showing how the default comment appears when appended to the end of a query.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/query-comment.md#2025-04-09_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\nselect ...\n/* {\"app\": \"dbt\", \"dbt_version\": \"1.6.0rc2\", \"profile_name\": \"debug\", \"target_name\": \"dev\", \"node_id\": \"model.dbt2.my_model\"} */\n;\n```\n\n----------------------------------------\n\nTITLE: Configuring GitHub PR Template URL in dbt Cloud\nDESCRIPTION: This snippet provides the PR template URL format for GitHub repositories in dbt Cloud. It includes variables for the organization and repository names.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/git/pr-template.md#2025-04-09_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://github.com/<org>/<repo>/compare/{{destination}}..{{source}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Dynamic Tables in Snowflake\nDESCRIPTION: SQL configuration for implementing dynamic tables in Snowflake through dbt. Snowflake uses dynamic tables instead of materialized views as they provide more functionality like joining, union, and aggregation capabilities across various table types.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-08-01-announcing-materialized-views.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{\nconfig(\n    materialized = 'dynamic_table',\n)\n}}\n```\n\n----------------------------------------\n\nTITLE: Incorrect Configuration of docs-paths with Absolute Path\nDESCRIPTION: This snippet illustrates an incorrect way to configure docs-paths using an absolute path. It's recommended to avoid using absolute paths in the configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/docs-paths.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ndocs-paths: [\"/Users/username/project/docs\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Public Models in dbt_project.yml\nDESCRIPTION: Example of setting the 'marts' directory as public in the dbt_project.yml file to allow consumption by downstream tools.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/hybrid-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  define_public_models: # This is my project name, remember it must be specified\n    marts:\n      +access: public\n```\n\n----------------------------------------\n\nTITLE: Source-based Selection\nDESCRIPTION: Selecting models that use specific sources\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/methods.md#2025-04-09_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --select \"source:snowplow+\"    # run all models that select from Snowplow sources\n```\n\n----------------------------------------\n\nTITLE: Configuring Materialized View in Redshift with dbt\nDESCRIPTION: This configuration block sets up a materialized view in Redshift using dbt. It disables auto-refresh and specifies the behavior on configuration changes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-08-01-announcing-materialized-views.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{{\\nconfig(\\n    materialized = 'materialized_view',\\n    on_configuration_change = 'apply',\\n    auto_refresh = False\\n)\\n}}\n```\n\n----------------------------------------\n\nTITLE: Listing Saved Queries\nDESCRIPTION: GraphQL query to retrieve saved queries with their parameters and configurations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-graphql.md#2025-04-09_snippet_9\n\nLANGUAGE: graphql\nCODE:\n```\n{\n  savedQueries(environmentId: 200532) {\n    name\n    description\n    label\n    queryParams {\n      metrics {\n        name\n      }\n      groupBy {\n        name\n        grain\n        datePart\n      }\n      where {\n        whereSqlTemplate\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Rendering Project Access Table for Account Permissions in Markdown\nDESCRIPTION: This code snippet defines a sortable table in Markdown format that displays project-level permissions for different account roles in dbt Cloud. It includes permissions for various project management features.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_enterprise-permissions-table.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n<SortableTable>\n\n{`\n|Project-level permission | Account Admin | Billing admin | Project creator | Security admin | Viewer | \n|:-------------------------|:-------------:|:-------------:|:---------------:|:--------------:|:------:| \n| Environment credentials |       W       |      -        |       W         |       -        |   R    |\n| Custom env. variables   |       W       |      -        |       W         |       -        |   R    |\n| Data platform configurations|   W       |      -        |       W         |       -        |   R    |\n| Develop (IDE or CLI)       | W       |      -        |       W         |       -        |   -    |\n| Environments            |       W       |      -        |       W         |       -        |   R    |\n| Jobs                    |       W       |      -        |       W         |       -        |   R    |\n| Metadata GraphQL API access |   R       |      -        |       R         |       -        |   R    |\n| Permissions             |       W       |      -        |       W         |       W        |   R    |\n| Projects                |       W       |      -        |       W         |       R        |   R    |\n| Repositories            |       W       |      -        |       W         |       -        |   R    |\n| Runs                    |       W       |      -        |       W         |       -        |   R    |\n| Semantic Layer config   |       W       |      -        |       W         |       v        |   R    |\n`}\n\n</SortableTable>\n```\n\n----------------------------------------\n\nTITLE: Installing ODBC Header Files on Debian/Ubuntu\nDESCRIPTION: Command to install ODBC header files on Debian/Ubuntu systems, which is a prerequisite for using dbt with SQL Server.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/mssql-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install unixodbc-dev\n```\n\n----------------------------------------\n\nTITLE: Snowflake Table Creation and Replacement in dbt\nDESCRIPTION: Shows how dbt transforms a simple SELECT statement into Snowflake-specific SQL for creating and replacing tables. Includes schema creation if it doesn't exist.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Models/sql-dialect.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\ncreate schema if not exists analytics.dbt_alice;\n\ncreate or replace table analytics.dbt_alice.test_model as (\n    select 1 as my_column\n);\n```\n\n----------------------------------------\n\nTITLE: Configuring pytest for dbt Adapter Testing in Python\nDESCRIPTION: This pytest configuration file sets up the environment for running dbt adapter tests. It includes filter warnings, environment file specification, and test path definition.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n[pytest]\nfilterwarnings =\n    ignore:.*'soft_unicode' has been renamed to 'soft_str'*:DeprecationWarning\n    ignore:unclosed file .*:ResourceWarning\nenv_files =\n    test.env  # uses pytest-dotenv plugin\n              # this allows you to store env vars for database connection in a file named test.env\n              # rather than passing them in every CLI command, or setting in `PYTEST_ADDOPTS`\n              # be sure to add \"test.env\" to .gitignore as well!\ntestpaths =\n    tests/functional  # name per convention\n```\n\n----------------------------------------\n\nTITLE: Configuring Data Tests with Different Storage Options in YAML\nDESCRIPTION: YAML configuration for dbt tests on a model column, showing how to set different storage options for test failures using the store_failures_as config. One test stores failures as a view, the other as ephemeral.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/artifacts/run-results-json.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: my_model\n    columns:\n      - name: created_at\n        tests:\n          - not_null:\n              config:\n                store_failures_as: view\n          - unique:\n              config:\n                store_failures_as: ephemeral\n```\n\n----------------------------------------\n\nTITLE: Example Output of Container Status Check\nDESCRIPTION: Example output from the container status check command, showing that all containers are properly exited.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/airflow-and-dbt-cloud.md#2025-04-09_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nName                                    State   Ports\nairflow-dbt-cloud_e3fe3c-webserver-1    exited\nairflow-dbt-cloud_e3fe3c-scheduler-1    exited\nairflow-dbt-cloud_e3fe3c-postgres-1     exited\n```\n\n----------------------------------------\n\nTITLE: Segmenting Customers with SQL CASE Statement\nDESCRIPTION: This final SQL snippet segments customers into different categories based on their recency and frequency scores. It uses a CASE statement to map combinations of scores to specific customer segments like 'Hibernating', 'At Risk', 'Champions', etc.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-08-building-a-historical-user-segmentation-model-with-dbt.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nrfm_segment AS(\nSELECT *,\n        CASE\n            WHEN recency_score <= 2\n                AND frequency_score <= 2 THEN 'Hibernating'\n            WHEN recency_score <= 2\n                AND frequency_score <= 4 THEN 'At Risk'\n            WHEN recency_score <= 2\n                AND frequency_score <= 5 THEN 'Cannot Lose Them'\n            WHEN recency_score <= 3\n                AND frequency_score <= 2 THEN 'About to Sleep'\n            WHEN recency_score <= 3\n                AND frequency_score <= 3 THEN 'Need Attention'\n            WHEN recency_score <= 4\n                AND frequency_score <= 1 THEN 'Promising'\n            WHEN recency_score <= 4\n                AND frequency_score <= 3 THEN 'Potential Loyalists'\n            WHEN recency_score <= 4\n                AND frequency_score <= 5 THEN 'Loyal Customers'\n            WHEN recency_score <= 5\n                AND frequency_score <= 1 THEN 'New Customers'\n            WHEN recency_score <= 5\n                AND frequency_score <= 3 THEN 'Potential Loyalists'\n            ELSE 'Champions'\n        END AS rfm_segment\nFROM  rfm_scores\n)\nSELECT *\nFROM rfm_segment\n```\n\n----------------------------------------\n\nTITLE: Checking dbt Version in CLI\nDESCRIPTION: Command to check the installed version of dbt Core in the command line interface.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/hybrid-setup.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndbt --version\n```\n\n----------------------------------------\n\nTITLE: Excluding Specific Seed in dbt Seed Command\nDESCRIPTION: This example shows how to use the --exclude flag with the dbt seed command to load all seeds except a specific one named 'account_parent_mappings'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/exclude.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndbt seed --exclude \"account_parent_mappings\"\n```\n\n----------------------------------------\n\nTITLE: Generating dbt YAML for Staff Member Model\nDESCRIPTION: This YAML snippet defines a dbt model for the staff member table. It includes column descriptions and generic tests such as unique, not_null, and accepted_values for various fields.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-07-17-GPT-and-dbt-test.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: STAFF_MEMBER\n    description: This table contains information about the staff members.\n    columns:\n      - name: ID\n        description: The unique identifier for the staff member.\n        tests:\n          - unique\n          - not_null\n      - name: CREATEDATETIME\n        description: The timestamp when the record was created.\n        tests:\n          - not_null\n      - name: UPDATEDATETIME\n        description: The timestamp when the record was last updated.\n      - name: VERSION\n        description: Version number of the record.\n      - name: FIRSTNAME\n        description: The first name of the staff member.\n        tests:\n          - not_null\n      - name: JOBTITLE\n        description: The job title of the staff member.\n        tests:\n          - not_null\n      - name: LASTNAME\n        description: The last name of the staff member.\n        tests:\n          - not_null\n      - name: MIDDLENAME\n        description: The middle name of the staff member.\n      - name: ISCARADMIN\n        description: Boolean value indicating if the staff member is a care administrator.\n        tests:\n          - accepted_values:\n              values: ['true', 'false']\n      - name: ISARCHIVED\n        description: Boolean value indicating if the staff member record is archived.\n        tests:\n          - accepted_values:\n              values: ['true', 'false']\n      - name: COMMUNITYID\n        description: Identifier for the community of the staff member.\n      - name: ENTERPRISEID\n        description: Identifier for the enterprise of the staff member.\n      - name: ISDELETED\n        description: Boolean value indicating if the staff member record is deleted.\n        tests:\n          - accepted_values:\n              values: ['true', 'false']\n```\n\n----------------------------------------\n\nTITLE: SQL Implementation of Activated Accounts Metric\nDESCRIPTION: SQL query that calculates activated accounts by counting accounts with more than five data model runs. Demonstrates the manual SQL approach before using metrics as dimensions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/ref-metrics-in-filters.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nwith data_models_per_user as (\n    select\n        account_id as account,\n        count(model_runs) as data_model_runs\n    from \n        {{ ref('fct_model_runs') }}\n    group by \n        account_id\n),\n\nactivated_accounts as (\n    select\n        count(distinct account_id) as activated_accounts\n    from \n        {{ ref('dim_accounts') }}\n    left join \n        data_models_per_user \n    on \n        {{ ref('dim_accounts') }}.account_id = data_models_per_user.account\n    where \n        data_models_per_user.data_model_runs > 5\n)\n\nselect\n    *\nfrom \n    activated_accounts\n```\n\n----------------------------------------\n\nTITLE: Defining Entities in a Semantic Model\nDESCRIPTION: Configuration for defining entities in a semantic model, including primary and foreign keys that will be used for joining with other models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_sl-create-semanticmodel.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n  #Entities. These usually correspond to keys in the table.\n    entities:\n      - name: order_id\n        type: primary\n      - name: location\n        type: foreign\n        expr: location_id\n      - name: customer\n        type: foreign\n        expr: customer_id\n```\n\n----------------------------------------\n\nTITLE: Creating a Daily Time Spine for BigQuery in SQL\nDESCRIPTION: Creates a time spine table at day granularity using dbt's date_spine macro specifically for BigQuery. This implementation uses BigQuery's DATE() function instead of TO_DATE() and includes filters to limit the date range.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metricflow-time-spine.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{{config(materialized='table')}}\nwith days as (\n    {{dbt.date_spine(\n        'day',\n        \"DATE(2000,01,01)\",\n        \"DATE(2025,01,01)\"\n    )\n    }}\n),\n\nfinal as (\n    select cast(date_day as date) as date_day\n    from days\n)\n\nselect *\nfrom final\n-- filter the time spine to a specific range\nwhere date_day > date_add(DATE(current_timestamp()), INTERVAL -4 YEAR)\nand date_day < date_add(DATE(current_timestamp()), INTERVAL 30 DAY)\n```\n\n----------------------------------------\n\nTITLE: Configuring Tags for Saved Queries in YAML\nDESCRIPTION: Example showing how to add tags to saved queries in the dbt_project.yml file to categorize and filter resources. The example adds the 'order_metrics' tag to the customer_order_metrics saved query.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/2024-release-notes.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsaved-queries:\n  jaffle_shop:\n    customer_order_metrics:\n      +tags: order_metrics\n```\n\n----------------------------------------\n\nTITLE: Configuring Seed Properties in YAML\nDESCRIPTION: Demonstrates how to configure seed properties like quote_columns, column_types, and delimiter in a properties.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/seed-configs.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nseeds:\n  - name: [<seed-name>]\n    config:\n      quote_columns: true | false\n      column_types: {column_name: datatype}\n      delimiter: <string>\n```\n\n----------------------------------------\n\nTITLE: Implementing Exception Handler for dbt Adapter in Python\nDESCRIPTION: Defines the 'exception_handler' method for a ConnectionManager, which provides a context manager for handling database-specific exceptions and converting them to dbt-compatible exceptions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n    @contextmanager\n    def exception_handler(self, sql: str):\n        try:\n            yield\n        except myadapter_library.DatabaseError as exc:\n            self.release(connection_name)\n\n            logger.debug('myadapter error: {}'.format(str(e)))\n            raise dbt.exceptions.DatabaseException(str(exc))\n        except Exception as exc:\n            logger.debug(\"Error running SQL: {}\".format(sql))\n            logger.debug(\"Rolling back transaction.\")\n            self.release(connection_name)\n            raise dbt.exceptions.RuntimeException(str(exc))\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating Virtual Environment for dbt Core Installation\nDESCRIPTION: This snippet demonstrates how to create a virtual environment for dbt and activate it on different operating systems. It isolates the dbt installation from other Python projects.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Core/install-pip-best-practices.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython3 -m venv dbt-env\t\t\t\t# create the environment\nsource dbt-env/bin/activate\t\t\t# activate the environment for Mac and Linux\ndbt-env\\Scripts\\activate\t\t\t# activate the environment for Windows\n```\n\n----------------------------------------\n\nTITLE: Issue Labels Table in Markdown\nDESCRIPTION: A markdown table explaining the most common issue labels used in dbt-labs repositories and their descriptions. The table provides details on labels such as triage, bug, enhancement, and others to help contributors understand how issues are categorized.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/community/resources/oss-expectations.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| tag                | description                                                                                                                                                                                                                                                            |\n| ------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `triage`           | This is a new issue which has not yet been reviewed by a maintainer. This label is removed when a maintainer reviews and responds to the issue.                                                                                                                        |\n| `bug`              | This issue represents a defect or regression from the behavior that's documented                                                                                                                                                        |\n| `enhancement`      | This issue represents a narrow extension of an existing capability                                                                                                                                                                          |\n| `good_first_issue` | This issue does not require deep knowledge of the codebase to implement, and it is appropriate for a first-time contributor.                                                                                                                                       |\n| `help_wanted`      | This issue is trickier than a \"good first issue.\" The required changes are scattered across the codebase, or more difficult to test. The maintainers are happy to help an experienced community contributor; they aren't planning to prioritize this issue themselves. |\n| `duplicate`        | This issue is functionally identical to another open issue. The maintainers will close this issue and encourage community members to focus conversation on the other one.                                                                                              |\n| `stale`            | This is an old issue which has not recently been updated. In repositories with a lot of activity, stale issues will periodically be closed.                                                                                                                            |\n| `wontfix`          | This issue does not require a code change in the repository, or the maintainers are unwilling to merge a change which implements the proposed behavior.                                                                                                                |\n```\n\n----------------------------------------\n\nTITLE: Registering Event Callbacks with dbtRunner\nDESCRIPTION: Shows how to register callback functions on dbt's EventManager to access structured events and implement custom logging, with an example that prints the dbt version number when it's reported.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/programmatic-invocations.md#2025-04-09_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dbt.cli.main import dbtRunner\nfrom dbt_common.events.base_types import EventMsg\n\ndef print_version_callback(event: EventMsg):\n    if event.info.name == \"MainReportVersion\":\n        print(f\"We are thrilled to be running dbt{event.data.version}\")\n\ndbt = dbtRunner(callbacks=[print_version_callback])\ndbt.invoke([\"list\"])\n```\n\n----------------------------------------\n\nTITLE: Creating Docs Blocks in Markdown\nDESCRIPTION: Demonstrates how to create docs blocks in a markdown file. Each block defines documentation for a specific column that can be referenced in YAML files.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-04-generating-dynamic-docs.md#2025-04-09_snippet_7\n\nLANGUAGE: markdown\nCODE:\n```\n{% docs activity_based_interest__id %}  \n\nPrimary key of the table. See sql for key definition.\n\n{% enddocs %}\n\n{% docs activity_based_interest__user_id %}  \n\nThe internal company id for a given user.\n\n{% enddocs %}\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Selector for Excluding Views in dbt Cloud YAML\nDESCRIPTION: This YAML snippet defines a custom selector that excludes materializing views without skipping tests on views. It's used to optimize job runs by reducing unnecessary view rebuilds while maintaining data quality checks.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/billing.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nselectors:\n  - name: skip_views_but_test_views\n    description: >\n      A default selector that will exclude materializing views\n      without skipping tests on views.\n    default: true\n    definition:\n      union:\n        - union: \n          - method: path\n            value: \"*\"\n          - exclude: \n            - method: config.materialized\n              value: view\n        - method: resource_type\n          value: test\n```\n\n----------------------------------------\n\nTITLE: Querying Mileage for a Specific Component\nDESCRIPTION: This SQL query calculates the total mileage accumulated on a specific component (in this case, 'Wheel-1') using the multivalued dimension table (mdim_components). It demonstrates how to handle the more complex relationship between daily mileage and components.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-02-modeling-ragged-time-varying-hierarchies.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    mdim_components.component_id,\n    sum(fct_daily_mileage.miles) as miles\nfrom\n    fct_daily_mileage\ninner join\n    mdim_components\n    on\n        fct_daily_mileage.component_sk = mdim_components.component_sk\ngroup by\n    1\nwhere\n    component_id = 'Wheel-1'\n```\n\n----------------------------------------\n\nTITLE: Querying a Saved Query\nDESCRIPTION: Example of querying a saved query with a limit parameter. This uses a predefined query configuration and applies a limit of 5 records to the results.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-jdbc.md#2025-04-09_snippet_29\n\nLANGUAGE: sql\nCODE:\n```\nselect * from {{ semantic_layer.query(saved_query=\"new_customer_orders\", limit=5}}\n```\n\n----------------------------------------\n\nTITLE: Assigning RFM Scores with SQL CASE Statements\nDESCRIPTION: This SQL snippet assigns scores from 1 to 5 for each of the RFM metrics based on their percentile values. It uses CASE statements to group percentiles into five categories for each metric.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-08-building-a-historical-user-segmentation-model-with-dbt.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nrfm_scores AS(\n    SELECT  *,\n            CASE\n                WHEN recency_percentile >= 0.8 THEN 5\n                WHEN recency_percentile >= 0.6 THEN 4\n                WHEN recency_percentile >= 0.4 THEN 3\n                WHEN recency_percentile >= 0.2 THEN 2\n                ELSE 1\n                END AS recency_score,\n            CASE\n                WHEN frequency_percentile >= 0.8 THEN 5\n                WHEN frequency_percentile >= 0.6 THEN 4\n                WHEN frequency_percentile >= 0.4 THEN 3\n                WHEN frequency_percentile >= 0.2 THEN 2\n                ELSE 1\n                END AS frequency_score,\n            CASE\n                WHEN monetary_percentile >= 0.8 THEN 5\n                WHEN monetary_percentile >= 0.6 THEN 4\n                WHEN monetary_percentile >= 0.4 THEN 3\n                WHEN monetary_percentile >= 0.2 THEN 2\n                ELSE 1\n                END AS monetary_score\n    FROM rfm_percentiles\n),\n```\n\n----------------------------------------\n\nTITLE: Specifying Compute Resources in dbt Project Configuration\nDESCRIPTION: Demonstrates setting compute resources at the project level in dbt_project.yml. This allows specifying compute resources for all models and overriding for specific directories.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/databricks-configs.md#2025-04-09_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\n...\n\nmodels:\n  +databricks_compute: \"Compute1\"     # use the `Compute1` warehouse/cluster for all models in the project...\n  my_project:\n    clickstream:\n      +databricks_compute: \"Compute2\" # ...except for the models in the `clickstream` folder, which will use `Compute2`.\n\nsnapshots:\n  +databricks_compute: \"Compute1\"     # all Snapshot models are configured to use `Compute1`.\n```\n\n----------------------------------------\n\nTITLE: Referencing DBT Configuration Files\nDESCRIPTION: Examples of configuration files and methods that cannot be used to set special properties in DBT.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_config-prop-callout.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndbt_project.yml\n```\n\n----------------------------------------\n\nTITLE: Power Query SelectColumns Example for dbt Semantic Layer\nDESCRIPTION: A Power Query example that demonstrates how to select specific columns from the METRICS.ALL table to generate a valid Semantic Layer query for importing data into Power BI.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud-integrations/semantic-layer/power-bi.md#2025-04-09_snippet_0\n\nLANGUAGE: powerquery\nCODE:\n```\n= Table.SelectColumns(Source{[Item=\"ALL\",Schema=\"METRICS\",Catalog=null]}[Data], {\"Total Profit\", \"Metric Time (Day)\"})\n```\n\n----------------------------------------\n\nTITLE: Using zip Context Method with Lists in Jinja\nDESCRIPTION: Example of using the zip context method to combine elements from two lists into tuples. This demonstrates how to create pairs of corresponding elements from separate lists.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/zip.md#2025-04-09_snippet_0\n\nLANGUAGE: jinja\nCODE:\n```\n{% set my_list_a = [1, 2] %}\n{% set my_list_b = ['alice', 'bob'] %}\n{% set my_zip = zip(my_list_a, my_list_b) | list %}\n{% do log(my_zip) %}  {# [(1, 'alice'), (2, 'bob')] #}\n```\n\n----------------------------------------\n\nTITLE: Defining an Ephemeral dbt Model for Summarizing Model Execution Stats\nDESCRIPTION: The original ephemeral model that performed complex window function calculations on model execution data. This model was causing performance issues due to memory limitations when processing approximately 5 billion rows of data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-08-12-how-we-shaved-90-minutes-off-long-running-model.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{{config(materialized = 'ephemeral')}}\n\nwith model_execution as (\n\n    select * from {{ ref('stg_dbt_run_model_events') }}\n\n),\n\ndiffed as (\n\n    select *,\n\n        row_number() over (\n            partition by project_id, model_id\n            order by dvce_created_tstamp\n        ) = 1 as is_new,\n\n        /*\n            The `mode` window function returns the most common content hash for a\n            given model on a given day. We use this a proxy for the 'production'\n            version of the model, running in deployment. When a different hash\n            is run, it likely reflects that the model is undergoing development.\n        */\n\n        contents != mode(contents) over (\n            partition by project_id, model_id, dvce_created_tstamp::date\n        ) as is_changed\n\n    from model_execution\n\n),\n\nfinal as (\n\n    select\n        invocation_id,\n        max(model_complexity) as model_complexity,\n        max(model_total) as count_models,\n        sum(case when is_new or is_changed then 1 else 0 end) as count_changed,\n        sum(case when skipped = true then 1 else 0 end) as count_skip,\n        sum(case when error is null or error = 'false' then 0 else 1 end) as count_error,\n        sum(case when (error is null or error = 'false') and skipped = false then 1 else 0 end) as count_succeed\n\n    from diffed\n    group by 1\n\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Launching fly.io Application\nDESCRIPTION: Command to initialize and launch the application on fly.io\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/serverless-datadog.md#2025-04-09_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nflyctl launch\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Secrets in fly.io\nDESCRIPTION: Command to set required security tokens and API keys as environment secrets in fly.io\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/serverless-datadog.md#2025-04-09_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nflyctl secrets set DBT_CLOUD_SERVICE_TOKEN=abc123 DBT_CLOUD_AUTH_TOKEN=def456 DD_API_KEY=ghi789 DD_SITE=datadoghq.com\n```\n\n----------------------------------------\n\nTITLE: Creating Corporate Email Column with Conditional Logic\nDESCRIPTION: SQL snippet that creates a corporate_email column using an IFF statement to return null for personal email domains and the email domain for corporate domains.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-02-07-customer-360-view-census-playbook.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\niff(users.email_domain in {{ personal_emails }}, null, users.email_domain)\n\n           as corporate_email\n```\n\n----------------------------------------\n\nTITLE: Command Type Description Table\nDESCRIPTION: Markdown table defining read and write command types and their characteristics for parallel execution\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-commands.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Command type | Description | <div style={{width:'200px'}}>Example</div> |\n|------|-------------|---------|\n| **Write** | These commands perform actions that change data or metadata in your data platform.<br /><br /> Limited to one invocation at any given time, which prevents any potential conflicts, such as overwriting the same table in your data platform at the same time. | `dbt build`<br />`dbt run` |\n```\n\n----------------------------------------\n\nTITLE: Displaying ERP Component Data for eBike Assembly\nDESCRIPTION: This SQL-like table structure shows how component data is stored in an ERP system, including assembly and component IDs, installation and removal dates. It demonstrates the initial assembly of an eBike and its components.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-02-modeling-ragged-time-varying-hierarchies.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n| assembly_id | component_id | installed_at | removed_at |\n| -             | -              | -              | -            |\n|               | Bike-1         | 2023-01-01     |              |\n| Bike-1        | Frame-1        | 2023-01-01     |              |\n| Bike-1        | Wheel-1        | 2023-01-01     |              |\n| Wheel-1       | Rim-1          | 2023-01-01     |              |\n| Wheel-1       | Tire-1         | 2023-01-01     |              |\n| Tire-1        | Tube-1         | 2023-01-01     |              |\n```\n\n----------------------------------------\n\nTITLE: Unit Test Selection in dbt\nDESCRIPTION: Demonstrates how to select and list unit tests in the project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/methods.md#2025-04-09_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\ndbt list --select \"unit_test:*\"                        # list all unit tests \ndbt list --select \"+unit_test:orders_with_zero_items\"  # list your unit test named \"orders_with_zero_items\" and all upstream resources\n```\n\n----------------------------------------\n\nTITLE: Processing API Response and Creating Freshness Graph with Python\nDESCRIPTION: This Python code processes the API response, extracts graph nodes, and constructs a lineage graph with freshness information. It calculates the freshness of models and their dependencies, and creates a NetworkX graph for visualization.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/discovery-use-cases-and-examples.md#2025-04-09_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Extract graph nodes from response\ndef extract_nodes(data):\n    models = []\n    sources = []\n    groups = []\n    for model_edge in data[\"applied\"][\"models\"][\"edges\"]:\n        models.append(model_edge[\"node\"])\n    for source_edge in data[\"applied\"][\"sources\"][\"edges\"]:\n        sources.append(source_edge[\"node\"])\n    for group_edge in data[\"definition\"][\"groups\"][\"edges\"]:\n        groups.append(group_edge[\"node\"])\n    models_df = pd.DataFrame(models)\n    sources_df = pd.DataFrame(sources)\n    groups_df = pd.DataFrame(groups)\n\n    return models_df, sources_df, groups_df\n\n# Construct a lineage graph with freshness info\ndef create_freshness_graph(models_df, sources_df):\n    G = nx.DiGraph()\n    current_time = datetime.now(timezone.utc)\n    for _, model in models_df.iterrows():\n        max_freshness = pd.Timedelta.min\n        if \"meta\" in models_df.columns:\n          freshness_sla = model[\"meta\"][\"freshness_sla\"]\n        else:\n          freshness_sla = None\n        if model[\"executionInfo\"][\"executeCompletedAt\"] is not None:\n          model_freshness = current_time - pd.Timestamp(model[\"executionInfo\"][\"executeCompletedAt\"])\n          for ancestor in model[\"ancestors\"]:\n              if ancestor[\"resourceType\"] == \"SourceAppliedStateNestedNode\":\n                  ancestor_freshness = current_time - pd.Timestamp(ancestor[\"freshness\"]['maxLoadedAt'])\n              elif ancestor[\"resourceType\"] == \"ModelAppliedStateNestedNode\":\n                  ancestor_freshness = current_time - pd.Timestamp(ancestor[\"executionInfo\"][\"executeCompletedAt\"])\n\n              if ancestor_freshness > max_freshness:\n                  max_freshness = ancestor_freshness\n\n          G.add_node(model[\"uniqueId\"], name=model[\"name\"], type=\"model\", max_ancestor_freshness = max_freshness, freshness = model_freshness, freshness_sla=freshness_sla)\n    for _, source in sources_df.iterrows():\n        if source[\"maxLoadedAt\"] is not None:\n          G.add_node(source[\"uniqueId\"], name=source[\"name\"], type=\"source\", freshness=current_time - pd.Timestamp(source[\"maxLoadedAt\"]))\n    for _, model in models_df.iterrows():\n        for parent in model[\"parents\"]:\n            G.add_edge(parent[\"uniqueId\"], model[\"uniqueId\"])\n\n    return G\n```\n\n----------------------------------------\n\nTITLE: Generated YAML Output for a Single dbt Model\nDESCRIPTION: This YAML snippet shows the generated documentation structure for the 'activity_based_interest_activated' model, including column names.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-04-generating-dynamic-docs.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: activity_based_interest_activated\n    description: \"\"\n    columns:\n      - name: id\n        description: \"\"\n\n      - name: user_id\n        description: \"\"\n\n      - name: start_date\n        description: \"\"\n\n      - name: end_date\n        description: \"\"\n\n      - name: tier_threshold_amount\n        description: \"\"\n\n      - name: tier_interest_percentage\n        description: \"\"\n\n      - name: event_time\n        description: \"\"\n\n      - name: event_day\n        description: \"\"\n```\n\n----------------------------------------\n\nTITLE: Source Status Selection\nDESCRIPTION: Selecting resources based on source freshness status\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/methods.md#2025-04-09_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ndbt source freshness # must be run again to compare current to previous state\ndbt build --select \"source_status:fresher+\" --state path/to/prod/artifacts\n```\n\n----------------------------------------\n\nTITLE: Configuring Extended Attributes for Production Environment in YAML\nDESCRIPTION: This YAML snippet illustrates the configuration of extended attributes for a production environment in dbt Cloud. It sets up a connection to a separate BigQuery account with specific project, dataset, and authentication settings for production work.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-04-22-extended-attributes.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\naccount: 456prod\nproject: analytics\ndataset: main\nmethod: service-account-json\nthreads: 16\n```\n\n----------------------------------------\n\nTITLE: Querying Customer Data in SQL\nDESCRIPTION: A simple SQL query to select all columns from the raw customers table in the jaffle_shop schema.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/sl-snowflake-qs.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nselect * from raw.jaffle_shop.customers\n```\n\n----------------------------------------\n\nTITLE: Defining a Documentation Block in Jinja2\nDESCRIPTION: This snippet shows how to define a documentation block using Jinja2 syntax in a Markdown file. The block is named 'orders' and contains a simple list of items.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/doc.md#2025-04-09_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n{% docs orders %}\n\n# docs\n- go\n- here\n \n{% enddocs %}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating PR Template URL Variables in dbt Cloud\nDESCRIPTION: This snippet shows how PR template URL variables are used and rendered in dbt Cloud. It demonstrates the use of {{source}} and {{destination}} variables in the URL template.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/git/pr-template.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<Tabs\n  defaultValue=\"template\"\n  values={[\n    { label: 'Template', value: 'template', },\n    { label: 'Rendered', value: 'rendered', },\n  ]}>\n<TabItem value=\"template\">\n\n```\nhttps://github.com/dbt-labs/jaffle_shop/compare/{{destination}}..{{source}}\n```\n\n</TabItem>\n<TabItem value=\"rendered\">\n\n```\nhttps://github.com/dbt-labs/jaffle_shop/compare/master..my-branch\n```\n\n</TabItem>\n</Tabs>\n```\n\n----------------------------------------\n\nTITLE: Storing Secret Keys in Zapier Storage using Python\nDESCRIPTION: This Python snippet shows how to store credentials securely in Zapier's StoreClient. It saves dbt webhook keys and Tableau authentication credentials that will be used later to authenticate API requests. Requires a Storage by Zapier connection UUID.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/zapier-refresh-tableau-workbook.md#2025-04-09_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nstore = StoreClient('abc123') #replace with your UUID secret\nstore.set('DBT_WEBHOOK_KEY', 'abc123') #replace with your dbt Cloud Webhook key\nstore.set('TABLEAU_SITE_URL', 'abc123') #replace with your Tableau Site URL, inclusive of https:// and .com\nstore.set('TABLEAU_SITE_NAME', 'abc123') #replace with your Tableau Site/Server Name\nstore.set('TABLEAU_API_TOKEN_NAME', 'abc123') #replace with your Tableau API Token Name\nstore.set('TABLEAU_API_TOKEN_SECRET', 'abc123') #replace with your Tableau API Secret\n```\n\n----------------------------------------\n\nTITLE: Using loop.last to Handle Trailing Commas\nDESCRIPTION: SQL model that uses Jinja's loop.last variable to conditionally add commas between columns, avoiding syntax errors from trailing commas.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/using-jinja.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{% set payment_methods = [\"bank_transfer\", \"credit_card\", \"gift_card\"] %}\n\nselect\norder_id,\n{% for payment_method in payment_methods %}\nsum(case when payment_method = '{{payment_method}}' then amount end) as {{payment_method}}_amount\n{% if not loop.last %},{% endif %}\n{% endfor %}\nfrom {{ ref('raw_payments') }}\ngroup by 1\n```\n\n----------------------------------------\n\nTITLE: Runtime Error - Profile Not Found in dbt\nDESCRIPTION: Error message displayed when dbt cannot locate a specified user profile, typically due to missing or invalid credentials. This occurs when running dbt version 1.9.0 and indicates an authentication issue that needs to be resolved.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Troubleshooting/runtime-error-could-not-find-profile.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nRunning with dbt=1.9.0\nEncountered an error while reading the project:\n  ERROR: Runtime Error\n  Could not find profile named 'user'\nRuntime Error\n  Could not run dbt'\n```\n\n----------------------------------------\n\nTITLE: Configuring Snapshots in YAML Properties File (v1.9+)\nDESCRIPTION: YAML configuration for snapshots in schema.yml file for versions 1.9 and above. Includes configuration options like database, schema, unique_key, strategy, updated_at, and newer options like snapshot_meta_column_names and hard_deletes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/snapshot-configs.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  - name: <string>\n    config:\n      [database](/reference/resource-configs/database): <string>\n      [schema](/reference/resource-configs/schema): <string>\n      [unique_key](/reference/resource-configs/unique_key): <column_name_or_expression>\n      [strategy](/reference/resource-configs/strategy): timestamp | check\n      [updated_at](/reference/resource-configs/updated_at): <column_name>\n      [check_cols](/reference/resource-configs/check_cols): [<column_name>] | all\n      [snapshot_meta_column_names](/reference/resource-configs/snapshot_meta_column_names): {<dictionary>}\n      [hard_deletes](/reference/resource-configs/hard-deletes): string\n      [dbt_valid_to_current](/reference/resource-configs/dbt_valid_to_current): <string>\n```\n\n----------------------------------------\n\nTITLE: Basic Debug Statement Placement Before Query Execution\nDESCRIPTION: A Jinja code snippet showing how to place a debug() statement before executing a query to diagnose issues during execution.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-03-30-guide-to-debug-in-jinja.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{ debug() }}\n{% set my_results = run_query(sql_statement) %}\n```\n\n----------------------------------------\n\nTITLE: Discouraged Use of Absolute Paths for macro-paths\nDESCRIPTION: An example showing the discouraged approach of using absolute paths for the macro-paths configuration, which should be avoided in dbt projects.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/macro-paths.md#2025-04-09_snippet_2\n\nLANGUAGE: yml\nCODE:\n```\nmacro-paths: [\"/Users/username/project/macros\"]\n```\n\n----------------------------------------\n\nTITLE: Basic Node Selection with Wildcards\nDESCRIPTION: Examples of using wildcards in node selection patterns to match multiple resources\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/methods.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndbt list --select \"*.folder_name.*\"\ndbt list --select \"package:*_source\"\n```\n\n----------------------------------------\n\nTITLE: File Structure for dbt Documentation\nDESCRIPTION: Shows the recommended folder structure for organizing docs blocks in a dbt project, with a dedicated markdown file for documentation blocks and YAML files for model configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-04-generating-dynamic-docs.md#2025-04-09_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nmodels/core/activity_based_interest\n _activity_based_interest_docs.md --New docs block markdown file\n _activity_based_interest_docs.yml\n events\n    activity_based_interest_activated.sql\n    activity_based_interest_deactivated.sql\n    activity_based_interest_updated.sql\n    downgrade_interest_level_for_user.sql\n    set_inactive_interest_rate_after_july_1st_in_bec_for_user.sql\n    set_inactive_interest_rate_from_july_1st_in_bec_for_user.sql\n    set_interest_levels_from_june_1st_in_bec_for_user.sql\n models\n     f_activity_based_interest.sql\n```\n\n----------------------------------------\n\nTITLE: Using last_day Macro in DBT SQL\nDESCRIPTION: Examples of using the last_day macro to get the last day of a month or quarter. Takes date expression and datepart as arguments.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/cross-database-macros.md#2025-04-09_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt.last_day(\"created_at\", \"month\") }}\n{{ dbt.last_day(\"'2016-03-09'\", \"year\") }}\n```\n\nLANGUAGE: sql\nCODE:\n```\ncast(\n    date_trunc('month', created_at) + ((interval '10 month') * (1))\n + ((interval '10 day') * (-1))\n        as date)\n\ncast(\n    date_trunc('year', '2016-03-09') + ((interval '10 year') * (1))\n + ((interval '10 day') * (-1))\n        as date)\n```\n\n----------------------------------------\n\nTITLE: dbt Cloud Environment Configuration\nDESCRIPTION: YAML configuration for setting custom defer environment in dbt Cloud\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-01-09-defer-in-development.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ndbt-cloud:\n  project-id: <Your project id>\n  defer-env-id: <An environment id>\n```\n\n----------------------------------------\n\nTITLE: Basic SQL Query Example for Syntax Tree Demonstration\nDESCRIPTION: This SQL query demonstrates a simple select statement with aggregation, filtering, and grouping that would be processed by a parser to generate a syntax tree. It selects order IDs and sums order amounts from an order_items table, filtered by year.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2025-01-24-sql-comprehension-technologies.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect \n  order_id, \n  sum(amount) as total_order_amount\nfrom order_items\nwhere \n  date_trunc('year', ordered_at) = '2025-01-01'\ngroup by 1\n```\n\n----------------------------------------\n\nTITLE: Database Schema Grant Macro\nDESCRIPTION: A Jinja2 macro that handles granting permissions across multiple databases and schemas.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/on-run-end-context.md#2025-04-09_snippet_3\n\nLANGUAGE: jinja2\nCODE:\n```\n{% macro grant_usage_to_schemas(database_schemas, user) %}\n  {% for (database, schema) in database_schemas %}\n    grant usage on {{ database }}.{{ schema }} to {{ user }};\n  {% endfor %}\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration for Testing Python Model Output\nDESCRIPTION: YAML configuration file that sets up a not_null test for the parsed_transaction_time column in the transactions model. This ensures the dateutil parsing was successful for all records.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-10-19-polyglot-dbt-python-dataframes-and-sql.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: transactions\n    columns:\n      - name: parsed_transaction_time\n        tests:\n          - not_null\n```\n\n----------------------------------------\n\nTITLE: Triggering dbt Cloud Job Run with dbt-cloud-cli\nDESCRIPTION: This snippet demonstrates how to trigger a dbt Cloud job run using the dbt-cloud-cli. It shows the simplicity and readability of the command compared to the cURL version.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-05-03-making-dbt-cloud-api-calls-using-dbt-cloud-cli.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndbt-cloud job run --job-id 43167\n```\n\n----------------------------------------\n\nTITLE: Flexible Account Classification in dbt_sage_intacct\nDESCRIPTION: This SQL snippet from the dbt_sage_intacct package demonstrates a flexible approach to account classification using variables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-09-07-leverage-accounting-principles-when-finacial-modeling.md#2025-04-09_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nmodels/intermediate/int_sage_intacct__account_classifications.sql#L12-L22\n```\n\n----------------------------------------\n\nTITLE: Setup Python Environment in POSIX PowerShell Core (Shell)\nDESCRIPTION: Shell commands for creating and activating a Python virtual environment, updating pip, and installing dependencies in PowerShell Core on POSIX systems.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-10-19-polyglot-dbt-python-dataframes-and-sql.md#2025-04-09_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\npython3 -m venv env\nenv/bin/Activate.ps1\npython3 -m pip install --upgrade pip\npython3 -m pip install -r requirements.txt\nenv/bin/Activate.ps1\n```\n\n----------------------------------------\n\nTITLE: Configuring Full Refresh for Seeds in dbt_project.yml\nDESCRIPTION: Sets the full_refresh config for seeds in the dbt_project.yml file. This determines whether seeds will always, never, or conditionally perform a full refresh based on the --full-refresh flag.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/full_refresh.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nseeds:\n  [<resource-path>]:\n    +full_refresh: false | true\n\n```\n\n----------------------------------------\n\nTITLE: Creating BigQuery Table for Slack Messages in SQL\nDESCRIPTION: This SQL snippet creates a table in BigQuery to store Slack messages. The table includes fields for message time, domain, username, real name, email, channel, permalink, and message text.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-11-29-open-source-community-growth.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE `openlineage.metrics.slack_messages`\n(\n  message_time TIMESTAMP NOT NULL,\n  domain STRING NOT NULL,\n  username STRING,\n  realname STRING,\n  email STRING,\n  channel STRING,\n  permalink STRING,\n  text STRING\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Development Requirements for dbt-glue Testing in Bash\nDESCRIPTION: Command to install development requirements for testing the dbt-glue adapter.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/glue-setup.md#2025-04-09_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\n$ pip3 install -r dev-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Rerunning Error Models with Modified Changes in dbt\nDESCRIPTION: Command to rerun models that previously errored, along with any new modifications that might relate to those models. This provides a targeted approach to fixing errors while including relevant changes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/best-practice-workflows.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --select state:modified+ result:error+ --defer --state path/to/prod/artifacts\n```\n\n----------------------------------------\n\nTITLE: Referencing a Specific Model Version in SQL\nDESCRIPTION: SQL example showing how to reference a specific version of a model using the ref function with a version parameter. This allows selecting from version 2 of dim_customers.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-01-evolving-data-engineer-craft.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nselect * from {{ ref('dim_customers', v=2) }}\n```\n\n----------------------------------------\n\nTITLE: Debugging Runtime Error: Profile Not Found\nDESCRIPTION: Error message shown when dbt cannot find the specified profile in the profiles.yml file. This typically happens when the profile name in dbt_project.yml doesn't match what's defined in profiles.yml.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/debug-errors.md#2025-04-09_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nRunning with dbt=1.7.1\n\nEncountered an error:\nRuntime Error\n  Could not run dbt\n  Could not find profile named 'jaffle_shops'\n```\n\n----------------------------------------\n\nTITLE: Calculating RFM Values with SQL\nDESCRIPTION: This SQL snippet calculates the Recency, Frequency, and Monetary values for each user. It uses aggregate functions to determine the last payment date, days since last transaction, transaction count, and total amount spent for each user.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-08-building-a-historical-user-segmentation-model-with-dbt.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nrfm_values AS (\n    SELECT  user_id,\n            MAX(payment_date) AS max_payment_date,\n            NOW() - MAX(payment_date) AS recency,\n            COUNT(DISTINCT payment_id) AS frequency,\n            SUM(payment_amount) AS monetary\n    FROM payments\n    GROUP BY user_id\n),\n```\n\n----------------------------------------\n\nTITLE: Querying Customer Orders in Snowflake\nDESCRIPTION: This SQL query analyzes customer ordering patterns in Snowflake by joining customer data with order information. It calculates metrics including first order date, most recent order date, and total number of orders. The query uses Snowflake's database.schema.table fully qualified naming convention.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/tutorial-sql-query.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nwith customers as (\n\n    select\n        id as customer_id,\n        first_name,\n        last_name\n\n    from raw.jaffle_shop.customers\n\n),\n\norders as (\n\n    select\n        id as order_id,\n        user_id as customer_id,\n        order_date,\n        status\n\n    from raw.jaffle_shop.orders\n\n),\n\ncustomer_orders as (\n\n    select\n        customer_id,\n\n        min(order_date) as first_order_date,\n        max(order_date) as most_recent_order_date,\n        count(order_id) as number_of_orders\n\n    from orders\n\n    group by 1\n\n),\n\nfinal as (\n\n    select\n        customers.customer_id,\n        customers.first_name,\n        customers.last_name,\n        customer_orders.first_order_date,\n        customer_orders.most_recent_order_date,\n        coalesce(customer_orders.number_of_orders, 0) as number_of_orders\n\n    from customers\n\n    left join customer_orders using (customer_id)\n\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Configuring Begin Parameter in SQL Model Config Block\nDESCRIPTION: Example showing how to set the 'begin' parameter directly in a SQL model file using a config block.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/begin.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    begin='2024-01-01 00:00:00'\n) }}\n```\n\n----------------------------------------\n\nTITLE: Versioning Entire Pages in dbt-versions.js (JavaScript)\nDESCRIPTION: Example of how to version an entire page by adding it to the versionedPages array in the dbt-versions.js file. This specifies the page path and the first version where it's available.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/contributing/single-sourcing-content.md#2025-04-09_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\nexports.versionedPages = [\n    {\n      \"page\": \"docs/supported-data-platforms\",\n      \"firstVersion\": \"1.0\",\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Database Connections in YAML\nDESCRIPTION: YAML configuration for dbt database connections using environment variables for CI and production environments. This profiles.yml file defines connection details for Postgres databases.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-04-14-add-ci-cd-to-bitbucket.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nyour_project:\n  target: ci\n  outputs:\n    ci:\n      type: postgres\n      host: \"{{ env_var('DB_CI_HOST') }}\"\n      port: \"{{ env_var('DB_CI_PORT') | int }}\"\n      user: \"{{ env_var('DB_CI_USER') }}\"\n      password: \"{{ env_var('DB_CI_PWD') }}\"\n      dbname: \"{{ env_var('DB_CI_DBNAME') }}\"\n      schema: \"{{ env_var('DB_CI_SCHEMA') }}\"\n      threads: 16\n      keepalives_idle: 0\n    prod:\n      type: postgres\n      host: \"{{ env_var('DB_PROD_HOST') }}\"\n      port: \"{{ env_var('DB_PROD_PORT') | int }}\"\n      user: \"{{ env_var('DB_PROD_USER') }}\"\n      password: \"{{ env_var('DB_PROD_PWD') }}\"\n      dbname: \"{{ env_var('DB_PROD_DBNAME') }}\"\n      schema: \"{{ env_var('DB_PROD_SCHEMA') }}\"\n      threads: 16\n      keepalives_idle: 0\n```\n\n----------------------------------------\n\nTITLE: Triggering dbt Cloud Job Run with cURL\nDESCRIPTION: This snippet shows how to trigger a dbt Cloud job run using cURL. It demonstrates the complexity of making a direct API call with authentication and JSON payload.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-05-03-making-dbt-cloud-api-calls-using-dbt-cloud-cli.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -H \"Authorization:Token $DBT_CLOUD_API_TOKEN\" -H \"Content-Type:application/json\" -d '{\"cause\":\"Triggered using cURL\"}' https://cloud.getdbt.com/api/v2/accounts/$DBT_CLOUD_ACCOUNT_ID/jobs/43167/run/\n```\n\n----------------------------------------\n\nTITLE: Running Unit Test via Shell Command\nDESCRIPTION: Shell command to execute the unit test macro using dbt run-operation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-08-22-unit-testing-dbt-package.md#2025-04-09_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndbt run-operation test_to_literal\n```\n\n----------------------------------------\n\nTITLE: Correct CLI Flag Placement Example\nDESCRIPTION: Example showing correct placement of flags after the dbt subcommand.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/command-line-options.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --no-populate-cache\n```\n\n----------------------------------------\n\nTITLE: Configuring All Models in dbt\nDESCRIPTION: Shows how to apply configuration to all models in a dbt project without using a resource path.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/resource-path.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  +enabled: false # this will disable all models (not a thing you probably want to do)\n```\n\n----------------------------------------\n\nTITLE: GitHub Stars Table Schema in BigQuery\nDESCRIPTION: SQL schema definition for the github_stars table in BigQuery that stores timestamp, project name, and star count.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-11-29-open-source-community-growth.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE `openlineage.metrics.github_stars`\n(\n  timestamp TIMESTAMP,\n  project STRING,\n  stars INT64\n)\n```\n\n----------------------------------------\n\nTITLE: Command Line Execution of Version Macro\nDESCRIPTION: Example command line output showing how to execute the get_version macro and its resulting output displaying the dbt version.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/dbt_version.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ dbt run-operation get_version\nThe installed version of dbt is 1.6.0\n```\n\n----------------------------------------\n\nTITLE: Running dbt Build Command\nDESCRIPTION: Shell command to execute the model build and run associated unit tests.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-05-07-unit-testing.md#2025-04-09_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndbt build --select hello_world\n```\n\n----------------------------------------\n\nTITLE: Feature Support Matrix Table in Markdown\nDESCRIPTION: A markdown table displaying the availability of different dbt Cloud features across various cloud deployment options, including AWS multi-tenant, AWS single tenant, Azure multi-tenant, Azure single tenant, and GCP multi-tenant (beta).\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/cloud-feature-parity.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature                       | AWS Multi-tenant | AWS single tenant     |Azure multi-tenant   | Azure single tenant | GCP multi-tenant <Lifecycle status='beta' /> |\n|-------------------------------|------------------|-----------------------|---------------------|---------------------|------------------|\n| Audit logs                    |                |                      |                   |                   |                 | \n| Continuous integration jobs   |                |                      |                   |                   |                 | \n| dbt Cloud CLI                 |                |                      |                   |                   |                 |\n| dbt Cloud IDE                 |                |                      |                   |                   |                 |\n| dbt Copilot                   |                |                      |                   |                   |                 |\n| dbt Explorer                  |                |                      |                   |                   |                 |\n| dbt Mesh                      |                |                      |                   |                   |                 |\n| dbt Semantic Layer            |                |  (Upon request)      |                   |                   |                 |\n| Discovery API                 |                |                      |                   |                   |                 |\n| IP restrictions               |                |                      |                   |                   |                 |\n| Job scheduler                 |                |                      |                   |                   |                 |\n| PrivateLink egress            |  (AWS only)    |                      |                   |                   |                 |\n| PrivateLink ingress           |                |                      |                   |                   |                 |\n| Webhooks (Outbound)           |                |                      |                   |                   |                 |\n```\n\n----------------------------------------\n\nTITLE: Fetching Data from Upstream Tables in dbt\nDESCRIPTION: SQL code demonstrating how to select data from upstream tables using Common Table Expressions (CTEs) in dbt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-04-18-building-a-kimball-dimensional-model-with-dbt.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nwith stg_product as (\n    select *\n    from {{ ref('product') }}\n),\n\nstg_product_subcategory as (\n    select *\n    from {{ ref('productsubcategory') }}\n),\n\nstg_product_category as (\n    select *\n    from {{ ref('productcategory') }}\n)\n\n... \n```\n\n----------------------------------------\n\nTITLE: Specifying Paths in dbt Projects\nDESCRIPTION: This snippet demonstrates the syntax for path specifications in dbt projects. Paths should be relative to the dbt_project.yml file location rather than using absolute paths, which can cause unexpected behavior.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_relative-path.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{props.path}\n```\n\nLANGUAGE: yaml\nCODE:\n```\n{props.absolute}\n```\n\n----------------------------------------\n\nTITLE: Defining Model Contracts in YAML for dbt v1.5\nDESCRIPTION: This YAML snippet demonstrates the syntax for defining model contracts in dbt v1.5. Model contracts allow data engineers to specify constraints on column names, data types, and null values, creating accountability between model creators and consumers.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-01-evolving-data-engineer-craft.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\n```\n\n----------------------------------------\n\nTITLE: Defining S3 Folder Structure for Jaffle Shop Data\nDESCRIPTION: This snippet shows the required folder structure for uploading the Jaffle Shop data files to an Amazon S3 bucket. The structure organizes customer, order, and payment data in separate folders under a main dbt-quickstart directory.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/starburst-galaxy-qs.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n<bucket/blob>\n    dbt-quickstart (folder)\n        jaffle-shop-customers (folder)\n            jaffle_shop_customers.csv (file)\n        jaffle-shop-orders (folder)\n            jaffle_shop_orders.csv (file)\n        stripe-payments (folder)\n            stripe-payments.csv (file)\n```\n\n----------------------------------------\n\nTITLE: Aliasing a Data Test in models/properties.yml\nDESCRIPTION: Specifies an alias for a unique data test named 'order_id' in the models/properties.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/alias.md#2025-04-09_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: orders\n    columns:\n      - name: order_id\n        tests:\n          - unique:\n              alias: unique_order_id_test\n```\n\n----------------------------------------\n\nTITLE: Using dbt Selectors in Command Line\nDESCRIPTION: How to reference a defined selector in a dbt command using the --selector flag.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/yaml-selectors.md#2025-04-09_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --selector nightly_diet_snowplow\n```\n\n----------------------------------------\n\nTITLE: Building the Website for Deployment\nDESCRIPTION: Commands to build the website for deployment using either make or npm, recommended before pushing changes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/README.md#2025-04-09_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm run build\n```\n\n----------------------------------------\n\nTITLE: Navigating to Project Directory in Shell\nDESCRIPTION: Command to change directory to the cloned repository location, which will be used for deploying the serverless application.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/serverless-pagerduty.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n#example: replace with your actual path\ncd ~/Documents/GitHub/dbt-cloud-webhooks-pagerduty\n```\n\n----------------------------------------\n\nTITLE: Querying Account Classification in dbt_xero\nDESCRIPTION: This SQL snippet from the dbt_xero package demonstrates how to query the account classification from the accounts table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-09-07-leverage-accounting-principles-when-finacial-modeling.md#2025-04-09_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nmodels/stg_xero__account.sql#L30\n```\n\n----------------------------------------\n\nTITLE: Viewing Active Session Status in dbt Cloud CLI\nDESCRIPTION: Use the 'dbt invocation list' command in a separate terminal window to view the status of active sessions. This helps in debugging long-running sessions and identifying arguments causing the issue.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Troubleshooting/long-sessions-cloud-cli.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndbt invocation list\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Models in SQL Files with Config Blocks (1.9+)\nDESCRIPTION: Example of configuring a dbt model directly in its SQL file using a config() Jinja macro for version 1.9 and later. Includes all previous options plus the new event_time parameter.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/model-configs.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    [enabled]=true | false,\n    [tags]=\"<string>\" | [\"<string>\"],\n    [pre_hook]=\"<sql-statement>\" | [\"<sql-statement>\"],\n    [post_hook]=\"<sql-statement>\" | [\"<sql-statement>\"],\n    [database]=\"<string>\",\n    [schema]=\"<string>\",\n    [alias]=\"<string>\",\n    [persist_docs]={<dict>},\n    [meta]={<dict>},\n    [grants]={<dict>},\n    [contract]={<dictionary>},\n    [event_time]='my_time_field',\n) }}\n```\n\n----------------------------------------\n\nTITLE: Coalescing and Type Casting in generate_surrogate_key Macro for SQL\nDESCRIPTION: This snippet shows how the generate_surrogate_key macro handles each field by coalescing it with a default value and casting it to string type. This is part of the internal implementation of the macro that runs when you call it.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-11-22-sql-surrogate-keys.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n  coalesce(cast(\" ~ field ~ \" as \" ~ dbt.type_string() ~ \"), '_dbt_utils_surrogate_key_null_')\n```\n\n----------------------------------------\n\nTITLE: Running and Testing the Fact Table Models via Command Line\nDESCRIPTION: A bash command that executes dbt run to build the fact table and dbt test to ensure all tests pass. This validates that the dimensional model is correctly constructed.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-04-18-building-a-kimball-dimensional-model-with-dbt.md#2025-04-09_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ndbt run && dbt test \n```\n\n----------------------------------------\n\nTITLE: Using + Prefix in DBT Configuration\nDESCRIPTION: Demonstrates the use of the + symbol as a prefix to denote configurations in dbt_project.yml. The prefix is specifically used to differentiate configuration names from folder names and only applies within the dbt_project.yml file under resource keys.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_plus-prefix.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n+prefix\n```\n\nLANGUAGE: jinja\nCODE:\n```\nconfig()\n```\n\n----------------------------------------\n\nTITLE: Defining Data Integrity Tests in YAML for dbt\nDESCRIPTION: This snippet demonstrates how to define Data Integrity tests (Generic Tests) in a YAML file for a dbt project. It includes uniqueness and not_null tests for a customer model's id column, with aliases for each test.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-01-24-aggregating-test-failures.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\nmodels:\n  - name: customer\n    columns:\n      - name: id\n        description: Unique ID associated with the record\n        tests:\n          - unique:\n              alias: id__unique\n          - not_null:\n              alias: id__not_null\n```\n\n----------------------------------------\n\nTITLE: Creating Data Buckets with SQL CASE and BETWEEN\nDESCRIPTION: This SQL snippet demonstrates how to use the BETWEEN operator within a CASE statement to create engagement buckets based on the 'time_engaged' value. It categorizes engagement levels as low, medium, or high.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/operators/sql-between.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\ncase when time_engaged between 0 and 9 then 'low_engagement'\n     when time_engaged between 10 and 29 then 'medium_engagement'\n     else 'high_engagement' end as engagement\n```\n\n----------------------------------------\n\nTITLE: Error Message from Failed dbt Run\nDESCRIPTION: The error output from a failed dbt run showing limited information about the issue, which prompted the need for debugging.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-03-30-guide-to-debug-in-jinja.md#2025-04-09_snippet_3\n\nLANGUAGE: plain text\nCODE:\n```\n16:49:26  Database error while running on-run-end\n16:49:26  Encountered an error:\nRuntime Error\n  Parser Error:\n```\n\n----------------------------------------\n\nTITLE: Configuring GitHub Enterprise PR Template URL in dbt Cloud\nDESCRIPTION: This snippet shows the PR template URL format for GitHub Enterprise repositories in dbt Cloud. It includes a placeholder for the company-specific domain.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/git/pr-template.md#2025-04-09_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://git.<mycompany>.com/<org>/<repo>/compare/{{destination}}..{{source}}\n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Connection in dbt for Upsolver\nDESCRIPTION: This snippet demonstrates how to configure a SQL connection in dbt for Upsolver. It uses the 'connection' materialization and requires specifying the connection type and options.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/upsolver-configs.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n        materialized='connection',\n        connection_type={ 'S3' | 'GLUE_CATALOG' | 'KINESIS' | 'KAFKA'| 'SNOWFLAKE' },\n        connection_options={}\n        )\n}}\n```\n\n----------------------------------------\n\nTITLE: Linking to Specific Sections Using Anchor Links in Markdown\nDESCRIPTION: How to link to specific sections of a document using anchor links with the # symbol followed by the hyphenated section title.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/contributing/content-style-guide.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\nTo better understand this model type, read our [incremental models page](/docs/build/incremental-models#understand-incremental-models).\n```\n\n----------------------------------------\n\nTITLE: Masking Database Roles for dbt Cloud and Bitbucket Users in SQL\nDESCRIPTION: SQL commands to mask roles for dbt Cloud and Bitbucket users, ensuring that tables and views created by either user are owned by the role_prod role.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-04-14-add-ci-cd-to-bitbucket.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nalter role dbt_bitbucket set role role_prod;\n\nalter role dbt_cloud set role role_prod;\n```\n\n----------------------------------------\n\nTITLE: Configuring Post-Hook Grants in Legacy dbt\nDESCRIPTION: Example of using post-hooks to grant select permissions on an incremental model in versions prior to dbt Core v1.2.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-07-26-configuring-grants.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n\tmaterialized = 'incremental',\n\tpost_hook = [\"grant select on {{ this }} to reporter\"]\n) }}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Adding Required Content to .gitignore File in dbt Project\nDESCRIPTION: This code snippet shows the necessary content to add to the .gitignore file in a dbt project. It specifies directories that Git should ignore, including target/, dbt_packages/, logs/, and the legacy dbt_modules/ directory.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Troubleshooting/gitignore.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntarget/\ndbt_packages/\nlogs/\n# legacy -- renamed to dbt_packages in dbt v1\ndbt_modules/\n```\n\n----------------------------------------\n\nTITLE: Running Multiple Custom Named Tests in dbt CLI\nDESCRIPTION: This shell command shows the execution of multiple custom-named tests in dbt, demonstrating how custom names help in differentiating and running specific tests.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/data-tests.md#2025-04-09_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\n$ dbt test\n12:48:03  Running with dbt=1.1.0-b1\n12:48:04  Found 1 model, 2 tests, 0 snapshots, 0 analyses, 167 macros, 0 operations, 1 seed file, 0 sources, 0 exposures, 0 metrics\n12:48:04\n12:48:04  Concurrency: 5 threads (target='dev')\n12:48:04\n12:48:04  1 of 2 START test unexpected_order_status_today ................................ [RUN]\n12:48:04  2 of 2 START test unexpected_order_status_yesterday ............................ [RUN]\n12:48:04  1 of 2 PASS unexpected_order_status_today ...................................... [PASS in 0.04s]\n12:48:04  2 of 2 PASS unexpected_order_status_yesterday .................................. [PASS in 0.04s]\n12:48:04\n12:48:04  Finished running 2 tests in 0.21s.\n12:48:04\n12:48:04  Completed successfully\n12:48:04\n12:48:04  Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2\n```\n\n----------------------------------------\n\nTITLE: Environment Configuration Using Git Release Tags\nDESCRIPTION: Example showing how to set up a QA environment using git release tags in dbt Cloud, including tag creation and environment configuration with custom branch settings.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2025-01-28-git-branching-strategies-and-dbt.md#2025-04-09_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n- We create a release tag, `v2`, from our repository.\n- We specify `v2` as our branch in our Production environment's **custom branch** setting.\nJobs using Production will now check out code at `v2`.\n- We set up an environment called \"QA\", with the **custom branch** setting as `main`. For the database and schema, we specify the `qa` database and `analytics` schema. Jobs created using this environment will check out code from `main` and built it to `qa.analytics`.\n```\n\n----------------------------------------\n\nTITLE: Configuring Private Packages for Azure DevOps in dbt YAML\nDESCRIPTION: Shows how to configure private packages for Azure DevOps in the packages.yml file. This is specific to Azure DevOps projects where the package repository is in the same project as the source repo.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/packages.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\npackages:\n  - private: my-org/my-repo # Works if your ADO source repo and package repo are in the same project\n```\n\n----------------------------------------\n\nTITLE: Creating Surrogate Keys in dbt\nDESCRIPTION: SQL code demonstrating how to generate surrogate keys using dbt_utils package.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-04-18-building-a-kimball-dimensional-model-with-dbt.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\n...\n\nselect\n    {{ dbt_utils.generate_surrogate_key(['stg_product.productid']) }} as product_key, \n    ... \nfrom stg_product\nleft join stg_product_subcategory on stg_product.productsubcategoryid = stg_product_subcategory.productsubcategoryid\nleft join stg_product_category on stg_product_subcategory.productcategoryid = stg_product_category.productcategoryid\n```\n\n----------------------------------------\n\nTITLE: DBT YAML Error Example\nDESCRIPTION: Example of an invalid test configuration error message in DBT, showing how syntax errors in YAML files are reported.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_configs-properties.md#2025-04-09_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n* Invalid test config given in models/schema.yml near {'namee': 'event', ...}\n  Invalid arguments passed to \"UnparsedNodeUpdate\" instance: 'name' is a required property, Additional properties are not allowed ('namee' was unexpected)\n```\n\n----------------------------------------\n\nTITLE: Aggregating Test Failures with Metadata in dbt\nDESCRIPTION: This SQL snippet demonstrates how to create a base model that aggregates test failure results with metadata. It uses BigQuery-specific features like wildcard selectors and includes configuration for incremental materialization and partitioning.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-01-24-aggregating-test-failures.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n       materialized = 'incremental',\n       partition_by = {'field': 'load_date', 'data_type': 'date'},\n       incremental_strategy = 'merge',\n       unique_key='row_key',\n       full_refresh=false,\n       tags=['dq_test_warning_failures','customer_mart', 'data_health']\n   )\n}}\n\nWITH failures as (\n   SELECT\n       count(*) as test_failures,\n       _TABLE_SUFFIX as table_suffix,\n   FROM {{ var('customer_mart_schema') }}_dbt_test__audit.`*`\n   GROUP BY _TABLE_SUFFIX\n),\n\nmetadata as (\n   SELECT\n       test_owner,\n       test_alias,\n       test_description,\n       split(test_alias, '__')[SAFE_ORDINAL(2)] as test_name,\n       test_severity\n   FROM {{ref('test_warning_metadata')}}\n),\n\nSELECT\n   m.*,\n   f.*\nFROM metadata m\nLEFT JOIN failures f on m.test_alias = f.table_suffix\nWHERE m.is_active is TRUE\n```\n\n----------------------------------------\n\nTITLE: Configuring Incremental Model with Insert Overwrite Strategy in SQL\nDESCRIPTION: This SQL snippet shows how to set up an incremental model using the insert_overwrite strategy with partitioning. It includes configuration for partition_by and file_format, and demonstrates how to filter data for incremental runs.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/glue-configs.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized='incremental',\n    partition_by=['date_day'],\n    file_format='parquet'\n) }}\n\n/*\n  Every partition returned by this query will be overwritten\n  when this model runs\n*/\n\nwith new_events as (\n\n    select * from {{ ref('events') }}\n\n    {% if is_incremental() %}\n    where date_day >= date_add(current_date, -1)\n    {% endif %}\n\n)\n\nselect\n    date_day,\n    count(*) as users\n\nfrom events\ngroup by 1\n```\n\n----------------------------------------\n\nTITLE: BigQuery Table Schema Definition\nDESCRIPTION: YAML representation of the resulting table schema after implementing ingestion-time partitioning.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-02-01-ingestion-time-partitioning-bigquery.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ncampaign_id INT64\nimpressions_count INT64\n```\n\n----------------------------------------\n\nTITLE: BigQuery Nested Column Constraints Model SQL\nDESCRIPTION: This SQL snippet defines a dbt model for BigQuery with nested columns, demonstrating how to structure data with nested fields.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/constraints.md#2025-04-09_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(\n    materialized = \"table\"\n  )\n}}\n\nselect\n  'string' as a,\n  struct(\n    1 as id,\n    'name' as name,\n    struct(2 as id, struct('test' as again, '2' as even_more) as another) as double_nested\n  ) as b\n```\n\n----------------------------------------\n\nTITLE: SQL TRIM Function Example with Custom Character Removal\nDESCRIPTION: Example showing how to use the TRIM function to remove asterisk characters from the beginning and end of strings. The query adds asterisks to names using CONCAT and then removes them with TRIM.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/string-functions/sql-trim.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    first_name,\n    concat('*', first_name, '**') as test_string,\n    trim(test_string, '*') as back_to_first_name\nfrom {{ ref('customers') }}\nlimit 3\n```\n\n----------------------------------------\n\nTITLE: Defining Unit Test Structure in YAML\nDESCRIPTION: This snippet outlines the structure for defining unit tests in a dbt project's YAML configuration. It includes options for specifying test names, model versions, input data, expected outputs, and environment overrides.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/unit-tests.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nunit_tests:\n  - name: <test-name> # this is the unique name of the test\n    model: <model-name> \n      versions: #optional\n        include: <list-of-versions-to-include> #optional\n        exclude: <list-of-versions-to-exclude> #optional\n    config: \n      meta: {dictionary}\n      tags: <string> | [<string>]\n      enabled: {boolean} # optional. v1.9 or higher. If not configured, defaults to `true`\n    given:\n      - input: <ref_or_source_call> # optional for seeds\n        format: dict | csv | sql\n        # either define rows inline or name of fixture\n        rows: {dictionary} | <string>\n        fixture: <fixture-name> # sql or csv \n      - input: ... # declare additional inputs\n    expect:\n      format: dict | csv | sql\n      # either define rows inline of rows or name of fixture\n      rows: {dictionary} | <string>\n      fixture: <fixture-name> # sql or csv \n    overrides: # optional: configuration for the dbt execution environment\n      macros:\n        is_incremental: true | false\n        dbt_utils.current_timestamp: <string>\n        # ... any other jinja function from https://docs.getdbt.com/reference/dbt-jinja-functions\n        # ... any other context property\n      vars: {dictionary}\n      env_vars: {dictionary}\n  - name: <test-name> ... # declare additional unit tests\n```\n\n----------------------------------------\n\nTITLE: Implementing File Component for Code Display in YAML\nDESCRIPTION: Demonstrates how to use the File component to display code snippets with a filename. This is useful for showing examples of configuration files or code snippets in context.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/contributing/adding-page-components.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n<File name=\"~/.dbt/profiles.yml\">\n\n```yaml\npassword: hunter2\n```\n</File>\n```\n\n----------------------------------------\n\nTITLE: Model Access Error Example\nDESCRIPTION: Shows the error message when attempting to reference a model outside its supported access level.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/govern/model-access.md#2025-04-09_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndbt run -s marketing_model\n...\ndbt.exceptions.DbtReferenceError: Parsing Error\n  Node model.jaffle_shop.marketing_model attempted to reference node model.jaffle_shop.finance_model, \n  which is not allowed because the referenced node is private to the finance group.\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark-Specific Test Cases\nDESCRIPTION: Example of implementing tests specific to Spark/Databricks functionality\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/adapter-creation.md#2025-04-09_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n# Snapshots require access to the Delta file format, available on our Databricks connection,\n# so let's skip on Apache Spark\n@pytest.mark.skip_profile('apache_spark')\nclass TestSnapshotCheckColsSpark(BaseSnapshotCheckCols):\n    @pytest.fixture(scope=\"class\")\n    def project_config_update(self):\n        return {\n            \"seeds\": {\n                \"+file_format\": \"delta\",\n            },\n            \"snapshots\": {\n                \"+file_format\": \"delta\",\n            }\n        }\n```\n\n----------------------------------------\n\nTITLE: Example of 403 Forbidden Error Response in dbt Cloud API\nDESCRIPTION: This JSON response example shows what is returned when a service token request is made from an IP address that isn't on the allowlist. The response includes status code 403, a user message indicating access is denied, and data showing the account and user IDs involved in the request.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Troubleshooting/ip-restrictions.md#2025-04-09_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n        {\n            \"status\": {\n                \"code\": 403,\n                \"is_success\": False,\n                \"user_message\": (\"Forbidden: Access denied\"),\n                \"developer_message\": None,\n            },\n            \"data\": {\n                \"account_id\": <account_id>,\n                \"user_id\": <user_id>,\n                \"is_service_token\": <boolean describing if it's a service token request>,\n                \"account_access_denied\": True,\n            },\n        }\n```\n\n----------------------------------------\n\nTITLE: Setup Python Environment in Windows PowerShell (Shell)\nDESCRIPTION: Shell commands for creating and activating a Python virtual environment, updating pip, and installing dependencies in Windows PowerShell.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-10-19-polyglot-dbt-python-dataframes-and-sql.md#2025-04-09_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\npython -m venv env\nenv\\Scripts\\Activate.ps1\npython -m pip install --upgrade pip\npython -m pip install -r requirements.txt\nenv\\Scripts\\Activate.ps1\n```\n\n----------------------------------------\n\nTITLE: Invalid SQL Query - Runtime Error Example\nDESCRIPTION: Illustrates a SQL query with an impossible date value that can only be caught by a Level 3 Executor.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2025-01-23-levels-of-sql-comprehension.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nselect cast('2025-01-32' as date) as tomorrow\n```\n\n----------------------------------------\n\nTITLE: dbt Project Configuration\nDESCRIPTION: YAML configuration for setting materialization type for dimensional models in the marts schema.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-04-18-building-a-kimball-dimensional-model-with-dbt.md#2025-04-09_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  adventureworks:\n    marts:\n      +materialized: table\n      +schema: marts\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Metric in dbt Semantic Layer YAML\nDESCRIPTION: This YAML snippet demonstrates how to define a simple metric for counting the total number of queries run in the Semantic Layer. It uses a measure called 'queries' and sets up metadata like name, description, and label.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-12-11-semantic-layer-on-semantic-layer.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  - name: queries\n    description: The total number of queries run\n    type: simple\n    label: Semantic Layer Queries\n    type_params:\n      measure: queries\n```\n\n----------------------------------------\n\nTITLE: Auto-Generated SQL Query for Microbatch Processing in dbt\nDESCRIPTION: Example of an auto-generated SQL query created by dbt when using the microbatch strategy. This shows how dbt automatically filters records for a specific day (2024-10-01) based on the configured event_time field.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/incremental-microbatch.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\nselect * from (\\n    select * from \"analytics\".\"stg_events\"\\n    where my_time_field >= '2024-10-01 00:00:00'\\n      and my_time_field < '2024-10-02 00:00:00'\\n)\\n\n```\n\n----------------------------------------\n\nTITLE: Generating Thread Summaries using Snowflake Cortex LLM Function\nDESCRIPTION: SQL query utilizing Snowflake's cortex.complete() function with the llama2-70b-chat model to generate concise summaries of Slack thread conversations. The query combines thread content with participant metadata to produce contextual summaries.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-02-29-cortex-slack.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n        select trim(\n            snowflake.cortex.complete(\n                'llama2-70b-chat',\n                concat(\n                    'Write a short, two sentence summary of this Slack thread. Focus on issues raised. Be brief. <thread>',\n                    text_to_summarize,\n                    '</thread>. The users involved are: <users>',\n                    participant_metadata.participant_users::text,\n                    '</users>'\n                )\n            )\n        ) as thread_summary,\n```\n\n----------------------------------------\n\nTITLE: Recommended Relative Path Configuration\nDESCRIPTION: Example of the recommended way to specify seed paths using relative paths in the project directory.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/seed-paths.md#2025-04-09_snippet_1\n\nLANGUAGE: yml\nCODE:\n```\nseed-paths: [\"seed\"]\n```\n\n----------------------------------------\n\nTITLE: SQL AND Operator Example Query\nDESCRIPTION: Example query using the AND operator to filter orders from the Jaffle Shop sample dataset where status is 'shipped' and amount is greater than $20.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/operators/sql-and.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\torder_id,\n\tstatus,\n\tround(amount) as amount\nfrom {{ ref('orders') }}\nwhere status = 'shipped' and amount > 20\nlimit 3\n```\n\n----------------------------------------\n\nTITLE: Showing Directory Structure of a dbt Project with requirements.txt\nDESCRIPTION: Illustrates the typical file structure of a dbt project, showing the location of requirements.txt alongside other key files like dbt_project.yml and folders for models and tests.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/duckdb-qs.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n/my_dbt_project/\n dbt_project.yml\n models/\n    my_model.sql\n tests/\n    my_test.sql\n requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Defining dbt Sources for Slack Messages in YAML\nDESCRIPTION: This YAML snippet defines the dbt sources for the Slack messages table in BigQuery. It specifies the database, schema, and table name for the slack_messages table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-11-29-open-source-community-growth.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  - name: metrics\n    database: openlineage\n    schema: metrics\n    tables:\n      - name: slack_messages\n```\n\n----------------------------------------\n\nTITLE: Runtime Error Message in dbt Packages\nDESCRIPTION: Error message that appears when there's a version incompatibility between dbt_utils package and dbt Cloud version. The error indicates an invalid config version mismatch.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Troubleshooting/runtime-packages.yml.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nRunning with dbt=xxx\nRuntime Error\n  Failed to read package: Runtime Error\n    Invalid config version: 1, expected 2  \n  Error encountered in dbt_utils/dbt_project.yml\n```\n\n----------------------------------------\n\nTITLE: DATEADD Function in BigQuery SQL\nDESCRIPTION: Illustrates the syntax for the date_add function in BigQuery. It uses from_date, INTERVAL, interval, and datepart as parameters. Does not support dateparts less than a day.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/date-functions/sql-dateadd.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\ndate_add( {{ from_date }}, INTERVAL {{ interval }} {{ datepart }} )\n```\n\n----------------------------------------\n\nTITLE: Setup Python Environment in POSIX csh/tcsh (Shell)\nDESCRIPTION: Shell commands for creating and activating a Python virtual environment, updating pip, and installing dependencies in csh/tcsh shells.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-10-19-polyglot-dbt-python-dataframes-and-sql.md#2025-04-09_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\npython3 -m venv env\nsource env/bin/activate.csh\npython3 -m pip install --upgrade pip\npython3 -m pip install -r requirements.txt\nsource env/bin/activate.csh\n```\n\n----------------------------------------\n\nTITLE: Resolving dict object macro error\nDESCRIPTION: Error occurs when a macro doesn't exist, usually due to namespace changes or missing dependencies. Resolution involves updating the macro reference to use the dbt namespace.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/core-upgrade/11-Older versions/upgrading-to-dbt-utils-v1.0.md#2025-04-09_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndbt_utils.MACRO_NAME() -> dbt.MACRO_NAME()\n```\n\n----------------------------------------\n\nTITLE: Configuring Analysis Paths with Relative Path\nDESCRIPTION: Demonstrates the recommended way to configure analysis-paths using a relative path. This approach is more portable and easier to manage across different environments.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/analysis-paths.md#2025-04-09_snippet_1\n\nLANGUAGE: yml\nCODE:\n```\nanalysis-paths: [\"analyses\"]\n```\n\n----------------------------------------\n\nTITLE: Models Health Criteria Table in Markdown\nDESCRIPTION: Markdown table defining the health state criteria for dbt models, including states for Healthy, Caution, Degraded, and Unknown conditions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/data-health-signals.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| **Health state** | **Criteria**   |\n|-------------------|---------------|\n|  **Healthy**    | All of the following must be true:<br /><br /> - Built successfully in the last run<br />- Built in the last 30 days<br />- Model has tests configured<br />- All tests passed<br />- All upstream [sources are fresh](/docs/build/sources#source-data-freshness) or freshness is not applicable (set to `null`)<br />- Has a description |\n|  **Caution**   | One of the following must be true: <br /><br />- Not built in the last 30 days<br />- Tests are not configured<br />- Tests return warnings<br />- One or more upstream sources are stale:<br />&nbsp;&nbsp;&nbsp;&nbsp;- Has a freshness check configured<br />&nbsp;&nbsp;&nbsp;&nbsp;- Freshness check ran in the past 30 days<br />&nbsp;&nbsp;&nbsp;&nbsp;- Freshness check returned a warning<br />- Missing a description |\n|  **Degraded**  | One of the following must be true: <br /><br />- Model failed to build<br />- Model has failing tests<br />- One or more upstream sources are stale:<br />&nbsp;&nbsp;&nbsp;&nbsp;- Freshness check hasn't run in the past 30 days<br />&nbsp;&nbsp;&nbsp;&nbsp;- Freshness check returned an error |\n|  **Unknown**    | - Unable to determine health of resource; no job runs have processed the resource.         |\n```\n\n----------------------------------------\n\nTITLE: Filtering Data with SQL OR Operator in dbt\nDESCRIPTION: This query demonstrates how to use the OR operator in a WHERE clause to filter orders with status 'shipped' or 'completed'. It references the Jaffle Shop's 'orders' table using dbt's ref function and limits the results to 3 records.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/operators/sql-or.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\torder_id,\n\tcustomer_id,\n\torder_date,\n\tstatus,\n\tamount\nfrom {{ ref('orders') }}\nwhere status = 'shipped' or status = 'completed'\nlimit 3\n```\n\n----------------------------------------\n\nTITLE: Listing Dimensions for a Metric in dbt Semantic Layer\nDESCRIPTION: This command lists all available dimensions for a given metric in the dbt Semantic Layer. It's useful for checking dimensionality of metrics.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-build-our-metrics/semantic-layer-2-setup.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndbt sl list dimensions --metrics [metric name]\n```\n\n----------------------------------------\n\nTITLE: Complete Dimension Table SQL in dbt\nDESCRIPTION: Complete SQL transformation code for creating a dimension table including surrogate keys and selected columns.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-04-18-building-a-kimball-dimensional-model-with-dbt.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\n...\n\nselect\n    {{ dbt_utils.generate_surrogate_key(['stg_product.productid']) }} as product_key, \n    stg_product.productid,\n    stg_product.name as product_name,\n    stg_product.productnumber,\n    stg_product.color,\n    stg_product.class,\n    stg_product_subcategory.name as product_subcategory_name,\n    stg_product_category.name as product_category_name\nfrom stg_product\nleft join stg_product_subcategory on stg_product.productsubcategoryid = stg_product_subcategory.productsubcategoryid\nleft join stg_product_category on stg_product_subcategory.productcategoryid = stg_product_category.productcategoryid\n```\n\n----------------------------------------\n\nTITLE: Granting Postgres Permissions for dbt Usage\nDESCRIPTION: This SQL snippet demonstrates how to grant necessary permissions for a user to run dbt in a Postgres database. It includes granting connect privileges, read access to source schema, and write access to destination schema. The code also shows how to create a new schema and set ownership.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/database-permissions/postgres-permissions.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\ngrant connect on database database_name to user_name;\n\n-- Grant read permissions on the source schema\ngrant usage on schema source_schema to user_name;\ngrant select on all tables in schema source_schema to user_name;\nalter default privileges in schema source_schema grant select on tables to user_name;\n\n-- Create destination schema and make user_name the owner\ncreate schema if not exists destination_schema;\nalter schema destination_schema owner to user_name;\n\n-- Grant write permissions on the destination schema\ngrant usage on schema destination_schema to user_name;\ngrant create on schema destination_schema to user_name;\ngrant insert, update, delete, truncate on all tables in schema destination_schema to user_name;\nalter default privileges in schema destination_schema grant insert, update, delete, truncate on tables to user_name;\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Cloud Environments for Workflow Initiative Organization\nDESCRIPTION: This configuration sets up dbt Cloud environments with CI schemas populating in the QA database, reflecting the workflow initiative. It uses separate databases for development, QA/CI, and production.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2025-01-28-git-branching-strategies-and-dbt.md#2025-04-09_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n| Environment Name | **Database** | **Schema** |\n| --- | --- | --- |\n| Development | `development` | User-specified in Profile Settings > Credentials |\n| Feature CI | `qa` | Any safe default, like `dev_ci` (it doesn't even have to exist). The job we intend to set up will override the schema here anyway to denote the unique PR. |\n| Quality Assurance | `qa` | `analytics` |\n| Release CI | `qa` | A safe default |\n| Production | `production` | `analytics` |\n```\n\n----------------------------------------\n\nTITLE: Using DATE_TRUNC in Google BigQuery and Amazon Redshift\nDESCRIPTION: The syntax for using the DATE_TRUNC function in Google BigQuery and Amazon Redshift, where the date/time field is passed as the first argument followed by the date part.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-07-05-date-trunc-sql-love-letter.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\ndate_trunc(<date/time field>, <date part>)\n```\n\n----------------------------------------\n\nTITLE: Enhanced get_payment_methods Macro with Result Transformation in dbt\nDESCRIPTION: Improved version of the get_payment_methods macro that transforms query results into a list and handles execution context.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/using-jinja.md#2025-04-09_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\n{% macro get_payment_methods() %}\n\n{% set payment_methods_query %}\nselect distinct\npayment_method\nfrom {{ ref('raw_payments') }}\norder by 1\n{% endset %}\n\n{% set results = run_query(payment_methods_query) %}\n\n{% if execute %}\n{# Return the first column #}\n{% set results_list = results.columns[0].values() %}\n{% else %}\n{% set results_list = [] %}\n{% endif %}\n\n{{ return(results_list) }}\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: DATEADD Function Syntax in BigQuery\nDESCRIPTION: The syntax for the date_add function in BigQuery which requires from_date, INTERVAL keyword, interval value, and datepart. Does not support dateparts less than a day.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-09-11-sql-dateadd.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\ndate_add( {{ from_date }}, INTERVAL {{ interval }} {{ datepart }} )\n```\n\n----------------------------------------\n\nTITLE: SQL CTE Alternative to HAVING\nDESCRIPTION: Demonstrates an alternative approach using CTE (Common Table Expression) to achieve the same result as HAVING clause.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/clauses/sql-having.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nwith counts as (\n\tselect\n\t\tcustomer_id,\n\t\tcount(order_id) as num_orders\n\tfrom {{ ref('orders') }}\n\tgroup by 1\n)\nselect\n\tcustomer_id,\n\tnum_orders\nfrom counts\nwhere num_orders > 1\n```\n\n----------------------------------------\n\nTITLE: Date Parsing Model with dateutil in dbt Python\nDESCRIPTION: A dbt Python model that uses the dateutil library to parse transaction time strings from various formats into standardized datetime objects. The function tries to parse each string and returns None when parsing fails.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-10-19-polyglot-dbt-python-dataframes-and-sql.md#2025-04-09_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dateutil\n\n\ndef try_dateutil_parse(x):\n    try:\n        return dateutil.parser.parse(x)\n    except:\n        return\n\n\ndef model(dbt, session):\n    df = dbt.ref(\"source_data\")\n\n    df['parsed_transaction_time'] = df['transaction_time'].apply(try_dateutil_parse)\n\n    return df\n```\n\n----------------------------------------\n\nTITLE: Basic Source Override Structure in dbt YAML Configuration\nDESCRIPTION: Basic structure for overriding a source from an included package in dbt. This snippet demonstrates the syntax for specifying which source to override and from which package, with placeholders for database and schema configurations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/overrides.md#2025-04-09_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: <source_name>\n    overrides: <package name>\n\n    database: ...\n    schema: ...\n```\n\n----------------------------------------\n\nTITLE: Disabling Cache Population in dbt CLI\nDESCRIPTION: Command to compile a specific model without populating the metadata cache, useful for cases where database metadata or introspective queries are not required.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/global-configs/cache.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ndbt --no-populate-cache compile --select my_model_name\n```\n\n----------------------------------------\n\nTITLE: SQL Self Join with Products Example\nDESCRIPTION: Practical example of a self join using a products table to connect child SKUs with their parent products. Uses dbt ref syntax and demonstrates how to join parent product names to child SKUs.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/joins/sql-self-join.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect\n   products.sku_id,\n   products.sku_name,\n   products.parent_id,\n   parents.sku_name as parent_name\nfrom {{ ref('products') }} as products\nleft join {{ ref('products') }} as parents\non products.parent_id = parents.sku_id\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Oracle Driver in Thin Mode (Bash)\nDESCRIPTION: Sets the ORA_PYTHON_DRIVER_TYPE environment variable to 'thin' for the Python Oracle driver. This mode does not require Oracle Client libraries.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/oracle-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport ORA_PYTHON_DRIVER_TYPE=thin # default\n```\n\n----------------------------------------\n\nTITLE: Creating Fact Table Documentation and Tests in YAML\nDESCRIPTION: A YAML file that defines documentation and tests for the fct_sales model, including surrogate key uniqueness tests and foreign key tests to ensure data integrity.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-04-18-building-a-kimball-dimensional-model-with-dbt.md#2025-04-09_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: fct_sales\n    columns:\n\n      - name: sales_key\n        description: The surrogate key of the fct sales\n        tests:\n          - not_null\n          - unique\n\n      - name: product_key\n        description: The foreign key of the product\n        tests:\n          - not_null\n\n      - name: customer_key\n        description: The foreign key of the customer\n        tests:\n          - not_null \n      \n      ... \n\n      - name: orderqty\n        description: The quantity of the product \n        tests:\n          - not_null\n\n      - name: revenue\n        description: The revenue obtained by multiplying unitprice and orderqty\n```\n\n----------------------------------------\n\nTITLE: Documenting Standard Macro in dbt YAML Schema\nDESCRIPTION: This snippet demonstrates how to document a standard macro named 'cents_to_dollars' in a dbt schema file. It includes the macro name, description, and details about its arguments.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Docs/documenting-macros.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmacros:\n  - name: cents_to_dollars\n    description: A macro to convert cents to dollars\n    arguments:\n      - name: column_name\n        type: column\n        description: The name of the column you want to convert\n      - name: precision\n        type: integer\n        description: Number of decimal places. Defaults to 2.\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Cloud Environments for 1:1 QA and Main Asset Mapping\nDESCRIPTION: This configuration sets up dbt Cloud environments with separate databases for development/CI, QA, and production. It keeps QA and production databases aligned with their corresponding branches.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2025-01-28-git-branching-strategies-and-dbt.md#2025-04-09_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n| Environment Name | **Database** | **Schema** |\n| --- | --- | --- |\n| Development | `development` | User-specified in Profile Settings > Credentials |\n| Feature CI | `development` | Any safe default, like `dev_ci` (it doesn't even have to exist). The job we intend to set up will override the schema here anyway to denote the unique PR. |\n| Quality Assurance | `qa` | `analytics` |\n| Release CI | `development` | A safe default |\n| Production | `production` | `analytics` |\n```\n\n----------------------------------------\n\nTITLE: Using DATE_TRUNC in Snowflake and Databricks\nDESCRIPTION: The syntax for using the DATE_TRUNC function in Snowflake and Databricks platforms, where the date_part is passed as the first argument followed by the date/time field.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-07-05-date-trunc-sql-love-letter.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\ndate_trunc(<date_part>, <date/time field>)\n```\n\n----------------------------------------\n\nTITLE: Basic COALESCE SQL Function Syntax\nDESCRIPTION: The general syntax for using the COALESCE function. It takes multiple inputs and returns the first non-null value.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-06-30-coalesce-sql.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\ncoalesce(<input_1>, <input_2>,...<input_n>)\n```\n\n----------------------------------------\n\nTITLE: Versioning Content Blocks with VersionBlock Component (Markdown)\nDESCRIPTION: Example of using the VersionBlock component in Markdown to version specific content blocks. This example shows how to set a version range for content visibility.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/contributing/single-sourcing-content.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n<VersionBlock firstVersion=\"1.5\" lastVersion=\"1.9\">\n\n\tVersioned content here\n\n</VersionBlock>\n```\n\n----------------------------------------\n\nTITLE: Adding Unique Tests in dbt YAML Configuration\nDESCRIPTION: YAML configuration for adding unique and not_null tests to the grain_id column in a dbt model, ensuring data integrity after deduplication.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-04-19-complex-deduplication.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: my_model\n    columns:\n      - name: grain_id\n        tests:\n          - unique\n          - not_null\n```\n\n----------------------------------------\n\nTITLE: AWS IAM Policy for Glue Monitoring\nDESCRIPTION: IAM policy JSON to enable CloudWatch metrics and logs for AWS Glue monitoring. Includes permissions for PutMetricData, CreateLogStream, CreateLogGroup, PutLogEvents, and S3 access for Spark UI logs.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/glue-setup.md#2025-04-09_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"CloudwatchMetrics\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"cloudwatch:PutMetricData\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"cloudwatch:namespace\": \"Glue\"\n                }\n            }\n        },\n        {\n            \"Sid\": \"CloudwatchLogs\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:PutObject\",\n                \"logs:CreateLogStream\",\n                \"logs:CreateLogGroup\",\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:*:*:/aws-glue/*\",\n                \"arn:aws:s3:::bucket-to-write-sparkui-logs/*\"\n            ]\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Parsing API Response in SQL Model\nDESCRIPTION: This SQL model parses the JSON response from the Carbon Intensity API stored in the previous Python model. It extracts relevant information and stores it in a structured format, using dbt's incremental materialization strategy.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-06-12-putting-your-dag-on-the-internet.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        materialized='incremental',\n        unique_key='dbt_invocation_id'\n    )\n}}\n\nwith raw as (\n    select parse_json(carbon_intensity) as carbon_intensity_json\n    from {{ ref('external_access_demo') }}\n)\n\nselect\n    '{{ invocation_id }}' as dbt_invocation_id,\n    value:from::TIMESTAMP_NTZ as start_time,\n    value:to::TIMESTAMP_NTZ as end_time,\n    value:intensity.actual::NUMBER as actual_intensity,\n    value:intensity.forecast::NUMBER as forecast_intensity,\n    value:intensity.index::STRING as intensity_index\nfrom raw,\n    lateral flatten(input => raw.carbon_intensity_json:data)\n```\n\n----------------------------------------\n\nTITLE: Model Reference Example with Multiple Sources\nDESCRIPTION: Shows how to reference multiple models in a dbt SQL file\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-01-09-defer-in-development.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n# in models/model_f.sql\nwith \n\nmodel_b as (\n select * from {{ ref('model_b') }}\n),\n\nmodel_c as (\n select * from {{ ref('model_c') }}\n),\n\n...\n```\n\n----------------------------------------\n\nTITLE: Creating Service Account and Granting IAM Permissions in Google Cloud\nDESCRIPTION: A series of gcloud commands to create a service account and grant necessary IAM permissions for using BigQuery Dataframes with dbt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-bigframes.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngcloud iam service-accounts create dbt-bigframes-sa\ngcloud projects add-iam-policy-binding ${GOOGLE_CLOUD_PROJECT} --member=serviceAccount:dbt-bigframes-sa@${GOOGLE_CLOUD_PROJECT}.iam.gserviceaccount.com --role=roles/bigquery.user\ngcloud projects add-iam-policy-binding ${GOOGLE_CLOUD_PROJECT} --member=serviceAccount:dbt-bigframes-sa@${GOOGLE_CLOUD_PROJECT}.iam.gserviceaccount.com --role=roles/bigquery.dataEditor\ngcloud projects add-iam-policy-binding ${GOOGLE_CLOUD_PROJECT} --member=serviceAccount:dbt-bigframes-sa@${GOOGLE_CLOUD_PROJECT}.iam.gserviceaccount.com --role=roles/iam.serviceAccountUser\ngcloud projects add-iam-policy-binding ${GOOGLE_CLOUD_PROJECT} --member=serviceAccount:dbt-bigframes-sa@${GOOGLE_CLOUD_PROJECT}.iam.gserviceaccount.com --role=roles/aiplatform.colabEnterpriseUser\n```\n\n----------------------------------------\n\nTITLE: Converting Customer Names to Lowercase with SQL LOWER Function in dbt\nDESCRIPTION: A practical example showing how to use the LOWER function to standardize first and last name fields in the Jaffle Shop's customers model. This creates consistency in the data and makes it easier for business users to filter and query.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/string-functions/sql-lower.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect \n\tcustomer_id,\n\tlower(first_name) as first_name,\n\tlower(last_name) as last_name\nfrom {{ ref('customers') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Snowflake SSO Authentication with Okta URL in dbt (v1.8 and earlier)\nDESCRIPTION: This YAML configuration sets up SSO authentication for Snowflake using an Okta URL in dbt versions 1.8 and earlier. It specifies the 'authenticator' as the Okta account URL and includes Okta username and password along with other Snowflake connection parameters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/snowflake-setup.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nmy-snowflake-db:\n  target: dev\n  outputs:\n    dev:\n      type: snowflake\n      account: [account id] # Snowflake <account_name>\n      user: [username] # Snowflake username\n      role: [user role] # Snowflake user role\n\n      # SSO config -- The three following fields are REQUIRED\n      authenticator: [Okta account URL]\n      username: [Okta username]\n      password: [Okta password]\n\n      database: [database name] # Snowflake database name\n      warehouse: [warehouse name] # Snowflake warehouse name\n      schema: [dbt schema]\n      threads: [between 1 and 8]\n      client_session_keep_alive: False\n      query_tag: [anything]\n\n      # optional\n      connect_retries: 0 # default 0\n      connect_timeout: 10 # default: 10\n      retry_on_database_errors: False # default: false\n      retry_all: False  # default: false\n      reuse_connections: False\n```\n\n----------------------------------------\n\nTITLE: Cloning dbt Dimensional Modeling Repository\nDESCRIPTION: Command to clone the example repository containing AdventureWorks dimensional modeling project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-04-18-building-a-kimball-dimensional-model-with-dbt.md#2025-04-09_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ngit clone https://github.com/Data-Engineer-Camp/dbt-dimensional-modelling.git\\ncd dbt-dimensional-modelling/adventureworks\n```\n\n----------------------------------------\n\nTITLE: GenericContribute Permission JSON Structure\nDESCRIPTION: JSON representation of the GenericContribute permission in Azure DevOps, showing the bit value, display name, and permission name used in the Git Repositories namespace.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/git/setup-azure-service-user.md#2025-04-09_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"bit\": 4,\n    \"displayName\": \"Contribute\",\n    \"name\": \"GenericContribute\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Cloud Environments for Indirect Promotion\nDESCRIPTION: This table outlines the configuration for dbt Cloud environments in an Indirect Promotion branching strategy. It specifies the environment name, type, deployment type, base branch, and purpose for each environment.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2025-01-28-git-branching-strategies-and-dbt.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Environment Name | [Environment Type](https://docs.getdbt.com/docs/dbt-cloud-environments#types-of-environments) | [Deployment Type](https://docs.getdbt.com/docs/deploy/deploy-environments#staging-environment) | Base Branch | Will handle |\n| --- | --- | --- | --- | --- |\n| Development | development | - | `qa` | Operations done in the IDE (including creating feature branches) |\n| Feature CI | deployment | General | `qa` | A continuous integration job |\n| Quality Assurance | deployment | Staging | `qa` | A deployment job |\n| Release CI | deployment | General | `main` | A continuous integration job |\n| Production | deployment | Production | `main` | A deployment job |\n```\n\n----------------------------------------\n\nTITLE: Key-value Definition in YAML Selectors\nDESCRIPTION: A simple key-value definition syntax for selectors that doesn't support graph or set operators.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/yaml-selectors.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ndefinition:\n  tag: nightly\n```\n\n----------------------------------------\n\nTITLE: Installing dbt-glue Adapter Locally for Testing in Bash\nDESCRIPTION: Commands to build and install the dbt-glue adapter locally for testing purposes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/glue-setup.md#2025-04-09_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\n$ python3 setup.py build && python3 setup.py install_lib\n```\n\n----------------------------------------\n\nTITLE: Using the ref() Function in dbt Models\nDESCRIPTION: The ref() function is used in dbt models to create dependencies between models, allowing for modular code structure. This is introduced in the toddlerhood stage of dbt project maturity to break repeated logic into separate models.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-12-05-how-to-build-a-mature-dbt-project-from-scratch.md#2025-04-09_snippet_0\n\nLANGUAGE: jinja-sql\nCODE:\n```\n{{ ref() }}\n```\n\n----------------------------------------\n\nTITLE: Formatting for Oracle ML Cloud Service URL\nDESCRIPTION: Shows the structure for an Oracle ML Cloud Service URL, which includes the tenancy ID, database name, datacenter region, and root domain.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/oracle-setup.md#2025-04-09_snippet_18\n\nLANGUAGE: text\nCODE:\n```\nhttps://tenant1-dbt.adb.us-sanjose-1.oraclecloudapps.com\n```\n\n----------------------------------------\n\nTITLE: Attempting to Reference a Private Model from Another Group\nDESCRIPTION: SQL example showing an attempt to reference a private finance model from a marketing model, which will fail due to access restrictions defined in the model access control configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-01-evolving-data-engineer-craft.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\n--models/marketing/marketing_model.sql\n\nselect * from {{ ref('finance_model') }}\n```\n\n----------------------------------------\n\nTITLE: Unit Test Description Configuration in YAML\nDESCRIPTION: Shows how to add descriptions to unit tests with input and expected output configurations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/description.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nunit_tests:\n  - name: unit_test_name\n    description: \"markdown_string\"\n    model: model_name \n    given: ts\n      - input: ref_or_source_call\n        rows:\n         - {column_name: column_value}\n         - {column_name: column_value}\n         - {column_name: column_value}\n         - {column_name: column_value}\n      - input: ref_or_source_call\n        format: csv\n        rows: dictionary | string\n    expect: \n      format: dict | csv | sql\n      fixture: fixture_name\n```\n\n----------------------------------------\n\nTITLE: Complete Snapshot Example with invalidate_hard_deletes (YAML)\nDESCRIPTION: Full example of a snapshot configuration using YAML format for dbt version 1.9+, including all relevant snapshot settings.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/invalidate_hard_deletes.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  - name: orders_snapshot\n    relation: source('jaffle_shop', 'orders')\n    config:\n      schema: snapshots\n      database: analytics\n      unique_key: id\n      strategy: timestamp\n      updated_at: updated_at\n      invalidate_hard_deletes: true\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variable for psycopg2 Installation\nDESCRIPTION: Bash command for versions 1.7 and earlier to install dbt-postgres with psycopg2 instead of psycopg2-binary by setting the DBT_PSYCOPG2_NAME environment variable.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/postgres-setup.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nDBT_PSYCOPG2_NAME=psycopg2 pip install dbt-postgres\n```\n\n----------------------------------------\n\nTITLE: Debugging Database Error in dbt SQL Model\nDESCRIPTION: This snippet demonstrates a database error caused by a syntax error in the SQL of a model. The error message provides details about the location of the syntax error in the compiled SQL file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/debug-errors.md#2025-04-09_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n$ dbt run\n...\nCompleted with 1 error and 0 warnings:\n\nDatabase Error in model customers (models/customers.sql)\n  001003 (42000): SQL compilation error:\n  syntax error line 14 at position 4 unexpected 'from'.\n  compiled SQL at target/run/jaffle_shop/models/customers.sql\n```\n\n----------------------------------------\n\nTITLE: Demonstrating SQL Join Order Optimization\nDESCRIPTION: This SQL snippet demonstrates a simple join operation between three tables. It's used to illustrate how query optimization can reorder joins for better performance based on table sizes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2025-01-24-sql-comprehension-technologies.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect \n  *\nfrom a\njoin b on a.id = b.a_id\njoin c on b.id = c.b_id\n```\n\n----------------------------------------\n\nTITLE: PyPI Downloads Incremental Model\nDESCRIPTION: dbt SQL model that incrementally processes PyPI download data for specific packages, filtering for relevant projects and timestamps.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-11-29-open-source-community-growth.md#2025-04-09_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        materialized='incremental'\n    )\n}}\n\nselect timestamp, country_code, url,\nfile.project as project, file.version as version,\nfile.type as type\n\nfrom {{ source('pypi', 'file_downloads') }}\n\nwhere (\n    file.project = 'marquez-python'\n    or file.project = 'marquez-airflow'\n)\n\nand timestamp > TIMESTAMP_SECONDS(1549497600)\n\n{% if is_incremental() %}\n  and timestamp > (select max(timestamp) from {{ this }})\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Avoiding Direct Queries to Raw Tables in dbt\nDESCRIPTION: Example showing what not to do in dbt models - directly querying raw tables instead of using refs and sources. Best practice is to always use refs and sources for all table references.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-02-05-dbt-project-checklist.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n--  Don't do this\nselect * from raw.stripe.payments\n\n--  Instead do this\nselect * from {{ source('stripe', 'payments') }}\n```\n\n----------------------------------------\n\nTITLE: Handling Error with zip_strict Context Method in Jinja\nDESCRIPTION: Example showing the error that occurs when using zip_strict with a non-iterable value. Unlike the regular zip method with a default parameter, zip_strict will raise a compilation error when invalid inputs are provided.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/zip.md#2025-04-09_snippet_3\n\nLANGUAGE: jinja\nCODE:\n```\n{% set my_list_a = 12 %}\n{% set my_list_b = ['alice', 'bob'] %}\n{% set my_zip = zip_strict(my_list_a, my_list_b) %}\n\nCompilation Error in ... (...)\n  'int' object is not iterable\n```\n\n----------------------------------------\n\nTITLE: Configuring Even Distribution Style in dbt Redshift Models\nDESCRIPTION: Sets up a table materialization with 'even' distribution style, which distributes data equally across nodes in a round-robin fashion. This is good for well-rounded workloads where specific data placement isn't critical.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-05-19-redshift-configurations-dbt-model-optimizations.md#2025-04-09_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n{{ config(materialized='table', dist='even') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Not_null and Unique Tests for Primary Keys in dbt YAML\nDESCRIPTION: This snippet demonstrates how to configure not_null and unique tests for primary keys in dbt models using YAML configuration. These tests ensure that primary key fields don't contain null values and maintain uniqueness across all rows.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-11-22-primary-keys.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: orders\n    columns:\n      - name: order_id\n        tests:\n          - unique\n          - not_null\n```\n\n----------------------------------------\n\nTITLE: Markdown Document Structure for Writing Contributions\nDESCRIPTION: Structured markdown document outlining the contribution guidelines for dbt's documentation, including sections for product documentation and developer blog contributions. Contains headers, links, and formatted text.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/community/contributing/contributing-writing.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: \"Writing contributions\"\nid: \"contributing-writing\"\n---\n\n### Contribute to the product documentation\n\n#### Overview\n\nThe [dbt Product Documentation](https://docs.getdbt.com/docs/introduction) sits at the heart of how people learn to use and engage with dbt. From explaining dbt to newcomers to providing references for advanced functionality and APIs, the product docs are a frequent resource for _every_ dbt Developer.\n```\n\n----------------------------------------\n\nTITLE: Refactoring Customer Model to Use Staging Models in SQL\nDESCRIPTION: This SQL code refactors the customer model to use the staging models (stg_customers and stg_orders) instead of directly querying raw tables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/snowflake-qs.md#2025-04-09_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nwith customers as (\n\n    select * from {{ ref('stg_customers') }}\n\n),\n\norders as (\n\n    select * from {{ ref('stg_orders') }}\n\n),\n\ncustomer_orders as (\n\n    select\n        customer_id,\n\n        min(order_date) as first_order_date,\n        max(order_date) as most_recent_order_date,\n        count(order_id) as number_of_orders\n\n    from orders\n\n    group by 1\n\n),\n\nfinal as (\n\n    select\n        customers.customer_id,\n        customers.first_name,\n        customers.last_name,\n        customer_orders.first_order_date,\n        customer_orders.most_recent_order_date,\n        coalesce(customer_orders.number_of_orders, 0) as number_of_orders\n\n    from customers\n\n    left join customer_orders using (customer_id)\n\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Staging Formula 1 Pit Stops Data\nDESCRIPTION: Creates a staging model for Formula 1 pit stops data, including duration in both seconds and milliseconds\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\nwith\n\nsource  as (\n\n    select * from {{ source('formula1','pit_stops') }}\n\n),\n\nrenamed as (\n    select\n        raceid as race_id,\n        driverid as driver_id,\n        stop as stop_number,\n        lap,\n        time as lap_time_formatted,\n        duration as pit_stop_duration_seconds,\n        milliseconds as pit_stop_milliseconds\n    from source\n)\n\nselect * from renamed\norder by pit_stop_duration_seconds desc\n```\n\n----------------------------------------\n\nTITLE: SQL File Reference Unit Test Configuration in YAML\nDESCRIPTION: Demonstrates how to reference an external SQL file for mock data in dbt unit tests. The fixture file should be located in the tests/fixtures directory.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/data-formats.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nunit_tests:\n  - name: test_my_model\n    model: my_model\n    given:\n      - input: ref('my_model_a')\n        format: sql\n        fixture: my_model_a_fixture\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure CLI Authentication for Azure SQL in dbt\nDESCRIPTION: This configuration uses Azure CLI credentials for authentication to Azure SQL. It requires installing the Azure CLI and logging in with 'az login' before running dbt commands.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/mssql-setup.md#2025-04-09_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: sqlserver\n      driver: 'ODBC Driver 18 for SQL Server' # (The ODBC Driver installed on your system)\n      server: hostname or IP of your server\n      port: 1433\n      database: exampledb\n      schema: schema_name\n      authentication: CLI\n```\n\n----------------------------------------\n\nTITLE: Model B SQL Definition\nDESCRIPTION: SQL model that aggregates data from model_a, demonstrating upstream dependency.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/defer.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\n    id,\n    count(*)\n\nfrom {{ ref('model_a') }}\ngroup by 1\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation Structure\nDESCRIPTION: Frontmatter and content structure for advanced dbt Semantic Layer documentation, including navigation cards for fill null values and metric filters topics.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/advanced-topics.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: \"Advanced data modeling\"\ndescription: \"Learn about advanced topics for dbt Semantic Layer and MetricFlow, such as modeling workflows and more.\"\npagination_prev: null\n---\n```\n\n----------------------------------------\n\nTITLE: Configuring a dbt Source for ML Model Results\nDESCRIPTION: Example of how to configure a dbt source to represent the predicted results table from a machine learning model. This helps maintain documentation and lineage within the dbt project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/maching-learning-dbt-baton-pass.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  - name: ml_results\n    schema: prediction_outputs\n    tables:\n      - name: churn_predictions\n```\n\n----------------------------------------\n\nTITLE: Basic SQL Model for Payment Method Amounts\nDESCRIPTION: A SQL query that calculates payment amounts by different payment methods (bank_transfer, credit_card, gift_card) for each order using case statements.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/using-jinja.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\norder_id,\nsum(case when payment_method = 'bank_transfer' then amount end) as bank_transfer_amount,\nsum(case when payment_method = 'credit_card' then amount end) as credit_card_amount,\nsum(case when payment_method = 'gift_card' then amount end) as gift_card_amount,\nsum(amount) as total_amount\nfrom {{ ref('raw_payments') }}\ngroup by 1\n```\n\n----------------------------------------\n\nTITLE: Implementing Catalog Data Models with Pydantic\nDESCRIPTION: Python code defining Pydantic models for representing the catalog.json structure including Stats, Column, Node, and Catalog classes\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-05-03-making-dbt-cloud-api-calls-using-dbt-cloud-cli.md#2025-04-09_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional, Dict, Any\nfrom pydantic import BaseModel, Field\n\nclass Stats(BaseModel):\n    \"\"\"Represent node stats in the Catalog.\"\"\"\n\n    id: str\n    label: str\n    value: Any\n    include: bool\n    description: str\n\n    def __str__(self):\n        return f\"{self.label}: {self.value}\"\n\n\nclass Column(BaseModel):\n    \"\"\"Represents a column in the Catalog.\"\"\"\n\n    type: str\n    index: int\n    name: str\n    comment: Optional[str]\n\n    def __str__(self):\n        return f\"{self.name} (type: {self.type}, index: {self.index}, comment: {self.comment})\"\n\n\nclass Node(BaseModel):\n    \"\"\"Represents a node in the Catalog.\"\"\"\n\n    unique_id: str\n    metadata: Dict[str, Optional[str]]\n    columns: Dict[str, Column]\n    stats: Dict[str, Stats]\n\n    @property\n    def name(self):\n        return self.metadata[\"name\"]\n\n    @property\n    def database(self):\n        return self.metadata[\"database\"]\n\n    @property\n    def schema(self):\n        return self.metadata[\"schema\"]\n\n    @property\n    def type(self):\n        return self.metadata[\"type\"]\n\n    def __gt__(self, other):\n        return self.name > other.name\n\n    def __lt__(self, other):\n        return self.name < other.name\n\n    def __str__(self):\n        return f\"{self.name} (type: {self.type}, schema: {self.schema}, database: {self.database})\"\n\n\nclass Catalog(BaseModel):\n    \"\"\"Represents a dbt catalog.json artifact.\"\"\"\n\n    metadata: Dict\n    nodes: Dict[str, Node]\n    sources: Dict[str, Node]\n    errors: Optional[Dict]\n```\n\n----------------------------------------\n\nTITLE: Integrating Modelbit API with dbt Macros\nDESCRIPTION: Code snippet showing how to integrate Modelbit's API for versioned ML models as external functions in SQL, which can be used within dbt macros.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/maching-learning-dbt-baton-pass.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{% macro predict_churn() %}\n    {{ modelbit.predict('churn_model', 'v1', [\n        'customer_id',\n        'daily_active_usage',\n        'num_users',\n        'amount_paid',\n        'historical_usage'\n    ]) }}\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Creating Tabbed Content for Multiple Resources in SQL and YAML\nDESCRIPTION: Shows how to use the Tabs and TabItem components to create a tabbed view for different resources like models and sources. This allows for organized presentation of related code snippets.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/contributing/adding-page-components.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n<Tabs\ndefaultValue=\"models\"\nvalues={[\n    { label: 'Models', value: 'models', },\n    { label: 'Sources', value:'sources', },\n]\n}>\n\n<TabItem value=\"models\">\n\n<File name='models/<modelname>.sql'>\n\n```\n\n{{ config(\n\n) }}\n\nselect ...\n\n\n```\n\n</File>\n\n<File name='dbt_project.yml'>\n\n```\nmodels:\n[resource-path](/reference/resource-configs/resource-path):\n\n\n```\n\n</File>\n\n</TabItem>\n\n<TabItem value=\"sources\">\n\n<File name='dbt_project.yml'>\n\n```\nsources:\n[resource-path](/reference/resource-configs/resource-path):\n\n\n```\n\n</File>\n\n</TabItem>\n\n</Tabs>\n```\n\n----------------------------------------\n\nTITLE: List of Command Line Options That Must Appear Before Subcommands\nDESCRIPTION: Shows the complete list of command line options that must now be specified before subcommands rather than after them in dbt v1.5.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-versions/core-upgrade/11-Older versions/10-upgrading-to-v1.5.md#2025-04-09_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n--cache-selected-only | --no-cache-selected-only\n--debug, -d | --no-debug\n--deprecated-print | --deprecated-no-print\n--enable-legacy-logger | --no-enable-legacy-logger\n--fail-fast, -x | --no-fail-fast\n--log-cache-events | --no-log-cache-events\n--log-format\n--log-format-file\n--log-level\n--log-level-file\n--log-path\n--macro-debugging | --no-macro-debugging\n--partial-parse | --no-partial-parse\n--partial-parse-file-path\n--populate-cache | --no-populate-cache\n--print | --no-print\n--printer-width\n--quiet, -q | --no-quiet\n--record-timing-info, -r\n--send-anonymous-usage-stats | --no-send-anonymous-usage-stats\n--single-threaded | --no-single-threaded\n--static-parser | --no-static-parser\n--use-colors | --no-use-colors\n--use-colors-file | --no-use-colors-file\n--use-experimental-parser | --no-use-experimental-parser\n--version, -V, -v\n--version-check | --no-version-check\n--warn-error\n--warn-error-options\n--write-json | --no-write-json\n```\n\n----------------------------------------\n\nTITLE: Overriding dbt_utils.star Macro in Tests\nDESCRIPTION: Example showing how to override the dbt_utils.star macro by explicitly setting column lists when the star macro cannot process mock input data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/unit-test-overrides.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nunit_tests:\n    - name: my_other_unit_test\n      model: my_model_that_uses_star\n      overrides:\n        macros:\n          # explicity set star to relevant list of columns\n          dbt_utils.star: col_a,col_b,col_c \n      ...\n```\n\n----------------------------------------\n\nTITLE: Installing dbt-extrica Package with pip\nDESCRIPTION: Command to install the dbt-extrica adapter package using pip package manager. This installation automatically includes dbt-core and other dependencies.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/extrica-setup.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install dbt-extrica\n```\n\n----------------------------------------\n\nTITLE: Metadata Generation Commands Table Markdown\nDESCRIPTION: Shows a markdown table mapping dbt Explorer features to required dbt commands that must be run successfully in the environment to generate necessary metadata.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/explore-projects.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| To view in Explorer | You must successfully run |\n|---------------------|---------------------------|\n| All metadata        |  [dbt build](/reference/commands/build), [dbt docs generate](/reference/commands/cmd-docs), and [dbt source freshness](/reference/commands/source#dbt-source-freshness) together as part of the same job in the environment\n| Model lineage, details, or results | [dbt run](/reference/commands/run) or [dbt build](/reference/commands/build) on a given model within a job in the environment |\n| Columns and statistics for models, sources, and snapshots| [dbt docs generate](/reference/commands/cmd-docs) within [a job](/docs/collaborate/build-and-view-your-docs) in the environment |\n| Test results | [dbt test](/reference/commands/test) or [dbt build](/reference/commands/build) within a job in the environment |\n| Source freshness results | [dbt source freshness](/reference/commands/source#dbt-source-freshness) within a job in the environment |\n| Snapshot details | [dbt snapshot](/reference/commands/snapshot) or [dbt build](/reference/commands/build) within a job in the environment |\n| Seed details | [dbt seed](/reference/commands/seed) or [dbt build](/reference/commands/build) within a job in the environment |\n```\n\n----------------------------------------\n\nTITLE: DBT YAML Syntax Error Example\nDESCRIPTION: Example of a YAML syntax error message in DBT, demonstrating how incorrect punctuation leads to parsing failures.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_configs-properties.md#2025-04-09_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nRuntime Error\n  Syntax error near line 6\n  ------------------------------\n  5  |   - name: events\n  6  |     description; \"A table containing clickstream events from the marketing website\"\n  7  |\n\n  Raw Error:\n  ------------------------------\n  while scanning a simple key\n    in \"<unicode string>\", line 6, column 5:\n          description; \"A table containing clickstream events from the marketing website\"\n          ^\n```\n\n----------------------------------------\n\nTITLE: Fetching Metric Metadata in dbt Semantic Layer\nDESCRIPTION: Query to fetch name and dimensions for a specific metric 'food_order_amount' using the metrics() function.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-jdbc.md#2025-04-09_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nselect name, dimensions from {{ \n\tsemantic_layer.metrics() \n\t}}\n\tWHERE name='food_order_amount'\n```\n\n----------------------------------------\n\nTITLE: Implementing dbt-cloud-cli Command with Pydantic and Click\nDESCRIPTION: This Python code snippet demonstrates how dbt-cloud-cli commands are implemented using pydantic models and click decorators. It shows the pattern used to create CLI commands that correspond to dbt Cloud API endpoints.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-05-03-making-dbt-cloud-api-calls-using-dbt-cloud-cli.md#2025-04-09_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport click\nfrom dbt_cloud.command import DbtCloudJobGetCommand\n\n@click.group()\ndef dbt_cloud():\n    pass\n\n@dbt_cloud.group()\ndef job():\n    pass\n\n@job.command(help=DbtCloudJobGetCommand.get_description())\n@DbtCloudJobGetCommand.click_options\ndef get(**kwargs):\n    command = DbtCloudJobGetCommand.from_click_options(**kwargs)\n    execute_and_print(command)\n```\n\n----------------------------------------\n\nTITLE: Defining Multiple Metric Types in YAML for dbt Semantic Layer\nDESCRIPTION: This YAML configuration defines various metric types including simple, ratio, cumulative, and derived metrics. Each metric includes properties such as name, description, label, and type-specific parameters.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/sl-snowflake-qs.md#2025-04-09_snippet_22\n\nLANGUAGE: yaml\nCODE:\n```\n# Newly added          \nmetrics: \n  # Simple type metrics\n  - name: \"order_total\"\n    description: \"Sum of orders value\"\n    type: simple\n    label: \"order_total\"\n    type_params:\n      measure:\n        name: order_total\n  - name: \"order_count\"\n    description: \"number of orders\"\n    type: simple\n    label: \"order_count\"\n    type_params:\n      measure:\n        name: order_count\n  - name: large_orders\n    description: \"Count of orders with order total over 20.\"\n    type: simple\n    label: \"Large Orders\"\n    type_params:\n      measure:\n        name: order_count\n    filter: |\n      {{ Metric('order_total', group_by=['order_id']) }} >=  20\n  # Ratio type metric\n  - name: \"avg_order_value\"\n    label: \"avg_order_value\"\n    description: \"average value of each order\"\n    type: ratio\n    type_params:\n      numerator: \n        name: order_total\n      denominator: \n        name: order_count\n  # Cumulative type metrics\n  - name: \"cumulative_order_amount_mtd\"\n    label: \"cumulative_order_amount_mtd\"\n    description: \"The month to date value of all orders\"\n    type: cumulative\n    type_params:\n      measure:\n        name: order_total\n      grain_to_date: month\n  # Derived metric\n  - name: \"pct_of_orders_that_are_large\"\n    label: \"pct_of_orders_that_are_large\"\n    description: \"percent of orders that are large\"\n    type: derived\n    type_params:\n      expr: large_orders/order_count\n      metrics:\n        - name: large_orders\n        - name: order_count\n```\n\n----------------------------------------\n\nTITLE: Using gcloud to Enable Google Drive Access with OAuth\nDESCRIPTION: Command to authenticate gcloud without a quota project to resolve permissions when using OAuth authentication.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Troubleshooting/access-gdrive-credential.md#2025-04-09_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngcloud auth application-default login --disable-quota-project\n```\n\n----------------------------------------\n\nTITLE: Inline Code Reference to dbt-core\nDESCRIPTION: Simple inline code reference to the dbt-core project within the markdown text.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/community/resources/oss-expectations.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n`dbt-core`\n```\n\n----------------------------------------\n\nTITLE: Rendering Account Access Table for Account Permissions in Markdown\nDESCRIPTION: This code snippet defines a sortable table in Markdown format that displays account-level permissions for different roles in dbt Cloud. It includes permissions for various account management features.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_enterprise-permissions-table.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<SortableTable>\n\n{`\n| Account-level permission| Account Admin | Billing admin |  Manage marketplace apps | Project creator | Security admin | Viewer | \n|:-------------------------|:-------------:|:------------:|:-------------------------:|:---------------:|:--------------:|:------:| \n| Account settings*       |     W         |      -        |            -              |        R        |       R        |   R    |\n| Audit logs              |     R         |      -        |            -              |        -        |       R        |   R    |\n| Auth provider           |     W         |      -        |            -              |        -        |       W        |   R    |\n| Billing                 |     W         |       W       |            -              |        -        |       -        |   R    |\n| Connections             |     W         |      -        |            -              |        W        |       -        |   -    |\n| Groups                  |     W         |      -        |            -              |        R        |       W        |   R    |\n| Invitations             |     W         |      -        |            -              |        W        |       W        |   R    |\n| IP restrictions         |     W         |      -        |            -              |        -        |       W        |   R    |\n| Licenses                |     W         |      -        |            -              |        W        |       W        |   R    |\n| Marketplace app         |     -         |      -        |            W              |        -        |       -        |   -    |\n| Members                 |     W         |      -        |            -              |        W        |       W        |   R    |\n| Project (create)        |     W         |      -        |            -              |        W        |       -        |   -    |\n| Public models           |     R         |       R       |            -              |        R        |       R        |   R    |\n| Service tokens          |     W         |      -        |            -              |        -        |       R        |   R    |\n| Webhooks                |     W         |      -        |            -              |        -        |       -        |   -    |\n`}\n\n</SortableTable>\n```\n\n----------------------------------------\n\nTITLE: YAML File Structure Example for Metrics\nDESCRIPTION: Example showing metric definitions placed in appropriate YAML files based on grain or key aggregation. Files like customers.yml or sem_customers.yml contain customer-related metrics, while orders.yml or sem_orders.yml contain order-related metrics.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-07-08-building-your-semantic-layer-in-pieces.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# customers.yml or sem_customers.yml\nmetrics:\n  - name: active_accounts_per_week\n    label: Active Accounts per Week\n```\n\n----------------------------------------\n\nTITLE: Debugging Compilation Error: Invalid YAML\nDESCRIPTION: Error message shown when a YAML file (schema.yml) contains syntax errors. The error includes context showing the problematic lines and the specific syntax issue.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/debug-errors.md#2025-04-09_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\n$ dbt run\nRunning with dbt=1.7.1\n\nEncountered an error:\nCompilation Error\n  Error reading jaffle_shop: schema.yml - Runtime Error\n    Syntax error near line 5\n    ------------------------------\n    2  |\n    3  | models:\n    4  | - name: customers\n    5  |     columns:\n    6  |       - name: customer_id\n    7  |         tests:\n    8  |           - unique\n\n    Raw Error:\n    ------------------------------\n    mapping values are not allowed in this context\n      in \"<unicode string>\", line 5, column 12\n```\n\n----------------------------------------\n\nTITLE: Setting Resource-Specific Configurations in properties.yml\nDESCRIPTION: This example demonstrates how to set resource-specific configurations in a properties.yml file, which is specific to each resource type (e.g., models/properties.yml for models).\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_config-description-resource.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels/properties.yml\n```\n\n----------------------------------------\n\nTITLE: Configuring Merge Exclude Columns in SQL Model\nDESCRIPTION: Example of using the 'merge_exclude_columns' config with the 'merge' strategy. This configuration specifies which columns should be excluded from updates during the merge operation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/incremental-strategy.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(\n    materialized = 'incremental',\n    unique_key = 'id',\n    merge_exclude_columns = ['created_at'],\n    ...\n  )\n}}\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Defining Markdown Terms with Custom Component\nDESCRIPTION: This snippet shows how to use a custom 'Term' component within Markdown text to define technical terms. It's likely part of a documentation system that allows for interactive or highlighted term definitions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-04-05-when-backend-devs-spark-joy.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n*\"The <Term id=\\\"primary-key\\\" /> for that <Term id=\\\"table\\\" /> is actually the `order_id`, not the `id` field.\"*\n```\n\n----------------------------------------\n\nTITLE: Rerunning Failed dbt Models in CLI\nDESCRIPTION: Commands for rerunning failed dbt models, showing both the manual selection approach and the new error-selective approach introduced in dbt 1.0\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2021-11-29-dbt-airflow-spiritual-alignment.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --select <manually-selected-failed-model>\n```\n\nLANGUAGE: bash\nCODE:\n```\ndbt build --select result:error+ --defer --state <previous_state_artifacts>\n```\n\n----------------------------------------\n\nTITLE: Installing dbt Core and DuckDB on Windows PowerShell using Virtual Environment\nDESCRIPTION: Commands to create a Python virtual environment, activate it, upgrade pip, and install the required dependencies for dbt Core with DuckDB on Windows using PowerShell.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/duckdb-qs.md#2025-04-09_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npython -m venv venv\nvenv\\Scripts\\Activate.ps1\npython -m pip install --upgrade pip\npython -m pip install -r requirements.txt\nvenv\\Scripts\\Activate.ps1\n```\n\n----------------------------------------\n\nTITLE: Usage Cheat Sheet Table in Markdown\nDESCRIPTION: A markdown table providing a quick reference guide for when to use defer versus clone commands.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-10-31-to-defer-or-to-clone.md#2025-04-09_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n|                                                                           | defer | clone |\n|---------------------------------------------------------------------------|-------|-------|\n| **Save time & cost by avoiding re-computation**                           |      |      |\n| **Create database objects to be available in downstream tools (e.g. BI)** |      |      |\n| **Safely modify objects in the target schema**                            |      |      |\n| **Avoid creating new database objects**                                   |      |      |\n| **Avoid data drift**                                                      |      |      |\n| **Support multiple dynamic sources**                                      |      |      |\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple Query in dbt Cloud\nDESCRIPTION: A basic SQL query to select all data from the jaffle_shop_customers table, used to test the connection between dbt Cloud and Starburst Galaxy.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/starburst-galaxy-qs.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nselect * from dbt_quickstart.jaffle_shop.jaffle_shop_customers\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Valid and Invalid Databricks Connection Strings\nDESCRIPTION: This snippet shows examples of correct and incorrect formats for Databricks connection strings. It illustrates the valid format for Azure Databricks and an invalid format using a Snowflake-style connection string.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/snowflake-acct-name.md#2025-04-09_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n `db5261993` or `db5261993.east-us-2.azure`\n `db5261993.eu-central-1.snowflakecomputing.com`\n```\n\n----------------------------------------\n\nTITLE: Setting Calculation Type for Conversion Metrics in YAML\nDESCRIPTION: YAML configuration for displaying raw number of conversions instead of conversion rate by setting the calculation parameter to 'conversions' in the conversion metric definition.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/conversion-metrics.md#2025-04-09_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n- name: visit_to_buy_conversions_1_week_window\n    description: \"Visit to Buy Conversions\"\n    type: conversion\n    label: Visit to Buy Conversions (1 week window)\n    type_params:\n      conversion_type_params:\n        calculation: conversions\n        base_measure:\n          name: visits\n        conversion_measure: \n          name: buys\n          fill_nulls_with: 0\n        entity: user\n        window: 1 week\n```\n\n----------------------------------------\n\nTITLE: Pattern Matching Extra Spaces in Text\nDESCRIPTION: Regular expression pattern to match words separated by 2 or more spaces. Uses word boundaries and space quantifiers to detect formatting issues.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/styles/config/vocabularies/EN/reject.txt#2025-04-09_snippet_0\n\nLANGUAGE: regex\nCODE:\n```\n\\b\\w+\\s{2,}\\w+\\b\n```\n\n----------------------------------------\n\nTITLE: Implementing Lightbox Component in MDX\nDESCRIPTION: This snippet demonstrates how to use the Lightbox component with various props to display an image. It shows setting the image source, alt text, title, collapsed state, custom width, and alignment.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/contributing/lightbox.md#2025-04-09_snippet_0\n\nLANGUAGE: mdx\nCODE:\n```\n<Lightbox\n  src=\"/img/hamburger-icon.jpg\"\n  lt=\"Alt text\"\n  title=\"This text is visible\" \n  collapsed={true}\n  width=\"600px\"\n  alignment=\"left\"\n/>\n```\n\n----------------------------------------\n\nTITLE: Creating a Snowflake Feature Store in Python\nDESCRIPTION: This Python code initializes a Snowflake Feature Store using the snowflake-ml-python package. It specifies the database, schema, and warehouse to use, and creates the Feature Store if it doesn't already exist.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-10-05-snowflake-feature-store.md#2025-04-09_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom snowflake.ml.feature_store import (\n    FeatureStore,\n    FeatureView,\n    Entity,\n    CreationMode\n)\n\nfs = FeatureStore(\n    session=session, \n    database=fs_db, \n    name=fs_schema, \n    default_warehouse='WH_DBT',\n    creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n)\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Snowflake Cortex User Permissions\nDESCRIPTION: SQL commands to set up the required roles and permissions for using Snowflake Cortex functions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-05-02-semantic-layer-llm.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nuse role accountadmin;\n\ncreate role cortex_user_role;\ngrant database role snowflake.cortex_user to role cortex_user_role;\n\ngrant role cortex_user_role to user some_user;\n```\n\n----------------------------------------\n\nTITLE: Configuring Bitbucket Pipeline for dbt Cloud PR Jobs\nDESCRIPTION: YAML configuration for Bitbucket pipeline that triggers dbt Cloud jobs when a pull request is created or updated. This setup passes branch information and creates custom schema names based on the PR details.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/custom-cicd-pipelines.md#2025-04-09_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nimage: python:3.11.1\n\n\npipelines:\n  # This job will run when pull requests are created in the repository\n  pull-requests:\n    '**':\n      - step:\n          name: 'Run dbt Cloud PR Job'\n          script:\n            # Check to only build if PR destination is master (or other branch). \n            # Comment or remove line below if you want to run on all PR's regardless of destination branch.\n            - if [ \"${BITBUCKET_PR_DESTINATION_BRANCH}\" != \"main\" ]; then printf 'PR Destination is not master, exiting.'; exit; fi\n            - export DBT_URL=\"https://cloud.getdbt.com\"\n            - export DBT_JOB_CAUSE=\"Bitbucket Pipeline CI Job\"\n            - export DBT_JOB_BRANCH=$BITBUCKET_BRANCH\n            - export DBT_JOB_SCHEMA_OVERRIDE=\"DBT_CLOUD_PR_\"$BITBUCKET_PROJECT_KEY\"_\"$BITBUCKET_PR_ID\n            - export DBT_ACCOUNT_ID=00000 # enter your account id here\n            - export DBT_PROJECT_ID=00000 # enter your project id here\n            - export DBT_PR_JOB_ID=00000 # enter your job id here\n            - python python/run_and_monitor_dbt_job.py\n```\n\n----------------------------------------\n\nTITLE: DBT Compile Reference\nDESCRIPTION: Reference link to DBT compile command documentation for interactive compilation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_indirect-selection-definitions.md#2025-04-09_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n/reference/commands/compile#interactive-compile\n```\n\n----------------------------------------\n\nTITLE: Postgres Data Types in dbt Unit Tests\nDESCRIPTION: Illustrates Postgres data type handling including numeric, string, boolean, date/time, and JSON types. Notes that arrays are not currently supported.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/data-types.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nunit_tests:\n  - name: test_my_data_types\n    model: fct_data_types\n    given:\n      - input: ref('stg_data_types')\n        rows:\n         - int_field: 1\n           float_field: 2.0\n           numeric_field: 1\n           str_field: my_string\n           str_escaped_field: \"my,cool'string\"\n           bool_field: true\n           date_field: 2020-01-02\n           timestamp_field: 2013-11-03 00:00:00-0\n           timestamptz_field: 2013-11-03 00:00:00-0\n           json_field: '{\"bar\": \"baz\", \"balance\": 7.77, \"active\": false}'\n```\n\n----------------------------------------\n\nTITLE: SQL Customer and Order Tables Structure\nDESCRIPTION: Example showing relationship between customers and orders tables using foreign keys\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/entities.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\ncustomer_id (primary key)\ncustomer_name\n```\n\nLANGUAGE: sql\nCODE:\n```\norder_id (primary key)\norder_date\ncustomer_id (foreign key)\n```\n\n----------------------------------------\n\nTITLE: Using target.name for Conditional Data Limiting in SQL\nDESCRIPTION: This SQL snippet demonstrates how to conditionally limit data in development environments using the target.name variable. It filters page_views data to only include the last 3 days when running in a development environment.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/target.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect\n  *\nfrom source('web_events', 'page_views')\n{% if target.name == 'dev' %}\nwhere created_at >= dateadd('day', -3, current_date)\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: ClickHouse Profile Fields Documentation Table\nDESCRIPTION: Markdown table documenting all available configuration fields for ClickHouse profiles in dbt, including field names, requirements, and detailed descriptions of their functionality and default values.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/clickhouse-setup.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Field                           | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n|---------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| `type`                          | This must be included either in `profiles.yml` or in the `dbt_project.yml` file. Must be set to `clickhouse`.                                                                                                                                                                                                                                                                                                                                                                |\n| `schema`                        | Required. A ClickHouse's database name. The dbt model database.schema.table is not compatible with ClickHouse because ClickHouse does not support a schema. So we use a simple model schema.table, where schema is the ClickHouse's database. We don't recommend using the `default` database.                                                                                                                                                                               |\n| `driver`                        | Optional. The ClickHouse client interface, `http` or `native`.  Defaults to `http` unless the `port` is set to 9440 or 9400, in which case the `native` driver is assumed.                                                                                                                                                                                                                                                                                                   |\n| `host`                          | Optional. The host name of the connection. Default is `localhost`.                                                                                                                                                                                                                                                                                                                                                                                                           |\n| `port`                          | Optional. ClickHouse server port number.  Defaults to 8123/8443 (secure) if the driver is `http`, and to 9000/9440(secure) if the driver is `native`.                                                                                                                                                                                                                                                                                                                        |\n| `user`                          | Required. A ClickHouse username with adequate permissions to access the specified `schema`.                                                                                                                                                                                                                                                                                                                                                                                  |\n| `password`                      | Required. The password associated with the specified `user`.                                                                                                                                                                                                                                                                                                                                                                                                                 |\n| `cluster`                       | Optional. If set, certain DDL/table operations will be executed with the `ON CLUSTER` clause using this cluster. Distributed materializations require this setting to work. See the following ClickHouse Cluster section for more details.                                                                                                                                                                                                                                   |\n| `verify`                        | Optional. For (`secure=True`) connections, validate the ClickHouse server TLS certificate, including matching hostname, expiration, and signed by a trusted Certificate Authority. Defaults to True.                                                                                                                                                                                                                                                                         |\n| `secure`                        | Optional. Whether the connection (either http or native) is secured by TLS.  This converts an http driver connection to https, and a native driver connection to the native ClickHouse protocol over TLS.  the Defaults to False.                                                                                                                                                                                                                                            |\n| `retries`                       | Optional. Number of times to retry the initial connection attempt if the error appears to be recoverable.                                                                                                                                                                                                                                                                                                                                                                    |\n| `compression`                   | Optional. Use compression in the connection.  Defaults to `False`.  If set to `True` for HTTP, this enables gzip compression.  If set to `True` for the native protocol, this enabled lz4 compression.  Other valid values are `lz4hc` and `zstd` for the native driver only.                                                                                                                                                                                                |\n| `connect_timeout`               | Optional. Connection timeout in seconds. Defaults is 10 seconds.                                                                                                                                                                                                                                                                                                                                                                                                             |\n| `send_receive_timeout`          | Optional. Timeout for receiving data from or sending data to ClickHouse.  Defaults to 5 minutes (300 seconds)                                                                                                                                                                                                                                                                                                                                                                |\n| `cluster_mode`                  | Optional. Add connection settings to improve compatibility with clusters using the Replicated Database Engine. Default False.                                                                                                                                                                                                                                                                                                                                                |\n| `use_lw_deletes`                | Optional. If ClickHouse experimental lightweight deletes are available, use the `delete+insert` strategy as the default strategy for incremental materializations.  Defaults to `False` (use legacy strategy).                                                                                                                                                                                                                                                               |\n| `check_exchange`                | Optional. On connecting to the ClickHouse, if this is parameter is `True` DBT will validate that the ClickHouse server supports atomic exchange of tables.  Using atomic exchange (when available) improves reliability and parallelism.  This check is unnecessary for ClickHouse running on recent Linux operating system, and in those circumstances can be disabled by setting `check_exchange` to `False` to avoid additional overhead on startup.  Defaults to `True`. |\n```\n\n----------------------------------------\n\nTITLE: Naming a Snapshot 'order_snapshot' in YAML (dbt 1.9+)\nDESCRIPTION: This example shows how to name a snapshot 'order_snapshot' using YAML configuration in dbt version 1.9 and above. It includes full configuration details for the snapshot.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snapshot_name.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  - name: order_snapshot\n    relation: source('my_source', 'my_table')\n    config:\n      schema: string\n      database: string\n      unique_key: column_name_or_expression\n      strategy: timestamp | check\n      updated_at: column_name  # Required if strategy is 'timestamp'\n```\n\n----------------------------------------\n\nTITLE: Creating Pit Stops Detail Table in SQL - pit_stops_joined.sql\nDESCRIPTION: Creates a table focused on pit stop level information with race and constructor context. Handles different dimensionality between results and pit stops to prepare for Python aggregation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/dbt-python-snowpark.md#2025-04-09_snippet_21\n\nLANGUAGE: sql\nCODE:\n```\nwith base_results as (\n\n    select * from {{ ref('fct_results') }}\n\n), \n\npit_stops as (\n\n    select * from {{ ref('int_pit_stops') }}\n\n),\n\npit_stops_joined as (\n\n    select \n        base_results.race_id,\n        race_year,\n        base_results.driver_id,\n        constructor_id,\n        constructor_name,\n        stop_number,\n        lap, \n        lap_time_formatted,\n        pit_stop_duration_seconds, \n        pit_stop_milliseconds\n    from base_results\n    left join pit_stops\n        on base_results.race_id=pit_stops.race_id and base_results.driver_id=pit_stops.driver_id\n)\nselect * from pit_stops_joined\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Client Implementation\nDESCRIPTION: Example of initializing and using the asynchronous AsyncSemanticLayerClient with asyncio to query metrics.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-python-sdk.md#2025-04-09_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom dbtsl.asyncio import AsyncSemanticLayerClient\n\nclient = AsyncSemanticLayerClient(\n    environment_id=123,\n    auth_token=\"<your-semantic-layer-api-token>\",\n    host=\"semantic-layer.cloud.getdbt.com\",\n)\n\nasync def main():\n    async with client.session():\n        metrics = await client.metrics()\n        table = await client.query(\n            metrics=[metrics[0].name],\n            group_by=[\"metric_time\"],\n        )\n        print(table)\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Alternative Format for Defining Tests in dbt YAML\nDESCRIPTION: This YAML configuration shows an alternative format for defining tests in dbt, using a single dictionary with top-level keys. This format can be more readable for tests with multiple arguments and configurations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/data-tests.md#2025-04-09_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: orders\n    columns:\n      - name: status\n        tests:\n          - name: unexpected_order_status_today\n            test_name: accepted_values  # name of the generic test to apply\n            values:\n              - placed\n              - shipped\n              - completed\n              - returned\n            config:\n              where: \"order_date = current_date\"\n```\n\n----------------------------------------\n\nTITLE: dbt Schema YAML with Column Constraints\nDESCRIPTION: This YAML configuration defines a model with enforced contracts and column constraints. It specifies data types and constraints for each column, including not_null and primary_key constraints.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/constraints.md#2025-04-09_snippet_17\n\nLANGUAGE: yml\nCODE:\n```\nmodels:\n  - name: dim_customers\n    config:\n      contract:\n        enforced: true\n    columns:\n      - name: id\n        data_type: int\n        constraints:\n          - type: not_null\n          - type: primary_key # not enforced  -- will warn & include\n          - type: check       # not supported -- will warn & skip\n            expression: \"id > 0\"\n        tests:\n          - unique            # primary_key constraint is not enforced\n      - name: customer_name\n        data_type: text\n      - name: first_transaction_date\n        data_type: date\n```\n\n----------------------------------------\n\nTITLE: Safe Access to Batch Properties\nDESCRIPTION: Example showing how to safely access batch properties by checking if model.batch is populated first.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/model.md#2025-04-09_snippet_3\n\nLANGUAGE: jinja\nCODE:\n```\n{% if model.batch %}\n  {{ log(model.batch.id) }}  # Log the batch ID #\n  {{ log(model.batch.event_time_start) }}  # Log the start time of the batch #\n  {{ log(model.batch.event_time_end) }}  # Log the end time of the batch #\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Sources Health Criteria Table in Markdown\nDESCRIPTION: Markdown table defining the health state criteria for dbt sources, including states for Healthy, Caution, Degraded, and Unknown conditions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/collaborate/data-health-signals.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| **Health state** | **Criteria**   |\n|-------------------|---------------|\n|  Healthy\t| All of the following must be true: <br /><br />- Freshness check configured<br />- Freshness check passed<br />- Freshness check ran in the past 30 days<br />- Has a description |\n|  Caution\t| One of the following must be true: <br /><br />- Freshness check returned a warning<br />- Freshness check not configured<br />- Freshness check not run in the past 30 days<br />- Missing a description |\n|  Degraded\t| - Freshness check returned an error |\n|  Unknown\t| Unable to determine health of resource; no job runs have processed the resource.     |\n```\n\n----------------------------------------\n\nTITLE: Configuring Incremental Materialization with unique_key\nDESCRIPTION: This example shows how a user would configure an incremental materialization in their model, specifying the materialization type and the unique_key.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/config.md#2025-04-09_snippet_1\n\nLANGUAGE: jinja\nCODE:\n```\n{{\n  config(\n    materialized='incremental',\n    unique_key='id'\n  )\n}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Default File Formats in dbt Project\nDESCRIPTION: YAML configuration for setting default Delta/Iceberg/Hudi file formats across models, seeds, and snapshots in the dbt project.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/spark-configs.md#2025-04-09_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  +file_format: delta # or iceberg or hudi\n  \nseeds:\n  +file_format: delta # or iceberg or hudi\n  \nsnapshots:\n  +file_format: delta # or iceberg or hudi\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Snapshots in YAML\nDESCRIPTION: This snippet illustrates how to configure a dbt snapshot using the 'config' property in a YAML file. It allows setting various snapshot-specific configurations.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/config.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsnapshots:\n  - name: <snapshot_name>\n    config:\n      [<snapshot_config>](/reference/snapshot-configs): <config_value>\n      ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Snapshot Meta Column Names in SQL Snapshot File\nDESCRIPTION: Example of customizing snapshot metadata column names directly in a dbt SQL snapshot file using the config() macro. This allows setting custom names for tracking timestamps and identifiers in snapshot tables.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/snapshot_meta_column_names.md#2025-04-09_snippet_1\n\nLANGUAGE: jinja2\nCODE:\n```\n{{\n    config(\n      snapshot_meta_column_names={\n        \"dbt_valid_from\": \"<string>\",\n        \"dbt_valid_to\": \"<string>\",\n        \"dbt_scd_id\": \"<string>\",\n        \"dbt_updated_at\": \"<string>\",\n        \"dbt_is_deleted\": \"<string>\",\n      }\n    )\n}}\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Replicated Distribution in Greenplum dbt Model\nDESCRIPTION: Configures a table to use replicated distribution where data is copied to all segments. This is useful for smaller lookup tables that need to be joined frequently.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/greenplum-configs.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        ...\n        distributed_replicated=true\n        ...\n    )\n}}\n\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: Defining Relationships in Looker Using LookML\nDESCRIPTION: This snippet demonstrates how to define relationships between tables in Looker using LookML. It shows a many-to-one relationship between a fact table (fct_order) and a dimension table (dim_user).\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-04-18-building-a-kimball-dimensional-model-with-dbt.md#2025-04-09_snippet_18\n\nLANGUAGE: lookml\nCODE:\n```\nexplore: fct_order {\n  join: dim_user {\n    sql_on: ${fct_order.user_key} = ${dim_user.user_key} ;;\n    relationship: many_to_one\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Validating dbt YAML Files with dbt-jsonschema\nDESCRIPTION: Use dbt-jsonschema to validate dbt YAML files, providing autocomplete and assistance capabilities in the dbt Cloud IDE. This helps ensure project configurations meet required standards.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/dbt-cloud-ide/develop-in-the-cloud.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# No specific code provided, but implies using dbt-jsonschema for YAML validation\n```\n\n----------------------------------------\n\nTITLE: Calling a macro in on-run-end hook\nDESCRIPTION: This example shows how to call a macro named 'grant_select' in the on-run-end hook. The macro is expected to handle granting select privileges on the schemas used by dbt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/on-run-start-on-run-end.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\non-run-end: \"{{ grant_select(schemas) }}\"\n```\n\n----------------------------------------\n\nTITLE: Example of Not Null Test Configuration\nDESCRIPTION: YAML configuration showing how to implement a not_null test on a column in a model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/data-tests.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: orders\n    columns:\n      - name: order_id\n        tests:\n          - not_null\n```\n\n----------------------------------------\n\nTITLE: Configuring Trino Profile for Incremental Overwrite with Hive\nDESCRIPTION: This YAML configuration sets up a Trino profile with OVERWRITE functionality for a Hive connector named 'minio'. It includes connection details and session properties.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/trino-configs.md#2025-04-09_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\ntrino-incremental-hive:\n  target: dev\n  outputs:\n    dev:\n      type: trino\n      method: none\n      user: admin\n      password:\n      catalog: minio\n      schema: tiny\n      host: localhost\n      port: 8080\n      http_scheme: http\n      session_properties:\n        minio.insert_existing_partitions_behavior: OVERWRITE\n      threads: 1\n```\n\n----------------------------------------\n\nTITLE: Quoting Identifiers Example\nDESCRIPTION: Shows how to properly quote column names and identifiers\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/dbt-jinja-functions/adapter.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nselect \n      'abc' as {{ adapter.quote('table_name') }},\n      'def' as {{ adapter.quote('group by') }}\n```\n\n----------------------------------------\n\nTITLE: Storing Zapier Secrets with Python\nDESCRIPTION: Python code for storing dbt Cloud API token securely in Zapier's storage. Uses StoreClient to save the token with a specified key for later access.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/zapier-slack.md#2025-04-09_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nstore = StoreClient('abc123') #replace with your UUID secret\nstore.set('DBT_CLOUD_SERVICE_TOKEN', 'abc123') #replace with your dbt Cloud API token\n```\n\n----------------------------------------\n\nTITLE: Directory Structure Example in dbt\nDESCRIPTION: Shows the basic folder structure for creating dimension model files in dbt.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-04-18-building-a-kimball-dimensional-model-with-dbt.md#2025-04-09_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nadventureworks/models/\n marts\n     dim_product.sql\n     dim_product.yml\n```\n\n----------------------------------------\n\nTITLE: PullRequestContribute Permission JSON Structure\nDESCRIPTION: JSON representation of the PullRequestContribute permission in Azure DevOps, showing the bit value, display name, and permission name used in the Git Repositories namespace.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/cloud/git/setup-azure-service-user.md#2025-04-09_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{ \t\n    \"bit\": 16384,  \n    \"displayName\": \"Contribute to pull requests\",\n    \"name\": \"PullRequestContribute\"\n}\n```\n\n----------------------------------------\n\nTITLE: Querying dbt Semantic Layer with Dimension Filter Example\nDESCRIPTION: Example of how to add a dimension filter to a where clause in a dbt Semantic Layer query using the Dimension template wrapper. This allows filtering metrics based on dimension values.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/metricflow-commands.md#2025-04-09_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\ndbt sl query --metrics order_total --group-by metric_time --where \"{{Dimension('order_id__is_food_order')}} = True\"\n```\n\n----------------------------------------\n\nTITLE: Example Infer Configuration with BigQuery\nDESCRIPTION: Complete example of an Infer profile configuration using BigQuery as the underlying data warehouse, showing all required fields and typical values\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/infer-setup.md#2025-04-09_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ninfer_bigquery:\n  apikey: 1234567890abcdef\n  username: my_name@example.com\n  url: https://app.getinfer.io\n  type: infer\n  data_config:\n    dataset: my_dataset\n    job_execution_timeout_seconds: 300\n    job_retries: 1\n    keyfile: bq-user-creds.json\n    location: EU\n    method: service-account\n    priority: interactive\n    project: my-bigquery-project\n    threads: 1\n    type: bigquery\n```\n\n----------------------------------------\n\nTITLE: Template for dbt Project Style Guide in Markdown\nDESCRIPTION: A comprehensive template for creating a dbt project style guide that covers SQL style, model organization, naming conventions, model configurations, testing standards and CTE usage. This template can be adapted and used as a starting point for teams implementing their own dbt project standards.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-style/6-how-we-style-conclusion.md#2025-04-09_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# dbt Example Style Guide\n\n## SQL Style\n\n- Use lowercase keywords.\n- Use trailing commas.\n\n## Model Organization\n\nOur models (typically) fit into two main categories:\\\n\n- Staging &mdash; Contains models that clean and standardize data.        \n- Marts &mdash; Contains models which combine or heavily transform data. \n\nThings to note:\n\n- There are different types of models that typically exist in each of the above categories. See [Model Layers](#model-layers) for more information.\n- Read [How we structure our dbt projects](/best-practices/how-we-structure/1-guide-overview) for an example and more details around organization.\n\n## Model Layers\n\n- Only models in `staging` should select from [sources](https://docs.getdbt.com/docs/building-a-dbt-project/using-sources).\n- Models not in the `staging` folder should select from [refs](https://docs.getdbt.com/reference/dbt-jinja-functions/ref).\n\n## Model File Naming and Coding\n\n- All objects should be plural.  \n  Example: `stg_stripe__invoices.sql` vs. `stg_stripe__invoice.sql`\n\n- All models should use the naming convention `<type/dag_stage>_<source/topic>__<additional_context>`. See [this article](https://docs.getdbt.com/blog/stakeholder-friendly-model-names) for more information.\n\n  - Models in the **staging** folder should use the source's name as the `<source/topic>` and the entity name as the `additional_context>`.\n\n    Examples:\n\n    - seed_snowflake_spend.csv\n    - base_stripe\\_\\_invoices.sql\n    - stg_stripe\\_\\_customers.sql\n    - stg_salesforce\\_\\_customers.sql\n    - int_customers\\_\\_unioned.sql\n    - fct_orders.sql\n\n- Schema, table, and column names should be in `snake_case`.\n\n- Limit the use of abbreviations that are related to domain knowledge. An onboarding employee will understand `current_order_status` better than `current_os`.\n\n- Use names based on the _business_ rather than the source terminology.\n\n- Each model should have a primary key to identify the unique row and should be named `<object>_id`. For example, `account_id`. This makes it easier to know what `id` is referenced in downstream joined models.\n\n- For `base` or `staging` models, columns should be ordered in categories, where identifiers are first and date/time fields are at the end.\n- Date/time columns should be named according to these conventions:\n\n  - Timestamps: `<event>_at`  \n    Format: UTC  \n    Example: `created_at`\n\n  - Dates: `<event>_date`\n    Format: Date  \n    Example: `created_date`\n\n- Booleans should be prefixed with `is_` or `has_`.\n  Example: `is_active_customer` and `has_admin_access`\n\n- Price/revenue fields should be in decimal currency (for example, `19.99` for $19.99; many app databases store prices as integers in cents). If a non-decimal currency is used, indicate this with suffixes. For example, `price_in_cents`.\n\n- Avoid using reserved words (such as [these](https://docs.snowflake.com/en/sql-reference/reserved-keywords.html) for Snowflake) as column names.\n\n- Consistency is key! Use the same field names across models where possible. For example, a key to the `customers` table should be named `customer_id` rather than `user_id`.\n\n## Model Configurations\n\n- Model configurations at the [folder level](https://docs.getdbt.com/reference/model-configs#configuring-directories-of-models-in-dbt_projectyml) should be considered (and if applicable, applied) first.\n- More specific configurations should be applied at the model level [using one of these methods](https://docs.getdbt.com/reference/model-configs#apply-configurations-to-one-model-only).\n- Models within the `marts` folder should be materialized as `table` or `incremental`.\n  - By default, `marts` should be materialized as `table` within `dbt_project.yml`.\n  - If switching to `incremental`, this should be specified in the model's configuration.\n\n## Testing\n\n- At a minimum, `unique` and `not_null` tests should be applied to the expected primary key of each model.\n\n## CTEs\n\nFor more information about why we use so many CTEs, read [this glossary entry](https://docs.getdbt.com/terms/cte).\n\n- Where performance permits, CTEs should perform a single, logical unit of work.\n- CTE names should be as verbose as needed to convey what they do.\n- CTEs with confusing or noteable logic should be commented with SQL comments as you would with any complex functions and should be located above the CTE.\n- CTEs duplicated across models should be pulled out and created as their own models.\n```\n\n----------------------------------------\n\nTITLE: Installing Prerelease Versions\nDESCRIPTION: Commands for installing prerelease versions of dbt Core and adapters in a virtual environment.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/pip-install.md#2025-04-09_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\npython3 -m pip install --pre dbt-core dbt-adapter-name\n```\n\nLANGUAGE: shell\nCODE:\n```\npython3 -m pip install --pre dbt-core dbt-snowflake\n```\n\nLANGUAGE: shell\nCODE:\n```\ndbt --version\npython3 -m venv .venv\nsource .venv/bin/activate\npython3 -m pip install --upgrade pip\npython3 -m pip install --pre dbt-core dbt-adapter-name\nsource .venv/bin/activate\ndbt --version\n```\n\n----------------------------------------\n\nTITLE: Preventing Model Sampling in SQL\nDESCRIPTION: SQL snippet showing how to prevent a referenced model from being sampled by appending .render() to the ref function.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/sample-flag.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nwith\n\nsource as (\n\n    select * from {{ ref('stg_customers').render() }}\n\n),\n\n...\n```\n\n----------------------------------------\n\nTITLE: Example of Timestamp Strategy in YAML (dbt v1.9+)\nDESCRIPTION: Complete YAML example of a snapshot using the timestamp strategy for a orders table, specifying the schema, unique key, and updated_at column.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/strategy.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nsnapshots:\n  - name: orders_snapshot_timestamp\n    relation: source('jaffle_shop', 'orders')\n    config:\n      schema: snapshots\n      strategy: timestamp\n      unique_key: id\n      updated_at: updated_at\n```\n\n----------------------------------------\n\nTITLE: Installing dbt Semantic Layer SDK - Sync Version\nDESCRIPTION: Installation command for the synchronous version of the dbt Semantic Layer SDK using pip.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-python-sdk.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"dbt-sl-sdk[sync]\"\n```\n\n----------------------------------------\n\nTITLE: Running Specific Models with dbt Run\nDESCRIPTION: Examples of using the --select flag with dbt run to execute specific models or subsets of models based on various criteria.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/syntax.md#2025-04-09_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndbt run --select \"my_dbt_project_name\"   # runs all models in your project\ndbt run --select \"my_dbt_model\"          # runs a specific model\ndbt run --select \"path/to/my/models\"     # runs all models in a specific directory\ndbt run --select \"my_package.some_model\" # run a specific model in a specific package\ndbt run --select \"tag:nightly\"           # run models with the \"nightly\" tag\ndbt run --select \"path/to/models\"        # run models contained in path/to/models\ndbt run --select \"path/to/my_model.sql\"  # run a specific model by its path\n```\n\n----------------------------------------\n\nTITLE: Configuring Query Retries in dbt Profile\nDESCRIPTION: YAML configuration for enabling query retries in case of intermittent errors. This sets the number of retries and timeout between attempts.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/spark-setup.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nretry_all: true\nconnect_timeout: 5\nconnect_retries: 3\n```\n\n----------------------------------------\n\nTITLE: Complete Materialized View Configuration Example for Databricks\nDESCRIPTION: A comprehensive example of configuring a materialized view in Databricks, including partition_by, refresh schedule, and table properties settings alongside the model query.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/databricks-configs.md#2025-04-09_snippet_23\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized='materialized_view',\n    partition_by='id',\n    schedule = {\n        'cron': '0 0 * * * ? *',\n        'time_zone_value': 'Etc/UTC'\n    },\n    tblproperties={\n        'key': 'value'\n    },\n) }}\nselect * from {{ ref('my_seed') }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Iceberg Table Materialization in dbt\nDESCRIPTION: Demonstrates how to set up an Iceberg table with custom configuration including format, partitioning, and table properties. Uses the 'table' materialization with Iceberg-specific configuration options.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/athena-configs.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(\n    materialized='table',\n    table_type='iceberg',\n    format='parquet',\n    partitioned_by=['bucket(user_id, 5)'],\n    table_properties={\n     'optimize_rewrite_delete_file_threshold': '2'\n     }\n) }}\n\nselect 'A'          as user_id,\n       'pi'         as name,\n       'active'     as status,\n       17.89        as cost,\n       1            as quantity,\n       100000000    as quantity_big,\n       current_date as my_date\n```\n\n----------------------------------------\n\nTITLE: Example dbt Exposures Configuration\nDESCRIPTION: Practical example showing how to configure multiple exposures including a dashboard, ML model, and application. Demonstrates different maturity levels, dependency configurations, and owner specifications.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/exposure-properties.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nexposures:\n\n  - name: weekly_jaffle_metrics\n    label: Jaffles by the Week\n    type: dashboard\n    maturity: high\n    url: https://bi.tool/dashboards/1\n    description: >\n      Did someone say \"exponential growth\"?\n\n    depends_on:\n      - ref('fct_orders')\n      - ref('dim_customers')\n      - source('gsheets', 'goals')\n      - metric('count_orders')\n\n    owner:\n      name: Callum McData\n      email: data@jaffleshop.com\n\n\n      \n  - name: jaffle_recommender\n    maturity: medium\n    type: ml\n    url: https://jupyter.org/mycoolalg\n    description: >\n      Deep learning to power personalized \"Discover Sandwiches Weekly\"\n    \n    depends_on:\n      - ref('fct_orders')\n      \n    owner:\n      name: Data Science Drew\n      email: data@jaffleshop.com\n\n      \n  - name: jaffle_wrapped\n    type: application\n    description: Tell users about their favorite jaffles of the year\n    depends_on: [ ref('fct_orders') ]\n    owner: { email: summer-intern@jaffleshop.com }\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple Revenue Metric in YAML\nDESCRIPTION: YAML configuration that defines a simple revenue metric based on the revenue measure from the order_items semantic model. This metric can be used to analyze revenue trends.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-build-our-metrics/semantic-layer-8-refactor-a-rollup.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\\n  - name: revenue\\n    description: Sum of the product revenue for each order item. Excludes tax.\\n    type: simple\\n    label: Revenue\\n    type_params:\\n      measure: revenue\n```\n\n----------------------------------------\n\nTITLE: Including Web Image in Model Description\nDESCRIPTION: This snippet shows how to include an image from the web in a model's description field using Markdown syntax with a URL.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/description.md#2025-04-09_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: customers\n    description: \"!\\[dbt Logo](https://github.com/dbt-labs/dbt-core/blob/main/etc/dbt-core.svg)\"\n\n    columns:\n      - name: customer_id\n        description: Primary key\n```\n\n----------------------------------------\n\nTITLE: Setting Current Records Only Variable in dbt_project.yml\nDESCRIPTION: Defines a global variable in dbt_project.yml that can be used to toggle between building historical or current-only datasets, allowing for more flexibility in data processing.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-05-24-joining-snapshot-complexity.md#2025-04-09_snippet_5\n\nLANGUAGE: jsx\nCODE:\n```\nfuture_proof_date: '9999-12-31'\ncurrent_records_only: true\n```\n\n----------------------------------------\n\nTITLE: Creating a Complete LLM-Powered Summary Model in dbt\nDESCRIPTION: A comprehensive dbt model that generates summaries from Slack threads using Snowflake Cortex LLM functions. The model implements incremental materialization, uses Jinja for DRY SQL, partitions data by multiple dimensions, and post-processes LLM outputs to improve quality and consistency.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-02-29-cortex-slack.md#2025-04-09_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{\n    config(\n        materialized='incremental',\n        unique_key='unique_key',\n        full_refresh=false\n    )\n-}}\n\n\n{# \n    This partition_by dict is to dry up the columns that are used in different parts of the query. \n    The SQL is used in the partition by components of the window function aggregates, and the column \n    names are used (in conjunction with the SQL) to select the relevant columns out in the final model.\n    They could be written out manually, but it creates a lot of places to update when changing from \n    day to week truncation for example. \n\n    Side note: I am still not thrilled with this approach, and would be happy to hear about alternatives!\n#}\n{%- set partition_by = [\n    {'column': 'summary_period', 'sql': 'date_trunc(day, sent_at)'},\n    {'column': 'product_segment', 'sql': 'lower(product_segment)'},\n    {'column': 'is_further_attention_needed', 'sql': 'is_further_attention_needed'},\n] -%}\n\n{% set partition_by_sqls = [] -%}\n{% set partition_by_columns = [] -%}\n\n{% for p in partition_by -%}\n    {% do partition_by_sqls.append(p.sql) -%}\n    {% do partition_by_columns.append(p.column) -%}\n{% endfor -%}\n\n\nwith\n\nsummaries as (\n\n    select * from {{ ref('fct_slack_thread_llm_summaries') }}\n    where not has_townie_participant\n\n),\n\naggregated as (\n    select distinct\n        {# Using the columns defined above #}\n        {% for p in partition_by -%}\n            {{ p.sql }} as {{ p.column }},\n        {% endfor -%}\n\n        -- This creates a JSON array, where each element is one thread + its permalink. \n        -- Each array is broken down by the partition_by columns defined above, so there's\n        -- one summary per time period and product etc.\n        array_agg(\n            object_construct(\n                'permalink', thread_permalink,\n                'thread', thread_summary\n            )\n        ) over (partition by {{ partition_by_sqls | join(', ') }}) as agg_threads,\n        count(*) over (partition by {{ partition_by_sqls | join(', ') }}) as num_records,\n        \n        -- The partition columns are the grain of the table, and can be used to create\n        -- a unique key for incremental purposes\n        {{ dbt_utils.generate_surrogate_key(partition_by_columns) }} as unique_key\n    from summaries\n    {% if is_incremental() %}\n        where unique_key not in (select this.unique_key from {{ this }} as this) \n    {% endif %}\n\n),\n\nsummarised as (\n\n    select\n        *,\n        trim(snowflake.cortex.complete(\n            'llama2-70b-chat',\n            concat(\n                'In a few bullets, describe the key takeaways from these threads. For each object in the array, summarise the `thread` field, then provide the Slack permalink URL from the `permalink` field for that element in markdown format at the end of each summary. Do not repeat my request back to me in your response.',\n                agg_threads::text\n            )\n        )) as overall_summary\n    from aggregated\n\n),\n\nfinal as (\n    select \n        * exclude overall_summary,\n        -- The LLM loves to say something like \"Sure, here's your summary:\" despite my best efforts. So this strips that line out\n        regexp_replace(\n            overall_summary, '(^Sure.+:\\n*)', ''\n        ) as overall_summary\n\n    from summarised\n)\n\nselect * from final\n```\n\n----------------------------------------\n\nTITLE: Generating Date Spine in dbt_quickbooks\nDESCRIPTION: This SQL model from dbt_quickbooks shows how to create a date spine for the general ledger using jinja and date spine macros.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-09-07-leverage-accounting-principles-when-finacial-modeling.md#2025-04-09_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nmodels/intermediate/int_quickbooks__general_ledger_date_spine.sql\n```\n\n----------------------------------------\n\nTITLE: Configuring on_configuration_change in model SQL files using config block\nDESCRIPTION: Implements the on_configuration_change setting directly in a model SQL file using a config block. This allows per-model configuration of how dbt should handle changes to the model's configuration.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/on_configuration_change.md#2025-04-09_snippet_2\n\nLANGUAGE: jinja\nCODE:\n```\n{{ config(\n    [materialized]=\"<materialization_name>\",\n    on_configuration_change=\"apply\" | \"continue\" | \"fail\"\n) }}\n```\n\n----------------------------------------\n\nTITLE: Compiled SQL Output with Quoting\nDESCRIPTION: Shows the final compiled SQL with applied quoting configurations, demonstrating how source references are resolved with appropriate quoting.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/quoting.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nselect\n  ...\n\n-- this should be quoted\nfrom \"raw\".\"jaffle_shop\".\"orders\"\n\n-- here, the identifier should be unquoted\nleft join \"raw\".\"jaffle_shop\".customers using (order_id)\n```\n\n----------------------------------------\n\nTITLE: Using COALESCE to Replace Null Values in Order Status\nDESCRIPTION: This SQL query demonstrates how to use COALESCE to replace null values in the order_status column with 'not_returned'. It selects from an orders table and improves readability of the data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-06-30-coalesce-sql.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\torder_id,\n\torder_date,\n\tcoalesce(order_status, 'not_returned') as order_status\nfrom {{ ref('orders') }}\n```\n\n----------------------------------------\n\nTITLE: Real World Unit Test Configuration\nDESCRIPTION: Complex YAML configuration showing a real-world unit test for order items summary, testing scenarios with zero drinks.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-05-07-unit-testing.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nunit_tests:\n\n  - name: test_order_items_count_drink_items_with_zero_drinks\n    description: >\n      Scenario: Order without any drinks\n        When the `order_items_summary` table is built\n        Given an order with nothing but 1 food item\n        Then the count of drink items is 0\n\n    # Model\n    model: order_items_summary\n\n    # Inputs\n    given:\n      - input: ref('order_items')\n        rows:\n          - {\n              order_id: 76,\n              order_item_id: 3,\n              is_drink_item: false,\n            }\n      - input: ref('stg_orders')\n        rows:\n          - { order_id: 76 }\n\n    # Output\n    expect:\n      rows:\n        - {\n            order_id: 76,\n            count_drink_items: 0,\n          }\n```\n\n----------------------------------------\n\nTITLE: Defining Constraints for BigQuery Nested Columns\nDESCRIPTION: This YAML snippet shows how to define constraints and data types for nested columns in a BigQuery dbt model, including constraints on deeply nested fields.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/constraints.md#2025-04-09_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: nested_column_constraints_example\n    config:\n      contract: \n        enforced: true\n    columns:\n      - name: a\n        data_type: string\n      - name: b.id\n        data_type: integer\n        constraints:\n          - type: not_null\n      - name: b.name\n        description: test description\n        data_type: string\n      - name: b.double_nested.id\n        data_type: integer\n      - name: b.double_nested.another.again\n        data_type: string\n      - name: b.double_nested.another.even_more\n        data_type: integer\n        constraints: \n          - type: not_null\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Model with Alternative Syntax\nDESCRIPTION: This snippet shows the correct way to configure a dbt model using the alternative config block syntax. It allows for special characters in configuration names and uses a dictionary-like structure.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/advanced-config-usage.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config({\n    \"post-hook\": \"grant select on {{ this }} to role reporter\",\n    \"materialized\": \"table\"\n  })\n}}\n\n\nselect ...\n```\n\n----------------------------------------\n\nTITLE: BigQuery Incremental Model with Static Partition Replacement\nDESCRIPTION: Configures an incremental model using insert_overwrite strategy with static partitions. This approach allows specifying which partitions to replace, making it efficient for daily refreshes of recent data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/bigquery-configs.md#2025-04-09_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\n{% set partitions_to_replace = [\n  'timestamp(current_date)',\n  'timestamp(date_sub(current_date, interval 1 day))'\n] %}\n\n{{\n  config(\n    materialized = 'incremental',\n    incremental_strategy = 'insert_overwrite',\n    partition_by = {'field': 'session_start', 'data_type': 'timestamp'},\n    partitions = partitions_to_replace\n  )\n}}\n\nwith events as (\n\n    select * from {{ref('events')}}\n\n    {% if is_incremental() %}\n        -- recalculate yesterday + today\n        where timestamp_trunc(event_timestamp, day) in ({{ partitions_to_replace | join(',') }})\n    {% endif %}\n\n),\n\n... rest of model ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Materialized View in BigQuery with dbt\nDESCRIPTION: This snippet demonstrates how to configure a materialized view in BigQuery using dbt. It enables refresh, sets the refresh interval, and specifies the maximum staleness allowed.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-08-01-announcing-materialized-views.md#2025-04-09_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{{\\nconfig(\\n    materialized = 'materialized_view',\\n    on_configuration_change = 'apply',\\n    enable_refresh = True,\\n    refresh_interval_minutes = 30\\n    max_staleness = 'INTERVAL 60 MINUTE'\\n)\\n}}\n```\n\n----------------------------------------\n\nTITLE: DBT Snapshot CLI Execution Output\nDESCRIPTION: Example output from running the dbt snapshot command showing successful execution.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/snapshots.md#2025-04-09_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n$ dbt snapshot\nRunning with dbt=1.8.0\n\n15:07:36 | Concurrency: 8 threads (target='dev')\n15:07:36 |\n15:07:36 | 1 of 1 START snapshot snapshots.orders_snapshot...... [RUN]\n15:07:36 | 1 of 1 OK snapshot snapshots.orders_snapshot..........[SELECT 3 in 1.82s]\n15:07:36 |\n15:07:36 | Finished running 1 snapshots in 0.68s.\n\nCompleted successfully\n\nDone. PASS=2 ERROR=0 SKIP=0 TOTAL=1\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Alias Generator for Firebolt Schema Workaround\nDESCRIPTION: SQL macro for dbt that uses the schema field from profiles.yml as a table name prefix. This provides a workaround for Firebolt's lack of database schema support to enable concurrent development without table name collisions.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/firebolt-setup.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n{% macro generate_alias_name(custom_alias_name=none, node=none) -%}\n    {%- if custom_alias_name is none -%}\n        {{ node.schema }}__{{ node.name }}\n    {%- else -%}\n        {{ node.schema }}__{{ custom_alias_name | trim }}\n    {%- endif -%}\n{%- endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Modifying OAuth Security Integration in Snowflake SQL\nDESCRIPTION: This SQL command alters the OAuth Security integration in Snowflake to explicitly set the external OAuth scope mapping attribute. This can resolve issues with role-based access in OAuth connections.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Troubleshooting/failed-snowflake-oauth-connection.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nALTER INTEGRATION <my_int_name> SET EXTERNAL_OAUTH_SCOPE_MAPPING_ATTRIBUTE = 'scp';\n```\n\n----------------------------------------\n\nTITLE: Configuration for SQLFluff Templater in dbt Cloud\nDESCRIPTION: Example configuration setting for the .sqlfluff file specifying dbt as the templater. This allows SQLFluff to understand and properly lint dbt templates when performing SQL linting in CI jobs.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/continuous-integration.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntemplater = dbt\n```\n\n----------------------------------------\n\nTITLE: Configuring persist_docs for Snapshots in SQL files\nDESCRIPTION: This snippet demonstrates how to configure persist_docs directly in a snapshot's SQL file using the config block. It enables persisting documentation for both relations and columns.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/persist_docs.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n{% snapshot [snapshot_name] %}\n\n{{ config(\n  persist_docs={\"relation\": true, \"columns\": true}\n) }}\n\nselect ...\n\n{% endsnapshot %}\n```\n\n----------------------------------------\n\nTITLE: Standard Test Compiled SQL\nDESCRIPTION: Compiled SQL for relationship test without using defer.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/defer.md#2025-04-09_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nselect count(*) as validation_errors\nfrom (\n    select id as id from dev_alice.model_b\n) as child\nleft join (\n    select id as id from dev_alice.model_a\n) as parent on parent.id = child.id\nwhere child.id is not null\n  and parent.id is null\n```\n\n----------------------------------------\n\nTITLE: Redshift dbt Model SQL\nDESCRIPTION: This SQL snippet defines a simple dbt model for Redshift, creating a table with an id, customer_name, and first_transaction_date.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-properties/constraints.md#2025-04-09_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\n{{\n  config(\n    materialized = \"table\"\n  )\n}}\n\nselect \n  1 as id, \n  'My Favorite Customer' as customer_name, \n  cast('2019-01-01' as date) as first_transaction_date\n```\n\n----------------------------------------\n\nTITLE: Iceberg Configuration for Apache Spark Session\nDESCRIPTION: Spark configuration options required for using Iceberg with dbt-glue. Includes settings for extensions, serializer, warehouse location, catalog implementation, and DynamoDB lock manager.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/glue-setup.md#2025-04-09_snippet_11\n\nLANGUAGE: text\nCODE:\n```\n--conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \n    --conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n    --conf spark.sql.warehouse=s3://<your-bucket-name>\n    --conf spark.sql.catalog.glue_catalog=org.apache.iceberg.spark.SparkCatalog \n    --conf spark.sql.catalog.glue_catalog.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog \n    --conf spark.sql.catalog.glue_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO \n    --conf spark.sql.catalog.glue_catalog.lock-impl=org.apache.iceberg.aws.dynamodb.DynamoDbLockManager\n    --conf spark.sql.catalog.glue_catalog.lock.table=myGlueLockTable  \n    --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\n```\n\n----------------------------------------\n\nTITLE: HTTP Request Query for Discovery API\nDESCRIPTION: Example of how to make an HTTP POST request to the Discovery API using curl, with headers for authorization and content type.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/discovery-querying.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl 'YOUR_API_URL' \\\n    -H 'authorization: Bearer YOUR_TOKEN' \\\n    -H 'content-type: application/json'\n    -X POST\n    --data QUERY_BODY\n```\n\n----------------------------------------\n\nTITLE: Retesting Failed Tests with Exclusions in dbt\nDESCRIPTION: Command to rerun only tests that failed, with the ability to exclude specific tests that are known to continue failing. Useful when some tests require external data refreshes or other non-code fixes.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/best-practice-workflows.md#2025-04-09_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndbt test --select result:fail --exclude <example test> --defer --state path/to/prod/artifacts\n```\n\n----------------------------------------\n\nTITLE: Creating and Registering Feature Views in Snowflake with Python\nDESCRIPTION: This Python code creates a feature view in Snowflake's Feature Store based on the feature table produced in dbt. It defines the entities, timestamp column, and other metadata for the feature view, enabling versioning and discoverability.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2024-10-05-snowflake-feature-store.md#2025-04-09_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Create a dataframe from our feature table produced in dbt\ncustomers_transactions_df = session.sql(f\"\"\"\n    SELECT \n        CUSTOMER_ID,\n        TX_DATETIME,\n        TX_AMOUNT_1D,\n        TX_AMOUNT_7D,\n        TX_AMOUNT_30D,\n        TX_AMOUNT_AVG_1D,\n        TX_AMOUNT_AVG_7D,\n        TX_AMOUNT_AVG_30D,\n        TX_CNT_1D,\n        TX_CNT_7D,\n        TX_CNT_30D     \n    FROM {fs_db}.{fs_data_schema}.ft_customer_transactions\n    \"\"\")\n\n# Create a feature view on top of these features\ncustomer_transactions_fv = FeatureView(\n    name=\"customer_transactions_fv\", \n    entities=[customer],\n    feature_df=customers_transactions_df,\n    timestamp_col=\"TX_DATETIME\",\n    refresh_freq=None,\n    desc=\"Customer transaction features with window aggregates\")\n\n# Register the feature view for use beyond the session\ncustomer_transactions_fv = fs.register_feature_view(\n    feature_view=customer_transactions_fv,\n    version=\"1\",\n    #overwrite=True,\n    block=True)\n\n```\n\n----------------------------------------\n\nTITLE: Setting Project Name in DBT Configuration\nDESCRIPTION: Configures the project name in dbt_project.yml file to 'jaffle_shop'.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/quickstarts/change-way-model-materialized.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nname: 'jaffle_shop'\n```\n\n----------------------------------------\n\nTITLE: Querying Exposure Information with GraphQL in dbt\nDESCRIPTION: A GraphQL query that retrieves information about a specific exposure from a dbt job, including owner details, URL, and information about parent sources and models. This query demonstrates how to access exposure metadata through the dbt API.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/schema-discovery-job-exposure.mdx#2025-04-09_snippet_0\n\nLANGUAGE: graphql\nCODE:\n```\n{\n  job(id: 123) {\n    exposure(name: \"my_awesome_exposure\") {\n      runId\n      projectId\n      name\n      uniqueId\n      resourceType\n      ownerName\n      url\n      ownerEmail\n      parentsSources {\n        uniqueId\n        sourceName\n        name\n        state\n        maxLoadedAt\n        criteria {\n          warnAfter {\n            period\n            count\n          }\n          errorAfter {\n            period\n            count\n          }\n        }\n        maxLoadedAtTimeAgoInS\n      }\n      parentsModels {\n        uniqueId\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Seed-Specific Delimiter in properties.yml\nDESCRIPTION: This YAML snippet shows how to set a custom delimiter for a specific seed in the properties.yml file.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/resource-configs/delimiter.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nseeds:\n  - name: <seed_name>\n    config: \n      delimiter: \"|\"\n```\n\n----------------------------------------\n\nTITLE: Deferred Run Compiled SQL\nDESCRIPTION: Compiled SQL for model_b using defer, referencing production schema.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/node-selection/defer.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\ncreate or replace view dev_me.model_b as (\n\n    select\n\n        id,\n        count(*)\n\n    from prod.model_a\n    group by 1\n\n)\n```\n\n----------------------------------------\n\nTITLE: Applying Generic Tests in YAML\nDESCRIPTION: This YAML snippet shows how to apply generic tests to a model named 'orders'. It includes examples of using built-in tests like unique, not_null, accepted_values, and relationships.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/data-tests.md#2025-04-09_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nmodels:\n  - name: orders\n    columns:\n      - name: order_id\n        tests:\n          - unique\n          - not_null\n      - name: status\n        tests:\n          - accepted_values:\n              values: ['placed', 'shipped', 'completed', 'returned']\n      - name: customer_id\n        tests:\n          - relationships:\n              to: ref('customers')\n              field: id\n```\n\n----------------------------------------\n\nTITLE: Traditional SQL Metric Calculation\nDESCRIPTION: SQL query showing how analysts typically calculate order_total metric with new customer segmentation using multiple table joins.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/about-metricflow.md#2025-04-09_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    date_trunc('day',orders.ordered_at) as day, \n    case when customers.first_ordered_at is not null then true else false end as is_new_customer,\n    sum(orders.order_total) as order_total\nfrom\n  orders\nleft join\n  customers\non\n  orders.customer_id = customers.customer_id\ngroup by 1, 2\n```\n\n----------------------------------------\n\nTITLE: Configuring Leads Metric with TimeSpin Join in YAML\nDESCRIPTION: Configuration example showing how to set up a leads metric with both null filling and time spine joining to ensure complete daily coverage of data.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/build/fill-nulls-advanced.md#2025-04-09_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- name: leads\n  type: simple\n  type_params:\n    measure:\n      name: bookings\n      fill_nulls_with: 0\n      join_to_timespine: true\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Payload for Run Completed Event\nDESCRIPTION: This example shows the JSON payload that dbt Cloud sends when a job run has completed successfully. It includes all the job and run details, including start and finish timestamps.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/deploy/webhooks.md#2025-04-09_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"accountId\": 1,\n  \"webhooksID\": \"wsu_12345abcde\",\n  \"eventId\": \"wev_2L6ZDoilyiWzKkSA59Gmc2d7FDD\",\n  \"timestamp\": \"2023-01-31T19:29:35.789265936Z\",\n  \"eventType\": \"job.run.completed\",\n  \"webhookName\": \"test\",\n  \"data\": {\n    \"jobId\": \"123\",\n    \"jobName\": \"Daily Job (dbt build)\",\n    \"runId\": \"12345\",\n    \"environmentId\": \"1234\",\n    \"environmentName\": \"Production\",\n    \"dbtVersion\": \"1.0.0\",\n    \"projectName\": \"Snowflake Github Demo\",\n    \"projectId\": \"167194\",\n    \"runStatus\": \"Success\",\n    \"runStatusCode\": 10,\n    \"runStatusMessage\": \"None\",\n    \"runReason\": \"Kicked off from UI by test@test.com\",\n    \"runStartedAt\": \"2023-01-31T19:28:07Z\",\n    \"runFinishedAt\": \"2023-01-31T19:29:32Z\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining clean-targets in dbt_project.yml\nDESCRIPTION: Specifies the clean-targets configuration in the dbt_project.yml file. This setting determines which directories will be removed when running the dbt clean command.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/reference/project-configs/clean-targets.md#2025-04-09_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nclean-targets: [directorypath]\n```\n\n----------------------------------------\n\nTITLE: Defining a Locations Semantic Model in YAML for dbt Semantic Layer\nDESCRIPTION: This YAML configuration creates a semantic model for locations data, specifying the primary entity, dimensions (including categorical and time dimensions), and a tax rate measure. It references a staging model as its data source and includes proper documentation of the model's grain and purpose.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/best-practices/how-we-build-our-metrics/semantic-layer-8-refactor-a-rollup.md#2025-04-09_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsemantic_models:\n  - name: locations\n    description: |\n      Location dimension table. The grain of the table is one row per location.\n    model: ref('stg_locations')\n    entities:\n      - name: location\n        type: primary\n        expr: location_id\n    dimensions:\n      - name: location_name\n        type: categorical\n      - name: date_trunc('day', opened_at)\n        type: time\n        type_params:\n          time_granularity: day\n    measures:\n      - name: average_tax_rate\n        description: Average tax rate.\n        expr: tax_rate\n        agg: avg\n```\n\n----------------------------------------\n\nTITLE: Creating Staging Model for Orders in dbt\nDESCRIPTION: This SQL snippet creates a staging model for orders, selecting and renaming columns from the orders table.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/guides/microsoft-fabric-qs.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nselect\n    ID as order_id,\n    USER_ID as customer_id,\n    ORDER_DATE as order_date,\n    STATUS as status\n\nfrom dbo.orders\n```\n\n----------------------------------------\n\nTITLE: Basic SQL UPPER Function Syntax\nDESCRIPTION: The basic syntax for using the UPPER function to convert a string column to uppercase.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/sql-reference/string-functions/sql-upper.md#2025-04-09_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nupper(<string_column>)\n```\n\n----------------------------------------\n\nTITLE: Querying Latest RFM Segments\nDESCRIPTION: Retrieves the most recent RFM segments for all customers by selecting records with the maximum date from the historical segmentation model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2023-05-08-building-a-historical-user-segmentation-model-with-dbt.md#2025-04-09_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nWITH rfm_segments AS(\n\tSELECT *\n\tFROM ref {{'model_rfm_segments_hist'}}\n),\t\ncurrent_segments AS(\n\tSELECT *\n\tFROM rfm_segments\n\tWHERE date_month = (SELECT MAX(date_month) FROM rfm_segments)\n)\nSELECT *\nFROM current_segments\n```\n\n----------------------------------------\n\nTITLE: Referencing DBT buildable Mode\nDESCRIPTION: Code reference showing the usage of the buildable mode parameter in DBT, which runs tests referring to selected nodes or their ancestors.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/snippets/_indirect-selection-definitions.md#2025-04-09_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\nbuildable\n```\n\n----------------------------------------\n\nTITLE: Running models downstream of a source in dbt\nDESCRIPTION: This command uses the source selector to run all models downstream of the 'jaffle_shop' source in dbt. It demonstrates the use of the '+' operator to include all downstream dependencies.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/faqs/Runs/running-models-downstream-of-source.md#2025-04-09_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ dbt run --select source:jaffle_shop+\n```\n\n----------------------------------------\n\nTITLE: Debugging Profile Directory Configuration\nDESCRIPTION: Command to check the expected location of profiles.yml file in dbt installation.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/core/connect-data-platform/connection-profiles.md#2025-04-09_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ dbt debug --config-dir\n```\n\n----------------------------------------\n\nTITLE: Creating a Query with Limit in GraphQL\nDESCRIPTION: Creates a new query with a limit of 10 rows, retrieving food order amount and gross profit metrics grouped by month and customer type. The environmentId parameter specifies the execution context.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/docs/docs/dbt-cloud-apis/sl-graphql.md#2025-04-09_snippet_14\n\nLANGUAGE: graphql\nCODE:\n```\nmutation {\n  createQuery(\n    environmentId: \"123\"\n    metrics: [{name:\"food_order_amount\"}, {name: \"order_gross_profit\"}]\n    groupBy: [{name:\"metric_time\", grain: MONTH}, {name: \"customer__customer_type\"}]\n    limit: 10 \n  ) {\n    queryId\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Calling Stream Model Macro in SQL\nDESCRIPTION: This SQL file calls the stream_model_macro with the 'container' variable to generate the data model.\nSOURCE: https://github.com/dbt-labs/docs.getdbt.com/blob/current/website/blog/2022-10-24-demystifying-event-streams.md#2025-04-09_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{{ stream_model_macro(var('container')) }}\n```"
  }
]
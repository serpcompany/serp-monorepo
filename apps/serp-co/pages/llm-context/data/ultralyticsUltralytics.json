[
  {
    "owner": "ultralytics",
    "repo": "ultralytics",
    "content": "TITLE: Using YOLO Trainers for Detection in Python\nDESCRIPTION: This Python code illustrates how to directly instantiate and use the `DetectionTrainer`, `DetectionValidator`, and `DetectionPredictor` classes from the `ultralytics.models.yolo` module for a detection task. It shows the workflow of training a model using `DetectionTrainer`, validating the best checkpoint with `DetectionValidator`, performing predictions with `DetectionPredictor`, and demonstrates how to resume training from the last checkpoint by setting the 'resume' key in the overrides dictionary.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/python.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.models.yolo import DetectionPredictor, DetectionTrainer, DetectionValidator\n\n# trainer\ntrainer = DetectionTrainer(overrides={})\ntrainer.train()\ntrained_model = trainer.best\n\n# Validator\nval = DetectionValidator(args=...)\nval(model=trained_model)\n\n# predictor\npred = DetectionPredictor(overrides={})\npred(source=SOURCE, model=trained_model)\n\n# resume from last weight\noverrides[\"resume\"] = trainer.last\ntrainer = DetectionTrainer(overrides=overrides)\n\n```\n\n----------------------------------------\n\nTITLE: Training Custom YOLO Model for Object Detection\nDESCRIPTION: Example code demonstrating how to load a pre-trained YOLO model and train it on a custom dataset for object detection. Includes both Python and CLI examples.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/index.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pre-trained YOLO model (you can choose n, s, m, l, or x versions)\nmodel = YOLO(\"yolo11n.pt\")\n\n# Start training on your custom dataset\nmodel.train(data=\"path/to/dataset.yaml\", epochs=100, imgsz=640)\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Train a YOLO model from the command line\nyolo detect train data=path/to/dataset.yaml epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Models to ONNX Format - Python\nDESCRIPTION: Demonstrates loading a YOLO11 model with the Ultralytics Python API and exporting it to the ONNX format for improved compatibility and inference speed. Requires the ultralytics Python package and a trained YOLO11 model checkpoint (.pt file). The 'format' parameter determines the output format; input may be an official or custom-trained model. Output is an ONNX model file suitable for deployment on multiple platforms.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/export.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load an official model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom trained model\n\n# Export the model\nmodel.export(format=\"onnx\")\n```\n\n----------------------------------------\n\nTITLE: Performing Multi-GPU YOLO11 Training with Python\nDESCRIPTION: Shows how to train a YOLO11 model across multiple GPUs using the Python API. A pretrained model is loaded, and the `model.train()` method is called with the `device` parameter set to a list of GPU IDs (e.g., `[0, 1]`) to distribute the training load. This example uses the COCO8 dataset, 100 epochs, and an image size of 640.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/train.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model with 2 GPUs\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640, device=[0, 1])\n```\n\n----------------------------------------\n\nTITLE: Loading and Using YOLOv8 Model in Python\nDESCRIPTION: This snippet demonstrates how to load a pre-trained YOLOv8 model, display model information, train it on a dataset, and run inference on an image using Python.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/index.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a COCO-pretrained YOLOv8n model\nmodel = YOLO(\"yolov8n.pt\")\n\n# Display model information (optional)\nmodel.info()\n\n# Train the model on the COCO8 example dataset for 100 epochs\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n\n# Run inference with the YOLOv8n model on the 'bus.jpg' image\nresults = model(\"path/to/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Object Detection with YOLO11 in Python\nDESCRIPTION: This snippet demonstrates how to load a pre-trained YOLO11 model and perform object detection on an image using Python. It shows the basic steps of importing the YOLO class, loading a model, making predictions, and displaying results.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/index.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pre-trained YOLO model (adjust model type as needed)\nmodel = YOLO(\"yolo11n.pt\")  # n, s, m, l, x versions available\n\n# Perform object detection on an image\nresults = model.predict(source=\"image.jpg\")  # Can also use video, directory, URL, etc.\n\n# Display the results\nresults[0].show()  # Show the first image results\n```\n\n----------------------------------------\n\nTITLE: Validating YOLO Model Accuracy Using CLI\nDESCRIPTION: This command is used to validate the accuracy of a pretrained YOLO detection model. Dependencies include having a pretrained model and a dataset. Key parameters are the model path, data configuration, batch size, and image size. It outputs performance metrics such as mAP, precision, and recall.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/cli.md#2025-04-22_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nyolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Initializing and Training YOLO11 Models with Python\nDESCRIPTION: Demonstrates how to start training a YOLO11 model using the Ultralytics Python API. It covers building a model from a YAML configuration, loading a pretrained model (.pt file), or building from YAML and transferring weights from a .pt file. The `model.train()` method is used to initiate training on the COCO8 dataset for 100 epochs with an image size of 640. The device (GPU or CPU) is typically auto-detected.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/train.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.yaml\")  # build a new model from YAML\nmodel = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\nmodel = YOLO(\"yolo11n.yaml\").load(\"yolo11n.pt\")  # build from YAML and transfer weights\n\n# Train the model\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Streaming Video Inference with YOLO and OpenCV in Python\nDESCRIPTION: This Python script processes a video file frame-by-frame, running Ultralytics YOLO inference on each frame and visualizing the results using OpenCV. It creates a YOLO model instance, opens a video capture stream, iterates over all frames, and displays annotated outputs in a GUI window. Requires the opencv-python and ultralytics packages, and a YOLO model path. User can exit by pressing 'q'. Input is a video file and outputs are displayed frames with predictions.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/predict.md#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import YOLO\n\n# Load the YOLO model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Open the video file\nvideo_path = \"path/to/your/video/file.mp4\"\ncap = cv2.VideoCapture(video_path)\n\n# Loop through the video frames\nwhile cap.isOpened():\n    # Read a frame from the video\n    success, frame = cap.read()\n\n    if success:\n        # Run YOLO inference on the frame\n        results = model(frame)\n\n        # Visualize the results on the frame\n        annotated_frame = results[0].plot()\n\n        # Display the annotated frame\n        cv2.imshow(\"YOLO Inference\", annotated_frame)\n\n        # Break the loop if 'q' is pressed\n        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n            break\n    else:\n        # Break the loop if the end of the video is reached\n        break\n\n# Release the video capture object and close the display window\ncap.release()\ncv2.destroyAllWindows()\n\n```\n\n----------------------------------------\n\nTITLE: Configuring YOLOv8 ONNX Runtime C++ Project with CMake\nDESCRIPTION: Complete CMake configuration that sets up a C++ project for YOLOv8 inference using ONNX Runtime. Handles cross-platform build settings, dependencies management, and post-build file copying. Supports CUDA acceleration when available and includes OpenCV integration.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-ONNXRuntime-CPP/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.5)\n\nset(PROJECT_NAME Yolov8OnnxRuntimeCPPInference)\nproject(${PROJECT_NAME} VERSION 0.0.1 LANGUAGES CXX)\n\n\n# -------------- Support C++17 for using filesystem  ------------------#\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\nset(CMAKE_CXX_EXTENSIONS ON)\nset(CMAKE_INCLUDE_CURRENT_DIR ON)\n\n\n# -------------- OpenCV  ------------------#\nfind_package(OpenCV REQUIRED)\ninclude_directories(${OpenCV_INCLUDE_DIRS})\n\n\n# -------------- Compile CUDA for FP16 inference if needed  ------------------#\noption(USE_CUDA \"Enable CUDA support\" ON)\nif (NOT APPLE AND USE_CUDA)\n    find_package(CUDA REQUIRED)\n    include_directories(${CUDA_INCLUDE_DIRS})\n    add_definitions(-DUSE_CUDA)\nelse ()\n    set(USE_CUDA OFF)\nendif ()\n\n# -------------- ONNXRUNTIME  ------------------#\n\n# Set ONNXRUNTIME_VERSION\nset(ONNXRUNTIME_VERSION 1.15.1)\n\nif (WIN32)\n    if (USE_CUDA)\n        set(ONNXRUNTIME_ROOT \"${CMAKE_CURRENT_SOURCE_DIR}/onnxruntime-win-x64-gpu-${ONNXRUNTIME_VERSION}\")\n    else ()\n        set(ONNXRUNTIME_ROOT \"${CMAKE_CURRENT_SOURCE_DIR}/onnxruntime-win-x64-${ONNXRUNTIME_VERSION}\")\n    endif ()\nelseif (LINUX)\n    if (USE_CUDA)\n        set(ONNXRUNTIME_ROOT \"${CMAKE_CURRENT_SOURCE_DIR}/onnxruntime-linux-x64-gpu-${ONNXRUNTIME_VERSION}\")\n    else ()\n        set(ONNXRUNTIME_ROOT \"${CMAKE_CURRENT_SOURCE_DIR}/onnxruntime-linux-x64-${ONNXRUNTIME_VERSION}\")\n    endif ()\nelseif (APPLE)\n    set(ONNXRUNTIME_ROOT \"${CMAKE_CURRENT_SOURCE_DIR}/onnxruntime-osx-arm64-${ONNXRUNTIME_VERSION}\")\n    # Apple X64 binary\n    # set(ONNXRUNTIME_ROOT \"${CMAKE_CURRENT_SOURCE_DIR}/onnxruntime-osx-x64-${ONNXRUNTIME_VERSION}\")\n    # Apple Universal binary\n    # set(ONNXRUNTIME_ROOT \"${CMAKE_CURRENT_SOURCE_DIR}/onnxruntime-osx-universal2-${ONNXRUNTIME_VERSION}\")\nelse ()\n    message(SEND_ERROR \"Variable ONNXRUNTIME_ROOT is not set properly. Please check if your cmake project \\\n    is not compiled with `-D WIN32=TRUE`, `-D LINUX=TRUE`, or `-D APPLE=TRUE`!\")\nendif ()\n\ninclude_directories(${PROJECT_NAME} ${ONNXRUNTIME_ROOT}/include)\n\nset(PROJECT_SOURCES\n        main.cpp\n        inference.h\n        inference.cpp\n)\n\nadd_executable(${PROJECT_NAME} ${PROJECT_SOURCES})\n\nif (WIN32)\n    target_link_libraries(${PROJECT_NAME} ${OpenCV_LIBS} ${ONNXRUNTIME_ROOT}/lib/onnxruntime.lib)\n    if (USE_CUDA)\n        target_link_libraries(${PROJECT_NAME} ${CUDA_LIBRARIES})\n    endif ()\nelseif (LINUX)\n    target_link_libraries(${PROJECT_NAME} ${OpenCV_LIBS} ${ONNXRUNTIME_ROOT}/lib/libonnxruntime.so)\n    if (USE_CUDA)\n        target_link_libraries(${PROJECT_NAME} ${CUDA_LIBRARIES})\n    endif ()\nelseif (APPLE)\n    target_link_libraries(${PROJECT_NAME} ${OpenCV_LIBS} ${ONNXRUNTIME_ROOT}/lib/libonnxruntime.dylib)\nendif ()\n\n# For windows system, copy onnxruntime.dll to the same folder of the executable file\nif (WIN32)\n    add_custom_command(TARGET ${PROJECT_NAME} POST_BUILD\n            COMMAND ${CMAKE_COMMAND} -E copy_if_different\n            \"${ONNXRUNTIME_ROOT}/lib/onnxruntime.dll\"\n            $<TARGET_FILE_DIR:${PROJECT_NAME}>)\nendif ()\n\n# Download https://raw.githubusercontent.com/ultralytics/ultralytics/main/ultralytics/cfg/datasets/coco.yaml\n# and put it in the same folder of the executable file\nconfigure_file(coco.yaml ${CMAKE_CURRENT_BINARY_DIR}/coco.yaml COPYONLY)\n\n# Copy yolov8n.onnx file to the same folder of the executable file\nconfigure_file(yolov8n.onnx ${CMAKE_CURRENT_BINARY_DIR}/yolov8n.onnx COPYONLY)\n\n# Create folder name images in the same folder of the executable file\nadd_custom_command(TARGET ${PROJECT_NAME} POST_BUILD\n    COMMAND ${CMAKE_COMMAND} -E make_directory ${CMAKE_CURRENT_BINARY_DIR}/images\n)\n```\n\n----------------------------------------\n\nTITLE: Performing Batched Inference with Ultralytics YOLO (List Output) in Python\nDESCRIPTION: This Python snippet shows how to load a pretrained Ultralytics YOLO11 model and perform inference on a batch of images specified by their file paths. The `model()` call returns a list of `Results` objects, one for each input image. The example then iterates through the results, extracting various prediction outputs (bounding boxes, masks, keypoints, classification probabilities, oriented bounding boxes) and demonstrates how to display and save the annotated image. Requires the `ultralytics` library and a model file like `yolo11n.pt`.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/predict.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # pretrained YOLO11n model\n\n# Run batched inference on a list of images\nresults = model([\"image1.jpg\", \"image2.jpg\"])  # return a list of Results objects\n\n# Process results list\nfor result in results:\n    boxes = result.boxes  # Boxes object for bounding box outputs\n    masks = result.masks  # Masks object for segmentation masks outputs\n    keypoints = result.keypoints  # Keypoints object for pose outputs\n    probs = result.probs  # Probs object for classification outputs\n    obb = result.obb  # Oriented boxes object for OBB outputs\n    result.show()  # display to screen\n    result.save(filename=\"result.jpg\")  # save to disk\n```\n\n----------------------------------------\n\nTITLE: Configuring Batch Size for YOLO11 Training in Python\nDESCRIPTION: Sets the batch size for YOLO11 model training. Using batch=-1 automatically determines the optimal batch size based on the device's capabilities.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/model-training-tips.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nbatch=-1\n```\n\n----------------------------------------\n\nTITLE: Running YOLO Inference on an OpenCV Image Array in Python\nDESCRIPTION: This snippet demonstrates running YOLO inference on an image loaded using OpenCV. It requires importing `cv2` and `YOLO` from `ultralytics`. An image is read using `cv2.imread` (resulting in a NumPy array in BGR format), which is then passed as the source to the loaded YOLO model. The output is a list of Results objects.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/predict.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Read an image using OpenCV\nsource = cv2.imread(\"path/to/image.jpg\")\n\n# Run inference on the source\nresults = model(source)  # list of Results objects\n```\n\n----------------------------------------\n\nTITLE: Validating YOLO11 Models with a Custom Dataset in Python (Python)\nDESCRIPTION: Demonstrates performing validation using a custom dataset configuration YAML file with the YOLO11 Python API. Shows how to load a model and specify custom validation data, outputting the usual set of performance metrics. Requires 'ultralytics', a .pt model, and a correctly formatted custom dataset config. The key parameter is 'data', pointing to the custom YAML file.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/val.md#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Validate with a custom dataset\nmetrics = model.val(data=\"path/to/your/custom_dataset.yaml\")\nprint(metrics.box.map)  # map50-95\n\n```\n\n----------------------------------------\n\nTITLE: Running Object Tracking with Ultralytics YOLO in Python\nDESCRIPTION: This Python code demonstrates how to load trained YOLO models and perform object tracking on video streams using the Ultralytics library. The primary function used is 'track', which operates on various types of models, such as Detect, Segment, and Pose. The code covers real-time video processing, tracker switching, and result visualization. Dependencies include the 'ultralytics' package.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/track.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load an official or custom model\nmodel = YOLO(\"yolo11n.pt\")  # Load an official Detect model\nmodel = YOLO(\"yolo11n-seg.pt\")  # Load an official Segment model\nmodel = YOLO(\"yolo11n-pose.pt\")  # Load an official Pose model\nmodel = YOLO(\"path/to/best.pt\")  # Load a custom trained model\n\n# Perform tracking with the model\nresults = model.track(\"https://youtu.be/LNwODJXcvt4\", show=True)  # Tracking with default tracker\nresults = model.track(\"https://youtu.be/LNwODJXcvt4\", show=True, tracker=\"bytetrack.yaml\")  # with ByteTrack\n```\n\n----------------------------------------\n\nTITLE: YOLO Prediction Implementation\nDESCRIPTION: Shows comprehensive prediction functionality including handling different input sources and processing prediction results.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/python.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nfrom PIL import Image\n\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"model.pt\")\n# accepts all formats - image/dir/Path/URL/video/PIL/ndarray. 0 for webcam\nresults = model.predict(source=\"0\")\nresults = model.predict(source=\"folder\", show=True)  # Display preds. Accepts all YOLO predict arguments\n\n# from PIL\nim1 = Image.open(\"bus.jpg\")\nresults = model.predict(source=im1, save=True)  # save plotted images\n\n# from ndarray\nim2 = cv2.imread(\"bus.jpg\")\nresults = model.predict(source=im2, save=True, save_txt=True)  # save predictions as labels\n\n# from list of PIL/ndarray\nresults = model.predict(source=[im1, im2])\n```\n\n----------------------------------------\n\nTITLE: Python YOLO Implementation\nDESCRIPTION: Complete Python example showing how to load a YOLO model, train it on a dataset, evaluate performance, perform detection, and export the model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Train the model on the COCO8 dataset for 100 epochs\ntrain_results = model.train(\n    data=\"coco8.yaml\",  # Path to dataset configuration file\n    epochs=100,  # Number of training epochs\n    imgsz=640,  # Image size for training\n    device=\"cpu\",  # Device to run on (e.g., 'cpu', 0, [0,1,2,3])\n)\n\n# Evaluate the model's performance on the validation set\nmetrics = model.val()\n\n# Perform object detection on an image\nresults = model(\"path/to/image.jpg\")  # Predict on an image\nresults[0].show()  # Display results\n\n# Export the model to ONNX format for deployment\npath = model.export(format=\"onnx\")  # Returns the path to the exported model\n```\n\n----------------------------------------\n\nTITLE: Basic YOLO Prediction with Results Object\nDESCRIPTION: Demonstrates how to load a YOLO model and run inference on single and batch images, returning Results objects.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/predict.md#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Run inference on an image\nresults = model(\"https://ultralytics.com/images/bus.jpg\")\nresults = model([\n    \"https://ultralytics.com/images/bus.jpg\",\n    \"https://ultralytics.com/images/zidane.jpg\",\n])  # batch inference\n```\n\n----------------------------------------\n\nTITLE: Basic YOLO Operations in Python\nDESCRIPTION: Demonstrates core YOLO operations including creating new models, loading pretrained models, training, validation, detection and export to ONNX format.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/python.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Create a new YOLO model from scratch\nmodel = YOLO(\"yolo11n.yaml\")\n\n# Load a pretrained YOLO model (recommended for training)\nmodel = YOLO(\"yolo11n.pt\")\n\n# Train the model using the 'coco8.yaml' dataset for 3 epochs\nresults = model.train(data=\"coco8.yaml\", epochs=3)\n\n# Evaluate the model's performance on the validation set\nresults = model.val()\n\n# Perform object detection on an image using the model\nresults = model(\"https://ultralytics.com/images/bus.jpg\")\n\n# Export the model to ONNX format\nsuccess = model.export(format=\"onnx\")\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Segmentation Model\nDESCRIPTION: Code examples showing how to train a YOLO11 segmentation model using both Python and CLI approaches. Demonstrates loading models from YAML or pretrained weights and training on the COCO8-seg dataset.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/segment.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-seg.yaml\")  # build a new model from YAML\nmodel = YOLO(\"yolo11n-seg.pt\")  # load a pretrained model (recommended for training)\nmodel = YOLO(\"yolo11n-seg.yaml\").load(\"yolo11n.pt\")  # build from YAML and transfer weights\n\n# Train the model\nresults = model.train(data=\"coco8-seg.yaml\", epochs=100, imgsz=640)\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Build a new model from YAML and start training from scratch\nyolo segment train data=coco8-seg.yaml model=yolo11n-seg.yaml epochs=100 imgsz=640\n\n# Start training from a pretrained *.pt model\nyolo segment train data=coco8-seg.yaml model=yolo11n-seg.pt epochs=100 imgsz=640\n\n# Build a new model from YAML, transfer pretrained weights to it and start training\nyolo segment train data=coco8-seg.yaml model=yolo11n-seg.yaml pretrained=yolo11n-seg.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Basic YOLO CLI Syntax in Bash\nDESCRIPTION: The foundational syntax for all Ultralytics YOLO CLI commands, illustrating the required structure with TASK, MODE, and ARGS parameters.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/cli.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nyolo TASK MODE ARGS\n```\n\n----------------------------------------\n\nTITLE: Loop-Based Object Tracking in Python Using OpenCV and Ultralytics YOLO\nDESCRIPTION: This Python script showcases a loop-based implementation of object tracking using OpenCV and the Ultralytics YOLO model on video files. The script processes each frame sequentially, applying YOLO tracking with persistent tracks, and displays annotated frames using OpenCV's 'imshow'. It requires 'opencv-python' and 'ultralytics' packages.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/track.md#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Open the video file\nvideo_path = \"path/to/video.mp4\"\ncap = cv2.VideoCapture(video_path)\n\n# Loop through the video frames\nwhile cap.isOpened():\n    # Read a frame from the video\n    success, frame = cap.read()\n\n    if success:\n        # Run YOLO11 tracking on the frame, persisting tracks between frames\n        results = model.track(frame, persist=True)\n\n        # Visualize the results on the frame\n        annotated_frame = results[0].plot()\n\n        # Display the annotated frame\n        cv2.imshow(\"YOLO11 Tracking\", annotated_frame)\n\n        # Break the loop if 'q' is pressed\n        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n            break\n    else:\n        # Break the loop if the end of the video is reached\n        break\n\n# Release the video capture object and close the display window\ncap.release()\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Training a YOLOv11 Model using Python\nDESCRIPTION: Demonstrates how to initiate training for a YOLOv11 object detection model using the Ultralytics Python library. This snippet imports the YOLO class, loads a pre-trained model ('yolo11n.pt'), and starts the training process on the 'coco8.yaml' dataset for 100 epochs with an image size of 640 pixels. Dependencies include the 'ultralytics' Python package.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolo11.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a COCO-pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Train the model on the COCO8 example dataset for 100 epochs\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking YOLOv8 Performance in Python\nDESCRIPTION: Provides Python code from the FAQ section to benchmark a YOLOv8 model (`yolov8n.pt`) using the `benchmark` utility from `ultralytics.utils.benchmarks`. Requires specifying the model, data (`coco8.yaml`), image size, precision (`half`), and target device (e.g., `device=0` for GPU).\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov8.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.utils.benchmarks import benchmark\n\n# Benchmark on GPU\nbenchmark(model=\"yolov8n.pt\", data=\"coco8.yaml\", imgsz=640, half=False, device=0)\n```\n\n----------------------------------------\n\nTITLE: Counting Specific Object Classes with Ultralytics YOLO11 in Python\nDESCRIPTION: This Python function demonstrates how to count specific classes of objects in a video using Ultralytics YOLO11. It creates a video processing pipeline that reads frames, applies object counting with specified class filters, and writes the processed frames to an output video file. The example specifically counts objects of classes 0 and 2 (person and car in COCO dataset).\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/object-counting.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\n\ndef count_specific_classes(video_path, output_video_path, model_path, classes_to_count):\n    \"\"\"Count specific classes of objects in a video.\"\"\"\n    cap = cv2.VideoCapture(video_path)\n    assert cap.isOpened(), \"Error reading video file\"\n    w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n    video_writer = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n\n    line_points = [(20, 400), (1080, 400)]\n    counter = solutions.ObjectCounter(show=True, region=line_points, model=model_path, classes=classes_to_count)\n\n    while cap.isOpened():\n        success, im0 = cap.read()\n        if not success:\n            print(\"Video frame is empty or processing is complete.\")\n            break\n        results = counter(im0)\n        video_writer.write(results.plot_im)\n\n    cap.release()\n    video_writer.release()\n    cv2.destroyAllWindows()\n\n\ncount_specific_classes(\"path/to/video.mp4\", \"output_specific_classes.avi\", \"yolo11n.pt\", [0, 2])\n```\n\n----------------------------------------\n\nTITLE: Validating YOLO11 Models Using Ultralytics Python API (Python)\nDESCRIPTION: Demonstrates loading an official or custom YOLO11 model in Python via the Ultralytics API and performing validation. The validation process automatically remembers training arguments such as dataset and image size, enabling easy metric computation (mAP50-95, mAP50, mAP75) through the returned metrics object. Requires the 'ultralytics' Python package and a pretrained or custom .pt model file. Outputs include numeric metrics accessed as object attributes; no inputs are required unless overriding defaults.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/val.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load an official model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom model\n\n# Validate the model\nmetrics = model.val()  # no arguments needed, dataset and settings remembered\nmetrics.box.map  # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps  # a list contains map50-95 of each category\n\n```\n\n----------------------------------------\n\nTITLE: Performing Thread-Safe Inference with Multiple YOLO Models in Python\nDESCRIPTION: This Python snippet demonstrates how to perform thread-safe inference by creating a separate YOLO model instance in each thread. It uses Python's threading module and the Ultralytics YOLO API. Each thread loads the model independently and runs prediction on a separate image, preventing shared state issues and ensuring reliable results. Requires the ultralytics library and a compatible model file (e.g., 'yolo11n.pt').\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/predict.md#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nfrom threading import Thread\n\nfrom ultralytics import YOLO\n\n\ndef thread_safe_predict(model, image_path):\n    \"\"\"Performs thread-safe prediction on an image using a locally instantiated YOLO model.\"\"\"\n    model = YOLO(model)\n    results = model.predict(image_path)\n    # Process results\n\n\n# Starting threads that each have their own model instance\nThread(target=thread_safe_predict, args=(\"yolo11n.pt\", \"image1.jpg\")).start()\nThread(target=thread_safe_predict, args=(\"yolo11n.pt\", \"image2.jpg\")).start()\n\n```\n\n----------------------------------------\n\nTITLE: Performing Object Detection with YOLO in Python\nDESCRIPTION: This Python snippet shows a basic example of integrating Ultralytics YOLO for object detection. It imports the YOLO class, loads a pretrained model (`yolo11n.pt`), runs inference on a sample image URL, and then iterates through the results to display them visually using the `show()` method available on each result object. This demonstrates the simplest way to get predictions from a YOLO model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/python.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Perform object detection on an image\nresults = model(\"https://ultralytics.com/images/bus.jpg\")\n\n# Visualize the results\nfor result in results:\n    result.show()\n\n```\n\n----------------------------------------\n\nTITLE: Training and Using YOLO11 Pose Models\nDESCRIPTION: This Python snippet focuses on using YOLO11 pose models for training and prediction tasks. It depends on the `ultralytics` library and employs parameters like `data` for dataset paths and `epochs` for specifying training duration. Outputs are available in the form of training performance and pose prediction results.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolo11n-pose.pt')  # load a pretrained YOLO pose model\nmodel.train(data='coco8-pose.yaml', epochs=3)  # train the model\nmodel('https://ultralytics.com/images/bus.jpg')  # predict on an image\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Mutual Information Ordering – Python\nDESCRIPTION: This Python snippet uses mathematical notation to express the chain of mutual information values through neural network transformations. It illustrates the Information Bottleneck Principle, emphasizing how information is progressively diminished from input (X) through function layers parameterized by theta and phi. No external dependencies are required; the snippet is conceptual, accepts symbolic variables X, f_theta, and g_phi, and outputs the information ordering relation. It serves for theoretical explanations and has no runtime limitations.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov9.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nI(X, X) >= I(X, f_theta(X)) >= I(X, g_phi(f_theta(X)))\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Models to TensorRT (INT8 Quantization) - Python\nDESCRIPTION: Shows how to export a YOLO11 model using the Python API to TensorRT ('engine') format with INT8 quantization enabled, optimizing for speed and size on compatible hardware. Requires ultralytics Python package and access to a trained model. The 'int8' flag triggers INT8 quantization; output is an engine file for NVIDIA hardware with improved efficiency.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/export.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolo11n.pt\")  # Load a model\nmodel.export(format=\"engine\", int8=True)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking YOLO11n Model in Python\nDESCRIPTION: This code snippet demonstrates how to load a YOLO11n PyTorch model and benchmark its speed and accuracy on the COCO8 dataset for all export formats using Python.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/raspberry-pi.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a YOLO11n PyTorch model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Benchmark YOLO11n speed and accuracy on the COCO8 dataset for all all export formats\nresults = model.benchmark(data=\"coco8.yaml\", imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Running YOLO Inference on an Image File in Python\nDESCRIPTION: This snippet demonstrates how to run YOLO inference on a single image file using the Ultralytics Python library. It involves loading a pretrained YOLO model ('yolo11n.pt'), specifying the path to the image file as the source, and then calling the model with the source path. The result is a list of Results objects containing the detection information.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/predict.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Define path to the image file\nsource = \"path/to/image.jpg\"\n\n# Run inference on the source\nresults = model(source)  # list of Results objects\n```\n\n----------------------------------------\n\nTITLE: Running Inference with YOLO11 Classification Models in Python\nDESCRIPTION: Shows how to use trained YOLO11 classification models to make predictions on new images in Python, supporting both official and custom trained models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/classify.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-cls.pt\")  # load an official model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom model\n\n# Predict with the model\nresults = model(\"https://ultralytics.com/images/bus.jpg\")  # predict on an image\n```\n\n----------------------------------------\n\nTITLE: Training and Running Ultralytics YOLO11 via Python API (PyTorch) - Python\nDESCRIPTION: This snippet shows how to use the Ultralytics Python API to load a pretrained YOLO11 model, train it on a dataset, and run inference on an image. It requires the ultralytics Python package with PyTorch backend and that the model and data files (e.g., 'yolo11n.pt', 'coco8.yaml') are accessible. The 'YOLO()' class is used to load the model, '.train()' performs training with configurable parameters such as data path, epochs, and image size, and inference is accomplished by directly calling the model. Outputs include training results and prediction results for the given input image.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolo11.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a COCO-pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Train the model on the COCO8 example dataset for 100 epochs\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n\n# Run inference with the YOLO11n model on the 'bus.jpg' image\nresults = model(\"path/to/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Predicting with YOLO-World in Python\nDESCRIPTION: This Python snippet shows how to initialize a YOLO-World model and perform object detection on an image using the predict method. The main input is the image path, and the output is the detection results displayed with the model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolo-world.md#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics import YOLOWorld\n\n# Initialize a YOLO-World model\nmodel = YOLOWorld(\"yolov8s-world.pt\")  # or select yolov8m/l-world.pt for different sizes\n\n# Execute inference with the YOLOv8s-world model on the specified image\nresults = model.predict(\"path/to/image.jpg\")\n\n# Show results\nresults[0].show()\n```\n\n----------------------------------------\n\nTITLE: Loading and Validating YOLO Segmentation Model in Python\nDESCRIPTION: This snippet demonstrates how to load a pretrained YOLO segmentation model and validate it using Python. It prints the Mean Average Precision for both boxes and masks.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/segment.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained model\nmodel = YOLO(\"yolo11n-seg.pt\")\n\n# Validate the model\nmetrics = model.val()\nprint(\"Mean Average Precision for boxes:\", metrics.box.map)\nprint(\"Mean Average Precision for masks:\", metrics.seg.map)\n```\n\n----------------------------------------\n\nTITLE: Mounting Local Directory in Ultralytics Docker Container using Bash\nDESCRIPTION: This command demonstrates how to mount a local directory into the Ultralytics Docker container for file accessibility.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/docker-quickstart.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nsudo docker run -it --ipc=host --gpus all -v /path/on/host:/path/in/container $t\n```\n\n----------------------------------------\n\nTITLE: Running YOLO Inference on a Directory (Streaming) in Python\nDESCRIPTION: This snippet demonstrates running YOLO inference on all compatible files (images/videos) within a specified directory using streaming mode. It loads a YOLO model, sets the `source` to the directory path, and runs inference with `stream=True`. The model iterates through the files in the directory, returning a generator of Results objects, making it suitable for large collections of media.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/predict.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Define path to directory containing images and videos for inference\nsource = \"path/to/dir\"\n\n# Run inference on the source\nresults = model(source, stream=True)  # generator of Results objects\n```\n\n----------------------------------------\n\nTITLE: Performing Multi-GPU YOLO11 Training via CLI\nDESCRIPTION: Demonstrates how to initiate multi-GPU training for a YOLO11 model using the command-line interface. The `device` argument is used to specify a comma-separated list of GPU IDs (e.g., `0,1`). This command starts training from a pretrained model on the COCO8 dataset for 100 epochs with an image size of 640, utilizing GPUs 0 and 1.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/train.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model using GPUs 0 and 1\nyolo detect train data=coco8.yaml model=yolo11n.pt epochs=100 imgsz=640 device=0,1\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n on COCO8 Dataset Using Python\nDESCRIPTION: Python code example for training a pretrained YOLO11n model on the COCO8 dataset for 100 epochs with 640px image size. This demonstrates how to load a model and configure training parameters programmatically.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/coco8.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Train the model on COCO8\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Training with YOLO-World in Python\nDESCRIPTION: This Python snippet demonstrates how to load a pre-trained YOLOv8s-worldv2 model using Ultralytics library, train it on the COCO8 dataset for 100 epochs, and perform inference on an image. Dependencies include PyTorch pretrained models and configuration YAML files. Key parameters are the number of epochs, image size, and dataset. The output is a trained model and inference results.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolo-world.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics import YOLOWorld\n\n# Load a pretrained YOLOv8s-worldv2 model\nmodel = YOLOWorld(\"yolov8s-worldv2.pt\")\n\n# Train the model on the COCO8 example dataset for 100 epochs\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n\n# Run inference with the YOLOv8n model on the 'bus.jpg' image\nresults = model(\"path/to/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Models on Open Images V7 with Python\nDESCRIPTION: Python code example for training a COCO-pretrained YOLO11n model on the Open Images V7 dataset with specified epochs and image size parameters.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/open-images-v7.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a COCO-pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Train the model on the Open Images V7 dataset\nresults = model.train(data=\"open-images-v7.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Pose Model in Python\nDESCRIPTION: Code for loading and training a YOLO11 pose estimation model using Python. Shows multiple ways to initialize the model including from YAML or pretrained weights.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/pose.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-pose.yaml\")  # build a new model from YAML\nmodel = YOLO(\"yolo11n-pose.pt\")  # load a pretrained model (recommended for training)\nmodel = YOLO(\"yolo11n-pose.yaml\").load(\"yolo11n-pose.pt\")  # build from YAML and transfer weights\n\n# Train the model\nresults = model.train(data=\"coco8-pose.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: YOLO Export Examples\nDESCRIPTION: Demonstrates exporting YOLO models to different formats including ONNX and TensorRT for deployment.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/python.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolo11n.pt\")\nmodel.export(format=\"onnx\", dynamic=True)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolo11n.pt\")\nmodel.export(format=\"engine\", device=0)\n```\n\n----------------------------------------\n\nTITLE: Fine-Tuning YOLOE Model in Python\nDESCRIPTION: This snippet demonstrates how to fine-tune a YOLOE model using a custom dataset. It requires the Ultralytics YOLOE library and a pre-trained YOLOE model file. The model is trained with custom settings like epochs, batch size, and other hyperparameters. The training uses a specified trainer class to guide the process.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yoloe.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLOE\nfrom ultralytics.models.yolo.yoloe import YOLOEPESegTrainer\n\nmodel = YOLOE(\"yoloe-11s-seg.pt\")\n\nmodel.train(\n    data=\"coco128-seg.yaml\",\n    epochs=80,\n    close_mosaic=10,\n    batch=128,\n    optimizer=\"AdamW\",\n    lr0=1e-3,\n    warmup_bias_lr=0.0,\n    weight_decay=0.025,\n    momentum=0.9,\n    workers=4,\n    device=\"0\",\n    trainer=YOLOEPESegTrainer,\n)\n```\n\n----------------------------------------\n\nTITLE: YOLO Model Training Examples\nDESCRIPTION: Shows different approaches to training YOLO models including training from pretrained models, training from scratch, and resuming training from checkpoints.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/python.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolo11n.pt\")  # pass any model type\nresults = model.train(epochs=5)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolo11n.yaml\")\nresults = model.train(data=\"coco8.yaml\", epochs=5)\n```\n\nLANGUAGE: python\nCODE:\n```\nmodel = YOLO(\"last.pt\")\nresults = model.train(resume=True)\n```\n\n----------------------------------------\n\nTITLE: Resuming Interrupted YOLO11 Training with Python\nDESCRIPTION: Demonstrates how to resume an interrupted YOLO11 training session using the Python API. A partially trained model is loaded from its last saved checkpoint (`last.pt`). The `model.train()` method is then called with the `resume` parameter set to `True`, which restores the model weights, optimizer state, learning rate scheduler, and epoch number.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/train.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"path/to/last.pt\")  # load a partially trained model\n\n# Resume training\nresults = model.train(resume=True)\n```\n\n----------------------------------------\n\nTITLE: Implementing Object Counting with YOLO11\nDESCRIPTION: Complete implementation of video-based object counting using Ultralytics YOLO11. The code handles video input/output, sets up an object counter with custom parameters, and processes frames for object detection and counting.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/object_counting.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\n# Open the video file\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nassert cap.isOpened(), \"Error reading video file\"\n\n# Get video properties: width, height, and frames per second (fps)\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n\n# Define points for a line or region of interest in the video frame\nline_points = [(20, 400), (1080, 400)]  # Line coordinates\n\n# Initialize the video writer to save the output video\nvideo_writer = cv2.VideoWriter(\"object_counting_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n\n# Initialize the Object Counter with visualization options and other parameters\ncounter = solutions.ObjectCounter(\n    show=True,  # Display the image during processing\n    region=line_points,  # Region of interest points\n    model=\"yolo11n.pt\",  # Ultralytics YOLO11 model file\n    line_width=2,  # Thickness of the lines and bounding boxes\n)\n\n# Process video frames in a loop\nwhile cap.isOpened():\n    success, im0 = cap.read()\n    if not success:\n        print(\"Video frame is empty or video processing has been successfully completed.\")\n        break\n\n    # Use the Object Counter to count objects in the frame and get the annotated image\n    results = counter(im0)\n\n    # Write the annotated frame to the output video\n    video_writer.write(results.plot_im)\n\n# Release the video capture and writer objects\ncap.release()\nvideo_writer.release()\n\n# Close all OpenCV windows\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Minimal Example of Real-time Queue Management with YOLO11\nDESCRIPTION: This Python script provides a minimal example of using Ultralytics YOLO11 for real-time queue management. It includes loading the model, capturing video, defining the region of interest, and processing frames.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/queue-management.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nqueue_region = [(20, 400), (1080, 400), (1080, 360), (20, 360)]\n\nqueuemanager = solutions.QueueManager(\n    model=\"yolo11n.pt\",\n    region=queue_region,\n    line_width=3,\n    show=True,\n)\n\nwhile cap.isOpened():\n    success, im0 = cap.read()\n    if success:\n        results = queuemanager(im0)\n\ncap.release()\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Using YOLO11 in Python for Model Operations\nDESCRIPTION: This code snippet demonstrates how to load a YOLO11 model in Python and perform operations like training, validation, prediction, and export. Dependencies include the `ultralytics` package, and key parameters used are `data` for dataset configuration, `epochs` for training duration, and `format` for export type. Outputs include model performance results and exported models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO('yolo11n.yaml')  # build a new model from scratch\nmodel = YOLO('yolo11n.pt')  # load a pretrained model (recommended for training)\n\n# Use the model\nresults = model.train(data='coco8.yaml', epochs=3)  # train the model\nresults = model.val()  # evaluate model performance on the validation set\nresults = model('https://ultralytics.com/images/bus.jpg')  # predict on an image\nresults = model.export(format='onnx')  # export the model to ONNX format\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Model on Custom Data via CLI\nDESCRIPTION: This example shows how to train a YOLO11 model on a custom dataset using the command line interface.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/index.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyolo train model=yolo11n.pt data='custom_data.yaml' epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Benchmarking YOLO Models on Roboflow 100 Dataset in Python\nDESCRIPTION: This script demonstrates how to programmatically benchmark an Ultralytics YOLO model (e.g., YOLOv11n) on all 100 datasets within the Roboflow 100 benchmark using the RF100Benchmark class. It handles dataset parsing, YAML configuration, model training, validation, and evaluation while storing results in designated log files.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/roboflow-100.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport shutil\nfrom pathlib import Path\n\nfrom ultralytics.utils.benchmarks import RF100Benchmark\n\n# Initialize RF100Benchmark and set API key\nbenchmark = RF100Benchmark()\nbenchmark.set_key(api_key=\"YOUR_ROBOFLOW_API_KEY\")\n\n# Parse dataset and define file paths\nnames, cfg_yamls = benchmark.parse_dataset()\nval_log_file = Path(\"ultralytics-benchmarks\") / \"validation.txt\"\neval_log_file = Path(\"ultralytics-benchmarks\") / \"evaluation.txt\"\n\n# Run benchmarks on each dataset in RF100\nfor ind, path in enumerate(cfg_yamls):\n    path = Path(path)\n    if path.exists():\n        # Fix YAML file and run training\n        benchmark.fix_yaml(str(path))\n        os.system(f\"yolo detect train data={path} model=yolo11s.pt epochs=1 batch=16\")\n\n        # Run validation and evaluate\n        os.system(f\"yolo detect val data={path} model=runs/detect/train/weights/best.pt > {val_log_file} 2>&1\")\n        benchmark.evaluate(str(path), str(val_log_file), str(eval_log_file), ind)\n\n        # Remove the 'runs' directory\n        runs_dir = Path.cwd() / \"runs\"\n        shutil.rmtree(runs_dir)\n    else:\n        print(\"YAML file path does not exist\")\n        continue\n\nprint(\"RF100 Benchmarking completed!\")\n```\n\n----------------------------------------\n\nTITLE: Using OBB in Ultralytics YOLO for Oriented Object Detection\nDESCRIPTION: This code demonstrates how to use a YOLO11n-obb model to perform oriented object detection on an image and access the oriented bounding boxes using the OBB object.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/predict.md#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n model\nmodel = YOLO(\"yolo11n-obb.pt\")\n\n# Run inference on an image\nresults = model(\"https://ultralytics.com/images/boats.jpg\")  # results list\n\n# View results\nfor r in results:\n    print(r.obb)  # print the OBB object containing the oriented detection bounding boxes\n```\n\n----------------------------------------\n\nTITLE: FAQ Example: Inference with Tiger-Pose Trained Model in Python\nDESCRIPTION: Python code example from the FAQ section demonstrating how to load a trained model and perform inference on a video source.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/tiger-pose.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"path/to/best.pt\")  # load a tiger-pose trained model\n\n# Run inference\nresults = model.predict(source=\"https://youtu.be/MIBAT6BGE6U\", show=True)\n```\n\n----------------------------------------\n\nTITLE: Exporting a YOLO Model to ONNX in Python\nDESCRIPTION: This Python snippet illustrates how to export a trained Ultralytics YOLO model (`yolo11n.pt`) to the ONNX format, which is suitable for deployment across various platforms. It imports the YOLO class, loads the model, and then calls the `export()` method, setting the `format` parameter specifically to \"onnx\".\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/python.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the YOLO model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to ONNX format\nmodel.export(format=\"onnx\")\n\n```\n\n----------------------------------------\n\nTITLE: Running Inference with OpenVINO YOLOv8 using Python\nDESCRIPTION: This Python snippet illustrates how to perform inference using a YOLOv8 model that has been previously exported to the OpenVINO format. It loads the model from the exported directory (`yolov8n_openvino_model/`) using the `YOLO` class and then runs prediction on a given image URL.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/openvino.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the exported OpenVINO model\nov_model = YOLO(\"yolov8n_openvino_model/\")\n\n# Run inference\nresults = ov_model(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Convert Segmentation Masks to YOLO Format\nDESCRIPTION: Function to convert segmentation mask images into Ultralytics YOLO segmentation format, saving converted masks to specified output directory.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/simple-utilities.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.data.converter import convert_segment_masks_to_yolo_seg\n\n# The classes here is the total classes in the dataset.\n# for COCO dataset we have 80 classes.\nconvert_segment_masks_to_yolo_seg(masks_dir=\"path/to/masks_dir\", output_dir=\"path/to/output_dir\", classes=80)\n```\n\n----------------------------------------\n\nTITLE: Implementing Mixup Augmentation for Object Detection\nDESCRIPTION: Mixup blends two images and their labels with a given probability. It improves model robustness and reduces overfitting. The 'mixup' parameter controls the probability of applying this transformation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/yolo-data-augmentation.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Example usage in Ultralytics configuration\nmodel = YOLO('yolov8n.yaml')\nmodel.train(data='coco128.yaml', epochs=100, mixup=0.5)\n```\n\n----------------------------------------\n\nTITLE: YOLO Inference with Custom Arguments\nDESCRIPTION: Demonstrates how to run YOLO inference with custom arguments to override default settings.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/predict.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Run inference on 'bus.jpg' with arguments\nmodel.predict(\"https://ultralytics.com/images/bus.jpg\", save=True, imgsz=320, conf=0.5)\n```\n\n----------------------------------------\n\nTITLE: Implementing Parking Management System with Ultralytics YOLO11\nDESCRIPTION: Complete implementation of a parking management system using Ultralytics YOLO11. Includes video capture, processing, and result visualization. The code handles real-time detection and tracking of vehicles in parking spaces.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/parking-management.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\n# Video capture\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nassert cap.isOpened(), \"Error reading video file\"\n\n# Video writer\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\nvideo_writer = cv2.VideoWriter(\"parking management.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n\n# Initialize parking management object\nparkingmanager = solutions.ParkingManagement(\n    model=\"yolo11n.pt\",  # path to model file\n    json_file=\"bounding_boxes.json\",  # path to parking annotations file\n)\n\nwhile cap.isOpened():\n    ret, im0 = cap.read()\n    if not ret:\n        break\n\n    results = parkingmanager(im0)\n\n    # print(results)  # access the output\n\n    video_writer.write(results.plot_im)  # write the processed frame.\n\ncap.release()\nvideo_writer.release()\ncv2.destroyAllWindows()  # destroy all opened windows\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11-pose Model with Custom Dataset\nDESCRIPTION: Demonstrates how to load and train a YOLO11-pose model using either a new model from YAML or a pre-trained model. The training process includes specifying the dataset, number of epochs, and image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/pose.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-pose.yaml\")  # build a new model from YAML\nmodel = YOLO(\"yolo11n-pose.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"your-dataset.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with YOLO11 Segmentation Model\nDESCRIPTION: Code examples for running predictions using a trained YOLO11 segmentation model. Shows how to load models and access prediction results including mask data in different formats.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/segment.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-seg.pt\")  # load an official model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom model\n\n# Predict with the model\nresults = model(\"https://ultralytics.com/images/bus.jpg\")  # predict on an image\n\n# Access the results\nfor result in results:\n    xy = result.masks.xy  # mask in polygon format\n    xyn = result.masks.xyn  # normalized\n    masks = result.masks.data  # mask in matrix format (num_objects x H x W)\n```\n\nLANGUAGE: bash\nCODE:\n```\nyolo segment predict model=yolo11n-seg.pt source='https://ultralytics.com/images/bus.jpg'  # predict with official model\nyolo segment predict model=path/to/best.pt source='https://ultralytics.com/images/bus.jpg' # predict with custom model\n```\n\n----------------------------------------\n\nTITLE: Utilizing RT-DETR with Ultralytics Python API\nDESCRIPTION: This Python snippet demonstrates loading a pre-trained RT-DETR-l model, obtaining model information, training the model on a sample dataset, and performing inference on an image. Dependencies include the `ultralytics` Python package and a trained model file. Key parameters include the path to the model and dataset configuration files, and the number of training epochs. The script outputs training results and inference outputs.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/rtdetr.md#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics import RTDETR\n\n# Load a COCO-pretrained RT-DETR-l model\nmodel = RTDETR(\"rtdetr-l.pt\")\n\n# Display model information (optional)\nmodel.info()\n\n# Train the model on the COCO8 example dataset for 100 epochs\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n\n# Run inference with the RT-DETR-l model on the 'bus.jpg' image\nresults = model(\"path/to/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO Model to Different Formats\nDESCRIPTION: Shows how to export YOLO models to various formats like ONNX using both Python API and CLI approaches for cross-platform compatibility.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/detect.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to ONNX format\nmodel.export(format=\"onnx\")\n```\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=yolo11n.pt format=onnx\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO Model to RKNN Format in Python\nDESCRIPTION: This snippet demonstrates how to load a YOLO model and export it to RKNN format for a specific Rockchip platform using the Ultralytics Python package. It uses the 'export()' method with format 'rknn' and specifies the target platform.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/rockchip-rknn.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load your YOLO model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export to RKNN format for a specific Rockchip platform\nmodel.export(format=\"rknn\", name=\"rk3588\")\n```\n\n----------------------------------------\n\nTITLE: Custom Hyperparameter Tuning for YOLO Model in Python\nDESCRIPTION: This code snippet demonstrates how to define a custom search space and use the model.tune() method to perform hyperparameter tuning on a YOLO11n model using the COCO8 dataset. It sets up specific tuning parameters and optimization settings for a faster tuning process.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/hyperparameter-tuning.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Initialize the YOLO model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Define search space\nsearch_space = {\n    \"lr0\": (1e-5, 1e-1),\n    \"degrees\": (0.0, 45.0),\n}\n\n# Tune hyperparameters on COCO8 for 30 epochs\nmodel.tune(\n    data=\"coco8.yaml\",\n    epochs=30,\n    iterations=300,\n    optimizer=\"AdamW\",\n    space=search_space,\n    plots=False,\n    save=False,\n    val=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Validating YOLO11 Model in Python\nDESCRIPTION: Illustrates how to load a trained YOLO11 model and validate its accuracy on the COCO8 dataset, including accessing various metric attributes.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/detect.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load an official model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom model\n\n# Validate the model\nmetrics = model.val()  # no arguments needed, dataset and settings remembered\nmetrics.box.map  # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps  # a list contains map50-95 of each category\n```\n\n----------------------------------------\n\nTITLE: Initializing YOLO with ROS for Depth Image Processing\nDESCRIPTION: Sets up a ROS node and initializes YOLO segmentation model with necessary publishers for distance detection. Includes basic ROS node setup and model initialization with topic publishing.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/ros-quickstart.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nimport rospy\nfrom std_msgs.msg import String\n\nfrom ultralytics import YOLO\n\nrospy.init_node(\"ultralytics\")\ntime.sleep(1)\n\nsegmentation_model = YOLO(\"yolo11m-seg.pt\")\n\nclasses_pub = rospy.Publisher(\"/ultralytics/detection/distance\", String, queue_size=5)\n```\n\n----------------------------------------\n\nTITLE: Resuming Interrupted YOLO11 Training with Python API\nDESCRIPTION: Illustrates how to resume a previously interrupted YOLO11 training session using the Python API. This code loads the last saved checkpoint and continues training from where it left off.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/train.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the partially trained model\nmodel = YOLO(\"path/to/last.pt\")\n\n# Resume training\nresults = model.train(resume=True)\n```\n\n----------------------------------------\n\nTITLE: Training and Using YOLO11 Segmentation Models\nDESCRIPTION: This Python snippet demonstrates loading a pretrained YOLO11 segmentation model, training it, and making predictions. Dependencies are the `ultralytics` library, with major parameters including `data` for dataset configuration and `epochs` for the number of training cycles. Expected outputs are training results and predicted images.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolo11n-seg.pt')  # load a pretrained YOLO segmentation model\nmodel.train(data='coco8-seg.yaml', epochs=3)  # train the model\nmodel('https://ultralytics.com/images/bus.jpg')  # predict on an image\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 OBB Models with Python\nDESCRIPTION: Load and train YOLO11 OBB models using Python API. Demonstrates loading models from YAML or pretrained weights and training on the DOTA8 dataset.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/obb.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-obb.yaml\")  # build a new model from YAML\nmodel = YOLO(\"yolo11n-obb.pt\")  # load a pretrained model (recommended for training)\nmodel = YOLO(\"yolo11n-obb.yaml\").load(\"yolo11n.pt\")  # build from YAML and transfer weights\n\n# Train the model\nresults = model.train(data=\"dota8.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with YOLO11 OBB Models\nDESCRIPTION: Examples showing how to run predictions using trained YOLO11 OBB models and access the results including bounding box coordinates and class information.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/obb.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-obb.pt\")  # load an official model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom model\n\n# Predict with the model\nresults = model(\"https://ultralytics.com/images/boats.jpg\")  # predict on an image\n\n# Access the results\nfor result in results:\n    xywhr = result.keypoints.xy  # center-x, center-y, width, height, angle (radians)\n    xyxyxyxy = result.obb.xyxyxyxy  # polygon format with 4-points\n    names = [result.names[cls.item()] for cls in result.obb.cls.int()]  # class name of each box\n    confs = result.obb.conf  # confidence score of each box\n```\n\nLANGUAGE: bash\nCODE:\n```\nyolo obb predict model=yolo11n-obb.pt source='https://ultralytics.com/images/boats.jpg'  # predict with official model\nyolo obb predict model=path/to/best.pt source='https://ultralytics.com/images/boats.jpg' # predict with custom model\n```\n\n----------------------------------------\n\nTITLE: Basic YOLOv8 Usage in Python\nDESCRIPTION: Shows how to load a pretrained YOLOv8 model using the `YOLO` class, optionally display model info, train it on a dataset (`coco8.yaml`), and perform inference on a single image using Python. Requires the `ultralytics` library and relevant model/data files (`yolov8n.pt`, `coco8.yaml`, `path/to/bus.jpg`).\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov8.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a COCO-pretrained YOLOv8n model\nmodel = YOLO(\"yolov8n.pt\")\n\n# Display model information (optional)\nmodel.info()\n\n# Train the model on the COCO8 example dataset for 100 epochs\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n\n# Run inference with the YOLOv8n model on the 'bus.jpg' image\nresults = model(\"path/to/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Performing Streaming Inference with Ultralytics YOLO (Generator Output) in Python\nDESCRIPTION: This Python snippet illustrates how to use Ultralytics YOLO11 for inference in streaming mode by setting `stream=True` during the model call. This approach is memory-efficient as it returns a generator yielding `Results` objects one by one instead of loading all results into memory at once. The code demonstrates loading the model, processing the generator, extracting prediction outputs, and displaying/saving results. Requires the `ultralytics` library and a model file like `yolo11n.pt`.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/predict.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # pretrained YOLO11n model\n\n# Run batched inference on a list of images\nresults = model([\"image1.jpg\", \"image2.jpg\"], stream=True)  # return a generator of Results objects\n\n# Process results generator\nfor result in results:\n    boxes = result.boxes  # Boxes object for bounding box outputs\n    masks = result.masks  # Masks object for segmentation masks outputs\n    keypoints = result.keypoints  # Keypoints object for pose outputs\n    probs = result.probs  # Probs object for classification outputs\n    obb = result.obb  # Oriented boxes object for OBB outputs\n    result.show()  # display to screen\n    result.save(filename=\"result.jpg\")  # save to disk\n```\n\n----------------------------------------\n\nTITLE: Running VisionEye Object Mapping via CLI\nDESCRIPTION: Command-line interface examples for using VisionEye to monitor object positions, process video sources, and track specific object classes.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/vision-eye.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Monitor objects position with visioneye\nyolo solutions visioneye show=True\n\n# Pass a source video\nyolo solutions visioneye source=\"path/to/video.mp4\"\n\n# Monitor the specific classes\nyolo solutions visioneye classes=\"[0, 5]\"\n```\n\n----------------------------------------\n\nTITLE: Implementing VisionEye Object Mapping in Python\nDESCRIPTION: Python script demonstrating how to set up VisionEye for object mapping and tracking, including video processing and result visualization.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/vision-eye.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nassert cap.isOpened(), \"Error reading video file\"\n\n# Video writer\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\nvideo_writer = cv2.VideoWriter(\"visioneye_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n\n# Initialize vision eye object\nvisioneye = solutions.VisionEye(\n    show=True,  # display the output\n    model=\"yolo11n.pt\",  # use any model that Ultralytics support, i.e, YOLOv10\n    classes=[0, 2],  # generate visioneye view for specific classes\n    vision_point=(50, 50),  # the point, where vision will view objects and draw tracks\n)\n\n# Process video\nwhile cap.isOpened():\n    success, im0 = cap.read()\n\n    if not success:\n        print(\"Video frame is empty or video processing has been successfully completed.\")\n        break\n\n    results = visioneye(im0)\n\n    print(results)  # access the output\n\n    video_writer.write(results.plot_im)  # write the video file\n\ncap.release()\nvideo_writer.release()\ncv2.destroyAllWindows()  # destroy all opened windows\n```\n\n----------------------------------------\n\nTITLE: Tracking Objects and Updating Line Graph with Ultralytics YOLO11 in Python\nDESCRIPTION: This code snippet demonstrates how to use Ultralytics YOLO11 to track objects in a video and dynamically update a line graph visualization. It uses OpenCV for video capture and writing, and the Ultralytics solutions module for analytics.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/analytics.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nassert cap.isOpened(), \"Error reading video file\"\n\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n\nout = cv2.VideoWriter(\n    \"ultralytics_analytics.avi\",\n    cv2.VideoWriter_fourcc(*\"MJPG\"),\n    fps,\n    (1280, 720),  # this is fixed\n)\n\nanalytics = solutions.Analytics(\n    analytics_type=\"line\",\n    show=True,\n)\n\nframe_count = 0\nwhile cap.isOpened():\n    success, im0 = cap.read()\n    if success:\n        frame_count += 1\n        results = analytics(im0, frame_count)  # update analytics graph every frame\n        out.write(results.plot_im)  # write the video file\n    else:\n        break\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with YOLO11 Model in Python\nDESCRIPTION: Shows how to use a trained YOLO11 model to make predictions on images and access various attributes of the results.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/detect.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load an official model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom model\n\n# Predict with the model\nresults = model(\"https://ultralytics.com/images/bus.jpg\")  # predict on an image\n\n# Access the results\nfor result in results:\n    xywh = result.boxes.xywh  # center-x, center-y, width, height\n    xywhn = result.boxes.xywhn  # normalized\n    xyxy = result.boxes.xyxy  # top-left-x, top-left-y, bottom-right-x, bottom-right-y\n    xyxyn = result.boxes.xyxyn  # normalized\n    names = [result.names[cls.item()] for cls in result.boxes.cls.int()]  # class name of each box\n    confs = result.boxes.conf  # confidence score of each box\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLOv8 Model to TensorRT Engine - Python\nDESCRIPTION: This snippet demonstrates how to export a trained YOLOv8 model to the TensorRT engine format using the Ultralytics Python API. The example covers exporting to FP32 and FP16 precisions, as well as INT8 quantization with calibration data. Required dependencies include the 'ultralytics' Python package and a compatible version of TensorRT installed. Key parameters include 'imgsz' (input size), 'dynamic' (enabling dynamic input), 'batch' (batch size), 'workspace' (workspace size for TensorRT), 'half' (enables FP16), 'int8' (enables INT8), and 'data' for INT8 calibration. The code takes in the source PyTorch model and exports TensorRT engines tailored to deployment requirements.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tensorrt.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\n\n# TensorRT FP32\nout = model.export(format=\"engine\", imgsz=640, dynamic=True, verbose=False, batch=8, workspace=2)\n\n# TensorRT FP16\nout = model.export(format=\"engine\", imgsz=640, dynamic=True, verbose=False, batch=8, workspace=2, half=True)\n\n# TensorRT INT8 with calibration `data` (i.e. COCO, ImageNet, or DOTAv1 for appropriate model task)\nout = model.export(\n    format=\"engine\", imgsz=640, dynamic=True, verbose=False, batch=8, workspace=2, int8=True, data=\"coco8.yaml\"\n)\n```\n\n----------------------------------------\n\nTITLE: Using YOLOv5 for Training and Inference in Python\nDESCRIPTION: This Python script demonstrates loading a pre-trained YOLOv5 model (`yolov5n.pt`) using the `ultralytics` library, displaying its information, training it on a specified dataset (`coco8.yaml`) for a set number of epochs, and running inference on a single image (`path/to/bus.jpg`). Key parameters include the dataset configuration file (`data`), number of training epochs (`epochs`), and input image size (`imgsz`). Requires the `ultralytics` package.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov5.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a COCO-pretrained YOLOv5n model\nmodel = YOLO(\"yolov5n.pt\")\n\n# Display model information (optional)\nmodel.info()\n\n# Train the model on the COCO8 example dataset for 100 epochs\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n\n# Run inference with the YOLOv5n model on the 'bus.jpg' image\nresults = model(\"path/to/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Benchmarking YOLOv8 Models with Ultralytics Python API (Python)\nDESCRIPTION: This Python snippet demonstrates how to benchmark a YOLOv8n model using the Ultralytics Python API. It loads the pretrained 'yolov8n.pt' model, then runs accuracy and speed benchmarks on the specified COCO8 dataset across all export formats. Dependencies include the Ultraytics Python package (install with 'pip install ultralytics') and access to the dataset YAML file. Results include metrics on model speed and accuracy per format. Inputs are model checkpoint path and dataset YAML; outputs are returned as a results object suitable for further analysis. Limited only by hardware and data availability.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/openvino.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a YOLOv8n PyTorch model\nmodel = YOLO(\"yolov8n.pt\")\n\n# Benchmark YOLOv8n speed and accuracy on the COCO8 dataset for all export formats\nresults = model.benchmark(data=\"coco8.yaml\")\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 on Apple Silicon (M1/M2/M3/M4) with CLI\nDESCRIPTION: Shows how to train a YOLO11 model on Apple Silicon chips using the command-line interface. This command specifies the MPS device to utilize Metal Performance Shaders for accelerated training on Apple M-series chips.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/train.md#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nyolo detect train data=coco8.yaml model=yolo11n.pt epochs=100 imgsz=640 device=mps\n```\n\n----------------------------------------\n\nTITLE: Querying and Visualizing High Similarity Data Points in Python\nDESCRIPTION: This code queries for data points with a similarity count greater than 30 and plots images similar to them using the LanceDB Explorer. It demonstrates how to analyze and visualize highly similar data points in the dataset.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/explorer.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nsim_count = np.array(sim_idx[\"count\"])\nsim_idx[\"im_file\"][sim_count > 30]\n\nexp.plot_similar(idx=[7146, 14035])  # Using avg embeddings of 2 images\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model to TorchScript in Python\nDESCRIPTION: Python code to load a YOLO11 model, export it to TorchScript format, and run inference with the exported model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/torchscript.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to TorchScript format\nmodel.export(format=\"torchscript\")  # creates 'yolo11n.torchscript'\n\n# Load the exported TorchScript model\ntorchscript_model = YOLO(\"yolo11n.torchscript\")\n\n# Run inference\nresults = torchscript_model(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Creating Interactive Object Detection Interface with Gradio and Ultralytics YOLO11\nDESCRIPTION: Complete Python script for implementing an interactive object detection interface using Gradio and Ultralytics YOLO11. The code initializes the YOLO model, defines a prediction function that handles image processing with adjustable parameters, and sets up the Gradio interface with appropriate input and output components.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/gradio.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport gradio as gr\nimport PIL.Image as Image\n\nfrom ultralytics import ASSETS, YOLO\n\nmodel = YOLO(\"yolo11n.pt\")\n\n\ndef predict_image(img, conf_threshold, iou_threshold):\n    \"\"\"Predicts objects in an image using a YOLO11 model with adjustable confidence and IOU thresholds.\"\"\"\n    results = model.predict(\n        source=img,\n        conf=conf_threshold,\n        iou=iou_threshold,\n        show_labels=True,\n        show_conf=True,\n        imgsz=640,\n    )\n\n    for r in results:\n        im_array = r.plot()\n        im = Image.fromarray(im_array[..., ::-1])\n\n    return im\n\n\niface = gr.Interface(\n    fn=predict_image,\n    inputs=[\n        gr.Image(type=\"pil\", label=\"Upload Image\"),\n        gr.Slider(minimum=0, maximum=1, value=0.25, label=\"Confidence threshold\"),\n        gr.Slider(minimum=0, maximum=1, value=0.45, label=\"IoU threshold\"),\n    ],\n    outputs=gr.Image(type=\"pil\", label=\"Result\"),\n    title=\"Ultralytics Gradio\",\n    description=\"Upload images for inference. The Ultralytics YOLO11n model is used by default.\",\n    examples=[\n        [ASSETS / \"bus.jpg\", 0.25, 0.45],\n        [ASSETS / \"zidane.jpg\", 0.25, 0.45],\n    ],\n)\n\nif __name__ == \"__main__\":\n    iface.launch()\n```\n\n----------------------------------------\n\nTITLE: Exporting and Using YOLO11 TorchScript Models in Python\nDESCRIPTION: This snippet demonstrates how to export a YOLO11 model to TorchScript format, load the exported model, and run inference using Python. It covers the process of loading a YOLO model, exporting it to TorchScript, and then using the exported model for prediction.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/torchscript.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to TorchScript format\nmodel.export(format=\"torchscript\")  # creates 'yolo11n.torchscript'\n\n# Load the exported TorchScript model\ntorchscript_model = YOLO(\"yolo11n.torchscript\")\n\n# Run inference\nresults = torchscript_model(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Model on Custom Data in Python\nDESCRIPTION: This example demonstrates how to load a YOLO11 model and train it on a custom dataset using Python.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/index.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a YOLO model\nmodel = YOLO(\"yolo11n.pt\")  # or any other YOLO model\n\n# Train the model on custom dataset\nresults = model.train(data=\"custom_data.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Segmenting Entire Content using SAM in Python\nDESCRIPTION: This example demonstrates how to segment entire content within an image or video using the SAM model, without any specific prompts like bounding boxes or points. It requires the SAM model file 'sam2.1_b.pt' and supports both images and video files as input.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/sam-2.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import SAM\n\n# Load a model\nmodel = SAM(\"sam2.1_b.pt\")\n\n# Display model information (optional)\nmodel.info()\n\n# Run inference\nmodel(\"path/to/video.mp4\")\n```\n\n----------------------------------------\n\nTITLE: Running YOLO Tracker via Command Line Interface\nDESCRIPTION: This code snippet shows how to use the Ultralytics YOLO tracker from the command line interface. It includes examples for different model types and tracker configurations.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/trackers/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Perform tracking with various models using the command line interface\nyolo track model=yolo11n.pt source=\"https://youtu.be/LNwODJXcvt4\" # Official Detect model\n# yolo track model=yolo11n-seg.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Official Segment model\n# yolo track model=yolo11n-pose.pt source=\"https://youtu.be/LNwODJXcvt4\" # Official Pose model\n# yolo track model=path/to/best.pt source=\"https://youtu.be/LNwODJXcvt4\" # Custom trained model\n\n# Track using ByteTrack tracker\n# yolo track model=path/to/best.pt tracker=\"bytetrack.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Complete Point Cloud Segmentation Pipeline with YOLO and ROS\nDESCRIPTION: This snippet combines all the previous code into a complete pipeline for point cloud segmentation using YOLO and ROS. It includes initialization, PointCloud2 processing, YOLO segmentation, and 3D visualization with Open3D.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/ros-quickstart.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport sys\nimport time\n\nimport numpy as np\nimport open3d as o3d\nimport ros_numpy\nimport rospy\nfrom sensor_msgs.msg import PointCloud2\n\nfrom ultralytics import YOLO\n\nrospy.init_node(\"ultralytics\")\ntime.sleep(1)\nsegmentation_model = YOLO(\"yolo11m-seg.pt\")\n\n\ndef pointcloud2_to_array(pointcloud2: PointCloud2) -> tuple:\n    \"\"\"\n    Convert a ROS PointCloud2 message to a numpy array.\n\n    Args:\n        pointcloud2 (PointCloud2): the PointCloud2 message\n\n    Returns:\n        (tuple): tuple containing (xyz, rgb)\n    \"\"\"\n    pc_array = ros_numpy.point_cloud2.pointcloud2_to_array(pointcloud2)\n    split = ros_numpy.point_cloud2.split_rgb_field(pc_array)\n    rgb = np.stack([split[\"b\"], split[\"g\"], split[\"r\"]], axis=2)\n    xyz = ros_numpy.point_cloud2.get_xyz_points(pc_array, remove_nans=False)\n    xyz = np.array(xyz).reshape((pointcloud2.height, pointcloud2.width, 3))\n    nan_rows = np.isnan(xyz).all(axis=2)\n    xyz[nan_rows] = [0, 0, 0]\n    rgb[nan_rows] = [0, 0, 0]\n    return xyz, rgb\n\n\nros_cloud = rospy.wait_for_message(\"/camera/depth/points\", PointCloud2)\nxyz, rgb = pointcloud2_to_array(ros_cloud)\nresult = segmentation_model(rgb)\n\nif not len(result[0].boxes.cls):\n    print(\"No objects detected\")\n    sys.exit()\n\nclasses = result[0].boxes.cls.cpu().numpy().astype(int)\nfor index, class_id in enumerate(classes):\n    mask = result[0].masks.data.cpu().numpy()[index, :, :].astype(int)\n    mask_expanded = np.stack([mask, mask, mask], axis=2)\n\n    obj_rgb = rgb * mask_expanded\n    obj_xyz = xyz * mask_expanded\n\n    pcd = o3d.geometry.PointCloud()\n    pcd.points = o3d.utility.Vector3dVector(obj_xyz.reshape((ros_cloud.height * ros_cloud.width, 3)))\n    pcd.colors = o3d.utility.Vector3dVector(obj_rgb.reshape((ros_cloud.height * ros_cloud.width, 3)) / 255)\n    o3d.visualization.draw_geometries([pcd])\n```\n\n----------------------------------------\n\nTITLE: Training and Predicting with YOLOv5 using CLI\nDESCRIPTION: These CLI commands demonstrate how to use the `yolo` command-line tool for training and prediction with a YOLOv5 model. The first command trains `yolov5n.pt` on the `coco8.yaml` dataset for 100 epochs with an image size of 640. The second command performs inference using the same model on the image `path/to/bus.jpg`. Requires the `ultralytics` package to be installed and the `yolo` command to be available in the system path.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov5.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Load a COCO-pretrained YOLOv5n model and train it on the COCO8 example dataset for 100 epochs\nyolo train model=yolov5n.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Load a COCO-pretrained YOLOv5n model and run inference on the 'bus.jpg' image\nyolo predict model=yolov5n.pt source=path/to/bus.jpg\n```\n\n----------------------------------------\n\nTITLE: Python Instance Segmentation and Object Tracking\nDESCRIPTION: Complete Python implementation for instance segmentation and object tracking using YOLO11, including video processing and visualization features.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/object_tracking.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom collections import defaultdict\n\nimport cv2\n\nfrom ultralytics import YOLO\nfrom ultralytics.utils.plotting import Annotator, colors\n\n# Dictionary to store tracking history with default empty lists\ntrack_history = defaultdict(lambda: [])\n\n# Load the YOLO model with segmentation capabilities\nmodel = YOLO(\"yolo11n-seg.pt\")\n\n# Open the video file\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\n\n# Retrieve video properties: width, height, and frames per second\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n\n# Initialize video writer to save the output video with the specified properties\nout = cv2.VideoWriter(\"instance-segmentation-object-tracking.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), fps, (w, h))\n\nwhile True:\n    # Read a frame from the video\n    ret, im0 = cap.read()\n    if not ret:\n        print(\"Video frame is empty or video processing has been successfully completed.\")\n        break\n\n    # Create an annotator object to draw on the frame\n    annotator = Annotator(im0, line_width=2)\n\n    # Perform object tracking on the current frame\n    results = model.track(im0, persist=True)\n\n    # Check if tracking IDs and masks are present in the results\n    if results[0].boxes.id is not None and results[0].masks is not None:\n        # Extract masks and tracking IDs\n        masks = results[0].masks.xy\n        track_ids = results[0].boxes.id.int().cpu().tolist()\n\n        # Annotate each mask with its corresponding tracking ID and color\n        for mask, track_id in zip(masks, track_ids):\n            annotator.seg_bbox(mask=mask, mask_color=colors(int(track_id), True), label=str(track_id))\n\n    # Write the annotated frame to the output video\n    out.write(im0)\n    # Display the annotated frame\n    cv2.imshow(\"instance-segmentation-object-tracking\", im0)\n\n    # Exit the loop if 'q' is pressed\n    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n        break\n\n# Release the video writer and capture objects, and close all OpenCV windows\nout.release()\ncap.release()\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Python Implementation for Analytics Visualization\nDESCRIPTION: Python code for implementing analytics visualization using Ultralytics YOLO11. The code demonstrates video processing and real-time analytics graph generation with configurable parameters.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/analytics.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nassert cap.isOpened(), \"Error reading video file\"\n\n# Video writer\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\nout = cv2.VideoWriter(\n    \"analytics_output.avi\",\n    cv2.VideoWriter_fourcc(*\"MJPG\"),\n    fps,\n    (1280, 720),  # this is fixed\n)\n\n# Initialize analytics object\nanalytics = solutions.Analytics(\n    show=True,  # display the output\n    analytics_type=\"line\",  # pass the analytics type, could be \"pie\", \"bar\" or \"area\".\n    model=\"yolo11n.pt\",  # path to the YOLO11 model file\n    # classes=[0, 2],  # display analytics for specific detection classes\n)\n\n# Process video\nframe_count = 0\nwhile cap.isOpened():\n    success, im0 = cap.read()\n    if success:\n        frame_count += 1\n        results = analytics(im0, frame_count)  # update analytics graph every frame\n\n        # print(results)  # access the output\n\n        out.write(results.plot_im)  # write the video file\n    else:\n        break\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()  # destroy all opened windows\n```\n\n----------------------------------------\n\nTITLE: Loading and Training YOLO11 Model in Python\nDESCRIPTION: This code snippet shows how to import the YOLO model from Ultralytics, load a pretrained model, and train it on a custom dataset. It demonstrates the core functionality of working with YOLO11 models in JupyterLab.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/jupyterlab.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolo11n.pt\")\nresults = model.train(data=\"path/to/your/data.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model to TensorFlow.js Format via CLI\nDESCRIPTION: Command-line instructions for exporting a YOLO11 PyTorch model to TensorFlow.js format and running inference with the exported model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tfjs.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Export a YOLO11n PyTorch model to TF.js format\nyolo export model=yolo11n.pt format=tfjs # creates '/yolo11n_web_model'\n\n# Run inference with the exported model\nyolo predict model='./yolo11n_web_model' source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Running YOLO Inference on Sources Listed in a CSV File in Python\nDESCRIPTION: This snippet demonstrates how to run YOLO inference on multiple sources specified in a CSV file. It loads a YOLO model and sets the `source` variable to the path of the CSV file. The CSV file should contain paths to images, URLs, videos, or directories, one per line. The model processes each entry in the CSV, returning a list of Results objects for all sources.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/predict.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Define a path to a CSV file with images, URLs, videos and directories\nsource = \"path/to/file.csv\"\n\n# Run inference on the source\nresults = model(source)  # list of Results objects\n```\n\n----------------------------------------\n\nTITLE: Benchmarking YOLO11 Models on COCO Dataset in Python\nDESCRIPTION: Python code to load a YOLO11n model and benchmark its speed and accuracy on the COCO8 dataset across all supported export formats. This script helps reproduce the benchmarking results shown in the performance tables.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/nvidia-jetson.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a YOLO11n PyTorch model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Benchmark YOLO11n speed and accuracy on the COCO8 dataset for all all export formats\nresults = model.benchmark(data=\"coco8.yaml\", imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Thread-Safe Example: Instantiating YOLO Model Inside Each Thread in Python\nDESCRIPTION: This example demonstrates a thread-safe approach by instantiating a separate YOLO model within each thread, ensuring isolated model instances and preventing race conditions.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/yolo-thread-safe-inference.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Safe: Instantiating a single model inside each thread\nfrom threading import Thread\n\nfrom ultralytics import YOLO\n\n\ndef thread_safe_predict(image_path):\n    \"\"\"Predict on an image using a new YOLO model instance in a thread-safe manner; takes image path as input.\"\"\"\n    local_model = YOLO(\"yolo11n.pt\")\n    results = local_model.predict(image_path)\n    # Process results\n\n\n# Starting threads that each have their own model instance\nThread(target=thread_safe_predict, args=(\"image1.jpg\",)).start()\nThread(target=thread_safe_predict, args=(\"image2.jpg\",)).start()\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model to Edge TPU Format in Python\nDESCRIPTION: Python code demonstrating how to load a YOLO11 model, export it to TFLite Edge TPU format, and run inference\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/edge-tpu.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to TFLite Edge TPU format\nmodel.export(format=\"edgetpu\")  # creates 'yolo11n_full_integer_quant_edgetpu.tflite'\n\n# Load the exported TFLite Edge TPU model\nedgetpu_model = YOLO(\"yolo11n_full_integer_quant_edgetpu.tflite\")\n\n# Run inference\nresults = edgetpu_model(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Configuring TrackZone with Ultralytics YOLO11 in Python\nDESCRIPTION: This code snippet shows how to configure zone points for video processing using Ultralytics TrackZone. It demonstrates defining region points and initializing the TrackZone object.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/trackzone.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Define region points\nregion_points = [(150, 150), (1130, 150), (1130, 570), (150, 570)]\n\n# Initialize trackzone\ntrackzone = solutions.TrackZone(\n    show=True,  # display the output\n    region=region_points,  # pass region points\n)\n```\n\n----------------------------------------\n\nTITLE: Converting COCO Dataset to YOLO Format in Python\nDESCRIPTION: This code demonstrates how to convert COCO JSON annotations to YOLO format using the convert_coco utility. The function handles the conversion of object detection annotations, with options to include segments, keypoints, and to map from the full 91 COCO classes to the standard 80 classes.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/simple-utilities.md#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.data.converter import convert_coco\n\nconvert_coco(\n    \"../datasets/coco/annotations/\",\n    use_segments=False,\n    use_keypoints=False,\n    cls91to80=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking YOLO11 Model with Python\nDESCRIPTION: This Python snippet benchmarks a YOLO11 model, providing metrics on speed and accuracy across different export formats, such as ONNX and TensorRT. It requires a pre-trained model and dataset, allowing the adjustment of parameters for customized benchmarking.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/index.md#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics.utils.benchmarks import benchmark\n\n# Run benchmark on GPU (device 0)\n# You can adjust parameters like model, dataset, image size, and precision as needed\nbenchmark(model=\"yolo11n.pt\", data=\"coco8.yaml\", imgsz=640, half=False, device=0)\n```\n\n----------------------------------------\n\nTITLE: Converting YOLO11n Model to NCNN and Running Inference (Python)\nDESCRIPTION: Python code to load a YOLO11n model, export it to NCNN format, and run inference using the exported model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/raspberry-pi.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a YOLO11n PyTorch model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to NCNN format\nmodel.export(format=\"ncnn\")  # creates 'yolo11n_ncnn_model'\n\n# Load the exported NCNN model\nncnn_model = YOLO(\"yolo11n_ncnn_model\")\n\n# Run inference\nresults = ncnn_model(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model to TensorFlow.js Format in Python\nDESCRIPTION: Python code snippet demonstrating how to load a YOLO11 model, export it to TensorFlow.js format, and run inference with the exported model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tfjs.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to TF.js format\nmodel.export(format=\"tfjs\")  # creates '/yolo11n_web_model'\n\n# Load the exported TF.js model\ntfjs_model = YOLO(\"./yolo11n_web_model\")\n\n# Run inference\nresults = tfjs_model(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Predicting with FastSAM Model - Python\nDESCRIPTION: Demonstrates loading a pre-trained FastSAM model with Ultralytics' Python API, running segmentation on an image, and using various prompt types (bounding boxes, points, texts, or any combination) for tailored predictions. Requires Ultralytics installed, pretrained FastSAM weights (e.g., 'FastSAM-s.pt'), and access to the input image path. Input parameters specify sources, device (CPU/GPU), segmentation mask format, image size, and confidence/IOU thresholds. The output is a Results object with predicted masks and related data for downstream access.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/fast-sam.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import FastSAM\n\n# Define an inference source\nsource = \"path/to/bus.jpg\"\n\n# Create a FastSAM model\nmodel = FastSAM(\"FastSAM-s.pt\")  # or FastSAM-x.pt\n\n# Run inference on an image\neverything_results = model(source, device=\"cpu\", retina_masks=True, imgsz=1024, conf=0.4, iou=0.9)\n\n# Run inference with bboxes prompt\nresults = model(source, bboxes=[439, 437, 524, 709])\n\n# Run inference with points prompt\nresults = model(source, points=[[200, 200]], labels=[1])\n\n# Run inference with texts prompt\nresults = model(source, texts=\"a photo of a dog\")\n\n# Run inference with bboxes and points and texts prompt at the same time\nresults = model(source, bboxes=[439, 437, 524, 709], points=[[200, 200]], labels=[1], texts=\"a photo of a dog\")\n```\n\n----------------------------------------\n\nTITLE: Performing Instance Segmentation with Ultralytics YOLO11 in Python (FAQ Example)\nDESCRIPTION: This code snippet, from the FAQ section, demonstrates how to initialize the YOLO model for instance segmentation and process video frames. It includes video capture, writing output, and displaying results.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/instance-segmentation-and-tracking.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nassert cap.isOpened(), \"Error reading video file\"\n\n# Video writer\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\nvideo_writer = cv2.VideoWriter(\"instance-segmentation.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n\n# Init InstanceSegmentation\nisegment = solutions.InstanceSegmentation(\n    show=True,  # display the output\n    model=\"yolo11n-seg.pt\",  # model=\"yolo11n-seg.pt\" for object segmentation using YOLO11.\n)\n\n# Process video\nwhile cap.isOpened():\n    success, im0 = cap.read()\n    if not success:\n        print(\"Video frame is empty or processing is complete.\")\n        break\n    results = isegment(im0)\n    video_writer.write(results.plot_im)\n\ncap.release()\nvideo_writer.release()\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Using RT-DETR for Object Detection in Python\nDESCRIPTION: This snippet demonstrates how to load a pre-trained RT-DETR model, display model information, train the model on a dataset, and perform inference on an image using the Ultralytics Python API.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/rtdetr.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import RTDETR\n\n# Load a COCO-pretrained RT-DETR-l model\nmodel = RTDETR(\"rtdetr-l.pt\")\n\n# Display model information (optional)\nmodel.info()\n\n# Train the model on the COCO8 example dataset for 100 epochs\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n\n# Run inference with the RT-DETR-l model on the 'bus.jpg' image\nresults = model(\"path/to/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: YOLO Integration with Custom Trainers\nDESCRIPTION: Shows how to use custom trainers with the high-level YOLO interface for flexible machine learning workflows.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/engine.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\nfrom ultralytics.models.yolo.detect import DetectionTrainer\n\n\n# Create a custom trainer\nclass MyCustomTrainer(DetectionTrainer):\n    def get_model(self, cfg, weights):\n        \"\"\"Custom code implementation.\"\"\"\n        ...\n\n\n# Initialize YOLO model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Train with custom trainer\nresults = model.train(trainer=MyCustomTrainer, data=\"coco8.yaml\", epochs=3)\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model to TensorRT in Python\nDESCRIPTION: Python code snippet demonstrating how to load a YOLO11 model, export it to TensorRT format, and then use the exported model for inference.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tensorrt.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to TensorRT format\nmodel.export(format=\"engine\")  # creates 'yolo11n.engine'\n\n# Load the exported TensorRT model\ntensorrt_model = YOLO(\"yolo11n.engine\")\n\n# Run inference\nresults = tensorrt_model(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Models to ONNX Format in Python\nDESCRIPTION: Python code snippet showing how to load a YOLO11 model, export it to ONNX format, and then use the exported model for inference.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/onnx.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to ONNX format\nmodel.export(format=\"onnx\")  # creates 'yolo11n.onnx'\n\n# Load the exported ONNX model\nonnx_model = YOLO(\"yolo11n.onnx\")\n\n# Run inference\nresults = onnx_model(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: YOLO Python API Implementation\nDESCRIPTION: Python code demonstrating how to load a YOLO model, train it, validate, predict and export to ONNX format\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/README.zh-CN.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# 加载一个预训练的 YOLO11n 模型\nmodel = YOLO(\"yolo11n.pt\")\n\n# 在 COCO8 数据集上训练模型 100 个周期\ntrain_results = model.train(\n    data=\"coco8.yaml\",  # 数据集配置文件路径\n    epochs=100,  # 训练周期数\n    imgsz=640,  # 训练图像尺寸\n    device=\"cpu\",  # 运行设备（例如 'cpu', 0, [0,1,2,3]）\n)\n\n# 评估模型在验证集上的性能\nmetrics = model.val()\n\n# 对图像执行目标检测\nresults = model(\"path/to/image.jpg\")  # 对图像进行预测\nresults[0].show()  # 显示结果\n\n# 将模型导出为 ONNX 格式以进行部署\npath = model.export(format=\"onnx\")  # 返回导出模型的路径\n```\n\n----------------------------------------\n\nTITLE: Tracking Objects with FastSAM Model - Python\nDESCRIPTION: Demonstrates object tracking in a video stream using FastSAM through Ultralytics' Python API. Input is a video file, model checkpoint, and optional image size for inference. Outputs tracked segmentation results, made available through the returned Results object. Requires that tracking is supported in the installed Ultralytics version.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/fast-sam.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import FastSAM\n\n# Create a FastSAM model\nmodel = FastSAM(\"FastSAM-s.pt\")  # or FastSAM-x.pt\n\n# Track with a FastSAM model on a video\nresults = model.track(source=\"path/to/video.mp4\", imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Segmenting an Entire Image using SAM - Python\nDESCRIPTION: This code shows how to perform semantic segmentation across the whole image using the Ultralytics SAM interface with Python. No manual prompt is needed; passing only the image path as input segments all detectable objects in the image. The primary dependency is the 'ultralytics' Python package. Input is an image file path, output is a 'Results' object containing all predicted masks. This approach is limited to fully automated whole-image segmentation tasks.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/sam.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import SAM\n\n# Load a model\nmodel = SAM(\"sam_b.pt\")\n\n# Display model information (optional)\nmodel.info()\n\n# Run inference\nmodel(\"path/to/image.jpg\")\n\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Ray Tune Search Space for Multiple Hyperparameters YOLO11 Python\nDESCRIPTION: This snippet defines a custom search space for YOLO11 tuning using a dictionary mapping multiple hyperparameter names (here: lr0, momentum) to Ray Tune sampling strategies. Inputs are the model, the search space, and the dataset config. Outputs are the result grid from Ray Tune. Required dependencies: ultralytics and ray. Ideal for advanced users customizing optimization domains.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/ray-tune.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import tune\\n\\nfrom ultralytics import YOLO\\n\\nmodel = YOLO(\"yolo11n.pt\")\\nsearch_space = {\"lr0\": tune.uniform(1e-5, 1e-1), \"momentum\": tune.uniform(0.6, 0.98)}\\nresult_grid = model.tune(data=\"coco8.yaml\", space=search_space, use_ray=True)\n```\n\n----------------------------------------\n\nTITLE: Basic YOLOv8 Usage via CLI\nDESCRIPTION: Provides CLI commands for training a YOLOv8 model (`yolo train`) using a specified model file (`yolov8n.pt`) and dataset config (`coco8.yaml`), and for running inference (`yolo predict`) on an image. Requires the `ultralytics` CLI tool and relevant model/data files.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov8.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Load a COCO-pretrained YOLOv8n model and train it on the COCO8 example dataset for 100 epochs\nyolo train model=yolov8n.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Load a COCO-pretrained YOLOv8n model and run inference on the 'bus.jpg' image\nyolo predict model=yolov8n.pt source=path/to/bus.jpg\n```\n\n----------------------------------------\n\nTITLE: Implementing Real-time Object Tracking with YOLO11 in Python\nDESCRIPTION: This code demonstrates how to load a pre-trained YOLO11 model and use it to track objects in a video file. The tracking functionality extends beyond detection to maintain object identities across frames.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/index.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pre-trained YOLO model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Start tracking objects in a video\n# You can also use live video streams or webcam input\nmodel.track(source=\"path/to/video.mp4\")\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n Model on Medical Pills Dataset in Python\nDESCRIPTION: This Python code demonstrates how to load a pre-trained YOLO11n model and train it on the medical-pills dataset for 100 epochs with an image size of 640.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/medical-pills.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"medical-pills.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Implementing Workout Monitoring in Python\nDESCRIPTION: Complete Python implementation for workout monitoring using video capture, including initialization of AIGym, video processing, and result visualization.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/workouts-monitoring.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nassert cap.isOpened(), \"Error reading video file\"\n\n# Video writer\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\nvideo_writer = cv2.VideoWriter(\"workouts_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n\n# Init AIGym\ngym = solutions.AIGym(\n    show=True,  # display the frame\n    kpts=[6, 8, 10],  # keypoints for monitoring specific exercise, by default it's for pushup\n    model=\"yolo11n-pose.pt\",  # path to the YOLO11 pose estimation model file\n    # line_width=2,  # adjust the line width for bounding boxes and text display\n)\n\n# Process video\nwhile cap.isOpened():\n    success, im0 = cap.read()\n\n    if not success:\n        print(\"Video frame is empty or processing is complete.\")\n        break\n\n    results = gym(im0)\n\n    # print(results)  # access the output\n\n    video_writer.write(results.plot_im)  # write the processed frame.\n\ncap.release()\nvideo_writer.release()\ncv2.destroyAllWindows()  # destroy all opened windows\n```\n\n----------------------------------------\n\nTITLE: Loading and Training YOLO11 Classification Models in Python\nDESCRIPTION: Demonstrates how to load a YOLO11 model from YAML or pretrained weights and train it on the MNIST160 dataset for 100 epochs with 64px images.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/classify.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-cls.yaml\")  # build a new model from YAML\nmodel = YOLO(\"yolo11n-cls.pt\")  # load a pretrained model (recommended for training)\nmodel = YOLO(\"yolo11n-cls.yaml\").load(\"yolo11n-cls.pt\")  # build from YAML and transfer weights\n\n# Train the model\nresults = model.train(data=\"mnist160\", epochs=100, imgsz=64)\n```\n\n----------------------------------------\n\nTITLE: Prompt Inference with FastSAMPredictor - Python\nDESCRIPTION: Shows how to use the FastSAMPredictor class to run an all-instance segmentation once and re-use its results for multiple prompt-based queries (bounding boxes, points, texts). Reduces redundant computation by separating initial inference from prompt-guided selection. Requires Ultralytics, model weights, and input image. Key parameters include model configuration overrides, prompt types, and source image. Outputs are segmentations corresponding to each type of prompt.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/fast-sam.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.models.fastsam import FastSAMPredictor\n\n# Create FastSAMPredictor\noverrides = dict(conf=0.25, task=\"segment\", mode=\"predict\", model=\"FastSAM-s.pt\", save=False, imgsz=1024)\npredictor = FastSAMPredictor(overrides=overrides)\n\n# Segment everything\neverything_results = predictor(\"ultralytics/assets/bus.jpg\")\n\n# Prompt inference\nbbox_results = predictor.prompt(everything_results, bboxes=[[200, 200, 300, 300]])\npoint_results = predictor.prompt(everything_results, points=[200, 200])\ntext_results = predictor.prompt(everything_results, texts=\"a photo of a dog\")\n```\n\n----------------------------------------\n\nTITLE: Validating a YOLO Model in Python\nDESCRIPTION: This Python code shows how to validate a YOLO model using the Ultralytics library. It first loads a model configuration (`yolo11n.yaml`), then optionally trains it on one dataset (`coco8.yaml` for 5 epochs as an example setup). After training (or loading a pretrained model), it calls the `val()` method to evaluate the model's performance on a specified validation dataset, potentially different from the training set, indicated by `data=\"path/to/separate/data.yaml\"`.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/python.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a YOLO model\nmodel = YOLO(\"yolo11n.yaml\")\n\n# Train the model\nmodel.train(data=\"coco8.yaml\", epochs=5)\n\n# Validate the model on a different dataset\nmodel.val(data=\"path/to/separate/data.yaml\")\n\n```\n\n----------------------------------------\n\nTITLE: YOLO CLI Tracking Command\nDESCRIPTION: Command line interface example for running object tracking on a video file.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/object_tracking.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!yolo track source=\"/path/to/video.mp4\" save=True\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Models to RKNN Format in Python\nDESCRIPTION: Python code to load a YOLO11 model and export it to RKNN format. The 'name' parameter specifies the target Rockchip processor, with RK3588 being the default option.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/rockchip-rknn.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to RKNN format\n# 'name' can be one of rk3588, rk3576, rk3566, rk3568, rk3562, rv1103, rv1106, rv1103b, rv1106b, rk2118\nmodel.export(format=\"rknn\", name=\"rk3588\")  # creates '/yolo11n_rknn_model'\n```\n\n----------------------------------------\n\nTITLE: Implementing Region-Based Object Counting with Ultralytics YOLO11\nDESCRIPTION: Python script demonstrating how to implement object counting in specified regions using Ultralytics YOLO11. The code handles video input, defines counting regions, processes frames, and outputs results with visualization.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/region-counting.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nassert cap.isOpened(), \"Error reading video file\"\n\n# Pass region as dictionary\nregion_points = {\n    \"region-01\": [(50, 50), (250, 50), (250, 250), (50, 250)],\n    \"region-02\": [(640, 640), (780, 640), (780, 720), (640, 720)],\n}\n\n# Video writer\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\nvideo_writer = cv2.VideoWriter(\"region_counting.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n\n# Initialize region counter object\nregioncounter = solutions.RegionCounter(\n    show=True,  # display the frame\n    region=region_points,  # pass region points\n    model=\"yolo11n.pt\",  # model for counting in regions i.e yolo11s.pt\n)\n\n# Process video\nwhile cap.isOpened():\n    success, im0 = cap.read()\n\n    if not success:\n        print(\"Video frame is empty or processing is complete.\")\n        break\n\n    results = regioncounter(im0)\n\n    video_writer.write(results.plot_im)\n\ncap.release()\nvideo_writer.release()\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Training and Using YOLO11 Detection Models\nDESCRIPTION: This Python snippet showcases loading a pretrained YOLO11 detection model and using it for training and prediction. It requires the `ultralytics` package, with key parameters being `data` for training dataset configuration and `epochs` for training duration. The operation outputs include training results and prediction outcomes on images.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolo11n.pt')  # load a pretrained YOLO detection model\nmodel.train(data='coco8.yaml', epochs=3)  # train the model\nmodel('https://ultralytics.com/images/bus.jpg')  # predict on an image\n```\n\n----------------------------------------\n\nTITLE: YOLO11 Python Implementation\nDESCRIPTION: Python code demonstrating model loading, training, validation, prediction, and export using YOLO11.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/azureml-quickstart.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load an official YOLO11n model\n\n# Use the model\nmodel.train(data=\"coco8.yaml\", epochs=3)  # train the model\nmetrics = model.val()  # evaluate model performance on the validation set\nresults = model(\"https://ultralytics.com/images/bus.jpg\")  # predict on an image\npath = model.export(format=\"onnx\")  # export the model to ONNX format\n```\n\n----------------------------------------\n\nTITLE: Running NVIDIA Triton Inference Server with Docker\nDESCRIPTION: This snippet demonstrates how to pull and run the NVIDIA Triton Inference Server Docker container, mount the model repository, and verify that the server is running correctly with the YOLO model loaded and ready.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/triton-inference-server.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport contextlib\nimport subprocess\nimport time\n\nfrom tritonclient.http import InferenceServerClient\n\n# Define image https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver\ntag = \"nvcr.io/nvidia/tritonserver:24.09-py3\"\n\nsubprocess.call(f\"docker pull {tag}\", shell=True)\n\ncontainer_id = (\n    subprocess.check_output(\n        f\"docker run -d --rm --gpus 0 -v {triton_repo_path}:/models -p 8000:8000 {tag} tritonserver --model-repository=/models\",\n        shell=True,\n    )\n    .decode(\"utf-8\")\n    .strip()\n)\n\ntriton_client = InferenceServerClient(url=\"localhost:8000\", verbose=False, ssl=False)\n\nfor _ in range(10):\n    with contextlib.suppress(Exception):\n        assert triton_client.is_model_ready(model_name)\n        break\n    time.sleep(1)\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLOv8 to OpenVINO using Python\nDESCRIPTION: This Python snippet demonstrates how to load a pre-trained YOLOv8 model (`.pt` file) using the `ultralytics` library and export it to the OpenVINO format. The `export()` method is called with `format='openvino'`, which creates a directory (`yolov8n_openvino_model/`) containing the exported model files.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/openvino.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a YOLOv8n PyTorch model\nmodel = YOLO(\"yolov8n.pt\")\n\n# Export the model\nmodel.export(format=\"openvino\")  # creates 'yolov8n_openvino_model/'\n```\n\n----------------------------------------\n\nTITLE: Implementing Instance Segmentation in Python with Ultralytics YOLO11\nDESCRIPTION: This code snippet shows how to perform instance segmentation on a video using Ultralytics YOLO11 in Python. It initializes the InstanceSegmentation object, processes video frames, and writes the output to a new video file.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/instance-segmentation-and-tracking.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nassert cap.isOpened(), \"Error reading video file\"\n\n# Video writer\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\nvideo_writer = cv2.VideoWriter(\"isegment_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n\n# Initialize instance segmentation object\nisegment = solutions.InstanceSegmentation(\n    show=True,  # display the output\n    model=\"yolo11n-seg.pt\",  # model=\"yolo11n-seg.pt\" for object segmentation using YOLO11.\n    # classes=[0, 2],  # segment specific classes i.e, person and car with pretrained model.\n)\n\n# Process video\nwhile cap.isOpened():\n    success, im0 = cap.read()\n\n    if not success:\n        print(\"Video frame is empty or video processing has been successfully completed.\")\n        break\n\n    results = isegment(im0)\n\n    # print(results)  # access the output\n\n    video_writer.write(results.plot_im)  # write the processed frame.\n\ncap.release()\nvideo_writer.release()\ncv2.destroyAllWindows()  # destroy all opened windows\n```\n\n----------------------------------------\n\nTITLE: Convert Segments to Bounding Boxes\nDESCRIPTION: Function to convert segmentation format data into upright bounding boxes in x y w h format.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/simple-utilities.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nfrom ultralytics.utils.ops import segments2boxes\n\nsegments = np.array(\n    [\n        [805, 392, 797, 400, ..., 808, 714, 808, 392],\n        [115, 398, 113, 400, ..., 150, 400, 149, 298],\n        [267, 412, 265, 413, ..., 300, 413, 299, 412],\n    ]\n)\n\nsegments2boxes([s.reshape(-1, 2) for s in segments])\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Models with Dynamic Input Size - Python\nDESCRIPTION: Demonstrates exporting a YOLO11 model via the Python API to ONNX format with dynamic input sizing, enabling the exported model to process varying input image dimensions. Dependencies include ultralytics Python package and a trained YOLO11 model file. The 'dynamic=True' parameter activates dynamic sizing; the ONNX file produced is suitable for video or multi-resolution applications.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/export.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolo11n.pt\")\nmodel.export(format=\"onnx\", dynamic=True)\n```\n\n----------------------------------------\n\nTITLE: Generating Heatmap from Video using Ultralytics YOLO11\nDESCRIPTION: Demonstrates how to create a heatmap from a video file using Ultralytics YOLO11. The script processes each frame of the video, applies the YOLO11 model for object detection, and generates a heatmap visualization.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/heatmaps.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\n# Open video file\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nassert cap.isOpened(), \"Error reading video file\"\n\n# Get video properties\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n\n# Initialize video writer\nvideo_writer = cv2.VideoWriter(\"heatmap_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n\n# Initialize heatmap object\nheatmap_obj = solutions.Heatmap(\n    colormap=cv2.COLORMAP_PARULA,  # Color of the heatmap\n    show=True,  # Display the image during processing\n    model=\"yolo11n.pt\",  # Ultralytics YOLO11 model file\n)\n\nwhile cap.isOpened():\n    success, im0 = cap.read()\n    if not success:\n        print(\"Video frame is empty or video processing has been successfully completed.\")\n        break\n\n    # Generate heatmap on the frame\n    results = heatmap_obj(im0)\n\n    # Write the frame to the output video\n    video_writer.write(results.plot_im)\n\n# Release resources\ncap.release()\nvideo_writer.release()\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model to TorchScript via CLI\nDESCRIPTION: Command-line instructions to export a YOLO11 model to TorchScript format and run inference using the exported model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/torchscript.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Export a YOLO11n PyTorch model to TorchScript format\nyolo export model=yolo11n.pt format=torchscript # creates 'yolo11n.torchscript'\n\n# Run inference with the exported model\nyolo predict model=yolo11n.torchscript source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Persisting YOLO-World Models with Custom Vocabulary in Python\nDESCRIPTION: Explains how to set custom classes on a YOLO-World model, save it with the new vocabulary, then load and run inference using the customized weights. This approach requires only the 'ultralytics' library and original/pre-trained weights. The process involves calling 'set_classes' followed by 'save', after which subsequent usage loads the specialized model from disk and runs predictions. Benefit: the model is optimized for only the specified classes on future usage.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolo-world.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Initialize a YOLO-World model\nmodel = YOLO(\"yolov8s-world.pt\")  # or select yolov8m/l-world.pt\n\n# Define custom classes\nmodel.set_classes([\"person\", \"bus\"])\n\n# Save the model with the defined offline vocabulary\nmodel.save(\"custom_yolov8s.pt\")\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load your custom model\nmodel = YOLO(\"custom_yolov8s.pt\")\n\n# Run inference to detect your custom classes\nresults = model.predict(\"path/to/image.jpg\")\n\n# Show results\nresults[0].show()\n```\n\n----------------------------------------\n\nTITLE: Implementing TrackZone with Ultralytics YOLO11 in Python\nDESCRIPTION: This Python script demonstrates how to use TrackZone with Ultralytics YOLO11. It includes setting up video capture, defining region points, initializing TrackZone, and processing video frames.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/trackzone.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nassert cap.isOpened(), \"Error reading video file\"\n\n# Define region points\nregion_points = [(150, 150), (1130, 150), (1130, 570), (150, 570)]\n\n# Video writer\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\nvideo_writer = cv2.VideoWriter(\"trackzone_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n\n# Init trackzone (object tracking in zones, not complete frame)\ntrackzone = solutions.TrackZone(\n    show=True,  # display the output\n    region=region_points,  # pass region points\n    model=\"yolo11n.pt\",  # use any model that Ultralytics support, i.e. YOLOv9, YOLOv10\n    # line_width=2,  # adjust the line width for bounding boxes and text display\n)\n\n# Process video\nwhile cap.isOpened():\n    success, im0 = cap.read()\n    if not success:\n        print(\"Video frame is empty or processing is complete.\")\n        break\n\n    results = trackzone(im0)\n\n    # print(results)  # access the output\n\n    video_writer.write(results.plot_im)  # write the video file\n\ncap.release()\nvideo_writer.release()\ncv2.destroyAllWindows()  # destroy all opened windows\n```\n\n----------------------------------------\n\nTITLE: Using Probs in Ultralytics YOLO for Classification\nDESCRIPTION: This example shows how to use a YOLO11n-cls Classify model to perform inference on an image and access the class probabilities using the Probs object.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/predict.md#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n-cls Classify model\nmodel = YOLO(\"yolo11n-cls.pt\")\n\n# Run inference on an image\nresults = model(\"https://ultralytics.com/images/bus.jpg\")  # results list\n\n# View results\nfor r in results:\n    print(r.probs)  # print the Probs object containing the detected class probabilities\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package via pip\nDESCRIPTION: Command to install the ultralytics package using pip package manager. Requires Python>=3.8 and PyTorch>=1.8.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics\n```\n\n----------------------------------------\n\nTITLE: Predicting with Ultralytics YOLO model using full keyword arguments in Python\nDESCRIPTION: This snippet illustrates use of the 'predict' method with all major keyword arguments for fine-tuned inference using Ultralytics YOLO. Brief inline descriptions clarify the role of each argument, many of which control source input, image size, thresholds, result saving, and output formats. Requires ultralytics package and potentially torch. Inputs: model object, image/video source, various configuration values. Outputs: prediction results, with options to save, display, or filter them. Some parameters are optional and customized as per user requirements.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/vscode.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel.predict(\n    source=src,  # (str, optional) source directory for images or videos\n    imgsz=640,  # (int | list) input images size as int or list[w,h] for predict\n    conf=0.25,  # (float) minimum confidence threshold\n    iou=0.7,  # (float) intersection over union (IoU) threshold for NMS\n    vid_stride=1,  # (int) video frame-rate stride\n    stream_buffer=False,  # (bool) buffer incoming frames in a queue (True) or only keep the most recent frame (False)\n    visualize=False,  # (bool) visualize model features\n    augment=False,  # (bool) apply image augmentation to prediction sources\n    agnostic_nms=False,  # (bool) class-agnostic NMS\n    classes=None,  # (int | list[int], optional) filter results by class, i.e. classes=0, or classes=[0,2,3]\n    retina_masks=False,  # (bool) use high-resolution segmentation masks\n    embed=None,  # (list[int], optional) return feature vectors/embeddings from given layers\n    show=False,  # (bool) show predicted images and videos if environment allows\n    save=True,  # (bool) save prediction results\n    save_frames=False,  # (bool) save predicted individual video frames\n    save_txt=False,  # (bool) save results as .txt file\n    save_conf=False,  # (bool) save results with confidence scores\n    save_crop=False,  # (bool) save cropped images with results\n    stream=False,  # (bool) for processing long videos or numerous images with reduced memory usage by returning a generator\n    verbose=True,  # (bool) enable/disable verbose inference logging in the terminal\n)\n\n```\n\n----------------------------------------\n\nTITLE: Using ThreadingLocked Decorator for Thread-Safe Execution in Python\nDESCRIPTION: This example shows how to use the ThreadingLocked decorator provided by Ultralytics to ensure thread-safe execution of functions, particularly useful when sharing a model instance across threads.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/yolo-thread-safe-inference.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\nfrom ultralytics.utils import ThreadingLocked\n\n# Create a model instance\nmodel = YOLO(\"yolo11n.pt\")\n\n\n# Decorate the predict method to make it thread-safe\n@ThreadingLocked()\ndef thread_safe_predict(image_path):\n    \"\"\"Thread-safe prediction using a shared model instance.\"\"\"\n    results = model.predict(image_path)\n    return results\n\n\n# Now you can safely call this function from multiple threads\n```\n\n----------------------------------------\n\nTITLE: Loading YOLOv5 Models with PyTorch Hub in Python\nDESCRIPTION: This Python script demonstrates loading YOLOv5 exported models using PyTorch Hub. It supports multiple formats including PyTorch, TorchScript, ONNX, and others. The script highlights how to import the model for inference and process images using the loaded model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/model_export.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n# Model\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", \"yolov5s.pt\")\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", \"yolov5s.torchscript \")  # TorchScript\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", \"yolov5s.onnx\")  # ONNX Runtime\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", \"yolov5s_openvino_model\")  # OpenVINO\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", \"yolov5s.engine\")  # TensorRT\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", \"yolov5s.mlmodel\")  # CoreML (macOS Only)\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", \"yolov5s_saved_model\")  # TensorFlow SavedModel\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", \"yolov5s.pb\")  # TensorFlow GraphDef\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", \"yolov5s.tflite\")  # TensorFlow Lite\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", \"yolov5s_edgetpu.tflite\")  # TensorFlow Edge TPU\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", \"yolov5s_paddle_model\")  # PaddlePaddle\n\n# Images\nimg = \"https://ultralytics.com/images/zidane.jpg\"  # or file, Path, PIL, OpenCV, numpy, list\n\n# Inference\nresults = model(img)\n\n# Results\nresults.print()  # or .show(), .save(), .crop(), .pandas(), etc.\n```\n\n----------------------------------------\n\nTITLE: Object Blurring using Ultralytics YOLO in Python\nDESCRIPTION: Python script for implementing object blurring on a video using Ultralytics YOLO. It demonstrates how to set up video capture, initialize the ObjectBlurrer, process each frame, and save the output video.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/object-blurring.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nassert cap.isOpened(), \"Error reading video file\"\n\n# Video writer\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\nvideo_writer = cv2.VideoWriter(\"object_blurring_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n\n# Initialize object blurrer object\nblurrer = solutions.ObjectBlurrer(\n    show=True,  # display the output\n    model=\"yolo11n.pt\",  # model for object blurring i.e. yolo11m.pt\n    # line_width=2,  # width of bounding box.\n    # classes=[0, 2],  # count specific classes i.e, person and car with COCO pretrained model.\n    # blur_ratio=0.5,  # adjust percentage of blur intensity, the value in range 0.1 - 1.0\n)\n\n# Process video\nwhile cap.isOpened():\n    success, im0 = cap.read()\n\n    if not success:\n        print(\"Video frame is empty or processing is complete.\")\n        break\n\n    results = blurrer(im0)\n\n    # print(results\")  # access the output\n\n    video_writer.write(results.plot_im)  # write the processed frame.\n\ncap.release()\nvideo_writer.release()\ncv2.destroyAllWindows()  # destroy all opened windows\n```\n\n----------------------------------------\n\nTITLE: SAM Image Segmentation with Prompts in Python\nDESCRIPTION: Using the SAM model in Python for segmentation with different prompt types, this code snippet runs the model with bounding box and point cues. The inputs are model weights and images, and outputs are segmentation masks.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/sam.md#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics import SAM\n\n# Load a model\nmodel = SAM(\"sam_b.pt\")\n\n# Segment with bounding box prompt\nmodel(\"ultralytics/assets/zidane.jpg\", bboxes=[439, 437, 524, 709])\n\n# Segment with points prompt\nmodel(\"ultralytics/assets/zidane.jpg\", points=[900, 370], labels=[1])\n\n# Segment with multiple points prompt\nmodel(\"ultralytics/assets/zidane.jpg\", points=[[400, 370], [900, 370]], labels=[[1, 1]])\n\n# Segment with multiple points prompt per object\nmodel(\"ultralytics/assets/zidane.jpg\", points=[[[400, 370], [900, 370]]], labels=[[1, 1]])\n\n# Segment with negative points prompt.\nmodel(\"ultralytics/assets/zidane.jpg\", points=[[[400, 370], [900, 370]]], labels=[[1, 0]])\n```\n\n----------------------------------------\n\nTITLE: Benchmarking YOLO11 Models on COCO Dataset via CLI\nDESCRIPTION: Command-line interface command to benchmark a YOLO11n model's speed and accuracy on the COCO8 dataset across all export formats. An alternative to the Python implementation for reproducing benchmark results.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/nvidia-jetson.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# Benchmark YOLO11n speed and accuracy on the COCO8 dataset for all all export formats\nyolo benchmark model=yolo11n.pt data=coco8.yaml imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Loading and Performing Inference with Ultralytics YOLO Model in Python\nDESCRIPTION: This snippet demonstrates how to load a trained Ultralytics YOLO model and perform inference on an image. It includes loading the model from a file and processing the results to access bounding boxes, masks, and class probabilities.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/help/FAQ.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"path/to/your/model.pt\")\n\nresults = model(\"path/to/image.jpg\")\n\nfor r in results:\n    print(r.boxes)  # print bounding box predictions\n    print(r.masks)  # print mask predictions\n    print(r.probs)  # print class probabilities\n```\n\n----------------------------------------\n\nTITLE: Implementing Heatmaps in Python\nDESCRIPTION: Complete Python implementation for generating heatmaps from video sources using Ultralytics YOLO. Includes video capture, processing, and writing capabilities with customizable parameters for heatmap generation and visualization.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/heatmaps.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nassert cap.isOpened(), \"Error reading video file\"\n\n# Video writer\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\nvideo_writer = cv2.VideoWriter(\"heatmap_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n\n# Initialize heatmap object\nheatmap = solutions.Heatmap(\n    show=True,  # display the output\n    model=\"yolo11n.pt\",  # path to the YOLO11 model file\n    colormap=cv2.COLORMAP_PARULA,  # colormap of heatmap\n)\n\n# Process video\nwhile cap.isOpened():\n    success, im0 = cap.read()\n\n    if not success:\n        print(\"Video frame is empty or processing is complete.\")\n        break\n\n    results = heatmap(im0)\n\n    video_writer.write(results.plot_im)  # write the processed frame.\n\ncap.release()\nvideo_writer.release()\ncv2.destroyAllWindows()  # destroy all opened windows\n```\n\n----------------------------------------\n\nTITLE: Python Implementation of Object Cropping\nDESCRIPTION: Python code demonstrating how to implement object cropping using YOLO11. Uses OpenCV for video capture and the Ultralytics solutions module for object cropping. Includes initialization of the cropper object with configurable parameters and video processing loop.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/object-cropping.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nassert cap.isOpened(), \"Error reading video file\"\n\n# Initialize object cropper object\ncropper = solutions.ObjectCropper(\n    show=True,  # display the output\n    model=\"yolo11n.pt\",  # model for object cropping i.e yolo11x.pt.\n    classes=[0, 2],  # crop specific classes i.e. person and car with COCO pretrained model.\n    # conf=0.5,  # adjust confidence threshold for the objects.\n    # crop_dir=\"cropped-detections\",  # set the directory name for cropped detections\n)\n\n# Process video\nwhile cap.isOpened():\n    success, im0 = cap.read()\n\n    if not success:\n        print(\"Video frame is empty or processing is complete.\")\n        break\n\n    results = cropper(im0)\n\n    # print(results)  # access the output\n\ncap.release()\ncv2.destroyAllWindows()  # destroy all opened windows\n```\n\n----------------------------------------\n\nTITLE: Hyperparameter Tuning with YOLO11 and Ray Tune in Python\nDESCRIPTION: Code example showing how to load a YOLO11n model and start hyperparameter tuning using Ray Tune on the COCO8 dataset. The model.tune() method integrates with Ray Tune to optimize model hyperparameters.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/ray-tune.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Start tuning hyperparameters for YOLO11n training on the COCO8 dataset\nresult_grid = model.tune(data=\"coco8.yaml\", use_ray=True)\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO Model to IMX500 Format - Python Implementation\nDESCRIPTION: Python code demonstrating how to load a YOLO11n model, export it to IMX500 format with PTQ quantization, and run inference on the exported model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/sony-imx500.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a YOLO11n PyTorch model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model\nmodel.export(format=\"imx\", data=\"coco8.yaml\")  # exports with PTQ quantization by default\n\n# Load the exported model\nimx_model = YOLO(\"yolo11n_imx_model\")\n\n# Run inference\nresults = imx_model(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model with Python\nDESCRIPTION: Python code to train a YOLO11n model on the African Wildlife Dataset for 100 epochs with 640px image size\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/african-wildlife.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"african-wildlife.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Python Implementation of Object Counting with YOLO11\nDESCRIPTION: Complete Python implementation for object counting using video input, including video capture setup, region definition, and frame processing with the ObjectCounter solution.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/object-counting.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nassert cap.isOpened(), \"Error reading video file\"\n\nregion_points = [(20, 400), (1080, 400), (1080, 360), (20, 360)]\n\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\nvideo_writer = cv2.VideoWriter(\"object_counting_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n\ncounter = solutions.ObjectCounter(\n    show=True,\n    region=region_points,\n    model=\"yolo11n.pt\"\n)\n\nwhile cap.isOpened():\n    success, im0 = cap.read()\n\n    if not success:\n        print(\"Video frame is empty or processing is complete.\")\n        break\n\n    results = counter(im0)\n\n    video_writer.write(results.plot_im)\n\ncap.release()\nvideo_writer.release()\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Plotting Results in Ultralytics YOLO\nDESCRIPTION: This snippet illustrates how to use the plot() method to visualize detection results, including saving and displaying the annotated images.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/predict.md#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nfrom PIL import Image\n\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Run inference on 'bus.jpg'\nresults = model([\"https://ultralytics.com/images/bus.jpg\", \"https://ultralytics.com/images/zidane.jpg\"])  # results list\n\n# Visualize the results\nfor i, r in enumerate(results):\n    # Plot results image\n    im_bgr = r.plot()  # BGR-order numpy array\n    im_rgb = Image.fromarray(im_bgr[..., ::-1])  # RGB-order PIL image\n\n    # Show results to screen (in supported environments)\n    r.show()\n\n    # Save results to disk\n    r.save(filename=f\"results{i}.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLOv8 Model to TensorRT with INT8 Quantization (Python)\nDESCRIPTION: Shows how to load a YOLOv8 model (.pt file) and export it to the TensorRT format (.engine file) with INT8 quantization enabled. It specifies parameters like batch size, workspace memory, enables INT8 quantization, and provides calibration data (coco.yaml).\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tensorrt.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\nmodel.export(format=\"engine\", batch=8, workspace=4, int8=True, data=\"coco.yaml\")\n```\n\n----------------------------------------\n\nTITLE: Performing Single-Stream YOLO11 Object Tracking with History Visualization in Python\nDESCRIPTION: This Python code snippet demonstrates real-time object tracking on a single video stream using an Ultralytics YOLO11 model. It reads frames from a video capture, applies the `model.track()` method with persistence enabled, extracts bounding boxes and track IDs, and visualizes both the current detections and the historical path of each tracked object using OpenCV. The loop continues until the video ends or the 'q' key is pressed, after which resources are released. Dependencies include OpenCV (`cv2`), NumPy (`np`), and an initialized Ultralytics `YOLO` model (`model`), video capture object (`cap`), and `track_history` dictionary.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/trackers/README.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Loop through the video frames\nwhile cap.isOpened():\n    # Read a frame from the video\n    success, frame = cap.read()\n\n    if success:\n        # Run YOLO11 tracking on the frame, persisting tracks between frames\n        result = model.track(frame, persist=True)[0]\n\n        # Get the boxes and track IDs\n        if result.boxes and result.boxes.id is not None:\n            boxes = result.boxes.xywh.cpu()\n            track_ids = result.boxes.id.int().cpu().tolist()\n        else:\n            pass\n\n        # Visualize the result on the frame\n        annotated_frame = result.plot()\n\n        # Plot the tracks\n        for box, track_id in zip(boxes, track_ids):\n            x, y, w, h = box\n            track = track_history[track_id]\n            track.append((float(x), float(y)))  # x, y center point\n            if len(track) > 30:  # retain 30 tracks for 30 frames\n                track.pop(0)\n\n            # Draw the tracking lines\n            points = np.hstack(track).astype(np.int32).reshape((-1, 1, 2))\n            cv2.polylines(annotated_frame, [points], isClosed=False, color=(230, 230, 230), thickness=10)\n\n        # Display the annotated frame\n        cv2.imshow(\"YOLO11 Tracking\", annotated_frame)\n\n        # Break the loop if 'q' is pressed\n        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n            break\n    else:\n        # Break the loop if the end of the video is reached\n        break\n\n# Release the video capture object and close the display window\ncap.release()\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Training a Custom YOLO Model in Python\nDESCRIPTION: This Python code snippet demonstrates how to train a custom YOLO model using the Ultralytics library. It imports the YOLO class, loads a model configuration from a YAML file (`yolo11n.yaml`), and then calls the `train()` method. The `train` method requires specifying the path to the custom dataset configuration file (`data=\"path/to/your/dataset.yaml\"`) and the number of training epochs (`epochs=10`).\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/python.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the YOLO model\nmodel = YOLO(\"yolo11n.yaml\")\n\n# Train the model with custom dataset\nmodel.train(data=\"path/to/your/dataset.yaml\", epochs=10)\n\n```\n\n----------------------------------------\n\nTITLE: Using Ultralytics YOLO for Image Prediction\nDESCRIPTION: Python code snippet demonstrating how to use Ultralytics YOLO model for image prediction.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/conda-quickstart.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolo11n.pt\")  # initialize model\nresults = model(\"path/to/image.jpg\")  # perform inference\nresults[0].show()  # display results for the first image\n```\n\n----------------------------------------\n\nTITLE: Calculating Object Distances in Video using Ultralytics YOLO11 in Python\nDESCRIPTION: This code snippet demonstrates how to use the Ultralytics YOLO11 model to calculate distances between objects in a video stream. It initializes a DistanceCalculation object, processes video frames, and writes the output to a new video file.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/distance-calculation.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nassert cap.isOpened(), \"Error reading video file\"\n\n# Video writer\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\nvideo_writer = cv2.VideoWriter(\"distance_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n\n# Initialize distance calculation object\ndistancecalculator = solutions.DistanceCalculation(\n    model=\"yolo11n.pt\",  # path to the YOLO11 model file.\n    show=True,  # display the output\n)\n\n# Process video\nwhile cap.isOpened():\n    success, im0 = cap.read()\n\n    if not success:\n        print(\"Video frame is empty or processing is complete.\")\n        break\n\n    results = distancecalculator(im0)\n\n    print(results)  # access the output\n\n    video_writer.write(results.plot_im)  # write the processed frame.\n\ncap.release()\nvideo_writer.release()\ncv2.destroyAllWindows()  # destroy all opened windows\n```\n\n----------------------------------------\n\nTITLE: Converting DOTA Format to YOLO OBB Format\nDESCRIPTION: Python script for converting DOTA dataset format to YOLO OBB format using the Ultralytics converter utility\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/obb/index.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.data.converter import convert_dota_to_yolo_obb\n\nconvert_dota_to_yolo_obb(\"path/to/DOTA\")\n```\n\n----------------------------------------\n\nTITLE: Multithreaded Object Tracking with YOLO\nDESCRIPTION: Implementation of concurrent object tracking on multiple video streams using threading. The script creates separate threads for each video source and model combination, enabling parallel processing of multiple video streams. Includes thread management and cleanup.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/track.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport threading\n\nimport cv2\n\nfrom ultralytics import YOLO\n\n# Define model names and video sources\nMODEL_NAMES = [\"yolo11n.pt\", \"yolo11n-seg.pt\"]\nSOURCES = [\"path/to/video.mp4\", \"0\"]  # local video, 0 for webcam\n\n\ndef run_tracker_in_thread(model_name, filename):\n    \"\"\"\n    Run YOLO tracker in its own thread for concurrent processing.\n\n    Args:\n        model_name (str): The YOLO11 model object.\n        filename (str): The path to the video file or the identifier for the webcam/external camera source.\n    \"\"\"\n    model = YOLO(model_name)\n    results = model.track(filename, save=True, stream=True)\n    for r in results:\n        pass\n\n\n# Create and start tracker threads using a for loop\ntracker_threads = []\nfor video_file, model_name in zip(SOURCES, MODEL_NAMES):\n    thread = threading.Thread(target=run_tracker_in_thread, args=(model_name, video_file), daemon=True)\n    tracker_threads.append(thread)\n    thread.start()\n\n# Wait for all tracker threads to finish\nfor thread in tracker_threads:\n    thread.join()\n\n# Clean up and close windows\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Validating YOLO11n-OBB Model Accuracy with Python\nDESCRIPTION: This Python code snippet demonstrates how to validate the accuracy of a YOLO11n-OBB model using the ultralytics library. By loading the model and specifying a dataset for validation, metrics providing validation results are obtained to evaluate model performance.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/obb.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-obb.pt\")\n\n# Validate the model\nmetrics = model.val(data=\"dota8.yaml\")\n```\n\n----------------------------------------\n\nTITLE: Splitting DOTA Test Set - Ultralytics DOTA Utilities - Python\nDESCRIPTION: Contains a function to partition test images of the DOTA dataset, ensuring standardized test set preparation for inference benchmarking. Accepts source directory information and outputs separated test tiles/folders as needed. Ensures test data integrity and structure for downstream model evaluation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/data/split_dota.md#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndef split_test(img_dir, save_dir, window_size, stride):\n    # Splits test dataset images into tiles for inference and benchmarking\n    pass  # Implementation crops and organizes test image tiles\n```\n\n----------------------------------------\n\nTITLE: Validating Image-Label Pairings - Python\nDESCRIPTION: The verify_image_label utility cross-validates an image against its annotation label to ensure consistency and completeness in training data. Requires image and label file paths as input, outputs a pass/fail result or set of discrepancies found. Assumes supported file formats; incomplete pairs or malformed files are flagged.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/data/utils.md#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n## ::: ultralytics.data.utils.verify_image_label\n```\n\n----------------------------------------\n\nTITLE: Segmenting Everything with Additional Arguments using SAMPredictor - Python\nDESCRIPTION: This Python example uses the 'SAMPredictor' to segment all objects in an image, demonstrating advanced configuration with extra arguments such as 'crop_n_layers' (for multi-scale segmentation) and 'points_stride' (point grid stride). Dependencies include 'ultralytics'. 'source' is the image to segment; other arguments adjust segmentation granularity and performance. Returns a 'Results' object with predicted masks. This is suitable for batch or exhaustive segmentation use-cases.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/sam.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.models.sam import Predictor as SAMPredictor\n\n# Create SAMPredictor\noverrides = dict(conf=0.25, task=\"segment\", mode=\"predict\", imgsz=1024, model=\"mobile_sam.pt\")\npredictor = SAMPredictor(overrides=overrides)\n\n# Segment with additional args\nresults = predictor(source=\"ultralytics/assets/zidane.jpg\", crop_n_layers=1, points_stride=64)\n\n```\n\n----------------------------------------\n\nTITLE: Initializing and Training YOLO11n Model in Python\nDESCRIPTION: Shows how to initialize a YOLO11n model from a YAML file, train it on the COCO8 dataset, and perform inference using Python. Includes model information display and alternative pre-trained model loading.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/models/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Initialize a YOLO11n model from a YAML configuration file\n# This creates a model architecture without loading pre-trained weights\nmodel = YOLO(\"yolo11n.yaml\")\n\n# Alternatively, load a pre-trained YOLO11n model directly\n# This loads both the architecture and the weights trained on COCO\n# model = YOLO(\"yolo11n.pt\")\n\n# Display model information (architecture, layers, parameters, etc.)\nmodel.info()\n\n# Train the model using the COCO8 dataset (a small subset of COCO) for 100 epochs\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n\n# Run inference with the trained model on an image\nresults = model(\"path/to/image.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model with CLI\nDESCRIPTION: This CLI command exports a YOLO11 model to the ONNX format, enabling deployment in various production environments. The command requires specifying the model and desired export format.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/index.md#2025-04-22_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\nyolo export model=yolo11n.pt format=onnx\n```\n\n----------------------------------------\n\nTITLE: Running FastSAM Inference - Segment by Point Prompts - Shell\nDESCRIPTION: Runs FastSAM segmentation for objects near user-specified points with assigned labels. Expects arguments for model, image, point_prompt (list of coordinates), and point_label. Returns segmentations for objects indicated by the points. Assumes prompt format compatibility with FastSAM script.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/fast-sam.md#2025-04-22_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\npython Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg --point_prompt \"[[520,360],[620,300]]\" --point_label \"[1,0]\"\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n on COCO8 Dataset Using Command Line\nDESCRIPTION: Command line example for training a YOLO11n model on the COCO8 dataset. This command specifies the dataset, pretrained model, number of epochs, and image size parameters.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/coco8.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Train YOLO11n on COCO8 using the command line\nyolo detect train data=coco8.yaml model=yolo11n.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Implementing Queue Management with Ultralytics YOLO in Python\nDESCRIPTION: This Python script demonstrates how to implement queue management using Ultralytics YOLO11. It includes video capture, queue region definition, and processing frames for queue management.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/queue-management.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nassert cap.isOpened(), \"Error reading video file\"\n\n# Video writer\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\nvideo_writer = cv2.VideoWriter(\"queue_management.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n\n# Define queue points\nqueue_region = [(20, 400), (1080, 400), (1080, 360), (20, 360)]  # region points\n# queue_region = [(20, 400), (1080, 400), (1080, 360), (20, 360), (20, 400)]    # polygon points\n\n# Initialize queue manager object\nqueuemanager = solutions.QueueManager(\n    show=True,  # display the output\n    model=\"yolo11n.pt\",  # path to the YOLO11 model file\n    region=queue_region,  # pass queue region points\n)\n\n# Process video\nwhile cap.isOpened():\n    success, im0 = cap.read()\n    if not success:\n        print(\"Video frame is empty or processing is complete.\")\n        break\n    results = queuemanager(im0)\n\n    # print(results)  # access the output\n\n    video_writer.write(results.plot_im)  # write the processed frame.\n\ncap.release()\nvideo_writer.release()\ncv2.destroyAllWindows()  # destroy all opened windows\n```\n\n----------------------------------------\n\nTITLE: Using Picamera2 with YOLO11 for Real-time Object Detection\nDESCRIPTION: Python script to set up the Raspberry Pi Camera Module using picamera2 library and perform real-time object detection with Ultralytics YOLO11 model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/raspberry-pi.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nfrom picamera2 import Picamera2\n\nfrom ultralytics import YOLO\n\npicam2 = Picamera2()\npicam2.preview_configuration.main.size = (1280, 720)\npicam2.preview_configuration.main.format = \"RGB888\"\npicam2.preview_configuration.align()\npicam2.configure(\"preview\")\npicam2.start()\n\nmodel = YOLO(\"yolo11n.pt\")\n\nwhile True:\n    frame = picam2.capture_array()\n    results = model(frame)\n    annotated_frame = results[0].plot()\n    cv2.imshow(\"Camera\", annotated_frame)\n\n    if cv2.waitKey(1) == ord(\"q\"):\n        break\n\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Basic Speed Estimation Example\nDESCRIPTION: Simple example demonstrating how to implement speed estimation with video capture and processing using YOLO11\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/speed-estimation.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\nvideo_writer = cv2.VideoWriter(\"speed_estimation.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n\n# Initialize SpeedEstimator\nspeedestimator = solutions.SpeedEstimator(\n    region=[(0, 360), (1280, 360)],\n    model=\"yolo11n.pt\",\n    show=True,\n)\n\nwhile cap.isOpened():\n    success, im0 = cap.read()\n    if not success:\n        break\n    results = speedestimator(im0)\n    video_writer.write(results.plot_im)\n\ncap.release()\nvideo_writer.release()\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Making API Request for YOLO Detection Model in Python\nDESCRIPTION: This Python code uses the requests library to send a POST request to the Ultralytics API for running inference with a YOLO detection model. It includes the API key, model ID, image file, and inference parameters.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/hub/inference-api.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\n# API URL\nurl = \"https://predict.ultralytics.com\"\n\n# Headers, use actual API_KEY\nheaders = {\"x-api-key\": \"API_KEY\"}\n\n# Inference arguments (use actual MODEL_ID)\ndata = {\"model\": \"https://hub.ultralytics.com/models/MODEL_ID\", \"imgsz\": 640, \"conf\": 0.25, \"iou\": 0.45}\n\n# Load image and send request\nwith open(\"path/to/image.jpg\", \"rb\") as image_file:\n    files = {\"file\": image_file}\n    response = requests.post(url, headers=headers, files=files, data=data)\n\nprint(response.json())\n```\n\n----------------------------------------\n\nTITLE: Benchmarking YOLO Models in Python\nDESCRIPTION: This Python snippet demonstrates how to use the `benchmark` function from `ultralytics.utils.benchmarks` to evaluate the performance (speed and accuracy) of a specified YOLO model (`yolo11n.pt`) using a dataset configuration (`coco8.yaml`) across various export formats. It specifies the image size (640), precision (half=False indicates FP32), and computation device (device=0).\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/python.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.utils.benchmarks import benchmark\n\n# Benchmark\nbenchmark(model=\"yolo11n.pt\", data=\"coco8.yaml\", imgsz=640, half=False, device=0)\n\n```\n\n----------------------------------------\n\nTITLE: Running Batch Predictions with YOLO11 and SAHI\nDESCRIPTION: Code for performing batch predictions on multiple images using YOLO11 with SAHI. The snippet configures slice dimensions, overlap ratios, and other parameters for optimal large-image processing.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/sahi-tiled-inference.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom sahi.predict import predict\n\npredict(\n    model_type=\"ultralytics\",\n    model_path=\"path/to/yolo11n.pt\",\n    model_device=\"cpu\",  # or 'cuda:0'\n    model_confidence_threshold=0.4,\n    source=\"path/to/dir\",\n    slice_height=256,\n    slice_width=256,\n    overlap_height_ratio=0.2,\n    overlap_width_ratio=0.2,\n)\n```\n\n----------------------------------------\n\nTITLE: Basic Heatmap Generation with YOLO11 Object Tracking\nDESCRIPTION: Demonstrates how to initialize and run simultaneous object tracking and heatmap generation using Ultralytics YOLO11. Uses OpenCV for video capture and implements real-time processing with the Heatmap solution.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/heatmaps.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nheatmap = solutions.Heatmap(colormap=cv2.COLORMAP_PARULA, show=True, model=\"yolo11n.pt\")\n\nwhile cap.isOpened():\n    success, im0 = cap.read()\n    if not success:\n        break\n    results = heatmap(im0)\ncap.release()\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Converting YOLO11 Model to TFLite using Python\nDESCRIPTION: Python script demonstrating how to load a YOLO11 model, export it to TFLite format, and run inference with the exported model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tflite.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to TFLite format\nmodel.export(format=\"tflite\")  # creates 'yolo11n_float32.tflite'\n\n# Load the exported TFLite model\ntflite_model = YOLO(\"yolo11n_float32.tflite\")\n\n# Run inference\nresults = tflite_model(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLOv8 to OpenVINO Format and Running Inference in Python\nDESCRIPTION: Demonstrates how to load a YOLOv8 model, export it to OpenVINO format, and run inference with the exported model. The example shows both standard inference and specifying a target device (GPU, NPU, or CPU).\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/openvino.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a YOLOv8n PyTorch model\nmodel = YOLO(\"yolov8n.pt\")\n\n# Export the model\nmodel.export(format=\"openvino\")  # creates 'yolov8n_openvino_model/'\n\n# Load the exported OpenVINO model\nov_model = YOLO(\"yolov8n_openvino_model/\")\n\n# Run inference\nresults = ov_model(\"https://ultralytics.com/images/bus.jpg\")\n\n# Run inference with specified device, available devices: [\"intel:gpu\", \"intel:npu\", \"intel:cpu\"]\nresults = ov_model(\"https://ultralytics.com/images/bus.jpg\", device=\"intel:gpu\")\n```\n\n----------------------------------------\n\nTITLE: Validating YOLO11 Models with a Custom Dataset via CLI (Bash)\nDESCRIPTION: Validates a YOLO11 model on a user-specified custom dataset from the command line, using the dataset's YAML configuration file. Results are shown in the terminal as usual. Requires the 'yolo' CLI and properly formatted model and data YAML paths. The 'data' argument is the main customization.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/val.md#2025-04-22_snippet_8\n\nLANGUAGE: Bash\nCODE:\n```\nyolo val model=yolo11n.pt data=path/to/your/custom_dataset.yaml\n\n```\n\n----------------------------------------\n\nTITLE: Building and Training YOLOv9c Model Using Python\nDESCRIPTION: This Python code snippet demonstrates how to create a YOLOv9c model instance using pretrained weights or from a configuration file. The snippet also includes commands for training the model on a dataset for 100 epochs and running inference on an image. Dependencies include PyTorch pretrained models and corresponding configuration files. Inputs include paths to the configuration or pretrained model file and the image for inference. Outputs include model information and training results.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov9.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Build a YOLOv9c model from scratch\nmodel = YOLO(\"yolov9c.yaml\")\n\n# Build a YOLOv9c model from pretrained weight\nmodel = YOLO(\"yolov9c.pt\")\n\n# Display model information (optional)\nmodel.info()\n\n# Train the model on the COCO8 example dataset for 100 epochs\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n\n# Run inference with the YOLOv9c model on the 'bus.jpg' image\nresults = model(\"path/to/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n Model on COCO128 Dataset using Python\nDESCRIPTION: Python code example showing how to load a pre-trained YOLO11n model and train it on the COCO128 dataset for 100 epochs with an image size of 640. The code utilizes the Ultralytics Python package.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/coco128.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"coco128.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Model on Open Images V7 - Python Implementation\nDESCRIPTION: Python code example demonstrating how to train a YOLO11n model on the Open Images V7 dataset for 100 epochs with 640px image size using the Ultralytics framework.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/open-images-v7.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a COCO-pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Train the model on the Open Images V7 dataset\nresults = model.train(data=\"open-images-v7.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Configuring Tracking Parameters in Python with Ultralytics YOLO\nDESCRIPTION: This Python example illustrates how to set up tracking parameters like confidence threshold, IOU, and visualization display when using Ultralytics YOLO for object tracking on videos. By configuring these parameters, users can optimize tracking performance according to specific use cases. The script requires the 'ultralytics' package to be installed.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/track.md#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Configure the tracking parameters and run the tracker\nmodel = YOLO(\"yolo11n.pt\")\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", conf=0.3, iou=0.5, show=True)\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n-OBB Model with CLI\nDESCRIPTION: This snippet shows how to train a YOLO11n-OBB model via the command line interface. By specifying the dataset and model paths, number of training epochs, and image size, the CLI command effectively manages the training process utilizing the ultralytics framework.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/obb.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nyolo obb train data=path/to/custom_dataset.yaml model=yolo11n-obb.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Running YOLO Inference on a Screenshot in Python\nDESCRIPTION: This snippet shows how to perform YOLO inference directly on the current screen content. It uses the Ultralytics library to load a pretrained model ('yolo11n.pt') and sets the source to the special string 'screen'. Executing the model captures the screen and runs inference, returning a list of Results objects.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/predict.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Define current screenshot as source\nsource = \"screen\"\n\n# Run inference on the source\nresults = model(source)  # list of Results objects\n```\n\n----------------------------------------\n\nTITLE: Running YOLO Inference on a Video File (Streaming) in Python\nDESCRIPTION: This snippet shows how to run YOLO inference on a video file efficiently using streaming mode. It loads a YOLO model, specifies the path to the video file, and calls the model with `stream=True`. This approach processes the video frame by frame, returning a generator of Results objects, which helps conserve memory compared to loading the entire video at once.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/predict.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Define path to video file\nsource = \"path/to/video.mp4\"\n\n# Run inference on the source\nresults = model(source, stream=True)  # generator of Results objects\n```\n\n----------------------------------------\n\nTITLE: Training YOLOv9 Model with Python\nDESCRIPTION: Code for instantiating and training a YOLOv9c model using the Ultralytics YOLO class in Python. This snippet loads a pretrained YOLOv9c model and trains it on the coco8 dataset for 100 epochs with 640x640 image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov9.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Build a YOLOv9c model from pretrained weights and train\nmodel = YOLO(\"yolov9c.pt\")\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Training YOLOv6n with Ultralytics (Python)\nDESCRIPTION: Shows how to create and train a YOLOv6n model using Ultralytics' Python API. Only the training command is included here (for FAQ context), with dependencies and expected data formats matching the other Python example. The 'model.train' call accepts a data YAML, epochs count, and image size, training on the COCO8 dataset.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov6.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Build a YOLOv6n model from scratch\nmodel = YOLO(\"yolov6n.yaml\")\n\n# Train the model on the COCO8 example dataset for 100 epochs\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n\n```\n\n----------------------------------------\n\nTITLE: Running DeepStream Inference\nDESCRIPTION: Executes DeepStream application with the specified configuration\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/deepstream-nvidia-jetson.md#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ndeepstream-app -c deepstream_app_config.txt\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Optimized OpenVINO YOLO Model in Python\nDESCRIPTION: This snippet demonstrates how to load an OpenVINO-optimized YOLO model and run inference on an image. It uses the YOLO class from the ultralytics library to load the model and perform inference.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/optimizing-openvino-latency-vs-throughput-modes.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Load the OpenVINO model\nov_model = YOLO(\"yolov8n_openvino_model/\")\n\n# Run inference with performance hints for latency\nresults = ov_model(\"path/to/image.jpg\", verbose=True)\n```\n\n----------------------------------------\n\nTITLE: Custom Table Creation for WandB in YOLO\nDESCRIPTION: Function for creating custom tables in WandB to log training metrics and results.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/callbacks/wb.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nultralytics.utils.callbacks.wb._custom_table\n```\n\n----------------------------------------\n\nTITLE: Running Inference with YOLO11 Pose Model\nDESCRIPTION: Code for performing predictions with a trained YOLO11 pose model and accessing keypoint results.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/pose.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-pose.pt\")  # load an official model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom model\n\n# Predict with the model\nresults = model(\"https://ultralytics.com/images/bus.jpg\")  # predict on an image\n\n# Access the results\nfor result in results:\n    xy = result.keypoints.xy  # x and y coordinates\n    xyn = result.keypoints.xyn  # normalized\n    kpts = result.keypoints.data  # x, y, visibility (if available)\n```\n\n----------------------------------------\n\nTITLE: TensorRT Model Conversion and Inference\nDESCRIPTION: Python and CLI examples for converting YOLO models to TensorRT format and running inference on Jetson devices.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/nvidia-jetson.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a YOLO11n PyTorch model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to TensorRT\nmodel.export(format=\"engine\")  # creates 'yolo11n.engine'\n\n# Load the exported TensorRT model\ntrt_model = YOLO(\"yolo11n.engine\")\n\n# Run inference\nresults = trt_model(\"https://ultralytics.com/images/bus.jpg\")\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Export a YOLO11n PyTorch model to TensorRT format\nyolo export model=yolo11n.pt format=engine # creates 'yolo11n.engine'\n\n# Run inference with the exported model\nyolo predict model=yolo11n.engine source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model to CoreML Format (Python)\nDESCRIPTION: Python code to load a YOLO11 model, export it to CoreML format, and run inference with the exported model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/coreml.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to CoreML format\nmodel.export(format=\"coreml\")  # creates 'yolo11n.mlpackage'\n\n# Load the exported CoreML model\ncoreml_model = YOLO(\"yolo11n.mlpackage\")\n\n# Run inference\nresults = coreml_model(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO Model for Edge TPU - Python Implementation\nDESCRIPTION: Python code to load and export a YOLO model to Edge TPU compatible format using the Ultralytics framework.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/coral-edge-tpu-on-raspberry-pi.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"path/to/model.pt\")  # Load an official model or custom model\n\n# Export the model\nmodel.export(format=\"edgetpu\")\n```\n\n----------------------------------------\n\nTITLE: Image Processing Callback Implementation\nDESCRIPTION: Implements the callback function that processes incoming images, performs detection and segmentation, and publishes annotated results. Uses ros_numpy for image conversion.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/ros-quickstart.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ros_numpy\n\n\ndef callback(data):\n    \"\"\"Callback function to process image and publish annotated images.\"\"\"\n    array = ros_numpy.numpify(data)\n    if det_image_pub.get_num_connections():\n        det_result = detection_model(array)\n        det_annotated = det_result[0].plot(show=False)\n        det_image_pub.publish(ros_numpy.msgify(Image, det_annotated, encoding=\"rgb8\"))\n\n    if seg_image_pub.get_num_connections():\n        seg_result = segmentation_model(array)\n        seg_annotated = seg_result[0].plot(show=False)\n        seg_image_pub.publish(ros_numpy.msgify(Image, seg_annotated, encoding=\"rgb8\"))\n\n\nrospy.Subscriber(\"/camera/color/image_raw\", Image, callback)\n\nwhile True:\n    rospy.spin()\n```\n\n----------------------------------------\n\nTITLE: Retrieving Multiple Evaluation Metrics from YOLO11 Validation (Python)\nDESCRIPTION: Shows how to access multiple types of validation metrics (mAP50-95, mAP50, mAP75, and classwise metrics) from the result of a YOLO11 model validation run in Python. Useful for comprehensive benchmarking or custom reporting. Assumes the 'metrics' object is returned from 'model.val()', compatible with the Ultralytics API.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/val.md#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nmetrics = model.val()  # assumes `model` has been loaded\nprint(metrics.box.map)  # mAP50-95\nprint(metrics.box.map50)  # mAP50\nprint(metrics.box.map75)  # mAP75\nprint(metrics.box.maps)  # list of mAP50-95 for each category\n\n```\n\n----------------------------------------\n\nTITLE: Converting COCO to YOLO Format Annotations using Ultralytics\nDESCRIPTION: Demonstrates how to convert COCO format dataset annotations to YOLO format using the convert_coco function from ultralytics.data.converter. The function supports both bounding box and segmentation annotations.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/index.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.data.converter import convert_coco\n\nconvert_coco(labels_dir=\"path/to/coco/annotations/\", use_segments=True)\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model to TensorRT and Running Inference (Python)\nDESCRIPTION: Demonstrates how to load a YOLO11 model (.pt file), export it to the TensorRT format (.engine file) using default precision (FP32/FP16), and then run inference on an image using the resulting TensorRT engine.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tensorrt.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolo11n.pt\")\nmodel.export(format=\"engine\")  # creates 'yolo11n.engine'\n\n# Run inference\nmodel = YOLO(\"yolo11n.engine\")\nresults = model(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Prompts for YOLOE Inference using Python SDK (Python)\nDESCRIPTION: Illustrates how to perform YOLOE inference with custom class prompts using the Ultralytics Python SDK. It initializes the model (`yoloe-s.pt`), dynamically sets the classes to detect (\"person\", \"bus\") using `model.set_classes()`, runs prediction on an image, and then displays the results. This demonstrates the 'prompt-then-detect' strategy.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yoloe.md#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Initialize a YOLOE model\nmodel = YOLO(\"yoloe-s.pt\")\n\n# Define custom classes\nmodel.set_classes([\"person\", \"bus\"])\n\n# Execute prediction on an image\nresults = model.predict(\"path/to/image.jpg\")\n\n# Show results\nresults[0].show()\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Model with TensorBoard Integration\nDESCRIPTION: Python code demonstrating how to load a pre-trained YOLO11 model and train it while logging metrics to TensorBoard.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tensorboard.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pre-trained model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Train the model\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Calculating Class Distribution for K-Fold Splits in Python\nDESCRIPTION: This snippet calculates the distribution of class labels for each fold as a ratio of the classes present in validation to those in training. It adds a small value to avoid division by zero.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/kfold-cross-validation.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfold_lbl_distrb = pd.DataFrame(index=folds, columns=cls_idx)\n\nfor n, (train_indices, val_indices) in enumerate(kfolds, start=1):\n    train_totals = labels_df.iloc[train_indices].sum()\n    val_totals = labels_df.iloc[val_indices].sum()\n\n    # To avoid division by zero, we add a small value (1E-7) to the denominator\n    ratio = val_totals / (train_totals + 1e-7)\n    fold_lbl_distrb.loc[f\"split_{n}\"] = ratio\n```\n\n----------------------------------------\n\nTITLE: Running YOLO Inference on a Remote URL in Python\nDESCRIPTION: This snippet demonstrates running YOLO inference on an image or video located at a remote URL. It utilizes the Ultralytics library to load a pretrained model ('yolo11n.pt') and specifies the URL string as the source. The model fetches the content from the URL and performs inference, returning a list of Results objects.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/predict.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Define remote image or video URL\nsource = \"https://ultralytics.com/images/bus.jpg\"\n\n# Run inference on the source\nresults = model(source)  # list of Results objects\n```\n\n----------------------------------------\n\nTITLE: Setting Up YOLOv8 Streamlit Inference Class\nDESCRIPTION: Reference documentation for the Inference class in the streamlit_inference.py file, which handles real-time object detection using YOLOv8 through Streamlit. This implementation allows for webcam-based inference and live object detection in web applications.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/solutions/streamlit_inference.md#2025-04-22_snippet_0\n\n\n\n----------------------------------------\n\nTITLE: Visualizing Results from YOLO11 and SAHI Inference\nDESCRIPTION: Python code to export and visualize the predicted bounding boxes and masks from YOLO11 and SAHI inference.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/sahi-tiled-inference.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nresult.export_visuals(export_dir=\"demo_data/\")\nImage(\"demo_data/prediction_visual.png\")\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Models to TensorRT (INT8 Quantization) - CLI\nDESCRIPTION: Executes YOLO11 model export to TensorRT (engine) format with INT8 quantization from the command line. Requires Ultralytics CLI installed. The 'int8=True' flag enables quantization for improved deployment on edge devices. The exported model is compatible with NVIDIA GPUs and supports fast inference.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/export.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=yolo11n.pt format=engine int8=True # export TensorRT model with INT8 quantization\n```\n\n----------------------------------------\n\nTITLE: Training Custom Object Detection Model with Ultralytics YOLO via CLI\nDESCRIPTION: This command-line instruction shows how to train a custom object detection model using Ultralytics YOLO through the command-line interface. It specifies the task, mode, model, dataset, and number of epochs for training.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/index.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyolo task=detect mode=train model=yolo11n.pt data=path/to/dataset.yaml epochs=50\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with YOLO11 Model via CLI\nDESCRIPTION: Demonstrates how to use the command-line interface to make predictions using both official and custom YOLO11 models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/detect.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nyolo detect predict model=yolo11n.pt source='https://ultralytics.com/images/bus.jpg'      # predict with official model\nyolo detect predict model=path/to/best.pt source='https://ultralytics.com/images/bus.jpg' # predict with custom model\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on ImageNet10 Dataset in Python\nDESCRIPTION: This code snippet demonstrates how to load a pre-trained YOLO model and train it on the ImageNet10 dataset using Python. It sets the image size to 224x224 and runs for 5 epochs.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/imagenet10.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-cls.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"imagenet10\", epochs=5, imgsz=224)\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Model on Open Images V7 - CLI Implementation\nDESCRIPTION: Command-line interface example for training a YOLO11n model on the Open Images V7 dataset using the Ultralytics CLI tool.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/open-images-v7.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# Train a COCO-pretrained YOLO11n model on the Open Images V7 dataset\nyolo detect train data=open-images-v7.yaml model=yolo11n.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Running Inference with YOLOv8 INT8 TensorRT Model (Python)\nDESCRIPTION: Illustrates how to load a previously exported YOLOv8 TensorRT engine file (`yolov8n.engine`), specifically one quantized to INT8, and perform object detection inference on an image URL.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tensorrt.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.engine\", task=\"detect\")\nresult = model.predict(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Running Inference with OpenVINO or ONNX Models\nDESCRIPTION: Executes inference using the specified model format (OpenVINO IR or ONNX) on a given image. Uses the compiled executable for object detection, displaying the results with the YOLOv8 model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-OpenVINO-CPP-Inference/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./detect path/to/your/model.xml path/to/your/image.jpg\n\n# Example using an ONNX model\n./detect path/to/your/model.onnx path/to/your/image.jpg\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Classification Models to Different Formats in Python\nDESCRIPTION: Shows how to export trained YOLO11 classification models to other formats (like ONNX) for deployment using Python API.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/classify.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-cls.pt\")  # load an official model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom trained model\n\n# Export the model\nmodel.export(format=\"onnx\")\n```\n\n----------------------------------------\n\nTITLE: Segmenting Images with Custom Prompts using SAMPredictor - Python\nDESCRIPTION: This code demonstrates the use of the more advanced 'SAMPredictor' class that enables setting the image once and running multiple prompt-based inferences efficiently (avoiding repeated image encoding). Requires 'ultralytics' and optionally OpenCV for reading images as numpy arrays. First, set up predictor parameters in the 'overrides' dict (e.g., confidence threshold, image size, model weights). The snippet covers setting the image by path or as array, running a variety of prompt-based inferences (bounding boxes, single/multiple points, negative prompts), and resetting image state. Input options support flexibility in prompt type and batch size. Output is a 'Results' object for each inference; limitations include proper shape/type of inputs.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/sam.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.models.sam import Predictor as SAMPredictor\n\n# Create SAMPredictor\noverrides = dict(conf=0.25, task=\"segment\", mode=\"predict\", imgsz=1024, model=\"mobile_sam.pt\")\npredictor = SAMPredictor(overrides=overrides)\n\n# Set image\npredictor.set_image(\"ultralytics/assets/zidane.jpg\")  # set with image file\npredictor.set_image(cv2.imread(\"ultralytics/assets/zidane.jpg\"))  # set with np.ndarray\nresults = predictor(bboxes=[439, 437, 524, 709])\n\n# Run inference with single point prompt\nresults = predictor(points=[900, 370], labels=[1])\n\n# Run inference with multiple points prompt\nresults = predictor(points=[[400, 370], [900, 370]], labels=[1, 1])\n\n# Run inference with negative points prompt\nresults = predictor(points=[[[400, 370], [900, 370]]], labels=[[1, 0]])\n\n# Reset image\npredictor.reset_image()\n\n```\n\n----------------------------------------\n\nTITLE: Object Tracking with Ultralytics YOLO using CLI\nDESCRIPTION: This Bash command snippet shows how to utilize the Ultralytics YOLO CLI to perform object tracking on video streams. The command allows switching between different YOLO models and configuring tracking parameters such as the tracker type using YAML files. It offers a straightforward integration for real-time video analytics using the 'yolo' CLI tool.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/track.md#2025-04-22_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\nyolo track model=yolo11n.pt source=\"https://youtu.be/LNwODJXcvt4\"      # Official Detect model\nyolo track model=yolo11n-seg.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Official Segment model\nyolo track model=yolo11n-pose.pt source=\"https://youtu.be/LNwODJXcvt4\" # Official Pose model\nyolo track model=path/to/best.pt source=\"https://youtu.be/LNwODJXcvt4\" # Custom trained model\n\n# Track using ByteTrack tracker\nyolo track model=path/to/best.pt tracker=\"bytetrack.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Running YOLO Inference on a PIL Image Object in Python\nDESCRIPTION: This snippet shows how to perform YOLO inference on an image opened using the Python Imaging Library (PIL). It requires importing `Image` from `PIL` and `YOLO` from `ultralytics`. After loading the model and opening an image file into a PIL Image object, this object is passed directly to the model for inference. The result is a list of Results objects.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/predict.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom PIL import Image\n\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Open an image using PIL\nsource = Image.open(\"path/to/image.jpg\")\n\n# Run inference on the source\nresults = model(source)  # list of Results objects\n```\n\n----------------------------------------\n\nTITLE: Running YOLO model prediction with Ultralytics in Python\nDESCRIPTION: This snippet demonstrates initializing a YOLO model of configurable scale and using it to predict on a sample image asset. It requires the ultralytics Python package and expects that the model weights (e.g., 'yolo11n.pt') and the sample image asset ('bus.jpg') are available. The code iterates through prediction results, printing detection box data; users can uncomment a line to show result images. Inputs: model weight file, image source. Outputs: console printout of detection data (and optionally image display).\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/vscode.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import ASSETS, YOLO\n\nmodel = YOLO(\"yolo11n.pt\", task=\"detect\")\nresults = model(source=ASSETS / \"bus.jpg\")\n\nfor result in results:\n    print(result.boxes.data)\n    # result.show()  # uncomment to view each result image\n\n```\n\n----------------------------------------\n\nTITLE: Initializing YOLO Models and ROS Node\nDESCRIPTION: Sets up YOLO detection and segmentation models and initializes a ROS node named 'ultralytics'. Includes a brief delay to ensure stable connection establishment.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/ros-quickstart.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nimport rospy\n\nfrom ultralytics import YOLO\n\ndetection_model = YOLO(\"yolo11m.pt\")\nsegmentation_model = YOLO(\"yolo11m-seg.pt\")\nrospy.init_node(\"ultralytics\")\ntime.sleep(1)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Dataset Annotations\nDESCRIPTION: Python code for visualizing dataset annotations to verify correctness before training the model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/index.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.data.utils import visualize_image_annotations\n\nlabel_map = {\n    0: \"person\",\n    1: \"car\",\n}\n\nvisualize_image_annotations(\n    \"path/to/image.jpg\",\n    \"path/to/annotations.txt\",\n    label_map,\n)\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Model with ClearML Integration\nDESCRIPTION: Python script demonstrating how to initialize a ClearML task, load a YOLO11 model, set training parameters and start training with automatic experiment tracking\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/clearml.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom clearml import Task\n\nfrom ultralytics import YOLO\n\n# Step 1: Creating a ClearML Task\ntask = Task.init(project_name=\"my_project\", task_name=\"my_yolov8_task\")\n\n# Step 2: Selecting the YOLO11 Model\nmodel_variant = \"yolo11n\"\ntask.set_parameter(\"model_variant\", model_variant)\n\n# Step 3: Loading the YOLO11 Model\nmodel = YOLO(f\"{model_variant}.pt\")\n\n# Step 4: Setting Up Training Arguments\nargs = dict(data=\"coco8.yaml\", epochs=16)\ntask.connect(args)\n\n# Step 5: Initiating Model Training\nresults = model.train(**args)\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 on Apple Silicon (MPS) via CLI\nDESCRIPTION: Shows the command-line interface command to train a YOLO11 model on Apple Silicon using the MPS backend. The `device` argument is set to `mps`. This example starts training from a pretrained model on the COCO8 dataset for 100 epochs with an image size of 640.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/train.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model using MPS\nyolo detect train data=coco8.yaml model=yolo11n.pt epochs=100 imgsz=640 device=mps\n```\n\n----------------------------------------\n\nTITLE: Validating YOLO11 OBB Models\nDESCRIPTION: Code examples for validating trained YOLO11 OBB models using both Python API and CLI approaches. Demonstrates accessing various metrics after validation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/obb.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-obb.pt\")  # load an official model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom model\n\n# Validate the model\nmetrics = model.val(data=\"dota8.yaml\")  # no arguments needed, dataset and settings remembered\nmetrics.box.map  # map50-95(B)\nmetrics.box.map50  # map50(B)\nmetrics.box.map75  # map75(B)\nmetrics.box.maps  # a list contains map50-95(B) of each category\n```\n\nLANGUAGE: bash\nCODE:\n```\nyolo obb val model=yolo11n-obb.pt data=dota8.yaml         # val official model\nyolo obb val model=path/to/best.pt data=path/to/data.yaml # val custom model\n```\n\n----------------------------------------\n\nTITLE: Initializing YOLO-World with Custom Classes\nDESCRIPTION: Demonstrates how to initialize a YOLO-World model and set custom classes for object detection. Shows basic prediction workflow using the model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolo-world.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLOWorld\n\n# Initialize a YOLO-World model\nmodel = YOLOWorld(\"yolov8s-world.pt\")\n\n# Define custom classes\nmodel.set_classes([\"person\", \"bus\"])\n\n# Execute prediction on an image\nresults = model.predict(\"path/to/image.jpg\")\n\n# Show results\nresults[0].show()\n```\n\n----------------------------------------\n\nTITLE: Validating YOLO11n-OBB Model Accuracy with CLI\nDESCRIPTION: This CLI snippet provides a method for validating the accuracy of a YOLO11n-OBB model. By utilizing the command line, the model's performance is evaluated against a specified dataset, with results informing on model precision and correctness.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/obb.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nyolo obb val model=yolo11n-obb.pt data=dota8.yaml\n```\n\n----------------------------------------\n\nTITLE: Cropping Isolated Object\nDESCRIPTION: Extracts the bounding box coordinates and crops the isolated object to its region.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/isolating-segmentation-objects.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# (1) Bounding box coordinates\nx1, y1, x2, y2 = c.boxes.xyxy.cpu().numpy().squeeze().astype(np.int32)\n# Crop image to object region\niso_crop = isolated[y1:y2, x1:x2]\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics and Checking Environment in Python\nDESCRIPTION: This snippet installs the Ultralytics package and performs environment checks. It uses pip to install the package and then imports it to run system checks.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install ultralytics\nimport ultralytics\nultralytics.checks()\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model to ONNX Format in Python\nDESCRIPTION: This code snippet demonstrates how to load a YOLO11 model, export it to ONNX format, and run inference using the exported model in Python.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/onnx.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to ONNX format\nmodel.export(format=\"onnx\")  # creates 'yolo11n.onnx'\n\n# Load the exported ONNX model\nonnx_model = YOLO(\"yolo11n.onnx\")\n\n# Run inference\nresults = onnx_model(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of Ultralytics Explorer in Python\nDESCRIPTION: Demonstrates how to create an Explorer object, generate embeddings, and perform similarity searches using either images or dataset indices.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/api.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import Explorer\n\n# Create an Explorer object\nexplorer = Explorer(data=\"coco128.yaml\", model=\"yolo11n.pt\")\n\n# Create embeddings for your dataset\nexplorer.create_embeddings_table()\n\n# Search for similar images to a given image/images\ndataframe = explorer.get_similar(img=\"path/to/image.jpg\")\n\n# Or search for similar images to a given index/indices\ndataframe = explorer.get_similar(idx=0)\n```\n\n----------------------------------------\n\nTITLE: Performing Sliced Inference with YOLO11 and SAHI\nDESCRIPTION: Python code to perform sliced inference by specifying the slice dimensions and overlap ratios using YOLO11 and SAHI.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/sahi-tiled-inference.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom sahi.predict import get_sliced_prediction\n\nresult = get_sliced_prediction(\n    \"demo_data/small-vehicles1.jpeg\",\n    detection_model,\n    slice_height=256,\n    slice_width=256,\n    overlap_height_ratio=0.2,\n    overlap_width_ratio=0.2,\n)\n```\n\n----------------------------------------\n\nTITLE: Profiling SAM, MobileSAM, FastSAM, and YOLO Models in Python\nDESCRIPTION: This code snippet demonstrates how to profile various SAM variants, MobileSAM, FastSAM, and YOLO segmentation models using the Ultralytics library. It loads each model, displays model information, and runs inference on sample assets.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/mobile-sam.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import ASSETS, SAM, YOLO, FastSAM\n\n# Profile SAM2-t, SAM2-b, SAM-b, MobileSAM\nfor file in [\"sam_b.pt\", \"sam2_b.pt\", \"sam2_t.pt\", \"mobile_sam.pt\"]:\n    model = SAM(file)\n    model.info()\n    model(ASSETS)\n\n# Profile FastSAM-s\nmodel = FastSAM(\"FastSAM-s.pt\")\nmodel.info()\nmodel(ASSETS)\n\n# Profile YOLO models\nfor file_name in [\"yolov8n-seg.pt\", \"yolo11n-seg.pt\"]:\n    model = YOLO(file_name)\n    model.info()\n    model(ASSETS)\n```\n\n----------------------------------------\n\nTITLE: Plotting All Validation Batches in Python\nDESCRIPTION: This snippet demonstrates how to use a callback to plot all validation batches instead of just the first three. It defines a 'plot_samples' function that plots validation samples and predictions for each batch.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/callbacks.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport inspect\n\nfrom ultralytics import YOLO\n\n\ndef plot_samples(validator):\n    frame = inspect.currentframe().f_back.f_back\n    v = frame.f_locals\n    validator.plot_val_samples(v[\"batch\"], v[\"batch_i\"])\n    validator.plot_predictions(v[\"batch\"], v[\"preds\"], v[\"batch_i\"])\n\n\nmodel = YOLO(\"yolo11n.pt\")\nmodel.add_callback(\"on_val_batch_end\", plot_samples)\nmodel.val(data=\"coco.yaml\")\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on Fashion-MNIST - CLI Implementation\nDESCRIPTION: Command-line interface command for training a YOLO model on the Fashion-MNIST dataset using the Ultralytics framework.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/fashion-mnist.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo classify train data=fashion-mnist model=yolo11n-cls.pt epochs=100 imgsz=28\n```\n\n----------------------------------------\n\nTITLE: Testing MobileSAM Using Ultralytics in Python\nDESCRIPTION: This Python snippet guides users to test the MobileSAM model within the Ultralytics framework by utilizing point and box prompts for segment prediction. Dependencies include the 'ultralytics' package and a compatible MobileSAM model file, such as 'mobile_sam.pt'. The inputs are image paths and optional coordinates for predictions, providing a convenient setup for testing MobileSAM's performance on various inputs.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/mobile-sam.md#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics import SAM\n\n# Load the model\nmodel = SAM(\"mobile_sam.pt\")\n\n# Predict a segment based on a point prompt\nmodel.predict(\"ultralytics/assets/zidane.jpg\", points=[900, 370], labels=[1])\n```\n\n----------------------------------------\n\nTITLE: Real-time Object Blurring Implementation with YOLO11\nDESCRIPTION: A simplified Python script demonstrating real-time object blurring using Ultralytics YOLO11. It includes video capture, ObjectBlurrer initialization with customizable parameters, and frame-by-frame processing.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/object-blurring.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nassert cap.isOpened(), \"Error reading video file\"\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n\n# Video writer\nvideo_writer = cv2.VideoWriter(\"object_blurring_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n\n# Init ObjectBlurrer\nblurrer = solutions.ObjectBlurrer(\n    show=True,  # display the output\n    model=\"yolo11n.pt\",  # model=\"yolo11n-obb.pt\" for object blurring using YOLO11 OBB model.\n    blur_ratio=0.5,  # set blur percentage i.e 0.7 for 70% blurred detected objects\n    # line_width=2,  # width of bounding box.\n    # classes=[0, 2],  # count specific classes i.e, person and car with COCO pretrained model.\n)\n\n# Process video\nwhile cap.isOpened():\n    success, im0 = cap.read()\n    if not success:\n        print(\"Video frame is empty or processing is complete.\")\n        break\n    results = blurrer(im0)\n    video_writer.write(results.plot_im)\n\ncap.release()\nvideo_writer.release()\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Constructing and Training YOLOv9c Model Using CLI\nDESCRIPTION: This CLI code snippet shows how to construct a YOLOv9c model using a YAML configuration file and train the model on a dataset. Additionally, it includes a command for running inference on a specific image. The requirements include a YAML configuration file and the dataset file. Expected inputs are paths to these configuration, data sources, and the image for prediction. Outputs are the trained model and inference results.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov9.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Build a YOLOv9c model from scratch and train it on the COCO8 example dataset for 100 epochs\nyolo train model=yolov9c.yaml data=coco8.yaml epochs=100 imgsz=640\n\n# Build a YOLOv9c model from scratch and run inference on the 'bus.jpg' image\nyolo predict model=yolov9c.yaml source=path/to/bus.jpg\n```\n\n----------------------------------------\n\nTITLE: Launching YOLO-World Model Training from Scratch with Multiple Datasets (Python)\nDESCRIPTION: Illustrates setting up dataset dictionaries and launching training for a YOLO-World model from scratch using Ultralytics' 'YOLOWorld' class and the 'WorldTrainerFromScratch' trainer. Dependencies include the 'ultralytics' library, custom dataset YAML and annotation files, and proper configuration paths. The 'train' method is invoked with dataset specifications, batch size, epoch count, and trainer choice. Inputs are data configuration and hyperparameters; outputs are a newly trained model and training results in designated directories.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolo-world.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLOWorld\nfrom ultralytics.models.yolo.world.train_world import WorldTrainerFromScratch\n\ndata = dict(\n    train=dict(\n        yolo_data=[\"Objects365.yaml\"],\n        grounding_data=[\n            dict(\n                img_path=\"../datasets/flickr30k/images\",\n                json_file=\"../datasets/flickr30k/final_flickr_separateGT_train.json\",\n            ),\n            dict(\n                img_path=\"../datasets/GQA/images\",\n                json_file=\"../datasets/GQA/final_mixed_train_no_coco.json\",\n            ),\n        ],\n    ),\n    val=dict(yolo_data=[\"lvis.yaml\"]),\n)\nmodel = YOLOWorld(\"yolov8s-worldv2.yaml\")\nmodel.train(data=data, batch=128, epochs=100, trainer=WorldTrainerFromScratch)\n```\n\n----------------------------------------\n\nTITLE: Exporting and Using YOLO11 Model in PaddlePaddle Format (Python)\nDESCRIPTION: This snippet demonstrates how to export a YOLO11 model to PaddlePaddle format using Python, load the exported model, and run inference with it. It uses the YOLO class from the ultralytics library.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/paddlepaddle.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to PaddlePaddle format\nmodel.export(format=\"paddle\")  # creates '/yolo11n_paddle_model'\n\n# Load the exported PaddlePaddle model\npaddle_model = YOLO(\"./yolo11n_paddle_model\")\n\n# Run inference\nresults = paddle_model(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Training a Detection Model Using Ultralytics YOLO CLI\nDESCRIPTION: Example command for training a detection model using the Ultralytics YOLO CLI. Specifies dataset, model, number of epochs, and learning rate.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/quickstart.md#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nyolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model to TF GraphDef in Python\nDESCRIPTION: This snippet demonstrates how to load a YOLO11 model, export it to TF GraphDef format, and run inference using the exported model in Python. It uses the Ultralytics YOLO class to handle model operations.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tf-graphdef.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to TF GraphDef format\nmodel.export(format=\"pb\")  # creates 'yolo11n.pb'\n\n# Load the exported TF GraphDef model\ntf_graphdef_model = YOLO(\"yolo11n.pb\")\n\n# Run inference\nresults = tf_graphdef_model(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Integrating Albumentations with YOLO11 Training\nDESCRIPTION: Example showing how to install required packages and train a YOLO model with automatic Albumentations augmentations. The code demonstrates basic setup and model training with COCO dataset.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/albumentations.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Install required packages\n# !pip install albumentations ultralytics\nfrom ultralytics import YOLO\n\n# Load and train model with automatic augmentations\nmodel = YOLO(\"yolo11n.pt\")\nmodel.train(data=\"coco8.yaml\", epochs=100)\n```\n\n----------------------------------------\n\nTITLE: Handling Bounding Boxes with Boxes Class — Python\nDESCRIPTION: The Boxes class is designed for efficient manipulation and storage of bounding box data generated by the Ultralytics engine. It provides methods for geometric operations and format conversions, depending on PyTorch for tensor computations. Input consists of predicted or ground truth bounding boxes, and output is a structured tensor representation with utility functions.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/engine/results.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n## ::: ultralytics.engine.results.Boxes\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n Model on VisDrone Dataset in Python\nDESCRIPTION: Python code snippet for loading a pretrained YOLO11n model and training it on the VisDrone dataset for 100 epochs with 640px image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/visdrone.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"VisDrone.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Loading and Running Inference with YOLO OBB Model in Python\nDESCRIPTION: This code snippet demonstrates how to load a YOLO OBB (Oriented Bounding Box) model and run inference on an image using the Ultralytics library. It loads the model, performs inference, and prints the results in JSON format.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/hub/inference-api.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load model\nmodel = YOLO(\"yolov8n-obb.pt\")\n\n# Run inference\nresults = model(\"image.jpg\")\n\n# Print image.jpg results in JSON format\nprint(results[0].tojson())\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLOv8 Model to ONNX Format\nDESCRIPTION: This command exports a YOLOv8 model (e.g., yolov8n.pt) to ONNX format with specific parameters for compatibility with OpenCV's DNN module.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-OpenCV-ONNX-Python/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=yolov8n.pt imgsz=640 format=onnx opset=12\n```\n\n----------------------------------------\n\nTITLE: Working with Boxes Objects in YOLO\nDESCRIPTION: Shows how to access and work with Boxes objects from YOLO detection results, which contain bounding box information.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/predict.md#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Run inference on an image\nresults = model(\"https://ultralytics.com/images/bus.jpg\")  # results list\n\n# View results\nfor r in results:\n    print(r.boxes)  # print the Boxes object containing the detection bounding boxes\n```\n\n----------------------------------------\n\nTITLE: Tracking and Segmenting Video using SAM2VideoPredictor in Python\nDESCRIPTION: This code snippet shows how to segment video content using specific prompts and track objects using the SAM2VideoPredictor class. It demonstrates running inference with various point prompts while tracking changes throughout the video. The snippet requires the 'sam2_b.pt' model file and the 'ultralytics' library.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/sam-2.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.models.sam import SAM2VideoPredictor\n\n# Create SAM2VideoPredictor\noverrides = dict(conf=0.25, task=\"segment\", mode=\"predict\", imgsz=1024, model=\"sam2_b.pt\")\npredictor = SAM2VideoPredictor(overrides=overrides)\n\n# Run inference with single point\nresults = predictor(source=\"test.mp4\", points=[920, 470], labels=[1])\n\n# Run inference with multiple points\nresults = predictor(source=\"test.mp4\", points=[[920, 470], [909, 138]], labels=[1, 1])\n\n# Run inference with multiple points prompt per object\nresults = predictor(source=\"test.mp4\", points=[[[920, 470], [909, 138]]], labels=[[1, 1]])\n\n# Run inference with negative points prompt\nresults = predictor(source=\"test.mp4\", points=[[[920, 470], [909, 138]]], labels=[[1, 0]])\n```\n\n----------------------------------------\n\nTITLE: Implementing Text Label Annotation with YOLO in Python\nDESCRIPTION: This snippet shows how to add text labels to detected objects using the YOLO model and OpenCV. It processes a video file frame by frame, detects objects with YOLO, and adds text annotations to each bounding box before saving the result to a new video file.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/simple-utilities.md#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import YOLO\nfrom ultralytics.solutions.solutions import SolutionAnnotator\nfrom ultralytics.utils.plotting import colors\n\nmodel = YOLO(\"yolo11s.pt\")\nnames = model.names\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\n\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\nwriter = cv2.VideoWriter(\"Ultralytics text annotation.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), fps, (w, h))\n\nwhile True:\n    ret, im0 = cap.read()\n    if not ret:\n        break\n\n    annotator = SolutionAnnotator(im0)\n    results = model.predict(im0)\n    boxes = results[0].boxes.xyxy.cpu()\n    clss = results[0].boxes.cls.cpu().tolist()\n\n    for box, cls in zip(boxes, clss):\n        annotator.text_label(box, label=names[int(cls)], color=colors(cls, True))\n\n    writer.write(im0)\n    cv2.imshow(\"Ultralytics text annotation\", im0)\n\n    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n        break\n\nwriter.release()\ncap.release()\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Validating YOLO11 Models Using Ultralytics CLI (Bash)\nDESCRIPTION: Showcases CLI usage for validating YOLO11 models, supporting both official and custom models with a single command. The CLI automatically applies remembered training settings unless overridden by arguments, outputting validation metrics in the terminal. Requires the 'yolo' CLI from the Ultralytics library and access to a .pt model file; arguments may be adjusted for data path or model variant.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/val.md#2025-04-22_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\nyolo detect val model=yolo11n.pt      # val official model\nyolo detect val model=path/to/best.pt # val custom model\n\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Model in Python\nDESCRIPTION: Demonstrates how to load a YOLO11 model and train it on the COCO8 dataset for 100 epochs with an image size of 640.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/detect.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.yaml\")  # build a new model from YAML\nmodel = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\nmodel = YOLO(\"yolo11n.yaml\").load(\"yolo11n.pt\")  # build from YAML and transfer weights\n\n# Train the model\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Loading YOLO Model and Running Prediction\nDESCRIPTION: Initializes a YOLO segmentation model and runs inference on default example images.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/isolating-segmentation-objects.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-seg.pt\")\n\n# Run inference\nresults = model.predict()\n```\n\n----------------------------------------\n\nTITLE: Loading YOLO Model for K-Fold Training in Python\nDESCRIPTION: This code loads the YOLO model from Ultralytics for object detection tasks. It specifies the path to the weights file and sets the task to 'detect'.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/kfold-cross-validation.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nweights_path = \"path/to/weights.pt\"  # use yolo11n.pt for a small model\nmodel = YOLO(weights_path, task=\"detect\")\n```\n\n----------------------------------------\n\nTITLE: Training End Callback\nDESCRIPTION: Callback function that executes at the end of training to perform final logging and cleanup.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/callbacks/wb.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nultralytics.utils.callbacks.wb.on_train_end\n```\n\n----------------------------------------\n\nTITLE: Training and Inferencing YOLO12 via CLI\nDESCRIPTION: This Bash snippet illustrates how to use the Ultralytics command-line interface (CLI) to train and run inference with a YOLO12 model. The first command (`yolo train`) loads a pretrained model (`yolo12n.pt`), specifies the training dataset configuration (`coco8.yaml`), number of epochs, and image size to initiate training. The second command (`yolo predict`) uses the same pretrained model to perform object detection inference on a source image (`path/to/bus.jpg`). Requires the `ultralytics` package to be installed and accessible from the command line.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolo12.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n```bash\n# Load a COCO-pretrained YOLO12n model and train on the COCO8 example dataset for 100 epochs\nyolo train model=yolo12n.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Load a COCO-pretrained YOLO12n model and run inference on the 'bus.jpg' image\nyolo predict model=yolo12n.pt source=path/to/bus.jpg\n```\n```\n\n----------------------------------------\n\nTITLE: YOLO11 Model Export Command\nDESCRIPTION: Command to export a YOLO11 model to ONNX format for integration with other frameworks\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/speed-estimation.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyolo export --weights yolo11n.pt --include onnx\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 to TFLite Edge TPU in Python\nDESCRIPTION: This code demonstrates how to load a YOLO11 model, export it to TFLite Edge TPU format, and then use the exported model for inference. The export process creates a file with '_full_integer_quant_edgetpu.tflite' suffix.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/edge-tpu.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to TFLite Edge TPU format\nmodel.export(format=\"edgetpu\")  # creates 'yolo11n_full_integer_quant_edgetpu.tflite'\n\n# Load the exported TFLite Edge TPU model\nedgetpu_model = YOLO(\"yolo11n_full_integer_quant_edgetpu.tflite\")\n\n# Run inference\nresults = edgetpu_model(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Exporting Ultralytics YOLO Model to OpenVINO Format in Python\nDESCRIPTION: This code snippet shows how to load a YOLO model and export it to OpenVINO format with FP16 precision. It uses the YOLO class from the ultralytics library to perform the export.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/optimizing-openvino-latency-vs-throughput-modes.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolov8n.pt\")\n\n# Export the model to OpenVINO format\nmodel.export(format=\"openvino\", half=True)  # Export with FP16 precision\n```\n\n----------------------------------------\n\nTITLE: Loading TensorBoard in Google Colab for YOLO11 Training Visualization\nDESCRIPTION: Sets up TensorBoard in Google Colab to visualize metrics from YOLO11 model training. This command loads the TensorBoard extension and points it to the directory containing training logs for visualization.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/train.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nload_ext tensorboard\ntensorboard --logdir ultralytics/runs # replace with 'runs' directory\n```\n\n----------------------------------------\n\nTITLE: Working with Masks Objects in YOLO\nDESCRIPTION: Demonstrates how to use Masks objects for instance segmentation with YOLO-seg models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/predict.md#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n-seg Segment model\nmodel = YOLO(\"yolo11n-seg.pt\")\n\n# Run inference on an image\nresults = model(\"https://ultralytics.com/images/bus.jpg\")  # results list\n\n# View results\nfor r in results:\n    print(r.masks)  # print the Masks object containing the detected instance masks\n```\n\n----------------------------------------\n\nTITLE: Visualizing YOLO11 Model Metrics in Python\nDESCRIPTION: This code snippet shows how to visualize and analyze model metrics using JupyterLab's plotting capabilities. It uses the plot_results function from Ultralytics to create visualizations of the model's performance metrics.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/jupyterlab.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.utils.plotting import plot_results\n\nplot_results(results.results_dict)\n```\n\n----------------------------------------\n\nTITLE: Counting Class Instances in Annotations\nDESCRIPTION: Processes each label file to count instances of each class and populates the DataFrame. Uses Counter to track occurrences of each class label in the YOLO format annotations.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/kfold-cross-validation.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom collections import Counter\n\nfor label in labels:\n    lbl_counter = Counter()\n\n    with open(label) as lf:\n        lines = lf.readlines()\n\n    for line in lines:\n        # classes for YOLO label uses integer at first position of each line\n        lbl_counter[int(line.split(\" \")[0])] += 1\n\n    labels_df.loc[label.stem] = lbl_counter\n\nlabels_df = labels_df.fillna(0.0)  # replace `nan` values with `0.0`\n```\n\n----------------------------------------\n\nTITLE: Running Edge TPU Inference - Python Implementation\nDESCRIPTION: Python code to load an Edge TPU compatible YOLO model and perform inference on an image.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/coral-edge-tpu-on-raspberry-pi.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"path/to/edgetpu_model.tflite\")  # Load an official model or custom model\n\n# Run Prediction\nmodel.predict(\"path/to/source.png\")\n```\n\n----------------------------------------\n\nTITLE: Training YOLOE Text Prompt Model\nDESCRIPTION: Demonstrates how to train a YOLOE model from scratch using text prompts. Configures training data from multiple sources and sets up training parameters for segmentation tasks.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yoloe.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLOE\nfrom ultralytics.models.yolo.yoloe import YOLOESegTrainerFromScratch\n\ndata = dict(\n    train=dict(\n        yolo_data=[\"Objects365.yaml\"],\n        grounding_data=[\n            dict(\n                img_path=\"../datasets/flickr/full_images/\",\n                json_file=\"../datasets/flickr/annotations/final_flickr_separateGT_train_segm.json\",\n            ),\n            dict(\n                img_path=\"../datasets/mixed_grounding/gqa/images\",\n                json_file=\"../datasets/mixed_grounding/annotations/final_mixed_train_no_coco_segm.json\",\n            ),\n        ],\n    ),\n    val=dict(yolo_data=[\"lvis.yaml\"]),\n)\n\nmodel = YOLOE(\"yoloe-11l-seg.yaml\")\nmodel.train(\n    data=data,\n    batch=128,\n    epochs=30,\n    close_mosaic=2,\n    optimizer=\"AdamW\",\n    lr0=2e-3,\n    warmup_bias_lr=0.0,\n    weight_decay=0.025,\n    momentum=0.9,\n    workers=4,\n    trainer=YOLOESegTrainerFromScratch,\n    device=\"0,1,2,3,4,5,6,7\",\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Security Alarm System with YOLO11 and OpenCV\nDESCRIPTION: Python script that implements a security alarm system using Ultralytics YOLO11 for object detection. The system processes video input, detects objects, and sends email notifications when detection thresholds are exceeded. Requires OpenCV, Ultralytics solutions, and email authentication setup.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/security-alarm-system.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nassert cap.isOpened(), \"Error reading video file\"\n\n# Video writer\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\nvideo_writer = cv2.VideoWriter(\"security_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n\nfrom_email = \"abc@gmail.com\"  # the sender email address\npassword = \"---- ---- ---- ----\"  # 16-digits password generated via: https://myaccount.google.com/apppasswords\nto_email = \"xyz@gmail.com\"  # the receiver email address\n\n# Initialize security alarm object\nsecurityalarm = solutions.SecurityAlarm(\n    show=True,  # display the output\n    model=\"yolo11n.pt\",  # i.e. yolo11s.pt, yolo11m.pt\n    records=1,  # total detections count to send an email\n)\n\nsecurityalarm.authenticate(from_email, password, to_email)  # authenticate the email server\n\n# Process video\nwhile cap.isOpened():\n    success, im0 = cap.read()\n\n    if not success:\n        print(\"Video frame is empty or video processing has been successfully completed.\")\n        break\n\n    results = securityalarm(im0)\n\n    # print(results)  # access the output\n\n    video_writer.write(results.plot_im)  # write the processed frame.\n\ncap.release()\nvideo_writer.release()\ncv2.destroyAllWindows()  # destroy all opened windows\n```\n\n----------------------------------------\n\nTITLE: Performing YOLO11 Inference with MNN in Python\nDESCRIPTION: This Python script loads a YOLO11 model using MNN, preprocesses an input image, performs inference, and applies non-maximum suppression to the results. It includes command-line argument parsing for flexible usage.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/mnn.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport argparse\n\nimport MNN\nimport MNN.cv as cv2\nimport MNN.numpy as np\n\n\ndef inference(model, img, precision, backend, thread):\n    config = {}\n    config[\"precision\"] = precision\n    config[\"backend\"] = backend\n    config[\"numThread\"] = thread\n    rt = MNN.nn.create_runtime_manager((config,))\n    # net = MNN.nn.load_module_from_file(model, ['images'], ['output0'], runtime_manager=rt)\n    net = MNN.nn.load_module_from_file(model, [], [], runtime_manager=rt)\n    original_image = cv2.imread(img)\n    ih, iw, _ = original_image.shape\n    length = max((ih, iw))\n    scale = length / 640\n    image = np.pad(original_image, [[0, length - ih], [0, length - iw], [0, 0]], \"constant\")\n    image = cv2.resize(\n        image, (640, 640), 0.0, 0.0, cv2.INTER_LINEAR, -1, [0.0, 0.0, 0.0], [1.0 / 255.0, 1.0 / 255.0, 1.0 / 255.0]\n    )\n    image = image[..., ::-1]  # BGR to RGB\n    input_var = np.expand_dims(image, 0)\n    input_var = MNN.expr.convert(input_var, MNN.expr.NC4HW4)\n    output_var = net.forward(input_var)\n    output_var = MNN.expr.convert(output_var, MNN.expr.NCHW)\n    output_var = output_var.squeeze()\n    # output_var shape: [84, 8400]; 84 means: [cx, cy, w, h, prob * 80]\n    cx = output_var[0]\n    cy = output_var[1]\n    w = output_var[2]\n    h = output_var[3]\n    probs = output_var[4:]\n    # [cx, cy, w, h] -> [y0, x0, y1, x1]\n    x0 = cx - w * 0.5\n    y0 = cy - h * 0.5\n    x1 = cx + w * 0.5\n    y1 = cy + h * 0.5\n    boxes = np.stack([x0, y0, x1, y1], axis=1)\n    # ensure ratio is within the valid range [0.0, 1.0]\n    boxes = np.clip(boxes, 0, 1)\n    # get max prob and idx\n    scores = np.max(probs, 0)\n    class_ids = np.argmax(probs, 0)\n    result_ids = MNN.expr.nms(boxes, scores, 100, 0.45, 0.25)\n    print(result_ids.shape)\n    # nms result box, score, ids\n    result_boxes = boxes[result_ids]\n    result_scores = scores[result_ids]\n    result_class_ids = class_ids[result_ids]\n    for i in range(len(result_boxes)):\n        x0, y0, x1, y1 = result_boxes[i].read_as_tuple()\n        y0 = int(y0 * scale)\n        y1 = int(y1 * scale)\n        x0 = int(x0 * scale)\n        x1 = int(x1 * scale)\n        # clamp to the original image size to handle cases where padding was applied\n        x1 = min(iw, x1)\n        y1 = min(ih, y1)\n        print(result_class_ids[i])\n        cv2.rectangle(original_image, (x0, y0), (x1, y1), (0, 0, 255), 2)\n    cv2.imwrite(\"res.jpg\", original_image)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model\", type=str, required=True, help=\"the yolo11 model path\")\n    parser.add_argument(\"--img\", type=str, required=True, help=\"the input image path\")\n    parser.add_argument(\"--precision\", type=str, default=\"normal\", help=\"inference precision: normal, low, high, lowBF\")\n    parser.add_argument(\n        \"--backend\",\n        type=str,\n        default=\"CPU\",\n        help=\"inference backend: CPU, OPENCL, OPENGL, NN, VULKAN, METAL, TRT, CUDA, HIAI\",\n    )\n    parser.add_argument(\"--thread\", type=int, default=4, help=\"inference using thread: int\")\n    args = parser.parse_args()\n    inference(args.model, args.img, args.precision, args.backend, args.thread)\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO Segmentation Model to ONNX in Python\nDESCRIPTION: This Python code snippet demonstrates how to load a pretrained YOLO segmentation model and export it to ONNX format. It uses the YOLO class from the ultralytics library.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/segment.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained model\nmodel = YOLO(\"yolo11n-seg.pt\")\n\n# Export the model to ONNX format\nmodel.export(format=\"onnx\")\n```\n\n----------------------------------------\n\nTITLE: Installing YOLO11 Dependencies\nDESCRIPTION: Installs YOLO11 requirements, the ultralytics package, and ONNX for model export capabilities.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/azureml-quickstart.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd ultralytics\npip install -r requirements.txt\npip install ultralytics\npip install onnx\n```\n\n----------------------------------------\n\nTITLE: Exporting and Using YOLO11 Model with Python\nDESCRIPTION: Python code demonstrating how to load a YOLO11 model, export it to TF SavedModel format, and perform inference with the exported model\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tf-savedmodel.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to TF SavedModel format\nmodel.export(format=\"saved_model\")  # creates '/yolo11n_saved_model'\n\n# Load the exported TF SavedModel model\ntf_savedmodel_model = YOLO(\"./yolo11n_saved_model\")\n\n# Run inference\nresults = tf_savedmodel_model(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO Model to TensorRT INT8 Format in Python\nDESCRIPTION: This snippet demonstrates how to export a YOLO model to TensorRT INT8 format using Python. It includes setting key parameters like dynamic axes, batch size, workspace allocation, and specifying the dataset for calibration.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tensorrt.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\nmodel.export(\n    format=\"engine\",\n    dynamic=True,\n    batch=8,\n    workspace=4,\n    int8=True,\n    data=\"coco.yaml\",\n)\n\n# Load the exported TensorRT INT8 model\nmodel = YOLO(\"yolov8n.engine\", task=\"detect\")\n\n# Run inference\nresult = model.predict(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Resuming Hyperparameter Tuning in Python with Ultralytics YOLO\nDESCRIPTION: Demonstrates how to resume an interrupted hyperparameter tuning session using the YOLO model's tune() method. It shows how to define a search space and resume tuning with or without specifying a run name.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/hyperparameter-tuning.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Define a YOLO model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Define search space\nsearch_space = {\n    \"lr0\": (1e-5, 1e-1),\n    \"degrees\": (0.0, 45.0),\n}\n\n# Resume previous run\nresults = model.tune(data=\"coco8.yaml\", epochs=50, iterations=300, space=search_space, resume=True)\n\n# Resume tuning run with name 'tune_exp'\nresults = model.tune(data=\"coco8.yaml\", epochs=50, iterations=300, space=search_space, name=\"tune_exp\", resume=True)\n```\n\n----------------------------------------\n\nTITLE: Exporting and Using YOLO11 CoreML Model (CLI)\nDESCRIPTION: CLI commands to export a YOLO11 PyTorch model to CoreML format and run inference with the exported model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/coreml.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Export a YOLO11n PyTorch model to CoreML format\nyolo export model=yolo11n.pt format=coreml # creates 'yolo11n.mlpackage''\n\n# Run inference with the exported model\nyolo predict model=yolo11n.mlpackage source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Pose Model via CLI\nDESCRIPTION: Command line interface commands for training YOLO11 pose models using different initialization methods.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/pose.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Build a new model from YAML and start training from scratch\nyolo pose train data=coco8-pose.yaml model=yolo11n-pose.yaml epochs=100 imgsz=640\n\n# Start training from a pretrained *.pt model\nyolo pose train data=coco8-pose.yaml model=yolo11n-pose.pt epochs=100 imgsz=640\n\n# Build a new model from YAML, transfer pretrained weights to it and start training\nyolo pose train data=coco8-pose.yaml model=yolo11n-pose.yaml pretrained=yolo11n-pose.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Quick YOLO11 Model Validation with Python API (Python)\nDESCRIPTION: A minimal Python code snippet for validating a YOLO11 model with Ultralytics, highlighting access to the primary metric (mAP50-95). Suitable for quick quality checks and basic workflows. Requires 'ultralytics' and a compatible model file; returns a metrics object with .box.map attributes.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/val.md#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Validate the model\nmetrics = model.val()\nprint(metrics.box.map)  # map50-95\n\n```\n\n----------------------------------------\n\nTITLE: Configuring DOTAv1 Dataset with YAML\nDESCRIPTION: YAML configuration file for the DOTA v1 dataset, specifying paths, classes, and other dataset parameters for use with Ultralytics models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/obb/dota-v2.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/DOTAv1.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Model on Brain Tumor Dataset in Python\nDESCRIPTION: Python code snippet to load a pretrained YOLO11 model and train it on the brain tumor dataset for 100 epochs with 640x640 image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/brain-tumor.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"brain-tumor.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with Trained Model on Medical Pills Dataset in Python\nDESCRIPTION: This Python code demonstrates how to load a fine-tuned YOLO model and use it for inference on a sample image from the medical-pills dataset.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/medical-pills.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"path/to/best.pt\")  # load a fine-tuned model\n\n# Inference using the model\nresults = model.predict(\"https://ultralytics.com/assets/medical-pills-sample.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Classification Models via CLI\nDESCRIPTION: Demonstrates how to export YOLO11 classification models to different formats using the command-line interface for deployment purposes.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/classify.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=yolo11n-cls.pt format=onnx  # export official model\nyolo export model=path/to/best.pt format=onnx # export custom trained model\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Pose Model with Dog-Pose Dataset in Python\nDESCRIPTION: Python code snippet for training a YOLO11n-pose model on the Dog-pose dataset for 100 epochs with an image size of 640. Loads a pretrained model before starting the training process.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/dog-pose.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-pose.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"dog-pose.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 to NCNN Format using Python API\nDESCRIPTION: Python code demonstrating how to load a YOLO11 model, export it to NCNN format, and run inference with the exported model. This shows the full workflow from loading to deployment.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/ncnn.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to NCNN format\nmodel.export(format=\"ncnn\")  # creates '/yolo11n_ncnn_model'\n\n# Load the exported NCNN model\nncnn_model = YOLO(\"./yolo11n_ncnn_model\")\n\n# Run inference\nresults = ncnn_model(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Models to ONNX Format - CLI\nDESCRIPTION: Uses the Ultralytics CLI to export a YOLO11 model to ONNX format, providing a fast alternative to Python scripting. Requires Ultralytics to be installed and a valid model checkpoint. The 'model' argument specifies the model file path, and 'format' selects ONNX export. Produces an ONNX file ready for inference.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/export.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=yolo11n.pt format=onnx      # export official model\nyolo export model=path/to/best.pt format=onnx # export custom trained model\n```\n\n----------------------------------------\n\nTITLE: Running Inference using Streamlit with Ultralytics YOLO (Python)\nDESCRIPTION: Python code to set up and run inference using Streamlit with Ultralytics YOLO. Demonstrates how to create an Inference object and start the inference process.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/streamlit-live-inference.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import solutions\n\ninf = solutions.Inference(\n    model=\"yolo11n.pt\",  # you can use any model that Ultralytics support, i.e. YOLO11, or custom trained model\n)\n\ninf.inference()\n\n# Make sure to run the file using command `streamlit run path/to/file.py`\n```\n\n----------------------------------------\n\nTITLE: Initializing, Training, and Inferencing YOLO12 in Python\nDESCRIPTION: This Python snippet demonstrates the basic workflow for using a YOLO12 model with the Ultralytics library. It shows how to load a pretrained model (`yolo12n.pt`) using the `YOLO` class, initiate training on a specified dataset (`coco8.yaml`) for a set number of epochs and image size, and perform inference on a single image (`path/to/bus.jpg`). Dependencies include the `ultralytics` package and a pretrained model file.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolo12.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom ultralytics import YOLO\n\n# Load a COCO-pretrained YOLO12n model\nmodel = YOLO(\"yolo12n.pt\")\n\n# Train the model on the COCO8 example dataset for 100 epochs\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n\n# Run inference with the YOLO12n model on the 'bus.jpg' image\nresults = model(\"path/to/bus.jpg\")\n```\n```\n\n----------------------------------------\n\nTITLE: Exporting and Running YOLO Model with DLA in Python\nDESCRIPTION: Code example showing how to load a YOLO11n model, export it to TensorRT format with DLA enabled, and run inference. DLA cores are specified using 'dla:0' or 'dla:1' notation, and half-precision (FP16) is required for DLA compatibility.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/nvidia-jetson.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a YOLO11n PyTorch model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to TensorRT with DLA enabled (only works with FP16 or INT8)\nmodel.export(format=\"engine\", device=\"dla:0\", half=True)  # dla:0 or dla:1 corresponds to the DLA cores\n\n# Load the exported TensorRT model\ntrt_model = YOLO(\"yolo11n.engine\")\n\n# Run inference\nresults = trt_model(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: YOLO Webcam Inference\nDESCRIPTION: Shows how to run YOLO inference on a connected camera device using the device index.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/predict.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Run inference on the source\nresults = model(source=0, stream=True)  # generator of Results objects\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics via conda\nDESCRIPTION: This snippet illustrates how to install Ultralytics using the conda package manager. It includes steps for installation in a CUDA environment, emphasizing the importance of package order to avoid conflicts. The snippet also suggests using the Miniconda3 Docker image for a predefined conda environment. Dependencies include the conda tool and internet connectivity.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/quickstart.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Install the ultralytics package using conda\nconda install -c conda-forge ultralytics\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Install all packages together using conda\nconda install -c pytorch -c nvidia -c conda-forge pytorch torchvision pytorch-cuda=11.8 ultralytics\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Set image name as a variable\nt=ultralytics/ultralytics:latest-conda\n\n# Pull the latest ultralytics image from Docker Hub\nsudo docker pull $t\n\n# Run the ultralytics image in a container with GPU support\nsudo docker run -it --ipc=host --gpus all $t            # all GPUs\nsudo docker run -it --ipc=host --gpus \"device=2,3\" $t # specify GPUs\n```\n\n----------------------------------------\n\nTITLE: Loading and Running Inference with YOLO Detection Model in Python\nDESCRIPTION: This code snippet demonstrates how to load a YOLO detection model and run inference on an image using the Ultralytics library. It loads the model, performs inference, and prints the results in JSON format.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/hub/inference-api.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load model\nmodel = YOLO(\"yolov8n.pt\")\n\n# Run inference\nresults = model(\"image.jpg\")\n\n# Print image.jpg results in JSON format\nprint(results[0].to_json())\n```\n\n----------------------------------------\n\nTITLE: Advanced DetectionTrainer Customization with Callbacks\nDESCRIPTION: Demonstrates complex customization including custom model implementation, loss function modification, and callback integration.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/engine.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.models.yolo.detect import DetectionTrainer\nfrom ultralytics.nn.tasks import DetectionModel\n\n\nclass MyCustomModel(DetectionModel):\n    def init_criterion(self):\n        \"\"\"Initializes the loss function and adds a callback for uploading the model to Google Drive every 10 epochs.\"\"\"\n        ...\n\n\nclass CustomTrainer(DetectionTrainer):\n    def get_model(self, cfg, weights):\n        \"\"\"Returns a customized detection model instance configured with specified config and weights.\"\"\"\n        return MyCustomModel(...)\n\n\n# Callback to upload model weights\ndef log_model(trainer):\n    \"\"\"Logs the path of the last model weight used by the trainer.\"\"\"\n    last_weight_path = trainer.last\n    print(last_weight_path)\n\n\ntrainer = CustomTrainer(overrides={...})\ntrainer.add_callback(\"on_train_epoch_end\", log_model)  # Adds to existing callbacks\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics and Ray Tune for YOLO11 Tuning Bash\nDESCRIPTION: This snippet installs the latest ultralytics, ray with tune support, and optionally wandb for experiment logging, using pip. There are no inputs or outputs beyond the installation steps. Assumes a Unix-like environment or Windows with Python and pip available.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/ray-tune.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install -U ultralytics \"ray[tune]\"\\npip install wandb # optional for logging\n```\n\n----------------------------------------\n\nTITLE: Extracting Object Dimensions from YOLO11 Prediction Results\nDESCRIPTION: This code demonstrates how to load a pre-trained YOLO11 model, make predictions on an image, and extract the width and height dimensions of detected objects from the bounding boxes. It uses the Ultralytics YOLO framework to process the prediction results.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/yolo-common-issues.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pre-trained YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Specify the source image\nsource = \"https://ultralytics.com/images/bus.jpg\"\n\n# Make predictions\nresults = model.predict(source, save=True, imgsz=320, conf=0.5)\n\n# Extract bounding box dimensions\nboxes = results[0].boxes.xywh.cpu()\nfor box in boxes:\n    x, y, w, h = box\n    print(f\"Width of Box: {w}, Height of Box: {h}\")\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset Cache Files - Python\nDESCRIPTION: The load_dataset_cache_file utility retrieves cached dataset metadata from disk to accelerate dataset initialization or validation. Inputs are cache file paths; outputs latest cache objects for reading. Depends on serialization formats such as JSON or pickle, with possible version compatibility constraints.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/data/utils.md#2025-04-22_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\n## ::: ultralytics.data.utils.load_dataset_cache_file\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on ImageNette320 Dataset using Python\nDESCRIPTION: This Python code loads a pretrained YOLO model and trains it on the ImageNette320 dataset, which contains images resized to 320x320 pixels for a balance between speed and quality.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/imagenette.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-cls.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model with ImageNette320\nresults = model.train(data=\"imagenette320\", epochs=100, imgsz=320)\n```\n\n----------------------------------------\n\nTITLE: Predicting with YOLO-World using CLI\nDESCRIPTION: This Bash command performs object detection using a pre-trained YOLO-World model on a specified image. The main inputs are the model type and image source path. The command leverages YOLO capabilities to produce and display the detection results.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolo-world.md#2025-04-22_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\nyolo predict model=yolov8s-world.pt source=path/to/image.jpg imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model in Python\nDESCRIPTION: Illustrates how to export a YOLO11 model to a different format (e.g., ONNX) using Python.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/detect.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load an official model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom trained model\n\n# Export the model\nmodel.export(format=\"onnx\")\n```\n\n----------------------------------------\n\nTITLE: Installing YOLO11 and TensorBoard Dependencies\nDESCRIPTION: Command to install the required package for YOLO11 and TensorBoard integration via pip.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tensorboard.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics\n```\n\n----------------------------------------\n\nTITLE: Using Custom Tracker Configuration in Python with Ultralytics YOLO\nDESCRIPTION: This example shows how to apply a custom tracker configuration in Ultralytics YOLO using Python. Users can modify and apply different parameters in a custom YAML file to adjust tracking behaviors without altering the core tracking algorithm. The code requires 'ultralytics' package and access to configuration files.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/track.md#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the model and run the tracker with a custom configuration file\nmodel = YOLO(\"yolo11n.pt\")\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", tracker=\"custom_tracker.yaml\")\n```\n\n----------------------------------------\n\nTITLE: COCO8-Pose Dataset YAML Configuration\nDESCRIPTION: YAML configuration file for the COCO8-Pose dataset that defines dataset paths, classes, and other settings for training pose detection models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/coco8-pose.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/coco8-pose.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 on Apple Silicon (M1/M2/M3/M4) with Python API\nDESCRIPTION: Demonstrates how to train a YOLO11 model on Apple Silicon chips using Metal Performance Shaders (MPS). This example loads a pre-trained YOLO11n model and trains it on the COCO8 dataset with the MPS device specified.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/train.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Train the model on Apple silicon chip (M1/M2/M3/M4)\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640, device=\"mps\")\n```\n\n----------------------------------------\n\nTITLE: Complete YOLO-ROS Integration for Depth Processing\nDESCRIPTION: Full implementation combining initialization, callback processing, and main loop for YOLO-based depth image processing in ROS. Includes all necessary imports and complete workflow.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/ros-quickstart.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nimport numpy as np\nimport ros_numpy\nimport rospy\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\n\nfrom ultralytics import YOLO\n\nrospy.init_node(\"ultralytics\")\ntime.sleep(1)\n\nsegmentation_model = YOLO(\"yolo11m-seg.pt\")\n\nclasses_pub = rospy.Publisher(\"/ultralytics/detection/distance\", String, queue_size=5)\n\n\ndef callback(data):\n    \"\"\"Callback function to process depth image and RGB image.\"\"\"\n    image = rospy.wait_for_message(\"/camera/color/image_raw\", Image)\n    image = ros_numpy.numpify(image)\n    depth = ros_numpy.numpify(data)\n    result = segmentation_model(image)\n\n    all_objects = []\n    for index, cls in enumerate(result[0].boxes.cls):\n        class_index = int(cls.cpu().numpy())\n        name = result[0].names[class_index]\n        mask = result[0].masks.data.cpu().numpy()[index, :, :].astype(int)\n        obj = depth[mask == 1]\n        obj = obj[~np.isnan(obj)]\n        avg_distance = np.mean(obj) if len(obj) else np.inf\n        all_objects.append(f\"{name}: {avg_distance:.2f}m\")\n\n    classes_pub.publish(String(data=str(all_objects)))\n\n\nrospy.Subscriber(\"/camera/depth/image_raw\", Image, callback)\n\nwhile True:\n    rospy.spin()\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Model on Brain Tumor Dataset via CLI\nDESCRIPTION: Command-line interface command to train a YOLO11 model on the brain tumor dataset for 100 epochs with 640x640 image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/brain-tumor.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo detect train data=brain-tumor.yaml model=yolo11n.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Training Example - CLI Implementation (FAQ)\nDESCRIPTION: Example command from FAQ section showing how to train a YOLO model on Fashion-MNIST dataset using CLI.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/fashion-mnist.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nyolo classify train data=fashion-mnist model=yolo11n-cls.pt epochs=100 imgsz=28\n```\n\n----------------------------------------\n\nTITLE: Training and Inference with Ultralytics YOLOv6 (CLI)\nDESCRIPTION: These shell commands demonstrate how to train and run inference on YOLOv6 models using the Ultralytics CLI. The 'yolo train' command builds and trains a model with specified configuration and data files, number of epochs, and input size. The 'yolo predict' command performs inference on the provided image. Ultralytics must be installed and in your PATH.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov6.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Build a YOLOv6n model from scratch and train it on the COCO8 example dataset for 100 epochs\nyolo train model=yolov6n.yaml data=coco8.yaml epochs=100 imgsz=640\n\n# Build a YOLOv6n model from scratch and run inference on the 'bus.jpg' image\nyolo predict model=yolov6n.yaml source=path/to/bus.jpg\n\n```\n\n----------------------------------------\n\nTITLE: YOLO Object Tracking Implementation\nDESCRIPTION: Shows how to implement object tracking using YOLO models with different tracking configurations.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/python.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load an official detection model\nmodel = YOLO(\"yolo11n-seg.pt\")  # load an official segmentation model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom model\n\n# Track with the model\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", show=True)\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", show=True, tracker=\"bytetrack.yaml\")\n```\n\n----------------------------------------\n\nTITLE: Executing Ray Tune for Hyperparameter Optimization in Python\nDESCRIPTION: This function runs Ray Tune for hyperparameter optimization. It sets up the tuning process, configures search spaces, and executes the optimization routine for machine learning models, likely focused on YOLO architectures.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/tuner.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nultralytics.utils.tuner.run_ray_tune\n```\n\n----------------------------------------\n\nTITLE: Performing Batch Prediction with YOLO11 and SAHI\nDESCRIPTION: Python code to perform batch prediction on a directory of images using YOLO11 and SAHI.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/sahi-tiled-inference.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom sahi.predict import predict\n\npredict(\n    model_type=\"ultralytics\",\n    model_path=\"path/to/yolo11n.pt\",\n    model_device=\"cpu\",  # or 'cuda:0'\n    model_confidence_threshold=0.4,\n    source=\"path/to/dir\",\n    slice_height=256,\n    slice_width=256,\n    overlap_height_ratio=0.2,\n    overlap_width_ratio=0.2,\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Modules and Downloading Resources for YOLO11 and SAHI\nDESCRIPTION: Python code to import necessary modules and download a YOLO11 model and test images for use with SAHI.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/sahi-tiled-inference.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sahi.utils.file import download_from_url\nfrom sahi.utils.ultralytics import download_yolo11n_model\n\n# Download YOLO11 model\nmodel_path = \"models/yolo11n.pt\"\ndownload_yolo11n_model(model_path)\n\n# Download test images\ndownload_from_url(\n    \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/small-vehicles1.jpeg\",\n    \"demo_data/small-vehicles1.jpeg\",\n)\ndownload_from_url(\n    \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/terrain2.png\",\n    \"demo_data/terrain2.png\",\n)\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model to MNN Format via CLI\nDESCRIPTION: Command-line instructions for exporting a YOLO11 PyTorch model to MNN format and running inference with the exported model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/mnn.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Export a YOLO11n PyTorch model to MNN format\nyolo export model=yolo11n.pt format=mnn # creates 'yolo11n.mnn'\n\n# Run inference with the exported model\nyolo predict model='yolo11n.mnn' source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Validating YOLO11 Classification Models via CLI\nDESCRIPTION: Shows how to validate official or custom YOLO11 classification models using the command-line interface.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/classify.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyolo classify val model=yolo11n-cls.pt  # val official model\nyolo classify val model=path/to/best.pt # val custom model\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Pose Model in Python\nDESCRIPTION: Code for exporting a YOLO11 pose model to different formats like ONNX for deployment.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/pose.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-pose.pt\")  # load an official model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom trained model\n\n# Export the model\nmodel.export(format=\"onnx\")\n```\n\n----------------------------------------\n\nTITLE: Saving Dataset Cache Files - Python\nDESCRIPTION: The save_dataset_cache_file utility serializes and writes dataset metadata to cache files for future quick reloading, reducing overhead in subsequent operations. Parameters include destination file path and data objects; output is a written file on disk. Uses standard serialization libraries with proper permission and disk space requirements.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/data/utils.md#2025-04-22_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\n## ::: ultralytics.data.utils.save_dataset_cache_file\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on COCO Dataset - Python\nDESCRIPTION: Python code example showing how to load a pretrained YOLO model and train it on the COCO dataset for 100 epochs with 640px image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/coco.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"coco.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Processing Depth and RGB Images with YOLO\nDESCRIPTION: Implements callback function to process depth and RGB images, perform object detection and segmentation, and calculate object distances. Handles both image types and publishes distance information to ROS topic.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/ros-quickstart.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport ros_numpy\nfrom sensor_msgs.msg import Image\n\n\ndef callback(data):\n    \"\"\"Callback function to process depth image and RGB image.\"\"\"\n    image = rospy.wait_for_message(\"/camera/color/image_raw\", Image)\n    image = ros_numpy.numpify(image)\n    depth = ros_numpy.numpify(data)\n    result = segmentation_model(image)\n\n    all_objects = []\n    for index, cls in enumerate(result[0].boxes.cls):\n        class_index = int(cls.cpu().numpy())\n        name = result[0].names[class_index]\n        mask = result[0].masks.data.cpu().numpy()[index, :, :].astype(int)\n        obj = depth[mask == 1]\n        obj = obj[~np.isnan(obj)]\n        avg_distance = np.mean(obj) if len(obj) else np.inf\n        all_objects.append(f\"{name}: {avg_distance:.2f}m\")\n\n    classes_pub.publish(String(data=str(all_objects)))\n\n\nrospy.Subscriber(\"/camera/depth/image_raw\", Image, callback)\n\nwhile True:\n    rospy.spin()\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Segmentation Model with Carparts Dataset via CLI\nDESCRIPTION: This command-line interface (CLI) snippet shows how to train a YOLO11 segmentation model using the Carparts Segmentation Dataset. It specifies the 'yolo11n-seg.pt' model, 'carparts-seg.yaml' configuration, 100 epochs, and 640x640 image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/carparts-seg.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nyolo segment train data=carparts-seg.yaml model=yolo11n-seg.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Automatically Annotating Datasets with Ultralytics in Python\nDESCRIPTION: This Python snippet demonstrates how to automatically annotate datasets using the Ultralytics framework's 'auto_annotate' function. The code utilizes a detection model and a segmentation model to process the dataset located at a specified path. Dependencies include the 'ultralytics' package and relevant pre-trained models such as 'yolo11x.pt' and 'mobile_sam.pt'. Ensure these models are accessible in your environment before running the code.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/mobile-sam.md#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics.data.annotator import auto_annotate\n\nauto_annotate(data=\"path/to/images\", det_model=\"yolo11x.pt\", sam_model=\"mobile_sam.pt\")\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLOv8 Model to ONNX Format in Python\nDESCRIPTION: This snippet demonstrates how to load a YOLOv8 model and export it to ONNX format using the Ultralytics YOLO library. It sets specific parameters for the export process, such as ONNX opset version, simplification, and input image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-ONNXRuntime-CPP/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a YOLOv8 model (e.g., yolov8n.pt)\nmodel = YOLO(\"yolov8n.pt\")\n\n# Export the model to ONNX format\n# opset=12 is recommended for compatibility\n# simplify=True optimizes the model graph\n# dynamic=False ensures fixed input size, often better for C++ deployment\n# imgsz=640 sets the input image size\nmodel.export(format=\"onnx\", opset=12, simplify=True, dynamic=False, imgsz=640)\nprint(\"Model exported successfully to yolov8n.onnx\")\n```\n\n----------------------------------------\n\nTITLE: Logging into Ultralytics HUB and Training YOLO Model in Python\nDESCRIPTION: This code snippet demonstrates how to log into Ultralytics HUB using an API key, load a YOLO model from HUB, and start training the model. It showcases the basic workflow for using Ultralytics HUB with YOLO models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/hub.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Log in to HUB using your API key (https://hub.ultralytics.com/settings?tab=api+keys)\nhub.login(\"YOUR_API_KEY\")\n\n# Load your model from HUB (replace 'YOUR_MODEL_ID' with your model ID)\nmodel = YOLO(\"https://hub.ultralytics.com/models/YOUR_MODEL_ID\")\n\n# Train the model\nresults = model.train()\n```\n\n----------------------------------------\n\nTITLE: Benchmarking YOLO Models using Ultralytics Python API\nDESCRIPTION: This Python snippet demonstrates using the `benchmark` function from `ultralytics.utils.benchmarks` to evaluate YOLO model performance. It shows how to run a general benchmark on a specified device (GPU 0) and how to target a specific export format like ONNX. Key parameters include `model`, `data`, `imgsz`, `half`, `device`, and `format`. Requires the `ultralytics` library and relevant model/data files.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/benchmark.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.utils.benchmarks import benchmark\n\n# Benchmark on GPU\nbenchmark(model=\"yolo11n.pt\", data=\"coco8.yaml\", imgsz=640, half=False, device=0)\n\n# Benchmark specific export format\nbenchmark(model=\"yolo11n.pt\", data=\"coco8.yaml\", imgsz=640, format=\"onnx\")\n```\n\n----------------------------------------\n\nTITLE: Running YOLOv10 Inference in Python\nDESCRIPTION: Example code demonstrating how to load a pre-trained YOLOv10 model and perform inference on an image using the Ultralytics YOLO Python library.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov10.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the pre-trained YOLOv10n model\nmodel = YOLO(\"yolov10n.pt\")\nresults = model(\"image.jpg\")\nresults[0].show()\n```\n\n----------------------------------------\n\nTITLE: Validating YOLO11 Pose Model in Python\nDESCRIPTION: Code for validating a trained YOLO11 pose model and accessing various validation metrics.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/pose.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-pose.pt\")  # load an official model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom model\n\n# Validate the model\nmetrics = model.val()  # no arguments needed, dataset and settings remembered\nmetrics.box.map  # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps  # a list contains map50-95 of each category\n```\n\n----------------------------------------\n\nTITLE: Converting YOLOv5 Results to Pandas DataFrame\nDESCRIPTION: Demonstrates how to convert YOLOv5 detection results to a Pandas DataFrame, which provides convenient data manipulation, filtering, and analysis capabilities for the detection outputs.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/pytorch_hub_model_loading.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nresults = model(im)  # inference\nresults.pandas().xyxy[0]  # Pandas DataFrame\n```\n\n----------------------------------------\n\nTITLE: YOLO CLI Prediction Command\nDESCRIPTION: Command line interface usage for making predictions with a pretrained YOLO model on an image.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyolo predict model=yolo11n.pt source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Running TrackZone using Ultralytics YOLO11 CLI\nDESCRIPTION: These commands demonstrate how to use the Ultralytics YOLO11 CLI to run TrackZone. They show basic usage, specifying a source video, and defining region coordinates.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/trackzone.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Run a trackzone example\nyolo solutions trackzone show=True\n\n# Pass a source video\nyolo solutions trackzone show=True source=\"path/to/video.mp4\"\n\n# Pass region coordinates\nyolo solutions trackzone show=True region=\"[(150, 150), (1130, 150), (1130, 570), (150, 570)]\"\n```\n\n----------------------------------------\n\nTITLE: Exporting and Using ONNX Models with OpenCV DNN\nDESCRIPTION: These Bash commands illustrate the process of exporting a YOLOv5 model to the ONNX format and using it with OpenCV DNN for inference. The commands require a specific model path and include flags to enable OpenCV's DNN module.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/model_export.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython export.py --weights yolov5s.pt --include onnx\n\npython detect.py --weights yolov5s.onnx --dnn # detect\npython val.py --weights yolov5s.onnx --dnn    # validate\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Classification Model in Python\nDESCRIPTION: Shows how to load a pretrained YOLO model and train it on a custom dataset using Python. Specifies the data path, number of epochs, and image size for training.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/index.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-cls.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"path/to/dataset\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model to CoreML Format using Python\nDESCRIPTION: Python code snippet to load a YOLO11 model and export it to CoreML format using the Ultralytics library.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/coreml.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolo11n.pt\")\nmodel.export(format=\"coreml\")\n```\n\n----------------------------------------\n\nTITLE: Prompt-Free YOLOE Usage in Python\nDESCRIPTION: This snippet shows how to use a prompt-free variant of the YOLOE model, which detects objects from a predefined class list without needing user-provided prompts. The example demonstrates model initialization and running prediction without any external prompt requirements.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yoloe.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLOE\n\n# Initialize a YOLOE model\nmodel = YOLOE(\"yoloe-11l-seg-pf.pt\")\n\n# Run prediction. No prompts required.\nresults = model.predict(\"path/to/image.jpg\")\n\n# Show results\nresults[0].show()\n```\n\n----------------------------------------\n\nTITLE: Visualize YOLO Dataset Annotations\nDESCRIPTION: Function to visualize YOLO annotations on images before training, drawing bounding boxes and labels with class names.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/simple-utilities.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.data.utils import visualize_image_annotations\n\nlabel_map = {  # Define the label map with all annotated class labels.\n    0: \"person\",\n    1: \"car\",\n}\n\n# Visualize\nvisualize_image_annotations(\n    \"path/to/image.jpg\",  # Input image path.\n    \"path/to/annotations.txt\",  # Annotation file path for the image.\n    label_map,\n)\n```\n\n----------------------------------------\n\nTITLE: Using YAML Configuration for Signature Detection Dataset\nDESCRIPTION: YAML configuration file that defines the signature detection dataset parameters including paths and class information. This file is referenced during model training to locate and properly use the dataset.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/signature.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/signature.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Running FastSAM Inference - Segment Everything - Shell\nDESCRIPTION: Runs the official FastSAM 'Inference.py' script to segment all objects in a specified image using a specified model checkpoint. Requires previously downloaded weights, input image, and all dependencies installed. Outputs the segmentation mask results for the input image.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/fast-sam.md#2025-04-22_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\npython Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating Conda Environment for Ultralytics\nDESCRIPTION: Commands to create a new Conda environment named 'ultralytics-env' with Python 3.11 and activate it.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/conda-quickstart.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create --name ultralytics-env python=3.11 -y\n```\n\nLANGUAGE: bash\nCODE:\n```\nconda activate ultralytics-env\n```\n\n----------------------------------------\n\nTITLE: Training YOLO OBB Model - Python Implementation\nDESCRIPTION: Python code example showing how to create and train a YOLO11n-OBB model on the DOTAv1 dataset using the Ultralytics API\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/obb/index.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Create a new YOLO11n-OBB model from scratch\nmodel = YOLO(\"yolo11n-obb.yaml\")\n\n# Train the model on the DOTAv1 dataset\nresults = model.train(data=\"DOTAv1.yaml\", epochs=100, imgsz=1024)\n```\n\n----------------------------------------\n\nTITLE: Training a YOLO Model with Python\nDESCRIPTION: Python code example showing how to load a pre-trained YOLO model and train it using a specified dataset. The example demonstrates initializing a YOLO model and configuring training parameters.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/index.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Streamlit Application with Custom YOLO Model\nDESCRIPTION: Python code demonstrating how to create a Streamlit application using a custom YOLO model for inference.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/streamlit-live-inference.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import solutions\n\ninf = solutions.Inference(\n    model=\"yolo11n.pt\",  # you can use any model that Ultralytics support, i.e. YOLO11, YOLOv10\n)\n\ninf.inference()\n\n# Make sure to run the file using command `streamlit run path/to/file.py`\n```\n\n----------------------------------------\n\nTITLE: Running YOLOv10 Inference via CLI\nDESCRIPTION: Command line interface example for performing object detection using a pre-trained YOLOv10 model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov10.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nyolo detect predict model=yolov10n.pt source=path/to/image.jpg\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO Model to IMX500 Format - CLI Implementation\nDESCRIPTION: Command-line interface commands for exporting a YOLO11n model to IMX500 format and running inference using the exported model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/sony-imx500.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Export a YOLO11n PyTorch model to imx format with Post-Training Quantization (PTQ)\nyolo export model=yolo11n.pt format=imx data=coco8.yaml\n\n# Run inference with the exported model\nyolo predict model=yolo11n_imx_model source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: YOLOE Linear Probing in Python\nDESCRIPTION: This code snippet shows how to perform linear probing by freezing certain layers of a YOLOE model. It involves selecting layers to freeze and running a modified training process with fewer epochs while customizing other training parameters. The purpose is to tune the model while retaining the pre-trained weights to a certain extent.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yoloe.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLOE\nfrom ultralytics.models.yolo.yoloe import YOLOEPESegTrainer\n\nmodel = YOLOE(\"yoloe-11s-seg.pt\")\nhead_index = len(model.model.model) - 1\nfreeze = [str(f) for f in range(0, head_index)]\nfor name, child in model.model.model[-1].named_children():\n    if \"cv3\" not in name:\n        freeze.append(f\"{head_index}.{name}\")\n\nfreeze.extend(\n    [\n        f\"{head_index}.cv3.0.0\",\n        f\"{head_index}.cv3.0.1\",\n        f\"{head_index}.cv3.1.0\",\n        f\"{head_index}.cv3.1.1\",\n        f\"{head_index}.cv3.2.0\",\n        f\"{head_index}.cv3.2.1\",\n    ]\n)\n\nmodel.train(\n    data=\"coco128-seg.yaml\",\n    epochs=2,\n    close_mosaic=0,\n    batch=16,\n    optimizer=\"AdamW\",\n    lr0=1e-3,\n    warmup_bias_lr=0.0,\n    weight_decay=0.025,\n    momentum=0.9,\n    workers=4,\n    device=\"0\",\n    trainer=YOLOEPESegTrainer,\n    freeze=freeze,\n)\n```\n\n----------------------------------------\n\nTITLE: Validating YOLO11-pose Model Performance\nDESCRIPTION: Shows how to validate a trained YOLO11-pose model using retained dataset parameters from training. Supports both official and custom-trained models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/pose.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-pose.pt\")  # load an official model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom model\n\n# Validate the model\nmetrics = model.val()  # no arguments needed, dataset and settings remembered\n```\n\n----------------------------------------\n\nTITLE: Using RT-DETR with Ultralytics CLI\nDESCRIPTION: This CLI snippet shows the command-line method to load and train a pre-trained RT-DETR-l model and run inference. It requires the `yolo` command-line tool and uses parameters like model path, data configuration file, epochs, and image size. The commands provide training results and inference outputs.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/rtdetr.md#2025-04-22_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\n# Load a COCO-pretrained RT-DETR-l model and train it on the COCO8 example dataset for 100 epochs\nyolo train model=rtdetr-l.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Load a COCO-pretrained RT-DETR-l model and run inference on the 'bus.jpg' image\nyolo predict model=rtdetr-l.pt source=path/to/bus.jpg\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with Trained Signature Detection Model using CLI\nDESCRIPTION: Command-line interface command for running inference with a fine-tuned signature detection model. This command specifies the model path, image size, source video, and confidence threshold for detection.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/signature.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Start prediction with a finetuned *.pt model\nyolo detect predict model='path/to/best.pt' imgsz=640 source=\"https://ultralytics.com/assets/signature-s.mp4\" conf=0.75\n```\n\n----------------------------------------\n\nTITLE: Validating YOLO Segmentation Model via CLI\nDESCRIPTION: This command-line instruction shows how to validate a YOLO segmentation model using the CLI. It uses the 'yolo' command with 'segment val' options.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/segment.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nyolo segment val model=yolo11n-seg.pt\n```\n\n----------------------------------------\n\nTITLE: Segmenting with Prompts using SAM in Python\nDESCRIPTION: This code snippet demonstrates how to utilize the SAM model with specific prompts such as bounding boxes and points to segment objects within an image. It highlights using different prompt combinations to achieve desired segmentation results. Prerequisites include the SAM model file 'sam2.1_b.pt' and images for testing.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/sam-2.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import SAM\n\n# Load a model\nmodel = SAM(\"sam2.1_b.pt\")\n\n# Display model information (optional)\nmodel.info()\n\n# Run inference with bboxes prompt\nresults = model(\"path/to/image.jpg\", bboxes=[100, 100, 200, 200])\n\n# Run inference with single point\nresults = model(points=[900, 370], labels=[1])\n\n# Run inference with multiple points\nresults = model(points=[[400, 370], [900, 370]], labels=[1, 1])\n\n# Run inference with multiple points prompt per object\nresults = model(points=[[[400, 370], [900, 370]]], labels=[[1, 1]])\n\n# Run inference with negative points prompt\nresults = model(points=[[[400, 370], [900, 370]]], labels=[[1, 0]])\n```\n\n----------------------------------------\n\nTITLE: Accessing Raw Embeddings in Ultralytics Explorer\nDESCRIPTION: Shows how to access and print raw embeddings from the LanceDB table in Ultralytics Explorer.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/api.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import Explorer\n\nexp = Explorer()\nexp.create_embeddings_table()\ntable = exp.table\n\nembeddings = table.to_pandas()[\"vector\"]\nprint(embeddings)\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO Model for Edge TPU in Python\nDESCRIPTION: Python code to load a YOLO model and export it in a format compatible with Edge TPU using the Ultralytics library.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/coral-edge-tpu-on-raspberry-pi.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"path/to/model.pt\")  # Load an official model or custom model\n\n# Export the model\nmodel.export(format=\"edgetpu\")\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n Model on SKU-110K Dataset in Python\nDESCRIPTION: Python code snippet for loading a pretrained YOLO11n model and training it on the SKU-110K dataset for 100 epochs with an image size of 640.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/sku-110k.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"SKU-110K.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: SAM2MaskDecoder Class Documentation\nDESCRIPTION: Extended version of MaskDecoder specifically optimized for SAM v2. Inherits from base MaskDecoder with additional capabilities for improved mask generation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/models/sam/modules/decoders.md#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nultralytics.models.sam.modules.decoders.SAM2MaskDecoder\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on Carparts Segmentation Dataset in Python\nDESCRIPTION: Python code snippet demonstrating how to train an Ultralytics YOLO11 model on the Carparts Segmentation dataset for 100 epochs with an image size of 640.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/carparts-seg.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained segmentation model like YOLO11n-seg\nmodel = YOLO(\"yolo11n-seg.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model on the Carparts Segmentation dataset\nresults = model.train(data=\"carparts-seg.yaml\", epochs=100, imgsz=640)\n\n# After training, you can validate the model's performance on the validation set\nresults = model.val()\n\n# Or perform prediction on new images or videos\nresults = model.predict(\"path/to/your/image.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Setting up YOLOv8/YOLOv5 C++ Inference Environment\nDESCRIPTION: This bash script outlines the steps to clone the Ultralytics repository, install dependencies, navigate to the C++ example directory, and prepare for model export and project building.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-CPP-Inference/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# 1. Clone the Ultralytics repository\ngit clone https://github.com/ultralytics/ultralytics\ncd ultralytics\n\n# 2. Install Ultralytics Python package (needed for exporting models)\npip install .\n\n# 3. Navigate to the C++ example directory\ncd examples/YOLOv8-CPP-Inference\n\n# 4. Export Models: Add yolov8*.onnx and/or yolov5*.onnx models (see export instructions below)\n#    Place the exported ONNX models in the current directory (YOLOv8-CPP-Inference).\n\n# 5. Update Source Code: Edit main.cpp and set the 'projectBasePath' variable\n#    to the absolute path of the 'YOLOv8-CPP-Inference' directory on your system.\n#    Example: std::string projectBasePath = \"/path/to/your/ultralytics/examples/YOLOv8-CPP-Inference\";\n\n# 6. Configure OpenCV DNN Backend (Optional - CUDA):\n#    - The default CMakeLists.txt attempts to use CUDA for GPU acceleration with OpenCV DNN.\n#    - If your OpenCV build doesn't support CUDA/cuDNN, or you want CPU inference,\n#      remove the CUDA-related lines from CMakeLists.txt.\n\n# 7. Build the project\nmkdir build\ncd build\ncmake ..\nmake\n\n# 8. Run the inference executable\n./Yolov8CPPInference\n```\n\n----------------------------------------\n\nTITLE: Function for Counting Objects in Video Region\nDESCRIPTION: Reusable function implementation for counting objects within a specified region in a video using YOLO11, including video processing and result output.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/object-counting.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\n\ndef count_objects_in_region(video_path, output_video_path, model_path):\n    \"\"\"Count objects in a specific region within a video.\"\"\"\n    cap = cv2.VideoCapture(video_path)\n    assert cap.isOpened(), \"Error reading video file\"\n    w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n    video_writer = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n\n    region_points = [(20, 400), (1080, 400), (1080, 360), (20, 360)]\n    counter = solutions.ObjectCounter(show=True, region=region_points, model=model_path)\n\n    while cap.isOpened():\n        success, im0 = cap.read()\n        if not success:\n            print(\"Video frame is empty or processing is complete.\")\n            break\n        results = counter(im0)\n        video_writer.write(results.plot_im)\n\n    cap.release()\n    video_writer.release()\n    cv2.destroyAllWindows()\n\n\ncount_objects_in_region(\"path/to/video.mp4\", \"output_video.avi\", \"yolo11n.pt\")\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 to TF SavedModel using Python\nDESCRIPTION: This snippet demonstrates how to load a YOLO11 model, export it to TensorFlow SavedModel format, and then use the exported model for inference.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tf-savedmodel.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to TF SavedModel format\nmodel.export(format=\"saved_model\")  # creates '/yolo11n_saved_model'\n\n# Load the exported TF SavedModel for inference\ntf_savedmodel_model = YOLO(\"./yolo11n_saved_model\")\nresults = tf_savedmodel_model(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Real-time Object Tracking with YOLO\nDESCRIPTION: Code examples for using YOLO to perform real-time object tracking on video sources. Includes both Python and CLI examples.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/index.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pre-trained YOLO model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Start tracking objects in a video\n# You can also use live video streams or webcam input\nmodel.track(source=\"path/to/video.mp4\")\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Perform object tracking on a video from the command line\n# You can specify different sources like webcam (0) or RTSP streams\nyolo track source=path/to/video.mp4\n```\n\n----------------------------------------\n\nTITLE: Inference with TCP Stream in Python\nDESCRIPTION: This Python script demonstrates how to load a YOLO11n model and run inference on the TCP stream from the Raspberry Pi camera.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/raspberry-pi.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a YOLO11n PyTorch model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Run inference\nresults = model(\"tcp://127.0.0.1:8888\")\n```\n\n----------------------------------------\n\nTITLE: Creating Similarity Index with Ultralytics Explorer\nDESCRIPTION: Creates a similarity index for a dataset using the Explorer API to find similar images based on embeddings.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/api.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import Explorer\n\nexp = Explorer()\nexp.create_embeddings_table()\n\nsim_idx = exp.similarity_index()\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n Model on COCO128 Dataset using CLI\nDESCRIPTION: Command-line interface example showing how to train a pre-trained YOLO11n model on the COCO128 dataset for 100 epochs with an image size of 640. This command uses the Ultralytics CLI tool.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/coco128.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo detect train data=coco128.yaml model=yolo11n.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Configuring YOLO Training with Data Augmentation in Python\nDESCRIPTION: Example showing how to initialize YOLO model and configure training with custom augmentation parameters using the Python API. Includes both enabled and disabled augmentation examples.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/yolo-data-augmentation.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Training with custom augmentation parameters\nmodel.train(data=\"coco.yaml\", epochs=100, hsv_h=0.03, hsv_s=0.6, hsv_v=0.5)\n\n# Training without any augmentations (disabled values omitted for clarity)\nmodel.train(\n    data=\"coco.yaml\",\n    epochs=100,\n    hsv_h=0.0,\n    hsv_s=0.0,\n    hsv_v=0.0,\n    translate=0.0,\n    scale=0.0,\n    fliplr=0.0,\n    mosaic=0.0,\n    erasing=0.0,\n    auto_augment=None,\n)\n```\n\n----------------------------------------\n\nTITLE: Converting YOLO11 Model to NCNN Format Using Python\nDESCRIPTION: Python code to convert a YOLO11n PyTorch model to NCNN format, which is optimized for Raspberry Pi and embedded devices, and then use the converted model for inference.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/raspberry-pi.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a YOLO11n PyTorch model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to NCNN format\nmodel.export(format=\"ncnn\")  # creates 'yolo11n_ncnn_model'\n\n# Load the exported NCNN model\nncnn_model = YOLO(\"yolo11n_ncnn_model\")\n\n# Run inference\nresults = ncnn_model(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Counting Regions\nDESCRIPTION: Python code example showing how to define custom counting regions using the Shapely library, including polygon shapes and region properties.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-Region-Counter/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom shapely.geometry import Polygon\n\n# Example definition of counting regions\ncounting_regions = [\n    {\n        \"name\": \"Region 1 (Pentagon)\",\n        \"polygon\": Polygon([(50, 80), (250, 20), (450, 80), (400, 350), (100, 350)]),  # 5-point polygon\n        \"counts\": 0,\n        \"dragging\": False,\n        \"region_color\": (255, 42, 4),  # BGR color for region\n        \"text_color\": (255, 255, 255),  # BGR color for text\n    },\n    {\n        \"name\": \"Region 2 (Rectangle)\",\n        \"polygon\": Polygon([(200, 250), (440, 250), (440, 550), (200, 550)]),  # 4-point polygon (rectangle)\n        \"counts\": 0,\n        \"dragging\": False,\n        \"region_color\": (37, 255, 225),  # BGR color for region\n        \"text_color\": (0, 0, 0),  # BGR color for text\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Inspecting Data Collection Settings in Ultralytics YOLO using Python\nDESCRIPTION: This code snippet illustrates how to view all settings and specifically check the analytics and crash reporting setting in Ultralytics YOLO using Python.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/help/privacy.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import settings\n\n# View all settings\nprint(settings)\n\n# Return analytics and crash reporting setting\nvalue = settings[\"sync\"]\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO Model for Edge TPU via CLI\nDESCRIPTION: Command-line instruction to export a YOLO model for Edge TPU using the Ultralytics CLI tool.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/coral-edge-tpu-on-raspberry-pi.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=path/to/model.pt format=edgetpu # Export an official model or custom model\n```\n\n----------------------------------------\n\nTITLE: Running GUI Applications with X11\nDESCRIPTION: Command to run Ultralytics Docker container with X11 display server access for GUI applications.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/docker-quickstart.md#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nxhost +local:docker && docker run -e DISPLAY=$DISPLAY \\\n  -v /tmp/.X11-unix:/tmp/.X11-unix \\\n  -v ~/.Xauthority:/root/.Xauthority \\\n  -it --ipc=host ultralytics/ultralytics:latest\n```\n\n----------------------------------------\n\nTITLE: Returning Additional Information with Prediction in Python\nDESCRIPTION: This snippet demonstrates how to use a custom callback to return the original frame along with each prediction result object. It defines an 'on_predict_batch_end' function that combines prediction results with corresponding frames.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/callbacks.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n\ndef on_predict_batch_end(predictor):\n    \"\"\"Combine prediction results with corresponding frames.\"\"\"\n    _, image, _, _ = predictor.batch\n\n    # Ensure that image is a list\n    image = image if isinstance(image, list) else [image]\n\n    # Combine the prediction results with the corresponding frames\n    predictor.results = zip(predictor.results, image)\n\n\n# Create a YOLO model instance\nmodel = YOLO(\"yolo11n.pt\")\n\n# Add the custom callback to the model\nmodel.add_callback(\"on_predict_batch_end\", on_predict_batch_end)\n\n# Iterate through the results and frames\nfor result, frame in model.predict():  # or model.track()\n    pass\n```\n\n----------------------------------------\n\nTITLE: Using Keypoints in Ultralytics YOLO for Pose Estimation\nDESCRIPTION: This snippet demonstrates how to load a YOLO11n-pose model, run inference on an image, and access the detected keypoints using the Keypoints object.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/predict.md#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n-pose Pose model\nmodel = YOLO(\"yolo11n-pose.pt\")\n\n# Run inference on an image\nresults = model(\"https://ultralytics.com/images/bus.jpg\")  # results list\n\n# View results\nfor r in results:\n    print(r.keypoints)  # print the Keypoints object containing the detected keypoints\n```\n\n----------------------------------------\n\nTITLE: Training and Inference with YOLOv3u in Python\nDESCRIPTION: This snippet demonstrates how to use the Ultralytics YOLO package to load a pre-trained YOLOv3u model, train it on a dataset, and run inference on an image using Python. Dependencies include the Ultralytics package and a suitable hardware setup for training. It accepts file paths for datasets and images as input, and outputs model training results and inference predictions.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov3.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a COCO-pretrained YOLOv3u model\nmodel = YOLO(\"yolov3u.pt\")\n\n# Display model information (optional)\nmodel.info()\n\n# Train the model on the COCO8 example dataset for 100 epochs\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n\n# Run inference with the YOLOv3u model on the 'bus.jpg' image\nresults = model(\"path/to/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Conditionally Saving YOLO Predictions with a Python Callback\nDESCRIPTION: This Python snippet demonstrates how to create and attach a custom callback function (`save_on_object`) to an Ultralytics YOLO model. The callback is triggered after postprocessing (`on_predict_postprocess_end`) and checks if a specific `class_id` (set to 2 in this example) is present in the detected bounding box classes (`r.boxes.cls`). If the class is found, it sets the predictor's save argument (`predictor.args.save`) to `True`, enabling saving for that specific prediction; otherwise, it sets it to `False`. This allows for selective saving of prediction results based on content.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/callbacks.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolo11n.pt\")\n\nclass_id = 2\n\n\ndef save_on_object(predictor):\n    r = predictor.results[0]\n    if class_id in r.boxes.cls:\n        predictor.args.save = True\n    else:\n        predictor.args.save = False\n\n\nmodel.add_callback(\"on_predict_postprocess_end\", save_on_object)\nresults = model(\"pedestrians.mp4\", stream=True, save=True)\n\nfor results in results:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Training a YOLOv11 Model using CLI\nDESCRIPTION: Shows the command-line interface (CLI) command to train a YOLOv11 object detection model. This command utilizes the `yolo` executable, specifying the 'train' mode, the pre-trained model file ('yolo11n.pt'), the dataset configuration file ('coco8.yaml'), the number of training epochs (100), and the input image size (640). Requires the Ultralytics package to be installed with CLI tools available.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolo11.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Load a COCO-pretrained YOLO11n model and train it on the COCO8 example dataset for 100 epochs\nyolo train model=yolo11n.pt data=coco8.yaml epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Model for Marine Litter Detection in CLI\nDESCRIPTION: This command-line instruction fine-tunes a pretrained YOLO11 model on the marine litter dataset. It specifies the task, mode, data configuration, model architecture, and training parameters.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/ibm-watsonx.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n!yolo task=detect mode=train data={work_dir}/trash_ICRA19/config.yaml model=yolo11n.pt epochs=2 batch=32 lr0=.04 plots=True\n```\n\n----------------------------------------\n\nTITLE: Instantiating YOLO11 Model for Object Detection with SAHI\nDESCRIPTION: Python code to instantiate a YOLO11 model for object detection using SAHI's AutoDetectionModel.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/sahi-tiled-inference.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sahi import AutoDetectionModel\n\ndetection_model = AutoDetectionModel.from_pretrained(\n    model_type=\"ultralytics\",\n    model_path=model_path,\n    confidence_threshold=0.3,\n    device=\"cpu\",  # or 'cuda:0'\n)\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO Model to MNN Format - Python\nDESCRIPTION: The snippet shows how to use the Ultralytics YOLO Python library to export a YOLO11 model to MNN format with different precision weights. The dependencies required are the Ultralytics YOLO library. Parameters include the model path and export options like 'half' and 'int8'.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/mnn.md#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export to MNN format\nmodel.export(format=\"mnn\")  # creates 'yolo11n.mnn' with fp32 weight\nmodel.export(format=\"mnn\", half=True)  # creates 'yolo11n.mnn' with fp16 weight\nmodel.export(format=\"mnn\", int8=True)  # creates 'yolo11n.mnn' with int8 weight\n```\n\n----------------------------------------\n\nTITLE: Auto Annotation with YOLO and SAM\nDESCRIPTION: Function to automatically annotate datasets using YOLO detection model and SAM for segmentation format. Requires paths to data, models and output directory.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/simple-utilities.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.data.annotator import auto_annotate\n\nauto_annotate(\n    data=\"path/to/new/data\",\n    det_model=\"yolo11n.pt\",\n    sam_model=\"mobile_sam.pt\",\n    device=\"cuda\",\n    output_dir=\"path/to/save_labels\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Binary Mask and Drawing Contours\nDESCRIPTION: Generates a binary mask from the source image and draws filled contours for object isolation using OpenCV.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/isolating-segmentation-objects.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\n# Create binary mask\nb_mask = np.zeros(img.shape[:2], np.uint8)\n\n# (1) Extract contour result\ncontour = c.masks.xy.pop()\n# (2) Changing the type\ncontour = contour.astype(np.int32)\n# (3) Reshaping\ncontour = contour.reshape(-1, 1, 2)\n\n\n# Draw contour onto mask\n_ = cv2.drawContours(b_mask, [contour], -1, (255, 255, 255), cv2.FILLED)\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11-OBB Model to ONNX with CLI\nDESCRIPTION: The command line snippet provides directions for exporting a YOLO11-OBB model to the ONNX format using CLI. This approach facilitates easy conversion of trained models into ONNX, enhancing model deployment capabilities.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/obb.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=yolo11n-obb.pt format=onnx\n```\n\n----------------------------------------\n\nTITLE: Handling Prediction Results from YOLO11 and SAHI\nDESCRIPTION: Python code to access and convert prediction results from YOLO11 and SAHI into various annotation formats.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/sahi-tiled-inference.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Access the object prediction list\nobject_prediction_list = result.object_prediction_list\n\n# Convert to COCO annotation, COCO prediction, imantics, and fiftyone formats\nresult.to_coco_annotations()[:3]\nresult.to_coco_predictions(image_id=1)[:3]\nresult.to_imantics_annotations()[:3]\nresult.to_fiftyone_detections()[:3]\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO Segmentation Model to ONNX via CLI\nDESCRIPTION: This command-line instruction shows how to export a YOLO segmentation model to ONNX format using the CLI. It uses the 'yolo export' command with the format specified as 'onnx'.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/segment.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=yolo11n-seg.pt format=onnx\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on Caltech-101 Dataset using CLI\nDESCRIPTION: This command line interface (CLI) command shows how to train a YOLO image classification model on the Caltech-101 dataset. It starts training from a pretrained model (yolo11n-cls.pt) for 100 epochs with an image size of 416 pixels.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/caltech101.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo classify train data=caltech101 model=yolo11n-cls.pt epochs=100 imgsz=416\n```\n\n----------------------------------------\n\nTITLE: Accessing YOLO11 Evaluation Metrics in Python\nDESCRIPTION: Python code demonstrating how to load a YOLO11 model, run evaluation, and access various performance metrics including average precision, F1 scores, and mean average precision values. Uses the ultralytics library to perform model validation and retrieve detailed performance statistics.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/model-evaluation-insights.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Run the evaluation\nresults = model.val(data=\"coco8.yaml\")\n\n# Print specific metrics\nprint(\"Class indices with average precision:\", results.ap_class_index)\nprint(\"Average precision for all classes:\", results.box.all_ap)\nprint(\"Average precision:\", results.box.ap)\nprint(\"Average precision at IoU=0.50:\", results.box.ap50)\nprint(\"Class indices for average precision:\", results.box.ap_class_index)\nprint(\"Class-specific results:\", results.box.class_result)\nprint(\"F1 score:\", results.box.f1)\nprint(\"F1 score curve:\", results.box.f1_curve)\nprint(\"Overall fitness score:\", results.box.fitness)\nprint(\"Mean average precision:\", results.box.map)\nprint(\"Mean average precision at IoU=0.50:\", results.box.map50)\nprint(\"Mean average precision at IoU=0.75:\", results.box.map75)\nprint(\"Mean average precision for different IoU thresholds:\", results.box.maps)\nprint(\"Mean results for different metrics:\", results.box.mean_results)\nprint(\"Mean precision:\", results.box.mp)\nprint(\"Mean recall:\", results.box.mr)\nprint(\"Precision:\", results.box.p)\nprint(\"Precision curve:\", results.box.p_curve)\nprint(\"Precision values:\", results.box.prec_values)\nprint(\"Specific precision metrics:\", results.box.px)\nprint(\"Recall:\", results.box.r)\nprint(\"Recall curve:\", results.box.r_curve)\n```\n\n----------------------------------------\n\nTITLE: YOLO OBB Models Performance Table\nDESCRIPTION: Markdown table displaying performance metrics for YOLO oriented bounding box (OBB) models including mAP scores, inference speeds, parameters and FLOPs.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/README.md#2025-04-22_snippet_7\n\nLANGUAGE: markdown\nCODE:\n```\n| Model                                                                                        | size<br><sup>(pixels) | mAP<sup>test<br>50 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n| -------------------------------------------------------------------------------------------- | --------------------- | ------------------ | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\n| [YOLO11n-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-obb.pt) | 1024                  | 78.4               | 117.6 ± 0.8                    | 4.4 ± 0.0                           | 2.7                | 17.2              |\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on ImageNette160 Dataset using CLI\nDESCRIPTION: This CLI command shows how to train a YOLO classification model on the ImageNette160 dataset, which contains smaller 160x160 images for faster training and reduced computational requirements.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/imagenette.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model with ImageNette160\nyolo classify train data=imagenette160 model=yolo11n-cls.pt epochs=100 imgsz=160\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Models on CIFAR-100 Dataset Using Python\nDESCRIPTION: Python code snippet showing how to load a pre-trained YOLO model and train it on the CIFAR-100 dataset for 100 epochs with 32x32 image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/cifar100.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-cls.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"cifar100\", epochs=100, imgsz=32)\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLOv8 Segmentation Model to ONNX (Bash)\nDESCRIPTION: Exports a pre-trained YOLOv8 segmentation model (e.g., 'yolov8s-seg.pt') to the ONNX format using the 'yolo' command-line interface provided by the 'ultralytics' package. The command specifies the model, input image size ('imgsz'), target format ('onnx'), ONNX opset version, and enables simplification. Requires the 'ultralytics' package to be installed and the specified '.pt' model file to be available.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-Segmentation-ONNXRuntime-Python/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=yolov8s-seg.pt imgsz=640 format=onnx opset=12 simplify\n```\n\n----------------------------------------\n\nTITLE: Configuration of VisDrone Dataset in YAML format\nDESCRIPTION: YAML configuration file for the VisDrone dataset, defining paths, class names, and dataset structure for use with Ultralytics YOLO models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/visdrone.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/VisDrone.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Segmenting and Visualizing 3D Point Clouds with YOLO and Open3D\nDESCRIPTION: This snippet processes a PointCloud2 message using YOLO segmentation, applies the segmentation mask to isolate objects, and visualizes the results using Open3D. It demonstrates the full pipeline from ROS data to 3D visualization.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/ros-quickstart.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport sys\n\nimport open3d as o3d\n\nros_cloud = rospy.wait_for_message(\"/camera/depth/points\", PointCloud2)\nxyz, rgb = pointcloud2_to_array(ros_cloud)\nresult = segmentation_model(rgb)\n\nif not len(result[0].boxes.cls):\n    print(\"No objects detected\")\n    sys.exit()\n\nclasses = result[0].boxes.cls.cpu().numpy().astype(int)\nfor index, class_id in enumerate(classes):\n    mask = result[0].masks.data.cpu().numpy()[index, :, :].astype(int)\n    mask_expanded = np.stack([mask, mask, mask], axis=2)\n\n    obj_rgb = rgb * mask_expanded\n    obj_xyz = xyz * mask_expanded\n\n    pcd = o3d.geometry.PointCloud()\n    pcd.points = o3d.utility.Vector3dVector(obj_xyz.reshape((ros_cloud.height * ros_cloud.width, 3)))\n    pcd.colors = o3d.utility.Vector3dVector(obj_rgb.reshape((ros_cloud.height * ros_cloud.width, 3)) / 255)\n    o3d.visualization.draw_geometries([pcd])\n```\n\n----------------------------------------\n\nTITLE: Complete Object Isolation and Cropping Implementation with Ultralytics YOLO\nDESCRIPTION: A comprehensive implementation that loads a YOLO model, performs segmentation, creates masks for each detected object, isolates objects with either black or transparent backgrounds, and crops them to their bounding boxes. Includes optional processing steps for further customization.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/isolating-segmentation-objects.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\n\nimport cv2\nimport numpy as np\n\nfrom ultralytics import YOLO\n\nm = YOLO(\"yolo11n-seg.pt\")  # (4)!\nres = m.predict()  # (3)!\n\n# Iterate detection results (5)\nfor r in res:\n    img = np.copy(r.orig_img)\n    img_name = Path(r.path).stem\n\n    # Iterate each object contour (6)\n    for ci, c in enumerate(r):\n        label = c.names[c.boxes.cls.tolist().pop()]\n\n        b_mask = np.zeros(img.shape[:2], np.uint8)\n\n        # Create contour mask (1)\n        contour = c.masks.xy.pop().astype(np.int32).reshape(-1, 1, 2)\n        _ = cv2.drawContours(b_mask, [contour], -1, (255, 255, 255), cv2.FILLED)\n\n        # Choose one:\n\n        # OPTION-1: Isolate object with black background\n        mask3ch = cv2.cvtColor(b_mask, cv2.COLOR_GRAY2BGR)\n        isolated = cv2.bitwise_and(mask3ch, img)\n\n        # OPTION-2: Isolate object with transparent background (when saved as PNG)\n        isolated = np.dstack([img, b_mask])\n\n        # OPTIONAL: detection crop (from either OPT1 or OPT2)\n        x1, y1, x2, y2 = c.boxes.xyxy.cpu().numpy().squeeze().astype(np.int32)\n        iso_crop = isolated[y1:y2, x1:x2]\n\n        # TODO your actions go here (2)\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Model with Integrated Albumentations\nDESCRIPTION: Python code to load a pre-trained YOLO11 model and train it using the COCO8 dataset, with Albumentations augmentations applied automatically.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/albumentations.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pre-trained model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Train the model\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Using SAM 2 with Prompts in Python\nDESCRIPTION: This code snippet demonstrates how to load and use the SAM 2 model for segmentation with different types of prompts. It shows initialization of the model, displaying model information, and performing segmentation using both bounding box and point prompts.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/sam-2.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import SAM\n\n# Load a model\nmodel = SAM(\"sam2_b.pt\")\n\n# Display model information (optional)\nmodel.info()\n\n# Segment with bounding box prompt\nresults = model(\"path/to/image.jpg\", bboxes=[100, 100, 200, 200])\n\n# Segment with point prompt\nresults = model(\"path/to/image.jpg\", points=[150, 150], labels=[1])\n```\n\n----------------------------------------\n\nTITLE: Simple VisionEye Object Mapping Example in Python\nDESCRIPTION: A concise Python script demonstrating basic usage of VisionEye for object mapping and tracking with video processing.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/vision-eye.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nassert cap.isOpened(), \"Error reading video file\"\n\n# Video writer\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\nvideo_writer = cv2.VideoWriter(\"vision-eye-mapping.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n\n# Init vision eye object\nvisioneye = solutions.VisionEye(\n    show=True,  # display the output\n    model=\"yolo11n.pt\",  # use any model that Ultralytics support, i.e, YOLOv10\n    classes=[0, 2],  # generate visioneye view for specific classes\n)\n\n# Process video\nwhile cap.isOpened():\n    success, im0 = cap.read()\n\n    if not success:\n        print(\"Video frame is empty or video processing has been successfully completed.\")\n        break\n\n    results = visioneye(im0)\n\n    print(results)  # access the output\n\n    video_writer.write(results.plot_im)  # write the video file\n\ncap.release()\nvideo_writer.release()\ncv2.destroyAllWindows()  # destroy all opened windows\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Model on Package Segmentation Dataset in Python\nDESCRIPTION: Python code snippet demonstrating how to load a pretrained YOLO11 segmentation model, train it on the Package Segmentation dataset, validate the model, and perform inference on an image.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/package-seg.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-seg.pt\")  # load a pretrained segmentation model (recommended for training)\n\n# Train the model on the Package Segmentation dataset\nresults = model.train(data=\"package-seg.yaml\", epochs=100, imgsz=640)\n\n# Validate the model\nresults = model.val()\n\n# Perform inference on an image\nresults = model(\"path/to/image.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Benchmarking YOLO11 Performance on DeepSparse Engine\nDESCRIPTION: Command to benchmark YOLO11 performance using DeepSparse. It specifies the model path, scenario, and input shapes to analyze throughput and latency.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/neural-magic.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ndeepsparse.benchmark model_path=\"path/to/yolo11n.onnx\" --scenario=sync --input_shapes=\"[1,3,640,640]\"\n```\n\n----------------------------------------\n\nTITLE: Running YOLO Inference with Edge TPU in Python\nDESCRIPTION: Python code to load a YOLO model exported for Edge TPU and run inference on an image using the Ultralytics library.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/coral-edge-tpu-on-raspberry-pi.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"path/to/<model_name>_full_integer_quant_edgetpu.tflite\")  # Load an official model or custom model\n\n# Run Prediction\nmodel.predict(\"path/to/source.png\")\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 OBB Models with CLI\nDESCRIPTION: Train YOLO11 OBB models using command line interface. Shows commands for training from scratch, using pretrained models, and transferring weights.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/obb.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Build a new model from YAML and start training from scratch\nyolo obb train data=dota8.yaml model=yolo11n-obb.yaml epochs=100 imgsz=640\n\n# Start training from a pretrained *.pt model\nyolo obb train data=dota8.yaml model=yolo11n-obb.pt epochs=100 imgsz=640\n\n# Build a new model from YAML, transfer pretrained weights to it and start training\nyolo obb train data=dota8.yaml model=yolo11n-obb.yaml pretrained=yolo11n-obb.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Customizing YOLO11 Validation Parameters via Python API (Python)\nDESCRIPTION: Illustrates advanced YOLO11 validation in Python with custom arguments for dataset path, image size, batch size, confidence, IOU threshold, and device selection. This enables flexible benchmarking against various conditions; the returned 'validation_results' provides standard performance metrics. Requires 'ultralytics', a valid YOLO11 model, and YAML-formatted dataset config. Inputs include: 'data' for dataset config file, 'imgsz' for input resizing, 'batch', 'conf', 'iou', and 'device' for hardware targeting.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/val.md#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Customize validation settings\nvalidation_results = model.val(data=\"coco8.yaml\", imgsz=640, batch=16, conf=0.25, iou=0.6, device=\"0\")\n\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Dataset for YOLO Classification\nDESCRIPTION: Illustrates how to use a custom dataset for training a YOLO classification model. Loads a pretrained model and specifies the path to the custom dataset for training.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/index.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-cls.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"path/to/your/dataset\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorBoard in Google Colab\nDESCRIPTION: Commands to initialize and configure TensorBoard in Google Colab environment for visualization.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tensorboard.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n%load_ext tensorboard\n%tensorboard --logdir path/to/runs\n```\n\n----------------------------------------\n\nTITLE: Defining Argoverse Dataset Configuration in YAML\nDESCRIPTION: YAML configuration file for the Argoverse dataset, specifying paths, classes, and other relevant information for use with YOLO models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/argoverse.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/Argoverse.yaml\"\n```\n\n----------------------------------------\n\nTITLE: YOLOv5 inference with TTA\nDESCRIPTION: Run object detection inference using YOLOv5s with Test-Time Augmentation enabled.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/test_time_augmentation.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython detect.py --weights yolov5s.pt --img 832 --source data/images --augment\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n-pose Model on COCO8-Pose Dataset using CLI\nDESCRIPTION: Command-line interface command for training a YOLO11n-pose model on the COCO8-Pose dataset for 100 epochs with 640px image size, using a pretrained model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/coco8-pose.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo pose train data=coco8-pose.yaml model=yolo11n-pose.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Validating a YOLOv8 TensorRT Model - Python\nDESCRIPTION: This snippet details how to validate a YOLOv8 TensorRT engine using the Ultralytics Python API for a specific dataset. It loads a pretrained TensorRT model, specifies the validation dataset and configurations such as batch size and image size, and runs the evaluation on a CUDA device. Required dependencies include 'ultralytics' and the availability of a compatible .engine file. The main parameters are the 'data' config file (defines the dataset), batch, image size, and device. The output is a results object containing metrics such as mAP and inference speed.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tensorrt.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.engine\")\nresults = model.val(\n    data=\"data.yaml\",  # COCO, ImageNet, or DOTAv1 for appropriate model task\n    batch=1,\n    imgsz=640,\n    verbose=False,\n    device=\"cuda\",\n)\n```\n\n----------------------------------------\n\nTITLE: Visual Prompting with YOLOE in Python\nDESCRIPTION: This code snippet illustrates how to use visual prompts with a YOLOE model. It demonstrates defining bounding boxes as visual examples for class detection, associating them with class IDs, and running inference using these prompts. It handles both single images and multiple images, and allows the use of reference images. This method requires YOLOE from the Ultralytics library and numpy for array handling.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yoloe.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nfrom ultralytics import YOLOE\nfrom ultralytics.models.yolo.yoloe import YOLOEVPSegPredictor\n\n# Initialize a YOLOE model\nmodel = YOLOE(\"yoloe-11l-seg.pt\")\n\n# Define visual prompts using bounding boxes and their corresponding class IDs.\n# Each box highlights an example of the object you want the model to detect.\nvisual_prompts = dict(\n    bboxes=np.array(\n        [\n            [221.52, 405.8, 344.98, 857.54],  # Box enclosing person\n            [120, 425, 160, 445],  # Box enclosing glasses\n        ],\n    ),\n    cls=np.array(\n        [\n            0,  # ID to be assigned for person\n            1,  # ID to be assigned for glasses\n        ]\n    ),\n)\n\n# Run inference on an image, using the provided visual prompts as guidance\nresults = model.predict(\n    \"ultralytics/assets/bus.jpg\",\n    visual_prompts=visual_prompts,\n    predictor=YOLOEVPSegPredictor,\n)\n\n# Show results\nresults[0].show()\n```\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nfrom ultralytics import YOLOE\nfrom ultralytics.models.yolo.yoloe import YOLOEVPSegPredictor\n\n# Initialize a YOLOE model\nmodel = YOLOE(\"yoloe-11l-seg.pt\")\n\n# Define visual prompts based on a separate reference image\nvisual_prompts = dict(\n    bboxes=np.array([[221.52, 405.8, 344.98, 857.54]]),  # Box enclosing person\n    cls=np.array([0]),  # ID to be assigned for person\n)\n\n# Run prediction on a different image, using reference image to guide what to look for\nresults = model.predict(\n    \"ultralytics/assets/zidane.jpg\",  # Target image for detection\n    refer_image=\"ultralytics/assets/bus.jpg\",  # Reference image used to get visual prompts\n    visual_prompts=visual_prompts,\n    predictor=YOLOEVPSegPredictor,\n)\n\n# Show results\nresults[0].show()\n```\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nfrom ultralytics import YOLOE\nfrom ultralytics.models.yolo.yoloe import YOLOEVPSegPredictor\n\n# Initialize a YOLOE model\nmodel = YOLOE(\"yoloe-11l-seg.pt\")\n\n# Define visual prompts using bounding boxes and their corresponding class IDs.\n# Each box highlights an example of the object you want the model to detect.\nvisual_prompts = dict(\n    bboxes=[\n        np.array(\n            [\n                [221.52, 405.8, 344.98, 857.54],  # Box enclosing person\n                [120, 425, 160, 445],  # Box enclosing glasses\n            ],\n        ),\n        np.array([[150, 200, 1150, 700]]),\n    ],\n    cls=[\n        np.array(\n            [\n                0,  # ID to be assigned for person\n                1,  # ID to be assigned for glasses\n            ]\n        ),\n        np.array([0]),\n    ],\n)\n\n# Run inference on multiple image, using the provided visual prompts as guidance\nresults = model.predict(\n    [\"ultralytics/assets/bus.jpg\", \"ultralytics/assets/zidane.jpg\"],\n    visual_prompts=visual_prompts,\n    predictor=YOLOEVPSegPredictor,\n)\n\n# Show results\nresults[0].show()\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n-OBB Model with Python\nDESCRIPTION: This code snippet demonstrates how to train a YOLO11n-OBB model using a custom dataset in Python. The example employs the ultralytics library, where a pretrained model is loaded and then trained with specified parameters such as dataset path, number of epochs, and image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/obb.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained model\nmodel = YOLO(\"yolo11n-obb.pt\")\n\n# Train the model\nresults = model.train(data=\"path/to/custom_dataset.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Validating YOLO-NAS Model with Ultralytics in Python\nDESCRIPTION: This Python code snippet demonstrates how to use the Ultralytics package to load and validate a YOLO-NAS model on a dataset. It requires the `ultralytics` package and a COCO-pretrained YOLO-NAS model (`yolo_nas_s.pt`). The example includes loading the model, displaying its info, validating it with a dataset, and running inference on an image.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolo-nas.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics import NAS\n\n# Load a COCO-pretrained YOLO-NAS-s model\nmodel = NAS(\"yolo_nas_s.pt\")\n\n# Display model information (optional)\nmodel.info()\n\n# Validate the model on the COCO8 example dataset\nresults = model.val(data=\"coco8.yaml\")\n\n# Run inference with the YOLO-NAS-s model on the 'bus.jpg' image\nresults = model(\"path/to/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on Caltech-256 using Python\nDESCRIPTION: Python code to load a pretrained YOLO model and train it on the Caltech-256 dataset for 100 epochs with 416px image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/caltech256.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-cls.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"caltech256\", epochs=100, imgsz=416)\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n Model on COCO8-Multispectral Dataset in Python\nDESCRIPTION: Python code snippet demonstrating how to load a pretrained YOLO11n model and train it on the COCO8-Multispectral dataset for 100 epochs with an image size of 640.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/coco8-multispectral.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Train the model on COCO8-Multispectral\nresults = model.train(data=\"coco8-multispectral.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Auto-Annotating Datasets using SAM 2 in Python\nDESCRIPTION: This snippet demonstrates how to quickly create segmentation datasets using the 'auto_annotate' function. It leverages pre-trained models to automatically annotate images. Necessary prerequisites include access to specified model files and the 'ultralytics.data.annotator' module.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/sam-2.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.data.annotator import auto_annotate\n\nauto_annotate(data=\"path/to/images\", det_model=\"yolo11x.pt\", sam_model=\"sam2_b.pt\")\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Segmentation Model in Python\nDESCRIPTION: Python code snippet demonstrating how to load a pre-trained YOLO segmentation model and train it on a custom dataset.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/index.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-seg.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"coco8-seg.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Profiling SAM, FastSAM, and YOLO Segmentation Models in Python\nDESCRIPTION: This code snippet demonstrates how to load and profile various SAM, FastSAM, and YOLO segmentation models using the Ultralytics library. It includes model loading, information display, and inference on sample assets.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/fast-sam.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import ASSETS, SAM, YOLO, FastSAM\n\n# Profile SAM2-t, SAM2-b, SAM-b, MobileSAM\nfor file in [\"sam_b.pt\", \"sam2_b.pt\", \"sam2_t.pt\", \"mobile_sam.pt\"]:\n    model = SAM(file)\n    model.info()\n    model(ASSETS)\n\n# Profile FastSAM-s\nmodel = FastSAM(\"FastSAM-s.pt\")\nmodel.info()\nmodel(ASSETS)\n\n# Profile YOLO models\nfor file_name in [\"yolov8n-seg.pt\", \"yolo11n-seg.pt\"]:\n    model = YOLO(file_name)\n    model.info()\n    model(ASSETS)\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n-seg Model on COCO8-Seg via CLI\nDESCRIPTION: Command-line interface command for training a pretrained YOLO11n-seg model on the COCO8-Seg dataset with specified parameters.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/coco8-seg.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo segment train data=coco8-seg.yaml model=yolo11n-seg.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on ImageNet with Python\nDESCRIPTION: Example showing how to load a pretrained YOLO model and train it on the ImageNet dataset for 100 epochs with 224x224 image size using Python.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/imagenet.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-cls.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"imagenet\", epochs=100, imgsz=224)\n```\n\n----------------------------------------\n\nTITLE: Reorganizing Dataset Directory Structure for YOLO in Python\nDESCRIPTION: This function reorganizes the dataset directory to match YOLO's required structure. It creates separate subdirectories for images and labels within train, test, and val folders, and moves files accordingly.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/ibm-watsonx.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Function to reorganize dir\ndef organize_files(directory):\n    for subdir in [\"train\", \"test\", \"val\"]:\n        subdir_path = os.path.join(directory, subdir)\n        if not os.path.exists(subdir_path):\n            continue\n\n        images_dir = os.path.join(subdir_path, \"images\")\n        labels_dir = os.path.join(subdir_path, \"labels\")\n\n        # Create image and label subdirs if non-existent\n        os.makedirs(images_dir, exist_ok=True)\n        os.makedirs(labels_dir, exist_ok=True)\n\n        # Move images and labels to respective subdirs\n        for filename in os.listdir(subdir_path):\n            if filename.endswith(\".txt\"):\n                shutil.move(os.path.join(subdir_path, filename), os.path.join(labels_dir, filename))\n            elif filename.endswith(\".jpg\") or filename.endswith(\".png\") or filename.endswith(\".jpeg\"):\n                shutil.move(os.path.join(subdir_path, filename), os.path.join(images_dir, filename))\n            # Delete .xml files\n            elif filename.endswith(\".xml\"):\n                os.remove(os.path.join(subdir_path, filename))\n\n\nif __name__ == \"__main__\":\n    directory = f\"{work_dir}/trash_ICRA19/dataset\"\n    organize_files(directory)\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on ImageNet10 Dataset via CLI\nDESCRIPTION: This command-line interface (CLI) command shows how to start training a pre-trained YOLO model on the ImageNet10 dataset. It specifies the model, dataset, number of epochs, and image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/imagenet10.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo classify train data=imagenet10 model=yolo11n-cls.pt epochs=5 imgsz=224\n```\n\n----------------------------------------\n\nTITLE: Initializing Comet ML Project\nDESCRIPTION: Python code to initialize a Comet ML project and login with the configured API key\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/comet.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport comet_ml\n\ncomet_ml.login(project_name=\"comet-example-yolo11-coco128\")\n```\n\n----------------------------------------\n\nTITLE: Training YOLOv6n with Ultralytics (CLI)\nDESCRIPTION: This shell command launches training of a YOLOv6n model with Ultralytics' CLI, specifying the YAML model and data files, number of training epochs, and image size. CLI must be installed, and data/model files must exist at the given paths. Output includes training logs and saved checkpoints.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov6.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyolo train model=yolov6n.yaml data=coco8.yaml epochs=100 imgsz=640\n\n```\n\n----------------------------------------\n\nTITLE: Benchmarking YOLO Models on GPU via CLI (FAQ Example)\nDESCRIPTION: This Bash command, provided in the FAQ section, demonstrates using the Ultralytics CLI to run a benchmark for the `yolo11n.pt` model using the `coco8.yaml` dataset configuration. It sets the image size to 640, explicitly disables half-precision inference (`half=False`), and designates the first GPU (`device=0`) for the benchmark execution.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/benchmark.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyolo benchmark model=yolo11n.pt data='coco8.yaml' imgsz=640 half=False device=0\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model to ONNX Format using Python\nDESCRIPTION: Python script to load a YOLO11 model and export it to ONNX format with opset 14. This is required for converting the model to work with reCamera.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/seeedstudio-recamera.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to ONNX format\nmodel.export(format=\"onnx\", opset=14)  # creates 'yolo11n.onnx'\n```\n\n----------------------------------------\n\nTITLE: Running TensorBoard Locally for YOLO11 Training Visualization\nDESCRIPTION: Launches TensorBoard locally to monitor and visualize YOLO11 model training metrics. This command starts TensorBoard and directs it to the training logs directory, making the dashboard accessible at http://localhost:6006/.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/train.md#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ntensorboard --logdir ultralytics/runs # replace with 'runs' directory\n```\n\n----------------------------------------\n\nTITLE: Accessing YOLO11 Model Evaluation Metrics in Python\nDESCRIPTION: Demonstrates how to load a YOLO11 model, run evaluation on a dataset, and access various evaluation metrics including average precision, mean average precision, and mean recall. Uses the ultralytics package to interface with YOLO11 models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/model-evaluation-insights.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Run the evaluation\nresults = model.val(data=\"coco8.yaml\")\n\n# Print specific metrics\nprint(\"Class indices with average precision:\", results.ap_class_index)\nprint(\"Average precision for all classes:\", results.box.all_ap)\nprint(\"Mean average precision at IoU=0.50:\", results.box.map50)\nprint(\"Mean recall:\", results.box.mr)\n```\n\n----------------------------------------\n\nTITLE: Converting YOLO Bounding Boxes to Segments with SAM\nDESCRIPTION: This snippet demonstrates how to convert existing YOLO bounding box annotations to segmentation format using the Segment Anything Model (SAM). This function processes the labels and saves the segmentation annotations in a separate directory.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/simple-utilities.md#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.data.converter import yolo_bbox2segment\n\nyolo_bbox2segment(\n    im_dir=\"path/to/images\",\n    save_dir=None,  # saved to \"labels-segment\" in the images directory\n    sam_model=\"sam_b.pt\",\n)\n```\n\n----------------------------------------\n\nTITLE: Loading and Running Inference with YOLO Pose Model in Python\nDESCRIPTION: This code snippet demonstrates how to load a YOLO pose model and run inference on an image using the Ultralytics library. It loads the model, performs inference, and prints the results in JSON format.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/hub/inference-api.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load model\nmodel = YOLO(\"yolov8n-pose.pt\")\n\n# Run inference\nresults = model(\"image.jpg\")\n\n# Print image.jpg results in JSON format\nprint(results[0].tojson())\n```\n\n----------------------------------------\n\nTITLE: Training Custom Object Detection Model with Ultralytics YOLO in Python\nDESCRIPTION: This code snippet demonstrates how to train a custom object detection model using Ultralytics YOLO. It loads a pre-trained YOLO model and initiates training on a custom dataset for 50 epochs.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/index.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolo11n.pt\")  # Load a pre-trained YOLO model\nmodel.train(data=\"path/to/dataset.yaml\", epochs=50)  # Train on custom dataset\n```\n\n----------------------------------------\n\nTITLE: Defining Canvas for Model Comparison Chart in HTML\nDESCRIPTION: This HTML tag defines a canvas element used as a drawing surface for the model comparison chart. It is identified by `modelComparisonChart`, has specified dimensions (1024x400 pixels), and includes a custom `active-models` attribute, presumably used by the associated JavaScript (`benchmark.js` and Chart.js) to configure which models (like YOLOv7) are initially displayed or highlighted in the chart.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov7.md#2025-04-22_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<canvas id=\"modelComparisonChart\" width=\"1024\" height=\"400\" active-models='[\"YOLOv7\"]'></canvas>\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Model via CLI\nDESCRIPTION: Shows how to train a YOLO11 model using the command-line interface with various options for model initialization and training.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/detect.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Build a new model from YAML and start training from scratch\nyolo detect train data=coco8.yaml model=yolo11n.yaml epochs=100 imgsz=640\n\n# Start training from a pretrained *.pt model\nyolo detect train data=coco8.yaml model=yolo11n.pt epochs=100 imgsz=640\n\n# Build a new model from YAML, transfer pretrained weights to it and start training\nyolo detect train data=coco8.yaml model=yolo11n.yaml pretrained=yolo11n.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Benchmarking YOLOv8 Formats using Python\nDESCRIPTION: This Python snippet shows how to benchmark the speed and accuracy of a YOLOv8 PyTorch model across various export formats (including PyTorch, TorchScript, ONNX, and OpenVINO) using the `ultralytics` library. It loads the base model (`.pt` file) and calls the `benchmark()` method, specifying the dataset configuration file (`coco8.yaml`). The results encompass performance metrics for all supported formats.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/openvino.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a YOLOv8n PyTorch model\nmodel = YOLO(\"yolov8n.pt\")\n\n# Benchmark YOLOv8n speed and [accuracy](https://www.ultralytics.com/glossary/accuracy) on the COCO8 dataset for all export formats\nresults = model.benchmark(data=\"coco8.yaml\")\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with Trained YOLO11 Model in Python\nDESCRIPTION: Python code snippet to load a fine-tuned YOLO11 model and perform inference on a sample brain tumor image.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/brain-tumor.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"path/to/best.pt\")  # load a brain-tumor fine-tuned model\n\n# Inference using the model\nresults = model.predict(\"https://ultralytics.com/assets/brain-tumor-sample.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package with Optional Dependencies\nDESCRIPTION: Command to install the Ultralytics package with optional dependencies for exporting PyTorch models to different formats.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/raspberry-pi.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics[export]\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on Hand Keypoints Dataset using Python\nDESCRIPTION: This Python code demonstrates how to load a pre-trained YOLO model and train it on the hand keypoints dataset for 100 epochs with an image size of 640.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/hand-keypoints.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-pose.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"hand-keypoints.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Running YOLOv5 Benchmarks\nDESCRIPTION: Execute benchmarks for YOLOv5 models using the benchmarks.py script, specifying the model weights, image size, and device.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/model_export.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython benchmarks.py --weights yolov5s.pt --imgsz 640 --device 0\n```\n\n----------------------------------------\n\nTITLE: Resuming Interrupted YOLO11 Training via CLI\nDESCRIPTION: Illustrates the command-line interface command to resume an interrupted YOLO11 training session. The `yolo train resume` command is used, followed by the `model` argument specifying the path to the last saved checkpoint file (`path/to/last.pt`). This command automatically loads the previous state and continues training.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/train.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# Resume an interrupted training\nyolo train resume model=path/to/last.pt\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Model with Dog-Pose Dataset in Python (FAQ Example)\nDESCRIPTION: Python code snippet from the FAQ section demonstrating how to train a YOLO11n-pose model on the Dog-pose dataset for 100 epochs with an image size of 640.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/dog-pose.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-pose.pt\")\n\n# Train the model\nresults = model.train(data=\"dog-pose.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Ultralytics Python Interface for Model Training and Inference - Python\nDESCRIPTION: This Python snippet demonstrates end-to-end model handling using the Ultralytics package. It covers instantiating a model (from config or pretrained model), training on a dataset, evaluating on validation data, performing prediction on images, and exporting the model to ONNX format. Dependencies include the 'ultralytics' Python package, and arguments specify dataset paths, epoch counts, and export formats. Inputs and outputs vary by operation: datasets for training, file or URL for inference, and model files for export. Results objects provide structured output, and export operations return success status.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/quickstart.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Create a new YOLO model from scratch\nmodel = YOLO(\"yolo11n.yaml\")\n\n# Load a pretrained YOLO model (recommended for training)\nmodel = YOLO(\"yolo11n.pt\")\n\n# Train the model using the 'coco8.yaml' dataset for 3 epochs\nresults = model.train(data=\"coco8.yaml\", epochs=3)\n\n# Evaluate the model's performance on the validation set\nresults = model.val()\n\n# Perform object detection on an image using the model\nresults = model(\"https://ultralytics.com/images/bus.jpg\")\n\n# Export the model to ONNX format\nsuccess = model.export(format=\"onnx\")\n```\n\n----------------------------------------\n\nTITLE: Markdown Image Embedding\nDESCRIPTION: Markdown syntax for embedding images with centered alignment and full width, used for displaying monitoring tools and data drift detection diagrams.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/model-monitoring-and-maintenance.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n<p align=\"center\">\n  <img width=\"100%\" src=\"https://github.com/ultralytics/docs/releases/download/0/evidently-prometheus-grafana-monitoring-tools.avif\" alt=\"Overview of Open Source Model Monitoring Tools\">\n</p>\n```\n\n----------------------------------------\n\nTITLE: Training YOLOv5 with DistributedDataParallel on Specific GPUs\nDESCRIPTION: Command to train YOLOv5 using DistributedDataParallel mode on specific GPU devices. This allows selection of particular GPUs for training.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/multi_gpu_training.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython -m torch.distributed.run --nproc_per_node 2 train.py --batch 64 --data coco.yaml --cfg yolov5s.yaml --weights '' --device 2,3\n```\n\n----------------------------------------\n\nTITLE: Text Prompting with YOLOE in Python\nDESCRIPTION: This snippet demonstrates the use of text prompting with a YOLOE model to detect specified classes in an image. It initializes a model, sets the text prompts for desired classes, and runs detection on an image. The results are then displayed. This approach requires the Ultralytics YOLOE library and an image path as input.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yoloe.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLOE\n\n# Initialize a YOLOE model\nmodel = YOLOE(\"yoloe-11l-seg.pt\")  # or select yoloe-11s/m-seg.pt for different sizes\n\n# Set text prompt to detect person and bus. You only need to do this once after you load the model.\nnames = [\"person\", \"bus\"]\nmodel.set_classes(names, model.get_text_pe(names))\n\n# Run detection on the given image\nresults = model.predict(\"path/to/image.jpg\")\n\n# Show results\nresults[0].show()\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n Model on VOC Dataset in Python\nDESCRIPTION: Python code snippet demonstrating how to load a pretrained YOLO11n model and train it on the VOC dataset for 100 epochs with an image size of 640.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/voc.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"VOC.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Deploying and Running Inferences with DeepSparse\nDESCRIPTION: Python code to deploy a YOLO11 model using DeepSparse and run inferences on images.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/neural-magic.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom deepsparse import Pipeline\n\n# Specify the path to your YOLO11 ONNX model\nmodel_path = \"path/to/yolo11n.onnx\"\n\n# Set up the DeepSparse Pipeline\nyolo_pipeline = Pipeline.create(task=\"yolov8\", model_path=model_path)\n\n# Run the model on your images\nimages = [\"path/to/image.jpg\"]\npipeline_outputs = yolo_pipeline(images=images)\n```\n\n----------------------------------------\n\nTITLE: Ultralytics Sweep Annotation - Python\nDESCRIPTION: This example uses the `YOLO` model to track objects in a video and annotate detected bounding boxes using Ultralytics' `SolutionAnnotator`. It demonstrates how to use a draggable sweep line to count objects moving across a frame. Dependencies include `cv2` and the Ultralytics library, with parameters for the video path, model, and annotations. Key outputs are the annotated video frames.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/simple-utilities.md#2025-04-22_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import YOLO\nfrom ultralytics.engine.results import Results\nfrom ultralytics.solutions.solutions import SolutionAnnotator\n\n# User defined video path and model file\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nmodel = YOLO(model=\"yolo11s-seg.pt\")  # Model file i.e. yolo11s.pt or yolo11m-seg.pt\n\nif not cap.isOpened():\n    print(\"Error: Could not open video.\")\n    exit()\n\n# Initialize the video writer object.\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\nvideo_writer = cv2.VideoWriter(\"ultralytics.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n\nmasks = None  # Initialize variable to store masks data\nf = 0  # Initialize frame count variable for enabling mouse event.\nline_x = w  # Store width of line.\ndragging = False  # Initialize bool variable for line dragging.\nclasses = model.names  # Store model classes names for plotting.\nwindow_name = \"Ultralytics Sweep Annotator\"\n\ndef drag_line(event, x, _, flags, param):\n    \"\"\"Mouse callback function to enable dragging a vertical sweep line across the video frame.\"\"\"\n    global line_x, dragging\n    if event == cv2.EVENT_LBUTTONDOWN or (flags & cv2.EVENT_FLAG_LBUTTON):\n        line_x = max(0, min(x, w))\n        dragging = True\n\nwhile cap.isOpened():  # Loop over the video capture object.\n    ret, im0 = cap.read()\n    if not ret:\n        break\n    f = f + 1  # Increment frame count.\n    count = 0  # Re-initialize count variable on every frame for precise counts.\n    results = model.track(im0, persist=True)[0]\n\n    if f == 1:\n        cv2.namedWindow(window_name)\n        cv2.setMouseCallback(window_name, drag_line)\n\n    if results.boxes.id is not None:\n        if results.masks is not None:\n            masks = results.masks\n        boxes = results.boxes\n\n        track_ids = results.boxes.id.int().cpu().tolist()\n        clss = results.boxes.cls.cpu().tolist()\n\n        for mask, box, cls, t_id in zip(masks or [None] * len(boxes), boxes, clss, track_ids):\n            box_data = box.xyxy.cpu().tolist()[0][0]\n            if box_data > line_x:\n                count += 1\n                results = Results(\n                    im0, path=None, names=classes, boxes=box.data, masks=None if mask is None else mask.data\n                )\n                im0 = results.plot(\n                    boxes=True,  # display bounding box\n                    conf=False,  # hide confidence score\n                    labels=True,  # display labels\n                    color_mode=\"instance\",\n                )\n\n    # Generate draggable sweep line\n    annotator = SolutionAnnotator(im0)\n    annotator.sweep_annotator(line_x=line_x, line_y=h, label=f\"COUNT:{count}\")\n\n    cv2.imshow(window_name, im0)\n    video_writer.write(im0)\n    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n        break\n\n# Release the resources\ncap.release()\nvideo_writer.release()\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on ImageWoof Dataset - Basic Example\nDESCRIPTION: Basic training setup for YOLO model using the ImageWoof dataset with 100 epochs and 224x224 image size. Uses a pretrained YOLO model for classification.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/imagewoof.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-cls.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"imagewoof\", epochs=100, imgsz=224)\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo classify train data=imagewoof model=yolo11n-cls.pt epochs=100 imgsz=224\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLOv8 Model to TFLite Format via Ultralytics CLI - Bash\nDESCRIPTION: This command uses the Ultralytics 'yolo' CLI to export a trained YOLOv8 model ('yolov8n.pt') to the TFLite format with INT8 quantization for optimal edge device performance. You can adjust parameters (e.g., 'imgsz', 'format', 'int8') for FP32 or FP16 models. The command creates a folder containing the .tflite model file and a metadata.yaml containing class and model details; ensure the source .pt file is accessible.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-TFLite-Python/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=yolov8n.pt imgsz=640 format=tflite int8=True # Exports yolov8n_saved_model/yolov8n_full_integer_quant.tflite\n\n```\n\n----------------------------------------\n\nTITLE: Training Example using CLI for FAQ Section\nDESCRIPTION: Command-line interface example in the FAQ section showing how to train a pre-trained YOLO11n model on the COCO128 dataset for 100 epochs with an image size of 640.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/coco128.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nyolo detect train data=coco128.yaml model=yolo11n.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Deploying with AWS CDK in Bash\nDESCRIPTION: Shows how to synthesize and deploy the CloudFormation stack, bootstrapping the environment. AWS CLI must be installed and configured.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/amazon-sagemaker.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncdk synth\ncdk bootstrap\ncdk deploy\n```\n\n----------------------------------------\n\nTITLE: Class-Specific Heatmap Visualization with YOLO11\nDESCRIPTION: Shows how to configure YOLO11 heatmap generation for specific object classes. The example demonstrates tracking and visualizing only selected classes (cars and persons) using class indices.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/heatmaps.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nheatmap = solutions.Heatmap(show=True, model=\"yolo11n.pt\", classes=[0, 2])\n\nwhile cap.isOpened():\n    success, im0 = cap.read()\n    if not success:\n        break\n    results = heatmap(im0)\ncap.release()\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Starting YOLO11 Model Training via CLI\nDESCRIPTION: Illustrates how to initiate YOLO11 model training using the command-line interface. It shows commands for building a model from YAML and training from scratch, starting training from a pretrained .pt model, or building from YAML while transferring weights from a pretrained model. Training is configured for the COCO8 dataset, 100 epochs, and an image size of 640.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/train.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Build a new model from YAML and start training from scratch\nyolo detect train data=coco8.yaml model=yolo11n.yaml epochs=100 imgsz=640\n\n# Start training from a pretrained *.pt model\nyolo detect train data=coco8.yaml model=yolo11n.pt epochs=100 imgsz=640\n\n# Build a new model from YAML, transfer pretrained weights to it and start training\nyolo detect train data=coco8.yaml model=yolo11n.yaml pretrained=yolo11n.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Segmentation Model on Crack Dataset via CLI\nDESCRIPTION: Command-line interface instructions for training an Ultralytics YOLO11n segmentation model on the Crack Segmentation dataset, specifying epochs and image size parameters.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/crack-seg.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model using the Command Line Interface\n# Ensure the dataset YAML file 'crack-seg.yaml' is correctly configured and accessible\nyolo segment train data=crack-seg.yaml model=yolo11n-seg.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Validating FastSAM Model - Python\nDESCRIPTION: Describes the process of validating a FastSAM model against a labeled dataset using the Ultralytics Python API. Requires a proper dataset YAML file formatted for single-class segmentation and FastSAM model checkpoint. Outputs validation metrics and possibly predictions, subject to Results object handling.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/fast-sam.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import FastSAM\n\n# Create a FastSAM model\nmodel = FastSAM(\"FastSAM-s.pt\")  # or FastSAM-x.pt\n\n# Validate the model\nresults = model.val(data=\"coco8-seg.yaml\")\n```\n\n----------------------------------------\n\nTITLE: Plotting Similar Images\nDESCRIPTION: Visualizes similar images using various input methods including external images\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/explorer.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nexp.plot_similar(idx=6500, limit=20)\nexp.plot_similar(idx=[100, 101], limit=10)\nexp.plot_similar(img=\"https://ultralytics.com/images/bus.jpg\", limit=10, labels=False)\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Model with CLI\nDESCRIPTION: This CLI command initiates the training of a YOLO11 model on a custom dataset. The command specifies the dataset path, number of training epochs, and image size. It requires the Ultralytics environment and dataset formatted for YOLO.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/index.md#2025-04-22_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\nyolo train data=path/to/dataset.yaml epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model to TF GraphDef using CLI\nDESCRIPTION: Command-line instructions for exporting a YOLO11 PyTorch model to TF GraphDef format and then running inference with the exported model on an image.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tf-graphdef.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Export a YOLO11n PyTorch model to TF GraphDef format\nyolo export model=yolo11n.pt format=pb # creates 'yolo11n.pb'\n\n# Run inference with the exported model\nyolo predict model='yolo11n.pb' source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Detection Model on COCO8 Dataset in Bash\nDESCRIPTION: Command to start training a YOLO11n detection model on the COCO8 dataset for 100 epochs with an image size of 640 pixels.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/cli.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nyolo detect train data=coco8.yaml model=yolo11n.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on ImageNette160 Dataset using Python\nDESCRIPTION: This code demonstrates training a YOLO model on the reduced-size ImageNette160 dataset with 160x160 image dimensions for faster prototyping and training.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/imagenette.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-cls.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model with ImageNette160\nresults = model.train(data=\"imagenette160\", epochs=100, imgsz=160)\n```\n\n----------------------------------------\n\nTITLE: BOTSORT Class Implementation for Multi-Object Tracking\nDESCRIPTION: The main tracking class that implements the Bot SORT algorithm for multi-object tracking. It combines Kalman filtering, appearance descriptors, and motion compensation for robust tracking performance.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/trackers/bot_sort.md#2025-04-22_snippet_1\n\n\n\n----------------------------------------\n\nTITLE: Running YOLOv5 Inference on Desktop Screenshots\nDESCRIPTION: Demonstrates how to capture a screenshot of your desktop and perform YOLOv5 inference on it. This is useful for desktop applications, screen analysis, or automated UI testing.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/pytorch_hub_model_loading.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom PIL import ImageGrab\n\n# Model\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\")\n\n# Image\nim = ImageGrab.grab()  # take a screenshot\n\n# Inference\nresults = model(im)\n```\n\n----------------------------------------\n\nTITLE: Inspecting YOLOv5 Model Architecture\nDESCRIPTION: This Python snippet shows how to inspect the names of all modules and their parameters in a loaded YOLOv5 model instance. It helps in understanding the model structure for deciding which layers to freeze.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/transfer_learning_with_frozen_layers.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Assuming 'model' is your loaded YOLOv5 model instance\nfor name, param in model.named_parameters():\n    print(name)\n\n\"\"\"\nExample Output:\nmodel.0.conv.conv.weight\nmodel.0.conv.bn.weight\nmodel.0.conv.bn.bias\nmodel.1.conv.weight\nmodel.1.bn.weight\nmodel.1.bn.bias\nmodel.2.cv1.conv.weight\nmodel.2.cv1.bn.weight\n...\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring YOLO Tracker Parameters in Python\nDESCRIPTION: This code snippet demonstrates how to configure tracking parameters such as confidence threshold, IOU threshold, and result display when using the YOLO tracker in Python.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/trackers/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Configure the tracking parameters and run the tracker\nmodel = YOLO(\"yolo11n.pt\")\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", conf=0.3, iou=0.5, show=True)\n```\n\n----------------------------------------\n\nTITLE: Iterating over prediction results with Ultralytics YOLO in Python\nDESCRIPTION: This snippet shows the typical iteration over model inference results from Ultralytics YOLO predictions. It highlights access to the results' bounding box data as returned by the 'boxes' attribute (as a torch.Tensor array). The snippet is flexible and commonly used when handling object detection outputs from the predict method, requiring torch and ultralytics packages. Input: prediction results list. Output: access to bounding box tensor data for each result.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/vscode.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# reference https://docs.ultralytics.com/modes/predict/#working-with-results\n\nfor result in results:\n    result.boxes.data  # torch.Tensor array\n\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running YOLO Tracker in Python\nDESCRIPTION: This code snippet demonstrates how to load a YOLO model and perform tracking on a video source using Python. It shows options for different model types and tracker configurations.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/trackers/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load an official or custom model\nmodel = YOLO(\"yolo11n.pt\")  # Load an official Detect model\n# model = YOLO(\"yolo11n-seg.pt\")  # Load an official Segment model\n# model = YOLO(\"yolo11n-pose.pt\")  # Load an official Pose model\n# model = YOLO(\"path/to/best.pt\")  # Load a custom trained model\n\n# Perform tracking with the model\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", show=True)  # Tracking with default tracker\n# results = model.track(source=\"https://youtu.be/LNwODJXcvt4\", show=True, tracker=\"bytetrack.yaml\")  # Tracking with ByteTrack tracker\n```\n\n----------------------------------------\n\nTITLE: Splitting DOTA Images for Training\nDESCRIPTION: Python code to split high-resolution DOTA images into 1024x1024 resolution images using a multiscale approach, which is necessary for efficient training with the dataset.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/obb/dota-v2.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.data.split_dota import split_test, split_trainval\n\n# split train and val set, with labels.\nsplit_trainval(\n    data_root=\"path/to/DOTAv1.0/\",\n    save_dir=\"path/to/DOTAv1.0-split/\",\n    rates=[0.5, 1.0, 1.5],  # multiscale\n    gap=500,\n)\n# split test set, without labels.\nsplit_test(\n    data_root=\"path/to/DOTAv1.0/\",\n    save_dir=\"path/to/DOTAv1.0-split/\",\n    rates=[0.5, 1.0, 1.5],  # multiscale\n    gap=500,\n)\n```\n\n----------------------------------------\n\nTITLE: YOLOE Segmentation Trainer Class References\nDESCRIPTION: Reference documentation for four YOLOE segmentation trainer classes: YOLOESegTrainer (standard training), YOLOEPESegTrainer (linear probing), YOLOESegTrainerFromScratch (training from scratch), and YOLOESegVPTrainer (visual prompt training). These classes provide different training approaches for YOLOE segmentation models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/models/yolo/yoloe/train_seg.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nultralytics.models.yolo.yoloe.train_seg.YOLOESegTrainer\nultralytics.models.yolo.yoloe.train_seg.YOLOEPESegTrainer\nultralytics.models.yolo.yoloe.train_seg.YOLOESegTrainerFromScratch\nultralytics.models.yolo.yoloe.train_seg.YOLOESegVPTrainer\n```\n\n----------------------------------------\n\nTITLE: Validating YOLO Model Accuracy using Python and CLI\nDESCRIPTION: Demonstrates how to validate a trained YOLO model using both Python API and CLI methods. Returns metrics like mAP50-95 for model performance evaluation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/detect.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the model\nmodel = YOLO(\"path/to/best.pt\")\n\n# Validate the model\nmetrics = model.val()\nprint(metrics.box.map)  # mAP50-95\n```\n\nLANGUAGE: bash\nCODE:\n```\nyolo detect val model=path/to/best.pt\n```\n\n----------------------------------------\n\nTITLE: Initializing ClearML SDK\nDESCRIPTION: Command to initialize the ClearML SDK in your environment.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/clearml.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nclearml-init\n```\n\n----------------------------------------\n\nTITLE: Defining Package Segmentation Dataset Configuration in YAML\nDESCRIPTION: YAML configuration file for the Package Segmentation dataset, specifying paths, classes, and other essential details. This file is maintained in the Ultralytics repository.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/package-seg.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/package-seg.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Result Aggregation and Manipulation with Results Class — Python\nDESCRIPTION: The Results class encapsulates detection, segmentation, or classification results, providing structured access to entities like boxes, masks, probabilities, and other output forms. This class is crucial for downstream processing, evaluation, and visualization. Required dependencies include PyTorch and associated Ultralytics components that interact with model outputs.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/engine/results.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n## ::: ultralytics.engine.results.Results\n```\n\n----------------------------------------\n\nTITLE: Loading and Displaying YOLO11 Experiment Data with DVC API and Pandas (Python)\nDESCRIPTION: Retrieves experiment data tracked by DVCLive using the `dvc.api.exp_show()` function. It then loads this data into a pandas DataFrame, selects specific columns of interest (like epoch count, image size, model name, and mAP metrics), cleans the data by removing rows with missing values, resets the index, and prints the resulting DataFrame. This is useful for analyzing results after training.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/dvc.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport dvc.api\nimport pandas as pd\n\n# Define columns of interest\ncolumns = [\"Experiment\", \"epochs\", \"imgsz\", \"model\", \"metrics.mAP50-95(B)\"]\n\n# Retrieve experiment data\ndf = pd.DataFrame(dvc.api.exp_show(), columns=columns)\n\n# Clean data\ndf.dropna(inplace=True)\ndf.reset_index(drop=True, inplace=True)\n\n# Display DataFrame\nprint(df)\n```\n\n----------------------------------------\n\nTITLE: Converting LanceDB Table to Pandas DataFrame and Arrow Table in Python\nDESCRIPTION: This code shows how to convert a LanceDB table to both a pandas DataFrame and an Apache Arrow table, facilitating interoperability with other data analysis tools.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/explorer.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf = table.to_pandas()\npa_table = table.to_arrow()\n```\n\n----------------------------------------\n\nTITLE: Starting YOLOv5 Training from Command Line using Bash\nDESCRIPTION: This command executes the `train.py` script using Python to start the YOLOv5 model training process. It specifies several hyperparameters and configuration paths: `--img 640` sets the input image size, `--batch 16` defines the batch size, `--epochs 100` sets the number of training epochs, `--data` points to the dataset configuration YAML file, and `--weights yolov5s.pt` specifies the pre-trained model weights to start from.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/train_custom_data.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython train.py --img 640 --batch 16 --epochs 100 --data path/to/your/dataset.yaml --weights yolov5s.pt\n```\n\n----------------------------------------\n\nTITLE: Running YOLOv8 TFLite Inference via Main Python Script - Bash\nDESCRIPTION: This bash snippet demonstrates how to run inference on an image using the provided 'main.py' Python script and a YOLOv8 model exported in TFLite format. It specifies arguments for the model path, test image, confidence threshold, IoU threshold, and optional metadata file containing class details. Expected output is an annotated image with detected classes and confidence scores; adjust parameters as needed for your model and use case.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-TFLite-Python/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython main.py \\\n  --model yolov8n_saved_model/yolov8n_full_integer_quant.tflite \\\n  --img image.jpg \\\n  --conf 0.25 \\\n  --iou 0.45 \\\n  --metadata yolov8n_saved_model/metadata.yaml\n\n```\n\n----------------------------------------\n\nTITLE: Saving YOLO11 Validation Results as JSON in Python (Python)\nDESCRIPTION: Shows how to trigger automatic export of validation results to a JSON file during the evaluation of a YOLO11 model in Python. This is useful for farther analysis or programmatic integration. Requires setting the 'save_json' flag to True. Needs Ultralytics, a model file, and optionally accepts validation argument overrides.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/val.md#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Save validation results to JSON\nmetrics = model.val(save_json=True)\n\n```\n\n----------------------------------------\n\nTITLE: Using MobileSAM with Point Prompts in Python\nDESCRIPTION: This code snippet shows how to use MobileSAM for image segmentation using point prompts. It demonstrates loading the model and predicting segments based on single and multiple point prompts, including both positive and negative prompts.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/mobile-sam.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import SAM\n\n# Load the model\nmodel = SAM(\"mobile_sam.pt\")\n\n# Predict a segment based on a single point prompt\nmodel.predict(\"ultralytics/assets/zidane.jpg\", points=[900, 370], labels=[1])\n\n# Predict multiple segments based on multiple points prompt\nmodel.predict(\"ultralytics/assets/zidane.jpg\", points=[[400, 370], [900, 370]], labels=[1, 1])\n\n# Predict a segment based on multiple points prompt per object\nmodel.predict(\"ultralytics/assets/zidane.jpg\", points=[[[400, 370], [900, 370]]], labels=[[1, 1]])\n\n# Predict a segment using both positive and negative prompts.\nmodel.predict(\"ultralytics/assets/zidane.jpg\", points=[[[400, 370], [900, 370]]], labels=[[1, 0]])\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Segmentation Model on Crack Dataset in Python\nDESCRIPTION: Python code snippet that demonstrates how to train an Ultralytics YOLO11n segmentation model on the Crack Segmentation dataset for 100 epochs with 640px image size. It shows loading a pretrained model and configuring the training process.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/crack-seg.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\n# Using a pretrained model like yolo11n-seg.pt is recommended for faster convergence\nmodel = YOLO(\"yolo11n-seg.pt\")\n\n# Train the model on the Crack Segmentation dataset\n# Ensure 'crack-seg.yaml' is accessible or provide the full path\nresults = model.train(data=\"crack-seg.yaml\", epochs=100, imgsz=640)\n\n# After training, the model can be used for prediction or exported\n# results = model.predict(source='path/to/your/images')\n```\n\n----------------------------------------\n\nTITLE: Running FastSAM Inference - Segment with Text Prompt - Shell\nDESCRIPTION: Executes FastSAM inference with a text prompt to segment objects described by the given string (e.g., 'the yellow dog'). Accepts model and image paths, text prompt argument, and returns segmentation results that match the prompt. Dependencies: FastSAM and CLIP installed, model weights and images present.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/fast-sam.md#2025-04-22_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\npython Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg --text_prompt \"the yellow dog\"\n```\n\n----------------------------------------\n\nTITLE: Using RT-DETR for Object Detection via Command Line\nDESCRIPTION: This snippet shows how to train an RT-DETR model on a dataset and perform inference on an image using the Ultralytics command-line interface.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/rtdetr.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Load a COCO-pretrained RT-DETR-l model and train it on the COCO8 example dataset for 100 epochs\nyolo train model=rtdetr-l.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Load a COCO-pretrained RT-DETR-l model and run inference on the 'bus.jpg' image\nyolo predict model=rtdetr-l.pt source=path/to/bus.jpg\n```\n\n----------------------------------------\n\nTITLE: Setting Up Triton Model Repository Structure\nDESCRIPTION: This snippet shows how to create the necessary directory structure for a Triton Inference Server model repository, including moving the exported ONNX model to the proper location and creating the required configuration file.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/triton-inference-server.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\n\n# Define paths\nmodel_name = \"yolo\"\ntriton_repo_path = Path(\"tmp\") / \"triton_repo\"\ntriton_model_path = triton_repo_path / model_name\n\n# Create directories\n(triton_model_path / \"1\").mkdir(parents=True, exist_ok=True)\nPath(onnx_file).rename(triton_model_path / \"1\" / \"model.onnx\")\n(triton_model_path / \"config.pbtxt\").touch()\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Classification Models via CLI\nDESCRIPTION: Shows how to train YOLO11 classification models using command-line interface with different initialization options including training from scratch, from pretrained models, or transferring weights.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/classify.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Build a new model from YAML and start training from scratch\nyolo classify train data=mnist160 model=yolo11n-cls.yaml epochs=100 imgsz=64\n\n# Start training from a pretrained *.pt model\nyolo classify train data=mnist160 model=yolo11n-cls.pt epochs=100 imgsz=64\n\n# Build a new model from YAML, transfer pretrained weights to it and start training\nyolo classify train data=mnist160 model=yolo11n-cls.yaml pretrained=yolo11n-cls.pt epochs=100 imgsz=64\n```\n\n----------------------------------------\n\nTITLE: Validating FastSAM Model - CLI\nDESCRIPTION: Uses the Ultralytics CLI to validate a FastSAM model on a specified dataset at a given image size. Assumes dataset and FastSAM model weights are available and accessible. Returns validation metrics including segmentation accuracy on the COCO8 dataset; results are typically saved or printed to the console.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/fast-sam.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Load a FastSAM model and validate it on the COCO8 example dataset at image size 640\nyolo segment val model=FastSAM-s.pt data=coco8.yaml imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Customizing YOLOv5 Input Channels\nDESCRIPTION: Shows how to load a YOLOv5 model with a custom number of input channels (4 instead of the default 3), which is useful for processing images with an additional channel such as depth or alpha.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/pytorch_hub_model_loading.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", channels=4)\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on Wheat Dataset using Python\nDESCRIPTION: Python code example showing how to load a YOLO11n model and train it on the Global Wheat Head Dataset for 100 epochs with 640px image size\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/globalwheat2020.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"GlobalWheat2020.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Making API Requests with Python\nDESCRIPTION: Python code example showing how to make requests to the Ultralytics HUB Inference API. Uses the requests library to send POST requests with image data and inference parameters.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/hub/inference-api.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\n# API URL\nurl = \"https://predict.ultralytics.com\"\n\n# Headers, use actual API_KEY\nheaders = {\"x-api-key\": \"API_KEY\"}\n\n# Inference arguments (use actual MODEL_ID)\ndata = {\"model\": \"https://hub.ultralytics.com/models/MODEL_ID\", \"imgsz\": 640, \"conf\": 0.25, \"iou\": 0.45}\n\n# Load image and send request\nwith open(\"path/to/image.jpg\", \"rb\") as image_file:\n    files = {\"file\": image_file}\n    response = requests.post(url, headers=headers, files=files, data=data)\n\nprint(response.json())\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model with Python\nDESCRIPTION: This Python code snippet exports a YOLO11 model to the ONNX format for deployment. The user must have a trained YOLO11 model and specifies the desired export format. The process supports integration with different platforms and devices.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/index.md#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load your trained YOLO model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to ONNX format (you can specify other formats as needed)\nmodel.export(format=\"onnx\")\n```\n\n----------------------------------------\n\nTITLE: Using Custom Configuration File in YOLO Training\nDESCRIPTION: Examples showing how to use a custom YAML configuration file for training using both Python API and CLI approaches.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/yolo-data-augmentation.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a COCO-pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Train the model with custom configuration\nmodel.train(cfg=\"train_custom.yaml\")\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Train the model with custom configuration\nyolo detect train model=\"yolo11n.pt\" cfg=train_custom.yaml\n```\n\n----------------------------------------\n\nTITLE: Predicting with YOLO MNN Model - Python\nDESCRIPTION: This snippet explains how to use the Ultralytics YOLO Python library to predict using an exported MNN model. It requires the Ultralytics YOLO library. Key parameters are the model path and image source URL. It outputs prediction results that can be displayed or saved.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/mnn.md#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the YOLO11 MNN model\nmodel = YOLO(\"yolo11n.mnn\")\n\n# Export to MNN format\nresults = model(\"https://ultralytics.com/images/bus.jpg\")  # predict with `fp32`\nresults = model(\"https://ultralytics.com/images/bus.jpg\", half=True)  # predict with `fp16` if device support\n\nfor result in results:\n    result.show()  # display to screen\n    result.save(filename=\"result.jpg\")  # save to disk\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n Model on VOC Dataset via CLI\nDESCRIPTION: Command-line interface (CLI) command to train a YOLO11n model on the VOC dataset for 100 epochs with an image size of 640.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/voc.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo detect train data=VOC.yaml model=yolo11n.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Exporting a YOLO Classification Model to ONNX in Bash\nDESCRIPTION: Command to export a YOLO classification model to ONNX format with a custom image size of 224x128 pixels.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/cli.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=yolo11n-cls.pt format=onnx imgsz=224,128\n```\n\n----------------------------------------\n\nTITLE: Defining the NASValidator Class in Python\nDESCRIPTION: This represents the `NASValidator` class definition within the `ultralytics.models.nas.val` module. This class is designed for validating Ultralytics YOLO models generated through Neural Architecture Search (NAS). It likely inherits from a base validator class and implements specific logic for NAS model evaluation, including data handling, inference, Non-Maximum Suppression (NMS), and performance metric calculation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/models/nas/val.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n::: ultralytics.models.nas.val.NASValidator\n```\n\n----------------------------------------\n\nTITLE: Visualizing YOLO11 Experiment Results with Plotly\nDESCRIPTION: Python code to create an interactive parallel coordinates plot using Plotly, allowing visual analysis of relationships between different parameters and metrics.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/dvc.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom plotly.express import parallel_coordinates\n\n# Create a parallel coordinates plot\nfig = parallel_coordinates(df, columns, color=\"metrics.mAP50-95(B)\")\n\n# Display the plot\nfig.show()\n```\n\n----------------------------------------\n\nTITLE: Customizing YOLO11 Validation Parameters via CLI (Bash)\nDESCRIPTION: Demonstrates passing custom arguments for dataset, image size, batch size, confidence, IOU, and device to the YOLO11 validation process via command line. Works for both official and custom models, extending flexibility in a terminal-based workflow. Requires access to the Ultralytics CLI and the specified model/dataset.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/val.md#2025-04-22_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\nyolo val model=yolo11n.pt data=coco8.yaml imgsz=640 batch=16 conf=0.25 iou=0.6 device=0\n\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Models with Dynamic Input Size - CLI\nDESCRIPTION: Illustrates command-line export of a YOLO11 model to ONNX format with support for dynamic input resolution, ensuring flexibility for deployments with variable image sizes. Requires Ultralytics CLI. The 'dynamic=True' flag is critical for workflows demanding variable input shapes. Output is a dynamic ONNX model ready for diverse data sources.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/export.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=yolo11n.pt format=onnx dynamic=True\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with Trained Signature Detection Model using Python\nDESCRIPTION: Python code for performing inference using a model trained on the signature detection dataset. The code loads a fine-tuned model and runs prediction on a video file with a confidence threshold of 0.75.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/signature.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"path/to/best.pt\")  # load a signature-detection fine-tuned model\n\n# Inference using the model\nresults = model.predict(\"https://ultralytics.com/assets/signature-s.mp4\", conf=0.75)\n```\n\n----------------------------------------\n\nTITLE: Plotting Trial Metric Histories from Ray Tune Results Python\nDESCRIPTION: This snippet visualizes the per-trial evolution of the mean_accuracy metric over training iterations for all Ray Tune trials. Requires matplotlib and result_grid from Ray Tune. Inputs are the metrics dataframe for each result; outputs are a labeled plot displaying accuracy curves for each trial. Assumes result.metrics_dataframe columns include 'training_iteration' and 'mean_accuracy'.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/ray-tune.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\\n\\nfor i, result in enumerate(result_grid):\\n    plt.plot(\\n        result.metrics_dataframe[\"training_iteration\"],\\n        result.metrics_dataframe[\"mean_accuracy\"],\\n        label=f\"Trial {i}\",\\n    )\\n\\nplt.xlabel(\"Training Iterations\")\\nplt.ylabel(\"Mean Accuracy\")\\nplt.legend()\\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Training and Running Ultralytics YOLO11 via Command Line Interface - Bash\nDESCRIPTION: This snippet provides CLI commands for training and inference using YOLO11. The 'yolo' command-line tool is used, requiring the ultralytics package to be installed in the environment. The first command trains a YOLO11n model on the specified dataset and for a defined number of epochs, while the second command performs inference on an image file. Parameters include model path, dataset configuration, number of epochs, image size for training, and the path to the input image. Outputs include trained models/checkpoints and prediction results.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolo11.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Load a COCO-pretrained YOLO11n model and train it on the COCO8 example dataset for 100 epochs\nyolo train model=yolo11n.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Load a COCO-pretrained YOLO11n model and run inference on the 'bus.jpg' image\nyolo predict model=yolo11n.pt source=path/to/bus.jpg\n```\n\n----------------------------------------\n\nTITLE: Using Pretrained Open Images V7 Models with CLI\nDESCRIPTION: Command-line interface commands for running prediction with an Open Images V7 pretrained model and starting training from a pretrained checkpoint.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/open-images-v7.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Predict using an Open Images Dataset V7 pretrained model\nyolo detect predict source=image.jpg model=yolov8n-oiv7.pt\n\n# Start training from an Open Images Dataset V7 pretrained checkpoint\nyolo detect train data=coco8.yaml model=yolov8n-oiv7.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Training and Inference with Ultralytics YOLOv6 (Python)\nDESCRIPTION: This Python code shows how to instantiate, train, and run inference with a YOLOv6 model using the Ultralytics library. It requires the 'ultralytics' Python package, a YOLOv6 configuration YAML (e.g., 'yolov6n.yaml'), and relevant data files. The 'model.train' method accepts parameters such as which data to use, number of epochs, and input image size. The 'model()' call performs inference on the specified image path, returning detection results.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov6.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Build a YOLOv6n model from scratch\nmodel = YOLO(\"yolov6n.yaml\")\n\n# Display model information (optional)\nmodel.info()\n\n# Train the model on the COCO8 example dataset for 100 epochs\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n\n# Run inference with the YOLOv6n model on the 'bus.jpg' image\nresults = model(\"path/to/bus.jpg\")\n\n```\n\n----------------------------------------\n\nTITLE: Disabling Weights & Biases logging for YOLO\nDESCRIPTION: This command demonstrates how to disable Weights & Biases logging in YOLO settings using the command line.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/weights-biases.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nyolo settings wandb=False\n```\n\n----------------------------------------\n\nTITLE: Expressing Reversible Functions in Deep Learning – Python\nDESCRIPTION: This code snippet exhibits the conceptual representation of a reversible function within a neural network, where the input X can be precisely reconstructed using the composition of functions parameterized by psi and zeta. It highlights the mathematical underpinnings of reversibility in deep networks, serving as a theoretical aid. The code is purely symbolic, expects function-like placeholders (v_zeta, r_psi) and variables, and does not require any dependencies. Limitations: it is meant for documentation and cannot be executed directly.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov9.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nX = v_zeta(r_psi(X))\n```\n\n----------------------------------------\n\nTITLE: Running YOLOv8 ONNX Segmentation Inference (Bash)\nDESCRIPTION: Executes the Python inference script ('main.py') to perform instance segmentation using the previously exported ONNX model. It requires specifying the path to the ONNX model file ('--model') and the path to the input image source ('--source'). Requires Python, the 'main.py' script, the specified ONNX model, the input image, and all installed dependencies (onnxruntime, opencv-python, numpy) to be present.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-Segmentation-ONNXRuntime-Python/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython main.py --model yolov8s-seg.onnx --source path/to/image.jpg\n```\n\n----------------------------------------\n\nTITLE: Advanced Querying with Pre- and Post-filters in Ultralytics Explorer\nDESCRIPTION: Demonstrates advanced querying techniques using pre- and post-filters on the LanceDB table in Ultralytics Explorer.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/api.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import Explorer\n\nexp = Explorer(model=\"yolo11n.pt\")\nexp.create_embeddings_table()\ntable = exp.table\n\n# Dummy embedding\nembedding = [i for i in range(256)]\nrs = table.search(embedding).metric(\"cosine\").where(\"\").limit(10)\n```\n\n----------------------------------------\n\nTITLE: Starting DeepSparse HTTP Server\nDESCRIPTION: Launches a DeepSparse server using FastAPI for YOLOv5 object detection.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/neural_magic_pruning_quantization.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndeepsparse.server \\\n  --task yolo \\\n  --model_path zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned65_quant-none\n```\n\n----------------------------------------\n\nTITLE: Validating Multiple YOLO11 Models\nDESCRIPTION: This code iterates over a set of YOLO11 models, running validation commands using different configurations. It requires the `yolo` command-line interface and presupposes that the corresponding model files are available and configured for the task.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfor x in 'nsmlx':\n  !yolo val model=yolo11{x}.pt data=coco.yaml\n```\n\n----------------------------------------\n\nTITLE: Basic YOLO Tracking Implementation in Python\nDESCRIPTION: Basic example of implementing object tracking using YOLO model. Shows how to load a model and track objects in a video source with confidence and IOU thresholds.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/track/index.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolo11n.pt\")\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", conf=0.3, iou=0.5, show=True)\n```\n\n----------------------------------------\n\nTITLE: Auto-Annotating Segmentation Datasets in Python\nDESCRIPTION: This Python snippet explains how to auto-annotate segmentation datasets using 'ultralytics.data.annotator.auto_annotate'. It requires paths to image data, detection model weights, and SAM weights. The function stores annotated results in a specified output directory.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/sam.md#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics.data.annotator import auto_annotate\n\nauto_annotate(data=\"path/to/images\", det_model=\"yolo11x.pt\", sam_model=\"sam_b.pt\")\n```\n\n----------------------------------------\n\nTITLE: Reproducing YOLO-World Official Results\nDESCRIPTION: Shows how to reproduce official YOLO-World results from scratch using custom datasets and training configuration. Includes setup for training with multiple datasets and custom trainer.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolo-world.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLOWorld\nfrom ultralytics.models.yolo.world.train_world import WorldTrainerFromScratch\n\ndata = {\n    \"train\": {\n        \"yolo_data\": [\"Objects365.yaml\"],\n        \"grounding_data\": [\n            {\n                \"img_path\": \"../datasets/flickr30k/images\",\n                \"json_file\": \"../datasets/flickr30k/final_flickr_separateGT_train.json\",\n            },\n            {\n                \"img_path\": \"../datasets/GQA/images\",\n                \"json_file\": \"../datasets/GQA/final_mixed_train_no_coco.json\",\n            },\n        ],\n    },\n    \"val\": {\"yolo_data\": [\"lvis.yaml\"]},\n}\n\nmodel = YOLOWorld(\"yolov8s-worldv2.yaml\")\nmodel.train(data=data, batch=128, epochs=100, trainer=WorldTrainerFromScratch)\n```\n\n----------------------------------------\n\nTITLE: Performing Object Tracking with YOLO11 via Command Line\nDESCRIPTION: This command-line example shows how to use the Ultralytics YOLO11 CLI to track objects in a video. It demonstrates the basic syntax for the track command, which can be used with various sources including video files, webcams, or RTSP streams.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/index.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n# Perform object tracking on a video from the command line\n# You can specify different sources like webcam (0) or RTSP streams\nyolo track source=path/to/video.mp4\n```\n\n----------------------------------------\n\nTITLE: Running YOLO Inference on a NumPy Array in Python\nDESCRIPTION: This snippet illustrates how to run YOLO inference on an image represented as a NumPy array. It requires importing `numpy` and `YOLO`. A sample NumPy array is created (representing an image in HWC format, BGR color order, with uint8 data type). This array is then used as the source for the loaded YOLO model, yielding a list of Results objects.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/predict.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Create a random numpy array of HWC shape (640, 640, 3) with values in range [0, 255] and type uint8\nsource = np.random.randint(low=0, high=255, size=(640, 640, 3), dtype=\"uint8\")\n\n# Run inference on the source\nresults = model(source)  # list of Results objects\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Exported CoreML Model using Python\nDESCRIPTION: Python code snippet to load an exported CoreML model and run inference on an image using the Ultralytics library.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/coreml.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\ncoreml_model = YOLO(\"yolo11n.mlpackage\")\nresults = coreml_model(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Associating Objects with Image Windows - Ultralytics DOTA Utilities - Python\nDESCRIPTION: Defines a function to map or find labeled objects (such as bounding boxes) within generated image windows, aiding efficient cropping and dataset partitioning. Relies on window and object coordinates, filters objects based on overlap/IOF, and outputs the subset of objects per window. Useful for creating cropped label sets for tiled images.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/data/split_dota.md#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef get_window_obj(objects, window, iof_threshold):\n    # Associates labeled objects with a given image window using IOF threshold filtering\n    pass  # Implementation selects objects that overlap with the current window\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Triton Server\nDESCRIPTION: Code to load YOLO model from Triton server and perform inference\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/triton-inference-server.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the Triton Server model\nmodel = YOLO(\"http://localhost:8000/yolo\", task=\"detect\")\n\n# Run inference on the server\nresults = model(\"path/to/image.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Running YOLO Inference Using Glob Patterns (Streaming) in Python\nDESCRIPTION: This snippet illustrates using glob patterns to specify multiple input files for YOLO inference in streaming mode. It loads a YOLO model and defines the `source` using a glob pattern (e.g., `path/to/dir/*.jpg` for all JPGs in a directory, or `path/to/dir/**/*.jpg` for recursive search). Running the model with `stream=True` processes all matching files and returns a generator of Results objects.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/predict.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Define a glob search for all JPG files in a directory\nsource = \"path/to/dir/*.jpg\"\n\n# OR define a recursive glob search for all JPG files including subdirectories\nsource = \"path/to/dir/**/*.jpg\"\n\n# Run inference on the source\nresults = model(source, stream=True)  # generator of Results objects\n```\n\n----------------------------------------\n\nTITLE: Training YOLOv5 on Multiple Machines (Master Node)\nDESCRIPTION: Command for training YOLOv5 on the master node in a multi-machine setup. This configures the distributed training across multiple machines.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/multi_gpu_training.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython -m torch.distributed.run --nproc_per_node G --nnodes N --node_rank 0 --master_addr \"192.168.1.1\" --master_port 1234 train.py --batch 64 --data coco.yaml --cfg yolov5s.yaml --weights ''\n```\n\n----------------------------------------\n\nTITLE: Loading YOLO-Formatted DOTA Annotations - Ultralytics DOTA Utilities - Python\nDESCRIPTION: Implements a function to load and parse DOTA dataset annotations formatted for YOLO object detection workflows. Handles file reading, parsing label data, and transforming it into a usable structure for image processing and model training. Requires knowledge of YOLO and DOTA format conventions; expects label file paths as input and returns structured annotation objects.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/data/split_dota.md#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndef load_yolo_dota(label_path):\n    # Loads YOLO-format annotations for a DOTA image from the given label file path\n    pass  # Implementation reads and parses YOLO-formatted labels\n```\n\n----------------------------------------\n\nTITLE: YOLOv5 Ensemble Inference\nDESCRIPTION: Command to perform inference using an ensemble of YOLOv5x and YOLOv5l6 models on image data.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/model_ensembling.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython detect.py --weights yolov5x.pt yolov5l6.pt --img 640 --source data/images\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on xView Dataset with Python\nDESCRIPTION: Python code snippet to train a YOLO model on the xView dataset for 100 epochs with 640px image size. The example loads a pretrained YOLO model and configures it for training on satellite imagery detection tasks.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/xview.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"xView.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Performing Standard Inference with YOLO11 and SAHI\nDESCRIPTION: Python code to perform standard inference using an image path or a numpy image with YOLO11 and SAHI.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/sahi-tiled-inference.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom sahi.predict import get_prediction\n\n# With an image path\nresult = get_prediction(\"demo_data/small-vehicles1.jpeg\", detection_model)\n\n# With a numpy image\nresult_with_np_image = get_prediction(read_image(\"demo_data/small-vehicles1.jpeg\"), detection_model)\n```\n\n----------------------------------------\n\nTITLE: Running YOLO Inference on YouTube Video\nDESCRIPTION: Demonstrates how to load a YOLO model and run inference on a YouTube video using streaming mode to efficiently handle memory usage.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/predict.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Define source as YouTube video URL\nsource = \"https://youtu.be/LNwODJXcvt4\"\n\n# Run inference on the source\nresults = model(source, stream=True)  # generator of Results objects\n```\n\n----------------------------------------\n\nTITLE: Evaluating YOLO11 Model Performance on Dataset\nDESCRIPTION: Command to evaluate the performance of a YOLO11 model on a dataset using DeepSparse.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/neural-magic.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndeepsparse.yolov8.eval --model_path \"path/to/yolo11n.onnx\"\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model via CLI\nDESCRIPTION: Shows how to export YOLO11 models (both official and custom) to different formats using the command-line interface.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/detect.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=yolo11n.pt format=onnx      # export official model\nyolo export model=path/to/best.pt format=onnx # export custom trained model\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on Argoverse Dataset using CLI\nDESCRIPTION: Command-line interface (CLI) command to start training a YOLO11n model on the Argoverse dataset for 100 epochs with an image size of 640.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/argoverse.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo detect train data=Argoverse.yaml model=yolo11n.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics and Weights & Biases packages\nDESCRIPTION: This snippet shows how to install the required packages (Ultralytics and Weights & Biases) and enable W&B logging in YOLO settings.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/weights-biases.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install -U ultralytics wandb\nyolo settings wandb=True\n```\n\n----------------------------------------\n\nTITLE: Converting Segmentation Masks to YOLO Format\nDESCRIPTION: Python code for converting binary segmentation masks to the YOLO segmentation format.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/index.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.data.converter import convert_segment_masks_to_yolo_seg\n\nconvert_segment_masks_to_yolo_seg(masks_dir=\"path/to/masks_dir\", output_dir=\"path/to/output_dir\", classes=80)\n```\n\n----------------------------------------\n\nTITLE: Managing Segmentation Masks with Masks Class — Python\nDESCRIPTION: Masks provides functionalities for representing and managing segmentation mask outputs in object detection and segmentation models. It extends tensor logic for efficient memory and computational handling of binary or probabilistic masks, interoperating with PyTorch. Outputs are typically tensors or arrays representing detected object regions.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/engine/results.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n## ::: ultralytics.engine.results.Masks\n```\n\n----------------------------------------\n\nTITLE: Benchmarking YOLO Models using Ultralytics CLI\nDESCRIPTION: This Bash snippet illustrates how to use the Ultralytics Command-Line Interface (CLI) to perform model benchmarking. It executes the `yolo benchmark` command, specifying the model, data, image size, precision (`half=False`), computation device (`device=0`), and optionally targeting a single export format (`format=onnx`). Requires the `ultralytics` package to be installed.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/benchmark.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyolo benchmark model=yolo11n.pt data='coco8.yaml' imgsz=640 half=False device=0\n\n# Benchmark specific export format\nyolo benchmark model=yolo11n.pt data='coco8.yaml' imgsz=640 format=onnx\n```\n\n----------------------------------------\n\nTITLE: Visualizing YOLO11 Training Results in IPython\nDESCRIPTION: This code snippet demonstrates how to visualize training results using JupyterLab's built-in plotting capabilities. It uses the matplotlib library and the Ultralytics plotting utility to create inline plots of the training results.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/jupyterlab.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n%matplotlib inline\nfrom ultralytics.utils.plotting import plot_results\nplot_results(results)\n```\n\n----------------------------------------\n\nTITLE: Training and Inference with Ultralytics Python API\nDESCRIPTION: This Python code snippet demonstrates how to load a pre-trained YOLOE model, train it on custom data, and perform inference using text prompts. It utilizes the Ultralytics Python library. Required dependencies include the Ultralytics library and a dataset formatted as per specifications. Inputs include paths to the model and dataset, with outputs being annotated images saved to disk.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yoloe.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load pre-trained YOLOE model and train on custom data\nmodel = YOLO(\"yoloe-11s.pt\")\nmodel.train(data=\"path/to/data.yaml\", epochs=50, imgsz=640)\n\n# Run inference using text prompts (\"person\", \"bus\")\nmodel.set_classes([\"person\", \"bus\"])\nresults = model.predict(source=\"test_images/street.jpg\")\nresults[0].save()  # save annotated output\n\n```\n\n----------------------------------------\n\nTITLE: Locating Dataset YAML Configuration Files - Python\nDESCRIPTION: The find_dataset_yaml utility searches directories or repositories for dataset YAML configuration files, critical for data loading and model training. Input is typically a dataset root or path; output is the discovered YAML file path(s). Depends on file system access and standard naming conventions.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/data/utils.md#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n## ::: ultralytics.data.utils.find_dataset_yaml\n```\n\n----------------------------------------\n\nTITLE: Predicting with a YOLO Segmentation Model in Bash\nDESCRIPTION: Command to run inference using a pre-trained segmentation model on a YouTube video with a custom image size of 320 pixels.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/cli.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nyolo predict model=yolo11n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n```\n\n----------------------------------------\n\nTITLE: Dataset Configuration with YAML\nDESCRIPTION: YAML configuration file defining the COCO-Pose dataset structure, paths and keypoint information.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/coco.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/coco-pose.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Using Ultralytics YOLO with xView Dataset (CLI Example)\nDESCRIPTION: Command-line approach for training a YOLO model on the xView dataset. This bash command specifies the dataset, model, and training parameters for object detection in satellite imagery.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/xview.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo detect train data=xView.yaml model=yolo11n.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Object Detection Model with CLI\nDESCRIPTION: Shows how to train a YOLO11 object detection model using the Ultralytics command-line interface. This command loads the YOLO11n model and trains it on the COCO8 dataset for 100 epochs with 640x640 image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/train.md#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nyolo detect train data=coco8.yaml model=yolo11n.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Benchmarking YOLO11 Model with CLI\nDESCRIPTION: This CLI command benchmarks a YOLO11 model to evaluate its performance metrics such as inference time and accuracy. Users can adjust parameters to fit their specific deployment scenarios. Requires a pre-trained model and dataset.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/index.md#2025-04-22_snippet_7\n\nLANGUAGE: Bash\nCODE:\n```\nyolo benchmark model=yolo11n.pt data='coco8.yaml' imgsz=640 half=False device=0\n```\n\n----------------------------------------\n\nTITLE: Calculating and Plotting Similarity Index in LanceDB Explorer\nDESCRIPTION: This snippet demonstrates how to calculate a similarity index using LanceDB Explorer and plot the results. It estimates how similar each data point is to the rest of the dataset based on image embeddings.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/explorer.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nsim_idx = exp.similarity_index(max_dist=0.2, top_k=0.01)\nexp.plot_similarity_index(max_dist=0.2, top_k=0.01)\n```\n\n----------------------------------------\n\nTITLE: Installing YOLOv5 Requirements with pip\nDESCRIPTION: Installs the necessary dependencies for running YOLOv5 models from PyTorch Hub. Requires Python>=3.8.0 and PyTorch>=1.8.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/pytorch_hub_model_loading.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Displaying Predicted Bounding Boxes on Test Images in Python\nDESCRIPTION: This function loads and displays the first ten images from the test set with their predicted bounding boxes. It uses the Image module from PIL to open and display the images.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/ibm-watsonx.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Show the first ten images from the preceding prediction task\nfor pred_dir in glob.glob(f\"{work_dir}/runs/detect/predict/*.jpg\")[:10]:\n    img = Image.open(pred_dir)\n    display(img)\n```\n\n----------------------------------------\n\nTITLE: Training YOLOv5 with Custom Port for Distributed Training\nDESCRIPTION: Command to train YOLOv5 using a custom port for distributed training. This is useful when the default port is already in use.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/multi_gpu_training.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython -m torch.distributed.run --master_port 1234 --nproc_per_node 2 ...\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Model with Dog-Pose Dataset via CLI (FAQ Example)\nDESCRIPTION: Command-line instruction from the FAQ section for training a YOLO11n-pose model on the Dog-pose dataset for 100 epochs with an image size of 640.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/dog-pose.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nyolo pose train data=dog-pose.yaml model=yolo11n-pose.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Annotating Bounding Boxes with Ultralytics - Python\nDESCRIPTION: This snippet uses the `Annotator` from `ultralytics.utils.plotting` to draw horizontal bounding boxes and labels on an image using OpenCV. It demonstrates label assignment and drawing colored boxes based on class indices. Required dependencies include `cv2` and `numpy`, and the key output is the annotated image.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/simple-utilities.md#2025-04-22_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2 as cv\nimport numpy as np\n\nfrom ultralytics.utils.plotting import Annotator, colors\n\nnames = {\n    0: \"person\",\n    5: \"bus\",\n    11: \"stop sign\",\n}\n\nimage = cv.imread(\"ultralytics/assets/bus.jpg\")\nann = Annotator(\n    image,\n    line_width=None,  # default auto-size\n    font_size=None,  # default auto-size\n    font=\"Arial.ttf\",  # must be ImageFont compatible\n    pil=False,  # use PIL, otherwise uses OpenCV\n)\n\nxyxy_boxes = np.array(\n    [\n        [5, 22.878, 231.27, 804.98, 756.83],  # class-idx x1 y1 x2 y2\n        [0, 48.552, 398.56, 245.35, 902.71],\n        [0, 669.47, 392.19, 809.72, 877.04],\n        [0, 221.52, 405.8, 344.98, 857.54],\n        [0, 0, 550.53, 63.01, 873.44],\n        [11, 0.0584, 254.46, 32.561, 324.87],\n    ]\n)\n\nfor nb, box in enumerate(xyxy_boxes):\n    c_idx, *box = box\n    label = f\"{str(nb).zfill(2)}:{names.get(int(c_idx))}\"\n    ann.box_label(box, label, color=colors(c_idx, bgr=True))\n\nimage_with_bboxes = ann.result()\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n on Signature Detection Dataset using Python\nDESCRIPTION: Python code for training a YOLO11n model on the signature detection dataset for 100 epochs with 640px image size. The code loads a pretrained model and configures the training parameters.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/signature.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"signature.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Performing Object Detection with YOLOv10 in Python\nDESCRIPTION: Demonstrates how to load a pre-trained YOLOv10n model using the Ultralytics Python library and perform object detection on a single image. The results object contains detection details, and `results[0].show()` displays the image with bounding boxes.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov10.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom ultralytics import YOLO\n\n# Load a pre-trained YOLOv10n model\nmodel = YOLO(\"yolov10n.pt\")\n\n# Perform object detection on an image\nresults = model(\"image.jpg\")\n\n# Display the results\nresults[0].show()\n```\n```\n\n----------------------------------------\n\nTITLE: Enabling Multi-scale Training for YOLO11 in Python\nDESCRIPTION: Activates multi-scale training for YOLO11 by setting the scale parameter. This example randomly adjusts training image sizes between 0.5x and 1.5x the original size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/model-training-tips.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nscale=0.5\n```\n\n----------------------------------------\n\nTITLE: Exporting and Using YOLO11 Model in TensorFlow.js Format (Python)\nDESCRIPTION: This snippet demonstrates how to load a YOLO11 model, export it to TensorFlow.js format, and then use the exported model for inference. It includes steps for loading the original model, exporting it, and running inference with the exported TF.js model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tfjs.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to TF.js format\nmodel.export(format=\"tfjs\")  # creates '/yolo11n_web_model'\n\n# Load the exported TF.js model\ntfjs_model = YOLO(\"./yolo11n_web_model\")\n\n# Run inference\nresults = tfjs_model(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO Model to RKNN Format via CLI\nDESCRIPTION: This command-line interface (CLI) command exports a YOLO model to RKNN format for a specific Rockchip platform. It uses the 'yolo export' command with the model file, format, and target platform name as arguments.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/rockchip-rknn.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=yolo11n.pt format=rknn name=rk3588\n```\n\n----------------------------------------\n\nTITLE: Example JSON Response for YOLO Detection Model\nDESCRIPTION: This JSON object represents a typical response from the Ultralytics API for a YOLO detection model. It includes detection results with class, confidence, and bounding box coordinates, as well as image shape and processing speed information.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/hub/inference-api.md#2025-04-22_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"images\": [\n    {\n      \"results\": [\n        {\n          \"class\": 0,\n          \"name\": \"person\",\n          \"confidence\": 0.92,\n          \"box\": {\n            \"x1\": 118,\n            \"x2\": 416,\n            \"y1\": 112,\n            \"y2\": 660\n          }\n        }\n      ],\n      \"shape\": [\n        750,\n        600\n      ],\n      \"speed\": {\n        \"inference\": 200.8,\n        \"postprocess\": 0.8,\n        \"preprocess\": 2.8\n      }\n    }\n  ],\n  \"metadata\": ...\n}\n```\n\n----------------------------------------\n\nTITLE: Optimizing YOLO11 Batch Size with Autobatch in Python\nDESCRIPTION: This code snippet shows how to use the autobatch feature from Ultralytics to determine the optimal batch size for training a YOLO11 model. This is useful for optimizing model performance and memory usage.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/jupyterlab.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.utils.autobatch import autobatch\n\noptimal_batch_size = autobatch(model)\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on Carparts Segmentation Dataset via CLI\nDESCRIPTION: Command-line interface commands for training, validating, and predicting with an Ultralytics YOLO model on the Carparts Segmentation dataset.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/carparts-seg.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model using the Command Line Interface\n# Specify the dataset config file, model, number of epochs, and image size\nyolo segment train data=carparts-seg.yaml model=yolo11n-seg.pt epochs=100 imgsz=640\n\n# Validate the trained model using the validation set\n# yolo segment val model=path/to/best.pt\n\n# Predict using the trained model on a specific image source\n# yolo segment predict model=path/to/best.pt source=path/to/your/image.jpg\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Segmentation Model via CLI\nDESCRIPTION: Command-line interface command for training a YOLO segmentation model using a pre-trained model and custom dataset.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/index.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyolo segment train data=coco8-seg.yaml model=yolo11n-seg.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Predicting with YOLO11 using Ultralytics CLI on AzureML\nDESCRIPTION: This command uses the Ultralytics CLI to run a prediction task with a pre-trained YOLO11 model on a sample image. It demonstrates how to use the CLI for quick tasks directly from the terminal.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/azureml-quickstart.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nyolo predict model=yolo11n.pt source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Line Graph Generation Example\nDESCRIPTION: Example code showing how to create line graphs using Ultralytics YOLO11 Analytics. Demonstrates video processing and real-time line graph updates.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/analytics.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nassert cap.isOpened(), \"Error reading video file\"\n\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n\nout = cv2.VideoWriter(\n    \"ultralytics_analytics.avi\",\n    cv2.VideoWriter_fourcc(*\"MJPG\"),\n    fps,\n    (1280, 720),  # this is fixed\n)\n\nanalytics = solutions.Analytics(\n    analytics_type=\"line\",\n    show=True,\n)\n\nframe_count = 0\nwhile cap.isOpened():\n    success, im0 = cap.read()\n    if success:\n        frame_count += 1\n        results = analytics(im0, frame_count)  # update analytics graph every frame\n        out.write(results.plot_im)  # write the video file\n    else:\n        break\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Segmenting Images with Prompts using SAM - Python\nDESCRIPTION: This snippet demonstrates loading the SAM model through the Ultralytics Python API, then performing inference on an image using various forms of prompts: bounding boxes, single/multiple points, points per object, and negative prompts. It depends on the 'ultralytics' Python package and a downloaded model weight (e.g., 'sam_b.pt'). The 'model' object takes image path or prompt data as input and returns a 'Results' object containing segmentation outputs. Required parameters are 'bboxes', 'points', or 'labels', flexible in type (list or nested lists). Outputs are segmentation masks aligned with prompts. Limitations include requiring pre-trained weights and valid image input.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/sam.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import SAM\n\n# Load a model\nmodel = SAM(\"sam_b.pt\")\n\n# Display model information (optional)\nmodel.info()\n\n# Run inference with bboxes prompt\nresults = model(\"ultralytics/assets/zidane.jpg\", bboxes=[439, 437, 524, 709])\n\n# Run inference with single point\nresults = model(points=[900, 370], labels=[1])\n\n# Run inference with multiple points\nresults = model(points=[[400, 370], [900, 370]], labels=[1, 1])\n\n# Run inference with multiple points prompt per object\nresults = model(points=[[[400, 370], [900, 370]]], labels=[[1, 1]])\n\n# Run inference with negative points prompt\nresults = model(points=[[[400, 370], [900, 370]]], labels=[[1, 0]])\n\n```\n\n----------------------------------------\n\nTITLE: Installing YOLOv7 - Git Clone Command\nDESCRIPTION: Command to clone the YOLOv7 repository from GitHub to get started with implementation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov7.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/WongKinYiu/yolov7\n```\n\n----------------------------------------\n\nTITLE: Benchmarking YOLO Models on GPU via Python API (FAQ Example)\nDESCRIPTION: This Python code snippet, presented in the FAQ section, shows how to invoke the `benchmark` function from the Ultralytics library to evaluate a YOLOv11 model (`yolo11n.pt`) on a specific dataset (`coco8.yaml`). It specifies the image size, disables half-precision (`half=False`), and targets the first GPU (`device=0`).\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/benchmark.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.utils.benchmarks import benchmark\n\n# Benchmark on GPU\nbenchmark(model=\"yolo11n.pt\", data=\"coco8.yaml\", imgsz=640, half=False, device=0)\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n Model on Medical Pills Dataset via CLI\nDESCRIPTION: This command-line instruction shows how to train a YOLO11n model on the medical-pills dataset using the CLI, specifying epochs and image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/medical-pills.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo detect train data=medical-pills.yaml model=yolo11n.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Defining Instances Class for Object Instance Handling in Python\nDESCRIPTION: The Instances class is probably designed to manage multiple object instances detected in an image, potentially including their bounding boxes, classes, and other attributes relevant to object detection results.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/instance.md#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nclass Instances:\n```\n\n----------------------------------------\n\nTITLE: Converting COCO to YOLO Format\nDESCRIPTION: Python code snippet for converting labels from the COCO dataset format to the YOLO format for segmentation tasks.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/index.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.data.converter import convert_coco\n\nconvert_coco(labels_dir=\"path/to/coco/annotations/\", use_segments=True)\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLOv8 to OpenVINO using CLI\nDESCRIPTION: This Bash command shows how to export a YOLOv8 PyTorch model (`.pt` file) to the OpenVINO format using the `yolo` command-line interface provided by the `ultralytics` package. The `export` command requires specifying the input `model` file and the desired output `format` ('openvino'). This process generates an output directory (`yolov8n_openvino_model/`).\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/openvino.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# Export a YOLOv8n PyTorch model to OpenVINO format\nyolo export model=yolov8n.pt format=openvino # creates 'yolov8n_openvino_model/'\n```\n\n----------------------------------------\n\nTITLE: Viewing Ultralytics Settings via Python and CLI - Python & Bash\nDESCRIPTION: These code snippets show how to inspect the persistent settings used by Ultralytics through both Python and CLI. The Python approach leverages the 'settings' object from 'ultralytics', supporting print and dictionary-style access. The CLI command displays all settings in the terminal. No external dependencies are required beyond Ultralytics. These operations are read-only and return either the full settings object or a specific key's value.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/quickstart.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import settings\n\n# View all settings\nprint(settings)\n\n# Return a specific setting\nvalue = settings[\"runs_dir\"]\n```\n\nLANGUAGE: bash\nCODE:\n```\nyolo settings\n```\n\n----------------------------------------\n\nTITLE: Embedding YouTube Video in HTML\nDESCRIPTION: This HTML snippet embeds a YouTube video explaining how to do computer vision projects step-by-step. It uses an iframe with specific attributes for video embedding.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/steps-of-a-cv-project.md#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<p align=\"center\">\n  <br>\n  <iframe loading=\"lazy\" width=\"720\" height=\"405\" src=\"https://www.youtube.com/embed/CfbHwPG01cE\"\n    title=\"YouTube video player\" frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    allowfullscreen>\n  </iframe>\n  <br>\n  <strong>Watch:</strong> How to Do <a href=\"https://www.ultralytics.com/glossary/computer-vision-cv\">Computer Vision</a> Projects | A Step-by-Step Guide\n</p>\n```\n\n----------------------------------------\n\nTITLE: Running YOLO11 SAHI Video Inference Script (Bash)\nDESCRIPTION: Executes the `yolov8_sahi.py` Python script to perform object detection on a video file using the SAHI technique with a YOLO11 model. Demonstrates basic usage with `--source` and `--save-img`, specifying a custom model weight file (`--weights`), and adjusting SAHI slice dimensions (`--slice-height`, `--slice-width`) for potentially improved small object detection. Assumes the necessary libraries are installed and the script is present in the current directory.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-SAHI-Inference-Video/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Run inference and save the output video with bounding boxes\npython yolov8_sahi.py --source \"path/to/your/video.mp4\" --save-img\n\n# Run inference using a specific YOLO11 model (e.g., yolo11n.pt) and save results\npython yolov8_sahi.py --source \"path/to/your/video.mp4\" --save-img --weights \"yolo11n.pt\"\n\n# Run inference with smaller slices for potentially better small object detection\npython yolov8_sahi.py --source \"path/to/your/video.mp4\" --save-img --slice-height 512 --slice-width 512\n```\n\n----------------------------------------\n\nTITLE: Checking Detection Dataset Structure and Validity - Python\nDESCRIPTION: The check_det_dataset utility analyzes the organization and contents of an object detection dataset, verifying file structure, class definitions, and the presence of images/labels. Inputs are dataset directories; output is a report or set of validation errors. Assumes datasets follow Ultralytics or YOLOv5/v8 conventions.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/data/utils.md#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n## ::: ultralytics.data.utils.check_det_dataset\n```\n\n----------------------------------------\n\nTITLE: CLI Commands for YOLOv3u Training and Inference\nDESCRIPTION: This snippet provides CLI commands to train a YOLOv3u model and perform inference. The commands require a terminal environment with the YOLO CLI tool installed. Inputs include file paths for the pretrained model and data, with outputs displaying results in the console.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov3.md#2025-04-22_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\n# Load a COCO-pretrained YOLOv3u model and train it on the COCO8 example dataset for 100 epochs\nyolo train model=yolov3u.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Load a COCO-pretrained YOLOv3u model and run inference on the 'bus.jpg' image\nyolo predict model=yolov3u.pt source=path/to/bus.jpg\n```\n\n----------------------------------------\n\nTITLE: Training YOLOv5 Models from Scratch\nDESCRIPTION: Commands for training YOLOv5 models of different architectures (s, m, l, x) from scratch without pretrained weights. Recommended for large datasets like COCO, Objects365, or OIv6.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/tips_for_best_training_results.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython train.py --data custom.yaml --weights '' --cfg yolov5s.yaml\npython train.py --data custom.yaml --weights '' --cfg yolov5m.yaml\npython train.py --data custom.yaml --weights '' --cfg yolov5l.yaml\npython train.py --data custom.yaml --weights '' --cfg yolov5x.yaml\n```\n\n----------------------------------------\n\nTITLE: Displaying Image in Terminal using Sixel\nDESCRIPTION: Code to create SixelWriter instance and display the image in terminal\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/view-results-in-terminal.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom sixel import SixelWriter\n\n# Create sixel writer object\nw = SixelWriter()\n\n# Draw the sixel image in the terminal\nw.draw(mem_file)\n```\n\n----------------------------------------\n\nTITLE: Automatically Determining Optimal Batch Size in YOLOv5 (Shell)\nDESCRIPTION: This snippet demonstrates how to use the YOLOv5 training script with the '--batch-size -1' flag, which automatically selects the optimal batch size based on available GPU memory. No additional dependencies are required beyond the YOLOv5 framework and a compatible CUDA environment. Input parameters include the '--batch-size' flag set to '-1', and the output is an efficiently selected batch size for your training session.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/tips_for_best_training_results.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n--batch-size -1\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Segmentation Model with Carparts Dataset in Python\nDESCRIPTION: This snippet demonstrates how to load a pretrained YOLO11 segmentation model and train it using the Carparts Segmentation Dataset. It uses the 'yolo11n-seg.pt' model and 'carparts-seg.yaml' configuration file for 100 epochs with 640x640 image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/carparts-seg.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-seg.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"carparts-seg.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Testing Baseline YOLOv5 Performance\nDESCRIPTION: Command to establish a baseline performance by testing YOLOv5x on COCO val2017 dataset at 640px image size with half-precision. This creates a reference point for comparing with pruned models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/model_pruning_and_sparsity.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython val.py --weights yolov5x.pt --data coco.yaml --img 640 --half\n```\n\n----------------------------------------\n\nTITLE: Importing RT-DETR Model Class - Python\nDESCRIPTION: Module import path reference for the RT-DETR (Real-Time Detection Transformer) model implementation in the Ultralytics framework. This class implements Baidu's real-time object detector with Vision Transformer architecture.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/models/rtdetr/model.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics.models.rtdetr.model import RTDETR\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on ImageNette Dataset using Python\nDESCRIPTION: This code snippet demonstrates how to load a pretrained YOLO model and train it on the ImageNette dataset for 100 epochs with a standard image size of 224x224.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/imagenette.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-cls.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"imagenette\", epochs=100, imgsz=224)\n```\n\n----------------------------------------\n\nTITLE: Loading Ray Tune Experiment Results after Tuning YOLO11 Python\nDESCRIPTION: This snippet loads Ray Tune tuning experiment results from a specified storage path and experiment name, useful for post-run analysis. Required dependencies are ray and the definition of train_mnist (or substitute trainable) if restoring. The primary input is the experiment directory. Outputs are the restored Ray Tune tuner object and result grid. This is intended for after experiment completion or for analyses outside the original training script.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/ray-tune.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nexperiment_path = f\"{storage_path}/{exp_name}\"\\nprint(f\"Loading results from {experiment_path}...\")\\n\\nrestored_tuner = tune.Tuner.restore(experiment_path, trainable=train_mnist)\\nresult_grid = restored_tuner.get_results()\n```\n\n----------------------------------------\n\nTITLE: Running YOLO11 Prediction\nDESCRIPTION: Executes image prediction using a pre-trained YOLO11 model on a sample image.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/azureml-quickstart.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nyolo predict model=yolo11n.pt source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Training YOLOv5 on Single GPU\nDESCRIPTION: Command to train YOLOv5 model on a single GPU. It specifies the batch size, dataset, pre-trained weights, and the GPU device to use.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/multi_gpu_training.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython train.py --batch 64 --data coco.yaml --weights yolov5s.pt --device 0\n```\n\n----------------------------------------\n\nTITLE: Profiling SAM and YOLO Models in Python\nDESCRIPTION: This Python code snippet demonstrates how to profile various SAM and YOLO models using the Ultralytics framework. It requires the 'ultralytics' library and specific model weights available as files. The snippet profiles SAM and YOLO models, outputting their information and processing given assets.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/sam.md#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics import ASSETS, SAM, YOLO, FastSAM\n\n# Profile SAM2-t, SAM2-b, SAM-b, MobileSAM\nfor file in [\"sam_b.pt\", \"sam2_b.pt\", \"sam2_t.pt\", \"mobile_sam.pt\"]:\n    model = SAM(file)\n    model.info()\n    model(ASSETS)\n\n# Profile FastSAM-s\nmodel = FastSAM(\"FastSAM-s.pt\")\nmodel.info()\nmodel(ASSETS)\n\n# Profile YOLO models\nfor file_name in [\"yolov8n-seg.pt\", \"yolo11n-seg.pt\"]:\n    model = YOLO(file_name)\n    model.info()\n    model(ASSETS)\n```\n\n----------------------------------------\n\nTITLE: Using Pretrained Open Images V7 Models with Python\nDESCRIPTION: Example code demonstrating how to load an Open Images V7 pretrained YOLOv8n model, run prediction on an image, and start training from the pretrained checkpoint using Python.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/open-images-v7.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load an Open Images Dataset V7 pretrained YOLOv8n model\nmodel = YOLO(\"yolov8n-oiv7.pt\")\n\n# Run prediction\nresults = model.predict(source=\"image.jpg\")\n\n# Start training from the pretrained checkpoint\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n-seg Model on COCO8-Seg in Python\nDESCRIPTION: Python code example demonstrating how to load a pretrained YOLO11n-seg model and train it on the COCO8-Seg dataset for 100 epochs with 640px images.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/coco8-seg.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-seg.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"coco8-seg.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Plotting Tracks Over Time with YOLO and OpenCV\nDESCRIPTION: This Python script demonstrates how to use YOLO's tracking capabilities to plot the movement of detected objects across consecutive frames. It uses OpenCV to read video frames, YOLO for object detection and tracking, and NumPy for numerical operations.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/trackers/README.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom collections import defaultdict\n\nimport cv2\nimport numpy as np\n\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Open the video file\nvideo_path = \"path/to/video.mp4\"\ncap = cv2.VideoCapture(video_path)\n\n# Store the track history\ntrack_history = defaultdict(lambda: [])\n```\n\n----------------------------------------\n\nTITLE: Running YOLO Inference with Multiple Edge TPUs\nDESCRIPTION: Python code demonstrating how to select specific Edge TPUs when running inference with multiple devices available.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/coral-edge-tpu-on-raspberry-pi.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"path/to/<model_name>_full_integer_quant_edgetpu.tflite\")  # Load an official model or custom model\n\n# Run Prediction\nmodel.predict(\"path/to/source.png\")  # Inference defaults to the first TPU\n\nmodel.predict(\"path/to/source.png\", device=\"tpu:0\")  # Select the first TPU\n\nmodel.predict(\"path/to/source.png\", device=\"tpu:1\")  # Select the second TPU\n```\n\n----------------------------------------\n\nTITLE: Training and Inference with Ultralytics CLI\nDESCRIPTION: This bash code snippet demonstrates how to train a YOLOE model on a custom dataset and perform inference with text prompts using the Ultralytics CLI. It requires the Ultralytics software and a dataset formatted according to the requirements. Key inputs include model path and dataset path, while the output includes annotated images displayed or saved as per command configuration.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yoloe.md#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\n# Training YOLOE on custom dataset\nyolo train model=yoloe-11s.pt data=path/to/data.yaml epochs=50 imgsz=640\n\n# Inference with text prompts\nyolo predict model=yoloe-11s.pt source=\"test_images/street.jpg\" classes=\"person,bus\"\n\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model to ONNX Format\nDESCRIPTION: Command to export a YOLO11 model to ONNX format, which is required for compatibility with DeepSparse.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/neural-magic.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyolo task=detect mode=export model=yolo11n.pt format=onnx opset=13\n```\n\n----------------------------------------\n\nTITLE: Running YOLOv8 Region Counter\nDESCRIPTION: Various command-line options for running the region counter script with different configurations including video source, device selection, and model weights.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-Region-Counter/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Run inference on a video source, saving results and viewing output\npython yolov8_region_counter.py --source \"path/to/video.mp4\" --save-img --view-img\n\n# Run inference using the CPU\npython yolov8_region_counter.py --source \"path/to/video.mp4\" --save-img --view-img --device cpu\n\n# Use a specific Ultralytics YOLOv8 model file\npython yolov8_region_counter.py --source \"path/to/video.mp4\" --save-img --weights \"path/to/yolov8n.pt\"\n\n# Detect only specific classes (e.g., class 0 and class 2)\npython yolov8_region_counter.py --source \"path/to/video.mp4\" --classes 0 2 --weights \"path/to/yolov8m.pt\"\n\n# Run inference without saving the output video/images\npython yolov8_region_counter.py --source \"path/to/video.mp4\" --view-img\n```\n\n----------------------------------------\n\nTITLE: Segmenting an Entire Image using SAM - CLI\nDESCRIPTION: This CLI command allows running SAM model inference for segmenting an entire image without any prompt, using the YOLO CLI utility. It assumes 'yolo' is installed and available in the environment, as well as SAM weights (e.g., 'sam_b.pt'). The 'source' argument is the image path, 'model' gives the checkpoint. This outputs a segmented version of the input image, typically written to disk in the working directory. Limitation: Only segmentation is supported, and appropriate weights must be present.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/sam.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Run inference with a SAM model\nyolo predict model=sam_b.pt source=path/to/image.jpg\n\n```\n\n----------------------------------------\n\nTITLE: Initializing Comet ML Project in Python\nDESCRIPTION: Imports comet_ml and logs into a specific project for tracking experiments.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/comet.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport comet_ml\n\ncomet_ml.login(project_name=\"comet-example-yolo11-coco128\")\n```\n\n----------------------------------------\n\nTITLE: Oriented Bounding Box Annotation with Ultralytics - Python\nDESCRIPTION: Illustrates the use of the `Annotator` for drawing oriented bounding boxes on an image. This snippet reads an image, defines oriented bounding boxes, and labels them with class names using the Ultralytics library. Key dependencies are `cv2` and `numpy`, and the output is the image with annotated oriented bounding boxes.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/simple-utilities.md#2025-04-22_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2 as cv\nimport numpy as np\n\nfrom ultralytics.utils.plotting import Annotator, colors\n\nobb_names = {10: \"small vehicle\"}\nobb_image = cv.imread(\"datasets/dota8/images/train/P1142__1024__0___824.jpg\")\nobb_boxes = np.array(\n    [\n        [0, 635, 560, 919, 719, 1087, 420, 803, 261],  # class-idx x1 y1 x2 y2 x3 y2 x4 y4\n        [0, 331, 19, 493, 260, 776, 70, 613, -171],\n        [9, 869, 161, 886, 147, 851, 101, 833, 115],\n    ]\n)\nann = Annotator(\n    obb_image,\n    line_width=None,  # default auto-size\n    font_size=None,  # default auto-size\n    font=\"Arial.ttf\",  # must be ImageFont compatible\n    pil=False,  # use PIL, otherwise uses OpenCV\n)\nfor obb in obb_boxes:\n    c_idx, *obb = obb\n    obb = np.array(obb).reshape(-1, 4, 2).squeeze()\n    label = f\"{obb_names.get(int(c_idx))}\"\n    ann.box_label(\n        obb,\n        label,\n        color=colors(c_idx, True),\n        rotated=True,\n    )\n\nimage_with_obb = ann.result()\n```\n\n----------------------------------------\n\nTITLE: Converting YOLOv5 Results to Base64 for API Services\nDESCRIPTION: Demonstrates how to convert YOLOv5 inference results to Base64 encoded images with detection boxes rendered, which is useful for web and API services that need to transfer image data efficiently.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/pytorch_hub_model_loading.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport base64\nfrom io import BytesIO\n\nfrom PIL import Image\n\nresults = model(im)  # inference\n\nresults.ims  # array of original images (as np array) passed to model for inference\nresults.render()  # updates results.ims with boxes and labels\nfor im in results.ims:\n    buffered = BytesIO()\n    im_base64 = Image.fromarray(im)\n    im_base64.save(buffered, format=\"JPEG\")\n    print(base64.b64encode(buffered.getvalue()).decode(\"utf-8\"))  # base64 encoded image with results\n```\n\n----------------------------------------\n\nTITLE: Performing Inference and Exporting Results as JSON using PyTorch\nDESCRIPTION: This snippet shows how to perform inference with a YOLOv5 model using PyTorch, and then convert the results into a JSON format using pandas' .to_json() function. The example highlights the ability to modify the JSON output with the orient argument.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/pytorch_hub_model_loading.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nresults = model(ims)  # inference\nresults.pandas().xyxy[0].to_json(orient=\"records\")  # JSON img1 predictions\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLOv5 Model to TorchScript and ONNX\nDESCRIPTION: Export a pretrained YOLOv5s model to TorchScript and ONNX formats using the export.py script.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/model_export.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython export.py --weights yolov5s.pt --include torchscript onnx\n```\n\n----------------------------------------\n\nTITLE: Validating YOLOv5 Model on COCO128 Dataset in AzureML\nDESCRIPTION: This snippet demonstrates how to validate a trained YOLOv5s model on the COCO128 validation set. It specifies the model weights, dataset configuration, and image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/environments/azureml_quickstart_tutorial.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Validate the yolov5s model on the COCO128 validation set\npython val.py --weights yolov5s.pt --data coco128.yaml --img 640\n```\n\n----------------------------------------\n\nTITLE: Convert Bounding Boxes to Segments\nDESCRIPTION: Function to convert YOLO format bounding boxes to segments using SAM model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/simple-utilities.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.data.converter import yolo_bbox2segment\n\nyolo_bbox2segment(\n    im_dir=\"path/to/images\",\n    save_dir=None,  # saved to \"labels-segment\" in images directory\n    sam_model=\"sam_b.pt\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Dataset in YAML Format for YOLOv5\nDESCRIPTION: Example YAML configuration file (coco128.yaml) that defines dataset paths and class names. The file specifies training and validation image directories along with the class mapping for object detection.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/train_custom_data.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# Dataset root directory relative to the yolov5 directory\npath: ../datasets/coco128\n\n# Train/val/test sets: specify directories, *.txt files, or lists\ntrain: images/train2017 # 128 images for training\nval: images/train2017 # 128 images for validation\ntest: # Optional path to test images\n\n# Classes (example using 80 COCO classes)\nnames:\n    0: person\n    1: bicycle\n    2: car\n    # ... (remaining COCO classes)\n    77: teddy bear\n    78: hair drier\n    79: toothbrush\n```\n\n----------------------------------------\n\nTITLE: Custom Inference Prompts with YOLO-World Python API\nDESCRIPTION: Demonstrates how to set custom inference classes ('person' and 'bus') for a YOLO-World model using the Python API. Prerequisites are the 'ultralytics' package and a compatible YOLO-World model weights file. The 'set_classes' method restricts predictions to the specified classes. The model's 'predict' method processes a given image, and results can be displayed using '.show()'. Useful for tailored object detection without retraining.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolo-world.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Initialize a YOLO-World model\nmodel = YOLO(\"yolov8s-world.pt\")  # or choose yolov8m/l-world.pt\n\n# Define custom classes\nmodel.set_classes([\"person\", \"bus\"])\n\n# Execute prediction for specified categories on an image\nresults = model.predict(\"path/to/image.jpg\")\n\n# Show results\nresults[0].show()\n```\n\n----------------------------------------\n\nTITLE: SQL Querying with Ultralytics Explorer\nDESCRIPTION: Shows how to perform SQL queries on the dataset using the sql_query method in Ultralytics Explorer.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/api.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import Explorer\n\n# create an Explorer object\nexp = Explorer(data=\"coco128.yaml\", model=\"yolo11n.pt\")\nexp.create_embeddings_table()\n\ndf = exp.sql_query(\"WHERE labels LIKE '%person%' AND labels LIKE '%dog%'\")\nprint(df.head())\n```\n\n----------------------------------------\n\nTITLE: Converting YOLO11n Model to NCNN and Running Inference (CLI)\nDESCRIPTION: CLI commands to export a YOLO11n model to NCNN format and run inference using the exported model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/raspberry-pi.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Export a YOLO11n PyTorch model to NCNN format\nyolo export model=yolo11n.pt format=ncnn # creates 'yolo11n_ncnn_model'\n\n# Run inference with the exported model\nyolo predict model='yolo11n_ncnn_model' source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 model using Python interface on AzureML\nDESCRIPTION: This Python code snippet shows how to use the Ultralytics Python interface to load a pre-trained YOLO11 model and train it on a custom dataset. It's suitable for more complex tasks requiring custom coding and integration within notebooks.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/azureml-quickstart.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolo11n.pt\")\nmodel.train(data=\"coco8.yaml\", epochs=3)\n```\n\n----------------------------------------\n\nTITLE: Training a YOLO Detection Model in Bash\nDESCRIPTION: Command to train a detection model for 10 epochs with a custom learning rate of 0.01 using the COCO8 dataset.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/cli.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n```\n\n----------------------------------------\n\nTITLE: Benchmarking YOLO11n Model via CLI\nDESCRIPTION: This command-line instruction shows how to benchmark the YOLO11n model's speed and accuracy on the COCO8 dataset for all export formats using the CLI.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/raspberry-pi.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# Benchmark YOLO11n speed and accuracy on the COCO8 dataset for all all export formats\nyolo benchmark model=yolo11n.pt data=coco8.yaml imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Testing YOLO11 Model on Marine Litter Dataset in CLI\nDESCRIPTION: This command-line instruction runs inference on the test set using the trained YOLO11 model. It specifies the task, mode, source images, model weights, confidence threshold, and output settings.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/ibm-watsonx.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n!yolo task=detect mode=predict source={work_dir}/trash_ICRA19/dataset/test/images model={work_dir}/runs/detect/train/weights/best.pt conf=0.5 iou=.5 save=True save_txt=True\n```\n\n----------------------------------------\n\nTITLE: Training YOLOv5 with Frozen Backbone on Pascal VOC Dataset\nDESCRIPTION: This bash command demonstrates how to train YOLOv5m on the Pascal VOC dataset for 50 epochs, starting from COCO pre-trained weights, with the backbone frozen. It uses specific hyperparameters for fine-tuning.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/transfer_learning_with_frozen_layers.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython train.py --batch 48 --weights yolov5m.pt --data voc.yaml --epochs 50 --cache --img 512 --hyp hyp.finetune.yaml --freeze 10\n```\n\n----------------------------------------\n\nTITLE: Inference with Raspberry Pi Camera using picamera2\nDESCRIPTION: This Python script demonstrates how to use picamera2 to access the Raspberry Pi camera and perform inference with a YOLO11n model on the captured frames.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/raspberry-pi.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nfrom picamera2 import Picamera2\n\nfrom ultralytics import YOLO\n\n# Initialize the Picamera2\npicam2 = Picamera2()\npicam2.preview_configuration.main.size = (1280, 720)\npicam2.preview_configuration.main.format = \"RGB888\"\npicam2.preview_configuration.align()\npicam2.configure(\"preview\")\npicam2.start()\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\nwhile True:\n    # Capture frame-by-frame\n    frame = picam2.capture_array()\n\n    # Run YOLO11 inference on the frame\n    results = model(frame)\n\n    # Visualize the results on the frame\n    annotated_frame = results[0].plot()\n\n    # Display the resulting frame\n    cv2.imshow(\"Camera\", annotated_frame)\n\n    # Break the loop if 'q' is pressed\n    if cv2.waitKey(1) == ord(\"q\"):\n        break\n\n# Release resources and close windows\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Defining Dataset Structure in YAML for Medical Pills Detection\nDESCRIPTION: This YAML configuration file defines the structure of the medical-pills dataset, including paths and classes. It is essential for setting up the dataset for training YOLO models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/medical-pills.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/medical-pills.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Logging YOLOv5 Model Checkpoints to Comet\nDESCRIPTION: Command to train YOLOv5 with checkpoint saving enabled, which logs model checkpoints to Comet at the specified interval (every epoch).\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/comet_logging_integration.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npython train.py \\\n  --img 640 \\\n  --batch 16 \\\n  --epochs 5 \\\n  --data coco128.yaml \\\n  --weights yolov5s.pt \\\n  --save-period 1\n```\n\n----------------------------------------\n\nTITLE: Setting Up TCP Camera Stream for YOLO11 Inference\nDESCRIPTION: Two-step process to use a Raspberry Pi camera with YOLO11 via TCP streaming: first starting the camera stream and then connecting to it for inference.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/raspberry-pi.md#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nrpicam-vid -n -t 0 --inline --listen -o tcp://127.0.0.1:8888\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolo11n.pt\")\nresults = model(\"tcp://127.0.0.1:8888\")\n```\n\n----------------------------------------\n\nTITLE: Exporting Custom-Trained YOLO Model to ONNX Format in Bash\nDESCRIPTION: Command to export a custom-trained YOLO model to the ONNX format using the best weights file.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/cli.md#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=path/to/best.pt format=onnx\n```\n\n----------------------------------------\n\nTITLE: Running YOLOv5 test with TTA\nDESCRIPTION: Perform Test-Time Augmentation by adding the --augment flag and increasing image size to 832px.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/test_time_augmentation.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython val.py --weights yolov5x.pt --data coco.yaml --img 832 --augment --half\n```\n\n----------------------------------------\n\nTITLE: Running Inference with OpenVINO YOLOv8 using CLI\nDESCRIPTION: This Bash command demonstrates running inference using the `yolo` command-line tool with a YOLOv8 model previously exported to OpenVINO format. The `predict` command takes the path to the exported OpenVINO model directory as the `model` argument and the image source (URL in this case) as the `source` argument.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/openvino.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n# Run inference with the exported model\nyolo predict model=yolov8n_openvino_model source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Resuming a Ray Tune Hyperparameter Tuning Session with YOLO11 Python\nDESCRIPTION: This snippet shows how to resume a previously interrupted Ray Tune tuning run for YOLO11 models. You must pass resume=True (and optionally name=...) to model.tune; other necessary training parameters (data, epochs, etc.) must be restated. Required dependencies are ultralytics and ray. Inputs include model config and Ray Tune tuning directory, outputs are the resumed Ray Tune results.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/ray-tune.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\\n\\n# Define a YOLO model\\nmodel = YOLO(\"yolo11n.pt\")\\n\\n# Resume previous run\\nresults = model.tune(use_ray=True, data=\"coco8.yaml\", epochs=50, resume=True)\\n\\n# Resume Ray Tune run with name 'tune_exp_2'\\nresults = model.tune(use_ray=True, data=\"coco8.yaml\", epochs=50, name=\"tune_exp_2\", resume=True)\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model with K-Fold Data Splits in Python\nDESCRIPTION: This snippet iterates over the dataset YAML files to run training for each K-Fold split. It specifies training parameters such as batch size, project name, and number of epochs.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/kfold-cross-validation.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nresults = {}\n\n# Define your additional arguments here\nbatch = 16\nproject = \"kfold_demo\"\nepochs = 100\n\nfor k, dataset_yaml in enumerate(ds_yamls):\n    model = YOLO(weights_path, task=\"detect\")\n    results[k] = model.train(\n        data=dataset_yaml, epochs=epochs, batch=batch, project=project, name=f\"fold_{k + 1}\"\n    )  # include any additional train arguments\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLOv8 to OpenVINO Format and Running Inference via CLI\nDESCRIPTION: Shows the command-line interface commands for exporting a YOLOv8 model to OpenVINO format and running inference. Includes options for specifying target inference devices.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/openvino.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Export a YOLOv8n PyTorch model to OpenVINO format\nyolo export model=yolov8n.pt format=openvino # creates 'yolov8n_openvino_model/'\n\n# Run inference with the exported model\nyolo predict model=yolov8n_openvino_model source='https://ultralytics.com/images/bus.jpg'\n\n# Run inference with specified device, available devices: [\"intel:gpu\", \"intel:npu\", \"intel:cpu\"]\nyolo predict model=yolov8n_openvino_model source='https://ultralytics.com/images/bus.jpg' device=\"intel:gpu\"\n```\n\n----------------------------------------\n\nTITLE: CLI Command for YOLO Pose Training\nDESCRIPTION: Command line interface example for training a YOLO pose estimation model\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/index.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyolo pose train data=coco8-pose.yaml model=yolo11n-pose.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Training Model with YAML Configuration\nDESCRIPTION: Demonstrates how to initialize model training with a YAML configuration file, specifying the data path and batch size parameters.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/yolo-common-issues.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmodel.train(data=\"/path/to/your/data.yaml\", batch=4)\n```\n\n----------------------------------------\n\nTITLE: Validating YOLO-World Model in Python\nDESCRIPTION: This Python snippet demonstrates model validation using Ultralytics YOLO framework. It creates a YOLO-World model and validates it against the COCO8 dataset, returning performance metrics. The input is a model path and validation dataset.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolo-world.md#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Create a YOLO-World model\nmodel = YOLO(\"yolov8s-world.pt\")  # or select yolov8m/l-world.pt for different sizes\n\n# Conduct model validation on the COCO8 example dataset\nmetrics = model.val(data=\"coco8.yaml\")\n```\n\n----------------------------------------\n\nTITLE: Converting Polygon Annotations to Masks - Python\nDESCRIPTION: The polygon2mask utility generates a binary mask from a polygon annotation, aiding segmentation tasks. It consumes vertex coordinate lists as input, producing a mask of the specified size. Requires NumPy and potentially image processing libraries; limitations include handling of self-intersecting or malformed polygons.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/data/utils.md#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n## ::: ultralytics.data.utils.polygon2mask\n```\n\n----------------------------------------\n\nTITLE: Inference with TCP Stream via CLI\nDESCRIPTION: This command-line instruction shows how to run inference using a YOLO11n model on the TCP stream from the Raspberry Pi camera.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/raspberry-pi.md#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nyolo predict model=yolo11n.pt source=\"tcp://127.0.0.1:8888\"\n```\n\n----------------------------------------\n\nTITLE: Exporting PyTorch YOLO11 Model to ONNX Format for reCamera\nDESCRIPTION: This snippet shows how to export a custom-trained YOLO11 PyTorch model to ONNX format, which is the first step in preparing the model for use with reCamera. The export function sets the format to ONNX and specifies opset version 14.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/seeedstudio-recamera.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel.export(format=\"onnx\", opset=14)\n```\n\n----------------------------------------\n\nTITLE: Plotting Object Tracks Over Video Frames with YOLO\nDESCRIPTION: Implementation for visualizing object tracking paths across video frames using YOLO model. The script processes video frames sequentially, tracks objects, and draws their movement paths using OpenCV. It maintains a history of tracked object positions and visualizes them as lines.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/track.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom collections import defaultdict\n\nimport cv2\nimport numpy as np\n\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Open the video file\nvideo_path = \"path/to/video.mp4\"\ncap = cv2.VideoCapture(video_path)\n\n# Store the track history\ntrack_history = defaultdict(lambda: [])\n\n# Loop through the video frames\nwhile cap.isOpened():\n    # Read a frame from the video\n    success, frame = cap.read()\n\n    if success:\n        # Run YOLO11 tracking on the frame, persisting tracks between frames\n        result = model.track(frame, persist=True)[0]\n\n        # Get the boxes and track IDs\n        if result.boxes and result.boxes.id is not None:\n            boxes = result.boxes.xywh.cpu()\n            track_ids = result.boxes.id.int().cpu().tolist()\n\n            # Visualize the result on the frame\n            frame = result.plot()\n\n            # Plot the tracks\n            for box, track_id in zip(boxes, track_ids):\n                x, y, w, h = box\n                track = track_history[track_id]\n                track.append((float(x), float(y)))  # x, y center point\n                if len(track) > 30:  # retain 30 tracks for 30 frames\n                    track.pop(0)\n\n                # Draw the tracking lines\n                points = np.hstack(track).astype(np.int32).reshape((-1, 1, 2))\n                cv2.polylines(frame, [points], isClosed=False, color=(230, 230, 230), thickness=10)\n\n        # Display the annotated frame\n        cv2.imshow(\"YOLO11 Tracking\", frame)\n\n        # Break the loop if 'q' is pressed\n        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n            break\n    else:\n        # Break the loop if the end of the video is reached\n        break\n\n# Release the video capture object and close the display window\ncap.release()\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Using Pre-built Solutions in Ultralytics CLI\nDESCRIPTION: This command utilizes the Ultralytics CLI to execute a pre-built solution for counting objects in a video. The key requirement is the path to the video file. The solution is designed for common computer vision tasks with minimal configuration needed.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/cli.md#2025-04-22_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\nyolo solutions count source=\"path/to/video.mp4\"\n```\n\n----------------------------------------\n\nTITLE: Quick YOLO11 Model Validation with CLI (Bash)\nDESCRIPTION: Performs vanilla YOLO11 model validation with a single CLI command, outputting evaluation metrics to the console. This technique is ideal for fast model checks and scripting. The only required argument is the model path. Requires the Ultralytics command-line tool.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/val.md#2025-04-22_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\nyolo val model=yolo11n.pt\n\n```\n\n----------------------------------------\n\nTITLE: Running YOLO Inference and Plotting Results\nDESCRIPTION: Code to load YOLO model, perform inference on an image, and plot the results\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/view-results-in-terminal.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Run inference on an image\nresults = model.predict(source=\"ultralytics/assets/bus.jpg\")\n\n# Plot inference results\nplot = results[0].plot()\n```\n\n----------------------------------------\n\nTITLE: Validating YOLO11 Model via CLI\nDESCRIPTION: Demonstrates how to validate a YOLO11 model using the command-line interface for both official and custom models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/detect.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyolo detect val model=yolo11n.pt      # val official model\nyolo detect val model=path/to/best.pt # val custom model\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics with CUDA Support Using conda\nDESCRIPTION: Command to install Ultralytics with PyTorch and CUDA support via conda. This ensures compatibility between packages for GPU environments.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/quickstart.md#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c pytorch -c nvidia -c conda-forge pytorch torchvision pytorch-cuda=11.8 ultralytics\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model to NCNN via Ultralytics CLI (Bash)\nDESCRIPTION: This Bash CLI snippet uses the Ultralytics command-line tool ('yolo') to export a YOLO11 model checkpoint directly to the NCNN format. The 'model' argument specifies the model checkpoint file (e.g., 'yolo11n.pt'), and the 'format' argument is set to 'ncnn'. Running this command creates a new exported model directory (e.g., '/yolo11n_ncnn_model'). The CLI requires the Ultralytics package to be installed and available in the system PATH. No additional parameters are needed, and output location is determined automatically. This approach benefits scripting and automation workflows.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/ncnn.md#2025-04-22_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\nyolo export model=yolo11n.pt format=ncnn # creates '/yolo11n_ncnn_model'\n```\n\n----------------------------------------\n\nTITLE: Visualizing Experiment Data with Plotly Parallel Coordinates (Python)\nDESCRIPTION: Generates an interactive parallel coordinates plot using Plotly Express to visualize the relationships between different hyperparameters and metrics stored in a pandas DataFrame (`df`). The plot uses specified columns and colors the lines based on the 'metrics.mAP50-95(B)' value, aiding in the visual analysis of experiment outcomes.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/dvc.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom plotly.express import parallel_coordinates\n\nfig = parallel_coordinates(df, columns, color=\"metrics.mAP50-95(B)\")\nfig.show()\n```\n\n----------------------------------------\n\nTITLE: Running YOLO Inference with Edge TPU via CLI\nDESCRIPTION: Command-line instruction to run inference using a YOLO model exported for Edge TPU with the Ultralytics CLI tool.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/coral-edge-tpu-on-raspberry-pi.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nyolo predict model=path/to/MODEL_NAME_full_integer_quant_edgetpu.tflite source=path/to/source.png # Load an official model or custom model\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11-OBB Model to ONNX with Python\nDESCRIPTION: This Python snippet illustrates the process of exporting a YOLO11-OBB model to the ONNX format. Utilizing the ultralytics library, the model is first loaded and then exported, facilitating the integration with various platforms and tools supporting the ONNX model format.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/obb.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-obb.pt\")\n\n# Export the model\nmodel.export(format=\"onnx\")\n```\n\n----------------------------------------\n\nTITLE: Tracking Objects Using YOLO-World via Ultralytics Command-Line Interface (CLI)\nDESCRIPTION: Shows how to execute object tracking on a video using the Ultralytics CLI, specifying model weights, image size, and video source path. Dependencies include the Ultralytics command-line tool and an accessible YOLO-World weights file. Parameters 'model', 'imgsz', and 'source' define the checkpoint, network input size, and input video. Output consists of tracked object results, typically saved or displayed as video overlays.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolo-world.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# Track with a YOLO-World model on the video with a specified image size\nyolo track model=yolov8s-world.pt imgsz=640 source=\"path/to/video.mp4\"\n```\n\n----------------------------------------\n\nTITLE: Running Ultralytics Docker Container with GPUs in Bash\nDESCRIPTION: These commands run the Ultralytics Docker container with GPU support, allowing use of all GPUs or specific GPU devices.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/docker-quickstart.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Run with all GPUs\nsudo docker run -it --ipc=host --gpus all $t\n\n# Run specifying which GPUs to use\nsudo docker run -it --ipc=host --gpus '\"device=2,3\"' $t\n```\n\n----------------------------------------\n\nTITLE: Training with YOLO-World using CLI\nDESCRIPTION: This Bash snippet illustrates how to train a YOLOv8s-worldv2 model on the COCO8 dataset using command-line interface (CLI) commands. It specifies parameters such as the model configuration, dataset, number of epochs, and image size. The training process outputs the trained model ready for inference.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolo-world.md#2025-04-22_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\nyolo train model=yolov8s-worldv2.yaml data=coco8.yaml epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model to PaddlePaddle Format in Python\nDESCRIPTION: Python code snippet demonstrating how to load a YOLO11 model, export it to PaddlePaddle format, and run inference with the exported model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/paddlepaddle.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to PaddlePaddle format\nmodel.export(format=\"paddle\")  # creates '/yolo11n_paddle_model'\n\n# Load the exported PaddlePaddle model\npaddle_model = YOLO(\"./yolo11n_paddle_model\")\n\n# Run inference\nresults = paddle_model(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Exporting and Running YOLO Model with DLA via Command Line\nDESCRIPTION: CLI commands for exporting a YOLO11n model to TensorRT format with DLA enabled and running inference with the exported model. The DLA core specified during export will be used during inference, with half-precision required for DLA compatibility.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/nvidia-jetson.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Export a YOLO11n PyTorch model to TensorRT format with DLA enabled (only works with FP16 or INT8)\n# Once DLA core number is specified at export, it will use the same core at inference\nyolo export model=yolo11n.pt format=engine device=\"dla:0\" half=True # dla:0 or dla:1 corresponds to the DLA cores\n\n# Run inference with the exported model on the DLA\nyolo predict model=yolo11n.engine source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: COCO Dataset Citation in BibTeX Format\nDESCRIPTION: BibTeX formatted citation for the Microsoft COCO dataset, which should be referenced when using the COCO8-Seg dataset in research or development work.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/coco8-seg.md#2025-04-22_snippet_3\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{lin2015microsoft,\n      title={Microsoft COCO: Common Objects in Context},\n      author={Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Dollár},\n      year={2015},\n      eprint={1405.0312},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n----------------------------------------\n\nTITLE: Convert Polygon to Binary Mask\nDESCRIPTION: Function to convert a polygon to a binary mask of specified image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/simple-utilities.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nfrom ultralytics.data.utils import polygon2mask\n\nimgsz = (1080, 810)\npolygon = np.array([805, 392, 797, 400, ..., 808, 714, 808, 392])  # (238, 2)\n\nmask = polygon2mask(\n    imgsz,  # tuple\n    [polygon],  # input as list\n    color=255,  # 8-bit binary\n    downsample_ratio=1,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics and Exporting YOLOv8 ONNX Models\nDESCRIPTION: Commands to install the Ultralytics package and export YOLOv8 models to ONNX format with dynamic shapes for various tasks.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-ONNXRuntime-Rust/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U ultralytics\n\nyolo export model=yolov8m.pt format=onnx simplify dynamic\nyolo export model=yolov8m-cls.pt format=onnx simplify dynamic\nyolo export model=yolov8m-pose.pt format=onnx simplify dynamic\nyolo export model=yolov8m-seg.pt format=onnx simplify dynamic\n```\n\n----------------------------------------\n\nTITLE: Saving Workout Monitoring Output\nDESCRIPTION: Example showing how to save processed workout monitoring video output to a file.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/workouts-monitoring.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nassert cap.isOpened(), \"Error reading video file\"\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n\nvideo_writer = cv2.VideoWriter(\"workouts.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n\ngym = solutions.AIGym(\n    line_width=2,\n    show=True,\n    kpts=[6, 8, 10],\n)\n\nwhile cap.isOpened():\n    success, im0 = cap.read()\n    if not success:\n        print(\"Video frame is empty or processing is complete.\")\n        break\n    results = gym(im0)\n    video_writer.write(results.plot_im)\n\ncap.release()\nvideo_writer.release()\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Running YOLOv8 Inference with TensorRT and FP16\nDESCRIPTION: Command to run YOLOv8 inference using TensorRT acceleration with FP16 precision in Rust.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-ONNXRuntime-Rust/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncargo run --release -- --trt --fp16 --model MODEL_PATH.onnx --source SOURCE_IMAGE.jpg\n```\n\n----------------------------------------\n\nTITLE: Converting PointCloud2 to Numpy Arrays in Python\nDESCRIPTION: This function converts a ROS PointCloud2 message into two numpy arrays: one for XYZ coordinates and another for RGB values. It handles null values and reshapes the data to match the original image dimensions.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/ros-quickstart.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport ros_numpy\n\n\ndef pointcloud2_to_array(pointcloud2: PointCloud2) -> tuple:\n    \"\"\"\n    Convert a ROS PointCloud2 message to a numpy array.\n\n    Args:\n        pointcloud2 (PointCloud2): the PointCloud2 message\n\n    Returns:\n        (tuple): tuple containing (xyz, rgb)\n    \"\"\"\n    pc_array = ros_numpy.point_cloud2.pointcloud2_to_array(pointcloud2)\n    split = ros_numpy.point_cloud2.split_rgb_field(pc_array)\n    rgb = np.stack([split[\"b\"], split[\"g\"], split[\"r\"]], axis=2)\n    xyz = ros_numpy.point_cloud2.get_xyz_points(pc_array, remove_nans=False)\n    xyz = np.array(xyz).reshape((pointcloud2.height, pointcloud2.width, 3))\n    nan_rows = np.isnan(xyz).all(axis=2)\n    xyz[nan_rows] = [0, 0, 0]\n    rgb[nan_rows] = [0, 0, 0]\n    return xyz, rgb\n```\n\n----------------------------------------\n\nTITLE: Exporting and Using YOLO11 TorchScript Models via CLI\nDESCRIPTION: This snippet shows how to export a YOLO11 PyTorch model to TorchScript format and run inference using the command-line interface. It includes commands for exporting the model and then using the exported model for prediction on an image.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/torchscript.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Export a YOLO11n PyTorch model to TorchScript format\nyolo export model=yolo11n.pt format=torchscript # creates 'yolo11n.torchscript'\n\n# Run inference with the exported model\nyolo predict model=yolo11n.torchscript source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Search Space and Running Ray Tune on YOLO11 Python\nDESCRIPTION: This snippet demonstrates how to create a YOLO11 model with pretrained weights using the Ultralytics Python API, then invoke the tune method with a custom hyperparameter search space for the initial learning rate via Ray Tune. It requires ultralytics and ray installed. The key parameter is space, which is a dictionary mapping parameter names to Ray Tune search distributions. Inputs are the dataset config file, search space, and other tuning parameters; outputs are the Ray Tune result grid. The snippet presumes available CUDA or CPU resources for training.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/ray-tune.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import tune\\n\\nfrom ultralytics import YOLO\\n\\n# Define a YOLO model\\nmodel = YOLO(\"yolo11n.pt\")\\n\\n# Run Ray Tune on the model\\nresult_grid = model.tune(\\n    data=\"coco8.yaml\",\\n    space={\"lr0\": tune.uniform(1e-5, 1e-1)},\\n    epochs=50,\\n    use_ray=True,\\n)\n```\n\n----------------------------------------\n\nTITLE: Automated Dataset Splitting with Autosplit in Ultralytics (Python)\nDESCRIPTION: The `autosplit` function provides automatic dataset split capabilities in Ultralytics, working across multiple dataset types. As a Python function, it takes in dataset parameters like directory paths and split fractions, then partitions data accordingly. Dependencies include Ultralytics core libraries and standard filesystem modules. Key parameters may include output locations and random seed. The function outputs reorganized dataset directories, ensuring balanced and reproducible splits. Limitations include required data consistency and permissions for file operations.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/data/split.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n## ::: ultralytics.data.split.autosplit\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO Model to TFLite Format using CLI\nDESCRIPTION: This command-line instruction shows how to export a YOLO model to TFLite format using the Ultralytics CLI. It creates a file named 'yolo11n_float32.tflite'.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tflite.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=yolo11n.pt format=tflite # creates 'yolo11n_float32.tflite'\n```\n\n----------------------------------------\n\nTITLE: Validating YOLOE Model with Reference Dataset for Visual Embeddings\nDESCRIPTION: Code to validate a YOLOE model using visual prompts from a reference dataset. This approach uses a separate reference dataset to extract visual embeddings for categories, which must match the categories in the validation dataset.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yoloe.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLOE\n\n# Create a YOLOE model\nmodel = YOLOE(\"yoloe-11l-seg.pt\")  # or select yoloe-m/l-seg.pt for different sizes\n\n# Conduct model validation on the COCO128-seg example dataset\nmetrics = model.val(data=\"coco128-seg.yaml\", load_vp=True, refer_data=\"coco.yaml\")\n```\n\n----------------------------------------\n\nTITLE: Software Setup Commands for Raspberry Pi\nDESCRIPTION: Terminal commands for setting up required software dependencies on Raspberry Pi for IMX500 deployment.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/sony-imx500.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt update && sudo apt full-upgrade\nsudo apt install imx500-all imx500-tools\nsudo apt install python3-opencv python3-munkres\nsudo reboot\n```\n\n----------------------------------------\n\nTITLE: Plotting Training Curves for WandB\nDESCRIPTION: Function for generating and formatting training curve plots for WandB visualization.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/callbacks/wb.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nultralytics.utils.callbacks.wb._plot_curve\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Models on DOTA Dataset (Python)\nDESCRIPTION: Python code example showing how to create and train a YOLO11n-OBB model from scratch using the DOTA v1 dataset with Ultralytics.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/obb/dota-v2.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Create a new YOLO11n-OBB model from scratch\nmodel = YOLO(\"yolo11n-obb.yaml\")\n\n# Train the model on the DOTAv1 dataset\nresults = model.train(data=\"DOTAv1.yaml\", epochs=100, imgsz=1024)\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on CIFAR-10 Dataset in Python\nDESCRIPTION: This snippet demonstrates how to load a pre-trained YOLO model and train it on the CIFAR-10 dataset for 100 epochs with an image size of 32x32 using Python.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/cifar10.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-cls.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"cifar10\", epochs=100, imgsz=32)\n```\n\n----------------------------------------\n\nTITLE: MaskDecoder Class Documentation\nDESCRIPTION: Core mask decoder class that uses transformers to predict segmentation masks. Processes encoded image features and prompts to generate binary masks.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/models/sam/modules/decoders.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nultralytics.models.sam.modules.decoders.MaskDecoder\n```\n\n----------------------------------------\n\nTITLE: Running YOLO Pose Estimation Tasks in Rust\nDESCRIPTION: Commands for running pose estimation tasks using YOLOv8 and YOLO11 models with the ONNXRuntime Rust implementation. These models detect and track human body keypoints.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLO-Series-ONNXRuntime-Rust/README.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ncargo run -r -- --task pose --ver v8 --scale n   # YOLOv8-Pose Estimation\ncargo run -r -- --task pose --ver v11 --scale n  # YOLO11-Pose Estimation\n```\n\n----------------------------------------\n\nTITLE: Configuring Queue Management for Airport Environments\nDESCRIPTION: This Python snippet demonstrates how to configure the QueueManager for airport environments by specifying a custom queue region and model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/queue-management.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nqueue_region_airport = [(50, 600), (1200, 600), (1200, 550), (50, 550)]\nqueue_airport = solutions.QueueManager(\n    model=\"yolo11n.pt\",\n    region=queue_region_airport,\n    line_width=3,\n)\n```\n\n----------------------------------------\n\nTITLE: Running Ultralytics Solutions via CLI\nDESCRIPTION: These command-line examples show how to use Ultralytics Solutions directly from the terminal. The first command demonstrates object counting, while the second specifies a video file path for processing.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/solutions/index.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyolo solutions count show=True # for object counting\n\nyolo solutions source=\"path/to/video.mp4\" # specify video file path\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n Model on Objects365 Dataset in Python\nDESCRIPTION: Python code snippet demonstrating how to load a pretrained YOLO11n model and train it on the Objects365 dataset for 100 epochs with an image size of 640.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/objects365.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"Objects365.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Running YOLO Segmentation Tasks with Different Models\nDESCRIPTION: Commands for running segmentation tasks using YOLOv5, YOLOv8, YOLO11, and FastSAM models with the ONNXRuntime Rust implementation. These models provide pixel-level object masks.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLO-Series-ONNXRuntime-Rust/README.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ncargo run -r -- --task segment --ver v5 --scale n  # YOLOv5-Segment\ncargo run -r -- --task segment --ver v8 --scale n  # YOLOv8-Segment\ncargo run -r -- --task segment --ver v11 --scale n # YOLO11-Segment\ncargo run -r -- --task segment --ver v8 --model path/to/FastSAM-s-dyn-f16.onnx # FastSAM Segmentation\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with PyTorch Hub in Python\nDESCRIPTION: Load a YOLOv5 model from PyTorch Hub and perform inference on an image. This method allows for easy model loading and execution using PyTorch.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/quickstart_tutorial.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n# Model loading\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\")  # Can be 'yolov5n' - 'yolov5x6', or 'custom'\n\n# Inference on images\nimg = \"https://ultralytics.com/images/zidane.jpg\"  # Can be a file, Path, PIL, OpenCV, numpy, or list of images\n\n# Run inference\nresults = model(img)\n\n# Display results\nresults.print()  # Other options: .show(), .save(), .crop(), .pandas(), etc. Explore these in the Predict mode documentation.\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model with Custom Pose Dataset\nDESCRIPTION: Demonstrates how to load a YOLO model and train it on a custom pose estimation dataset using a YAML configuration file. Specifies epochs and image size parameters for training.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/index.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolo11n-pose.pt\")\nresults = model.train(data=\"your-dataset.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Using pip\nDESCRIPTION: Command to install the latest stable release of Ultralytics from PyPI using pip. This is the standard method for installing the package.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/quickstart.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n-pose Model on Tiger-Pose Dataset in Python\nDESCRIPTION: Python code to load a pretrained YOLO11n-pose model and train it on the Tiger-Pose dataset for 100 epochs with 640px image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/tiger-pose.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-pose.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"tiger-pose.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Running Basic Action Recognition with YOLOv8\nDESCRIPTION: Basic command examples for running action recognition on videos, including options for using YouTube videos, local files, different models, and custom action labels.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-Action-Recognition/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Quick start with default video and labels\npython action_recognition.py\n\n# Basic usage with a YouTube video and custom labels\npython action_recognition.py --source \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\" --labels \"dancing\" \"singing a song\"\n\n# Use a local video file\npython action_recognition.py --source path/to/video.mp4\n\n# Use a medium-sized YOLOv8 model for potentially better detector performance\npython action_recognition.py --weights yolov8m.pt\n\n# Run inference on the CPU instead of GPU\npython action_recognition.py --device cpu\n\n# Use a different video classifier model from TorchVision\npython action_recognition.py --video-classifier-model \"s3d\"\n\n# Use FP16 (half-precision) for faster inference (only for HuggingFace models)\npython action_recognition.py --fp16\n\n# Export the output video with recognized actions to an mp4 file\npython action_recognition.py --output-path output.mp4\n\n# Combine multiple options: specific YouTube source, GPU device 0, specific HuggingFace model, custom labels, and FP16\npython action_recognition.py --source \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\" --device 0 --video-classifier-model \"microsoft/xclip-base-patch32\" --labels \"dancing\" \"singing a song\" --fp16\n```\n\n----------------------------------------\n\nTITLE: Training a YOLOv8 Model in Python\nDESCRIPTION: Demonstrates training a YOLOv8 model using the Python API, as shown in the FAQ section. It involves loading a pre-trained model (`yolov8n.pt`) and calling the `train` method with dataset configuration (`coco8.yaml`), number of epochs, and image size. Requires the `ultralytics` library and necessary files.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov8.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a COCO-pretrained YOLOv8n model\nmodel = YOLO(\"yolov8n.pt\")\n\n# Train the model on the COCO8 example dataset for 100 epochs\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Using YOLOv8 Model via Command Line Interface\nDESCRIPTION: This snippet shows how to train a YOLOv8 model on a dataset and run inference on an image using the command line interface.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/index.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Load a COCO-pretrained YOLOv8n model and train it on the COCO8 example dataset for 100 epochs\nyolo train model=yolov8n.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Load a COCO-pretrained YOLOv8n model and run inference on the 'bus.jpg' image\nyolo predict model=yolov8n.pt source=path/to/bus.jpg\n```\n\n----------------------------------------\n\nTITLE: Scaling Bounding Boxes with Ultralytics - Python\nDESCRIPTION: This Python snippet demonstrates how to scale bounding box coordinates for an image that is resized using OpenCV. The `scale_boxes` function from `ultralytics.utils.ops` adjusts the bounding boxes according to the new image dimensions. Dependencies include `cv2` and `numpy`, and the main parameters are the original and new image dimensions. The output is an array of resized bounding box coordinates.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/simple-utilities.md#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2 as cv\nimport numpy as np\n\nfrom ultralytics.utils.ops import scale_boxes\n\nimage = cv.imread(\"ultralytics/assets/bus.jpg\")\nh, w, c = image.shape\nresized = cv.resize(image, None, (), fx=1.2, fy=1.2)\nnew_h, new_w, _ = resized.shape\n\nxyxy_boxes = np.array(\n    [\n        [22.878, 231.27, 804.98, 756.83],\n        [48.552, 398.56, 245.35, 902.71],\n        [669.47, 392.19, 809.72, 877.04],\n        [221.52, 405.8, 344.98, 857.54],\n        [0, 550.53, 63.01, 873.44],\n        [0.0584, 254.46, 32.561, 324.87],\n    ]\n)\n\nnew_boxes = scale_boxes(\n    img1_shape=(h, w),  # original image dimensions\n    boxes=xyxy_boxes,  # boxes from original image\n    img0_shape=(new_h, new_w),  # resized image dimensions (scale to)\n    ratio_pad=None,\n    padding=False,\n    xywh=False,\n)\n\nprint(new_boxes)\n# >>> array(\n#     [[  27.454,  277.52,  965.98,   908.2],\n#     [   58.262,  478.27,  294.42,  1083.3],\n#     [   803.36,  470.63,  971.66,  1052.4],\n#     [   265.82,  486.96,  413.98,    1029],\n#     [        0,  660.64,  75.612,  1048.1],\n#     [   0.0701,  305.35,  39.073,  389.84]]\n# )\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 to TFLite Edge TPU via CLI\nDESCRIPTION: This command-line example shows how to export a YOLO11n model to TFLite Edge TPU format and then run inference with the exported model using the Ultralytics CLI interface.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/edge-tpu.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Export a YOLO11n PyTorch model to TFLite Edge TPU format\nyolo export model=yolo11n.pt format=edgetpu # creates 'yolo11n_full_integer_quant_edgetpu.tflite'\n\n# Run inference with the exported model\nyolo predict model=yolo11n_full_integer_quant_edgetpu.tflite source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on Hand Keypoints Dataset using CLI\nDESCRIPTION: This bash command shows how to train a YOLO model on the hand keypoints dataset using the command line interface, specifying the model, dataset, epochs, and image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/hand-keypoints.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo pose train data=hand-keypoints.yaml model=yolo11n-pose.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLOv5 Models in Various Formats with PyTorch\nDESCRIPTION: This snippet shows loading a YOLOv5 model in different formats like PyTorch, TorchScript, ONNX, OpenVINO, TensorRT, CoreML, TFLite, and PaddlePaddle. It emphasizes the support for various deployment environments, indicating flexibility and performance optimization across platforms.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/pytorch_hub_model_loading.md#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", path=\"yolov5s.pt\")  # PyTorch\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", path=\"yolov5s.torchscript\")  # TorchScript\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", path=\"yolov5s.onnx\")  # ONNX\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", path=\"yolov5s_openvino_model/\")  # OpenVINO\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", path=\"yolov5s.engine\")  # TensorRT\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", path=\"yolov5s.mlmodel\")  # CoreML (macOS-only)\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", path=\"yolov5s.tflite\")  # TFLite\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", path=\"yolov5s_paddle_model/\")  # PaddlePaddle\n```\n\n----------------------------------------\n\nTITLE: Uploading Dataset to ClearML\nDESCRIPTION: Command to upload a dataset to ClearML using the ClearML Data tool. This creates a versioned dataset in ClearML that can be referenced in training scripts.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/clearml.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd your_dataset_folder\nclearml-data sync --project YOLOv11 --name your_dataset_name --folder .\n```\n\n----------------------------------------\n\nTITLE: Alternative YOLO11 Training Example via CLI for FAQ Section\nDESCRIPTION: A simplified CLI command example for training a YOLO11n segmentation model on the Crack Segmentation dataset, included in the FAQ section as a quick reference for users preferring command-line interaction.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/crack-seg.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained model via CLI\nyolo segment train data=crack-seg.yaml model=yolo11n-seg.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: FAQ Example: Training YOLO11n-pose Model in Python\nDESCRIPTION: Python code example from the FAQ section showing how to load a pretrained YOLO11n-pose model and train it on the Tiger-Pose dataset.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/tiger-pose.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-pose.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"tiger-pose.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Creating 3D Scatterplot of Reduced Embeddings using PCA and Matplotlib in Python\nDESCRIPTION: This code reduces 256-dimensional embeddings to 3 components using PCA, then creates a 3D scatter plot using Matplotlib. It visualizes the reduced embeddings in a 3D space.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/explorer.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA  # pip install scikit-learn\n\n# Reduce dimensions using PCA to 3 components for visualization in 3D\npca = PCA(n_components=3)\nreduced_data = pca.fit_transform(embeddings)\n\n# Create a 3D scatter plot using Matplotlib's Axes3D\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection=\"3d\")\n\n# Scatter plot\nax.scatter(reduced_data[:, 0], reduced_data[:, 1], reduced_data[:, 2], alpha=0.5)\nax.set_title(\"3D Scatter Plot of Reduced 256-Dimensional Data (PCA)\")\nax.set_xlabel(\"Component 1\")\nax.set_ylabel(\"Component 2\")\nax.set_zlabel(\"Component 3\")\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Using Inference Web Interface Solution in Bash\nDESCRIPTION: Commands to launch the browser-based inference solution for object detection, segmentation, or pose estimation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/cli.md#2025-04-22_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nyolo solutions inference\nyolo solutions inference model=\"path/to/model.pt\" # use custom model\n```\n\n----------------------------------------\n\nTITLE: Running YOLO Classification Tasks with Different Model Versions\nDESCRIPTION: Commands for running classification tasks using different YOLO versions (v5, v8, v11) with specific scaling factors and input dimensions for the ONNXRuntime Rust implementation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLO-Series-ONNXRuntime-Rust/README.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncargo run -r -- --task classify --ver v5 --scale s --width 224 --height 224 --nc 1000  # YOLOv5 Classification\ncargo run -r -- --task classify --ver v8 --scale n --width 224 --height 224 --nc 1000  # YOLOv8 Classification\ncargo run -r -- --task classify --ver v11 --scale n --width 224 --height 224 --nc 1000 # YOLO11 Classification\n```\n\n----------------------------------------\n\nTITLE: Convert COCO to YOLO Format\nDESCRIPTION: Function to convert COCO JSON annotations into YOLO format for object detection datasets.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/simple-utilities.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.data.converter import convert_coco\n\nconvert_coco(\n    \"../datasets/coco/annotations/\",\n    use_segments=False,\n    use_keypoints=False,\n    cls91to80=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Predicting with FastSAM Model - CLI\nDESCRIPTION: Illustrates using the Ultralytics CLI to segment an image with a specified FastSAM model checkpoint and configurable image size. Requires installation of Ultralytics/yolo CLI and the relevant model weights in the expected path. Takes parameters such as input model file and source image path; produces and optionally saves segmentation results to disk.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/fast-sam.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Load a FastSAM model and segment everything with it\nyolo segment predict model=FastSAM-s.pt source=path/to/bus.jpg imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Extracting and Converting Image Embeddings to NumPy Array in Python\nDESCRIPTION: This snippet extracts the 'vector' column from a LanceDB table, which contains image embeddings, converts it to a list, and then to a NumPy array for further analysis.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/explorer.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nembeddings = table.to_pandas()[\"vector\"].tolist()\nembeddings = np.array(embeddings)\n```\n\n----------------------------------------\n\nTITLE: Validating YOLO11 Model with CLI\nDESCRIPTION: This CLI command validates a YOLO11 model using a specified validation dataset. It evaluates model performance using key metrics such as mAP and precision. Requires a pre-trained or custom model and validation dataset.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/index.md#2025-04-22_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\nyolo val data=path/to/validation.yaml\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Tiger-Pose Trained Model in Python\nDESCRIPTION: Python code to load a Tiger-Pose trained model and perform inference on a video source, displaying the results in real-time.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/tiger-pose.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"path/to/best.pt\")  # load a tiger-pose trained model\n\n# Run inference\nresults = model.predict(source=\"https://youtu.be/MIBAT6BGE6U\", show=True)\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Model with Multi-scale and Batch Size Configuration\nDESCRIPTION: Example command for training a YOLO11 model with specified batch size and multi-scale option enabled. This configuration helps optimize training performance, especially when working with multiple GPUs.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/yolo-common-issues.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmodel.train(data=\"/path/to/your/data.yaml\", batch=32, multi_scale=True)\n```\n\n----------------------------------------\n\nTITLE: Basic DetectionTrainer Usage in Python\nDESCRIPTION: Demonstrates how to initialize and use the DetectionTrainer class with custom overrides.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/engine.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.models.yolo.detect import DetectionTrainer\n\ntrainer = DetectionTrainer(overrides={...})\ntrainer.train()\ntrained_model = trainer.best  # Get the best model\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model with COCO-Pose in Python\nDESCRIPTION: Python code example showing how to load and train a YOLO11n-pose model on the COCO-Pose dataset for 100 epochs with 640px image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/coco.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-pose.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"coco-pose.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Models on Open Images V7 with CLI\nDESCRIPTION: Command-line interface command for training a COCO-pretrained YOLO11n model on the Open Images V7 dataset with specified epochs and image size parameters.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/open-images-v7.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Train a COCO-pretrained YOLO11n model on the Open Images V7 dataset\nyolo detect train data=open-images-v7.yaml model=yolo11n.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Using Multiple GPUs for YOLOv5 Training (Shell)\nDESCRIPTION: This snippet shows how to specify multiple GPUs for distributed YOLOv5 training using the '--device' flag with a comma-separated GPU index list. Requires a multi-GPU system and the YOLOv5 framework installed with CUDA support. The parameter '--device 0,1,2,3' sets the training script to utilize GPUs 0 through 3; inputs are GPU IDs and outputs are improved parallelization and faster training.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/tips_for_best_training_results.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n--device 0,1,2,3\n```\n\n----------------------------------------\n\nTITLE: AIGym Class Implementation for Pose Detection and Exercise Counting\nDESCRIPTION: The AIGym class provides functionality for detecting human poses and counting exercise repetitions in real-time. It utilizes Ultralytics YOLO models for pose estimation and implements logic for tracking exercises like squats, push-ups, and pull-ups.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/solutions/ai_gym.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass AIGym:\n    \"\"\"A class for counting exercises using pose estimation with YOLO.\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model to MNN Format in Python\nDESCRIPTION: Python code snippet demonstrating how to load a YOLO11 model, export it to MNN format, and run inference with the exported model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/mnn.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to MNN format\nmodel.export(format=\"mnn\")  # creates 'yolo11n.mnn'\n\n# Load the exported MNN model\nmnn_model = YOLO(\"yolo11n.mnn\")\n\n# Run inference\nresults = mnn_model(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Referencing the NASPredictor Class in Python Documentation\nDESCRIPTION: This documentation directive points to the `NASPredictor` class defined within the `ultralytics.models.nas.predict` module. The `NASPredictor` class encapsulates the logic required to perform inference (prediction) using Ultralytics Neural Architecture Search (NAS) models. It likely handles model loading, input preprocessing, running the model, and post-processing the results (like applying non-maximum suppression).\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/models/nas/predict.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n::: ultralytics.models.nas.predict.NASPredictor\n```\n\n----------------------------------------\n\nTITLE: Running YOLOv8 Inference with Dynamic Image Size\nDESCRIPTION: Command to run YOLOv8 inference with custom image dimensions using a dynamically shaped ONNX model in Rust.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-ONNXRuntime-Rust/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncargo run --release -- --cuda --width 480 --height 640 --model MODEL_PATH_dynamic.onnx --source SOURCE_IMAGE.jpg\n```\n\n----------------------------------------\n\nTITLE: Converting COCO Format to YOLO Format with Python\nDESCRIPTION: Python code snippet demonstrating how to convert dataset annotations from COCO format to YOLO format using Ultralytics conversion tools. This is useful for preparing COCO-formatted datasets for use with YOLO models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/index.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.data.converter import convert_coco\n\nconvert_coco(labels_dir=\"path/to/coco/annotations/\")\n```\n\n----------------------------------------\n\nTITLE: Enabling Weights & Biases logging for YOLO\nDESCRIPTION: This command shows how to re-enable Weights & Biases logging in YOLO settings using the command line.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/weights-biases.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nyolo settings wandb=True\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Commands for cloning the Ultralytics repository and installing necessary packages for region counting implementation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-Region-Counter/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Clone the ultralytics repository\ngit clone https://github.com/ultralytics/ultralytics\n\n# Navigate to the example directory\ncd ultralytics/examples/YOLOv8-Region-Counter\n\n# Install required packages (if not already installed)\n# pip install ultralytics shapely\n```\n\n----------------------------------------\n\nTITLE: Implementing Object Cropping Using CLI Commands\nDESCRIPTION: Command line interface examples for object cropping using YOLO11. Shows basic cropping, video source specification, and class-specific cropping options.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/object-cropping.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Crop the objects\nyolo solutions crop show=True\n\n# Pass a source video\nyolo solutions crop source=\"path/to/video.mp4\"\n\n# Crop specific classes\nyolo solutions crop classes=\"[0, 2]\"\n```\n\n----------------------------------------\n\nTITLE: Using Ultralytics YOLO with xView Dataset (Python Example)\nDESCRIPTION: Python implementation for training a YOLO model on the xView satellite imagery dataset. This code loads a pretrained YOLO model and configures it for training with appropriate parameters.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/xview.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"xView.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Configuring GPU Device Selection in YAML for YOLO11 Training\nDESCRIPTION: YAML configuration snippet for explicitly assigning a specific GPU for YOLO11 model training. This setting allows you to control which GPU device is used when multiple are available in your system.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/yolo-common-issues.md#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\ndevice: 0\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n-pose Model on COCO8-Pose Dataset using Python\nDESCRIPTION: Python code snippet for training a YOLO11n-pose model on the COCO8-Pose dataset for 100 epochs with 640px image size. It loads a pretrained model and initiates the training process.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/coco8-pose.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-pose.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"coco8-pose.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Validating YOLO11 Classification Models in Python\nDESCRIPTION: Demonstrates how to validate trained YOLO11 classification models in Python, loading either official or custom models and accessing top1/top5 accuracy metrics.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/classify.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-cls.pt\")  # load an official model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom model\n\n# Validate the model\nmetrics = model.val()  # no arguments needed, dataset and settings remembered\nmetrics.top1  # top1 accuracy\nmetrics.top5  # top5 accuracy\n```\n\n----------------------------------------\n\nTITLE: Training a YOLO Model with Command Line Interface\nDESCRIPTION: Command line example for training a YOLO model using the CLI. This command starts training from a pre-trained model with specified data, epochs, and image size parameters.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/index.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo detect train data=coco8.yaml model=yolo11n.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n Model on VisDrone Dataset with CLI\nDESCRIPTION: Command-line interface command for training a pretrained YOLO11n model on the VisDrone dataset for 100 epochs with 640px image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/visdrone.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo detect train data=VisDrone.yaml model=yolo11n.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-GPU Inference with YOLOv5\nDESCRIPTION: Shows how to load YOLOv5 models on multiple GPUs and run parallel inference using threading. This approach can significantly increase throughput for applications processing multiple images simultaneously.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/pytorch_hub_model_loading.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport threading\n\nimport torch\n\n\ndef run(model, im):\n    \"\"\"Performs inference on an image using a given model and saves the output; model must support `.save()` method.\"\"\"\n    results = model(im)\n    results.save()\n\n\n# Models\nmodel0 = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", device=0)\nmodel1 = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", device=1)\n\n# Inference\nthreading.Thread(target=run, args=[model0, \"https://ultralytics.com/images/zidane.jpg\"], daemon=True).start()\nthreading.Thread(target=run, args=[model1, \"https://ultralytics.com/images/bus.jpg\"], daemon=True).start()\n```\n\n----------------------------------------\n\nTITLE: Speed Estimation Python Implementation\nDESCRIPTION: Complete Python implementation for speed estimation using OpenCV and Ultralytics solutions, including video capture, processing, and writing results\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/speed-estimation.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nassert cap.isOpened(), \"Error reading video file\"\n\n# Video writer\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\nvideo_writer = cv2.VideoWriter(\"speed_management.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n\n# speed region points\nspeed_region = [(20, 400), (1080, 400), (1080, 360), (20, 360)]\n\n# Initialize speed estimation object\nspeedestimator = solutions.SpeedEstimator(\n    show=True,  # display the output\n    model=\"yolo11n.pt\",  # path to the YOLO11 model file.\n    region=speed_region,  # pass region points\n    # classes=[0, 2],  # estimate speed of specific classes.\n    # line_width=2,  # adjust the line width for bounding boxes\n)\n\n# Process video\nwhile cap.isOpened():\n    success, im0 = cap.read()\n\n    if not success:\n        print(\"Video frame is empty or processing is complete.\")\n        break\n\n    results = speedestimator(im0)\n\n    # print(results)  # access the output\n\n    video_writer.write(results.plot_im)  # write the processed frame.\n\ncap.release()\nvideo_writer.release()\ncv2.destroyAllWindows()  # destroy all opened windows\n```\n\n----------------------------------------\n\nTITLE: Training a YOLOv5 Model using CLI (FAQ Example)\nDESCRIPTION: This CLI command, shown in the FAQ section, illustrates how to train a YOLOv5 model (`yolov5n.pt`) using the `yolo` command-line tool. It specifies the dataset configuration (`coco8.yaml`), the number of epochs (100), and the image size (640) for training. Requires the `ultralytics` package to be installed and the `yolo` command to be available in the system path.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov5.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Load a COCO-pretrained YOLOv5n model and train it on the COCO8 example dataset for 100 epochs\nyolo train model=yolov5n.pt data=coco8.yaml epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Classification Model via CLI\nDESCRIPTION: Demonstrates how to train a YOLO classification model using the command-line interface. Specifies the task type, data path, model, epochs, and image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/index.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo classify train data=path/to/data model=yolo11n-cls.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Running Bash Commands in AzureML Notebook Cell\nDESCRIPTION: This snippet demonstrates how to run bash commands within an AzureML notebook cell using the %%bash magic command. It shows how to activate the Conda environment and run a validation command.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/environments/azureml_quickstart_tutorial.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n%%bash\nsource activate yolov5env # Activate environment within the cell\n\n# Example: Run validation using the activated environment\npython val.py --weights yolov5s.pt --data coco128.yaml --img 640\n```\n\n----------------------------------------\n\nTITLE: Inference with YOLO Model using Python\nDESCRIPTION: Python code to perform inference using a trained YOLO model on African Wildlife images\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/african-wildlife.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"path/to/best.pt\")  # load a brain-tumor fine-tuned model\n\n# Inference using the model\nresults = model.predict(\"https://ultralytics.com/assets/african-wildlife-sample.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics YOLO11 on Raspberry Pi\nDESCRIPTION: Steps to install Ultralytics YOLO11 on a Raspberry Pi without Docker by updating package lists, installing pip, and installing the Ultralytics package with export dependencies.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/raspberry-pi.md#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt update\nsudo apt install python3-pip -y\npip install -U pip\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics[export]\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo reboot\n```\n\n----------------------------------------\n\nTITLE: NeptuneAI Callback Functions for Ultralytics YOLO\nDESCRIPTION: Contains a collection of callback functions that integrate Neptune.ai logging capabilities with Ultralytics YOLO training pipeline. Functions include logging of scalar metrics, images, plots and handlers for training events like epoch end and validation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/callbacks/neptune.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# Note: No code snippets are actually shown in the provided text, but references are made to:\n# _log_scalars\n# _log_images\n# _log_plot\n# on_pretrain_routine_start\n# on_train_epoch_end\n# on_fit_epoch_end\n# on_val_end\n# on_train_end\n```\n\n----------------------------------------\n\nTITLE: Visualizing Image Annotations - Python\nDESCRIPTION: The visualize_image_annotations utility overlays annotation data on images for inspection, debugging, or presentation purposes. Inputs include image paths and annotation definitions; produces image visualizations highlighting object boundaries, masks, or classes. Depends on visualization libraries such as OpenCV or Matplotlib and handles standard annotation formats.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/data/utils.md#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n## ::: ultralytics.data.utils.visualize_image_annotations\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on Argoverse Dataset using Python\nDESCRIPTION: Python code snippet to load a pretrained YOLO11n model and train it on the Argoverse dataset for 100 epochs with an image size of 640.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/argoverse.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"Argoverse.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Converting XYXY to XYWH Format with Ultralytics - Python\nDESCRIPTION: This Python snippet converts bounding box coordinates from (x1, y1, x2, y2) format to (x, y, width, height) format using the `xyxy2xywh` function from `ultralytics.utils.ops`. It requires `numpy` and the key input is an array of bounding boxes in XYXY format, outputting the same set of boxes in XYWH format.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/simple-utilities.md#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\n\nfrom ultralytics.utils.ops import xyxy2xywh\n\nxyxy_boxes = np.array(\n    [\n        [22.878, 231.27, 804.98, 756.83],\n        [48.552, 398.56, 245.35, 902.71],\n        [669.47, 392.19, 809.72, 877.04],\n        [221.52, 405.8, 344.98, 857.54],\n        [0, 550.53, 63.01, 873.44],\n        [0.0584, 254.46, 32.561, 324.87],\n    ]\n)\nxywh = xyxy2xywh(xyxy_boxes)\n\nprint(xywh)\n# >>> array(\n#     [[ 413.93,  494.05,   782.1, 525.56],\n#     [  146.95,  650.63,   196.8, 504.15],\n#     [   739.6,  634.62,  140.25, 484.85],\n#     [  283.25,  631.67,  123.46, 451.74],\n#     [  31.505,  711.99,   63.01, 322.91],\n#     [   16.31,  289.67,  32.503,  70.41]]\n# )\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorBoard for Google Colab with YOLO11 Training\nDESCRIPTION: This snippet demonstrates how to set up TensorBoard in a Google Colab environment and train a YOLO11 model. It includes loading the TensorBoard extension, specifying the log directory, and running a basic YOLO11 training script.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tensorboard.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n%load_ext tensorboard\n%tensorboard --logdir path/to/runs\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pre-trained model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Train the model\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Configuring COCO Dataset with YAML\nDESCRIPTION: YAML configuration file defining the COCO dataset paths, classes and other parameters used for training YOLO models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/coco.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/coco.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n-pose Model on Tiger-Pose Dataset via CLI\nDESCRIPTION: Command line interface commands to train a pretrained YOLO11n-pose model on the Tiger-Pose dataset for 100 epochs with 640px image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/tiger-pose.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo task=pose mode=train data=tiger-pose.yaml model=yolo11n-pose.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on ImageNette320 Dataset using CLI\nDESCRIPTION: This CLI command demonstrates how to train a YOLO model for image classification on the ImageNette320 dataset, which contains images resized to 320x320 pixels.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/imagenette.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model with ImageNette320\nyolo classify train data=imagenette320 model=yolo11n-cls.pt epochs=100 imgsz=320\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Model with Comet ML Integration\nDESCRIPTION: Example of loading and training a YOLO11 model with automatic Comet ML logging enabled\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/comet.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Train the model\nresults = model.train(\n    data=\"coco8.yaml\",\n    project=\"comet-example-yolo11-coco128\",\n    batch=32,\n    save_period=1,\n    save_json=True,\n    epochs=3,\n)\n```\n\n----------------------------------------\n\nTITLE: Converting Absolute Coordinates to Relative Coordinates in YOLO11\nDESCRIPTION: This snippet demonstrates how to convert bounding box coordinates from absolute pixel values to relative coordinates (0 to 1) by dividing by the image dimensions. This is useful when working with prediction results from a custom YOLO11 model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/yolo-common-issues.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Convert absolute coordinates to relative coordinates\nx1 = x1 / 640  # Divide x-coordinates by image width\nx2 = x2 / 640\ny1 = y1 / 640  # Divide y-coordinates by image height\ny2 = y2 / 640\n```\n\n----------------------------------------\n\nTITLE: Handling Dataset Statistics with Ultralytics Data - Python\nDESCRIPTION: Provides access and manipulation features for dataset statistics within the Ultralytics data module through the HUBDatasetStats utility. This is intended for capturing metrics and summary information about datasets, potentially relying on integrations with Ultralytics cloud or hub resources. Input is typically a dataset specification or directory path; output includes structured statistics to aid in validation and analysis.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/data/utils.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n## ::: ultralytics.data.utils.HUBDatasetStats\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model to TorchScript Python\nDESCRIPTION: This command exports a YOLO11 model to the TorchScript format. No additional dependencies are needed besides having a YOLO11 model and PyTorch installed. The `model` parameter specifies the model file to export, and the `format` parameter sets the desired output format. This usage example wraps the command in Python with a shell execution call.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n!yolo export model=yolo11n.pt format=torchscript\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n Model on COCO8-Multispectral Dataset via CLI\nDESCRIPTION: Command-line interface (CLI) command for training a YOLO11n model on the COCO8-Multispectral dataset with specified parameters.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/coco8-multispectral.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Train YOLO11n on COCO8-Multispectral using the command line\nyolo detect train data=coco8-multispectral.yaml model=yolo11n.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Starting a Local MLflow Server\nDESCRIPTION: Command to start a local MLflow tracking server with a specified backend storage URI, which enables viewing and managing experiments through a web interface.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/mlflow.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmlflow server --backend-store-uri runs/mlflow\n```\n\n----------------------------------------\n\nTITLE: Code Profiling with Ultralytics Profile in Python\nDESCRIPTION: This code demonstrates how to use the Profile utility from Ultralytics to measure the execution time of code blocks. The Profile class can be used as a context manager with the 'with' statement to time operations, particularly useful for performance optimization.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/simple-utilities.md#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.utils.ops import Profile\n\nwith Profile(device=\"cuda:0\") as dt:\n    pass  # operation to measure\n\nprint(dt)\n# >>> \"Elapsed time is 9.5367431640625e-07 s\"\n```\n\n----------------------------------------\n\nTITLE: Dataset Validation Check\nDESCRIPTION: Python code to validate a dataset ZIP file before uploading to Ultralytics HUB. This ensures the dataset meets all formatting requirements and prevents upload rejection.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/hub/datasets.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.hub import check_dataset\n\ncheck_dataset(\"path/to/dataset.zip\", task=\"detect\")\n```\n\n----------------------------------------\n\nTITLE: Installing ONNX Runtime GPU Backend\nDESCRIPTION: Command to install ONNX Runtime with GPU support for NVIDIA hardware acceleration.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-ONNXRuntime/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install onnxruntime-gpu\n```\n\n----------------------------------------\n\nTITLE: FAQ Example: Training YOLO11n-pose Model via CLI\nDESCRIPTION: Command line interface example from the FAQ section showing how to train a pretrained YOLO11n-pose model on the Tiger-Pose dataset.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/tiger-pose.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo task=pose mode=train data=tiger-pose.yaml model=yolo11n-pose.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Training YOLOv5 Models with Pretrained Weights\nDESCRIPTION: Commands for training YOLOv5 models of different sizes (s, m, l, x) starting from pretrained weights. Recommended for small to medium-sized datasets like VOC, VisDrone, or GlobalWheat.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/tips_for_best_training_results.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython train.py --data custom.yaml --weights yolov5s.pt\npython train.py --data custom.yaml --weights yolov5m.pt\npython train.py --data custom.yaml --weights yolov5l.pt\npython train.py --data custom.yaml --weights yolov5x.pt\npython train.py --data custom.yaml --weights custom_pretrained.pt\n```\n\n----------------------------------------\n\nTITLE: Storing and Processing Keypoints Data with Keypoints Class — Python\nDESCRIPTION: The Keypoints class specializes in organizing and managing keypoint outputs like those from pose estimation tasks. It offers structures for storing coordinates, performing transformations, and evaluating pose accuracy. Requires PyTorch and accepts keypoint arrays as input, returning coordinate tensors ready for evaluation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/engine/results.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n## ::: ultralytics.engine.results.Keypoints\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration for YOLO Training\nDESCRIPTION: Example YAML configuration file defining training parameters including augmentations. This configuration can override the default settings.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/yolo-data-augmentation.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# train_custom.yaml\n# 'mode' is required only for CLI usage\nmode: train\ndata: coco8.yaml\nmodel: yolo11n.pt\nepochs: 100\nhsv_h: 0.03\nhsv_s: 0.6\nhsv_v: 0.5\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics via pip\nDESCRIPTION: This snippet provides instructions for installing the Ultralytics package via pip, offering users the latest stable release from PyPI. It requires the pip tool and internet access. Users can also opt for the development version by fetching the package directly from the GitHub repository. The expected input is the pip command and the output is the installed Ultralytics package.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/quickstart.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the ultralytics package from PyPI\npip install ultralytics\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Install the ultralytics package from GitHub\npip install git+https://github.com/ultralytics/ultralytics.git@main\n```\n\n----------------------------------------\n\nTITLE: Running YOLO-NAS Model Using CLI Commands\nDESCRIPTION: This Bash snippet shows how to use CLI to validate and run inference on a YOLO-NAS model using pretrained weights and a specific dataset. Execute the `yolo val` command to validate the model and `yolo predict` for inference. Ensure you have the YOLO executable and COCO-pretrained model file (`yolo_nas_s.pt`).\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolo-nas.md#2025-04-22_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\n# Load a COCO-pretrained YOLO-NAS-s model and validate it's performance on the COCO8 example dataset\nyolo val model=yolo_nas_s.pt data=coco8.yaml\n\n# Load a COCO-pretrained YOLO-NAS-s model and run inference on the 'bus.jpg' image\nyolo predict model=yolo_nas_s.pt source=path/to/bus.jpg\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 with Weights & Biases via CLI\nDESCRIPTION: Command line instruction for training a YOLO11 model with Weights & Biases integration, specifying dataset, number of epochs, project name, and run name.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/weights-biases.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Train a YOLO11 model with Weights & Biases\nyolo train data=coco8.yaml epochs=5 project=ultralytics name=yolo11n\n```\n\n----------------------------------------\n\nTITLE: Persistent Frame Tracking with YOLO and OpenCV\nDESCRIPTION: Implementation of continuous object tracking across video frames using YOLO with OpenCV integration and frame persistence.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/track/index.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import YOLO\n\n# Load the YOLO model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Open the video file\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\n\nwhile cap.isOpened():\n    success, frame = cap.read()\n    if success:\n        # Run tracking with persistence between frames\n        results = model.track(frame, persist=True)\n\n        # Visualize the results\n        annotated_frame = results[0].plot()\n        cv2.imshow(\"Tracking\", annotated_frame)\n\n        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n            break\n    else:\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Customizing Comet ML Logging for Image Predictions\nDESCRIPTION: Sets an environment variable to change the number of image predictions logged by Comet ML during YOLO11 training.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/comet.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"COMET_MAX_IMAGE_PREDICTIONS\"] = \"200\"\n```\n\n----------------------------------------\n\nTITLE: Running YOLOv5 Evolution Commands\nDESCRIPTION: Bash commands for running hyperparameter evolution on single or multiple GPUs, including continuous training options.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/hyperparameter_evolution.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Single-GPU\npython train.py --epochs 10 --data coco128.yaml --weights yolov5s.pt --cache --evolve\n\n# Multi-GPU with delay\nfor i in {0..7}; do\n  sleep $((30 * i))\n  echo \"Starting GPU $i...\"\n  nohup python train.py --epochs 10 --data coco128.yaml --weights yolov5s.pt --cache --device $i --evolve > \"evolve_gpu_$i.log\" &\ndone\n```\n\n----------------------------------------\n\nTITLE: Enabling/Disabling Weights & Biases Logging in Ultralytics\nDESCRIPTION: Command line instructions for enabling or disabling Weights & Biases logging in Ultralytics using the yolo settings command.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/weights-biases.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Enable Weights & Biases logging\nyolo settings wandb=True\n\n# Disable Weights & Biases logging\nyolo settings wandb=False\n```\n\n----------------------------------------\n\nTITLE: Running YOLO11 Prediction on an Image using CLI\nDESCRIPTION: This command uses YOLO11n model to perform object detection on a sample image. It demonstrates how to use the YOLO CLI for prediction tasks.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nyolo predict model=yolo11n.pt source='https://ultralytics.com/images/zidane.jpg'\n```\n\n----------------------------------------\n\nTITLE: Training YOLOv5 with Custom Dataset\nDESCRIPTION: Example command for training YOLOv5 using a custom dataset with specific parameters\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/environments/google_cloud_quickstart_tutorial.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Example: Train YOLOv5s on a custom dataset for 100 epochs\npython train.py --img 640 --batch 16 --epochs 100 --data custom_dataset.yaml --weights yolov5s.pt\n```\n\n----------------------------------------\n\nTITLE: Running Hyperparameter Optimization with ClearML\nDESCRIPTION: This command runs a hyperparameter optimization script for YOLOv5 using ClearML and Optuna.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/clearml_logging_integration.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install optuna\npython utils/loggers/clearml/hpo.py\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Threaded YOLO Object Tracking in Python\nDESCRIPTION: This Python script showcases multi-threaded object tracking using the Ultralytics YOLO library and Python's `threading` module. It defines a function `run_tracker_in_thread` that processes a single video stream, performing object tracking and displaying results in a separate window. The main part of the script loads two distinct YOLO models (`yolo11n.pt`, `yolo11n-seg.pt`), sets up two threads to track objects in different video files concurrently using these models, starts the threads, and waits for their completion before closing all OpenCV windows. Dependencies include `threading`, `cv2`, and the Ultralytics `YOLO` library, along with the specified model and video files.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/trackers/README.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Python\nimport threading\n\nimport cv2\n\nfrom ultralytics import YOLO\n\n\ndef run_tracker_in_thread(filename, model, file_index):\n    \"\"\"\n    Runs tracking on a video file using the specified model.\n\n    Args:\n        filename (str): The path to the video file.\n        model (YOLO): The YOLO model to use for tracking.\n        file_index (int): An index to identify the video thread.\n    \"\"\"\n    video = cv2.VideoCapture(filename)  # Read the video file\n    while True:\n        ret, frame = video.read()  # Read the video frames\n        if not ret:\n            break  # Exit the loop if no more frames are available\n        results = model.track(frame, persist=True)  # Track objects in the frame\n        res_plotted = results[0].plot()  # Visualize the results\n        cv2.imshow(f\"Tracking_Stream_{file_index}\", res_plotted)  # Display the results\n\n        key = cv2.waitKey(1)  # Wait for a key event\n        if key == ord(\"q\"):  # Check if the 'q' key was pressed\n            break  # Exit the loop if 'q' is pressed\n\n    video.release()  # Release the video capture object\n\n\n# Load the models\nmodel1 = YOLO(\"yolo11n.pt\")\nmodel2 = YOLO(\"yolo11n-seg.pt\")\n\n# Define the video files for the trackers\nvideo_file1 = \"path/to/video1.mp4\"  # Path to video file 1\nvideo_file2 = \"path/to/video2.mp4\"  # Path to video file 2\n\n# Create the tracker threads\ntracker_thread1 = threading.Thread(target=run_tracker_in_thread, args=(video_file1, model1, 1), daemon=True)\ntracker_thread2 = threading.Thread(target=run_tracker_in_thread, args=(video_file2, model2, 2), daemon=True)\n\n# Start the tracker threads\ntracker_thread1.start()\ntracker_thread2.start()\n\n# Wait for the tracker threads to finish\ntracker_thread1.join()\ntracker_thread2.join()\n\n# Clean up and close windows\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Converting YOLO11 Models to ONNX Format\nDESCRIPTION: CLI command to export YOLO11 models to ONNX format for compatibility with DeepSparse. This converts a PyTorch model to ONNX with specified task, mode, and opset version.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/neural-magic.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nyolo task=detect mode=export model=yolo11n.pt format=onnx opset=13\n```\n\n----------------------------------------\n\nTITLE: Generating Sliding Windows for Image Tiling - Ultralytics DOTA Utilities - Python\nDESCRIPTION: Supplies a function to generate coordinates for sliding windows across an image, facilitating crop-based dataset splitting and augmentation strategies. Useful for tiling large images into smaller regions and handling detection tasks. Accepts parameters such as image shape and window stride/size; emits a list of window coordinates.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/data/split_dota.md#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef get_windows(img_shape, window_size, stride):\n    # Generates window coordinates for sliding window cropping over the image\n    pass  # Implementation yields (x1, y1, x2, y2) window bounding boxes\n```\n\n----------------------------------------\n\nTITLE: Running Ultralytics in Docker Container\nDESCRIPTION: This snippet demonstrates how to pull and run the Ultralytics Docker image for consistent cross-environment performance. Users must have Docker installed and internet access. It highlights the use of GPU support and IPC sharing for enhanced performance and notes the ability to mount host directories into the container.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/quickstart.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Set image name as a variable\nt=ultralytics/ultralytics:latest\n\n# Pull the latest ultralytics image from Docker Hub\nsudo docker pull $t\n\n# Run the ultralytics image in a container with GPU support\nsudo docker run -it --ipc=host --gpus all $t            # all GPUs\nsudo docker run -it --ipc=host --gpus \"device=2,3\" $t # specify GPUs\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Mount local directory to a directory inside the container\nsudo docker run -it --ipc=host --gpus all -v /path/on/host:/path/in/container $t\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Segmentation Model\nDESCRIPTION: Examples showing how to export a YOLO11 segmentation model to different formats like ONNX using both Python and CLI methods.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/segment.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-seg.pt\")  # load an official model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom trained model\n\n# Export the model\nmodel.export(format=\"onnx\")\n```\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=yolo11n-seg.pt format=onnx  # export official model\nyolo export model=path/to/best.pt format=onnx # export custom trained model\n```\n\n----------------------------------------\n\nTITLE: Splitting Images and Labels for Dataset Preparation - Ultralytics DOTA Utilities - Python\nDESCRIPTION: Produces a function that systematically splits input DOTA images and their labels into cropped tiles with updated annotation files, ready for model training or validation. Integrates previous utilities (windowing, cropping, label mapping) to automate dataset partitioning. Inputs are typically directory paths and configuration parameters; outputs include multiple subfolders of processed images and labels.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/data/split_dota.md#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndef split_images_and_labels(img_dir, label_dir, save_dir, window_size, stride):\n    # Splits all images and labels in the directory into tiles for training/validation\n    pass  # Implementation recursively processes, crops, and saves tiled data\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenVINO for Throughput Optimization in Python\nDESCRIPTION: This snippet demonstrates how to configure OpenVINO for throughput optimization using performance hints. It sets the performance mode to THROUGHPUT and compiles the model for GPU execution.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/optimizing-openvino-latency-vs-throughput-modes.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openvino.properties.hint as hints\n\nconfig = {hints.performance_mode: hints.PerformanceMode.THROUGHPUT}\ncompiled_model = core.compile_model(model, \"GPU\", config)\n```\n\n----------------------------------------\n\nTITLE: Creating DDP Command for Ultralytics Training\nDESCRIPTION: Function to generate a Distributed Data Parallel (DDP) command for initiating multi-node training in Ultralytics. It constructs the command string for distributed training execution.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/dist.md#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nultralytics.utils.dist.generate_ddp_command\n```\n\n----------------------------------------\n\nTITLE: GPU Device Configuration in YAML\nDESCRIPTION: Example YAML configuration for specifying GPU device selection, showing how to explicitly set the training device.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/yolo-common-issues.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ndevice: 0\n```\n\n----------------------------------------\n\nTITLE: Customizing Inference Logic in Python\nDESCRIPTION: Demonstrates how to modify the `inference.py` file to customize the output function for the deployed YOLO11 model, formatting results as JSON. JSON library is used.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/amazon-sagemaker.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\n\ndef output_fn(prediction_output):\n    \"\"\"Formats model outputs as JSON string, extracting attributes like boxes, masks, keypoints.\"\"\"\n    infer = {}\n    for result in prediction_output:\n        if result.boxes is not None:\n            infer[\"boxes\"] = result.boxes.numpy().data.tolist()\n        # Add more processing logic if necessary\n    return json.dumps(infer)\n```\n\n----------------------------------------\n\nTITLE: DeepSparse Python Pipeline Implementation\nDESCRIPTION: Creates and runs a DeepSparse Pipeline for YOLOv5 object detection using the Python API.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/neural_magic_pruning_quantization.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom deepsparse import Pipeline\n\n# list of images in local filesystem\nimages = [\"basilica.jpg\"]\n\n# create Pipeline\nmodel_stub = \"zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned65_quant-none\"\nyolo_pipeline = Pipeline.create(\n    task=\"yolo\",\n    model_path=model_stub,\n)\n\n# run inference on images, receive bounding boxes + classes\npipeline_outputs = yolo_pipeline(images=images, iou_thres=0.6, conf_thres=0.001)\nprint(pipeline_outputs)\n```\n\n----------------------------------------\n\nTITLE: Freezing All YOLOv5 Layers Except Final Detection Layers\nDESCRIPTION: This bash command shows how to freeze almost the entire YOLOv5 network, leaving only the final output convolution layers trainable. It's useful for adjusting the model for a different number of output classes while keeping most learned features intact.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/transfer_learning_with_frozen_layers.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython train.py --weights yolov5m.pt --data your_dataset.yaml --freeze 24\n```\n\n----------------------------------------\n\nTITLE: Including Chart.js Library via CDN in HTML\nDESCRIPTION: This HTML script tag asynchronously loads the Chart.js library from its content delivery network (CDN). This inclusion is essential for enabling chart rendering functionalities on the webpage, likely used by other scripts to display benchmark comparisons visually. The `async` attribute allows the page parsing to continue without waiting for the script download.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov7.md#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<script async src=\"https://cdn.jsdelivr.net/npm/chart.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: Visualizing YOLO11 with SAHI Prediction Results\nDESCRIPTION: Code to export and visualize prediction results when using YOLO11 with SAHI. The snippet saves visualization to a specified directory and displays it using IPython.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/sahi-tiled-inference.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import Image\n\nresult.export_visuals(export_dir=\"demo_data/\")\nImage(\"demo_data/prediction_visual.png\")\n```\n\n----------------------------------------\n\nTITLE: Cropping and Saving Image Tiles with Labels - Ultralytics DOTA Utilities - Python\nDESCRIPTION: Provides a function that crops sections of larger DOTA images based on sliding window coordinates and saves each tile along with its associated label information. Coordinates the use of window generation, object mapping, and file I/O for both images and annotation files. Inputs include path references, window/label data, and save directories; outputs are saved image tiles and annotation files on disk.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/data/split_dota.md#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndef crop_and_save(img_path, objects, windows, save_dir):\n    # Crops image tiles from original using window coordinates and saves tiles with labels\n    pass  # Implementation performs cropping, label adjustment, disk writing\n```\n\n----------------------------------------\n\nTITLE: ClearML Training Epoch End Callback\nDESCRIPTION: Callback function that executes at the end of each training epoch\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/callbacks/clearml.md#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nultralytics.utils.callbacks.clearml.on_train_epoch_end\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Dependencies\nDESCRIPTION: Commands to clone and install Ultralytics repository with required dependencies for model export\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/deepstream-nvidia-jetson.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd ~\npip install -U pip\ngit clone https://github.com/ultralytics/ultralytics\ncd ultralytics\npip install -e \".[export]\" onnxslim\n```\n\n----------------------------------------\n\nTITLE: Using YOLOv5 with PyTorch Hub and TTA\nDESCRIPTION: Load a YOLOv5 model from PyTorch Hub and perform inference with Test-Time Augmentation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/test_time_augmentation.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n# Model\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\")  # or yolov5m, yolov5x, custom\n\n# Images\nimg = \"https://ultralytics.com/images/zidane.jpg\"  # or file, PIL, OpenCV, numpy, multiple\n\n# Inference\nresults = model(img, augment=True)  # <--- TTA inference\n\n# Results\nresults.print()  # or .show(), .save(), .crop(), .pandas(), etc.\n```\n\n----------------------------------------\n\nTITLE: Bounding Box Instance Management\nDESCRIPTION: Class to manage bounding box data with functions for format conversion, scaling, area calculation and offset handling.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/simple-utilities.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nfrom ultralytics.utils.instance import Bboxes\n\nboxes = Bboxes(\n    bboxes=np.array(\n        [\n            [22.878, 231.27, 804.98, 756.83],\n            [48.552, 398.56, 245.35, 902.71],\n            [669.47, 392.19, 809.72, 877.04],\n            [221.52, 405.8, 344.98, 857.54],\n            [0, 550.53, 63.01, 873.44],\n            [0.0584, 254.46, 32.561, 324.87],\n        ]\n    ),\n    format=\"xyxy\",\n)\n\nboxes.areas()\nboxes.convert(\"xywh\")\nprint(boxes.bboxes)\n```\n\n----------------------------------------\n\nTITLE: Initializing Comet ML for YOLO11 Model Training Logging\nDESCRIPTION: Sets up Comet ML for tracking experiments when training YOLO11 models. Requires installing the comet_ml package and initializing the client. Users need to sign in to their Comet account and configure their API key in environment variables or within the script.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/train.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# pip install comet_ml\nimport comet_ml\n\ncomet_ml.init()\n```\n\n----------------------------------------\n\nTITLE: Dataset Configuration with YAML\nDESCRIPTION: YAML configuration file defining the dataset paths and parameters for COCO-Seg\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/coco.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/coco.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Importing RegionCounter Class from Ultralytics\nDESCRIPTION: The RegionCounter class can be found in the ultralytics.solutions.region_counter module. This class provides functionality for counting objects in specific regions of video streams or images.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/solutions/region_counter.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.solutions.region_counter import RegionCounter\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n on LVIS Dataset via CLI\nDESCRIPTION: Command-line interface command for training a YOLO11n model on the LVIS dataset with the same parameters as the Python example, demonstrating the CLI alternative approach.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/lvis.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo detect train data=lvis.yaml model=yolo11n.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Freezing YOLOv5 Backbone for Transfer Learning\nDESCRIPTION: This bash command demonstrates how to freeze the entire backbone (layers 0 through 9) of YOLOv5 for transfer learning. It's useful when adapting the model to new object classes while retaining general feature extraction capabilities.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/transfer_learning_with_frozen_layers.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython train.py --weights yolov5m.pt --data your_dataset.yaml --freeze 10\n```\n\n----------------------------------------\n\nTITLE: Running Edge TPU Inference - CLI Implementation\nDESCRIPTION: Command line interface command to perform inference using an Edge TPU compatible YOLO model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/coral-edge-tpu-on-raspberry-pi.md#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nyolo predict model=path/to/edgetpu_model.tflite source=path/to/source.png # Load an official model or custom model\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for YOLO11 NCNN Export\nDESCRIPTION: Command to install the Ultralytics package required for exporting YOLO11 models to NCNN format.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/ncnn.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the required package for YOLO11\npip install ultralytics\n```\n\n----------------------------------------\n\nTITLE: Disabling Confusion Matrix Logging\nDESCRIPTION: Configuration to disable per-epoch confusion matrix logging in Comet ML\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/comet.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"COMET_EVAL_LOG_CONFUSION_MATRIX\"] = \"false\"\n```\n\n----------------------------------------\n\nTITLE: Running Ultralytics Commands via CLI - Bash\nDESCRIPTION: These CLI snippets demonstrate how to invoke various YOLO tasks directly from the command line. Each command lets you perform high-level tasks such as model training, prediction, validation, export, and advanced video processing (counting, workouts, queuing, streamlit inference) using argument-value pairs. No Python code is required, and arguments are passed as space-delimited key-value pairs (no '--' options). Outputs and model artifacts are managed by the Ultralytics framework. The input and output formats depend on the selected task and mode.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/quickstart.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nyolo TASK MODE ARGS\n```\n\nLANGUAGE: bash\nCODE:\n```\nyolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n```\n\nLANGUAGE: bash\nCODE:\n```\nyolo predict model=yolo11n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n```\n\nLANGUAGE: bash\nCODE:\n```\nyolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n```\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=yolo11n-cls.pt format=onnx imgsz=224,128\n```\n\nLANGUAGE: bash\nCODE:\n```\nyolo solutions count show=True\n```\n\nLANGUAGE: bash\nCODE:\n```\nyolo solutions count source=\"path/to/video.mp4\" # specify video file path\n```\n\nLANGUAGE: bash\nCODE:\n```\nyolo solutions workout show=True\n```\n\nLANGUAGE: bash\nCODE:\n```\nyolo solutions workout source=\"path/to/video.mp4\" # specify video file path\n```\n\nLANGUAGE: bash\nCODE:\n```\nyolo solutions workout kpts=\"[5, 11, 13]\" # left side\n```\n\nLANGUAGE: bash\nCODE:\n```\nyolo solutions workout kpts=\"[6, 12, 14]\" # right side\n```\n\nLANGUAGE: bash\nCODE:\n```\nyolo solutions queue show=True\n```\n\nLANGUAGE: bash\nCODE:\n```\nyolo solutions queue source=\"path/to/video.mp4\" # specify video file path\n```\n\nLANGUAGE: bash\nCODE:\n```\nyolo solutions queue region=\"[(20, 400), (1080, 400), (1080, 360), (20, 360)]\" # configure queue coordinates\n```\n\nLANGUAGE: bash\nCODE:\n```\nyolo solutions inference\n```\n\nLANGUAGE: bash\nCODE:\n```\nyolo solutions inference model=\"path/to/model.pt\" # use model fine-tuned with Ultralytics Python package\n```\n\nLANGUAGE: bash\nCODE:\n```\nyolo help\nyolo checks\nyolo version\nyolo settings\nyolo copy-cfg\nyolo cfg\nyolo solutions help\n```\n\n----------------------------------------\n\nTITLE: Configuring COCO8-Seg Dataset in YAML\nDESCRIPTION: YAML configuration file for the COCO8-Seg dataset that defines paths, classes, and other dataset parameters for use with Ultralytics models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/coco8-seg.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/coco8-seg.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Inference with YOLO Model using CLI\nDESCRIPTION: Command line interface command to perform inference using a trained YOLO model\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/african-wildlife.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Start prediction with a finetuned *.pt model\nyolo detect predict model='path/to/best.pt' imgsz=640 source=\"https://ultralytics.com/assets/african-wildlife-sample.jpg\"\n```\n\n----------------------------------------\n\nTITLE: Installing TensorFlow Lite Runtime on Raspberry Pi\nDESCRIPTION: Command to install or update the TensorFlow Lite runtime on Raspberry Pi.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/coral-edge-tpu-on-raspberry-pi.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install -U tflite-runtime\n```\n\n----------------------------------------\n\nTITLE: Implementing Circle Label Annotation with YOLO in Python\nDESCRIPTION: This snippet demonstrates how to annotate detected objects with circle labels using the YOLO model and OpenCV. It processes a video file frame by frame, detects objects with YOLO, and adds circle annotations to each bounding box before saving the output to a new video file.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/simple-utilities.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import YOLO\nfrom ultralytics.solutions.solutions import SolutionAnnotator\nfrom ultralytics.utils.plotting import colors\n\nmodel = YOLO(\"yolo11s.pt\")\nnames = model.names\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\n\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\nwriter = cv2.VideoWriter(\"Ultralytics circle annotation.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), fps, (w, h))\n\nwhile True:\n    ret, im0 = cap.read()\n    if not ret:\n        break\n\n    annotator = SolutionAnnotator(im0)\n    results = model.predict(im0)\n    boxes = results[0].boxes.xyxy.cpu()\n    clss = results[0].boxes.cls.cpu().tolist()\n\n    for box, cls in zip(boxes, clss):\n        annotator.circle_label(box, label=names[int(cls)], color=colors(cls, True))\n\n    writer.write(im0)\n    cv2.imshow(\"Ultralytics circle annotation\", im0)\n\n    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n        break\n\nwriter.release()\ncap.release()\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Configuring YOLOv5 to Use a Dataset from Comet Artifacts\nDESCRIPTION: Example YAML configuration showing how to reference a dataset stored in Comet Artifacts for YOLOv5 training.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/comet_logging_integration.md#2025-04-22_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\n# Contents of artifact.yaml file\npath: \"comet://WORKSPACE_NAME>/ARTIFACT_NAME:ARTIFACT_VERSION_OR_ALIAS\"\n```\n\n----------------------------------------\n\nTITLE: Running YOLO Tasks via CLI and Python\nDESCRIPTION: Demonstrates how to execute YOLO tasks using both command-line interface and Python code. The syntax shows how to specify the task, mode, and additional arguments.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/cfg.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nyolo TASK MODE ARGS\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a YOLO model from a pre-trained weights file\nmodel = YOLO(\"yolo11n.pt\")\n\n# Run MODE mode using the custom arguments ARGS (guess TASK)\nmodel.MODE(ARGS)\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 with Weights & Biases in Python\nDESCRIPTION: Python code demonstrating how to load a YOLO model and train it with Weights & Biases integration. This specifies project and run name for W&B logging.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/weights-biases.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a YOLO model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Train and Fine-Tune the Model\nmodel.train(data=\"coco8.yaml\", epochs=5, project=\"ultralytics\", name=\"yolo11n\")\n```\n\n----------------------------------------\n\nTITLE: Defining VOC Dataset Configuration in YAML\nDESCRIPTION: YAML configuration file for the VOC dataset, specifying paths, train/val/test splits, and object classes. This file is used to set up the dataset for training YOLO models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/voc.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/VOC.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Running Queue Management Examples with Ultralytics YOLO CLI\nDESCRIPTION: This snippet demonstrates how to use the Ultralytics YOLO CLI to run queue management examples, including passing a source video and specifying queue coordinates.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/queue-management.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Run a queue example\nyolo solutions queue show=True\n\n# Pass a source video\nyolo solutions queue source=\"path/to/video.mp4\"\n\n# Pass queue coordinates\nyolo solutions queue region=\"[(20, 400), (1080, 400), (1080, 360), (20, 360)]\"\n```\n\n----------------------------------------\n\nTITLE: Visualizing 3D Point Clouds with YOLO in ROS using Python\nDESCRIPTION: The purpose of this code is to visualize 3D point clouds by using YOLO for segmentation within the ROS framework. It relies on Open3D for visualization and converts `sensor_msgs/PointCloud2` messages to numpy arrays. The YOLO model segments RGB images, and the code applies the segmentation mask to the point cloud. It requires dependencies such as `open3d`, `ros_numpy`, `rospy`, `sensor_msgs`, and `ultralytics`. Input parameters include RGB images for segmentation and the point cloud data. The expected output is a visualization of segmented objects within the 3D space. This functionality is crucial for applications like navigation and manipulation in robotics.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/ros-quickstart.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport sys\n\nimport open3d as o3d\nimport ros_numpy\nimport rospy\nfrom sensor_msgs.msg import PointCloud2\n\nfrom ultralytics import YOLO\n\nrospy.init_node(\"ultralytics\")\nsegmentation_model = YOLO(\"yolo11m-seg.pt\")\n\ndef pointcloud2_to_array(pointcloud2):\n    pc_array = ros_numpy.point_cloud2.pointcloud2_to_array(pointcloud2)\n    split = ros_numpy.point_cloud2.split_rgb_field(pc_array)\n    rgb = np.stack([split[\"b\"], split[\"g\"], split[\"r\"]], axis=2)\n    xyz = ros_numpy.point_cloud2.get_xyz_points(pc_array, remove_nans=False)\n    xyz = np.array(xyz).reshape((pointcloud2.height, pointcloud2.width, 3))\n    return xyz, rgb\n\nros_cloud = rospy.wait_for_message(\"/camera/depth/points\", PointCloud2)\nxyz, rgb = pointcloud2_to_array(ros_cloud)\nresult = segmentation_model(rgb)\n\nif not len(result[0].boxes.cls):\n    print(\"No objects detected\")\n    sys.exit()\n\nclasses = result[0].boxes.cls.cpu().numpy().astype(int)\nfor index, class_id in enumerate(classes):\n    mask = result[0].masks.data.cpu().numpy()[index, :, :].astype(int)\n    mask_expanded = np.stack([mask, mask, mask], axis=2)\n\n    obj_rgb = rgb * mask_expanded\n    obj_xyz = xyz * mask_expanded\n\n    pcd = o3d.geometry.PointCloud()\n    pcd.points = o3d.utility.Vector3dVector(obj_xyz.reshape((-1, 3)))\n    pcd.colors = o3d.utility.Vector3dVector(obj_rgb.reshape((-1, 3)) / 255)\n    o3d.visualization.draw_geometries([pcd])\n```\n\n----------------------------------------\n\nTITLE: Defining HUBModelError Class for Ultralytics YOLO in Python\nDESCRIPTION: A custom exception class that extends Python's Exception to handle errors related to HUB model fetching or processing. This specialized exception allows for more precise error reporting when interacting with models from the Ultralytics HUB.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/errors.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nclass HUBModelError(Exception):\n    \"\"\"Error raised by HUB Models when model fetching fails.\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Repository Setup for Ultralytics YOLO11 Region Counting\nDESCRIPTION: Bash commands for cloning the Ultralytics repository and navigating to the region counter example directory\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/region-counting.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ultralytics/ultralytics\ncd ultralytics/examples/YOLOv8-Region-Counter\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model to TF GraphDef via CLI\nDESCRIPTION: This snippet shows how to export a YOLO11 PyTorch model to TF GraphDef format and run inference using the command-line interface. It demonstrates the use of the 'yolo' command for both export and prediction tasks.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tf-graphdef.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Export a YOLO11n PyTorch model to TF GraphDef format\nyolo export model=\"yolo11n.pt\" format=\"pb\" # creates 'yolo11n.pb'\n\n# Run inference with the exported model\nyolo predict model=\"yolo11n.pb\" source=\"https://ultralytics.com/images/bus.jpg\"\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Tiger-Pose Trained Model via CLI\nDESCRIPTION: Command line interface commands to perform inference using a Tiger-Pose trained model on a video source, showing results visually.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/tiger-pose.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Run inference using a tiger-pose trained model\nyolo task=pose mode=predict source=\"https://youtu.be/MIBAT6BGE6U\" show=True model=\"path/to/best.pt\"\n```\n\n----------------------------------------\n\nTITLE: Defining YOLOv5 Fitness Function\nDESCRIPTION: Python function that calculates model fitness using weighted metrics including mAP@0.5 and mAP@0.5:0.95.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/hyperparameter_evolution.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef fitness(x):\n    \"\"\"Return model fitness as the sum of weighted metrics [P, R, mAP@0.5, mAP@0.5:0.95].\"\"\"\n    w = [0.0, 0.0, 0.1, 0.9]  # weights for [P, R, mAP@0.5, mAP@0.5:0.95]\n    return (x[:, :4] * w).sum(1)\n```\n\n----------------------------------------\n\nTITLE: YOLOv5 Ensemble Model Testing\nDESCRIPTION: Command to test an ensemble of YOLOv5x and YOLOv5l6 models together on the COCO dataset for improved accuracy.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/model_ensembling.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython val.py --weights yolov5x.pt yolov5l6.pt --data coco.yaml --img 640 --half\n```\n\n----------------------------------------\n\nTITLE: Splitting DOTA Images for Training in Python\nDESCRIPTION: This Python code demonstrates how to split large DOTA dataset images into smaller resolutions for more efficient training. It uses functions from the ultralytics.data.split_dota module to process both training/validation and test sets at multiple scales.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/obb/dota-v2.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.data.split_dota import split_test, split_trainval\n\n# split train and val set, with labels.\nsplit_trainval(\n    data_root=\"path/to/DOTAv1.0/\",\n    save_dir=\"path/to/DOTAv1.0-split/\",\n    rates=[0.5, 1.0, 1.5],  # multiscale\n    gap=500,\n)\n# split test set, without labels.\nsplit_test(\n    data_root=\"path/to/DOTAv1.0/\",\n    save_dir=\"path/to/DOTAv1.0-split/\",\n    rates=[0.5, 1.0, 1.5],  # multiscale\n    gap=500,\n)\n```\n\n----------------------------------------\n\nTITLE: Batch Conversion of Polygon Annotations to Masks - Python\nDESCRIPTION: The polygons2masks utility transforms multiple polygonal annotations into a list or array of binary masks, supporting batch segmentation workflows. Inputs are collections of polygon coordinates and target mask shapes, outputs are aggregate mask objects. Relies on polygon2mask and array manipulation libraries.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/data/utils.md#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n## ::: ultralytics.data.utils.polygons2masks\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 to NCNN Format using Command Line Interface\nDESCRIPTION: CLI commands to export a YOLO11n PyTorch model to NCNN format and then run inference with the exported model. Useful for scripting and automation workflows.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/ncnn.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Export a YOLO11n PyTorch model to NCNN format\nyolo export model=yolo11n.pt format=ncnn # creates '/yolo11n_ncnn_model'\n\n# Run inference with the exported model\nyolo predict model='./yolo11n_ncnn_model' source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Setting OpenCV Environment Variable\nDESCRIPTION: Sets the OPENCV environment variable required for INT8 calibration\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/deepstream-nvidia-jetson.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENCV=1\n```\n\n----------------------------------------\n\nTITLE: Customizing YOLOv5 Output Classes\nDESCRIPTION: Demonstrates how to load a YOLOv5 model with a custom number of output classes, which is useful when adapting the model to a specific dataset with fewer or more classes than the default 80 COCO classes.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/pytorch_hub_model_loading.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", classes=10)\n```\n\n----------------------------------------\n\nTITLE: Auto-split Dataset Utility\nDESCRIPTION: Function to automatically split a dataset into train/val/test splits using random sampling.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/simple-utilities.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.data.utils import autosplit\n\nautosplit(\n    path=\"path/to/images\",\n    weights=(0.9, 0.1, 0.0),  # (train, validation, test) fractional splits\n    annotated_only=False,  # split only images with annotation file when True\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Weights & Biases for YOLO11 Tuning Integration Bash\nDESCRIPTION: This short bash snippet installs the wandb experiment tracking tool, a common integration for monitoring Ray Tune runs. Input/output is limited to package installation. Intended for environments where pip is configured.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/ray-tune.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npip install wandb\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Model in Python\nDESCRIPTION: Demonstrates how to load a pretrained YOLO11 model and train it on a custom dataset using Python. Includes setting epochs and image size parameters.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/index.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolo11n.pt\")  # Load a pretrained model\nresults = model.train(data=\"path/to/your_dataset.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Executing SQL Queries\nDESCRIPTION: Performs SQL queries on dataset to filter specific conditions\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/explorer.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntable = exp.sql_query(\"WHERE labels LIKE '%person, person%' AND labels LIKE '%dog%' LIMIT 10\")\nexp.plot_sql_query(\"WHERE labels LIKE '%person, person%' AND labels LIKE '%dog%' LIMIT 10\", labels=True)\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on MNIST Dataset - Python Implementation\nDESCRIPTION: Code example showing how to load a pretrained YOLO model and train it on the MNIST dataset for 100 epochs with 32x32 image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/mnist.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-cls.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"mnist\", epochs=100, imgsz=32)\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n on LVIS Dataset in Python\nDESCRIPTION: Python code snippet for training a YOLO11n model on the LVIS dataset for 100 epochs with an image size of 640. This demonstrates how to load a pretrained model and start the training process.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/lvis.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"lvis.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package\nDESCRIPTION: Command to install the Ultralytics package via pip package manager\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/README.zh-CN.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics\n```\n\n----------------------------------------\n\nTITLE: Loading YOLOv5 Model and Running Simple Inference\nDESCRIPTION: Demonstrates how to load a pretrained YOLOv5s model from PyTorch Hub and perform inference on a single image. Returns detection results in a pandas DataFrame format with bounding box coordinates and confidence scores.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/pytorch_hub_model_loading.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n# Model\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\")\n\n# Image\nim = \"https://ultralytics.com/images/zidane.jpg\"\n\n# Inference\nresults = model(im)\n\nresults.pandas().xyxy[0]\n#      xmin    ymin    xmax   ymax  confidence  class    name\n# 0  749.50   43.50  1148.0  704.5    0.874023      0  person\n# 1  433.50  433.50   517.5  714.5    0.687988     27     tie\n# 2  114.75  195.75  1095.0  708.0    0.624512      0  person\n# 3  986.00  304.00  1028.0  420.0    0.286865     27     tie\n```\n\n----------------------------------------\n\nTITLE: Using Object Counting Solution in Bash\nDESCRIPTION: Commands to use the pre-built object counting solution with optional video source specification.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/cli.md#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nyolo solutions count show=True\nyolo solutions count source=\"path/to/video.mp4\" # specify video file path\n```\n\n----------------------------------------\n\nTITLE: Setting Up Triton Model Repository Structure\nDESCRIPTION: Code to create the required directory structure for Triton model deployment\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/triton-inference-server.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\n\n# Define paths\nmodel_name = \"yolo\"\ntriton_repo_path = Path(\"tmp\") / \"triton_repo\"\ntriton_model_path = triton_repo_path / model_name\n\n# Create directories\n(triton_model_path / \"1\").mkdir(parents=True, exist_ok=True)\n```\n\n----------------------------------------\n\nTITLE: Training YOLOE Visual Prompt Model\nDESCRIPTION: Demonstrates training a YOLOE model with visual prompts, including freezing layers except for the SAVPE module.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yoloe.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLOE\nfrom ultralytics.models.yolo.yoloe import YOLOEVPTrainer\n\ndata = dict(\n    train=dict(\n        yolo_data=[\"Objects365.yaml\"],\n        grounding_data=[\n            dict(\n                img_path=\"../datasets/flickr/full_images/\",\n                json_file=\"../datasets/flickr/annotations/final_flickr_separateGT_train_segm.json\",\n            ),\n            dict(\n                img_path=\"../datasets/mixed_grounding/gqa/images\",\n                json_file=\"../datasets/mixed_grounding/annotations/final_mixed_train_no_coco_segm.json\",\n            ),\n        ],\n    ),\n    val=dict(yolo_data=[\"lvis.yaml\"]),\n)\n\nmodel = YOLOE(\"yoloe-11l-seg.pt\")\n\nhead_index = len(model.model.model) - 1\nfreeze = list(range(0, head_index))\nfor name, child in model.model.model[-1].named_children():\n    if \"savpe\" not in name:\n        freeze.append(f\"{head_index}.{name}\")\n\nmodel.train(\n    data=data,\n    batch=128,\n    epochs=2,\n    close_mosaic=2,\n    optimizer=\"AdamW\",\n    lr0=16e-3,\n    warmup_bias_lr=0.0,\n    weight_decay=0.025,\n    momentum=0.9,\n    workers=4,\n    trainer=YOLOEVPTrainer,\n    device=\"0,1,2,3,4,5,6,7\",\n    freeze=freeze,\n)\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Object Detection Model with Python API\nDESCRIPTION: Demonstrates training a YOLO11 object detection model using the Ultralytics Python API. This example loads a pre-trained YOLO11n model and trains it on the COCO8 dataset for 100 epochs with 640x640 image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/train.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Representing Oriented Bounding Boxes with OBB Class — Python\nDESCRIPTION: The OBB class manages oriented bounding box data, supporting geometric transformation and alignment tasks where standard axis-aligned bounding boxes are inadequate. Relies on PyTorch for tensor operations and accepts box parameter arrays or tensors, outputting structured OBB representations for subsequent processing.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/engine/results.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n## ::: ultralytics.engine.results.OBB\n```\n\n----------------------------------------\n\nTITLE: Dataset Configuration in YAML\nDESCRIPTION: YAML configuration file for the Global Wheat Head Dataset defining paths and dataset properties\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/globalwheat2020.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/GlobalWheat2020.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Profiling YOLOv8 Inference Performance\nDESCRIPTION: Command to run YOLOv8 inference with performance profiling enabled in Rust.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-ONNXRuntime-Rust/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncargo run --release -- --trt --fp16 --profile --model MODEL_PATH.onnx --source SOURCE_IMAGE.jpg\n```\n\n----------------------------------------\n\nTITLE: Compiling DeepStream Library\nDESCRIPTION: Commands to set CUDA version and compile the DeepStream custom implementation\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/deepstream-nvidia-jetson.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport CUDA_VER=12.6\nmake -C nvdsinfer_custom_impl_Yolo clean && make -C nvdsinfer_custom_impl_Yolo\n```\n\n----------------------------------------\n\nTITLE: Generating Heatmaps Using CLI Commands\nDESCRIPTION: Command line interface examples for generating heatmaps with Ultralytics YOLO. Shows basic usage, custom video source specification, colormap selection, and object counting with region definition.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/heatmaps.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Run a heatmap example\nyolo solutions heatmap show=True\n\n# Pass a source video\nyolo solutions heatmap source=\"path/to/video.mp4\"\n\n# Pass a custom colormap\nyolo solutions heatmap colormap=cv2.COLORMAP_INFERNO\n\n# Heatmaps + object counting\nyolo solutions heatmap region=\"[(20, 400), (1080, 400), (1080, 360), (20, 360)]\"\n```\n\n----------------------------------------\n\nTITLE: Setting up ROS Image Publishers\nDESCRIPTION: Initializes ROS publishers for detection and segmentation results using sensor_msgs/Image message type with a queue size of 5.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/ros-quickstart.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom sensor_msgs.msg import Image\n\ndet_image_pub = rospy.Publisher(\"/ultralytics/detection/image\", Image, queue_size=5)\nseg_image_pub = rospy.Publisher(\"/ultralytics/segmentation/image\", Image, queue_size=5)\n```\n\n----------------------------------------\n\nTITLE: Integrating Weights & Biases Logging with YOLO11 Ray Tune Python\nDESCRIPTION: This snippet outlines how to connect YOLO11 Ray Tune tuning runs with Weights & Biases for real-time experiment tracking. It requires the wandb, ultralytics, and ray packages. The wandb.init requires a project and (optionally) entity. Outputs are the Ray Tune result grid, with W&B logging enabled.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/ray-tune.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport wandb\\n\\nfrom ultralytics import YOLO\\n\\nwandb.init(project=\"YOLO-Tuning\", entity=\"your-entity\")\\n\\n# Load YOLO model\\nmodel = YOLO(\"yolo11n.pt\")\\n\\n# Tune hyperparameters\\nresult_grid = model.tune(data=\"coco8.yaml\", use_ray=True)\n```\n\n----------------------------------------\n\nTITLE: Configuring YOLO11 Benchmark Parameters\nDESCRIPTION: Common configuration arguments used for customizing YOLO11 benchmark execution. These parameters control model selection, data input, processing settings, and hardware utilization for optimal performance testing.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/benchmark.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmodel: yolo11n.pt\ndata: coco8.yaml\nimgsz: [640, 640]\nhalf: true\nint8: false\ndevice: cuda:0\nverbose: true\n```\n\n----------------------------------------\n\nTITLE: Running Workout Monitoring via CLI\nDESCRIPTION: Command line interface commands for running workout monitoring examples with various options including source video specification and keypoint selection.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/workouts-monitoring.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Run a workout example\nyolo solutions workout show=True\n\n# Pass a source video\nyolo solutions workout source=\"path/to/video.mp4\"\n\n# Use keypoints for pushups\nyolo solutions workout kpts=\"[6, 8, 10]\"\n```\n\n----------------------------------------\n\nTITLE: Running YOLOv5 Core Operations\nDESCRIPTION: Commands for training, validating, running inference, and exporting YOLOv5 models, demonstrating basic model operations\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/environments/google_cloud_quickstart_tutorial.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Train a YOLOv5 model on your dataset (e.g., yolov5s)\npython train.py --data coco128.yaml --weights yolov5s.pt --img 640\n\n# Validate the trained model to check Precision, Recall, and mAP\npython val.py --weights yolov5s.pt --data coco128.yaml\n\n# Run inference using the trained model on images or videos\npython detect.py --weights yolov5s.pt --source path/to/your/images_or_videos\n\n# Export the trained model to various formats like ONNX, CoreML, TFLite for deployment\npython export.py --weights yolov5s.pt --include onnx coreml tflite\n```\n\n----------------------------------------\n\nTITLE: Object Isolation with Transparent Background\nDESCRIPTION: Creates a 4-channel image with transparent background for PNG output using NumPy depth stacking.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/isolating-segmentation-objects.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Isolate object with transparent background (when saved as PNG)\nisolated = np.dstack([img, b_mask])\n```\n\n----------------------------------------\n\nTITLE: Exporting and Using YOLO11 Model via CLI\nDESCRIPTION: Command-line instructions for exporting a YOLO11 model to TF SavedModel format and running inference\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tf-savedmodel.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Export a YOLO11n PyTorch model to TF SavedModel format\nyolo export model=yolo11n.pt format=saved_model # creates '/yolo11n_saved_model'\n\n# Run inference with the exported model\nyolo predict model='./yolo11n_saved_model' source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: SQL query for filtering images with specific objects\nDESCRIPTION: Example SQL query to filter images that contain at least one person and one dog in the dataset.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/dashboard.md#2025-04-22_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nWHERE labels LIKE '%person%' AND labels LIKE '%dog%'\n```\n\n----------------------------------------\n\nTITLE: Training Example - Python Implementation (FAQ)\nDESCRIPTION: Example code from FAQ section showing how to train a YOLO model on Fashion-MNIST dataset using Python.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/fashion-mnist.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained model\nmodel = YOLO(\"yolo11n-cls.pt\")\n\n# Train the model on Fashion-MNIST\nresults = model.train(data=\"fashion-mnist\", epochs=100, imgsz=28)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for YOLOv8 ONNX Runtime Demo (Bash)\nDESCRIPTION: Installs the necessary Python packages for the YOLOv8 ONNX Runtime segmentation demo using pip. It includes 'ultralytics' for model export and utilities, 'onnxruntime-gpu' for GPU inference (with a commented option for CPU-only 'onnxruntime'), 'numpy' for numerical operations, and 'opencv-python' for image processing. Requires Python and pip to be installed.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-Segmentation-ONNXRuntime-Python/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics\npip install onnxruntime-gpu # For GPU support\n# pip install onnxruntime # For CPU-only support\npip install numpy opencv-python\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 to TF SavedModel using CLI\nDESCRIPTION: This snippet shows how to export a YOLO11 model to TensorFlow SavedModel format and run inference using the command-line interface.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tf-savedmodel.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Export the YOLO11 model to TF SavedModel format\nyolo export model=yolo11n.pt format=saved_model # creates '/yolo11n_saved_model'\n\n# Run inference with the exported model\nyolo predict model='./yolo11n_saved_model' source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Implementing Layer Freezing in YOLOv5 Training Script\nDESCRIPTION: This Python code snippet demonstrates how YOLOv5 implements layer freezing in its training script. It defines layers to freeze based on module index and disables gradient calculation for frozen layers.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/transfer_learning_with_frozen_layers.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Freeze specified layers\nfreeze = [f\"model.{x}.\" for x in range(freeze)]  # Define layers to freeze based on module index\nfor k, v in model.named_parameters():\n    v.requires_grad = True  # Ensure all parameters are initially trainable\n    if any(x in k for x in freeze):\n        print(f\"Freezing layer: {k}\")\n        v.requires_grad = False  # Disable gradient calculation for frozen layers\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for YOLOv8 with OpenVINO\nDESCRIPTION: This CMake file configures the build environment for a YOLOv8 example application using OpenVINO and OpenCV. It specifies C++14 as the standard, includes the necessary directories for OpenCV and OpenVINO, and links the required libraries. The output executable named 'detect' is defined with its source files. Dependencies include OpenCV and the OpenVINO runtime, which must be installed and available in specified paths.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-OpenVINO-CPP-Inference/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.12)\nproject(yolov8_openvino_example)\n\nset(CMAKE_CXX_STANDARD 14)\n\nfind_package(OpenCV REQUIRED)\n\ninclude_directories(\n\t${OpenCV_INCLUDE_DIRS}\n\t/path/to/intel/openvino/runtime/include\n)\n\nadd_executable(detect \n\tmain.cc\n\tinference.cc\n)\n\ntarget_link_libraries(detect\n\t${OpenCV_LIBS}\n\t/path/to/intel/openvino/runtime/lib/intel64/libopenvino.so\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Caching Options for YOLO11 Training in Python\nDESCRIPTION: Sets caching options for YOLO11 training to improve data loading efficiency. Options include caching in RAM, on disk, or disabling caching.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/model-training-tips.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncache=True  # Cache in RAM\ncache='disk'  # Cache on disk\ncache=False  # Disable caching\n```\n\n----------------------------------------\n\nTITLE: Image Compression Utility\nDESCRIPTION: Function to compress image files while preserving aspect ratio and quality.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/simple-utilities.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\n\nfrom ultralytics.data.utils import compress_one_image\n\nfor f in Path(\"path/to/dataset\").rglob(\"*.jpg\"):\n    compress_one_image(f)\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package\nDESCRIPTION: Command to install the Ultralytics Python package using pip.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/streamlit-live-inference.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on Fashion-MNIST - Python Implementation\nDESCRIPTION: Python code for training a YOLO model on the Fashion-MNIST dataset using the Ultralytics framework. Demonstrates loading a pretrained model and configuring training parameters.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/fashion-mnist.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-cls.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"fashion-mnist\", epochs=100, imgsz=28)\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n Model on SKU-110K Dataset via CLI\nDESCRIPTION: Command-line interface (CLI) command for training a YOLO11n model on the SKU-110K dataset for 100 epochs with an image size of 640.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/sku-110k.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo detect train data=SKU-110K.yaml model=yolo11n.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on Wheat Dataset using CLI\nDESCRIPTION: Command-line interface command for training a YOLO11n model on the Global Wheat Head Dataset\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/globalwheat2020.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo detect train data=GlobalWheat2020.yaml model=yolo11n.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Pose Estimation Label Format Examples\nDESCRIPTION: Examples showing the format for 2D and 3D pose estimation labels, including class index, bounding box coordinates, and keypoint information\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/index.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n<class-index> <x> <y> <width> <height> <px1> <py1> <px2> <py2> ... <pxn> <pyn>\n```\n\nLANGUAGE: text\nCODE:\n```\n<class-index> <x> <y> <width> <height> <px1> <py1> <p1-visibility> <px2> <py2> <p2-visibility> <pxn> <pyn> <pn-visibility>\n```\n\n----------------------------------------\n\nTITLE: Hyperparameter Tuning for YOLO11 using Ray Tune in Python\nDESCRIPTION: This code snippet demonstrates how to implement hyperparameter tuning for YOLO11 models using the Ray Tune library. It uses the run_ray_tune utility from Ultralytics to find the best hyperparameters for the model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/jupyterlab.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.utils.tuner import run_ray_tune\n\nbest_results = run_ray_tune(model, data=\"path/to/data.yaml\")\n```\n\n----------------------------------------\n\nTITLE: Training YOLOE Prompt-Free Model\nDESCRIPTION: Demonstrates training a prompt-free YOLOE model with specialized prompt embedding and layer freezing.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yoloe.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLOE\n\ndata = dict(\n    train=dict(\n        yolo_data=[\"Objects365.yaml\"],\n        grounding_data=[\n            dict(\n                img_path=\"../datasets/flickr/full_images/\",\n                json_file=\"../datasets/flickr/annotations/final_flickr_separateGT_train_segm.json\",\n            ),\n            dict(\n                img_path=\"../datasets/mixed_grounding/gqa/images\",\n                json_file=\"../datasets/mixed_grounding/annotations/final_mixed_train_no_coco_segm.json\",\n            ),\n        ],\n    ),\n    val=dict(yolo_data=[\"lvis.yaml\"]),\n)\n\nmodel = YOLOE(\"yoloe-11l-seg.pt\")\n\nhead_index = len(model.model.model) - 1\nfreeze = [str(f) for f in range(0, head_index)]\nfor name, child in model.model.model[-1].named_children():\n    if \"cv3\" not in name:\n        freeze.append(f\"{head_index}.{name}\")\n\nfreeze.extend([\n    f\"{head_index}.cv3.0.0\",\n    f\"{head_index}.cv3.0.1\",\n    f\"{head_index}.cv3.1.0\",\n    f\"{head_index}.cv3.1.1\",\n    f\"{head_index}.cv3.2.0\",\n    f\"{head_index}.cv3.2.1\",\n])\n\nmodel.train(\n    data=data,\n    batch=128,\n    epochs=1,\n    close_mosaic=1,\n    optimizer=\"AdamW\",\n    lr0=2e-3,\n    warmup_bias_lr=0.0,\n    weight_decay=0.025,\n    momentum=0.9,\n    workers=4,\n    trainer=YOLOEPEFreeTrainer,\n    device=\"0,1,2,3,4,5,6,7\",\n    freeze=freeze,\n    single_cls=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Validating YOLOE Model with Visual Prompt\nDESCRIPTION: Code to validate a YOLOE model using visual prompts. The model extracts visual embeddings for each category from the provided dataset with the load_vp parameter enabled.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yoloe.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLOE\n\n# Create a YOLOE model\nmodel = YOLOE(\"yoloe-11l-seg.pt\")  # or select yoloe-m/l-seg.pt for different sizes\n\n# Conduct model validation on the COCO128-seg example dataset\nmetrics = model.val(data=\"coco128-seg.yaml\", load_vp=True)\n```\n\n----------------------------------------\n\nTITLE: Importing and Using TritonRemoteModel Class\nDESCRIPTION: The file contains a TritonRemoteModel class implementation that enables interaction with remote Triton Inference Server models. It provides functionality for model inference and handling remote model connections.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/triton.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics.utils.triton import TritonRemoteModel\n```\n\n----------------------------------------\n\nTITLE: Run YOLOv5 Docker Container with GPU\nDESCRIPTION: Commands to run YOLOv5 Docker container with GPU support, including options for specific GPU selection.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/environments/docker_image_quickstart_tutorial.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Run with access to all available GPUs\nsudo docker run -it --ipc=host --gpus all $t\n\n# Run with access to specific GPUs (e.g., GPUs 2 and 3)\nsudo docker run -it --ipc=host --gpus '\"device=2,3\"' $t\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO Model to TFLite Format using Python\nDESCRIPTION: This code demonstrates how to load a YOLO model and export it to TFLite format using the Ultralytics library in Python. It creates a file named 'yolo11n_float32.tflite'.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tflite.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to TFLite format\nmodel.export(format=\"tflite\")  # creates 'yolo11n_float32.tflite'\n```\n\n----------------------------------------\n\nTITLE: Creating Virtual Environment for YOLO11 in Azure\nDESCRIPTION: Sets up a conda virtual environment with Python 3.12 and installs required dependencies for YOLO11.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/azureml-quickstart.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create --name yolo11env -y python=3.12\nconda activate yolo11env\nconda install pip -y\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating Conda Environment for YOLOv5 in AzureML\nDESCRIPTION: This snippet demonstrates how to create a new Conda environment named 'yolov5env' with Python 3.10, activate it, and ensure pip is installed. This is the first step in setting up the environment for YOLOv5 on AzureML.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/environments/azureml_quickstart_tutorial.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create --name yolov5env -y python=3.10 # Create a new Conda environment\nconda activate yolov5env                     # Activate the environment\nconda install pip -y                         # Ensure pip is installed\n```\n\n----------------------------------------\n\nTITLE: Running YOLOv8 Classification Inference\nDESCRIPTION: Command to run YOLOv8 classification inference with specific image dimensions and profiling in Rust.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-ONNXRuntime-Rust/README.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncargo run --release -- --model ../assets/weights/yolov8m-cls-dyn.onnx --source ../assets/images/dog.jpg --height 224 --width 224 --plot --profile\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Explorer GUI using pip\nDESCRIPTION: Command to install Ultralytics Explorer GUI with pip. This installs the explorer package along with its dependencies.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/dashboard.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics[explorer]\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Build for YOLOv8 C++ with CUDA and OpenCV\nDESCRIPTION: Sets up a CMake project for YOLOv8 inference in C++. Configures CUDA 11+ and OpenCV dependencies, defines source files, and creates an executable target with necessary linkage.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-CPP-Inference/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.5)\n\nproject(Yolov8CPPInference VERSION 0.1)\n\nset(CMAKE_INCLUDE_CURRENT_DIR ON)\n\n# CUDA\nset(CUDA_TOOLKIT_ROOT_DIR \"/usr/local/cuda\")\nfind_package(CUDA 11 REQUIRED)\n\nset(CMAKE_CUDA_STANDARD 11)\nset(CMAKE_CUDA_STANDARD_REQUIRED ON)\n# !CUDA\n\n# OpenCV\nfind_package(OpenCV REQUIRED)\ninclude_directories(${OpenCV_INCLUDE_DIRS})\n# !OpenCV\n\nset(PROJECT_SOURCES\n    main.cpp\n\n    inference.h\n    inference.cpp\n)\n\nadd_executable(Yolov8CPPInference ${PROJECT_SOURCES})\ntarget_link_libraries(Yolov8CPPInference ${OpenCV_LIBS})\n```\n\n----------------------------------------\n\nTITLE: Logging Plots to WandB\nDESCRIPTION: Function for logging various training plots and visualizations to WandB dashboard.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/callbacks/wb.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nultralytics.utils.callbacks.wb._log_plots\n```\n\n----------------------------------------\n\nTITLE: Benchmarking YOLOv8 Models Using Ultralytics Command-Line Interface (CLI, Bash)\nDESCRIPTION: This Bash snippet illustrates the use of the Ultralytics CLI to benchmark speed and accuracy of the YOLOv8n model on the COCO8 dataset, covering all export formats. It requires the Ultralytics tool (installed with 'pip install ultralytics'), and the ability to invoke the 'yolo' command from a shell. Parameters include the model weights file and dataset YAML; results are displayed in the terminal or processed as desired. The approach is ideal for automation or scripting in headless or server environments.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/openvino.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Benchmark YOLOv8n speed and accuracy on the COCO8 dataset for all export formats\nyolo benchmark model=yolov8n.pt data=coco8.yaml\n```\n\n----------------------------------------\n\nTITLE: YOLO11 Object Detection via Command Line\nDESCRIPTION: This snippet shows how to run YOLO11 object detection from the command line interface. It demonstrates the basic syntax for specifying the model and input source for detection tasks.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/index.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Run YOLO detection from the command line\nyolo detect model=yolo11n.pt source=\"image.jpg\" # Adjust model and source as needed\n```\n\n----------------------------------------\n\nTITLE: Speed Estimation CLI Commands\nDESCRIPTION: Command line interface commands for running speed estimation examples with optional parameters for video source and region coordinates\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/speed-estimation.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Run a speed example\nyolo solutions speed show=True\n\n# Pass a source video\nyolo solutions speed source=\"path/to/video.mp4\"\n\n# Pass region coordinates\nyolo solutions speed region=\"[(20, 400), (1080, 400), (1080, 360), (20, 360)]\"\n```\n\n----------------------------------------\n\nTITLE: Performing Batched Inference with Multiple Image Sources\nDESCRIPTION: Shows how to perform batch inference with both PIL and OpenCV image sources. Demonstrates downloading images, processing them, and handling the inference results in different formats including printing, saving, and converting to tensors or pandas dataframes.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/pytorch_hub_model_loading.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport torch\nfrom PIL import Image\n\n# Model\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\")\n\n# Images\nfor f in \"zidane.jpg\", \"bus.jpg\":\n    torch.hub.download_url_to_file(\"https://ultralytics.com/images/\" + f, f)  # download 2 images\nim1 = Image.open(\"zidane.jpg\")  # PIL image\nim2 = cv2.imread(\"bus.jpg\")[..., ::-1]  # OpenCV image (BGR to RGB)\n\n# Inference\nresults = model([im1, im2], size=640)  # batch of images\n\n# Results\nresults.print()\nresults.save()  # or .show()\n\nresults.xyxy[0]  # im1 predictions (tensor)\nresults.pandas().xyxy[0]  # im1 predictions (pandas)\n#      xmin    ymin    xmax   ymax  confidence  class    name\n# 0  749.50   43.50  1148.0  704.5    0.874023      0  person\n# 1  433.50  433.50   517.5  714.5    0.687988     27     tie\n# 2  114.75  195.75  1095.0  708.0    0.624512      0  person\n# 3  986.00  304.00  1028.0  420.0    0.286865     27     tie\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on CIFAR-10 Dataset via CLI\nDESCRIPTION: This snippet shows how to train a pre-trained YOLO model on the CIFAR-10 dataset for 100 epochs with an image size of 32x32 using the command-line interface.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/cifar10.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo classify train data=cifar10 model=yolo11n-cls.pt epochs=100 imgsz=32\n```\n\n----------------------------------------\n\nTITLE: Installing ROS Dependencies\nDESCRIPTION: Commands for installing required ROS dependencies including ros_numpy package for handling image conversions between ROS and numpy arrays.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/ros-quickstart.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install ros_numpy\n```\n\n----------------------------------------\n\nTITLE: Predicting with Official YOLO Detection Model in Bash\nDESCRIPTION: Command to run inference on an image using an official pre-trained YOLO11n detection model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/cli.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nyolo detect predict model=yolo11n.pt source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Running RT-DETR Inference Script\nDESCRIPTION: Executes the main inference script with configurable parameters for model path, input image, confidence threshold, and IoU threshold for NMS.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/RTDETR-ONNXRuntime-Python/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython main.py --model rtdetr-l.onnx --img image.jpg --conf-thres 0.5 --iou-thres 0.5\n```\n\n----------------------------------------\n\nTITLE: Analyzing YOLO11 Experiment Results with DVC API and Pandas\nDESCRIPTION: Python code to extract experiment data from DVC, process it with Pandas, and display training results in a tabular format for analysis.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/dvc.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport dvc.api\nimport pandas as pd\n\n# Define the columns of interest\ncolumns = [\"Experiment\", \"epochs\", \"imgsz\", \"model\", \"metrics.mAP50-95(B)\"]\n\n# Retrieve experiment data\ndf = pd.DataFrame(dvc.api.exp_show(), columns=columns)\n\n# Clean the data\ndf.dropna(inplace=True)\ndf.reset_index(drop=True, inplace=True)\n\n# Display the DataFrame\nprint(df)\n```\n\n----------------------------------------\n\nTITLE: Dog-Pose Dataset YAML Configuration\nDESCRIPTION: YAML configuration file for the Dog-Pose dataset. Specifies dataset paths, keypointss and other configuration details used by YOLO11 models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/dog-pose.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/dog-pose.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Persisting Tracks Loop with OpenCV and YOLO\nDESCRIPTION: This Python script demonstrates how to use OpenCV and Ultralytics YOLO to perform object tracking on video frames, persisting tracks between frames. It processes each video frame, visualizes the tracking results, and displays them.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/trackers/README.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Open the video file\nvideo_path = \"path/to/video.mp4\"\ncap = cv2.VideoCapture(video_path)\n\n# Loop through the video frames\nwhile cap.isOpened():\n    # Read a frame from the video\n    success, frame = cap.read()\n\n    if success:\n        # Run YOLO11 tracking on the frame, persisting tracks between frames\n        results = model.track(frame, persist=True)\n\n        # Visualize the results on the frame\n        annotated_frame = results[0].plot()\n\n        # Display the annotated frame\n        cv2.imshow(\"YOLO11 Tracking\", annotated_frame)\n\n        # Break the loop if 'q' is pressed\n        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n            break\n    else:\n        # Break the loop if the end of the video is reached\n        break\n\n# Release the video capture object and close the display window\ncap.release()\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Running Inference Loop with a YOLOv8 TensorRT Engine - Python\nDESCRIPTION: This snippet shows how to run a batch prediction loop on a YOLOv8 TensorRT engine using Ultralytics' Python API. It loads the optimized engine, prepares an input image with OpenCV, and repeatedly performs batch predictions for benchmarking or warm-up. Required dependencies are 'ultralytics' and 'opencv-python', and CUDA is used for device acceleration. Key parameters are the batch input (using the same image), verbosity, and device selection. It expects an optimized .engine file and outputs result objects for each iteration.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tensorrt.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.engine\")\nimg = cv2.imread(\"path/to/image.jpg\")\n\nfor _ in range(100):\n    result = model.predict(\n        [img] * 8,  # batch=8 of the same image\n        verbose=False,\n        device=\"cuda\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Updating YAML Configuration for YOLO Dataset in Python\nDESCRIPTION: This function updates the YAML configuration file for the YOLO dataset. It sets the path to the dataset directory, defines train/val/test subdirectories, and specifies class names and their corresponding IDs.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/ibm-watsonx.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Contents of new config.yaml file\ndef update_yaml_file(file_path):\n    data = {\n        \"path\": \"work_dir/trash_ICRA19/dataset\",\n        \"train\": \"train/images\",\n        \"val\": \"train/images\",\n        \"test\": \"test/images\",\n        \"names\": {0: \"plastic\", 1: \"bio\", 2: \"rov\"},\n    }\n\n    # Ensures the \"names\" list appears after the sub/directories\n    names_data = data.pop(\"names\")\n    with open(file_path, \"w\") as yaml_file:\n        yaml.dump(data, yaml_file)\n        yaml_file.write(\"\\n\")\n        yaml.dump({\"names\": names_data}, yaml_file)\n\n\nif __name__ == \"__main__\":\n    file_path = f\"{work_dir}/trash_ICRA19/config.yaml\"  # .yaml file path\n    update_yaml_file(file_path)\n    print(f\"{file_path} updated successfully.\")\n```\n\n----------------------------------------\n\nTITLE: ClearML Debug Sample Logging Function\nDESCRIPTION: Function to log debug samples to ClearML during model training\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/callbacks/clearml.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nultralytics.utils.callbacks.clearml._log_debug_samples\n```\n\n----------------------------------------\n\nTITLE: Non-Thread-Safe Example: Single Model Instance in Python\nDESCRIPTION: This example demonstrates an unsafe pattern of sharing a single YOLO model instance across multiple threads, which can lead to race conditions and unpredictable results.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/yolo-thread-safe-inference.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Unsafe: Sharing a single model instance across threads\nfrom threading import Thread\n\nfrom ultralytics import YOLO\n\n# Instantiate the model outside the thread\nshared_model = YOLO(\"yolo11n.pt\")\n\n\ndef predict(image_path):\n    \"\"\"Predicts objects in an image using a preloaded YOLO model, take path string to image as argument.\"\"\"\n    results = shared_model.predict(image_path)\n    # Process results\n\n\n# Starting threads that share the same model instance\nThread(target=predict, args=(\"image1.jpg\",)).start()\nThread(target=predict, args=(\"image2.jpg\",)).start()\n```\n\n----------------------------------------\n\nTITLE: Loading YOLOv5 Model Silently without Verbose Output\nDESCRIPTION: Demonstrates how to load YOLOv5 models silently by suppressing the default verbose output, which is useful for cleaner application logs or embedding in other applications.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/pytorch_hub_model_loading.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", _verbose=False)  # load silently\n```\n\n----------------------------------------\n\nTITLE: FAQ Example: Inference with Tiger-Pose Trained Model via CLI\nDESCRIPTION: Command line interface example from the FAQ section showing how to run inference with a Tiger-Pose trained model on a video source.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/tiger-pose.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# Run inference using a tiger-pose trained model\nyolo task=pose mode=predict source=\"https://youtu.be/MIBAT6BGE6U\" show=True model=\"path/to/best.pt\"\n```\n\n----------------------------------------\n\nTITLE: Force Reloading YOLOv5 Model from PyTorch Hub\nDESCRIPTION: Shows how to force a fresh download of the latest YOLOv5 model from PyTorch Hub, which can help resolve issues with cached models or ensure you're using the most recent version.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/pytorch_hub_model_loading.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", force_reload=True)  # force reload\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on Caltech-101 Dataset using Python\nDESCRIPTION: This code demonstrates how to train a YOLO image classification model on the Caltech-101 dataset using Python. It loads a pretrained YOLO model (yolo11n-cls.pt) and trains it for 100 epochs with an image size of 416 pixels.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/caltech101.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-cls.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"caltech101\", epochs=100, imgsz=416)\n```\n\n----------------------------------------\n\nTITLE: Loading Custom YOLOv5 Models Using PyTorch Hub\nDESCRIPTION: The code example demonstrates loading a custom YOLOv5 model, trained on a VOC dataset, using PyTorch Hub. It indicates the flexibility of specifying a local path to the model's weights for inference purposes.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/pytorch_hub_model_loading.md#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", path=\"path/to/best.pt\")  # local model\nmodel = torch.hub.load(\"path/to/yolov5\", \"custom\", path=\"path/to/best.pt\", source=\"local\")  # local repo\n```\n\n----------------------------------------\n\nTITLE: Validating a YOLO Detection Model in Bash\nDESCRIPTION: Command to validate a pre-trained detection model on the COCO8 dataset with a batch size of 1 and image size of 640 pixels.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/cli.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Training YOLO-World Model with CLI\nDESCRIPTION: Demonstrates how to train a YOLO-World model using the command-line interface.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolo-world.md#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nyolo train model=yolov8s-worldv2.yaml data=coco8.yaml epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Validating YOLOE Model with Text Prompt\nDESCRIPTION: Code to validate a YOLOE model using text prompts. The model is initialized with pre-trained weights and validated on the COCO128-seg dataset.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yoloe.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLOE\n\n# Create a YOLOE model\nmodel = YOLOE(\"yoloe-11l-seg.pt\")  # or select yoloe-m/l-seg.pt for different sizes\n\n# Conduct model validation on the COCO128-seg example dataset\nmetrics = model.val(data=\"coco128-seg.yaml\")\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLOv8 Model to TorchScript Format in Bash\nDESCRIPTION: This command uses the Ultralytics CLI to export a YOLOv8 model to TorchScript format with specified parameters such as model name and input image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-LibTorch-CPP-Inference/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=yolov8s.pt imgsz=640 format=torchscript\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package for YOLO Model Export to TFLite\nDESCRIPTION: This snippet shows how to install the Ultralytics package using pip, which is required for exporting YOLO models to TFLite format.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tflite.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics\n```\n\n----------------------------------------\n\nTITLE: Training and Using YOLO11 Model for Package Segmentation via CLI\nDESCRIPTION: Command-line instructions for training a YOLO11 segmentation model on the Package Segmentation dataset, resuming training, validating the model, and performing inference using the trained model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/package-seg.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Load a pretrained segmentation model and start training\nyolo segment train data=package-seg.yaml model=yolo11n-seg.pt epochs=100 imgsz=640\n\n# Resume training from the last checkpoint\nyolo segment train data=package-seg.yaml model=path/to/last.pt resume=True\n\n# Validate the trained model\nyolo segment val data=package-seg.yaml model=path/to/best.pt\n\n# Perform inference using the trained model\nyolo segment predict model=path/to/best.pt source=path/to/image.jpg\n```\n\n----------------------------------------\n\nTITLE: Training YOLOv5 with DistributedDataParallel on Multiple GPUs\nDESCRIPTION: Command to train YOLOv5 using DistributedDataParallel mode on multiple GPUs. This is the recommended method for multi-GPU training, offering better performance scaling.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/multi_gpu_training.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m torch.distributed.run --nproc_per_node 2 train.py --batch 64 --data coco.yaml --weights yolov5s.pt --device 0,1\n```\n\n----------------------------------------\n\nTITLE: Cloning the Ultralytics Repository for Development\nDESCRIPTION: Series of commands to clone the Ultralytics repository and set up a development environment. Installs the package in editable mode for making contributions or experimenting with the code.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/quickstart.md#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\n# Clone the ultralytics repository\ngit clone https://github.com/ultralytics/ultralytics\n\n# Navigate to the cloned directory\ncd ultralytics\n\n# Install the package in editable mode for development\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Configuring xView Dataset Structure with YAML\nDESCRIPTION: YAML configuration file for the xView dataset that defines paths, classes, and dataset structure for use with Ultralytics models. The file is maintained in the Ultralytics GitHub repository.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/xview.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/xView.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Training a YOLOv8 Model via CLI\nDESCRIPTION: Illustrates how to initiate YOLOv8 model training from the command line using the `yolo train` command, as shown in the FAQ section. Specifies the model (`yolov8n.pt`), data configuration (`coco8.yaml`), number of epochs, and image size as arguments. Requires the `ultralytics` CLI tool and necessary files.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov8.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nyolo train model=yolov8n.pt data=coco8.yaml epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Viewing Ultralytics Settings via CLI\nDESCRIPTION: Command-line interface command to check current Ultralytics settings. This provides a simple way to inspect configuration without writing Python code.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/help/privacy.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyolo settings\n```\n\n----------------------------------------\n\nTITLE: Logging in to Weights & Biases account in Python\nDESCRIPTION: This code demonstrates how to log in to a Weights & Biases account using the API key in a Python script.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/weights-biases.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport wandb\n\nwandb.login(key=\"YOUR_API_KEY\")\n```\n\n----------------------------------------\n\nTITLE: Predicting with YOLO MNN Model - CLI\nDESCRIPTION: This snippet shows how to use the YOLO CLI tool to make predictions with an exported MNN model. Dependencies include the Ultralytics YOLO CLI. The model path and image source URL are required parameters. Outputs predictions using fp32 or fp16 if supported by the device.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/mnn.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nyolo predict model='yolo11n.mnn' source='https://ultralytics.com/images/bus.jpg'             # predict with `fp32`\nyolo predict model='yolo11n.mnn' source='https://ultralytics.com/images/bus.jpg' --half=True # predict with `fp16` if device support\n```\n\n----------------------------------------\n\nTITLE: Using ObjectCounter Solution in Python\nDESCRIPTION: This snippet demonstrates how to use the ObjectCounter solution from Ultralytics to count objects in an image or video stream. It shows how to initialize the counter with specific parameters and retrieve the counting results.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/solutions/index.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncounter = solutions.ObjectCounter(\n    show=True,  # display the output\n    region=region_points,  # pass region points\n    model=\"yolo11n.pt\",  # model=\"yolo11n-obb.pt\" for object counting with OBB model.\n    # classes=[0, 2],           # count specific classes i.e. person and car with COCO pretrained model.\n    # tracker=\"botsort.yaml\"    # Choose trackers i.e \"bytetrack.yaml\"\n)\nresults = counter.count(im0)\nprint(results.in_counts)  # display in_counts\nprint(results.out_counts)  # display out_counts\n```\n\n----------------------------------------\n\nTITLE: CLI Commands for Object Counting with YOLO11\nDESCRIPTION: Command line interface examples for running object counting with various options including source video and region coordinates specification.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/object-counting.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Run a counting example\nyolo solutions count show=True\n\n# Pass a source video\nyolo solutions count source=\"path/to/video.mp4\"\n\n# Pass region coordinates\nyolo solutions count region=\"[(20, 400), (1080, 400), (1080, 360), (20, 360)]\"\n```\n\n----------------------------------------\n\nTITLE: Folder Structure Example for CIFAR-10 Dataset\nDESCRIPTION: Demonstrates the required folder structure for organizing a classification dataset, using CIFAR-10 as an example. Shows the hierarchy of train, test, and optional validation directories, each containing subdirectories for individual classes.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/index.md#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ncifar-10-/\n|\n|-- train/\n|   |-- airplane/\n|   |   |-- 10008_airplane.png\n|   |   |-- 10009_airplane.png\n|   |   |-- ...\n|   |\n|   |-- automobile/\n|   |   |-- 1000_automobile.png\n|   |   |-- 1001_automobile.png\n|   |   |-- ...\n|   |\n|   |-- bird/\n|   |   |-- 10014_bird.png\n|   |   |-- 10015_bird.png\n|   |   |-- ...\n|   |\n|   |-- ...\n|\n|-- test/\n|   |-- airplane/\n|   |   |-- 10_airplane.png\n|   |   |-- 11_airplane.png\n|   |   |-- ...\n|   |\n|   |-- automobile/\n|   |   |-- 100_automobile.png\n|   |   |-- 101_automobile.png\n|   |   |-- ...\n|   |\n|   |-- bird/\n|   |   |-- 1000_bird.png\n|   |   |-- 1001_bird.png\n|   |   |-- ...\n|   |\n|   |-- ...\n|\n|-- val/ (optional)\n|   |-- airplane/\n|   |   |-- 105_airplane.png\n|   |   |-- 106_airplane.png\n|   |   |-- ...\n|   |\n|   |-- automobile/\n|   |   |-- 102_automobile.png\n|   |   |-- 103_automobile.png\n|   |   |-- ...\n|   |\n|   |-- bird/\n|   |   |-- 1045_bird.png\n|   |   |-- 1046_bird.png\n|   |   |-- ...\n|   |\n|   |-- ...\n```\n\n----------------------------------------\n\nTITLE: Alternative YOLO11 Training Example in Python for FAQ Section\nDESCRIPTION: A simplified Python code example showing how to train a YOLO11n segmentation model on the Crack Segmentation dataset, included in the FAQ section as a quick reference for users.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/crack-seg.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained model (recommended)\nmodel = YOLO(\"yolo11n-seg.pt\")\n\n# Train the model\nresults = model.train(data=\"crack-seg.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Predicting with Custom-Trained YOLO Model in Bash\nDESCRIPTION: Command to run inference on an image using a custom-trained YOLO model with the best weights file.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/cli.md#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nyolo detect predict model=path/to/best.pt source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model to PaddlePaddle Format via CLI\nDESCRIPTION: Command-line instructions for exporting a YOLO11 PyTorch model to PaddlePaddle format and running inference with the exported model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/paddlepaddle.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Export a YOLO11n PyTorch model to PaddlePaddle format\nyolo export model=yolo11n.pt format=paddle # creates '/yolo11n_paddle_model'\n\n# Run inference with the exported model\nyolo predict model='./yolo11n_paddle_model' source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Example JSON Response for YOLO OBB Model\nDESCRIPTION: This JSON object represents a typical response from the Ultralytics API for a YOLO OBB model. It includes detection results with class, confidence, and oriented bounding box coordinates, as well as image shape and processing speed information.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/hub/inference-api.md#2025-04-22_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"images\": [\n    {\n      \"results\": [\n        {\n          \"class\": 0,\n          \"name\": \"person\",\n          \"confidence\": 0.92,\n          \"box\": {\n            \"x1\": 374.85565,\n            \"x2\": 392.31824,\n            \"x3\": 412.81805,\n            \"x4\": 395.35547,\n            \"y1\": 264.40704,\n            \"y2\": 267.45728,\n            \"y3\": 150.0966,\n            \"y4\": 147.04634\n          }\n        }\n      ],\n      \"shape\": [\n        750,\n        600\n      ],\n      \"speed\": {\n        \"inference\": 200.8,\n        \"postprocess\": 0.8,\n        \"preprocess\": 2.8\n      }\n    }\n  ],\n  \"metadata\": ...\n}\n```\n\n----------------------------------------\n\nTITLE: Modifying YOLO11 Inference Function in Python\nDESCRIPTION: Python code to modify the output_fn function in inference.py for formatting model outputs as a JSON string, extracting attributes like boxes, masks, and keypoints.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/amazon-sagemaker.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\n\ndef output_fn(prediction_output):\n    \"\"\"Formats model outputs as JSON string, extracting attributes like boxes, masks, keypoints.\"\"\"\n    print(\"Executing output_fn from inference.py ...\")\n    infer = {}\n    for result in prediction_output:\n        if result.boxes is not None:\n            infer[\"boxes\"] = result.boxes.numpy().data.tolist()\n        if result.masks is not None:\n            infer[\"masks\"] = result.masks.numpy().data.tolist()\n        if result.keypoints is not None:\n            infer[\"keypoints\"] = result.keypoints.numpy().data.tolist()\n        if result.obb is not None:\n            infer[\"obb\"] = result.obb.numpy().data.tolist()\n        if result.probs is not None:\n            infer[\"probs\"] = result.probs.numpy().data.tolist()\n    return json.dumps(infer)\n```\n\n----------------------------------------\n\nTITLE: Deploying Streamlit Object Detection Application\nDESCRIPTION: Command to run and deploy the Streamlit application in a web browser after integrating Ultralytics YOLO11.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/streamlit-live-inference.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nstreamlit run path/to/file.py\n```\n\n----------------------------------------\n\nTITLE: Python Implementation for YOLO Pose Training\nDESCRIPTION: Example showing how to load and train a YOLO pose estimation model using Python\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/index.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-pose.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"coco8-pose.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Example JSON Response for YOLO Pose Model\nDESCRIPTION: This JSON object represents a typical response from the Ultralytics API for a YOLO pose model. It includes detection results with class, confidence, bounding box coordinates, and keypoint coordinates with visibility scores, as well as image shape and processing speed information.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/hub/inference-api.md#2025-04-22_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"images\": [\n    {\n      \"results\": [\n        {\n          \"class\": 0,\n          \"name\": \"person\",\n          \"confidence\": 0.92,\n          \"box\": {\n            \"x1\": 118,\n            \"x2\": 416,\n            \"y1\": 112,\n            \"y2\": 660\n          },\n          \"keypoints\": {\n            \"visible\": [\n              0.9909399747848511,\n              0.8162999749183655,\n              0.9872099757194519,\n              ...\n            ],\n            \"x\": [\n              316.3871765136719,\n              315.9374694824219,\n              304.878173828125,\n              ...\n            ],\n            \"y\": [\n              156.4207763671875,\n              148.05775451660156,\n              144.93240356445312,\n              ...\n            ]\n          }\n        }\n      ],\n      \"shape\": [\n        750,\n        600\n      ],\n      \"speed\": {\n        \"inference\": 200.8,\n        \"postprocess\": 0.8,\n        \"preprocess\": 2.8\n      }\n    }\n  ],\n  \"metadata\": ...\n}\n```\n\n----------------------------------------\n\nTITLE: Verifying Image File Validity - Python\nDESCRIPTION: The verify_image utility checks if an image file is readable, decodable, and optionally if it meets format expectations, ensuring dataset images are correct before training or analysis. Accepts image file paths and outputs a validity indicator or error message. May depend on OpenCV, PIL, or similar libraries; corrupted or unsupported files will fail the check.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/data/utils.md#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n## ::: ultralytics.data.utils.verify_image\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package for YOLO11 (CLI)\nDESCRIPTION: Command to install the required Ultralytics package for YOLO11 using pip.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/coreml.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO Model to ONNX Format\nDESCRIPTION: Script to load a YOLO model and export it to ONNX format with metadata collection using callbacks\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/triton-inference-server.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load an official model\n\n# Retrieve metadata during export. Metadata needs to be added to config.pbtxt. See next section.\nmetadata = []\n\n\ndef export_cb(exporter):\n    metadata.append(exporter.metadata)\n\n\nmodel.add_callback(\"on_export_end\", export_cb)\n\n# Export the model\nonnx_file = model.export(format=\"onnx\", dynamic=True)\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model with COCO-Pose via CLI\nDESCRIPTION: Command line interface example for training a YOLO11n-pose model on COCO-Pose dataset.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/coco.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo pose train data=coco-pose.yaml model=yolo11n-pose.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Setting Up ClearML Task for YOLO11 Training\nDESCRIPTION: Python script demonstrating how to set up a ClearML task for YOLO11 model training, including task initialization, model selection, and training configuration.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/clearml.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom clearml import Task\n\nfrom ultralytics import YOLO\n\n# Step 1: Creating a ClearML Task\ntask = Task.init(project_name=\"my_project\", task_name=\"my_yolov8_task\")\n\n# Step 2: Selecting the YOLO11 Model\nmodel_variant = \"yolo11n\"\ntask.set_parameter(\"model_variant\", model_variant)\n\n# Step 3: Loading the YOLO11 Model\nmodel = YOLO(f\"{model_variant}.pt\")\n\n# Step 4: Setting Up Training Arguments\nargs = dict(data=\"coco8.yaml\", epochs=16)\ntask.connect(args)\n\n# Step 5: Initiating Model Training\nresults = model.train(**args)\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 model with Weights & Biases logging\nDESCRIPTION: This snippet shows how to train a YOLO11 model using Ultralytics with Weights & Biases logging enabled, specifying the model, dataset, and project details.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/weights-biases.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolo11n.pt\")\nmodel.train(data=\"coco8.yaml\", epochs=5, project=\"ultralytics\", name=\"yolo11n\")\n```\n\n----------------------------------------\n\nTITLE: Exporting Models to OpenVINO or ONNX Format\nDESCRIPTION: Uses the 'yolo export' command to convert Ultralytics YOLOv8 models to OpenVINO IR or ONNX formats. This step is necessary for compatibility with the C++ inference examples.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-OpenVINO-CPP-Inference/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=yolov8s.pt imgsz=640 format=openvino\n\n# Export to ONNX format\nyolo export model=yolov8s.pt imgsz=640 format=onnx\n```\n\n----------------------------------------\n\nTITLE: Enabling Mixed Precision Training for YOLO11 in Python\nDESCRIPTION: Activates Automatic Mixed Precision (AMP) training for YOLO11 to optimize the training process and improve efficiency.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/model-training-tips.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\namp=True\n```\n\n----------------------------------------\n\nTITLE: Converting Prompt-Free Model Back to Segmentation\nDESCRIPTION: Shows how to convert a prompt-free model back to a segmentation model and configure its classes.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yoloe.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom copy import deepcopy\n\nfrom ultralytics import YOLOE\n\nmodel = YOLOE(\"yoloe-11l-seg.pt\")\nmodel.eval()\n\npf_model = YOLOE(\"yoloe-11l-seg-pf.pt\")\nnames = [\"object\"]\ntpe = model.get_text_pe(names)\nmodel.set_classes(names, tpe)\nmodel.model.model[-1].fuse(model.model.pe)\n\nmodel.model.model[-1].cv3[0][2] = deepcopy(pf_model.model.model[-1].cv3[0][2]).requires_grad_(True)\nmodel.model.model[-1].cv3[1][2] = deepcopy(pf_model.model.model[-1].cv3[1][2]).requires_grad_(True)\nmodel.model.model[-1].cv3[2][2] = deepcopy(pf_model.model.model[-1].cv3[2][2]).requires_grad_(True)\ndel model.model.pe\nmodel.save(\"yoloe-11l-seg-pf.pt\")\n```\n\n----------------------------------------\n\nTITLE: Running YOLOv8 LibTorch Inference in Bash\nDESCRIPTION: This command executes the compiled binary to perform inference using the exported YOLOv8 model on a sample image or video.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-LibTorch-CPP-Inference/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n./yolov8_libtorch_inference\n```\n\n----------------------------------------\n\nTITLE: Benchmarking YOLOv8 Formats using CLI\nDESCRIPTION: This Bash command demonstrates how to use the `yolo` command-line tool to benchmark the speed and accuracy of a YOLOv8 PyTorch model across multiple export formats. The `benchmark` command requires the path to the input `model` (`.pt` file) and the dataset configuration file (`data=coco8.yaml`). It automatically tests formats like PyTorch, ONNX, and OpenVINO if available.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/openvino.md#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n# Benchmark YOLOv8n speed and accuracy on the COCO8 dataset for all export formats\nyolo benchmark model=yolov8n.pt data=coco8.yaml\n```\n\n----------------------------------------\n\nTITLE: Project Structure for YOLO-Interactive-Tracking-UI\nDESCRIPTION: Displays the project directory structure showing the main Python script and README file organization.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLO-Interactive-Tracking-UI/README.md#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nYOLO-Interactive-Tracking-UI/\n├── interactive_tracker.py   # Main Python tracking UI script\n└── README.md                # You're here!\n```\n\n----------------------------------------\n\nTITLE: Initializing Weights & Biases in Python\nDESCRIPTION: Python code snippet for initializing Weights & Biases environment in your workspace using your API key for authentication.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/weights-biases.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport wandb\n\n# Initialize your Weights & Biases environment\nwandb.login(key=\"YOUR_API_KEY\")\n```\n\n----------------------------------------\n\nTITLE: Configuring YOLOv8 Detection Parameters in C++\nDESCRIPTION: This C++ code snippet demonstrates how to configure parameters for YOLOv8 object detection using ONNX Runtime. It sets up detection thresholds, model path, input image size, CUDA enablement, and model type.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-ONNXRuntime-CPP/README.md#2025-04-22_snippet_3\n\nLANGUAGE: c++\nCODE:\n```\n//change your param as you like\n//Pay attention to your device and the onnx model type(fp32 or fp16)\nDL_INIT_PARAM params;\nparams.rectConfidenceThreshold = 0.1;\nparams.iouThreshold = 0.5;\nparams.modelPath = \"yolov8n.onnx\";\nparams.imgSize = { 640, 640 };\nparams.cudaEnable = true;\nparams.modelType = YOLO_DETECT_V8;\nyoloDetector->CreateSession(params);\nDetector(yoloDetector);\n```\n\n----------------------------------------\n\nTITLE: Running SAM Inference from Command Line\nDESCRIPTION: This Bash command demonstrates how to run SAM model predictions using the command line interface (CLI) with the 'yolo' tool, specifying the model and source image.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/sam.md#2025-04-22_snippet_9\n\nLANGUAGE: Bash\nCODE:\n```\nyolo predict model=sam_b.pt source=path/to/image.jpg\n```\n\n----------------------------------------\n\nTITLE: Natural Language Querying with Ultralytics Explorer\nDESCRIPTION: Demonstrates how to use the Ask AI feature for natural language querying of the dataset and plotting the results.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/api.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import Explorer\nfrom ultralytics.data.explorer import plot_query_result\n\n# create an Explorer object\nexp = Explorer(data=\"coco128.yaml\", model=\"yolo11n.pt\")\nexp.create_embeddings_table()\n\ndf = exp.ask_ai(\"show me 100 images with exactly one person and 2 dogs. There can be other objects too\")\nprint(df.head())\n\n# plot the results\nplt = plot_query_result(df)\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: FAQ Python Example for Training YOLO11n-pose\nDESCRIPTION: Python code example from the FAQ section demonstrating how to train a YOLO11n-pose model on the COCO8-Pose dataset, highlighting the simplicity of the Ultralytics API.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/coco8-pose.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-pose.pt\")\n\n# Train the model\nresults = model.train(data=\"coco8-pose.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Running YOLOv5 Tasks on AWS\nDESCRIPTION: Commands for training, validating, running inference, and exporting YOLOv5 models on an AWS instance. These examples demonstrate core YOLOv5 functionality using sample dataset configurations.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/environments/aws_quickstart_tutorial.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Train a YOLOv5 model on a custom dataset (e.g., coco128.yaml)\npython train.py --data coco128.yaml --weights yolov5s.pt --img 640\n\n# Validate the performance (Precision, Recall, mAP) of a trained model (e.g., yolov5s.pt)\npython val.py --weights yolov5s.pt --data coco128.yaml --img 640\n\n# Run inference (object detection) on images or videos using a trained model\npython detect.py --weights yolov5s.pt --source path/to/your/images_or_videos/ --img 640\n\n# Export the trained model to various formats like ONNX, CoreML, TFLite for deployment\n# See https://docs.ultralytics.com/modes/export/ for more details\npython export.py --weights yolov5s.pt --include onnx coreml tflite --img 640\n```\n\n----------------------------------------\n\nTITLE: YOLO Pose Models Performance Table\nDESCRIPTION: Markdown table showing performance metrics for YOLO pose estimation models including mAP scores, inference speeds, parameters and FLOPs.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/README.md#2025-04-22_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n| Model                                                                                          | size<br><sup>(pixels) | mAP<sup>pose<br>50-95 | mAP<sup>pose<br>50 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n| ---------------------------------------------------------------------------------------------- | --------------------- | --------------------- | ------------------ | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\n| [YOLO11n-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-pose.pt) | 640                   | 50.0                  | 81.0               | 52.4 ± 0.5                     | 1.7 ± 0.0                           | 2.9                | 7.6               |\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package with pip\nDESCRIPTION: Commands for installing the core ultralytics package using pip, which is required for running the interactive tracker.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLO-Interactive-Tracking-UI/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics\n```\n\n----------------------------------------\n\nTITLE: Enabling Ray Tune for YOLO11 Hyperparameter Optimization\nDESCRIPTION: Shows how to enable Ray Tune integration with YOLO11 models for hyperparameter optimization using the model.tune() method. This allows access to advanced search algorithms like Bayesian Optimization and Hyperband with parallel execution capabilities.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/hyperparameter-tuning.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel.tune(use_ray=True)\n```\n\n----------------------------------------\n\nTITLE: Modifying Ultralytics Settings via CLI\nDESCRIPTION: Command-line interface commands to disable analytics/crash reporting or reset settings to default values. This provides a convenient way to manage settings without writing Python code.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/help/privacy.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Disable analytics and crash reporting\nyolo settings sync=False\n\n# Reset settings to default values\nyolo settings reset\n```\n\n----------------------------------------\n\nTITLE: Type-Hinted Python Function with Docstring\nDESCRIPTION: Shows how to combine Python type hints with Google-style docstrings for enhanced code documentation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/help/contributing.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef example_function(arg1: int, arg2: int = 4) -> bool:\n    \"\"\"\n    Example function demonstrating Google-style docstrings.\n\n    Args:\n        arg1: The first argument.\n        arg2: The second argument, with a default value of 4.\n\n    Returns:\n        True if successful, False otherwise.\n\n    Examples:\n        >>> result = example_function(1, 2)  # returns False\n    \"\"\"\n    if arg1 == arg2:\n        return True\n    return False\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 on Apple Silicon (MPS) with Python\nDESCRIPTION: Provides a Python code example for training a YOLO11 model on Apple Silicon hardware using the Metal Performance Shaders (MPS) backend. A pretrained model is loaded, and the `model.train()` method is invoked with the `device` parameter set to `\"mps\"`. Training is performed on the COCO8 dataset for 100 epochs at an image size of 640.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/train.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model with MPS\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640, device=\"mps\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Tracking Parameters using CLI with Ultralytics YOLO\nDESCRIPTION: This Bash snippet provides commands to set up tracking parameters such as confidence threshold and IOU directly from the command line interface. It enhances control over tracking processes executed through the CLI, demonstrating real-time processing capabilities.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/track.md#2025-04-22_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\nyolo track model=yolo11n.pt source=\"https://youtu.be/LNwODJXcvt4\" conf=0.3, iou=0.5 show\n```\n\n----------------------------------------\n\nTITLE: Generating Parameter Table using Jinja2 Macro for Ultralytics Documentation\nDESCRIPTION: This macro creates a markdown-formatted table of parameters for Ultralytics documentation. It includes default parameters with their types, default values, and descriptions. The macro can also handle custom parameter sets passed as arguments.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/macros/track-args.md#2025-04-22_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n{% macro param_table(params=None) %}\n\n| Argument | Type | Default | Description |\n| -------- | ---- | ------- | ----------- |\n\n{%- set default_params = {\n    \"source\": [\"str\", \"None\", \"Specifies the source directory for images or videos. Supports file paths, URLs, and video streams.\"],\n    \"persist\": [\"bool\", \"False\", \"Enables persistent tracking of objects between frames, maintaining IDs across video sequences.\"],\n    \"stream\": [\"bool\", \"False\", \"Treats the input source as a continuous video stream for real-time processing.\"],\n    \"tracker\": [\"str\", \"'botsort.yaml'\", \"Specifies the tracking algorithm to use, e.g., `bytetrack.yaml` or `botsort.yaml`.\"],\n    \"conf\": [\"float\", \"0.3\", \"Sets the confidence threshold for detections; lower values allow more objects to be tracked but may include false positives.\"],\n    \"iou\": [\"float\", \"0.5\", \"Sets the [Intersection over Union](https://www.ultralytics.com/glossary/intersection-over-union-iou) (IoU) threshold for filtering overlapping detections.\"],\n    \"classes\": [\"list\", \"None\", \"Filters results by class index. For example, `classes=[0, 2, 3]` only tracks the specified classes.\"],\n    \"verbose\": [\"bool\", \"True\", \"Controls the display of tracking results, providing a visual output of tracked objects.\"],\n    \"device\": [\"str\", \"None\", \"Specifies the device for inference (e.g., `cpu`, `cuda:0` or `0`). Allows users to select between CPU, a specific GPU, or other compute devices for model execution.\"],\n    \"show\": [\"bool\", \"False\", \"If `True`, displays the annotated images or videos in a window for immediate visual feedback.\"],\n    \"line_width\": [\"None or int\", \"None\", \"Specifies the line width of bounding boxes. If `None`, the line width is automatically adjusted based on the image size.\"]\n} %}\n\n{%- if not params %}\n{%- for param, details in default_params.items() %}\n| `{{ param }}` | `{{ details[0] }}` | `{{ details[1] }}` | {{ details[2] }} |\n{%- endfor %}\n{%- else %}\n{%- for param in params %}\n{%- if param in default_params %}\n| `{{ param }}` | `{{ default_params[param][0] }}` | `{{ default_params[param][1] }}` | {{ default_params[param][2] }} |\n{%- endif %}\n{%- endfor %}\n{%- endif %}\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Training YOLO OBB Model - CLI Implementation\nDESCRIPTION: Command-line interface example for training a YOLO11n-OBB model on the DOTAv1 dataset\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/obb/index.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nyolo obb train data=DOTAv1.yaml model=yolo11n-obb.pt epochs=100 imgsz=1024\n```\n\n----------------------------------------\n\nTITLE: Example JSON Response for YOLO Segmentation Model\nDESCRIPTION: This JSON object represents a typical response from the Ultralytics API for a YOLO segmentation model. It includes detection results with class, confidence, bounding box coordinates, and segmentation mask coordinates, as well as image shape and processing speed information.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/hub/inference-api.md#2025-04-22_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"images\": [\n    {\n      \"results\": [\n        {\n          \"class\": 0,\n          \"name\": \"person\",\n          \"confidence\": 0.92,\n          \"box\": {\n            \"x1\": 118,\n            \"x2\": 416,\n            \"y1\": 112,\n            \"y2\": 660\n          },\n          \"segments\": {\n            \"x\": [\n              266.015625,\n              266.015625,\n              258.984375,\n              ...\n            ],\n            \"y\": [\n              110.15625,\n              113.67188262939453,\n              120.70311737060547,\n              ...\n            ]\n          }\n        }\n      ],\n      \"shape\": [\n        750,\n        600\n      ],\n      \"speed\": {\n        \"inference\": 200.8,\n        \"postprocess\": 0.8,\n        \"preprocess\": 2.8\n      }\n    }\n  ],\n  \"metadata\": ...\n}\n```\n\n----------------------------------------\n\nTITLE: Installing JupyterLab and Ultralytics for YOLO11 in Python\nDESCRIPTION: This code snippet demonstrates how to install JupyterLab and the Ultralytics package using pip. These are the necessary prerequisites for working with YOLO11 models in JupyterLab.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/jupyterlab.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install jupyterlab ultralytics\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model to ONNX Format via CLI\nDESCRIPTION: This code snippet shows how to export a YOLO11 PyTorch model to ONNX format and run inference using the command-line interface.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/onnx.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Export a YOLO11n PyTorch model to ONNX format\nyolo export model=yolo11n.pt format=onnx # creates 'yolo11n.onnx'\n\n# Run inference with the exported model\nyolo predict model=yolo11n.onnx source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Running Custom YOLO Model for Detection Task in Rust\nDESCRIPTION: Example command to run a custom YOLOv8 detection model with specific parameters including task type, version, and class count using the Rust implementation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLO-Series-ONNXRuntime-Rust/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncargo run -r -- --task detect --ver v8 --nc 6 --model path/to/your/model.onnx\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n-obb Model on DOTA8 Dataset using Python\nDESCRIPTION: Python code snippet demonstrating how to load a pretrained YOLO11n-obb model and train it on the DOTA8 dataset for 100 epochs with an image size of 640.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/obb/dota8.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-obb.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"dota8.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Handling Probability Tensors with Probs Class — Python\nDESCRIPTION: Probs is tailored for storing and manipulating class probability tensors from classification or detection heads. Provides methods for indexing, thresholding, and class probability extraction. Uses PyTorch as backend and expects tensors of probability values as input.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/engine/results.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n## ::: ultralytics.engine.results.Probs\n```\n\n----------------------------------------\n\nTITLE: Object Isolation with Black Background\nDESCRIPTION: Creates a 3-channel mask and isolates the object with black background pixels using bitwise operations.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/isolating-segmentation-objects.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Create 3-channel mask\nmask3ch = cv2.cvtColor(b_mask, cv2.COLOR_GRAY2BGR)\n\n# Isolate object with binary mask\nisolated = cv2.bitwise_and(mask3ch, img)\n```\n\n----------------------------------------\n\nTITLE: YOLO Inference on Single Video Stream\nDESCRIPTION: Shows how to perform inference on a single live video stream using RTSP, RTMP, TCP, or IP protocols with batch size 1.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/predict.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Single stream with batch-size 1 inference\nsource = \"rtsp://example.com/media.mp4\"  # RTSP, RTMP, TCP, or IP streaming address\n\n# Run inference on the source\nresults = model(source, stream=True)  # generator of Results objects\n```\n\n----------------------------------------\n\nTITLE: Converting Visual Prompt Model Back to Segmentation\nDESCRIPTION: Shows how to convert a visual prompt model back to a segmentation model after training.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yoloe.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom copy import deepcopy\n\nfrom ultralytics import YOLOE\n\nmodel = YOLOE(\"yoloe-11l-seg.yaml\")\nmodel.load(\"yoloe-11l-seg.pt\")\n\nvp_model = YOLOE(\"yoloe-11l-vp.pt\")\nmodel.model.model[-1].savpe = deepcopy(vp_model.model.model[-1].savpe)\nmodel.eval()\nmodel.save(\"yoloe-11l-seg.pt\")\n```\n\n----------------------------------------\n\nTITLE: Saving Isolated Object to File with OpenCV\nDESCRIPTION: Demonstrates how to save an isolated object crop to a file using OpenCV's imwrite function. The filename includes the original image name, detected class label, and object index for organization.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/isolating-segmentation-objects.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Save isolated object to file\n_ = cv2.imwrite(f\"{img_name}_{label}-{ci}.png\", iso_crop)\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Validation Results to JSON via CLI (Bash)\nDESCRIPTION: Illustrates saving YOLO11 model validation results as a JSON file directly from the CLI by enabling the 'save_json' flag. Useful for reproducibility and downstream analysis. Requires Ultralytics CLI and a valid YOLO11 model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/val.md#2025-04-22_snippet_10\n\nLANGUAGE: Bash\nCODE:\n```\nyolo val model=yolo11n.pt save_json=True\n\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Development Version from GitHub\nDESCRIPTION: Command to install the development version of Ultralytics directly from the GitHub repository. Requires Git to be installed on the system.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/quickstart.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/ultralytics/ultralytics.git\n```\n\n----------------------------------------\n\nTITLE: YOLOv5 Training Command Example\nDESCRIPTION: Example command for training YOLOv5s model on COCO128 dataset with specified parameters for image size, batch size, and epochs.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/train_custom_data.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt\n```\n\n----------------------------------------\n\nTITLE: Pull YOLOv5 Docker Image\nDESCRIPTION: Commands to pull the latest YOLOv5 Docker image from Docker Hub.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/environments/docker_image_quickstart_tutorial.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Define the image name with tag\nt=ultralytics/yolov5:latest\n\n# Pull the latest YOLOv5 image from Docker Hub\nsudo docker pull $t\n```\n\n----------------------------------------\n\nTITLE: TensorBoard Callback for Fit Epoch End in Python\nDESCRIPTION: This callback function is executed at the end of each fit epoch. It may log additional metrics or summaries specific to the fitting process.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/callbacks/tensorboard.md#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nultralytics.utils.callbacks.tensorboard.on_fit_epoch_end\n```\n\n----------------------------------------\n\nTITLE: Uploading YOLO11 Model Weights to Roboflow using Python\nDESCRIPTION: This Python script facilitates uploading trained YOLO11 model weights to Roboflow, leveraging the Roboflow API. Required dependencies include the 'roboflow' Python package, which can be installed via pip, and user credentials in the form of an API key. Key parameters include workspace and project identifiers, version number, and the path to the model weights directory. Inputs include the specified configuration details, whereas outputs encompass the deployment of model weights on Roboflow, potentially taking up to 30 minutes.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/roboflow.md#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport roboflow  # install with 'pip install roboflow'\n\n# Log in to Roboflow (requires API key)\nroboflow.login()\n\n# Initialize Roboflow client\nrf = roboflow.Roboflow()\n\n# Define your workspace and project details\nWORKSPACE_ID = \"your-workspace-id\"  # Replace with your actual Workspace ID\nPROJECT_ID = \"your-project-id\"  # Replace with your actual Project ID\nVERSION = 1  # Replace with your desired dataset version number\nMODEL_PATH = \"path/to/your/runs/detect/train/\"  # Replace with the path to your YOLO11 training results directory\n\n# Get project and version\nproject = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\ndataset = project.version(VERSION)\n\n# Upload model weights for deployment\n# Ensure model_path points to the directory containing 'best.pt'\nproject.version(dataset.version).deploy(\n    model_type=\"yolov8\", model_path=MODEL_PATH\n)  # Note: Use \"yolov8\" as model_type for YOLO11 compatibility in Roboflow deployment\n\nprint(f\"Model from {MODEL_PATH} uploaded to Roboflow project {PROJECT_ID}, version {VERSION}.\")\nprint(\"Deployment may take up to 30 minutes.\")\n```\n\n----------------------------------------\n\nTITLE: Exporting and Using YOLO11 Model with TensorRT via CLI\nDESCRIPTION: Command-line interface commands for exporting a YOLO11 PyTorch model to TensorRT format and running inference with the exported model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tensorrt.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Export a YOLO11n PyTorch model to TensorRT format\nyolo export model=yolo11n.pt format=engine # creates 'yolo11n.engine''\n\n# Run inference with the exported model\nyolo predict model=yolo11n.engine source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on ImageWoof Dataset Variants\nDESCRIPTION: Examples of training using different ImageWoof dataset variants (320px and 160px versions) for faster training and prototyping purposes.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/imagewoof.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-cls.pt\")  # load a pretrained model (recommended for training)\n\n# For medium-sized dataset\nmodel.train(data=\"imagewoof320\", epochs=100, imgsz=224)\n\n# For small-sized dataset\nmodel.train(data=\"imagewoof160\", epochs=100, imgsz=224)\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Load a pretrained model and train on the medium-sized dataset\nyolo classify train model=yolo11n-cls.pt data=imagewoof320 epochs=100 imgsz=224\n```\n\n----------------------------------------\n\nTITLE: Training YOLOv5 Model on COCO128 Dataset in AzureML\nDESCRIPTION: This code snippet shows how to start training a YOLOv5s model on the COCO128 dataset using pretrained weights. It specifies parameters such as image size, number of epochs, and batch size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/environments/azureml_quickstart_tutorial.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Start training using yolov5s pretrained weights on the COCO128 dataset\npython train.py --data coco128.yaml --weights yolov5s.pt --img 640 --epochs 10 --batch 16\n```\n\n----------------------------------------\n\nTITLE: Google-Style Function Documentation Example\nDESCRIPTION: Example of a function with Google-style docstrings showing proper documentation format including args, returns, and examples\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/CONTRIBUTING.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef example_function(arg1, arg2=4):\n    \"\"\"\n    Example function demonstrating Google-style docstrings.\n\n    Args:\n        arg1 (int): The first argument.\n        arg2 (int): The second argument, with a default value of 4.\n\n    Returns:\n        (bool): True if successful, False otherwise.\n\n    Examples:\n        >>> result = example_function(1, 2)  # returns False\n    \"\"\"\n    if arg1 == arg2:\n        return True\n    return False\n```\n\n----------------------------------------\n\nTITLE: Configuring Carparts Segmentation Dataset in YAML\nDESCRIPTION: YAML configuration file for the Carparts Segmentation dataset, specifying paths, class names, and other essential details for use with Ultralytics YOLO.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/carparts-seg.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/carparts-seg.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Launching Ultralytics Explorer GUI\nDESCRIPTION: Command to start the Ultralytics Explorer graphical user interface in the browser, allowing users to create embeddings, perform semantic search, and run SQL queries on their datasets.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/index.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyolo explorer\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model to IMX500 Format\nDESCRIPTION: Uses the Ultralytics YOLO Python API to export a YOLO11 model to IMX500 format with default Post-Training Quantization (PTQ).\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/sony-imx500.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolo11n.pt\")\nmodel.export(format=\"imx\")  # Exports with PTQ quantization by default\n```\n\n----------------------------------------\n\nTITLE: Building the Project with CMake\nDESCRIPTION: Creates a build directory, enters it, and compiles the project using CMake. Ensures that the project dependencies are correctly configured and compiled for running YOLOv8 inference.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-OpenVINO-CPP-Inference/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmkdir build\ncd build\ncmake ..\nmake\n```\n\n----------------------------------------\n\nTITLE: Running Inference with YOLOv5 Model in AzureML\nDESCRIPTION: This code snippet shows how to perform inference using a YOLOv5s model on sample images. It specifies the model weights, source directory for images, and image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/environments/azureml_quickstart_tutorial.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Run inference with yolov5s on sample images\npython detect.py --weights yolov5s.pt --source data/images --img 640\n```\n\n----------------------------------------\n\nTITLE: Running Inference and Validation on YOLOv5 Exported Models using Bash\nDESCRIPTION: These Bash commands demonstrate how to run inference and validation on YOLOv5 models exported in various formats using provided Python scripts (detect.py and val.py). Each command specifies a different format such as PyTorch, TorchScript, ONNX, etc., by adjusting the --weights parameter to match the model file.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/model_export.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython detect.py --weights yolov5s.pt             # PyTorch\npython detect.py --weights yolov5s.torchscript    # TorchScript\npython detect.py --weights yolov5s.onnx           # ONNX Runtime or OpenCV DNN with dnn=True\npython detect.py --weights yolov5s_openvino_model # OpenVINO\npython detect.py --weights yolov5s.engine         # TensorRT\npython detect.py --weights yolov5s.mlmodel        # CoreML (macOS only)\npython detect.py --weights yolov5s_saved_model    # TensorFlow SavedModel\npython detect.py --weights yolov5s.pb             # TensorFlow GraphDef\npython detect.py --weights yolov5s.tflite         # TensorFlow Lite\npython detect.py --weights yolov5s_edgetpu.tflite # TensorFlow Edge TPU\npython detect.py --weights yolov5s_paddle_model   # PaddlePaddle\n```\n\nLANGUAGE: bash\nCODE:\n```\npython val.py --weights yolov5s.pt             # PyTorch\npython val.py --weights yolov5s.torchscript    # TorchScript\npython val.py --weights yolov5s.onnx           # ONNX Runtime or OpenCV DNN with dnn=True\npython val.py --weights yolov5s_openvino_model # OpenVINO\npython val.py --weights yolov5s.engine         # TensorRT\npython val.py --weights yolov5s.mlmodel        # CoreML (macOS Only)\npython val.py --weights yolov5s_saved_model    # TensorFlow SavedModel\npython val.py --weights yolov5s.pb             # TensorFlow GraphDef\npython val.py --weights yolov5s.tflite         # TensorFlow Lite\npython val.py --weights yolov5s_edgetpu.tflite # TensorFlow Edge TPU\npython val.py --weights yolov5s_paddle_model   # PaddlePaddle\n```\n\n----------------------------------------\n\nTITLE: Defining COCO8-Multispectral Dataset Configuration in YAML\nDESCRIPTION: YAML configuration file for the COCO8-Multispectral dataset, specifying paths, class names, and other metadata used for training YOLO models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/coco8-multispectral.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/coco8-multispectral.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Training Configuration\nDESCRIPTION: Shows how to configure and execute model training across multiple GPUs with adjusted batch size and multi-scale training enabled.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/yolo-common-issues.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel.train(data=\"/path/to/your/data.yaml\", batch=32, multi_scale=True)\n```\n\n----------------------------------------\n\nTITLE: Installing and Enabling Libmamba Solver for Conda\nDESCRIPTION: Commands to install the conda-libmamba-solver package and configure Conda to use libmamba for faster package installation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/conda-quickstart.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nconda install conda-libmamba-solver\n```\n\nLANGUAGE: bash\nCODE:\n```\nconda config --set solver libmamba\n```\n\n----------------------------------------\n\nTITLE: Compressing a Single Image File - Python\nDESCRIPTION: The compress_one_image utility reduces the file size of a specified image while preserving content, using lossless or configurable lossy compression. Takes an image file path and optional compression parameters as input, outputs a compressed image file or statistics. Depends on image codecs and may have constraints on format and compression ratios.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/data/utils.md#2025-04-22_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\n## ::: ultralytics.data.utils.compress_one_image\n```\n\n----------------------------------------\n\nTITLE: Creating Directories and YAML Files for K-Fold Splits in Python\nDESCRIPTION: This code creates the necessary directories and dataset YAML files for each split. It gathers image files, creates split directories, and generates YAML files with dataset information.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/kfold-cross-validation.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\n\nsupported_extensions = [\".jpg\", \".jpeg\", \".png\"]\n\n# Initialize an empty list to store image file paths\nimages = []\n\n# Loop through supported extensions and gather image files\nfor ext in supported_extensions:\n    images.extend(sorted((dataset_path / \"images\").rglob(f\"*{ext}\")))\n\n# Create the necessary directories and dataset YAML files\nsave_path = Path(dataset_path / f\"{datetime.date.today().isoformat()}_{ksplit}-Fold_Cross-val\")\nsave_path.mkdir(parents=True, exist_ok=True)\nds_yamls = []\n\nfor split in folds_df.columns:\n    # Create directories\n    split_dir = save_path / split\n    split_dir.mkdir(parents=True, exist_ok=True)\n    (split_dir / \"train\" / \"images\").mkdir(parents=True, exist_ok=True)\n    (split_dir / \"train\" / \"labels\").mkdir(parents=True, exist_ok=True)\n    (split_dir / \"val\" / \"images\").mkdir(parents=True, exist_ok=True)\n    (split_dir / \"val\" / \"labels\").mkdir(parents=True, exist_ok=True)\n\n    # Create dataset YAML files\n    dataset_yaml = split_dir / f\"{split}_dataset.yaml\"\n    ds_yamls.append(dataset_yaml)\n\n    with open(dataset_yaml, \"w\") as ds_y:\n        yaml.safe_dump(\n            {\n                \"path\": split_dir.as_posix(),\n                \"train\": \"train\",\n                \"val\": \"val\",\n                \"names\": classes,\n            },\n            ds_y,\n        )\n```\n\n----------------------------------------\n\nTITLE: Running Inference with detect.py\nDESCRIPTION: Use the detect.py script for versatile inference on various sources including webcams, images, videos, directories, and streams. This method is ideal for command-line usage and system integration.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/quickstart_tutorial.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython detect.py --weights yolov5s.pt --source 0                              # webcam\npython detect.py --weights yolov5s.pt --source image.jpg                      # image\npython detect.py --weights yolov5s.pt --source video.mp4                      # video\npython detect.py --weights yolov5s.pt --source screen                         # screenshot\npython detect.py --weights yolov5s.pt --source path/                          # directory\npython detect.py --weights yolov5s.pt --source list.txt                       # list of images\npython detect.py --weights yolov5s.pt --source list.streams                   # list of streams\npython detect.py --weights yolov5s.pt --source 'path/*.jpg'                   # glob pattern\npython detect.py --weights yolov5s.pt --source 'https://youtu.be/LNwODJXcvt4' # YouTube video\npython detect.py --weights yolov5s.pt --source 'rtsp://example.com/media.mp4' # RTSP, RTMP, HTTP stream\n```\n\n----------------------------------------\n\nTITLE: Validating YOLO-World Model using CLI\nDESCRIPTION: This Bash command is used to validate a YOLO-World model on the COCO8 dataset, specifying the model type and desired image size. Validation provides performance metrics essential for assessing model accuracy and reliability.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolo-world.md#2025-04-22_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\nyolo val model=yolov8s-world.pt data=coco8.yaml imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Defining Objects365 Dataset Configuration in YAML\nDESCRIPTION: YAML configuration file for the Objects365 dataset, specifying paths, class names, and other relevant information for use with YOLO models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/objects365.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/Objects365.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Using Ultralytics YOLO CLI Basic Syntax\nDESCRIPTION: Example of the basic syntax for the Ultralytics YOLO command-line interface. Shows the structure for running tasks directly from the terminal.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/quickstart.md#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nyolo TASK MODE ARGS\n```\n\n----------------------------------------\n\nTITLE: Filtering Objects by Class in YOLO11 Predictions Using Shell Command\nDESCRIPTION: This command shows how to detect only specific classes when running YOLO11 using the Ultralytics library. The example filters to show only cars (class index 2) during object detection and segmentation tasks.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/yolo-common-issues.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nyolo task=detect mode=segment model=yolo11n-seg.pt source='path/to/car.mp4' show=True classes=2\n```\n\n----------------------------------------\n\nTITLE: Cloning YOLOv5 Repository and Installing Dependencies using Bash\nDESCRIPTION: These shell commands clone the official Ultralytics YOLOv5 repository from GitHub using `git clone`, navigate into the newly created directory using `cd`, and then install the necessary Python package dependencies listed in the `requirements.txt` file using `pip`. This setup is a prerequisite for training or running YOLOv5 locally.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/train_custom_data.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ultralytics/yolov5\ncd yolov5\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running baseline YOLOv5 test\nDESCRIPTION: Execute a standard test using YOLOv5x on COCO val2017 dataset with 640px image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/test_time_augmentation.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython val.py --weights yolov5x.pt --data coco.yaml --img 640 --half\n```\n\n----------------------------------------\n\nTITLE: Training a YOLOv10 Model from Scratch in Python\nDESCRIPTION: Illustrates how to train a YOLOv10 model using the Ultralytics Python library. It loads the YOLOv10n model architecture from a YAML configuration file (`yolov10n.yaml`) and starts training on a dataset specified by `coco8.yaml` for 100 epochs with an image size of 640x640.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov10.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom ultralytics import YOLO\n\n# Load YOLOv10n model from scratch\nmodel = YOLO(\"yolov10n.yaml\")\n\n# Train the model\nmodel.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n```\n```\n\n----------------------------------------\n\nTITLE: Installing YOLOv5 Dependencies\nDESCRIPTION: Commands for cloning the YOLOv5 repository and installing required dependencies for hyperparameter evolution.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/hyperparameter_evolution.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ultralytics/yolov5 # clone\ncd yolov5\npip install -r requirements.txt # install\n```\n\n----------------------------------------\n\nTITLE: Cloning YOLOv5 Repository in AzureML\nDESCRIPTION: This code snippet shows how to clone the official Ultralytics YOLOv5 repository from GitHub and navigate into the directory. This step is crucial for obtaining the YOLOv5 codebase.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/environments/azureml_quickstart_tutorial.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ultralytics/yolov5 # Clone the repository\ncd yolov5                                       # Navigate into the directory\n# Initialize submodules (if any, though YOLOv5 typically doesn't require this step)\n# git submodule update --init --recursive\n```\n\n----------------------------------------\n\nTITLE: Using Ultralytics Autosplit for Dataset Splitting in Python\nDESCRIPTION: This code demonstrates the use of Ultralytics' autosplit function for automatic dataset splitting into train, validation, and test sets. It specifies the path to images and the desired split ratios.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/kfold-cross-validation.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.data.utils import autosplit\n\n# Automatically split dataset into train/val/test\nautosplit(path=\"path/to/images\", weights=(0.8, 0.2, 0.0), annotated_only=True)\n```\n\n----------------------------------------\n\nTITLE: Filtering Dataset Using Similarity Count\nDESCRIPTION: Demonstrates how to filter images based on their similarity count using numpy arrays.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/api.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nsim_count = np.array(sim_idx[\"count\"])\nsim_idx[\"im_file\"][sim_count > 30]\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO Models to ONNX Format Using CLI\nDESCRIPTION: This command exports a YOLO model to the ONNX format using the CLI. The main dependency is the model to be exported. The command's primary parameter is the model path, alongside the desired export format. This operation facilitates the model's deployment across different environments.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/cli.md#2025-04-22_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=yolo11n.pt format=onnx\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11-pose Model to Different Formats\nDESCRIPTION: Illustrates the process of exporting a YOLO11-pose model to various formats like ONNX for deployment. Compatible with both official and custom-trained models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/pose.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-pose.pt\")  # load an official model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom trained model\n\n# Export the model\nmodel.export(format=\"onnx\")\n```\n\n----------------------------------------\n\nTITLE: BOTrack Class Definition for YOLOv8 Object Tracking\nDESCRIPTION: A class that handles individual object tracking with Kalman filter integration. It extends base tracking capabilities with additional features specific to the Bot SORT algorithm.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/trackers/bot_sort.md#2025-04-22_snippet_0\n\n\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 OBB Models\nDESCRIPTION: Code examples for exporting YOLO11 OBB models to different formats like ONNX using both Python API and CLI methods.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/obb.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-obb.pt\")  # load an official model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom trained model\n\n# Export the model\nmodel.export(format=\"onnx\")\n```\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=yolo11n-obb.pt format=onnx  # export official model\nyolo export model=path/to/best.pt format=onnx # export custom trained model\n```\n\n----------------------------------------\n\nTITLE: Cloning Ultralytics Repository for YOLOv8-OpenCV-ONNX Example\nDESCRIPTION: These commands clone the Ultralytics repository and navigate to the YOLOv8-OpenCV-ONNX example directory.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-OpenCV-ONNX-Python/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ultralytics/ultralytics.git\ncd ultralytics/examples/YOLOv8-OpenCV-ONNX-Python/\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model with CLI\nDESCRIPTION: Command line interface command to train a YOLO11n model on the African Wildlife Dataset\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/african-wildlife.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo detect train data=african-wildlife.yaml model=yolo11n.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Enabling Offline Logging Mode\nDESCRIPTION: Configures Comet ML to log experiments locally when internet access is limited\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/comet.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"COMET_MODE\"] = \"offline\"\n```\n\n----------------------------------------\n\nTITLE: Defining Markdown Table for Ultralytics Model Export Formats\nDESCRIPTION: This markdown snippet defines a table that outlines various export formats for Ultralytics YOLO models. It includes columns for the format name, format argument, model file name, metadata support, and available export arguments. The table provides a comprehensive overview of supported export options and their configurations.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/macros/export-table.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Format                                             | `format` Argument | Model                                           | Metadata | Arguments                                                                                                           |\n| -------------------------------------------------- | ----------------- | ----------------------------------------------- | -------- | ------------------------------------------------------------------------------------------------------------------- |\n| [PyTorch](https://pytorch.org/)                    | -                 | `{{ model_name or \"yolo11n\" }}.pt`              | ✅       | -                                                                                                                   |\n| [TorchScript](../integrations/torchscript.md)      | `torchscript`     | `{{ model_name or \"yolo11n\" }}.torchscript`     | ✅       | `imgsz`, `optimize`, `nms`{{ tip1 }}, `batch`, `device`                                                             |\n| [ONNX](../integrations/onnx.md)                    | `onnx`            | `{{ model_name or \"yolo11n\" }}.onnx`            | ✅       | `imgsz`, `half`, `dynamic`, `simplify`, `opset`, `nms`{{ tip1 }}, `batch`, `device`                                 |\n| [OpenVINO](../integrations/openvino.md)            | `openvino`        | `{{ model_name or \"yolo11n\" }}_openvino_model/` | ✅       | `imgsz`, `half`, `dynamic`, `int8`, `nms`{{ tip1 }}, `batch`, `data`, `fraction`, `device`                          |\n| [TensorRT](../integrations/tensorrt.md)            | `engine`          | `{{ model_name or \"yolo11n\" }}.engine`          | ✅       | `imgsz`, `half`, `dynamic`, `simplify`, `workspace`, `int8`, `nms`{{ tip1 }}, `batch`, `data`, `fraction`, `device` |\n| [CoreML](../integrations/coreml.md)                | `coreml`          | `{{ model_name or \"yolo11n\" }}.mlpackage`       | ✅       | `imgsz`, `half`, `int8`, `nms`{{ tip2 }}, `batch`, `device`                                                         |\n| [TF SavedModel](../integrations/tf-savedmodel.md)  | `saved_model`     | `{{ model_name or \"yolo11n\" }}_saved_model/`    | ✅       | `imgsz`, `keras`, `int8`, `nms`{{ tip1 }}, `batch`, `device`                                                        |\n| [TF GraphDef](../integrations/tf-graphdef.md)      | `pb`              | `{{ model_name or \"yolo11n\" }}.pb`              | ❌       | `imgsz`, `batch`, `device`                                                                                          |\n| [TF Lite](../integrations/tflite.md)               | `tflite`          | `{{ model_name or \"yolo11n\" }}.tflite`          | ✅       | `imgsz`, `half`, `int8`, `nms`{{ tip1 }}, `batch`, `data`, `fraction`, `device`                                     |\n| [TF Edge TPU](../integrations/edge-tpu.md)         | `edgetpu`         | `{{ model_name or \"yolo11n\" }}_edgetpu.tflite`  | ✅       | `imgsz`, `device`                                                                                                   |\n| [TF.js](../integrations/tfjs.md)                   | `tfjs`            | `{{ model_name or \"yolo11n\" }}_web_model/`      | ✅       | `imgsz`, `half`, `int8`, `nms`{{ tip1 }}, `batch`, `device`                                                         |\n| [PaddlePaddle](../integrations/paddlepaddle.md)    | `paddle`          | `{{ model_name or \"yolo11n\" }}_paddle_model/`   | ✅       | `imgsz`, `batch`, `device`                                                                                          |\n| [MNN](../integrations/mnn.md)                      | `mnn`             | `{{ model_name or \"yolo11n\" }}.mnn`             | ✅       | `imgsz`, `batch`, `int8`, `half`, `device`                                                                          |\n| [NCNN](../integrations/ncnn.md)                    | `ncnn`            | `{{ model_name or \"yolo11n\" }}_ncnn_model/`     | ✅       | `imgsz`, `half`, `batch`, `device`                                                                                  |\n| [IMX500](../integrations/sony-imx500.md){{ tip3 }} | `imx`             | `{{ model_name or \"yolov8n\" }}_imx_model/`      | ✅       | `imgsz`, `int8`, `data`, `fraction`, `device`                                                                       |\n| [RKNN](../integrations/rockchip-rknn.md)           | `rknn`            | `{{ model_name or \"yolo11n\" }}_rknn_model/`     | ✅       | `imgsz`, `batch`, `name`, `device`                                                                                  |\n```\n\n----------------------------------------\n\nTITLE: Plotting Similar Images Using Ultralytics Explorer\nDESCRIPTION: Shows how to plot similar images using the plot_similar method in Ultralytics Explorer, using either images or dataset indices.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/api.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import Explorer\n\n# create an Explorer object\nexp = Explorer(data=\"coco128.yaml\", model=\"yolo11n.pt\")\nexp.create_embeddings_table()\n\nplt = exp.plot_similar(img=\"https://ultralytics.com/images/bus.jpg\", limit=10)\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Checking File Read Speeds for I/O Benchmarking - Python\nDESCRIPTION: The check_file_speeds utility measures and reports the read speeds of given file paths to benchmark disk I/O performance. Used to diagnose training bottlenecks or hardware issues, it accepts a sequence of files and outputs timing statistics. Requires access to the file system and proper permissions; large files may affect system performance during benchmarking.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/data/utils.md#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n## ::: ultralytics.data.utils.check_file_speeds\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics and Checking Environment\nDESCRIPTION: Basic setup code to install the ultralytics package and verify the installation environment using pip and system checks.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/object_counting.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install ultralytics\nimport ultralytics\n\nultralytics.checks()\n```\n\n----------------------------------------\n\nTITLE: Configuring LVIS Dataset with YAML\nDESCRIPTION: YAML configuration file for the LVIS dataset that defines dataset paths, classes, and other relevant information for use with Ultralytics models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/lvis.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/lvis.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Auto-Annotating Segmentation Dataset\nDESCRIPTION: Python code for automatically annotating a segmentation dataset using pre-trained detection and segmentation models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/index.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.data.annotator import auto_annotate\n\nauto_annotate(data=\"path/to/images\", det_model=\"yolo11x.pt\", sam_model=\"sam_b.pt\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up IPython Kernel for YOLO11\nDESCRIPTION: Creates and configures an IPython kernel for running YOLO11 in Jupyter notebooks on AzureML.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/azureml-quickstart.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nconda create --name yolo11env -y python=3.12\nconda activate yolo11env\nconda install pip -y\nconda install ipykernel -y\npython -m ipykernel install --user --name yolo11env --display-name \"yolo11env\"\n```\n\n----------------------------------------\n\nTITLE: Loading YAML Configuration for Hand Keypoints Dataset\nDESCRIPTION: This YAML snippet defines the configuration for the hand keypoints dataset, including paths, number of classes, and keypoint information.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/hand-keypoints.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/hand-keypoints.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Defining benchmark Function for Comprehensive YOLO Model Evaluation in Python\nDESCRIPTION: This function implements a comprehensive benchmarking process for YOLO models. It likely combines functionality from RF100Benchmark and ProfileModels to assess model performance across various metrics and deployment scenarios.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/benchmarks.md#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef benchmark():\n```\n\n----------------------------------------\n\nTITLE: Defining Dataset Configuration using YAML for COCO128\nDESCRIPTION: YAML configuration file for the COCO128 dataset that defines paths, classes, and other relevant settings. This file is maintained in the Ultralytics repository and referenced when training models on the COCO128 dataset.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/coco128.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/coco128.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Model\nDESCRIPTION: Trains a YOLO11 detection model for 10 epochs using COCO8 dataset with specified learning rate.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/azureml-quickstart.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO Model to TensorRT Format\nDESCRIPTION: Optional code to export YOLO model directly to TensorRT format for enhanced performance\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/triton-inference-server.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to TensorRT format\nmodel.export(format=\"engine\")  # creates 'yolo11n.engine'\n```\n\n----------------------------------------\n\nTITLE: Custom Tracker Configuration with YOLO\nDESCRIPTION: Example showing how to use a custom tracker configuration file with YOLO tracking implementation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/track/index.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolo11n.pt\")\nresults = model.track(source=\"video.mp4\", tracker=\"custom_tracker.yaml\")\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Output for YOLOv5 Inference Results\nDESCRIPTION: This is a sample JSON output showing the structure of YOLOv5 inference results. Each entry contains bounding box coordinates, confidence level, class ID, and object name, illustrating the detailed information available from model predictions.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/pytorch_hub_model_loading.md#2025-04-22_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"xmin\": 749.5,\n        \"ymin\": 43.5,\n        \"xmax\": 1148.0,\n        \"ymax\": 704.5,\n        \"confidence\": 0.8740234375,\n        \"class\": 0,\n        \"name\": \"person\"\n    },\n    {\n        \"xmin\": 433.5,\n        \"ymin\": 433.5,\n        \"xmax\": 517.5,\n        \"ymax\": 714.5,\n        \"confidence\": 0.6879882812,\n        \"class\": 27,\n        \"name\": \"tie\"\n    },\n    {\n        \"xmin\": 115.25,\n        \"ymin\": 195.75,\n        \"xmax\": 1096.0,\n        \"ymax\": 708.0,\n        \"confidence\": 0.6254882812,\n        \"class\": 0,\n        \"name\": \"person\"\n    },\n    {\n        \"xmin\": 986.0,\n        \"ymin\": 304.0,\n        \"xmax\": 1028.0,\n        \"ymax\": 420.0,\n        \"confidence\": 0.2873535156,\n        \"class\": 27,\n        \"name\": \"tie\"\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Dataset Path Structure Example in Bash\nDESCRIPTION: Demonstrates the expected file path structure for images and their corresponding label files in a YOLOv5 dataset.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/train_custom_data.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n../datasets/coco128/images/im0.jpg # Path to the image file\n../datasets/coco128/labels/im0.txt # Path to the corresponding label file\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Container for YOLO11 Model Deployment\nDESCRIPTION: Dockerfile configuration for deploying YOLO11 models, setting up the necessary environment, dependencies, and execution parameters. Uses the ultralytics base image and configures workspace, model paths, and startup commands.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/model-deployment-practices.md#2025-04-22_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM ultralytics/ultralytics:latest\n\nWORKDIR /app\n\n# Copy your model and any additional files\nCOPY ./models/yolo11.pt /app/models/\nCOPY ./scripts /app/scripts/\n\n# Set up any environment variables\nENV MODEL_PATH=/app/models/yolo11.pt\n\n# Command to run when the container starts\nCMD [\"python\", \"/app/scripts/predict.py\"]\n```\n\n----------------------------------------\n\nTITLE: Extracting Cropped Detections from YOLOv5 Results\nDESCRIPTION: Shows how to extract and save cropped images of detected objects from YOLOv5 inference results, which is useful for creating datasets of detected objects or for further processing of individual detections.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/pytorch_hub_model_loading.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nresults = model(im)  # inference\ncrops = results.crop(save=True)  # cropped detections dictionary\n```\n\n----------------------------------------\n\nTITLE: Initializing Git and DVC Environment (Bash)\nDESCRIPTION: Sets up the initial project environment by initializing a Git repository, configuring local Git user details, initializing DVC for experiment tracking, and making the first commit. This prepares the project structure for version control and DVC integration.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/dvc.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ngit init -q\ngit config --local user.email \"you@example.com\"\ngit config --local user.name \"Your Name\"\ndvc init -q\ngit commit -m \"DVC init\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Comet ML API Key\nDESCRIPTION: Sets up the Comet ML API key as an environment variable for authentication\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/comet.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport COMET_API_KEY=YOUR_API_KEY\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package in Developer Mode\nDESCRIPTION: Command to install the Ultralytics package in editable mode with development dependencies using pip.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -e '.[dev]'\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Explorer Dependencies\nDESCRIPTION: Installation of required packages and system checks for Ultralytics Explorer functionality\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/explorer.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n%pip install ultralytics[explorer] openai\nyolo checks\n```\n\n----------------------------------------\n\nTITLE: Making API Requests with cURL\nDESCRIPTION: cURL command example demonstrating how to make requests to the Ultralytics HUB Inference API from the command line with image upload and parameters.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/hub/inference-api.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \"https://predict.ultralytics.com\" \\\n  -H \"x-api-key: API_KEY\" \\\n  -F \"model=https://hub.ultralytics.com/models/MODEL_ID\" \\\n  -F \"file=@/path/to/image.jpg\" \\\n  -F \"imgsz=640\" \\\n  -F \"conf=0.25\" \\\n  -F \"iou=0.45\"\n```\n\n----------------------------------------\n\nTITLE: Training and Predicting with YOLOv10 using CLI\nDESCRIPTION: Provides command-line examples for building and training a YOLOv10n model from scratch using its YAML configuration (`yolov10n.yaml`) on the `coco8.yaml` dataset for 100 epochs. It also shows how to run prediction using the model configuration file as the model source, which would typically use uninitialized weights.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov10.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n```bash\n# Build a YOLOv10n model from scratch and train it on the COCO8 example dataset for 100 epochs\nyolo train model=yolov10n.yaml data=coco8.yaml epochs=100 imgsz=640\n\n# Build a YOLOv10n model from scratch and run inference on the 'bus.jpg' image\nyolo predict model=yolov10n.yaml source=path/to/bus.jpg\n```\n```\n\n----------------------------------------\n\nTITLE: Additional Sweep Parameters with Comet YOLOv5 Shell\nDESCRIPTION: Runs a Comet hyperparameter optimization sweep with additional save-period and bounding box interval parameters. Enhances the sweep configuration by specifying further arguments.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/comet_logging_integration.md#2025-04-22_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\npython utils/loggers/comet/hpo.py \\\n  --comet_optimizer_config \"utils/loggers/comet/optimizer_config.json\" \\\n  --save-period 1 \\\n  --bbox_interval 1\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics CLIP with pip - Shell\nDESCRIPTION: Installs the CLIP package from Ultralytics' GitHub via pip for enhanced text prompt functionality in FastSAM. Assumes pip and internet access. Satisfies FastSAM's requirement for CLIP-based text features.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/fast-sam.md#2025-04-22_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\npip install git+https://github.com/ultralytics/CLIP.git\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n Model on Objects365 Dataset via CLI\nDESCRIPTION: Command-line interface (CLI) command for training a YOLO11n model on the Objects365 dataset for 100 epochs with an image size of 640.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/objects365.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo detect train data=Objects365.yaml model=yolo11n.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on MNIST Dataset - CLI Implementation\nDESCRIPTION: Command-line interface example for training a YOLO model on the MNIST dataset using pretrained weights.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/mnist.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo classify train data=mnist model=yolo11n-cls.pt epochs=100 imgsz=28\n```\n\n----------------------------------------\n\nTITLE: Compiling YOLO Custom Implementation Library\nDESCRIPTION: Compiles the custom YOLO implementation library for DeepStream\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/deepstream-nvidia-jetson.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nmake -C nvdsinfer_custom_impl_Yolo clean && make -C nvdsinfer_custom_impl_Yolo\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Using conda\nDESCRIPTION: Command to install Ultralytics from conda-forge channel. This provides an alternative to pip installation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/quickstart.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c conda-forge ultralytics\n```\n\n----------------------------------------\n\nTITLE: Creating AWS CloudFormation Stack in Bash\nDESCRIPTION: Commands to synthesize the CDK application, bootstrap the AWS environment, and deploy the stack for creating necessary AWS resources.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/amazon-sagemaker.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncdk synth\ncdk bootstrap\ncdk deploy\n```\n\n----------------------------------------\n\nTITLE: Pulling Ultralytics Docker Image\nDESCRIPTION: Command to pull the latest Ultralytics Docker image from Docker Hub. This provides an isolated environment for running Ultralytics applications.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/quickstart.md#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n# Pull the latest ultralytics image from Docker Hub\nsudo docker pull ultralytics/ultralytics:latest\n```\n\n----------------------------------------\n\nTITLE: Dataset Configuration with YAML\nDESCRIPTION: YAML configuration file for the African Wildlife Dataset defining paths, classes and other dataset parameters\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/african-wildlife.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/african-wildlife.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Downloading COCO Dataset for Validation in Python\nDESCRIPTION: This Python code downloads the COCO2017 validation dataset and extracts it. It uses PyTorch's hub utility to download the dataset zip file and then unzips it using a shell command.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\ntorch.hub.download_url_to_file('https://ultralytics.com/assets/coco2017val.zip', 'tmp.zip')  # download (780M - 5000 images)\n!unzip -q tmp.zip -d datasets && rm tmp.zip  # unzip\n```\n\n----------------------------------------\n\nTITLE: Creating Calibration Directory\nDESCRIPTION: Creates a new directory to store calibration images\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/deepstream-nvidia-jetson.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nmkdir calibration\n```\n\n----------------------------------------\n\nTITLE: Defining Brain Tumor Dataset Configuration in YAML\nDESCRIPTION: YAML configuration file for the brain tumor dataset, specifying paths, classes, and other relevant information for training and evaluation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/brain-tumor.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/brain-tumor.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Implementing ProfileModels Class for YOLO Model Profiling in Python\nDESCRIPTION: This class provides methods for profiling YOLO models across different formats such as PyTorch, ONNX, and TensorRT. It likely includes functionality for measuring inference speed and resource usage.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/benchmarks.md#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nclass ProfileModels:\n```\n\n----------------------------------------\n\nTITLE: Defining DOTA8 Dataset Configuration in YAML\nDESCRIPTION: YAML configuration file for the DOTA8 dataset, specifying paths, class names, and other relevant information for use with Ultralytics YOLO11.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/obb/dota8.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/dota8.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Models with Different Configurations\nDESCRIPTION: Example commands for training YOLO11 models with various configurations, demonstrating how to adjust parameters like model type, dataset, epochs, and image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/dvc.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Example training commands for YOLO11 with varying configurations\nyolo train model=yolo11n.pt data=coco8.yaml epochs=5 imgsz=512\nyolo train model=yolo11n.pt data=coco8.yaml epochs=5 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Mapping Images to Label Paths for Dataset Management - Python\nDESCRIPTION: The img2label_paths utility generates corresponding label file paths for each image in a dataset, facilitating training workflows by ensuring each image-map to a valid annotation. It expects a list of image file paths as input and outputs a list of the inferred label paths. Dependencies typically include standard file system and path libraries, with constraints that label files must exist in specified locations.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/data/utils.md#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n## ::: ultralytics.data.utils.img2label_paths\n```\n\n----------------------------------------\n\nTITLE: Splitting DOTA Train and Validation Set - Ultralytics DOTA Utilities - Python\nDESCRIPTION: Implements a routine to divide DOTA dataset images and labels into distinct training and validation sets based on input ratios or configurations. Used to ensure robust evaluation and reproducibility in ML pipelines. Expects directories and split parameters as input; creates organized output folder structure.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/data/split_dota.md#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ndef split_trainval(img_dir, label_dir, save_dir, val_ratio):\n    # Splits dataset into training and validation sets according to specified ratio\n    pass  # Implementation handles file partitioning and directory creation\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search\nDESCRIPTION: Executes similarity search on dataset using index references\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/explorer.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsimilar = exp.get_similar(idx=1, limit=10)\nsimilar.head()\n```\n\n----------------------------------------\n\nTITLE: Converting YOLOE Model to Detection Model\nDESCRIPTION: Shows how to convert a trained text-prompt model to a detection model for visual prompt training.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yoloe.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom ultralytics import YOLOE\n\ndet_model = YOLOE(\"yoloe-11l.yaml\")\nstate = torch.load(\"yoloe-11l-seg.pt\")\ndet_model.load(state[\"model\"])\ndet_model.save(\"yoloe-11l-seg-det.pt\")\n```\n\n----------------------------------------\n\nTITLE: Defining BaseTensor for Inference Outputs — Python\nDESCRIPTION: BaseTensor serves as a foundational tensor class for handling raw inference results in the Ultralytics engine. It provides core interoperability with PyTorch tensors and supports custom result processing workflows. Users must ensure they have PyTorch installed and should use this class for building higher-level result abstractions.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/engine/results.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n## ::: ultralytics.engine.results.BaseTensor\n```\n\n----------------------------------------\n\nTITLE: Generating Hash Digests for Data Integrity - Python\nDESCRIPTION: The get_hash utility computes one or more hash digest values for a given file, directory, or data buffer, supporting dataset versioning and integrity checks. Inputs include paths or data objects; outputs are string representations of computed hashes. Implementation may depend on Python's hashlib; limitations depend on file size and hash collision probabilities.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/data/utils.md#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n## ::: ultralytics.data.utils.get_hash\n```\n\n----------------------------------------\n\nTITLE: Classification API Response Format\nDESCRIPTION: Example JSON response format from the API when using a classification model, showing the structure of results including class predictions and processing metrics.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/hub/inference-api.md#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"images\": [\n    {\n      \"results\": [\n        {\n          \"class\": 0,\n          \"name\": \"person\",\n          \"confidence\": 0.92\n        }\n      ],\n      \"shape\": [\n        750,\n        600\n      ],\n      \"speed\": {\n        \"inference\": 200.8,\n        \"postprocess\": 0.8,\n        \"preprocess\": 2.8\n      }\n    }\n  ],\n  \"metadata\": ...\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics and ClearML\nDESCRIPTION: Command to install the necessary packages for integrating Ultralytics YOLO11 with ClearML.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/clearml.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics clearml\n```\n\n----------------------------------------\n\nTITLE: Converting YOLO11 Model to TFLite using CLI\nDESCRIPTION: Command line instructions for exporting a YOLO11 model to TFLite format and running inference using the exported model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tflite.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Export a YOLO11n PyTorch model to TFLite format\nyolo export model=yolo11n.pt format=tflite # creates 'yolo11n_float32.tflite'\n\n# Run inference with the exported model\nyolo predict model='yolo11n_float32.tflite' source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Updating MLflow Settings in Python\nDESCRIPTION: Python code to enable MLflow logging in Ultralytics settings by updating the settings object. Includes options to both update and reset settings.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/mlflow.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import settings\n\n# Update a setting\nsettings.update({\"mlflow\": True})\n\n# Reset settings to default values\nsettings.reset()\n```\n\n----------------------------------------\n\nTITLE: Tracking Objects with FastSAM Model - CLI\nDESCRIPTION: Shows the use of the Ultralytics CLI for video tracking with the FastSAM segmentation model. Accepts video source path, model weights, and image size as arguments. Segmentation and tracking results may be stored or displayed, depending on CLI settings. Requires CLI setup and model weight availability.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/fast-sam.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nyolo segment track model=FastSAM-s.pt source=\"path/to/video.mp4\" imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Generating Comparative Plots with DVC for YOLO11 Experiments\nDESCRIPTION: Command to generate comparative visualization plots of metrics across different YOLO11 experiments using DVC's plotting functionality.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/dvc.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Generate DVC comparative plots\ndvc plots diff $(dvc exp list --names-only)\n```\n\n----------------------------------------\n\nTITLE: Auto-annotating Images with Ultralytics YOLO\nDESCRIPTION: Demonstrates the use of auto_annotate function to generate segmentation annotations automatically using pre-trained detection and SAM models. This feature reduces manual labeling effort by automating the annotation process.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/index.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.data.annotator import auto_annotate\n\nauto_annotate(data=\"path/to/images\", det_model=\"yolo11x.pt\", sam_model=\"sam_b.pt\")  # or sam_model=\"mobile_sam.pt\"\n```\n\n----------------------------------------\n\nTITLE: Initializing DataFrame for Label Analysis\nDESCRIPTION: Creates an empty pandas DataFrame with columns for each class index and rows indexed by label filenames. This structure will store the count of instances for each class.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/kfold-cross-validation.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nindex = [label.stem for label in labels]  # uses base filename as ID (no extension)\nlabels_df = pd.DataFrame([], columns=cls_idx, index=index)\n```\n\n----------------------------------------\n\nTITLE: Resuming Interrupted YOLO11 Training with CLI\nDESCRIPTION: Shows how to resume a previously interrupted YOLO11 training session using the command-line interface. This command loads the last saved checkpoint and continues the training process.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/train.md#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nyolo train resume model=path/to/last.pt\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics and Checking Environment in Python\nDESCRIPTION: This snippet installs the ultralytics package, imports necessary modules, and runs environment checks. It's used to set up the development environment for working with YOLO models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/hub.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install ultralytics  # install\nfrom ultralytics import YOLO, checks, hub\n\nchecks()  # checks\n```\n\n----------------------------------------\n\nTITLE: Installing TensorFlow with GPU (CUDA) Support - Bash\nDESCRIPTION: This snippet demonstrates how to install TensorFlow with GPU (CUDA) support via pip, allowing inference on compatible NVIDIA GPUs for faster performance. It is crucial to have the correct versions of NVIDIA drivers and CUDA/cuDNN installed beforehand, as specified in the official TensorFlow documentation. The exact package specifier (e.g., 'and-cuda') and installation process may vary according to the TensorFlow version and operating system; always verify the official guide.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-TFLite-Python/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Check TensorFlow documentation for specific CUDA/cuDNN version requirements\npip install tensorflow[and-cuda] # Or follow specific instructions on TF website\n\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow with pip\nDESCRIPTION: Command to install MLflow using pip package manager, which is a prerequisite for using MLflow logging with Ultralytics YOLO.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/mlflow.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mlflow\n```\n\n----------------------------------------\n\nTITLE: Generating Parameter Documentation Table using Jinja2 for Ultralytics YOLO\nDESCRIPTION: This Jinja2 macro creates a markdown table documenting parameters for Ultralytics YOLO projects. It includes default parameters and their details, with the option to specify custom parameters. The macro handles formatting and populates the table with parameter names, types, default values, and descriptions.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/macros/solutions-args.md#2025-04-22_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n{% macro param_table(params=None) %}\n\n| Argument | Type | Default | Description |\n| -------- | ---- | ------- | ----------- |\n\n{%- set default_params = {\n    \"model\": [\"str\", \"None\", \"Path to Ultralytics YOLO Model File.\"],\n    \"region\": [\"list\", \"[(20, 400), (1260, 400)]\", \"List of points defining the counting region.\"],\n    \"show_in\": [\"bool\", \"True\", \"Flag to control whether to display the in counts on the video stream.\"],\n    \"show_out\": [\"bool\", \"True\", \"Flag to control whether to display the out counts on the video stream.\"],\n    \"analytics_type\": [\"str\", \"line\", \"Type of graph, i.e., `line`, `bar`, `area`, or `pie`.\"],\n    \"colormap\": [\"int\", \"cv2.COLORMAP_JET\", \"Colormap to use for the heatmap.\"],\n    \"json_file\": [\"str\", \"None\", \"Path to the JSON file that contains all parking coordinates data.\"],\n    \"up_angle\": [\"float\", \"145.0\", \"Angle threshold for the 'up' pose.\"],\n    \"kpts\": [\"list[int, int, int]\", \"[6, 8, 10]\", \"List of keypoints used for monitoring workouts. These keypoints correspond to body joints or parts, such as shoulders, elbows, and wrists, for exercises like push-ups, pull-ups, squats, ab-workouts.\"],\n    \"down_angle\": [\"float\", \"90.0\", \"Angle threshold for the 'down' pose.\"],\n    \"blur_ratio\": [\"float\", \"0.5\", \"Adjusts percentage of blur intensity, with values in range `0.1 - 1.0`.\"],\n    \"crop_dir\": [\"str\", \"\\\"cropped-detections\\\"\", \"Directory name for storing cropped detections.\"],\n    \"records\": [\"int\", \"5\", \"Total detections count to trigger an email with security alarm system.\"],\n    \"vision_point\": [\"tuple[int, int]\", \"(50, 50)\", \"The point where vision will track objects and draw paths using VisionEye Solution.\"],\n    \"tracker\": [\"str\", \"'botsort.yaml'\", \"Specifies the tracking algorithm to use, e.g., `bytetrack.yaml` or `botsort.yaml`.\"],\n    \"conf\": [\"float\", \"0.3\", \"Sets the confidence threshold for detections; lower values allow more objects to be tracked but may include false positives.\"],\n    \"iou\": [\"float\", \"0.5\", \"Sets the Intersection over Union (IoU) threshold for filtering overlapping detections.\"],\n    \"classes\": [\"list\", \"None\", \"Filters results by class index. For example, `classes=[0, 2, 3]` only tracks the specified classes.\"],\n    \"verbose\": [\"bool\", \"True\", \"Controls the display of tracking results, providing a visual output of tracked objects.\"],\n    \"device\": [\"str\", \"None\", \"Specifies the device for inference (e.g., `cpu`, `cuda:0` or `0`). Allows users to select between CPU, a specific GPU, or other compute devices for model execution.\"],\n    \"show\": [\"bool\", \"False\", \"If `True`, displays the annotated images or videos in a window. Useful for immediate visual feedback during development or testing.\"],\n    \"line_width\": [\"None or int\", \"None\", \"Specifies the line width of bounding boxes. If `None`, the line width is automatically adjusted based on the image size. Provides visual customization for clarity.\"]\n} %}\n\n{%- if not params %}\n{%- for param, details in default_params.items() %}\n| `{{ param }}` | `{{ details[0] }}` | `{{ details[1] }}` | {{ details[2] }} |\n{%- endfor %}\n{%- else %}\n{%- for param in params %}\n{%- if param in default_params %}\n| `{{ param }}` | `{{ default_params[param][0] }}` | `{{ default_params[param][1] }}` | {{ default_params[param][2] }} |\n{%- endif %}\n{%- endfor %}\n{%- endif %}\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Setting Up Python Environment for CDK\nDESCRIPTION: This snippet sets up a Python virtual environment, activates it, and installs necessary dependencies, including upgrading the AWS CDK library. Python3 and pip must be installed.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/amazon-sagemaker.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m venv .venv\nsource .venv/bin/activate\npip3 install -r requirements.txt\npip install --upgrade aws-cdk-lib\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration for Segmentation Dataset\nDESCRIPTION: Example YAML configuration file defining the dataset and model configuration for training segmentation models with Ultralytics YOLO.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/index.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\npath: ../datasets/coco8-seg\ntrain: images/train\nval: images/val\ntest:\n\nnames:\n    0: person\n    1: bicycle\n    2: car\n    # ...\n    77: teddy bear\n    78: hair drier\n    79: toothbrush\n```\n\n----------------------------------------\n\nTITLE: Configuring YAML for Ultralytics YOLO Training\nDESCRIPTION: Example YAML configuration file structure for training YOLO models, showing how to specify dataset paths and class names. The configuration includes paths for training and validation datasets relative to the root directory.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/index.md#2025-04-22_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\npath: ../datasets/coco8-seg # dataset root dir\ntrain: images/train # train images (relative to 'path')\nval: images/val # val images (relative to 'path')\n\nnames:\n    0: person\n    1: bicycle\n    2: car\n    # ...\n```\n\n----------------------------------------\n\nTITLE: Converting Plot Results to Bytes\nDESCRIPTION: Code to convert numpy array plot results to bytes format using OpenCV\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/view-results-in-terminal.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport io\nimport cv2\n\n# Results image as bytes\nim_bytes = cv2.imencode(\n    \".png\",\n    plot,\n)[1].tobytes()\n\n# Image bytes as a file-like object\nmem_file = io.BytesIO(im_bytes)\n```\n\n----------------------------------------\n\nTITLE: Deploying to GitHub Pages\nDESCRIPTION: Command to deploy the MkDocs documentation to GitHub Pages using the built-in deployment feature.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/README.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmkdocs gh-deploy\n```\n\n----------------------------------------\n\nTITLE: Embedding YouTube Video with HTML\nDESCRIPTION: HTML iframe code for embedding a YouTube video that shows a discussion between Ultralytics founder Glenn Jocher and James Skelton from Paperspace about optimizing YOLO11 training environments.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/paperspace.md#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<iframe loading=\"lazy\" width=\"720\" height=\"405\" src=\"https://www.youtube.com/embed/3HbbQHitN7g?si=DjuwrzMkW1WEoH5Y\"\n  title=\"YouTube video player\" frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n  allowfullscreen>\n</iframe>\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model to TFLite Format\nDESCRIPTION: Commands to export a YOLO11 model to TensorFlow Lite format for mobile deployment. Can be executed either through Python API or command line interface.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/model-deployment-options.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Export command for TFLite format\nmodel.export(format=\"tflite\")\n```\n\nLANGUAGE: bash\nCODE:\n```\n# CLI command for TFLite export\nyolo export --format tflite\n```\n\n----------------------------------------\n\nTITLE: CLI Commands for Analytics Visualization\nDESCRIPTION: Command line interface commands for generating different types of analytics visualizations using YOLO. Includes commands for showing analytics, specifying video source, and generating pie charts, bar plots, and area plots.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/analytics.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nyolo solutions analytics show=True\n\n# Pass the source\nyolo solutions analytics source=\"path/to/video.mp4\"\n\n# Generate the pie chart\nyolo solutions analytics analytics_type=\"pie\" show=True\n\n# Generate the bar plots\nyolo solutions analytics analytics_type=\"bar\" show=True\n\n# Generate the area plots\nyolo solutions analytics analytics_type=\"area\" show=True\n```\n\n----------------------------------------\n\nTITLE: Increasing Swap Memory on AWS Instance\nDESCRIPTION: Commands to allocate and configure swap memory on an AWS instance to handle memory-intensive operations when working with large datasets or models. This helps prevent out-of-memory errors during training.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/environments/aws_quickstart_tutorial.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Allocate a 64GB swap file (adjust size as needed)\nsudo fallocate -l 64G /swapfile\n\n# Set correct permissions\nsudo chmod 600 /swapfile\n\n# Set up the file as a Linux swap area\nsudo mkswap /swapfile\n\n# Enable the swap file\nsudo swapon /swapfile\n\n# Verify the swap memory is active\nfree -h\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Models on CIFAR-100 Dataset Using CLI\nDESCRIPTION: Command-line interface command for training a pre-trained YOLO model on the CIFAR-100 dataset for 100 epochs with 32x32 image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/cifar100.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo classify train data=cifar100 model=yolo11n-cls.pt epochs=100 imgsz=32\n```\n\n----------------------------------------\n\nTITLE: Configuring Interactive Tracker Parameters in Python\nDESCRIPTION: Global configuration settings for the interactive tracker script, including model selection, display options, detection parameters, and tracking settings. These parameters control the behavior of the object tracking system.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLO-Interactive-Tracking-UI/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# --- Configuration ---\nenable_gpu = False  # Set True if running with CUDA and PyTorch model\nmodel_file = \"yolo11s.pt\"  # Path to model file (.pt for GPU, _ncnn_model dir for CPU)\nshow_fps = True  # Display current FPS in the top-left corner\nshow_conf = False  # Display confidence score for each detection\nsave_video = False  # Set True to save the output video stream\nvideo_output_path = \"interactive_tracker_output.avi\"  # Output video file name\n\n# --- Detection & Tracking Parameters ---\nconf = 0.3  # Minimum confidence threshold for object detection\niou = 0.3  # IoU threshold for Non-Maximum Suppression (NMS)\nmax_det = 20  # Maximum number of objects to detect per frame\n\ntracker = \"bytetrack.yaml\"  # Tracker configuration: 'bytetrack.yaml' or 'botsort.yaml'\ntrack_args = {\n    \"persist\": True,  # Keep track history across frames\n    \"verbose\": False,  # Suppress detailed tracker debug output\n}\n\nwindow_name = \"Ultralytics YOLO Interactive Tracking\"  # Name for the OpenCV display window\n# --- End Configuration ---\n```\n\n----------------------------------------\n\nTITLE: Initializing YOLO Segmentation Model in ROS\nDESCRIPTION: This snippet initializes a ROS node and loads a YOLO segmentation model. It sets up the environment for processing point cloud data with YOLO.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/ros-quickstart.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nimport rospy\n\nfrom ultralytics import YOLO\n\nrospy.init_node(\"ultralytics\")\ntime.sleep(1)\nsegmentation_model = YOLO(\"yolo11m-seg.pt\")\n```\n\n----------------------------------------\n\nTITLE: TensorBoard Callback for Training Epoch End in Python\nDESCRIPTION: This callback function is executed at the end of each training epoch. It likely logs epoch-specific metrics to TensorBoard.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/callbacks/tensorboard.md#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nultralytics.utils.callbacks.tensorboard.on_train_epoch_end\n```\n\n----------------------------------------\n\nTITLE: Cloning YOLO11 SageMaker Repository in Bash\nDESCRIPTION: This snippet demonstrates how to clone the YOLO11 model repository from GitHub and navigate to the relevant directory for deployment. Ensure Git is installed as a prerequisite.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/amazon-sagemaker.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/aws-samples/host-yolov8-on-sagemaker-endpoint.git\ncd host-yolov8-on-sagemaker-endpoint/yolov8-pytorch-cdk\n```\n\n----------------------------------------\n\nTITLE: Performing YOLO11 Inference with MNN in C++\nDESCRIPTION: This C++ program loads a YOLO11 model using MNN, preprocesses an input image, performs inference, and applies non-maximum suppression to the results. It includes command-line argument parsing for flexible usage and demonstrates low-level MNN API usage.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/mnn.md#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n#include <stdio.h>\n#include <MNN/ImageProcess.hpp>\n#include <MNN/expr/Module.hpp>\n#include <MNN/expr/Executor.hpp>\n#include <MNN/expr/ExprCreator.hpp>\n#include <MNN/expr/Executor.hpp>\n\n#include <cv/cv.hpp>\n\nusing namespace MNN;\nusing namespace MNN::Express;\nusing namespace MNN::CV;\n\nint main(int argc, const char* argv[]) {\n    if (argc < 3) {\n        MNN_PRINT(\"Usage: ./yolo11_demo.out model.mnn input.jpg [forwardType] [precision] [thread]\\n\");\n        return 0;\n    }\n    int thread = 4;\n    int precision = 0;\n    int forwardType = MNN_FORWARD_CPU;\n    if (argc >= 4) {\n        forwardType = atoi(argv[3]);\n    }\n    if (argc >= 5) {\n        precision = atoi(argv[4]);\n    }\n    if (argc >= 6) {\n        thread = atoi(argv[5]);\n    }\n    MNN::ScheduleConfig sConfig;\n    sConfig.type = static_cast<MNNForwardType>(forwardType);\n    sConfig.numThread = thread;\n    BackendConfig bConfig;\n    bConfig.precision = static_cast<BackendConfig::PrecisionMode>(precision);\n    sConfig.backendConfig = &bConfig;\n    std::shared_ptr<Executor::RuntimeManager> rtmgr = std::shared_ptr<Executor::RuntimeManager>(Executor::RuntimeManager::createRuntimeManager(sConfig));\n    if(rtmgr == nullptr) {\n        MNN_ERROR(\"Empty RuntimeManger\\n\");\n        return 0;\n    }\n    rtmgr->setCache(\".cachefile\");\n\n    std::shared_ptr<Module> net(Module::load(std::vector<std::string>{}, std::vector<std::string>{}, argv[1], rtmgr));\n    auto original_image = imread(argv[2]);\n    auto dims = original_image->getInfo()->dim;\n    int ih = dims[0];\n    int iw = dims[1];\n    int len = ih > iw ? ih : iw;\n    float scale = len / 640.0;\n    std::vector<int> padvals { 0, len - ih, 0, len - iw, 0, 0 };\n    auto pads = _Const(static_cast<void*>(padvals.data()), {3, 2}, NCHW, halide_type_of<int>());\n    auto image = _Pad(original_image, pads, CONSTANT);\n    image = resize(image, Size(640, 640), 0, 0, INTER_LINEAR, -1, {0., 0., 0.}, {1./255., 1./255., 1./255.});\n    image = cvtColor(image, COLOR_BGR2RGB);\n    auto input = _Unsqueeze(image, {0});\n    input = _Convert(input, NC4HW4);\n    auto outputs = net->onForward({input});\n    auto output = _Convert(outputs[0], NCHW);\n    output = _Squeeze(output);\n    // output shape: [84, 8400]; 84 means: [cx, cy, w, h, prob * 80]\n    auto cx = _Gather(output, _Scalar<int>(0));\n    auto cy = _Gather(output, _Scalar<int>(1));\n    auto w = _Gather(output, _Scalar<int>(2));\n    auto h = _Gather(output, _Scalar<int>(3));\n    std::vector<int> startvals { 4, 0 };\n    auto start = _Const(static_cast<void*>(startvals.data()), {2}, NCHW, halide_type_of<int>());\n    std::vector<int> sizevals { -1, -1 };\n    auto size = _Const(static_cast<void*>(sizevals.data()), {2}, NCHW, halide_type_of<int>());\n    auto probs = _Slice(output, start, size);\n    // [cx, cy, w, h] -> [y0, x0, y1, x1]\n    auto x0 = cx - w * _Const(0.5);\n    auto y0 = cy - h * _Const(0.5);\n    auto x1 = cx + w * _Const(0.5);\n    auto y1 = cy + h * _Const(0.5);\n    auto boxes = _Stack({x0, y0, x1, y1}, 1);\n    // ensure ratio is within the valid range [0.0, 1.0]\n    boxes = _Maximum(boxes, _Scalar<float>(0.0f));\n    boxes = _Minimum(boxes, _Scalar<float>(1.0f));\n    auto scores = _ReduceMax(probs, {0});\n    auto ids = _ArgMax(probs, 0);\n    auto result_ids = _Nms(boxes, scores, 100, 0.45, 0.25);\n    auto result_ptr = result_ids->readMap<int>();\n    auto box_ptr = boxes->readMap<float>();\n    auto ids_ptr = ids->readMap<int>();\n    auto score_ptr = scores->readMap<float>();\n    for (int i = 0; i < 100; i++) {\n        auto idx = result_ptr[i];\n        if (idx < 0) break;\n        auto x0 = box_ptr[idx * 4 + 0] * scale;\n        auto y0 = box_ptr[idx * 4 + 1] * scale;\n        auto x1 = box_ptr[idx * 4 + 2] * scale;\n        auto y1 = box_ptr[idx * 4 + 3] * scale;\n        // clamp to the original image size to handle cases where padding was applied\n        x1 = std::min(static_cast<float>(iw), x1);\n        y1 = std::min(static_cast<float>(ih), y1);\n        auto class_idx = ids_ptr[idx];\n        auto score = score_ptr[idx];\n        rectangle(original_image, {x0, y0}, {x1, y1}, {0, 0, 255}, 2);\n    }\n    if (imwrite(\"res.jpg\", original_image)) {\n        MNN_PRINT(\"result image write to `res.jpg`.\\n\");\n    }\n    rtmgr->updateCache();\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: YOLO Model Validation Examples\nDESCRIPTION: Demonstrates validation operations on YOLO models, including validation after training and validation on separate datasets.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/python.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a YOLO model\nmodel = YOLO(\"yolo11n.yaml\")\n\n# Train the model\nmodel.train(data=\"coco8.yaml\", epochs=5)\n\n# Validate on training data\nmodel.val()\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a YOLO model\nmodel = YOLO(\"yolo11n.yaml\")\n\n# Train the model\nmodel.train(data=\"coco8.yaml\", epochs=5)\n\n# Validate on separate data\nmodel.val(data=\"path/to/separate/data.yaml\")\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO Model to MNN Format - CLI\nDESCRIPTION: The snippet demonstrates using the command line to export a YOLO11 model to MNN format with different weight precisions. This requires the Ultralytics YOLO CLI tool. Parameters include model path and export options such as 'half' for fp16 and 'int8' for integer quantized weight.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/mnn.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=yolo11n.pt format=mnn           # creates 'yolo11n.mnn' with fp32 weight\nyolo export model=yolo11n.pt format=mnn half=True # creates 'yolo11n.mnn' with fp16 weight\nyolo export model=yolo11n.pt format=mnn int8=True # creates 'yolo11n.mnn' with int8 weight\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up After DDP Training in Ultralytics\nDESCRIPTION: Function to perform cleanup operations after Distributed Data Parallel (DDP) training in Ultralytics. It handles post-training tasks and resource management.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/dist.md#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nultralytics.utils.dist.ddp_cleanup\n```\n\n----------------------------------------\n\nTITLE: Building the Project with CMake and Make in Bash\nDESCRIPTION: These commands create a build directory, use CMake to configure the project, and compile it using Make. It includes an optional command to specify LibTorch and OpenCV paths if needed.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-LibTorch-CPP-Inference/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmkdir build\ncd build\ncmake .. # Add -DCMAKE_PREFIX_PATH=/path/to/libtorch;/path/to/opencv if needed\nmake\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on ImageNet via CLI\nDESCRIPTION: Command line interface example for training a pretrained YOLO model on ImageNet dataset with specified parameters.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/imagenet.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo classify train data=imagenet model=yolo11n-cls.pt epochs=100 imgsz=224\n```\n\n----------------------------------------\n\nTITLE: Cloning Ultralytics Repository and Installing\nDESCRIPTION: This sequence of commands clones the Ultralytics GitHub repository and installs its packages in editable mode. It requires `git` and `pip` to be installed on the system. The snippet demonstrates an approach for working on the development version of Ultralytics.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n!git clone https://github.com/ultralytics/ultralytics -b main\n%pip install -qe ultralytics\n```\n\n----------------------------------------\n\nTITLE: Running Tests on Ultralytics Repository\nDESCRIPTION: This command executes tests on the Ultralytics repository cloned on the system. Necessary dependencies include the `pytest` testing framework. The operation validates that all tests run correctly in the current package state.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n!pytest ultralytics/tests\n```\n\n----------------------------------------\n\nTITLE: Configuring Dataset in Ultralytics YOLO Format (YAML)\nDESCRIPTION: Example YAML configuration file for defining a dataset in Ultralytics YOLO format. This specifies the dataset root directory, paths to training/validation/testing images, and a dictionary of class names.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/index.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\npath: ../datasets/coco8 # dataset root dir (absolute or relative; if relative, it's relative to default datasets_dir)\ntrain: images/train # train images (relative to 'path') 4 images\nval: images/val # val images (relative to 'path') 4 images\ntest: # test images (optional)\n\n# Classes (80 COCO classes)\nnames:\n    0: person\n    1: bicycle\n    2: car\n    # ...\n    77: teddy bear\n    78: hair drier\n    79: toothbrush\n```\n\n----------------------------------------\n\nTITLE: Mounting Local Directories in Docker\nDESCRIPTION: Command to run Ultralytics Docker container with a mounted local directory for file access.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/docker-quickstart.md#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nsudo docker run -it --ipc=host --gpus all -v /path/on/host:/path/in/container ultralytics/ultralytics:latest\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame for K-Fold Split Results in Python\nDESCRIPTION: This code constructs a DataFrame to display the results of the K-Fold split more clearly. It labels each data point as either 'train' or 'val' for each split.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/kfold-cross-validation.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfolds = [f\"split_{n}\" for n in range(1, ksplit + 1)]\nfolds_df = pd.DataFrame(index=index, columns=folds)\n\nfor i, (train, val) in enumerate(kfolds, start=1):\n    folds_df[f\"split_{i}\"].loc[labels_df.iloc[train].index] = \"train\"\n    folds_df[f\"split_{i}\"].loc[labels_df.iloc[val].index] = \"val\"\n```\n\n----------------------------------------\n\nTITLE: Training YOLOv5 with Default ClearML Settings\nDESCRIPTION: This command runs YOLOv5 training with default ClearML settings, capturing experiment details automatically.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/clearml_logging_integration.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython train.py --img 640 --batch 16 --epochs 3 --data coco8.yaml --weights yolov5s.pt --cache\n```\n\n----------------------------------------\n\nTITLE: Copying Images and Labels for K-Fold Splits in Python\nDESCRIPTION: This snippet copies images and labels into the respective 'train' or 'val' directory for each split. It uses tqdm for progress tracking and shutil for file copying.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/kfold-cross-validation.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport shutil\n\nfrom tqdm import tqdm\n\nfor image, label in tqdm(zip(images, labels), total=len(images), desc=\"Copying files\"):\n    for split, k_split in folds_df.loc[image.stem].items():\n        # Destination directory\n        img_to_path = save_path / split / k_split / \"images\"\n        lbl_to_path = save_path / split / k_split / \"labels\"\n\n        # Copy image and label files to new directory (SamefileError if file already exists)\n        shutil.copy(image, img_to_path / image.name)\n        shutil.copy(label, lbl_to_path / label.name)\n```\n\n----------------------------------------\n\nTITLE: Using Custom Tracker Configuration via Command Line\nDESCRIPTION: This code snippet shows how to use a custom tracker configuration file when running the YOLO tracker from the command line interface.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/trackers/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Load the model and run the tracker with a custom configuration file using the command line interface\nyolo track model=yolo11n.pt source=\"https://youtu.be/LNwODJXcvt4\" tracker='custom_tracker.yaml'\n```\n\n----------------------------------------\n\nTITLE: Disabling Data Collection in Ultralytics YOLO using Python\nDESCRIPTION: This snippet demonstrates how to disable analytics and crash reporting in Ultralytics YOLO using Python. It also shows how to reset settings to default values.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/help/privacy.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import settings\n\n# Disable analytics and crash reporting\nsettings.update({\"sync\": False})\n\n# Reset settings to default values\nsettings.reset()\n```\n\n----------------------------------------\n\nTITLE: Running YOLOv8 Inference with CUDA\nDESCRIPTION: Command to run YOLOv8 inference using CUDA acceleration with Rust.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-ONNXRuntime-Rust/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncargo run --release -- --cuda --model MODEL_PATH.onnx --source SOURCE_IMAGE.jpg\n```\n\n----------------------------------------\n\nTITLE: Comet ML Integration Setup\nDESCRIPTION: Commands for setting up Comet ML integration with YOLOv5 for experiment tracking and visualization.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/train_custom_data.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npip install comet_ml                                                          # 1. Install Comet library\nexport COMET_API_KEY=YOUR_API_KEY_HERE                                        # 2. Set your Comet API key (create a free account at Comet.ml)\npython train.py --img 640 --epochs 3 --data coco128.yaml --weights yolov5s.pt # 3. Train your model - Comet automatically logs everything!\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Standard Pruned-Quantized YOLOv5s with DeepSparse\nDESCRIPTION: Benchmark command for running pruned65_quant-none YOLOv5s model using DeepSparse with sync scenario, batch size 1, and single stream. Achieves 135 items/sec throughput.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/neural_magic_pruning_quantization.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndeepsparse.benchmark zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned65_quant-none -s sync -b 1 -nstreams 1\n\n# Original Model Path: zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned65_quant-none\n# Batch Size: 1\n# Scenario: sync\n# Throughput (items/sec): 134.9468\n```\n\n----------------------------------------\n\nTITLE: Citing Ultralytics YOLOv8 with BibTeX\nDESCRIPTION: Presents the official BibTeX entry for citing the Ultralytics YOLOv8 software in academic work or publications. Includes authors, title, version, year, URL, ORCID identifiers, and license information.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov8.md#2025-04-22_snippet_4\n\nLANGUAGE: bibtex\nCODE:\n```\n@software{yolov8_ultralytics,\n  author = {Glenn Jocher and Ayush Chaurasia and Jing Qiu},\n  title = {Ultralytics YOLOv8},\n  version = {8.0.0},\n  year = {2023},\n  url = {https://github.com/ultralytics/ultralytics},\n  orcid = {0000-0001-5950-6979, 0000-0002-7603-6750, 0000-0003-3783-7069},\n  license = {AGPL-3.0}\n}\n```\n\n----------------------------------------\n\nTITLE: Organizing Dataset for YOLO Training in Python\nDESCRIPTION: This Python script is used for preprocessing a dataset to comply with the YOLO11 training requirements on IBM Watsonx. It organizes files into appropriate directories and updates configurations accordingly. The script depends on `os` and `shutil` libraries. It requires the dataset to be structured with separate subdirectories for train, test, and validation data. Inputs include the path to the dataset directory, and outputs are reorganized data files.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/ibm-watsonx.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport shutil\n\n\ndef organize_files(directory):\n    for subdir in [\"train\", \"test\", \"val\"]:\n        subdir_path = os.path.join(directory, subdir)\n        if not os.path.exists(subdir_path):\n            continue\n\n        images_dir = os.path.join(subdir_path, \"images\")\n        labels_dir = os.path.join(subdir_path, \"labels\")\n\n        os.makedirs(images_dir, exist_ok=True)\n        os.makedirs(labels_dir, exist_ok=True)\n\n        for filename in os.listdir(subdir_path):\n            if filename.endswith(\".txt\"):\n                shutil.move(os.path.join(subdir_path, filename), os.path.join(labels_dir, filename))\n            elif filename.endswith(\".jpg\") or filename.endswith(\".png\") or filename.endswith(\".jpeg\"):\n                shutil.move(os.path.join(subdir_path, filename), os.path.join(images_dir, filename))\n\n\nif __name__ == \"__main__\":\n    directory = f\"{work_dir}/trash_ICRA19/dataset\"\n    organize_files(directory)\n```\n\n----------------------------------------\n\nTITLE: Validating YOLOE Model without Prompts\nDESCRIPTION: Code to validate a YOLOE model in prompt-free mode. The model is validated on the COCO128-seg dataset without any specific prompting mechanism.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yoloe.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLOE\n\n# Create a YOLOE model\nmodel = YOLOE(\"yoloe-11l-seg.pt\")  # or select yoloe-m/l-seg.pt for different sizes\n\n# Conduct model validation on the COCO128-seg example dataset\nmetrics = model.val(data=\"coco128-seg.yaml\")\n```\n\n----------------------------------------\n\nTITLE: YOLOv5 Hyperparameter Configuration\nDESCRIPTION: YAML configuration defining initial hyperparameters for YOLOv5 training, including learning rates, augmentation settings, and loss parameters.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/hyperparameter_evolution.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# YOLOv5 🚀 by Ultralytics, AGPL-3.0 license\nlr0: 0.01\nlrf: 0.01\nmomentum: 0.937\nweight_decay: 0.0005\nwarmup_epochs: 3.0\nwarmup_momentum: 0.8\nwarmup_bias_lr: 0.1\nbox: 0.05\ncls: 0.5\ncls_pw: 1.0\nobj: 1.0\nobj_pw: 1.0\niou_t: 0.20\nanchor_t: 4.0\nfl_gamma: 0.0\nhsv_h: 0.015\nhsv_s: 0.7\nhsv_v: 0.4\ndegrees: 0.0\ntranslate: 0.1\nscale: 0.5\nshear: 0.0\nperspective: 0.0\nflipud: 0.0\nfliplr: 0.5\nmosaic: 1.0\nmixup: 0.0\ncopy_paste: 0.0\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package for YOLO11 Export\nDESCRIPTION: This command installs the ultralytics package, which is necessary for exporting YOLO11 models to TensorFlow SavedModel format.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tf-savedmodel.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package\nDESCRIPTION: Command to install the required Ultralytics package for YOLO11 via pip\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tf-savedmodel.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics\n```\n\n----------------------------------------\n\nTITLE: Best Hyperparameters YAML Output from Ultralytics YOLO Tuning\nDESCRIPTION: Example of the best_hyperparameters.yaml file generated after hyperparameter tuning. It contains the optimized hyperparameter values and performance metrics for the best iteration.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/hyperparameter-tuning.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# 558/900 iterations complete ✅ (45536.81s)\n# Results saved to /usr/src/ultralytics/runs/detect/tune\n# Best fitness=0.64297 observed at iteration 498\n# Best fitness metrics are {'metrics/precision(B)': 0.87247, 'metrics/recall(B)': 0.71387, 'metrics/mAP50(B)': 0.79106, 'metrics/mAP50-95(B)': 0.62651, 'val/box_loss': 2.79884, 'val/cls_loss': 2.72386, 'val/dfl_loss': 0.68503, 'fitness': 0.64297}\n# Best fitness model is /usr/src/ultralytics/runs/detect/train498\n# Best fitness hyperparameters are printed below.\n\nlr0: 0.00269\nlrf: 0.00288\nmomentum: 0.73375\nweight_decay: 0.00015\nwarmup_epochs: 1.22935\nwarmup_momentum: 0.1525\nbox: 18.27875\ncls: 1.32899\ndfl: 0.56016\nhsv_h: 0.01148\nhsv_s: 0.53554\nhsv_v: 0.13636\ndegrees: 0.0\ntranslate: 0.12431\nscale: 0.07643\nshear: 0.0\nperspective: 0.0\nflipud: 0.0\nfliplr: 0.08631\nmosaic: 0.42551\nmixup: 0.0\ncopy_paste: 0.0\n```\n\n----------------------------------------\n\nTITLE: Loading and Running Inference with YOLO Segmentation Model in Python\nDESCRIPTION: This code snippet demonstrates how to load a YOLO segmentation model and run inference on an image using the Ultralytics library. It loads the model, performs inference, and prints the results in JSON format.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/hub/inference-api.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load model\nmodel = YOLO(\"yolov8n-seg.pt\")\n\n# Run inference\nresults = model(\"image.jpg\")\n\n# Print image.jpg results in JSON format\nprint(results[0].tojson())\n```\n\n----------------------------------------\n\nTITLE: Disabling MLflow Integration\nDESCRIPTION: Command to disable MLflow logging in Ultralytics YOLO settings when it's no longer needed for experiment tracking.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/mlflow.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nyolo settings mlflow=False\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model to TF GraphDef using Python\nDESCRIPTION: Python code that demonstrates how to load a YOLO11 model, export it to TF GraphDef format, and then use the exported model for inference on an image.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tf-graphdef.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to TF GraphDef format\nmodel.export(format=\"pb\")  # creates 'yolo11n.pb'\n\n# Load the exported TF GraphDef model\ntf_graphdef_model = YOLO(\"yolo11n.pb\")\n\n# Run inference\nresults = tf_graphdef_model(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLOv8 Model to ONNX Format\nDESCRIPTION: Command to export a YOLOv8 model from PyTorch format to ONNX format using Ultralytics CLI.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-ONNXRuntime/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=yolov8n.pt format=onnx\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n-obb Model on DOTA8 Dataset using CLI\nDESCRIPTION: Command-line interface (CLI) command for training a YOLO11n-obb model on the DOTA8 dataset for 100 epochs with an image size of 640.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/obb/dota8.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo obb train data=dota8.yaml model=yolo11n-obb.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Converting YOLO11 Model to NCNN Format Using CLI\nDESCRIPTION: Command-line instructions to export a YOLO11n PyTorch model to NCNN format and run inference with the exported model on the Raspberry Pi.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/raspberry-pi.md#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n# Export a YOLO11n PyTorch model to NCNN format\nyolo export model=yolo11n.pt format=ncnn # creates 'yolo11n_ncnn_model'\n\n# Run inference with the exported model\nyolo predict model='yolo11n_ncnn_model' source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Visualizing Embeddings with PCA and Matplotlib\nDESCRIPTION: Creates a 3D visualization of embedding space using PCA dimensionality reduction and matplotlib plotting.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/api.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Reduce dimensions using PCA to 3 components for visualization in 3D\npca = PCA(n_components=3)\nreduced_data = pca.fit_transform(embeddings)\n\n# Create a 3D scatter plot using Matplotlib Axes3D\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection=\"3d\")\n\n# Scatter plot\nax.scatter(reduced_data[:, 0], reduced_data[:, 1], reduced_data[:, 2], alpha=0.5)\nax.set_title(\"3D Scatter Plot of Reduced 256-Dimensional Data (PCA)\")\nax.set_xlabel(\"Component 1\")\nax.set_ylabel(\"Component 2\")\nax.set_zlabel(\"Component 3\")\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Accessing Ultralytics Supported File Formats in Python\nDESCRIPTION: This snippet shows how to programmatically access the supported image and video formats in Ultralytics. These constants are useful when implementing file validation or filtering in applications that use the Ultralytics package.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/simple-utilities.md#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.data.utils import IMG_FORMATS, VID_FORMATS\n\nprint(IMG_FORMATS)\n# {'tiff', 'pfm', 'bmp', 'mpo', 'dng', 'jpeg', 'png', 'webp', 'tif', 'jpg'}\n\nprint(VID_FORMATS)\n# {'avi', 'mpg', 'wmv', 'mpeg', 'm4v', 'mov', 'mp4', 'asf', 'mkv', 'ts', 'gif', 'webm'}\n```\n\n----------------------------------------\n\nTITLE: Type-Hinted Function Documentation Example\nDESCRIPTION: Example of a function using both Google-style docstrings and Python type hints for improved code clarity\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/CONTRIBUTING.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef example_function(arg1: int, arg2: int = 4) -> bool:\n    \"\"\"\n    Example function demonstrating Google-style docstrings.\n\n    Args:\n        arg1: The first argument.\n        arg2: The second argument, with a default value of 4.\n\n    Returns:\n        True if successful, False otherwise.\n\n    Examples:\n        >>> result = example_function(1, 2)  # returns False\n    \"\"\"\n    if arg1 == arg2:\n        return True\n    return False\n```\n\n----------------------------------------\n\nTITLE: Citing YOLOv6 (BibTeX)\nDESCRIPTION: This BibTeX entry provides the recommended citation for the YOLOv6 v3.0 research paper. It should be included in the References section of academic works that use or build on YOLOv6. No dependencies are required; paste directly into your .bib file.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov6.md#2025-04-22_snippet_4\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{li2023yolov6,\n      title={YOLOv6 v3.0: A Full-Scale Reloading},\n      author={Chuyi Li and Lulu Li and Yifei Geng and Hongliang Jiang and Meng Cheng and Bo Zhang and Zaidan Ke and Xiaoming Xu and Xiangxiang Chu},\n      year={2023},\n      eprint={2301.05586},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n\n```\n\n----------------------------------------\n\nTITLE: Converting YOLOv8 ONNX Model from FP32 to FP16\nDESCRIPTION: This Python script shows how to convert an exported YOLOv8 ONNX model from FP32 to FP16 precision. It uses the ONNX library and onnxconverter-common to load the FP32 model, convert it to FP16, and save the resulting model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-ONNXRuntime-CPP/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\nfrom onnxconverter_common import (\n    float16,\n)  # Ensure you have onnxconverter-common installed: pip install onnxconverter-common\n\n# Load your FP32 ONNX model\nfp32_model_path = \"yolov8n.onnx\"\nmodel = onnx.load(fp32_model_path)\n\n# Convert the model to FP16\nmodel_fp16 = float16.convert_float_to_float16(model)\n\n# Save the FP16 model\nfp16_model_path = \"yolov8n_fp16.onnx\"\nonnx.save(model_fp16, fp16_model_path)\nprint(f\"Model converted and saved to {fp16_model_path}\")\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model to ONNX Format using CLI\nDESCRIPTION: Command line interface command to export a YOLO11n PyTorch model to ONNX format with opset 14\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/seeedstudio-recamera.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=yolo11n.pt format=onnx opset=14\n```\n\n----------------------------------------\n\nTITLE: Installing YOLOv5 Dependencies\nDESCRIPTION: Clone the YOLOv5 repository and install the required dependencies. This setup requires Python>=3.8.0 and PyTorch>=1.8.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/quickstart_tutorial.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ultralytics/yolov5 # clone repository\ncd yolov5\npip install -r requirements.txt # install dependencies\n```\n\n----------------------------------------\n\nTITLE: Configuring Optimizer Sweep with Comet YOLOv5 Shell\nDESCRIPTION: Configures a hyperparameter optimizer sweep using Comet with a predefined JSON configuration. The script 'hpo.py' runs the sweep utilizing the configurations specified in the JSON file and additional parameters.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/comet_logging_integration.md#2025-04-22_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\npython utils/loggers/comet/hpo.py \\\n  --comet_optimizer_config \"utils/loggers/comet/optimizer_config.json\"\n```\n\n----------------------------------------\n\nTITLE: Installing YOLOv7 Dependencies\nDESCRIPTION: Commands to navigate to the YOLOv7 directory and install required dependencies.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov7.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd yolov7\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running YOLOv8 Object Detection Inference with CUDA\nDESCRIPTION: Command to run YOLOv8 object detection inference using CUDA acceleration and custom image dimensions in Rust.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-ONNXRuntime-Rust/README.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncargo run --release -- --cuda --model ../assets/weights/yolov8m-dynamic.onnx --source ../assets/images/bus.jpg --plot --height 640 --width 480\n```\n\n----------------------------------------\n\nTITLE: Image Annotation with DeepSparse YOLO11\nDESCRIPTION: Command to annotate an image using a YOLO11 model deployed with DeepSparse.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/neural-magic.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndeepsparse.yolov8.annotate --source \"path/to/image.jpg\" --model_filepath \"path/to/yolo11n.onnx\"\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Exported RKNN Model via CLI\nDESCRIPTION: Command-line interface command to run inference with an exported RKNN model on an image. This shows how to perform object detection using the CLI on Rockchip devices.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/rockchip-rknn.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Run inference with the exported model\nyolo predict model='./yolo11n_rknn_model' source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Displaying DVC Plots in a Jupyter Notebook (Python)\nDESCRIPTION: Displays the HTML file containing plots generated by DVC (e.g., by `dvc plots diff`) directly within a Jupyter Notebook or similar environment. It uses the `IPython.display.HTML` class to render the content of the specified HTML file (`./dvc_plots/index.html`).\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/dvc.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import HTML\n\n# Display plots as HTML\nHTML(filename=\"./dvc_plots/index.html\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Swap Space on GCP VM\nDESCRIPTION: Commands for allocating and configuring swap space on the GCP VM to handle large datasets exceeding RAM capacity\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/environments/google_cloud_quickstart_tutorial.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Allocate a 64GB swap file\nsudo fallocate -l 64G /swapfile\n\n# Set the correct permissions for the swap file\nsudo chmod 600 /swapfile\n\n# Set up the Linux swap area\nsudo mkswap /swapfile\n\n# Enable the swap file\nsudo swapon /swapfile\n\n# Verify the swap space allocation (should show increased swap memory)\nfree -h\n```\n\n----------------------------------------\n\nTITLE: Plotting SQL Query Results with Ultralytics Explorer\nDESCRIPTION: Demonstrates how to plot the results of SQL queries using the plot_sql_query method in Ultralytics Explorer.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/api.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import Explorer\n\n# create an Explorer object\nexp = Explorer(data=\"coco128.yaml\", model=\"yolo11n.pt\")\nexp.create_embeddings_table()\n\n# plot the SQL Query\nexp.plot_sql_query(\"WHERE labels LIKE '%person%' AND labels LIKE '%dog%' LIMIT 10\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Triton Model Repository\nDESCRIPTION: Script to move the ONNX model and create configuration file with metadata and TensorRT optimization settings\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/triton-inference-server.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\n\n# Move ONNX model to Triton Model path\nPath(onnx_file).rename(triton_model_path / \"1\" / \"model.onnx\")\n\n# Create config file\n(triton_model_path / \"config.pbtxt\").touch()\n\ndata = \"\"\"\n# Add metadata\nparameters {\n  key: \"metadata\"\n  value {\n    string_value: \"%s\"\n  }\n}\n\n# (Optional) Enable TensorRT for GPU inference\n# First run will be slow due to TensorRT engine conversion\noptimization {\n  execution_accelerators {\n    gpu_execution_accelerator {\n      name: \"tensorrt\"\n      parameters {\n        key: \"precision_mode\"\n        value: \"FP16\"\n      }\n      parameters {\n        key: \"max_workspace_size_bytes\"\n        value: \"3221225472\"\n      }\n      parameters {\n        key: \"trt_engine_cache_enable\"\n        value: \"1\"\n      }\n      parameters {\n        key: \"trt_engine_cache_path\"\n        value: \"/models/yolo/1\"\n      }\n    }\n  }\n}\n\"\"\" % metadata[0]  # noqa\n\nwith open(triton_model_path / \"config.pbtxt\", \"w\") as f:\n    f.write(data)\n```\n\n----------------------------------------\n\nTITLE: Dataset Configuration YAML Structure\nDESCRIPTION: Example YAML configuration file structure for defining dataset paths and class names in Ultralytics YOLO. Specifies paths for training and validation image sets along with class definitions.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/index.md#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\npath: ../datasets/coco8-pose\ntrain: images/train\nval: images/val\nnames:\n    0: person\n```\n\n----------------------------------------\n\nTITLE: Benchmarking VNNI-Optimized Pruned-Quantized YOLOv5s with DeepSparse\nDESCRIPTION: Benchmark command for running pruned35_quant-none-vnni YOLOv5s model using DeepSparse with VNNI optimization, sync scenario, batch size 1, and single stream. Achieves 180 items/sec throughput.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/neural_magic_pruning_quantization.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ndeepsparse.benchmark zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned35_quant-none-vnni -s sync -b 1 -nstreams 1\n\n# Original Model Path: zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned35_quant-none-vnni\n# Batch Size: 1\n# Scenario: sync\n# Throughput (items/sec): 179.7375\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model with Python\nDESCRIPTION: Python code example showing how to train a YOLO11n-seg model on COCO-Seg dataset for 100 epochs with 640px image size\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/coco.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-seg.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"coco.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Exported CoreML Model using CLI\nDESCRIPTION: Command-line interface command to run inference on an image using an exported CoreML model with the Ultralytics CLI tool.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/coreml.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nyolo predict model=yolo11n.mlpackage source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API key for Ask AI feature\nDESCRIPTION: Command to set the OpenAI API key, which is required for the Ask AI feature in Ultralytics Explorer GUI.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/dashboard.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyolo settings openai_api_key=\"...\"\n```\n\n----------------------------------------\n\nTITLE: Verify Docker NVIDIA Runtime\nDESCRIPTION: Command to check if NVIDIA runtime is properly configured in Docker.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/environments/docker_image_quickstart_tutorial.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker info | grep -i runtime\n```\n\n----------------------------------------\n\nTITLE: Training YOLOv5 Models on COCO Dataset\nDESCRIPTION: Train YOLOv5 models of various sizes (n, s, m, l, x) on the COCO dataset. The commands show different configurations for epochs, weights, and batch sizes optimized for V100-16GB GPUs.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/quickstart_tutorial.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Train YOLOv5n on COCO128 for 3 epochs\npython train.py --data coco128.yaml --epochs 3 --weights yolov5n.pt --batch-size 128\n\n# Train YOLOv5s on COCO for 300 epochs\npython train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5s.yaml --batch-size 64\n\n# Train YOLOv5m on COCO for 300 epochs\npython train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5m.yaml --batch-size 40\n\n# Train YOLOv5l on COCO for 300 epochs\npython train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5l.yaml --batch-size 24\n\n# Train YOLOv5x on COCO for 300 epochs\npython train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5x.yaml --batch-size 16\n```\n\n----------------------------------------\n\nTITLE: Citing Segment Anything Model (SAM) in BibTeX\nDESCRIPTION: Provides a BibTeX citation format for referencing the Segment Anything model in academic work, highlighting the significance of SAM in computer vision.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/sam.md#2025-04-22_snippet_8\n\nLANGUAGE: BibTeX\nCODE:\n```\n@misc{kirillov2023segment,\n      title={Segment Anything},\n      author={Alexander Kirillov and Eric Mintun and Nikhila Ravi and Hanzi Mao and Chloe Rolland and Laura Gustafson and Tete Xiao and Spencer Whitehead and Alexander C. Berg and Wan-Yen Lo and Piotr Doll\\'ar and Ross Girshick},\n      year={2023},\n      eprint={2304.02643},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying DVC Plots in Jupyter Notebook\nDESCRIPTION: Python code to display DVC-generated HTML plots directly in a Jupyter Notebook using IPython's display functionality.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/dvc.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import HTML\n\n# Display the DVC plots as HTML\nHTML(filename=\"./dvc_plots/index.html\")\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model with CLI\nDESCRIPTION: Command-line interface command for training a YOLO11n-seg model on COCO-Seg dataset\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/coco.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo segment train data=coco.yaml model=yolo11n-seg.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on Caltech-256 using CLI\nDESCRIPTION: Command-line interface command to train a pretrained YOLO model on the Caltech-256 dataset for 100 epochs with 416px image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/caltech256.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo classify train data=caltech256 model=yolo11n-cls.pt epochs=100 imgsz=416\n```\n\n----------------------------------------\n\nTITLE: Pruning YOLOv5x Model to 30% Sparsity\nDESCRIPTION: Code snippet showing how to modify val.py to apply pruning to a YOLOv5x model with 30% sparsity (30% of weights set to zero). This demonstrates the implementation of the torch_utils.prune() function.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/model_pruning_and_sparsity.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Example code to apply pruning\nfrom utils.torch_utils import prune\n\n# Load the model\nmodel = attempt_load(weights, map_location=device)  # load FP32 model\n\n# Apply pruning with 0.3 (30%) sparsity\nprune(model, 0.3)  # prune model to 30% sparsity\n```\n\n----------------------------------------\n\nTITLE: Calculating Bounding Box Intersection Over Foreground (IOF) - Ultralytics DOTA Utilities - Python\nDESCRIPTION: Provides a function to compute the intersection-over-foreground (IOF) value between bounding boxes, a metric commonly used for dataset processing and filtering in object detection workflows. This function helps determine the overlap between given regions, essential for accurate annotation handling. Requires NumPy for array operations and expects bounding boxes as input arrays with coordinates; outputs a floating-point IOF matrix.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/data/split_dota.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef bbox_iof(bboxes_a, bboxes_b):\n    # Calculates the intersection over foreground between two sets of bounding boxes\n    # bboxes_a: (n, 4), bboxes_b: (k, 4)\n    pass  # Implementation computes intersection/foreground metrics\n```\n\n----------------------------------------\n\nTITLE: Installing Triton Client Dependencies\nDESCRIPTION: Command to install the Triton client Python package with all optional dependencies\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/triton-inference-server.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install tritonclient[all]\n```\n\n----------------------------------------\n\nTITLE: Configuring Crack Segmentation Dataset with YAML\nDESCRIPTION: The YAML configuration file for the Crack Segmentation dataset, defining dataset paths, classes, and other relevant information required for training YOLO models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/crack-seg.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/crack-seg.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Exporting and Using YOLO11 Models via Command Line\nDESCRIPTION: Command line instructions for exporting a YOLO11 PyTorch model to ONNX format and running inference with the exported model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/onnx.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Export a YOLO11n PyTorch model to ONNX format\nyolo export model=yolo11n.pt format=onnx # creates 'yolo11n.onnx'\n\n# Run inference with the exported model\nyolo predict model=yolo11n.onnx source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Single Model YOLOv5 Testing\nDESCRIPTION: Command to test a single YOLOv5x model on COCO val2017 dataset at 640px image size with half-precision.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/model_ensembling.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython val.py --weights yolov5x.pt --data coco.yaml --img 640 --half\n```\n\n----------------------------------------\n\nTITLE: Customizing Image Prediction Logging\nDESCRIPTION: Sets the maximum number of image predictions to log in Comet ML\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/comet.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"COMET_MAX_IMAGE_PREDICTIONS\"] = \"200\"\n```\n\n----------------------------------------\n\nTITLE: Viewing Available Solutions in Bash\nDESCRIPTION: Command to display help information about all available pre-built solutions in the Ultralytics YOLO CLI.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/cli.md#2025-04-22_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nyolo solutions help\n```\n\n----------------------------------------\n\nTITLE: Running YOLOv8 Instance Segmentation Inference with TensorRT and FP16\nDESCRIPTION: Command to run YOLOv8 instance segmentation inference using TensorRT acceleration with FP16 precision in Rust.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-ONNXRuntime-Rust/README.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ncargo run --release -- --trt --fp16 --model ../assets/weights/yolov8m-seg.onnx --source ../assets/images/0172.jpg --plot\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Models to RKNN Format via CLI\nDESCRIPTION: Command-line interface command to export a YOLO11n PyTorch model to RKNN format. The name parameter specifies the target Rockchip processor model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/rockchip-rknn.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Export a YOLO11n PyTorch model to RKNN format\n# 'name' can be one of rk3588, rk3576, rk3566, rk3568, rk3562, rv1103, rv1106, rv1103b, rv1106b, rk2118\nyolo export model=yolo11n.pt format=rknn name=rk3588 # creates '/yolo11n_rknn_model'\n```\n\n----------------------------------------\n\nTITLE: Running YOLOv5 Training with Comet Integration\nDESCRIPTION: Command to train YOLOv5s model on COCO128 dataset for 5 epochs, which will automatically log metrics to Comet.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/comet_logging_integration.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n# Train YOLOv5s on COCO128 for 5 epochs\npython train.py --img 640 --batch 16 --epochs 5 --data coco128.yaml --weights yolov5s.pt\n```\n\n----------------------------------------\n\nTITLE: Making API Request for YOLO Detection Model Using cURL\nDESCRIPTION: This cURL command sends a POST request to the Ultralytics API for running inference with a YOLO detection model. It includes the API key, model ID, image file, and inference parameters.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/hub/inference-api.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \"https://predict.ultralytics.com\" \\\n  -H \"x-api-key: API_KEY\" \\\n  -F \"model=https://hub.ultralytics.com/models/MODEL_ID\" \\\n  -F \"file=@/path/to/image.jpg\" \\\n  -F \"imgsz=640\" \\\n  -F \"conf=0.25\" \\\n  -F \"iou=0.45\"\n```\n\n----------------------------------------\n\nTITLE: Downloading Sample Image\nDESCRIPTION: Downloads a sample image for testing YOLOv5 object detection.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/neural_magic_pruning_quantization.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nwget -O basilica.jpg https://raw.githubusercontent.com/neuralmagic/deepsparse/main/src/deepsparse/yolo/sample_images/basilica.jpg\n```\n\n----------------------------------------\n\nTITLE: Custom DetectionTrainer Implementation\nDESCRIPTION: Shows how to create a custom trainer by extending the DetectionTrainer class and overriding the get_model method.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/engine.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.models.yolo.detect import DetectionTrainer\n\n\nclass CustomTrainer(DetectionTrainer):\n    def get_model(self, cfg, weights):\n        \"\"\"Loads a custom detection model given configuration and weight files.\"\"\"\n        ...\n\n\ntrainer = CustomTrainer(overrides={...})\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Initializing K-Fold Split for Dataset in Python\nDESCRIPTION: This snippet uses the KFold class from sklearn.model_selection to generate k splits of the dataset. It sets shuffle=True for randomized distribution and random_state for reproducibility.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/kfold-cross-validation.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport random\n\nfrom sklearn.model_selection import KFold\n\nrandom.seed(0)  # for reproducibility\nksplit = 5\nkf = KFold(n_splits=ksplit, shuffle=True, random_state=20)  # setting random_state for repeatable results\n\nkfolds = list(kf.split(labels_df))\n```\n\n----------------------------------------\n\nTITLE: COCO to YOLO Format Conversion\nDESCRIPTION: Python code example showing how to convert COCO format labels to YOLO format for pose estimation\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/index.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.data.converter import convert_coco\n\nconvert_coco(labels_dir=\"path/to/coco/annotations/\", use_keypoints=True)\n```\n\n----------------------------------------\n\nTITLE: Exporting and Using YOLO11 Model in PaddlePaddle Format (CLI)\nDESCRIPTION: This snippet shows how to export a YOLO11 model to PaddlePaddle format and run inference using the command-line interface. It uses the yolo command for both exporting and prediction.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/paddlepaddle.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Export a YOLO11n PyTorch model to PaddlePaddle format\nyolo export model=yolo11n.pt format=paddle # creates '/yolo11n_paddle_model'\n\n# Run inference with the exported model\nyolo predict model='./yolo11n_paddle_model' source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Setting Up ClearML for YOLO11 Model Training Tracking\nDESCRIPTION: Initializes ClearML for experiment tracking when training YOLO11 models. This snippet installs the clearML package and performs browser-based authentication, requiring users to sign in via browser to authenticate their session.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/train.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# pip install clearml\nimport clearml\n\nclearml.browser_login()\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLOv8 Model to ONNX Format via CLI\nDESCRIPTION: This bash command demonstrates how to export a YOLOv8 model to ONNX format using the Ultralytics YOLO command-line interface. It specifies the model file, export format, and various parameters for the export process.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-ONNXRuntime-CPP/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Export the model using the command line\nyolo export model=yolov8n.pt format=onnx opset=12 simplify=True dynamic=False imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Installing Gradio for Ultralytics YOLO11 Integration\nDESCRIPTION: Simple pip command to install Gradio, which is required for creating the interactive object detection interface with Ultralytics YOLO11.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/gradio.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install gradio\n```\n\n----------------------------------------\n\nTITLE: Installing YOLOv8 ONNX Runtime Dependencies\nDESCRIPTION: Commands to clone the repository and install required dependencies from requirements.txt file.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-ONNXRuntime/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ultralytics/ultralytics.git\ncd ultralytics/examples/YOLOv8-ONNXRuntime\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Configuring YOLOv5 Inference Settings\nDESCRIPTION: Shows how to configure various inference parameters for YOLOv5 models such as confidence threshold, IoU threshold, class filtering, and other detection settings to customize the model's behavior during inference.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/pytorch_hub_model_loading.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel.conf = 0.25  # NMS confidence threshold\nmodel.iou = 0.45  # NMS IoU threshold\nmodel.agnostic = False  # NMS class-agnostic\nmodel.multi_label = False  # NMS multiple labels per box\nmodel.classes = None  # (optional list) filter by class, i.e. = [0, 15, 16] for COCO persons, cats and dogs\nmodel.max_det = 1000  # maximum number of detections per image\nmodel.amp = False  # Automatic Mixed Precision (AMP) inference\n\nresults = model(im, size=320)  # custom inference size\n```\n\n----------------------------------------\n\nTITLE: Semantic Search Using Images in Ultralytics Explorer\nDESCRIPTION: Shows how to perform semantic search using images with the Ultralytics Explorer API, including searching with multiple images.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/api.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import Explorer\n\n# create an Explorer object\nexp = Explorer(data=\"coco128.yaml\", model=\"yolo11n.pt\")\nexp.create_embeddings_table()\n\nsimilar = exp.get_similar(img=\"https://ultralytics.com/images/bus.jpg\", limit=10)\nprint(similar.head())\n\n# Search using multiple indices\nsimilar = exp.get_similar(\n    img=[\"https://ultralytics.com/images/bus.jpg\", \"https://ultralytics.com/images/bus.jpg\"],\n    limit=10,\n)\nprint(similar.head())\n```\n\n----------------------------------------\n\nTITLE: Dataset Citation in BibTeX Format\nDESCRIPTION: BibTeX citation for the Global Wheat Head Dataset research paper\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/globalwheat2020.md#2025-04-22_snippet_3\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{david2020global,\n         title={Global Wheat Head Detection (GWHD) Dataset: A Large and Diverse Dataset of High-Resolution RGB-Labelled Images to Develop and Benchmark Wheat Head Detection Methods},\n         author={David, Etienne and Madec, Simon and Sadeghi-Tehran, Pouria and Aasen, Helge and Zheng, Bangyou and Liu, Shouyang and Kirchgessner, Norbert and Ishikawa, Goro and Nagasawa, Koichi and Badhon, Minhajul and others},\n         journal={arXiv preprint arXiv:2005.02162},\n         year={2020}\n}\n```\n\n----------------------------------------\n\nTITLE: Cloning DeepStream-Yolo Repository\nDESCRIPTION: Commands to clone the DeepStream-Yolo repository for NVIDIA DeepStream SDK support\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/deepstream-nvidia-jetson.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd ~\ngit clone https://github.com/marcoslucianops/DeepStream-Yolo\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLOv5 Model to ONNX Format in AzureML\nDESCRIPTION: This snippet demonstrates how to export a YOLOv5s model to ONNX format. It specifies the model weights, export format, and image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/environments/azureml_quickstart_tutorial.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Export yolov5s to ONNX format\npython export.py --weights yolov5s.pt --include onnx --img 640\n```\n\n----------------------------------------\n\nTITLE: Generating Comparative DVC Plots (Bash)\nDESCRIPTION: Uses the DVC command-line interface to generate comparative plots showing differences across various tracked experiments. The `dvc exp list --names-only` command provides the names of all experiments to compare. This command typically generates HTML files containing the plots.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/dvc.md#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ndvc plots diff $(dvc exp list --names-only)\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration for Pose Dataset\nDESCRIPTION: Example YAML configuration for defining a pose estimation dataset, including paths, keypoint shape, flip indices and class names\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/index.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\npath: ../datasets/coco8-pose\ntrain: images/train\nval: images/val\ntest:\n\nkpt_shape: [17, 3]\nflip_idx: [0, 2, 1, 4, 3, 6, 5, 8, 7, 10, 9, 12, 11, 14, 13, 16, 15]\n\nnames:\n    0: person\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model to CoreML Format using CLI\nDESCRIPTION: Command-line interface command to export a YOLO11 model to CoreML format using the Ultralytics CLI tool.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/coreml.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=yolo11n.pt format=coreml\n```\n\n----------------------------------------\n\nTITLE: Referencing the NAS Model Class Definition in Python\nDESCRIPTION: This reference points to the definition of the `NAS` class within the `ultralytics.models.nas.model` module. The `NAS` class provides the core implementation for YOLO-NAS models in the Ultralytics framework, handling model loading, inference, and training.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/models/nas/model.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n::: ultralytics.models.nas.model.NAS\n```\n\n----------------------------------------\n\nTITLE: COCO Dataset Citation\nDESCRIPTION: BibTeX citation for the original COCO dataset paper\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/coco.md#2025-04-22_snippet_3\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{lin2015microsoft,\n      title={Microsoft COCO: Common Objects in Context},\n      author={Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Dollár},\n      year={2015},\n      eprint={1405.0312},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search with Explorer API\nDESCRIPTION: Example of using Explorer API to perform similarity search on images using embeddings.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/api.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import Explorer\n\n# Create an Explorer object\nexplorer = Explorer(data=\"coco128.yaml\", model=\"yolo11n.pt\")\nexplorer.create_embeddings_table()\n\n# Search for similar images to a given image\nsimilar_images_df = explorer.get_similar(img=\"path/to/image.jpg\")\nprint(similar_images_df.head())\n```\n\n----------------------------------------\n\nTITLE: YOLOv3 Academic Citation\nDESCRIPTION: This BibTeX entry provides the academic citation format for YOLOv3, offering details for authors and publications that use this model in their research.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov3.md#2025-04-22_snippet_2\n\nLANGUAGE: BibTeX\nCODE:\n```\n@article{redmon2018yolov3,\n  title={YOLOv3: An Incremental Improvement},\n  author={Redmon, Joseph and Farhadi, Ali},\n  journal={arXiv preprint arXiv:1804.02767},\n  year={2018}\n}\n```\n\n----------------------------------------\n\nTITLE: Installing YOLO11 and ClearML Dependencies\nDESCRIPTION: Command to install the required Python packages ultralytics and clearml via pip\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/clearml.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics clearml\n```\n\n----------------------------------------\n\nTITLE: Training YOLOv5 with SyncBatchNorm\nDESCRIPTION: Command to train YOLOv5 using DistributedDataParallel mode with SyncBatchNorm. This can increase accuracy for multi-GPU training but may slow down the process.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/multi_gpu_training.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython -m torch.distributed.run --nproc_per_node 2 train.py --batch 64 --data coco.yaml --cfg yolov5s.yaml --weights '' --sync-bn\n```\n\n----------------------------------------\n\nTITLE: Setting Up PiCamera2 for IMX500 Deployment\nDESCRIPTION: Clones the picamera2 repository, installs it, and navigates to the IMX500 examples directory for deployment preparation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/sony-imx500.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/raspberrypi/picamera2\ncd picamera2\npip install -e . --break-system-packages\ncd examples/imx500\n```\n\n----------------------------------------\n\nTITLE: Configuring YOLO Tracker Parameters via Command Line\nDESCRIPTION: This code snippet shows how to configure tracking parameters such as confidence threshold, IOU threshold, and result display when using the YOLO tracker from the command line interface.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/trackers/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Configure tracking parameters and run the tracker using the command line interface\nyolo track model=yolo11n.pt source=\"https://youtu.be/LNwODJXcvt4\" conf=0.3 iou=0.5 show\n```\n\n----------------------------------------\n\nTITLE: Defining Bboxes Class for Bounding Box Operations in Python\nDESCRIPTION: The Bboxes class is likely defined to handle various operations on bounding boxes, including format conversions (e.g., xyxy, xywh, ltwh) and other utility functions for object detection tasks.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/instance.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nclass Bboxes:\n```\n\n----------------------------------------\n\nTITLE: YOLO Training with Data Augmentation via CLI\nDESCRIPTION: Command line interface example for training YOLO with custom augmentation parameters using the CLI approach.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/yolo-data-augmentation.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyolo detect train data=coco8.yaml model=yolo11n.pt epochs=100 hsv_h=0.03 hsv_s=0.6 hsv_v=0.5\n```\n\n----------------------------------------\n\nTITLE: Using Queue Management Solution in Bash\nDESCRIPTION: Commands to use the pre-built queue management solution with optional video source and region specification.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/cli.md#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nyolo solutions queue show=True\nyolo solutions queue source=\"path/to/video.mp4\"                                # specify video file path\nyolo solutions queue region=\"[(20, 400), (1080, 400), (1080, 360), (20, 360)]\" # configure queue coordinates\n```\n\n----------------------------------------\n\nTITLE: Running YOLOv8 Inference with Specific GPU and Batch Size\nDESCRIPTION: Command to run YOLOv8 inference on a specific GPU device with a custom batch size in Rust.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-ONNXRuntime-Rust/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncargo run --release -- --cuda --device_id 0 --batch 2 --model MODEL_PATH.onnx --source SOURCE_IMAGE.jpg\n```\n\n----------------------------------------\n\nTITLE: Using Workout Monitoring Solution in Bash\nDESCRIPTION: Commands to use the pre-built workout monitoring solution with optional video source and keypoint specifications.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/cli.md#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nyolo solutions workout show=True\nyolo solutions workout source=\"path/to/video.mp4\" # specify video file path\n\n# Use keypoints for ab-workouts\nyolo solutions workout kpts=[5, 11, 13] # left side\nyolo solutions workout kpts=[6, 12, 14] # right side\n```\n\n----------------------------------------\n\nTITLE: Validating YOLO11 Segmentation Model\nDESCRIPTION: Examples of validating a trained YOLO11 segmentation model using both Python and CLI methods. Shows how to access various evaluation metrics for both bounding boxes and segmentation masks.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/segment.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n-seg.pt\")  # load an official model\nmodel = YOLO(\"path/to/best.pt\")  # load a custom model\n\n# Validate the model\nmetrics = model.val()  # no arguments needed, dataset and settings remembered\nmetrics.box.map  # map50-95(B)\nmetrics.box.map50  # map50(B)\nmetrics.box.map75  # map75(B)\nmetrics.box.maps  # a list contains map50-95(B) of each category\nmetrics.seg.map  # map50-95(M)\nmetrics.seg.map50  # map50(M)\nmetrics.seg.map75  # map75(M)\nmetrics.seg.maps  # a list contains map50-95(M) of each category\n```\n\nLANGUAGE: bash\nCODE:\n```\nyolo segment val model=yolo11n-seg.pt  # val official model\nyolo segment val model=path/to/best.pt # val custom model\n```\n\n----------------------------------------\n\nTITLE: Running YOLO Prediction with GUI in Ultralytics Docker Container\nDESCRIPTION: This command runs a YOLO prediction inside the Docker container, displaying the results graphically.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/docker-quickstart.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nyolo predict model=yolo11n.pt show=True\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n on Signature Detection Dataset using CLI\nDESCRIPTION: Command-line interface command for training a YOLO11n model on the signature detection dataset with the same parameters as the Python example. This provides an alternative approach for users who prefer CLI.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/signature.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo detect train data=signature.yaml model=yolo11n.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package via CLI\nDESCRIPTION: Command to install the required Ultralytics package for YOLO11 model conversion using pip.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tflite.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics\n```\n\n----------------------------------------\n\nTITLE: Setting Up Export Script\nDESCRIPTION: Commands to copy the YOLO11 export script and download model weights\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/deepstream-nvidia-jetson.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncp ~/DeepStream-Yolo/utils/export_yolo11.py ~/ultralytics\ncd ultralytics\nwget https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s.pt\n```\n\n----------------------------------------\n\nTITLE: Publishing Detected Classes Using String Messages\nDESCRIPTION: Demonstrates how to publish detected object classes as string messages, providing a lightweight alternative to full image publishing for applications like inventory tracking.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/ros-quickstart.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nimport ros_numpy\nimport rospy\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\n\nfrom ultralytics import YOLO\n\ndetection_model = YOLO(\"yolo11m.pt\")\nrospy.init_node(\"ultralytics\")\ntime.sleep(1)\nclasses_pub = rospy.Publisher(\"/ultralytics/detection/classes\", String, queue_size=5)\n\n\ndef callback(data):\n    \"\"\"Callback function to process image and publish detected classes.\"\"\"\n    array = ros_numpy.numpify(data)\n    if classes_pub.get_num_connections():\n        det_result = detection_model(array)\n        classes = det_result[0].boxes.cls.cpu().numpy().astype(int)\n        names = [det_result[0].names[i] for i in classes]\n        classes_pub.publish(String(data=str(names)))\n\n\nrospy.Subscriber(\"/camera/color/image_raw\", Image, callback)\nwhile True:\n    rospy.spin()\n```\n\n----------------------------------------\n\nTITLE: Running Triton Inference Server with Docker\nDESCRIPTION: Code to launch Triton server in a Docker container and wait for model initialization\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/triton-inference-server.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport contextlib\nimport subprocess\nimport time\n\nfrom tritonclient.http import InferenceServerClient\n\n# Define image https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver\ntag = \"nvcr.io/nvidia/tritonserver:24.09-py3\"  # 8.57 GB\n\n# Pull the image\nsubprocess.call(f\"docker pull {tag}\", shell=True)\n\n# Run the Triton server and capture the container ID\ncontainer_id = (\n    subprocess.check_output(\n        f\"docker run -d --rm --gpus 0 -v {triton_repo_path}:/models -p 8000:8000 {tag} tritonserver --model-repository=/models\",\n        shell=True,\n    )\n    .decode(\"utf-8\")\n    .strip()\n)\n\n# Wait for the Triton server to start\ntriton_client = InferenceServerClient(url=\"localhost:8000\", verbose=False, ssl=False)\n\n# Wait until model is ready\nfor _ in range(10):\n    with contextlib.suppress(Exception):\n        assert triton_client.is_model_ready(model_name)\n        break\n    time.sleep(1)\n```\n\n----------------------------------------\n\nTITLE: Setting Up MNN Libraries and Headers\nDESCRIPTION: Commands for copying MNN libraries and header files to the project directory\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-MNN-CPP/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p libs include\n\ncp MNN/build/libMNN.a libs/\ncp MNN/build/express/libMNN_Express.a libs/\ncp MNN/build/tools/cv/libMNNOpenCV.a libs/\n\ncp -r MNN/include .\ncp -r MNN/tools/cv/include .\n```\n\n----------------------------------------\n\nTITLE: Training Example in Python for FAQ Section\nDESCRIPTION: Python code snippet in the FAQ section demonstrating how to load a pre-trained YOLO11n model and train it on the COCO128 dataset for 100 epochs with an image size of 640.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/coco128.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Train the model\nresults = model.train(data=\"coco128.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Setting INT8 Calibration Environment Variables\nDESCRIPTION: Sets environment variables for INT8 calibration process including image path and batch size\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/deepstream-nvidia-jetson.md#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nexport INT8_CALIB_IMG_PATH=calibration.txt\nexport INT8_CALIB_BATCH_SIZE=1\n```\n\n----------------------------------------\n\nTITLE: Installing DeepSparse for YOLO11 Deployment\nDESCRIPTION: Command to install the required packages for deploying YOLO11 with Neural Magic's DeepSparse using pip.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/neural-magic.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install deepsparse[yolov8]\n```\n\n----------------------------------------\n\nTITLE: Running Ultralytics Docker Container on Raspberry Pi\nDESCRIPTION: Command to pull and run the Ultralytics Docker container optimized for Raspberry Pi's ARM64 architecture.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/raspberry-pi.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nt=ultralytics/ultralytics:latest-arm64 && sudo docker pull $t && sudo docker run -it --ipc=host $t\n```\n\n----------------------------------------\n\nTITLE: Installing/Updating Ultralytics Package using pip (Bash)\nDESCRIPTION: Installs or upgrades the `ultralytics` Python package to the latest version using the pip package manager. This command is essential for setting up the environment to use YOLOE and other models within the Ultralytics framework.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yoloe.md#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\npip install -U ultralytics\n```\n\n----------------------------------------\n\nTITLE: Starting TCP Stream for Raspberry Pi Camera\nDESCRIPTION: This command initiates a TCP stream from the connected Raspberry Pi camera, which can be used as an input for YOLO11 inference.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/raspberry-pi.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nrpicam-vid -n -t 0 --inline --listen -o tcp://127.0.0.1:8888\n```\n\n----------------------------------------\n\nTITLE: Executing Instance Segmentation via CLI with Ultralytics YOLO11\nDESCRIPTION: This snippet demonstrates how to use the command line interface to perform instance segmentation using Ultralytics YOLO11. It includes options for showing results, specifying a video source, and monitoring specific classes.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/instance-segmentation-and-tracking.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Instance segmentation using Ultralytics YOLO11\nyolo solutions isegment show=True\n\n# Pass a source video\nyolo solutions isegment source=\"path/to/video.mp4\"\n\n# Monitor the specific classes\nyolo solutions isegment classes=\"[0, 5]\"\n```\n\n----------------------------------------\n\nTITLE: Running Special YOLO Commands in Bash\nDESCRIPTION: Various utility commands for viewing YOLO version information, checking system compatibility, viewing settings, and accessing configuration.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/cli.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nyolo help\nyolo checks\nyolo version\nyolo settings\nyolo copy-cfg\nyolo cfg\n```\n\n----------------------------------------\n\nTITLE: Referencing HUBTrainingSession Class Documentation in Python/MkDocs\nDESCRIPTION: This directive uses MkDocs with a plugin like `mkdocstrings` to automatically fetch and display documentation for the `HUBTrainingSession` class located within the `ultralytics.hub.session` Python module. The class itself manages Ultralytics YOLO model training sessions, including interaction with the Ultralytics HUB, heartbeats, and checkpointing.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/hub/session.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n::: ultralytics.hub.session.HUBTrainingSession\n```\n\n----------------------------------------\n\nTITLE: Configuring Mosaic Augmentation in Ultralytics\nDESCRIPTION: The Mosaic augmentation combines four training images into one. It's highly effective for improving small object detection and context understanding. The probability of applying this transformation is controlled by the 'mosaic' parameter.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/yolo-data-augmentation.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Example usage in Ultralytics configuration\nmodel = YOLO('yolov8n.yaml')\nmodel.train(data='coco128.yaml', epochs=100, mosaic=0.5)\n```\n\n----------------------------------------\n\nTITLE: Cloning YOLO11 SageMaker Repository in Bash\nDESCRIPTION: Commands to clone the AWS repository containing resources for deploying YOLO11 on SageMaker and navigate to the project directory.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/amazon-sagemaker.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/aws-samples/host-yolov8-on-sagemaker-endpoint.git\ncd host-yolov8-on-sagemaker-endpoint/yolov8-pytorch-cdk\n```\n\n----------------------------------------\n\nTITLE: Setting up OpenCV Dependencies\nDESCRIPTION: Configures OpenCV library paths and includes required dependencies with status messages for debugging.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-LibTorch-CPP-Inference/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(OpenCV_DIR \"/path/to/opencv/lib/cmake/opencv4\")\nfind_package(OpenCV REQUIRED)\n\nmessage(STATUS \"OpenCV library status:\")\nmessage(STATUS \"    config: ${OpenCV_DIR}\")\nmessage(STATUS \"    version: ${OpenCV_VERSION}\")\nmessage(STATUS \"    libraries: ${OpenCV_LIBS}\")\nmessage(STATUS \"    include path: ${OpenCV_INCLUDE_DIRS}\")\n\ninclude_directories(${OpenCV_INCLUDE_DIRS})\n```\n\n----------------------------------------\n\nTITLE: Pulling Ultralytics Docker Image in Bash\nDESCRIPTION: These commands set the Ultralytics Docker image name as a variable and pull the latest image from Docker Hub.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/docker-quickstart.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Set image name as a variable\nt=ultralytics/ultralytics:latest\n\n# Pull the latest Ultralytics image from Docker Hub\nsudo docker pull $t\n```\n\n----------------------------------------\n\nTITLE: Displaying Help Information for YOLO Rust Demo\nDESCRIPTION: Command to display all available options and parameters for the YOLO ONNXRuntime Rust demo implementation. This helps users understand the full range of configurable options.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLO-Series-ONNXRuntime-Rust/README.md#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ncargo run -- --help\n```\n\n----------------------------------------\n\nTITLE: YOLO Segmentation Dataset Format Example\nDESCRIPTION: Example of the YOLO dataset format for a single image with two objects made up of a 3-point segment and a 5-point segment.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/index.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n0 0.681 0.485 0.670 0.487 0.676 0.487\n1 0.504 0.000 0.501 0.004 0.498 0.004 0.493 0.010 0.492 0.0104\n```\n\n----------------------------------------\n\nTITLE: Comparing SPP and SPPF Performance in Python\nDESCRIPTION: This code snippet demonstrates the speed difference between SPP (Spatial Pyramid Pooling) and SPPF (Spatial Pyramid Pooling - Fast) structures used in YOLOv5. It defines both modules, runs them on a random tensor, and compares their execution times.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/architecture_description.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nimport torch\nimport torch.nn as nn\n\n\nclass SPP(nn.Module):\n    def __init__(self):\n        \"\"\"Initializes an SPP module with three different sizes of max pooling layers.\"\"\"\n        super().__init__()\n        self.maxpool1 = nn.MaxPool2d(5, 1, padding=2)\n        self.maxpool2 = nn.MaxPool2d(9, 1, padding=4)\n        self.maxpool3 = nn.MaxPool2d(13, 1, padding=6)\n\n    def forward(self, x):\n        \"\"\"Applies three max pooling layers on input `x` and concatenates results along channel dimension.\"\"\"\n        o1 = self.maxpool1(x)\n        o2 = self.maxpool2(x)\n        o3 = self.maxpool3(x)\n        return torch.cat([x, o1, o2, o3], dim=1)\n\n\nclass SPPF(nn.Module):\n    def __init__(self):\n        \"\"\"Initializes an SPPF module with a specific configuration of MaxPool2d layer.\"\"\"\n        super().__init__()\n        self.maxpool = nn.MaxPool2d(5, 1, padding=2)\n\n    def forward(self, x):\n        \"\"\"Applies sequential max pooling and concatenates results with input tensor.\"\"\"\n        o1 = self.maxpool(x)\n        o2 = self.maxpool(o1)\n        o3 = self.maxpool(o2)\n        return torch.cat([x, o1, o2, o3], dim=1)\n\n\ndef main():\n    \"\"\"Compares outputs and performance of SPP and SPPF on a random tensor (8, 32, 16, 16).\"\"\"\n    input_tensor = torch.rand(8, 32, 16, 16)\n    spp = SPP()\n    sppf = SPPF()\n    output1 = spp(input_tensor)\n    output2 = sppf(input_tensor)\n\n    print(torch.equal(output1, output2))\n\n    t_start = time.time()\n    for _ in range(100):\n        spp(input_tensor)\n    print(f\"SPP time: {time.time() - t_start}\")\n\n    t_start = time.time()\n    for _ in range(100):\n        sppf(input_tensor)\n    print(f\"SPPF time: {time.time() - t_start}\")\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Using make_divisible Function in Ultralytics\nDESCRIPTION: This code demonstrates the make_divisible utility function from Ultralytics, which calculates the nearest whole number to x that is evenly divisible by y. This is particularly useful in neural network architecture design where dimensions often need to be multiples of specific values.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/simple-utilities.md#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.utils.ops import make_divisible\n\nmake_divisible(7, 3)\n# >>> 9\nmake_divisible(7, 2)\n# >>> 8\n```\n\n----------------------------------------\n\nTITLE: Running YOLOv8 Detection Script with OpenCV and ONNX\nDESCRIPTION: This command executes the main Python script for object detection, specifying the ONNX model path and input image.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-OpenCV-ONNX-Python/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython main.py --model yolov8n.onnx --img image.jpg\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package for YOLO11\nDESCRIPTION: Command to install the required Ultralytics package for working with YOLO11 models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/paddlepaddle.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics\n```\n\n----------------------------------------\n\nTITLE: Sorting YOLOv5 Detection Results\nDESCRIPTION: Shows how to sort YOLOv5 detection results by column values, such as sorting license plate digits from left to right based on the x-coordinate. This is useful for sequence-dependent detection applications.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/pytorch_hub_model_loading.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nresults = model(im)  # inference\nresults.pandas().xyxy[0].sort_values(\"xmin\")  # sorted left-right\n```\n\n----------------------------------------\n\nTITLE: Run YOLOv5 Docker Container with CPU\nDESCRIPTION: Command to run YOLOv5 Docker container using CPU only.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/environments/docker_image_quickstart_tutorial.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Run an interactive container instance using CPU\nsudo docker run -it --ipc=host $t\n```\n\n----------------------------------------\n\nTITLE: Creating Custom IPython Kernel for YOLOv5 in AzureML\nDESCRIPTION: This code snippet shows how to create a custom IPython kernel linked to the YOLOv5 Conda environment. This allows running YOLOv5 commands in AzureML notebooks.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/environments/azureml_quickstart_tutorial.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# Ensure your Conda environment is active\n# conda activate yolov5env\n\n# Install ipykernel if not already present\nconda install ipykernel -y\n\n# Create a new kernel linked to your environment\npython -m ipykernel install --user --name yolov5env --display-name \"Python (yolov5env)\"\n```\n\n----------------------------------------\n\nTITLE: Managing Device Selection for YOLOv5 Models\nDESCRIPTION: Demonstrates how to move YOLOv5 models between different devices (CPU/GPU) after creation, and how to specify the device when first loading the model. This enables flexibility for deployment on different hardware configurations.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/pytorch_hub_model_loading.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel.cpu()  # CPU\nmodel.cuda()  # GPU\nmodel.to(device)  # i.e. device=torch.device(0)\n```\n\n----------------------------------------\n\nTITLE: Training YOLO-World Model with Python\nDESCRIPTION: Shows how to train a YOLO-World model using the Python API. Demonstrates loading a pretrained model and training it on the COCO8 dataset.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolo-world.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLOWorld\n\n# Load a pretrained YOLOv8s-worldv2 model\nmodel = YOLOWorld(\"yolov8s-worldv2.pt\")\n\n# Train the model on the COCO8 dataset for 100 epochs\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: YOLO CLI Image Prediction\nDESCRIPTION: Command line interface usage for running predictions using a pre-trained YOLO model on an image\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/README.zh-CN.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyolo predict model=yolo11n.pt source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on ImageNette Dataset using CLI\nDESCRIPTION: This command-line instruction shows how to train a YOLO model for classification on the ImageNette dataset using the CLI interface, starting from a pretrained model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/imagenette.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo classify train data=imagenette model=yolo11n-cls.pt epochs=100 imgsz=224\n```\n\n----------------------------------------\n\nTITLE: Exporting and Running YOLO11 Model via CLI\nDESCRIPTION: CLI commands to export a YOLO11 model to Edge TPU format and run inference\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/edge-tpu.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Export a YOLO11n PyTorch model to TFLite Edge TPU format\nyolo export model=yolo11n.pt format=edgetpu # creates 'yolo11n_full_integer_quant_edgetpu.tflite'\n\n# Run inference with the exported model\nyolo predict model=yolo11n_full_integer_quant_edgetpu.tflite source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: HTTP Client Request Implementation\nDESCRIPTION: Example of sending requests to DeepSparse server using Python requests library.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/neural_magic_pruning_quantization.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nimport requests\n\n# list of images for inference (local files on client side)\npath = [\"basilica.jpg\"]\nfiles = [(\"request\", open(img, \"rb\")) for img in path]\n\n# send request over HTTP to /predict/from_files endpoint\nurl = \"http://0.0.0.0:5543/predict/from_files\"\nresp = requests.post(url=url, files=files)\n\n# response is returned in JSON\nannotations = json.loads(resp.text)  # dictionary of annotation results\nbounding_boxes = annotations[\"boxes\"]\nlabels = annotations[\"labels\"]\n```\n\n----------------------------------------\n\nTITLE: OBBPredictor Class Import Reference for Oriented Bounding Box Detection in Python\nDESCRIPTION: This code snippet references the OBBPredictor class from the ultralytics.models.yolo.obb.predict module. The OBBPredictor is specialized for making predictions with oriented bounding boxes in the YOLO framework.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/models/yolo/obb/predict.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.models.yolo.obb.predict import OBBPredictor\n```\n\n----------------------------------------\n\nTITLE: Validating Official YOLO Model in Bash\nDESCRIPTION: Command to validate the accuracy of an official pre-trained YOLO11n model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/cli.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nyolo detect val model=yolo11n.pt\n```\n\n----------------------------------------\n\nTITLE: Executing Region Counting Script\nDESCRIPTION: Command to run the region-based object counting script with video input\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/region-counting.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython yolov8_region_counter.py --source \"path/to/video.mp4\" --save-img\n```\n\n----------------------------------------\n\nTITLE: Batch Conversion of Polygon Annotations with Overlap Support - Python\nDESCRIPTION: The polygons2masks_overlap utility expands mask generation capabilities by supporting overlapping polygon annotations, producing a single mask with overlapping regions marked accordingly. Expects input as multiple polygons and outputs a composite mask; constraints include proper management of overlapping classes or priorities.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/data/utils.md#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n## ::: ultralytics.data.utils.polygons2masks_overlap\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics and Ray Tune Packages via Command Line\nDESCRIPTION: Commands to install the required Ultralytics and Ray Tune packages through pip, with an optional command for installing Weights & Biases for logging.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/ray-tune.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install and update Ultralytics and Ray Tune packages\npip install -U ultralytics \"ray[tune]\"\n\n# Optionally install W&B for logging\npip install wandb\n```\n\n----------------------------------------\n\nTITLE: Running YOLOv8 Pose Detection Inference with TensorRT\nDESCRIPTION: Command to run YOLOv8 pose detection inference using TensorRT acceleration in Rust.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-ONNXRuntime-Rust/README.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ncargo run --release -- --trt --model ../assets/weights/yolov8m-pose.onnx --source ../assets/images/bus.jpg --plot\n```\n\n----------------------------------------\n\nTITLE: Caching Dataset for Faster YOLOv5 Training (Shell)\nDESCRIPTION: This snippet illustrates how to use the '--cache' flag to cache the dataset on disk or in memory for subsequent epochs, thereby reducing data loading times and accelerating training. The parameter '--cache' can be used alone or with a value such as 'ram' or 'disk'. It requires that the dataset fits within the available disk or RAM; otherwise, it may lead to storage issues.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/tips_for_best_training_results.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n--cache\n```\n\n----------------------------------------\n\nTITLE: Installing ONNX Runtime CPU Package\nDESCRIPTION: Installs the CPU-only version of ONNX Runtime for systems without GPU or preferring CPU-based inference.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/RTDETR-ONNXRuntime-Python/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install onnxruntime\n```\n\n----------------------------------------\n\nTITLE: Configuring Batch Logging Interval\nDESCRIPTION: Controls how frequently batches of image predictions are logged to Comet ML\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/comet.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"COMET_EVAL_BATCH_LOGGING_INTERVAL\"] = \"4\"\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package for YOLOv8 Model Export\nDESCRIPTION: This command installs the latest ultralytics package, which is required for exporting YOLOv8 models to ONNX format.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-OpenCV-ONNX-Python/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics\n```\n\n----------------------------------------\n\nTITLE: Exploring Bounding Box Conversions with Ultralytics - Python\nDESCRIPTION: This snippet showcases several bounding box conversion functions from the `ultralytics.utils.ops` module. It lists different conversion methods for bounding boxes and allows users to print out the docstrings for these functions to understand their usage. Dependencies are not specifically required for this example.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/simple-utilities.md#2025-04-22_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics.utils.ops import (\n    ltwh2xywh,\n    ltwh2xyxy,\n    xywh2ltwh,  # xywh → top-left corner, w, h\n    xywh2xyxy,\n    xywhn2xyxy,  # normalized → pixel\n    xyxy2ltwh,  # xyxy → top-left corner, w, h\n    xyxy2xywhn,  # pixel → normalized\n)\n\nfor func in (ltwh2xywh, ltwh2xyxy, xywh2ltwh, xywh2xyxy, xywhn2xyxy, xyxy2ltwh, xyxy2xywhn):\n    print(help(func))  # print function docstrings\n```\n\n----------------------------------------\n\nTITLE: Splitting Classification Dataset using Ultralytics in Python\nDESCRIPTION: This function (`split_classify_dataset`) provides tools to divide image classification datasets into train, validation, and test splits. It is a Python function that, given a dataset directory and optional split ratios, organizes data into appropriate folders. Inputs typically include a root directory and ratio arguments. The operation may require dependencies on standard Python modules and Ultralytics utility functions. The output dataset is reorganized for seamless model training and evaluation. Constraints include expected folder structure and sufficient data for each split.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/data/split.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n## ::: ultralytics.data.split.split_classify_dataset\n```\n\n----------------------------------------\n\nTITLE: Accessing Package Segmentation Dataset Configuration in YAML\nDESCRIPTION: The package-seg.yaml file contains essential information about the dataset's paths, classes, and configuration. It is hosted on Ultralytics' GitHub repository and is crucial for configuring models to use the dataset efficiently.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/package-seg.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n# https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/package-seg.yaml\n```\n\n----------------------------------------\n\nTITLE: Performing Vector Search in LanceDB with Python\nDESCRIPTION: This snippet demonstrates how to perform a vector search on a LanceDB table using a dummy image embedding. It limits the results to 5 and converts the output to a pandas DataFrame.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/explorer.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndummy_img_embedding = [i for i in range(256)]\ntable.search(dummy_img_embedding).limit(5).to_pandas()\n```\n\n----------------------------------------\n\nTITLE: Running Inference with YOLO11 Classification Models via CLI\nDESCRIPTION: Demonstrates running prediction with YOLO11 classification models on images using the command-line interface.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/classify.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nyolo classify predict model=yolo11n-cls.pt source='https://ultralytics.com/images/bus.jpg'  # predict with official model\nyolo classify predict model=path/to/best.pt source='https://ultralytics.com/images/bus.jpg' # predict with custom model\n```\n\n----------------------------------------\n\nTITLE: Applying Copy-Paste Augmentation for Instance Segmentation\nDESCRIPTION: Copy-Paste augmentation copies objects within or between images based on a specified probability. It's particularly useful for instance segmentation tasks and rare object classes. The 'copy_paste' parameter controls the probability of applying this transformation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/yolo-data-augmentation.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Example usage in Ultralytics configuration\nmodel = YOLO('yolov8n-seg.yaml')\nmodel.train(data='coco128-seg.yaml', epochs=100, copy_paste=0.5)\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with Trained YOLO11 Model via CLI\nDESCRIPTION: Command-line interface command to perform inference using a fine-tuned YOLO11 model on a sample brain tumor image.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/brain-tumor.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Start prediction with a finetuned *.pt model\nyolo detect predict model='path/to/best.pt' imgsz=640 source=\"https://ultralytics.com/assets/brain-tumor-sample.jpg\"\n```\n\n----------------------------------------\n\nTITLE: Installing YOLOv5 Dependencies\nDESCRIPTION: Commands to clone the YOLOv5 repository and install required dependencies from requirements.txt. Requires Python>=3.8.0 and PyTorch>=1.8.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/model_ensembling.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ultralytics/yolov5 # clone\ncd yolov5\npip install -r requirements.txt # install\n```\n\n----------------------------------------\n\nTITLE: Get Bounding Box Dimensions\nDESCRIPTION: Code to extract and print bounding box dimensions (width, height, area) from YOLO model predictions.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/simple-utilities.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import YOLO\nfrom ultralytics.utils.plotting import Annotator\n\nmodel = YOLO(\"yolo11n.pt\")  # Load pretrain or fine-tune model\n\n# Process the image\nsource = cv2.imread(\"path/to/image.jpg\")\nresults = model(source)\n\n# Extract results\nannotator = Annotator(source, example=model.names)\n\nfor box in results[0].boxes.xyxy.cpu():\n    width, height, area = annotator.get_bbox_dimension(box)\n    print(f\"Bounding Box Width {width.item()}, Height {height.item()}, Area {area.item()}\")\n```\n\n----------------------------------------\n\nTITLE: Training YOLOv5 Using ClearML Dataset\nDESCRIPTION: This command runs YOLOv5 training using a dataset versioned in ClearML, specified by its dataset ID.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/clearml_logging_integration.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython train.py --img 640 --batch 16 --epochs 3 --data clearml://YOUR_DATASET_ID --weights yolov5s.pt --cache\n```\n\n----------------------------------------\n\nTITLE: Basic Workout Monitoring Example\nDESCRIPTION: Simplified Python example for monitoring workouts using pose estimation and video processing.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/workouts-monitoring.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\nfrom ultralytics import solutions\n\ncap = cv2.VideoCapture(\"path/to/video.mp4\")\nassert cap.isOpened(), \"Error reading video file\"\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n\ngym = solutions.AIGym(\n    line_width=2,\n    show=True,\n    kpts=[6, 8, 10],\n)\n\nwhile cap.isOpened():\n    success, im0 = cap.read()\n    if not success:\n        print(\"Video frame is empty or processing is complete.\")\n        break\n    results = gym(im0)\n\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on COCO Dataset - CLI\nDESCRIPTION: Command line interface example for training a YOLO model on COCO dataset using specified parameters.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/coco.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo detect train data=coco.yaml model=yolo11n.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO Model for Edge TPU - CLI Implementation\nDESCRIPTION: Command line interface command to export a YOLO model to Edge TPU compatible format.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/coral-edge-tpu-on-raspberry-pi.md#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=path/to/model.pt format=edgetpu # Export an official model or custom model\n```\n\n----------------------------------------\n\nTITLE: Training YOLOv5 with DataParallel on Multiple GPUs\nDESCRIPTION: Command to train YOLOv5 using DataParallel mode on multiple GPUs. This method is not recommended due to its inefficiency compared to DistributedDataParallel.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/multi_gpu_training.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython train.py --batch 64 --data coco.yaml --weights yolov5s.pt --device 0,1\n```\n\n----------------------------------------\n\nTITLE: YOLO OBB Format Example\nDESCRIPTION: Example of YOLO OBB format showing class index and normalized corner point coordinates (x1,y1,x2,y2,x3,y3,x4,y4)\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/obb/index.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n0 0.780811 0.743961 0.782371 0.74686 0.777691 0.752174 0.776131 0.749758\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLOv8 Models to MNN Format\nDESCRIPTION: Commands for exporting YOLOv8 models to MNN format using both direct export and ONNX conversion\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-MNN-CPP/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=yolov8n.pt imgsz=640 format=mnn\n```\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=yolov8n.pt format=onnx\n./MNN/build/MNNConvert -f ONNX --modelFile yolov8n.onnx --MNNModel yolov8n.mnn --bizCode biz\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating a Conda Environment for FastSAM - Shell\nDESCRIPTION: Commands for setting up a new Conda environment named 'FastSAM' with Python 3.9 and activating it. Requires Conda installed. Output is an active Python environment isolated for FastSAM usage.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/fast-sam.md#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nconda create -n FastSAM python=3.9\nconda activate FastSAM\n```\n\n----------------------------------------\n\nTITLE: Generating DDP File for Ultralytics Training\nDESCRIPTION: Function to generate a Distributed Data Parallel (DDP) file for multi-node training in Ultralytics. It creates a configuration file for distributed setup.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/dist.md#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nultralytics.utils.dist.generate_ddp_file\n```\n\n----------------------------------------\n\nTITLE: Setting Up CDK Environment in Bash\nDESCRIPTION: Commands to create and activate a Python virtual environment, install dependencies, and upgrade the AWS CDK library for deployment preparation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/amazon-sagemaker.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m venv .venv\nsource .venv/bin/activate\npip3 install -r requirements.txt\npip install --upgrade aws-cdk-lib\n```\n\n----------------------------------------\n\nTITLE: Checking Classification Dataset Structure and Validity - Python\nDESCRIPTION: The check_cls_dataset utility reviews classification datasets for proper foldering, file naming, and label assignment, ensuring compatibility with Ultralytics models. Inputs include dataset directories, with outputs being reports or error lists. Relies on directory and label mapping conventions.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/data/utils.md#2025-04-22_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\n## ::: ultralytics.data.utils.check_cls_dataset\n```\n\n----------------------------------------\n\nTITLE: Disabling Confusion Matrix Logging in Comet ML\nDESCRIPTION: Sets an environment variable to disable confusion matrix logging in Comet ML during YOLO11 training.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/comet.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"COMET_EVAL_LOG_CONFUSION_MATRIX\"] = \"false\"\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCV Dependencies\nDESCRIPTION: Installs required OpenCV dependencies for cloud environments.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/neural_magic_pruning_quantization.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\napt-get install libgl1\n```\n\n----------------------------------------\n\nTITLE: Setting Up YOLOv5 Environment on AWS\nDESCRIPTION: Clone the YOLOv5 repository and install required dependencies using pip. This prepares the AWS instance environment for running YOLOv5 operations.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/environments/aws_quickstart_tutorial.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Clone the YOLOv5 repository\ngit clone https://github.com/ultralytics/yolov5\ncd yolov5\n\n# Install required packages\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Setting up YOLOv5 environment\nDESCRIPTION: Clone the YOLOv5 repository and install the required dependencies using pip.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/test_time_augmentation.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ultralytics/yolov5 # clone\ncd yolov5\npip install -r requirements.txt # install\n```\n\n----------------------------------------\n\nTITLE: Setting Comet ML API Key\nDESCRIPTION: Sets the Comet ML API key as an environment variable for authentication.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/comet.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nexport COMET_API_KEY=YOUR_API_KEY\n```\n\n----------------------------------------\n\nTITLE: Benchmarking YOLO11 Performance with DeepSparse\nDESCRIPTION: Command to benchmark the performance of a YOLO11 model using DeepSparse, analyzing throughput and latency.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/neural-magic.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndeepsparse.benchmark model_path=\"path/to/yolo11n.onnx\" --scenario=sync --input_shapes=\"[1,3,640,640]\"\n```\n\n----------------------------------------\n\nTITLE: Tuning Results CSV Sample from Ultralytics YOLO\nDESCRIPTION: Example of the tune_results.csv file content, showing the fitness scores and hyperparameter values for each iteration of the tuning process.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/hyperparameter-tuning.md#2025-04-22_snippet_3\n\nLANGUAGE: csv\nCODE:\n```\nfitness,lr0,lrf,momentum,weight_decay,warmup_epochs,warmup_momentum,box,cls,dfl,hsv_h,hsv_s,hsv_v,degrees,translate,scale,shear,perspective,flipud,fliplr,mosaic,mixup,copy_paste\n0.05021,0.01,0.01,0.937,0.0005,3.0,0.8,7.5,0.5,1.5,0.015,0.7,0.4,0.0,0.1,0.5,0.0,0.0,0.0,0.5,1.0,0.0,0.0\n0.07217,0.01003,0.00967,0.93897,0.00049,2.79757,0.81075,7.5,0.50746,1.44826,0.01503,0.72948,0.40658,0.0,0.0987,0.4922,0.0,0.0,0.0,0.49729,1.0,0.0,0.0\n0.06584,0.01003,0.00855,0.91009,0.00073,3.42176,0.95,8.64301,0.54594,1.72261,0.01503,0.59179,0.40658,0.0,0.0987,0.46955,0.0,0.0,0.0,0.49729,0.80187,0.0,0.0\n```\n\n----------------------------------------\n\nTITLE: Segmenting Entire Content using SAM in CLI\nDESCRIPTION: This command snippet illustrates how to utilize the SAM model for segmenting entire content in videos via the command-line interface using a specified SAM model file. It requires the 'yolo' CLI tool and the 'sam2.1_b.pt' model file to execute.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/sam-2.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyolo predict model=sam2.1_b.pt source=path/to/video.mp4\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics and SAHI Libraries\nDESCRIPTION: Command to install the latest versions of SAHI and Ultralytics libraries using pip.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/sahi-tiled-inference.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U ultralytics sahi\n```\n\n----------------------------------------\n\nTITLE: Viewing Ultralytics Settings in Python\nDESCRIPTION: Code snippet demonstrating how to view all settings and specifically check the analytics and crash reporting setting in Python. This allows users to inspect their current configuration before making any changes.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/help/privacy.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import settings\n\n# View all settings\nprint(settings)\n\n# Return analytics and crash reporting setting\nvalue = settings[\"sync\"]\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with Trained Model on Medical Pills Dataset via CLI\nDESCRIPTION: This command-line instruction shows how to use a fine-tuned YOLO model for prediction on a sample image from the medical-pills dataset using the CLI.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/medical-pills.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Start prediction with a fine-tuned *.pt model\nyolo detect predict model='path/to/best.pt' imgsz=640 source=\"https://ultralytics.com/assets/medical-pills-sample.jpg\"\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics with CUDA Support\nDESCRIPTION: Command to install Ultralytics, PyTorch, and CUDA support for GPU acceleration.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/conda-quickstart.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c pytorch -c nvidia -c conda-forge pytorch torchvision pytorch-cuda=11.8 ultralytics\n```\n\n----------------------------------------\n\nTITLE: Train Epoch End Callback\nDESCRIPTION: Callback function that executes at the end of each training epoch to log training-specific metrics.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/callbacks/wb.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nultralytics.utils.callbacks.wb.on_train_epoch_end\n```\n\n----------------------------------------\n\nTITLE: Training YOLOv5 with Custom ClearML Project and Task Names\nDESCRIPTION: This command runs YOLOv5 training with custom project and task names for ClearML experiment tracking.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/clearml_logging_integration.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython train.py --project my_project --name my_training --img 640 --batch 16 --epochs 3 --data coco8.yaml --weights yolov5s.pt --cache\n```\n\n----------------------------------------\n\nTITLE: Exporting Official YOLO Model to ONNX Format in Bash\nDESCRIPTION: Command to export an official pre-trained YOLO11n model to the ONNX format for deployment.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/cli.md#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=yolo11n.pt format=onnx\n```\n\n----------------------------------------\n\nTITLE: Controlling Number of YOLOv5 Prediction Images Logged to Comet\nDESCRIPTION: Command to set the maximum number of validation images logged to Comet during training using the COMET_MAX_IMAGE_UPLOADS environment variable.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/comet_logging_integration.md#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nenv COMET_MAX_IMAGE_UPLOADS=200 python train.py \\\n  --img 640 \\\n  --batch 16 \\\n  --epochs 5 \\\n  --data coco128.yaml \\\n  --weights yolov5s.pt \\\n  --bbox_interval 1\n```\n\n----------------------------------------\n\nTITLE: Running YOLOE Prompted Detection via Command Line (Bash)\nDESCRIPTION: Performs object detection using the YOLOE model (`yoloe-s.pt`) on an image (`kitchen.jpg`) with specific text prompts. The `classes` argument filters detection results to only include the specified objects, in this case, \"bowl\" and \"apple\".\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yoloe.md#2025-04-22_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nyolo predict model=yoloe-s.pt source=\"kitchen.jpg\" classes=\"bowl,apple\"\n```\n\n----------------------------------------\n\nTITLE: Cloning the Ultralytics Repository using Git\nDESCRIPTION: Clones the official Ultralytics repository and navigates to the specific example directory for YOLOv8 OpenVINO C++ Inference. This is the initial setup step for working with the example files.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-OpenVINO-CPP-Inference/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ultralytics/ultralytics.git\ncd ultralytics/examples/YOLOv8-OpenVINO-CPP-Inference\n```\n\n----------------------------------------\n\nTITLE: Running YOLO11 Object Detection on Raspberry Pi AI Camera\nDESCRIPTION: Executes the IMX500 object detection demo script with the packaged model, specifying FPS, bounding box normalization, and label file.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/sony-imx500.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython imx500_object_detection_demo.py --model path/to/network.rpk --fps 17 --bbox-normalization --ignore-dash-labels --bbox-order xy --labels path/to/labels.txt\n```\n\n----------------------------------------\n\nTITLE: Image Resizing with OpenCV\nDESCRIPTION: Example of resizing images using OpenCV's resize function with bilinear interpolation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/preprocessing_annotated_data.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\n\n# Read image\nimage = cv2.imread('image.jpg')\n\n# Resize image to 640x640 using bilinear interpolation\nresized_image = cv2.resize(image, (640, 640), interpolation=cv2.INTER_LINEAR)\n```\n\n----------------------------------------\n\nTITLE: Classification Models Performance Table in Markdown\nDESCRIPTION: Markdown table showing metrics for YOLO11 classification models trained on ImageNet, including accuracy, speed, parameters, and FLOPs.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/README.md#2025-04-22_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n| Model | size<br><sup>(pixels) | acc<br><sup>top1 | acc<br><sup>top5 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) at 224 |\n| -------------------------------------------------------------------------------------------- | --------------------- | ---------------- | ---------------- | ------------------------------ | ----------------------------------- | ------------------ | ------------------------ |\n```\n\n----------------------------------------\n\nTITLE: Setting MLflow Project Name via Environment Variable\nDESCRIPTION: Command to set the MLflow experiment name using an environment variable, which allows for organizing experiments under a specific project.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/mlflow.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport MLFLOW_EXPERIMENT_NAME=YOUR_EXPERIMENT_NAME\n```\n\n----------------------------------------\n\nTITLE: Pulling Ultralytics Docker Image\nDESCRIPTION: Command to download the latest Ultralytics Docker image from Docker Hub.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/docker-quickstart.md#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nsudo docker pull ultralytics/ultralytics:latest\n```\n\n----------------------------------------\n\nTITLE: Initializing Points Selection with Ultralytics YOLO11\nDESCRIPTION: Code to launch the graphical interface for selecting parking points using Ultralytics solutions. Requires tkinter for the GUI implementation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/parking-management.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import solutions\n\nsolutions.ParkingPtsSelection()\n```\n\n----------------------------------------\n\nTITLE: Mount Local Directories in Docker Container\nDESCRIPTION: Command to mount local host directories into the Docker container for accessing local files.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/environments/docker_image_quickstart_tutorial.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Mount /path/on/host (your local machine) to /path/in/container (inside the container)\nsudo docker run -it --ipc=host --gpus all -v /path/on/host:/path/in/container $t\n```\n\n----------------------------------------\n\nTITLE: Copying Generated Files\nDESCRIPTION: Commands to copy the generated ONNX model and labels file to DeepStream directory\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/deepstream-nvidia-jetson.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncp yolo11s.pt.onnx labels.txt ~/DeepStream-Yolo\ncd ~/DeepStream-Yolo\n```\n\n----------------------------------------\n\nTITLE: Training YOLOv9 Model with CLI\nDESCRIPTION: Command-line interface command for training a YOLOv9c model using the Ultralytics framework. This command trains a model defined by yolov9c.yaml on the coco8 dataset for 100 epochs with 640x640 image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov9.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nyolo train model=yolov9c.yaml data=coco8.yaml epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Cloning Ultralytics Repository in Bash\nDESCRIPTION: This command clones the Ultralytics repository containing the example code and necessary files using Git.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-LibTorch-CPP-Inference/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ultralytics/ultralytics\n```\n\n----------------------------------------\n\nTITLE: Selecting Random Calibration Images\nDESCRIPTION: Copies 1000 random images from COCO dataset for calibration\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/deepstream-nvidia-jetson.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nfor jpg in $(ls -1 val2017/*.jpg | sort -R | head -1000); do\n  cp ${jpg} calibration/\ndone\n```\n\n----------------------------------------\n\nTITLE: Results Plotting Utility\nDESCRIPTION: Python code snippet demonstrating how to plot training results from a CSV file using the built-in plotting utility.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/train_custom_data.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom utils.plots import plot_results\n\n# Plot results from a specific training run directory\nplot_results(\"runs/train/exp/results.csv\")  # This will generate 'results.png' in the same directory\n```\n\n----------------------------------------\n\nTITLE: Updating Ultralytics Settings via Python and CLI - Python & Bash\nDESCRIPTION: These snippets explain how to update library configuration settings both in Python via the 'settings.update()' method and with the CLI using 'yolo settings'. They support updating single or multiple settings in a dictionary-style format and allow resetting to factory defaults. Changes are persisted in the user's configuration directory. Provide key-value pairs for desired settings; resetting requires no additional arguments.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/quickstart.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import settings\n\n# Update a setting\nsettings.update({\"runs_dir\": \"/path/to/runs\"})\n\n# Update multiple settings\nsettings.update({\"runs_dir\": \"/path/to/runs\", \"tensorboard\": False})\n\n# Reset settings to default values\nsettings.reset()\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Update a setting\nyolo settings runs_dir='/path/to/runs'\n\n# Update multiple settings\nyolo settings runs_dir='/path/to/runs' tensorboard=False\n\n# Reset settings to default values\nyolo settings reset\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for YOLO11 SAHI Video Inference (Bash)\nDESCRIPTION: Clones the Ultralytics repository from GitHub, installs the necessary Python packages (`sahi`, `ultralytics`, `opencv-python`) using pip, and changes the current directory to the specific example folder containing the SAHI video inference script. Requires git and Python 3.8 or later with pip installed.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-SAHI-Inference-Video/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Clone the ultralytics repository\ngit clone https://github.com/ultralytics/ultralytics\n\n# Install dependencies\n# Ensure you have Python 3.8 or later installed\npip install -U sahi ultralytics opencv-python\n\n# Change directory to the example folder\ncd ultralytics/examples/YOLOv8-SAHI-Inference-Video\n```\n\n----------------------------------------\n\nTITLE: Updating MLflow Settings via CLI\nDESCRIPTION: Command-line interface commands to enable MLflow integration in Ultralytics YOLO and reset settings to default values.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/mlflow.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Update a setting\nyolo settings mlflow=True\n\n# Reset settings to default values\nyolo settings reset\n```\n\n----------------------------------------\n\nTITLE: Importing WorldTrainerFromScratch from YOLO World Module\nDESCRIPTION: Module reference for the WorldTrainerFromScratch class that extends YOLO's training capabilities for open-set datasets. This class enables building, training and evaluating models with dynamic class sets.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/models/yolo/world/train_world.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics.models.yolo.world.train_world import WorldTrainerFromScratch\n```\n\n----------------------------------------\n\nTITLE: Logging TensorBoard Graph in Python\nDESCRIPTION: This function logs the model graph to TensorBoard. It likely handles the conversion of the model structure to a format compatible with TensorBoard.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/callbacks/tensorboard.md#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nultralytics.utils.callbacks.tensorboard._log_tensorboard_graph\n```\n\n----------------------------------------\n\nTITLE: Fit Epoch End Callback\nDESCRIPTION: Callback function that executes at the end of each fit epoch to log metrics and results.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/callbacks/wb.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nultralytics.utils.callbacks.wb.on_fit_epoch_end\n```\n\n----------------------------------------\n\nTITLE: ClearML Pretrain Start Callback\nDESCRIPTION: Callback function that executes at the start of the pretraining routine\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/callbacks/clearml.md#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nultralytics.utils.callbacks.clearml.on_pretrain_routine_start\n```\n\n----------------------------------------\n\nTITLE: Cloning YOLOv5 Repository and Installing Dependencies\nDESCRIPTION: This snippet shows how to clone the YOLOv5 repository and install the necessary dependencies listed in requirements.txt. It requires Python>=3.8.0 and PyTorch>=1.8.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/transfer_learning_with_frozen_layers.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ultralytics/yolov5 # clone repository\ncd yolov5\npip install -r requirements.txt # install dependencies\n```\n\n----------------------------------------\n\nTITLE: Installing FastSAM and Dependencies - Shell\nDESCRIPTION: Change directories into the FastSAM repository and install required Python packages listed in requirements.txt. Input: previously cloned repo is present and active Python environment. Output: all dependencies installed, ready to run FastSAM scripts.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/fast-sam.md#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\ncd FastSAM\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Cloning the FastSAM Repository - Shell\nDESCRIPTION: Instructions to clone the official FastSAM repository from GitHub to a local directory. Prerequisite: git installed. No inputs other than the GitHub repository URL; output is a local copy of the FastSAM codebase in a subdirectory.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/fast-sam.md#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/CASIA-IVA-Lab/FastSAM.git\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Explorer\nDESCRIPTION: Command to install Ultralytics Explorer API with all dependencies using pip.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/api.md#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics[explorer]\n```\n\n----------------------------------------\n\nTITLE: Packaging IMX500 Model into RPK File\nDESCRIPTION: Uses the imx500-package tool to convert the packerOut.zip file into a network.rpk file for deployment on the AI Camera.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/sony-imx500.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nimx500-package -i path/to/packerOut.zip -o path/to/output/folder\n```\n\n----------------------------------------\n\nTITLE: Deploying ClearML Agent\nDESCRIPTION: Command to deploy a ClearML agent for remote execution with specified queue configuration\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/clearml.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nclearml-agent daemon --queue QUEUES_TO_LISTEN_TO [--docker]\n```\n\n----------------------------------------\n\nTITLE: Experiment-Level Error Checking with Ray Tune Result Grid Python\nDESCRIPTION: This snippet performs a quick check for errors across all tuning trials in the Ray Tune result grid, printing a message if any trials failed. Required dependency is ray. Inputs: Ray Tune ResultGrid object. Outputs: console message about trial statuses. Assumes result_grid contains results from tuner.get_results().\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/ray-tune.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nif result_grid.errors:\\n    print(\"One or more trials failed!\")\\nelse:\\n    print(\"No errors!\")\n```\n\n----------------------------------------\n\nTITLE: Running the Object Tracking Script\nDESCRIPTION: Command to execute the interactive tracker script from the terminal, launching the object detection and tracking UI.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLO-Interactive-Tracking-UI/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython interactive_tracker.py\n```\n\n----------------------------------------\n\nTITLE: Running YOLO Oriented Bounding Box (OBB) Detection Tasks\nDESCRIPTION: Commands for running OBB detection tasks using YOLOv8 and YOLO11 models with the ONNXRuntime Rust implementation. These examples specifically target rotated object detection on aerial imagery.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLO-Series-ONNXRuntime-Rust/README.md#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ncargo run -r -- --ver v8 --task obb --scale n --width 1024 --height 1024 --source images/dota.png  # YOLOv8-OBB\ncargo run -r -- --ver v11 --task obb --scale n --width 1024 --height 1024 --source images/dota.png # YOLO11-OBB\n```\n\n----------------------------------------\n\nTITLE: Auto-Annotating Datasets with Ultralytics and SAM\nDESCRIPTION: This snippet shows how to use Ultralytics' auto_annotate function to automatically label a dataset using a pre-trained YOLO detection model together with the Segment Anything Model (SAM). This utility significantly reduces the manual effort required for dataset annotation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/simple-utilities.md#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.data.annotator import auto_annotate\n\nauto_annotate(\n    data=\"path/to/new/data\",\n    det_model=\"yolo11n.pt\",\n    sam_model=\"mobile_sam.pt\",\n    device=\"cuda\",\n    output_dir=\"path/to/save_labels\",\n)\n```\n\n----------------------------------------\n\nTITLE: Check NVIDIA GPU Status for Docker\nDESCRIPTION: Command to verify NVIDIA drivers installation and display GPU information.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/environments/docker_image_quickstart_tutorial.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnvidia-smi\n```\n\n----------------------------------------\n\nTITLE: YOLO Model Inference Example with Zero-Channel Image\nDESCRIPTION: Demonstrates a bug report example showing an error when running inference on a 0-channel image using YOLO model\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/help/minimum-reproducible-example.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom ultralytics import YOLO\n\n# Load the model\nmodel = YOLO(\"yolov8n.pt\")\n\n# Load a 0-channel image\nimage = torch.rand(1, 0, 640, 640)\n\n# Run the model\nresults = model(image)\n```\n\n----------------------------------------\n\nTITLE: Managing Data with Google Cloud Storage\nDESCRIPTION: Commands for copying data between Google Cloud Storage buckets and the VM instance\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/environments/google_cloud_quickstart_tutorial.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Ensure Google Cloud SDK is installed and initialized\n# If not installed: curl https://sdk.cloud.google.com/ | bash\n# Then initialize: gcloud init\n\n# Example: Copy your dataset from a GCS bucket to your VM\ngsutil cp -r gs://your-data-bucket/my_dataset ./datasets/\n\n# Example: Copy trained model weights from your VM to a GCS bucket\ngsutil cp -r ./runs/train/exp/weights gs://your-models-bucket/yolov5_custom_weights/\n```\n\n----------------------------------------\n\nTITLE: Configuring VSCode Terminal Settings for Image Display\nDESCRIPTION: VSCode settings required to enable image display and GPU acceleration in the integrated terminal\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/view-results-in-terminal.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n\"terminal.integrated.gpuAcceleration\": \"auto\" # \"auto\" is default, can also use \"on\"\n\"terminal.integrated.enableImages\": true\n```\n\n----------------------------------------\n\nTITLE: Installing YOLOv5 Requirements\nDESCRIPTION: Clone the YOLOv5 repository and install the required dependencies from the requirements.txt file in a Python 3.8+ environment with PyTorch 1.8+.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/model_export.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ultralytics/yolov5 # clone\ncd yolov5\npip install -r requirements.txt # install\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics and Checking Environment\nDESCRIPTION: Installs the Ultralytics package and performs environment checks. This snippet sets up the necessary dependencies for using YOLO11.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/heatmaps.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install ultralytics\nimport ultralytics\n\nultralytics.checks()\n```\n\n----------------------------------------\n\nTITLE: Data Splitting Using scikit-learn Reference\nDESCRIPTION: A common method for splitting data into training, validation and test sets using scikit-learn's train_test_split function.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/preprocessing_annotated_data.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import train_test_split\n\n# Split data into train and temp sets (80/20)\nX_train, X_temp, y_train, y_temp = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\n# Split temp into validation and test sets (20/10)\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, test_size=0.33, random_state=42\n)\n```\n\n----------------------------------------\n\nTITLE: Training YOLOv5 with a Dataset from Comet Artifacts\nDESCRIPTION: Command to train YOLOv5 using a dataset stored in Comet Artifacts, referenced through the YAML configuration file.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/comet_logging_integration.md#2025-04-22_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\npython train.py \\\n  --img 640 \\\n  --batch 16 \\\n  --epochs 5 \\\n  --data artifact.yaml \\\n  --weights yolov5s.pt\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics from Source Using Pip\nDESCRIPTION: This command installs the Ultralytics package directly from the GitHub source using pip. It is a convenient method for installing the latest version of the library. Ensure that `pip` is installed and available in your Python environment. This command is executed in the shell via Python.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n!pip install git+https://github.com/ultralytics/ultralytics@main\n```\n\n----------------------------------------\n\nTITLE: Configuring YOLO11 Logger in Python\nDESCRIPTION: This code snippet sets up logging for YOLO11 training. It allows selection between TensorBoard and Weights & Biases loggers, and configures the chosen logger.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nlogger = 'TensorBoard' #@param ['TensorBoard', 'Weights & Biases']\n\nif logger == 'TensorBoard':\n  !yolo settings tensorboard=True\n  %load_ext tensorboard\n  %tensorboard --logdir .\nelif logger == 'Weights & Biases':\n  !yolo settings wandb=True\n```\n\n----------------------------------------\n\nTITLE: ClearML Fit Epoch End Callback\nDESCRIPTION: Callback function that executes at the end of each fit epoch\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/callbacks/clearml.md#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nultralytics.utils.callbacks.clearml.on_fit_epoch_end\n```\n\n----------------------------------------\n\nTITLE: Defining RF100Benchmark Class for YOLO Model Benchmarking in Python\nDESCRIPTION: This class implements benchmarking functionality for YOLO models using the RF100 dataset. It likely includes methods for data loading, model inference, and performance evaluation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/benchmarks.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nclass RF100Benchmark:\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics YOLO with pip\nDESCRIPTION: Command to install the Ultralytics package using pip, which provides access to YOLO models and functionality.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/index.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics\n```\n\n----------------------------------------\n\nTITLE: Installing NVIDIA Docker Runtime in Bash\nDESCRIPTION: This script installs the NVIDIA Docker runtime to enable GPU support in Docker containers. It adds NVIDIA package repositories, installs the runtime, and restarts the Docker service.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/docker-quickstart.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Add NVIDIA package repositories\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\ndistribution=$(lsb_release -cs)\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n\n# Install NVIDIA Docker runtime\nsudo apt-get update\nsudo apt-get install -y nvidia-docker2\n\n# Restart Docker service to apply changes\nsudo systemctl restart docker\n```\n\n----------------------------------------\n\nTITLE: YOLO Multi-Stream Inference\nDESCRIPTION: Demonstrates batch inference on multiple video streams using a .streams configuration file.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/predict.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Multiple streams with batched inference (e.g., batch-size 8 for 8 streams)\nsource = \"path/to/list.streams\"  # *.streams text file with one streaming address per line\n\n# Run inference on the source\nresults = model(source, stream=True)  # generator of Results objects\n```\n\nLANGUAGE: txt\nCODE:\n```\nrtsp://example.com/media1.mp4\nrtsp://example.com/media2.mp4\nrtmp://example2.com/live\ntcp://192.168.1.100:554\n...\n```\n\n----------------------------------------\n\nTITLE: YOLOv5 v6.0 Backbone and Head Structure\nDESCRIPTION: This YAML snippet illustrates the structure of YOLOv5 v6.0 backbone and head. It shows the layer configuration including convolutions, C3 modules, and spatial pyramid pooling.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/transfer_learning_with_frozen_layers.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n# Example YOLOv5 v6.0 backbone structure\nbackbone:\n    # [from, number, module, args]\n    - [-1, 1, Conv, [64, 6, 2, 2]]  # Layer 0: Initial convolution (P1/2 stride)\n    - [-1, 1, Conv, [128, 3, 2]] # Layer 1: Downsampling convolution (P2/4 stride)\n    - [-1, 3, C3, [128]]          # Layer 2: C3 module\n    - [-1, 1, Conv, [256, 3, 2]] # Layer 3: Downsampling convolution (P3/8 stride)\n    - [-1, 6, C3, [256]]          # Layer 4: C3 module\n    - [-1, 1, Conv, [512, 3, 2]] # Layer 5: Downsampling convolution (P4/16 stride)\n    - [-1, 9, C3, [512]]          # Layer 6: C3 module\n    - [-1, 1, Conv, [1024, 3, 2]]# Layer 7: Downsampling convolution (P5/32 stride)\n    - [-1, 3, C3, [1024]]         # Layer 8: C3 module\n    - [-1, 1, SPPF, [1024, 5]]    # Layer 9: Spatial Pyramid Pooling Fast\n\n# Example YOLOv5 v6.0 head structure\nhead:\n    - [-1, 1, Conv, [512, 1, 1]] # Layer 10\n    - [-1, 1, nn.Upsample, [None, 2, \"nearest\"]] # Layer 11\n    - [[-1, 6], 1, Concat, [1]] # Layer 12: Concatenate with backbone P4 (from layer 6)\n    - [-1, 3, C3, [512, False]] # Layer 13: C3 module\n    # ... subsequent head layers for feature fusion and detection\n```\n\n----------------------------------------\n\nTITLE: Resuming Training with Comet YOLOv5 Shell\nDESCRIPTION: Resumes a YOLOv5 training session using Comet's Run Path. The script trains the model and restores its state from a previous run, including hyperparameters and checkpoints.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/comet_logging_integration.md#2025-04-22_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\npython train.py \\\n  --resume \"comet://YOUR_RUN_PATH\"\n```\n\n----------------------------------------\n\nTITLE: Installing Edge TPU Runtime on Raspberry Pi\nDESCRIPTION: Command to install the Edge TPU runtime package on a Raspberry Pi using dpkg. The specific package file depends on the Raspberry Pi OS version and desired frequency mode.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/coral-edge-tpu-on-raspberry-pi.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo dpkg -i path/to/package.deb\n```\n\n----------------------------------------\n\nTITLE: Creating ZIP Archive for Dataset Upload\nDESCRIPTION: Command to zip a dataset directory named 'coco8' for uploading to Ultralytics HUB. The ZIP file must contain the dataset directory with its YAML file inside.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/hub/datasets.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nzip -r coco8.zip coco8\n```\n\n----------------------------------------\n\nTITLE: Citing the YOLOv4 Paper in BibTeX Format\nDESCRIPTION: Provides the BibTeX entry for citing the original YOLOv4 paper by Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. This format is commonly used in academic publications for managing references and includes details like title, authors, year, and arXiv information.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov4.md#2025-04-22_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n```bibtex\n@misc{bochkovskiy2020yolov4,\n      title={YOLOv4: Optimal Speed and Accuracy of Object Detection},\n      author={Alexey Bochkovskiy and Chien-Yao Wang and Hong-Yuan Mark Liao},\n      year={2020},\n      eprint={2004.10934},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Uninstalling TensorFlow on Raspberry Pi\nDESCRIPTION: Command to uninstall TensorFlow packages on Raspberry Pi before installing TensorFlow Lite runtime.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/coral-edge-tpu-on-raspberry-pi.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip uninstall tensorflow tensorflow-aarch64\n```\n\n----------------------------------------\n\nTITLE: Segmentation Models Performance Table in Markdown\nDESCRIPTION: Markdown table detailing performance metrics for YOLO11 segmentation models trained on COCO-Seg dataset, including box and mask mAP, speed, parameters, and FLOPs.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/README.md#2025-04-22_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n| Model | size<br><sup>(pixels) | mAP<sup>box<br>50-95 | mAP<sup>mask<br>50-95 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n| -------------------------------------------------------------------------------------------- | --------------------- | -------------------- | --------------------- | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\n```\n\n----------------------------------------\n\nTITLE: Running YOLO Rust Demo with Automatic ONNXRuntime Download\nDESCRIPTION: Command to run the YOLO example with the 'auto' feature flag, which automatically downloads the appropriate ONNXRuntime library during build instead of manual linking.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLO-Series-ONNXRuntime-Rust/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncargo run -r --example yolo --features auto\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package\nDESCRIPTION: Command to install the Ultralytics package for YOLO object detection functionality.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/ros-quickstart.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics for SAM 2 with pip (Bash)\nDESCRIPTION: This snippet demonstrates how to install the Ultralytics Python package via pip, which is required to run Meta's SAM 2 and SAM 2.1 models. No additional dependencies are necessary beyond a working Python and pip installation; all model weights will be automatically downloaded on first use. This command line snippet should be run in a terminal or command prompt to prepare the environment for performing real-time image and video segmentation with Ultralytics.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/sam-2.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package\nDESCRIPTION: Basic setup commands to install the ultralytics package and perform system checks.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/object_tracking.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install ultralytics\nimport ultralytics\n\nultralytics.checks()\n```\n\n----------------------------------------\n\nTITLE: TensorBoard Callback for Pre-training Routine Start in Python\nDESCRIPTION: This callback function is executed at the start of the pre-training routine. It likely sets up initial logging or configurations for TensorBoard.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/callbacks/tensorboard.md#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nultralytics.utils.callbacks.tensorboard.on_pretrain_routine_start\n```\n\n----------------------------------------\n\nTITLE: Docker Deployment Commands for Different JetPack Versions\nDESCRIPTION: Docker commands to pull and run Ultralytics YOLO containers for different JetPack versions on Jetson devices.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/nvidia-jetson.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nt=ultralytics/ultralytics:latest-jetson-jetpack4\nsudo docker pull $t && sudo docker run -it --ipc=host --runtime=nvidia $t\n```\n\nLANGUAGE: bash\nCODE:\n```\nt=ultralytics/ultralytics:latest-jetson-jetpack5\nsudo docker pull $t && sudo docker run -it --ipc=host --runtime=nvidia $t\n```\n\nLANGUAGE: bash\nCODE:\n```\nt=ultralytics/ultralytics:latest-jetson-jetpack6\nsudo docker pull $t && sudo docker run -it --ipc=host --runtime=nvidia $t\n```\n\n----------------------------------------\n\nTITLE: Installing Coral Edge TPU Runtime on Raspberry Pi\nDESCRIPTION: Command to install the Coral Edge TPU runtime package on Raspberry Pi using dpkg package manager.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/coral-edge-tpu-on-raspberry-pi.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nsudo dpkg -i path/to/package.deb\n```\n\n----------------------------------------\n\nTITLE: Configuring LibTorch Dependencies\nDESCRIPTION: Sets up LibTorch paths and includes required dependencies with compilation flags setup.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-LibTorch-CPP-Inference/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nlist(APPEND CMAKE_PREFIX_PATH \"/path/to/libtorch\")\nset(Torch_DIR \"/path/to/libtorch/share/cmake/Torch\")\n\nfind_package(Torch REQUIRED)\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}\")\nmessage(\"${TORCH_LIBRARIES}\")\nmessage(\"${TORCH_INCLUDE_DIRS}\")\n\ninclude_directories(${TORCH_INCLUDE_DIRS})\n```\n\n----------------------------------------\n\nTITLE: Cloning Ultralytics from GitHub\nDESCRIPTION: This snippet guides users on cloning the Ultralytics repository for development or experimentation purposes. It requires Git and pip tools and navigates users through installing the package in editable mode. Expected outputs are a local clone of the repository and an installed package ready for modification.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/quickstart.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Clone the ultralytics repository\ngit clone https://github.com/ultralytics/ultralytics\n\n# Navigate to the cloned directory\ncd ultralytics\n\n# Install the package in editable mode for development\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package for YOLO11\nDESCRIPTION: Command to install the required Ultralytics package via pip for working with YOLO11 models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tensorrt.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the required package for YOLO11\npip install ultralytics\n```\n\n----------------------------------------\n\nTITLE: Implementing Subset Training for YOLO11 in Python\nDESCRIPTION: Configures YOLO11 to train on a subset of the dataset by setting the fraction parameter. This example uses 10% of the data for training.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/model-training-tips.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfraction=0.1\n```\n\n----------------------------------------\n\nTITLE: Training and Using YOLO11 OBB Models\nDESCRIPTION: This Python code snippet is intended for working with YOLO11 Oriented Bounding Boxes models. You need the `ultralytics` package, and it uses parameters such as `data` for the dataset and `epochs` for iteration count. Expected results include performance metrics and prediction outputs on images.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolo11n-obb.pt')  # load a pretrained YOLO OBB model\nmodel.train(data='dota8.yaml', epochs=3)  # train the model\nmodel('https://ultralytics.com/images/boats.jpg')  # predict on an image\n```\n\n----------------------------------------\n\nTITLE: Enabling Mixed Precision Training in YOLOv5 (Shell)\nDESCRIPTION: This snippet demonstrates enabling mixed precision training via the '--amp' flag in YOLOv5, which can speed up training and reduce memory usage on supported GPUs. Requires NVIDIA GPUs with CUDA support and a YOLOv5 installation supporting AMP. The '--amp' parameter activates automatic mixed precision; outputs include faster iteration and potentially reduced training time.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/tips_for_best_training_results.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n--amp\n```\n\n----------------------------------------\n\nTITLE: Setting up Build Target\nDESCRIPTION: Creates executable target and links required libraries with C++17 standard specification.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-LibTorch-CPP-Inference/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_executable(yolov8_libtorch_inference \"${CMAKE_CURRENT_SOURCE_DIR}/main.cc\")\ntarget_link_libraries(yolov8_libtorch_inference ${TORCH_LIBRARIES} ${OpenCV_LIBS})\nset_property(TARGET yolov8_libtorch_inference PROPERTY CXX_STANDARD 17)\n```\n\n----------------------------------------\n\nTITLE: CLA Signing Comment Example\nDESCRIPTION: Example text for signing the Contributor License Agreement in pull request comments\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/CONTRIBUTING.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nI have read the CLA Document and I sign the CLA\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package for YOLO11 Export\nDESCRIPTION: Command to install the Ultralytics package required for working with YOLO11 models and exporting them to TF GraphDef format.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tf-graphdef.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the required package for YOLO11\npip install ultralytics\n```\n\n----------------------------------------\n\nTITLE: Pixel Value Normalization\nDESCRIPTION: Example of normalizing pixel values using min-max scaling to range [0,1].\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/preprocessing_annotated_data.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\n# Normalize pixel values to range [0,1]\nnormalized_image = image.astype('float32') / 255.0\n\n# Z-score normalization\n#normalized_image = (image - np.mean(image)) / np.std(image)\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLOv8 Model to ONNX Format\nDESCRIPTION: This bash command demonstrates how to export a YOLOv8 model to ONNX format using the Ultralytics CLI. It specifies the image size and ONNX opset version for compatibility.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-CPP-Inference/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyolo export model=yolov8s.pt imgsz=640,480 format=onnx opset=12 # Example: 640x480 resolution\n```\n\n----------------------------------------\n\nTITLE: Displaying Image in Markdown\nDESCRIPTION: This Markdown snippet displays an image showing an overview of the computer vision project steps. It uses HTML-like syntax within Markdown for centering and sizing the image.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/steps-of-a-cv-project.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n<p align=\"center\">\n  <img width=\"100%\" src=\"https://github.com/ultralytics/docs/releases/download/0/five-stages-of-ml-development-lifecycle.avif\" alt=\"Computer Vision Project Steps Overview\">\n</p>\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package via CLI\nDESCRIPTION: Command to install the required Ultralytics package for YOLO11 model export\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/edge-tpu.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics\n```\n\n----------------------------------------\n\nTITLE: YOLO Tracking via Command Line Interface\nDESCRIPTION: CLI command for implementing object tracking using YOLO model, demonstrating how to specify model, source, and tracking parameters.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/track/index.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyolo track model=yolo11n.pt source=\"https://youtu.be/LNwODJXcvt4\" conf=0.3, iou=0.5 show\n```\n\n----------------------------------------\n\nTITLE: Documenting YOLOv8 Validation Arguments in Markdown\nDESCRIPTION: A markdown table listing and describing all available arguments for YOLOv8 model validation. Each row represents an argument with its type, default value, and a detailed explanation of its purpose and impact on the validation process.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/macros/validation-args.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Argument       | Type        | Default | Description                                                                                                                                                                                                                                               |\n| -------------- | ----------- | ------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `data`         | `str`       | `None`  | Specifies the path to the dataset configuration file (e.g., `coco8.yaml`). This file includes paths to [validation data](https://www.ultralytics.com/glossary/validation-data), class names, and number of classes.                                       |\n| `imgsz`        | `int`       | `640`   | Defines the size of input images. All images are resized to this dimension before processing. Larger sizes may improve accuracy for small objects but increase computation time.                                                                          |\n| `batch`        | `int`       | `16`    | Sets the number of images per batch. Higher values utilize GPU memory more efficiently but require more VRAM. Adjust based on available hardware resources.                                                                                               |\n| `save_json`    | `bool`      | `False` | If `True`, saves the results to a JSON file for further analysis, integration with other tools, or submission to evaluation servers like COCO.                                                                                                            |\n| `conf`         | `float`     | `0.001` | Sets the minimum confidence threshold for detections. Lower values increase recall but may introduce more false positives. Used during [validation](https://docs.ultralytics.com/modes/val/) to compute precision-recall curves.                          |\n| `iou`          | `float`     | `0.6`   | Sets the [Intersection Over Union](https://www.ultralytics.com/glossary/intersection-over-union-iou) threshold for [Non-Maximum Suppression](https://www.ultralytics.com/glossary/non-maximum-suppression-nms). Controls duplicate detection elimination. |\n| `max_det`      | `int`       | `300`   | Limits the maximum number of detections per image. Useful in dense scenes to prevent excessive detections and manage computational resources.                                                                                                             |\n| `half`         | `bool`      | `True`  | Enables half-[precision](https://www.ultralytics.com/glossary/precision) (FP16) computation, reducing memory usage and potentially increasing speed with minimal impact on [accuracy](https://www.ultralytics.com/glossary/accuracy).                     |\n| `device`       | `str`       | `None`  | Specifies the device for validation (`cpu`, `cuda:0`, etc.). When `None`, automatically selects the best available device. Multiple CUDA devices can be specified with comma separation.                                                                  |\n| `dnn`          | `bool`      | `False` | If `True`, uses the [OpenCV](https://www.ultralytics.com/glossary/opencv) DNN module for ONNX model inference, offering an alternative to [PyTorch](https://www.ultralytics.com/glossary/pytorch) inference methods.                                      |\n| `plots`        | `bool`      | `False` | When set to `True`, generates and saves plots of predictions versus ground truth, confusion matrices, and PR curves for visual evaluation of model performance.                                                                                           |\n| `classes`      | `list[int]` | `None`  | Specifies a list of class IDs to train on. Useful for filtering out and focusing only on certain classes during evaluation.                                                                                                                               |\n| `rect`         | `bool`      | `True`  | If `True`, uses rectangular inference for batching, reducing padding and potentially increasing speed and efficiency by processing images in their original aspect ratio.                                                                                 |\n| `split`        | `str`       | `'val'` | Determines the dataset split to use for validation (`val`, `test`, or `train`). Allows flexibility in choosing the data segment for performance evaluation.                                                                                               |\n| `project`      | `str`       | `None`  | Name of the project directory where validation outputs are saved. Helps organize results from different experiments or models.                                                                                                                            |\n| `name`         | `str`       | `None`  | Name of the validation run. Used for creating a subdirectory within the project folder, where validation logs and outputs are stored.                                                                                                                     |\n| `verbose`      | `bool`      | `False` | If `True`, displays detailed information during the validation process, including per-class metrics, batch progress, and additional debugging information.                                                                                                |\n| `save_txt`     | `bool`      | `False` | If `True`, saves detection results in text files, with one file per image, useful for further analysis, custom post-processing, or integration with other systems.                                                                                        |\n| `save_conf`    | `bool`      | `False` | If `True`, includes confidence values in the saved text files when `save_txt` is enabled, providing more detailed output for analysis and filtering.                                                                                                      |\n| `save_crop`    | `bool`      | `False` | If `True`, saves cropped images of detected objects, which can be useful for creating focused datasets, visual verification, or further analysis of individual detections.                                                                                |\n| `workers`      | `int`       | `8`     | Number of worker threads for data loading. Higher values can speed up data preprocessing but may increase CPU usage. Setting to 0 uses main thread, which can be more stable in some environments.                                                        |\n| `augment`      | `bool`      | `False` | Enables test-time augmentation (TTA) during validation, potentially improving detection accuracy at the cost of inference speed by running inference on transformed versions of the input.                                                                |\n| `agnostic_nms` | `bool`      | `False` | Enables class-agnostic [Non-Maximum Suppression](https://www.ultralytics.com/glossary/non-maximum-suppression-nms), which merges overlapping boxes regardless of their predicted class. Useful for instance-focused applications.                         |\n| `single_cls`   | `bool`      | `False` | Treats all classes as a single class during validation. Useful for evaluating model performance on binary detection tasks or when class distinctions aren't important.                                                                                    |\n```\n\n----------------------------------------\n\nTITLE: Updating and Installing Pip on Raspberry Pi\nDESCRIPTION: Commands to update package list, install pip, and upgrade it to the latest version on Raspberry Pi.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/raspberry-pi.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt update\nsudo apt install python3-pip -y\npip install -U pip\n```\n\n----------------------------------------\n\nTITLE: Disabling Data Collection in Ultralytics YOLO using CLI\nDESCRIPTION: This snippet shows how to disable analytics and crash reporting in Ultralytics YOLO using the command-line interface. It also demonstrates how to reset settings to default values.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/help/privacy.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Disable analytics and crash reporting\nyolo settings sync=False\n\n# Reset settings to default values\nyolo settings reset\n```\n\n----------------------------------------\n\nTITLE: OBBValidator Class Reference\nDESCRIPTION: Documentation reference for the OBBValidator class which extends YOLO's DetectionValidator for oriented bounding box detection validation. Located in ultralytics/models/yolo/obb/val.py.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/models/yolo/obb/val.md#2025-04-22_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n::: ultralytics.models.yolo.obb.val.OBBValidator\n```\n\n----------------------------------------\n\nTITLE: Cloning Repositories for YOLOv8 MNN Setup\nDESCRIPTION: Commands to clone the Ultralytics and Alibaba MNN repositories for setup\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-MNN-CPP/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ultralytics/ultralytics.git\ncd ultralytics/examples/YOLOv8-MNN-CPP\n```\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/alibaba/MNN.git\ncd MNN\n```\n\n----------------------------------------\n\nTITLE: Citing YOLO12 in Academic Publications with BibTeX\nDESCRIPTION: This BibTeX snippet provides a citation entry for the original YOLOv12 research article as published on arXiv. It is used for referencing the core paper detailing the YOLOv12 architecture, innovations, and experimental results. To use, copy into your LaTeX bibliography file; primary required fields are author, title, journal, year, and arXiv identifier.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolo12.md#2025-04-22_snippet_2\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{tian2025yolov12,\\n  title={YOLOv12: Attention-Centric Real-Time Object Detectors},\\n  author={Tian, Yunjie and Ye, Qixiang and Doermann, David},\\n  journal={arXiv preprint arXiv:2502.12524},\\n  year={2025}\\n}\n```\n\n----------------------------------------\n\nTITLE: YOLOv7 Citation in BibTeX Format\nDESCRIPTION: BibTeX citation for the YOLOv7 research paper, used for academic references.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov7.md#2025-04-22_snippet_5\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{wang2022yolov7,\n  title={YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors},\n  author={Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},\n  journal={arXiv preprint arXiv:2207.02696},\n  year={2022}\n}\n```\n\n----------------------------------------\n\nTITLE: Pre-training Routine Start Callback\nDESCRIPTION: Callback function that executes at the start of pre-training routine to initialize WandB logging.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/callbacks/wb.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nultralytics.utils.callbacks.wb.on_pretrain_routine_start\n```\n\n----------------------------------------\n\nTITLE: Retrieving EXIF Image Size Metadata - Python\nDESCRIPTION: The exif_size utility extracts size information from an image's EXIF metadata, assisting in quickly obtaining dimensions without loading entire images. Accepts a file path or file-like object as input and returns (width, height) as output. Depends on PIL or similar libraries; may return None for files without valid EXIF data.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/data/utils.md#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n## ::: ultralytics.data.utils.exif_size\n```\n\n----------------------------------------\n\nTITLE: Running GUI Applications with Wayland\nDESCRIPTION: Command to run Ultralytics Docker container with Wayland display server access for GUI applications.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/docker-quickstart.md#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nxhost +local:docker && docker run -e DISPLAY=$DISPLAY \\\n  -v $XDG_RUNTIME_DIR/$WAYLAND_DISPLAY:/tmp/$WAYLAND_DISPLAY \\\n  --net=host -it --ipc=host ultralytics/ultralytics:latest\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package in Bash\nDESCRIPTION: These commands navigate to the cloned directory and install the Ultralytics package using pip. This step is necessary for exporting the model.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-LibTorch-CPP-Inference/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd ultralytics\npip install .\n```\n\n----------------------------------------\n\nTITLE: Setting MLflow Run Name via Environment Variable\nDESCRIPTION: Command to set a specific MLflow run name using an environment variable, providing better organization and identification of individual runs.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/mlflow.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport MLFLOW_RUN=YOUR_RUN_NAME\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics and DVCLive via pip (Bash)\nDESCRIPTION: Installs the necessary Python packages, 'ultralytics' and 'dvclive', using the pip package manager. This is the first step required to integrate DVCLive with Ultralytics YOLO11.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/dvc.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics dvclive\n```\n\n----------------------------------------\n\nTITLE: Rebooting Raspberry Pi\nDESCRIPTION: Command to reboot the Raspberry Pi device after installation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/raspberry-pi.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo reboot\n```\n\n----------------------------------------\n\nTITLE: Running Ultralytics Docker Container with X11 GUI Support in Bash\nDESCRIPTION: This command allows the Ultralytics Docker container to access the X11 socket for running GUI applications.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/docker-quickstart.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nxhost +local:docker && docker run -e DISPLAY=$DISPLAY \\\n  -v /tmp/.X11-unix:/tmp/.X11-unix \\\n  -v ~/.Xauthority:/root/.Xauthority \\\n  -it --ipc=host $t\n```\n\n----------------------------------------\n\nTITLE: Running Ultralytics Docker Container with GPU Support\nDESCRIPTION: Command to run the Ultralytics Docker image in a container with GPU support. This provides an isolated environment with access to GPU resources.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/quickstart.md#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n# Run the ultralytics image in a container with GPU support\nsudo docker run -it --ipc=host --gpus all ultralytics/ultralytics:latest\n```\n\n----------------------------------------\n\nTITLE: Fast Segment Anything Paper Citation - BibTeX\nDESCRIPTION: Provides a BibTeX entry to cite the FastSAM academic paper. For use in scholarly works referencing FastSAM. Outputs a standard BibTeX formatted citation block.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/fast-sam.md#2025-04-22_snippet_16\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{zhao2023fast,\n      title={Fast Segment Anything},\n      author={Xu Zhao and Wenchao Ding and Yongqi An and Yinglong Du and Tao Yu and Min Li and Ming Tang and Jinqiao Wang},\n      year={2023},\n      eprint={2306.12156},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n----------------------------------------\n\nTITLE: Citing Ultralytics YOLOv11 using BibTeX\nDESCRIPTION: Provides the BibTeX entry for citing the Ultralytics YOLOv11 software in research papers or other works. It includes essential metadata such as authors, title, version, year, URL, ORCID identifiers, and the software license (AGPL-3.0). A DOI is noted as pending.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolo11.md#2025-04-22_snippet_2\n\nLANGUAGE: bibtex\nCODE:\n```\n@software{yolo11_ultralytics,\n  author = {Glenn Jocher and Jing Qiu},\n  title = {Ultralytics YOLO11},\n  version = {11.0.0},\n  year = {2024},\n  url = {https://github.com/ultralytics/ultralytics},\n  orcid = {0000-0001-5950-6979, 0000-0002-7603-6750, 0000-0003-3783-7069},\n  license = {AGPL-3.0}\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package for YOLO11\nDESCRIPTION: Command to install the required Ultralytics package using pip package manager\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/seeedstudio-recamera.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics\n```\n\n----------------------------------------\n\nTITLE: Object Blurring using Ultralytics YOLO CLI Commands\nDESCRIPTION: CLI commands for blurring objects in images or videos using Ultralytics YOLO. These commands allow for basic blurring, specifying a source video, and blurring specific object classes.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/object-blurring.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Blur the objects\nyolo solutions blur show=True\n\n# Pass a source video\nyolo solutions blur source=\"path/to/video.mp4\"\n\n# Blur the specific classes\nyolo solutions blur classes=\"[0, 5]\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Arc GPU Benchmarks Image in Markdown\nDESCRIPTION: This code snippet displays an image of Arc GPU benchmarks using Markdown syntax. It centers the image and sets its width to 800 pixels.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/openvino.md#2025-04-22_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n<div align=\"center\">\n<img width=\"800\" src=\"https://github.com/ultralytics/docs/releases/download/0/arc-gpu-benchmarks.avif\" alt=\"Arc GPU benchmarks\">\n</div>\n```\n\n----------------------------------------\n\nTITLE: Enabling Offline Logging for Comet ML\nDESCRIPTION: Configures Comet ML to log experiment data locally for offline use during YOLO11 training.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/comet.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"COMET_MODE\"] = \"offline\"\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key for Ask AI Feature\nDESCRIPTION: Command to configure the OpenAI API key required for the Ask AI feature, which allows natural language queries of datasets within Ultralytics Explorer.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/index.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nyolo settings openai_api_key=\"YOUR_API_KEY\"\n```\n\n----------------------------------------\n\nTITLE: Running DeepStream Inference\nDESCRIPTION: Command to run inference using DeepStream application with the configured YOLO11 model\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/deepstream-nvidia-jetson.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndeepstream-app -c deepstream_app_config.txt\n```\n\n----------------------------------------\n\nTITLE: Creating Calibration Image List\nDESCRIPTION: Generates a text file containing paths to all calibration images\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/deepstream-nvidia-jetson.md#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nrealpath calibration/*jpg > calibration.txt\n```\n\n----------------------------------------\n\nTITLE: Defining YOLO-SAM Auto-Annotation Parameters in Markdown\nDESCRIPTION: This markdown table defines the configuration parameters for the YOLO-SAM auto-annotation process. It includes details on data input, model selection, computation settings, detection thresholds, and output specifications. Each parameter is described with its type, default value, and purpose.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/macros/sam-auto-annotate.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Argument     | Type        | Default        | Description                                                                          |\n| ------------ | ----------- | -------------- | ------------------------------------------------------------------------------------ |\n| `data`       | `str`       | required       | Path to directory containing target images for annotation or segmentation.           |\n| `det_model`  | `str`       | `'yolo11x.pt'` | YOLO detection model path for initial object detection.                              |\n| `sam_model`  | `str`       | `'sam_b.pt'`   | SAM model path for segmentation (supports SAM, SAM2 variants and mobile_sam models). |\n| `device`     | `str`       | `''`           | Computation device (e.g., 'cuda:0', 'cpu', or '' for automatic device detection).    |\n| `conf`       | `float`     | `0.25`         | YOLO detection confidence threshold for filtering weak detections.                   |\n| `iou`        | `float`     | `0.45`         | IoU threshold for Non-Maximum Suppression to filter overlapping boxes.               |\n| `imgsz`      | `int`       | `640`          | Input size for resizing images (must be multiple of 32).                             |\n| `max_det`    | `int`       | `300`          | Maximum number of detections per image for memory efficiency.                        |\n| `classes`    | `list[int]` | `None`         | List of class indices to detect (e.g., `[0, 1]` for person & bicycle).               |\n| `output_dir` | `str`       | `None`         | Save directory for annotations (defaults to './labels' relative to data path).       |\n```\n\n----------------------------------------\n\nTITLE: Configuring SKU-110K Dataset in YAML\nDESCRIPTION: YAML configuration file for the SKU-110K dataset, defining paths, classes, and other relevant information for training object detection models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/sku-110k.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/SKU-110K.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package via Conda\nDESCRIPTION: Command to install the Ultralytics package from the conda-forge channel.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/conda-quickstart.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c conda-forge ultralytics\n```\n\n----------------------------------------\n\nTITLE: Logging Scalars to TensorBoard in Python\nDESCRIPTION: This function logs scalar values to TensorBoard. It handles different data types and structures, including dictionaries and lists.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/callbacks/tensorboard.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nultralytics.utils.callbacks.tensorboard._log_scalars\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package\nDESCRIPTION: Installs the latest version of the ultralytics package from PyPI using pip. This is a prerequisite for training custom models with Ultralytics HUB.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/hub/models.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U ultralytics\n```\n\n----------------------------------------\n\nTITLE: Cloning Ultralytics Repository\nDESCRIPTION: Command to clone the Ultralytics repository from GitHub to the local machine using Git.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ultralytics/ultralytics.git\n```\n\n----------------------------------------\n\nTITLE: Installing YOLOv5 Environment on GCP\nDESCRIPTION: Basic commands to clone the YOLOv5 repository and install required dependencies on a GCP Deep Learning VM\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/environments/google_cloud_quickstart_tutorial.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Clone the YOLOv5 repository from GitHub\ngit clone https://github.com/ultralytics/yolov5\n\n# Navigate into the cloned repository directory\ncd yolov5\n\n# Install the required Python packages listed in requirements.txt\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Verifying NVIDIA Runtime in Docker using Bash\nDESCRIPTION: This command checks if the NVIDIA runtime is listed among the available Docker runtimes.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/docker-quickstart.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker info | grep -i runtime\n```\n\n----------------------------------------\n\nTITLE: Installing Albumentations Package for Ultralytics\nDESCRIPTION: This command installs the albumentations package, which is required for data augmentation in Ultralytics. Once installed, it should be automatically detected and used during training.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/yolo-data-augmentation.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install albumentations\n```\n\n----------------------------------------\n\nTITLE: Setting Up YOLOv5 Environment with Git and Pip\nDESCRIPTION: Commands to clone the YOLOv5 repository and install the required dependencies. Requires Python>=3.8.0 and PyTorch>=1.8 as prerequisites for the environment.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/train_custom_data.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ultralytics/yolov5 # Clone the repository\ncd yolov5\npip install -r requirements.txt # Install dependencies\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package (Bash)\nDESCRIPTION: Installs the necessary Ultralytics Python package using pip, which is required to perform model export and inference tasks.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tensorrt.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics\n```\n\n----------------------------------------\n\nTITLE: Natural Language AI Queries\nDESCRIPTION: Demonstrates AI-powered natural language queries for filtering dataset\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/explorer.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf = exp.ask_ai(\"show me images containing more than 10 objects with at least 2 persons\")\ndf.head(5)\n```\n\n----------------------------------------\n\nTITLE: Uninstalling TensorFlow for tflite-runtime\nDESCRIPTION: Commands to remove existing TensorFlow installation and install tflite-runtime on Raspberry Pi.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/coral-edge-tpu-on-raspberry-pi.md#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npip uninstall tensorflow tensorflow-aarch64\npip install -U tflite-runtime\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Models on DOTA Dataset (CLI)\nDESCRIPTION: Command-line interface command for training a YOLO11n-OBB model on the DOTA v1 dataset using Ultralytics CLI tools.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/obb/dota-v2.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Train a new YOLO11n-OBB model on the DOTAv1 dataset\nyolo obb train data=DOTAv1.yaml model=yolo11n-obb.pt epochs=100 imgsz=1024\n```\n\n----------------------------------------\n\nTITLE: Citing YOLO-World: BibTeX Reference\nDESCRIPTION: Provides a standard BibTeX entry for referencing the YOLO-World academic paper. This citation should be included in academic works that build on YOLO-World or Ultralytics research. The entry requires no code execution and is intended for LaTeX/BibTeX-compatible environments, serving solely documentation and citation purposes.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolo-world.md#2025-04-22_snippet_11\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{cheng2024yolow,\ntitle={YOLO-World: Real-Time Open-Vocabulary Object Detection},\nauthor={Cheng, Tianheng and Song, Lin and Ge, Yixiao and Liu, Wenyu and Wang, Xinggang and Shan, Ying},\njournal={arXiv preprint arXiv:2401.17270},\nyear={2024}\n}\n```\n\n----------------------------------------\n\nTITLE: Installing YOLO11 and DVCLive via PIP\nDESCRIPTION: Command to install the required Python packages for YOLO11 and DVCLive via pip.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/dvc.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the required packages for YOLO11 and DVCLive\npip install ultralytics dvclive\n```\n\n----------------------------------------\n\nTITLE: Installing Albumentations and Ultralytics for YOLO11\nDESCRIPTION: Command to install the required packages Albumentations and Ultralytics using pip.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/albumentations.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install albumentations ultralytics\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Exported RKNN Model in Python\nDESCRIPTION: Python code to load an exported RKNN model and run inference on an image. This demonstrates how to use the exported model for object detection on Rockchip devices.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/rockchip-rknn.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the exported RKNN model\nrknn_model = YOLO(\"./yolo11n_rknn_model\")\n\n# Run inference\nresults = rknn_model(\"https://ultralytics.com/images/bus.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Exporting and Running Inference with YOLO TensorRT INT8 Model via CLI\nDESCRIPTION: This code snippet shows how to export a YOLO model to TensorRT INT8 format and run inference using the command-line interface. It includes the export command with key parameters and a separate command for running inference.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tensorrt.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Export a YOLO11n PyTorch model to TensorRT format with INT8 quantization\nyolo export model=yolo11n.pt format=engine batch=8 workspace=4 int8=True data=coco.yaml # creates 'yolov8n.engine''\n\n# Run inference with the exported TensorRT quantized model\nyolo predict model=yolov8n.engine source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Visualizing Object Tracks in Video with YOLO and OpenCV\nDESCRIPTION: Script that demonstrates how to track objects across video frames and visualize their movement paths using Ultralytics YOLO and OpenCV. The code maintains a history of tracked object positions and draws polylines to show their trajectories, limited to 30 previous positions per object.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/track.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom collections import defaultdict\n\nimport cv2\nimport numpy as np\n\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolo11n.pt\")\nvideo_path = \"path/to/video.mp4\"\ncap = cv2.VideoCapture(video_path)\ntrack_history = defaultdict(lambda: [])\n\nwhile cap.isOpened():\n    success, frame = cap.read()\n    if success:\n        results = model.track(frame, persist=True)\n        boxes = results[0].boxes.xywh.cpu()\n        track_ids = results[0].boxes.id.int().cpu().tolist()\n        annotated_frame = results[0].plot()\n        for box, track_id in zip(boxes, track_ids):\n            x, y, w, h = box\n            track = track_history[track_id]\n            track.append((float(x), float(y)))\n            if len(track) > 30:\n                track.pop(0)\n            points = np.hstack(track).astype(np.int32).reshape((-1, 1, 2))\n            cv2.polylines(annotated_frame, [points], isClosed=False, color=(230, 230, 230), thickness=10)\n        cv2.imshow(\"YOLO11 Tracking\", annotated_frame)\n        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n            break\n    else:\n        break\ncap.release()\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Initializing Git and DVC Environment for DVCLive\nDESCRIPTION: Commands to set up a Git repository and initialize DVCLive for tracking machine learning experiments.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/dvc.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Initialize a Git repository\ngit init -q\n\n# Configure Git with your details\ngit config --local user.email \"you@example.com\"\ngit config --local user.name \"Your Name\"\n\n# Initialize DVCLive in your project\ndvc init -q\n\n# Commit the DVCLive setup to your Git repository\ngit commit -m \"DVC init\"\n```\n\n----------------------------------------\n\nTITLE: Installing DeepSparse with YOLO11 Support\nDESCRIPTION: Command to install the required packages for using DeepSparse with YOLO11 models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/neural-magic.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install deepsparse[yolov8]\n```\n\n----------------------------------------\n\nTITLE: Converting Model to ONNX\nDESCRIPTION: Command to convert YOLO11 model to ONNX format for DeepStream compatibility\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/deepstream-nvidia-jetson.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 export_yolo11.py -w yolo11s.pt\n```\n\n----------------------------------------\n\nTITLE: Installing NVIDIA Container Toolkit\nDESCRIPTION: Series of commands to install and configure NVIDIA Container Toolkit for GPU support in Docker containers.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/environments/docker_image_quickstart_tutorial.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Add NVIDIA package repositories (refer to official guide for latest setup)\ncurl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\\n  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \\\n  | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' \\\n    | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n\n# Update package list and install the toolkit\nsudo apt-get update\nsudo apt-get install -y nvidia-container-toolkit\n\n# Configure Docker to use the NVIDIA runtime\nsudo nvidia-ctk runtime configure --runtime=docker\n\n# Restart Docker service to apply changes\nsudo systemctl restart docker\n```\n\n----------------------------------------\n\nTITLE: Displaying Image Comparison in Markdown\nDESCRIPTION: This Markdown snippet displays an image comparing training from scratch vs. using transfer learning. It uses HTML-like syntax within Markdown for centering and sizing the image.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/steps-of-a-cv-project.md#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n<p align=\"center\">\n  <img width=\"100%\" src=\"https://github.com/ultralytics/docs/releases/download/0/training-from-scratch-vs-transfer-learning.avif\" alt=\"Training From Scratch Vs. Using Transfer Learning\">\n</p>\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for YOLO11 and MNN\nDESCRIPTION: Command-line instructions for installing the necessary packages to work with YOLO11 models and MNN format.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/mnn.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics\npip install MNN\n```\n\n----------------------------------------\n\nTITLE: Navigating to Example Directory in Bash\nDESCRIPTION: This command changes the directory to the C++ LibTorch inference example location.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-LibTorch-CPP-Inference/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/YOLOv8-LibTorch-CPP-Inference\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package for YOLO11\nDESCRIPTION: Command to install the required Ultralytics package for YOLO11 using pip.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tfjs.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics\n```\n\n----------------------------------------\n\nTITLE: Installing Comet ML Package\nDESCRIPTION: Command to install the Comet ML Python package using pip. This is the first step for integrating Comet with YOLOv5.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/comet_logging_integration.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install comet_ml\n```\n\n----------------------------------------\n\nTITLE: ClearML Validation End Callback\nDESCRIPTION: Callback function that executes at the end of validation\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/callbacks/clearml.md#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nultralytics.utils.callbacks.clearml.on_val_end\n```\n\n----------------------------------------\n\nTITLE: AGLU Activation Function Class Reference in Ultralytics\nDESCRIPTION: Class reference for the AGLU (Adaptive Gated Linear Unit) activation function module in Ultralytics neural networks. AGLU is a variant of GLU that adapts its behavior based on input.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/nn/modules/activation.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nultralytics.nn.modules.activation.AGLU\n```\n\n----------------------------------------\n\nTITLE: Overriding Configuration with Custom YAML File in Bash\nDESCRIPTION: Commands to create a copy of the default configuration file and use it with custom parameters for YOLO operations.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/cli.md#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nyolo copy-cfg\nyolo cfg=default_copy.yaml imgsz=320\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package for RKNN Export\nDESCRIPTION: Command to install the required Ultralytics package for YOLO11 model export to RKNN format. This needs to be run on an X86-based Linux machine as exporting on ARM64 Rockchip devices is not supported.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/rockchip-rknn.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the required package for YOLO11\npip install ultralytics\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics YOLO and Weights & Biases via CLI\nDESCRIPTION: Command line instructions for installing the required packages for Ultralytics YOLO and Weights & Biases, and enabling W&B logging for Ultralytics.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/weights-biases.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the required packages for Ultralytics YOLO and Weights & Biases\npip install -U ultralytics wandb\n\n# Enable W&B logging for Ultralytics\nyolo settings wandb=True\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Comet ML and YOLO11 Integration\nDESCRIPTION: Installs the necessary Python packages for integrating Comet ML with Ultralytics YOLO11, including ultralytics, comet_ml, torch, and torchvision.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/comet.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics comet_ml torch torchvision\n```\n\n----------------------------------------\n\nTITLE: Embedding YouTube Video for Dataset Contribution Guide\nDESCRIPTION: This code snippet shows how to embed a YouTube video specifically for the dataset contribution guide. It uses similar iframe parameters as the previous snippet but with a different video source and start time.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/index.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n<p align=\"center\">\n  <br>\n  <iframe loading=\"lazy\" width=\"720\" height=\"405\" src=\"https://www.youtube.com/embed/yMR7BgwHQ3g?start=427\"\n    title=\"YouTube video player\" frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    allowfullscreen>\n  </iframe>\n  <br>\n  <strong>Watch:</strong> How to Contribute to Ultralytics Datasets 🚀\n</p>\n```\n\n----------------------------------------\n\nTITLE: Installing NVIDIA JetPack on Jetson Devices\nDESCRIPTION: Command to update and install NVIDIA JetPack components on Jetson devices after flashing the system. This step is necessary for methods 3 and 4 of JetPack installation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/nvidia-jetson.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt update && sudo apt install nvidia-jetpack -y\n```\n\n----------------------------------------\n\nTITLE: Verifying NVIDIA Driver Installation in Bash\nDESCRIPTION: This command checks if NVIDIA drivers are properly installed on the system by running the nvidia-smi utility.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/docker-quickstart.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnvidia-smi\n```\n\n----------------------------------------\n\nTITLE: Citation BibTeX for VisDrone Dataset\nDESCRIPTION: BibTeX citation format for the research paper that introduced the VisDrone dataset, to be used when referencing the dataset in academic publications.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/visdrone.md#2025-04-22_snippet_3\n\nLANGUAGE: bibtex\nCODE:\n```\n@ARTICLE{9573394,\n  author={Zhu, Pengfei and Wen, Longyin and Du, Dawei and Bian, Xiao and Fan, Heng and Hu, Qinghua and Ling, Haibin},\n  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\n  title={Detection and Tracking Meet Drones Challenge},\n  year={2021},\n  volume={},\n  number={},\n  pages={1-1},\n  doi={10.1109/TPAMI.2021.3119563}}\n```\n\n----------------------------------------\n\nTITLE: Embedding the Chart.js Library in HTML\nDESCRIPTION: This HTML snippet embeds the Chart.js library and a custom benchmark JavaScript file for rendering performance charts. It also includes a <canvas> element with predefined dimensions to display model comparison data. Dependencies include the Chart.js library via a CDN.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov5.md#2025-04-22_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<script async src=\\\"https://cdn.jsdelivr.net/npm/chart.js\\\"></script>\\n<script defer src=\\\"../../javascript/benchmark.js\\\"></script>\\n<canvas id=\\\"modelComparisonChart\\\" width=\\\"1024\\\" height=\\\"400\\\" active-models='[\\\"YOLOv5\\\"]'></canvas>\n```\n\n----------------------------------------\n\nTITLE: Launching JupyterLab Server\nDESCRIPTION: Command to start the JupyterLab server. This should be executed in the directory containing your YOLO11 notebook files. The server will open automatically in your default web browser.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/jupyterlab.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\njupyter lab\n```\n\n----------------------------------------\n\nTITLE: Running YOLO11 Hyperparameter Tuning with Default Search Space Python\nDESCRIPTION: This snippet shows the canonical implementation for loading a YOLO11 model and initiating hyperparameter tuning using the default search space and the COCO8 dataset. It requires the ultralytics and ray Python packages. Primary parameters specified are the dataset and enabling Ray Tune. Outputs are the Ray Tune result grid. This is the recommended default usage.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/ray-tune.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\\n\\n# Load a YOLO11 model\\nmodel = YOLO(\"yolo11n.pt\")\\n\\n# Start tuning with the COCO8 dataset\\nresult_grid = model.tune(data=\"coco8.yaml\", use_ray=True)\n```\n\n----------------------------------------\n\nTITLE: Open Images V7 Dataset YAML Configuration\nDESCRIPTION: YAML configuration file for the Open Images V7 dataset that defines dataset paths, class names, and other settings required for training models on this dataset.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/open-images-v7.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/open-images-v7.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements for YOLOv8-OpenCV-ONNX Example\nDESCRIPTION: This command installs the necessary Python packages listed in the requirements.txt file.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-OpenCV-ONNX-Python/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Existing Coral Edge TPU Runtime\nDESCRIPTION: Commands to remove existing Coral Edge TPU runtime packages before installing a new version. Different commands are provided for standard and high-frequency versions.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/coral-edge-tpu-on-raspberry-pi.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# If you installed the standard version\nsudo apt remove libedgetpu1-std\n\n# If you installed the high frequency version\nsudo apt remove libedgetpu1-max\n```\n\n----------------------------------------\n\nTITLE: FAQ CLI Example for Training YOLO11n-pose\nDESCRIPTION: Command-line interface example from the FAQ section showing how to train a YOLO11n-pose model on the COCO8-Pose dataset using the Ultralytics CLI tool.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/coco8-pose.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nyolo pose train data=coco8-pose.yaml model=yolo11n-pose.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Running Ultralytics Docker Container with Wayland GUI Support in Bash\nDESCRIPTION: This command allows the Ultralytics Docker container to access the Wayland socket for running GUI applications.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/docker-quickstart.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nxhost +local:docker && docker run -e DISPLAY=$DISPLAY \\\n  -v $XDG_RUNTIME_DIR/$WAYLAND_DISPLAY:/tmp/$WAYLAND_DISPLAY \\\n  --net=host -it --ipc=host $t\n```\n\n----------------------------------------\n\nTITLE: Example Repository Structure for AGPL-3.0 Compliant YOLO Projects\nDESCRIPTION: Directory structure template showing the recommended organization for a project using Ultralytics YOLO, including license, documentation, source code, and other essential components.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/help/contributing.md#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nmy-yolo-project/\n│\n├── LICENSE               # Full AGPL-3.0 license text\n├── README.md             # Project description, setup, usage, license info & attribution\n├── pyproject.toml        # Dependencies (or requirements.txt)\n├── scripts/              # Training/inference scripts\n│   └── train.py\n├── src/                  # Your project's source code\n│   ├── __init__.py\n│   ├── data_loader.py\n│   └── model_wrapper.py  # Code interacting with YOLO\n├── tests/                # Unit/integration tests\n├── configs/              # YAML/JSON config files\n├── docker/               # Dockerfiles, if used\n│   └── Dockerfile\n└── .github/              # GitHub specific files (e.g., workflows for CI)\n    └── workflows/\n        └── ci.yml\n```\n\n----------------------------------------\n\nTITLE: Installing Jetson Stats for Monitoring System Performance\nDESCRIPTION: Commands to install and run the Jetson Stats application, which allows monitoring of system temperatures, CPU/GPU/RAM utilization, and provides the ability to change power modes and check JetPack information.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/nvidia-jetson.md#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt update\nsudo pip install jetson-stats\nsudo reboot\njtop\n```\n\n----------------------------------------\n\nTITLE: Running YOLOE Quick Inference via Command Line (Bash)\nDESCRIPTION: Executes prompt-free object detection using the YOLOE model (`yoloe-s.pt`) on a specified image (`image.jpg`) via the `yolo` command-line interface provided by Ultralytics. This is the simplest way to run inference without specifying target classes.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yoloe.md#2025-04-22_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nyolo predict model=yoloe-s.pt source=\"image.jpg\"\n```\n\n----------------------------------------\n\nTITLE: Including Chart.js and Benchmark Script in HTML\nDESCRIPTION: This HTML snippet includes two JavaScript files essential for rendering the performance chart. The first script tag loads the Chart.js library asynchronously from a CDN, providing the necessary functions for chart creation. The second script tag loads a local script named 'benchmark.js' using the 'defer' attribute, indicating that it should be executed after the document has been parsed but before the DOMContentLoaded event, likely containing the logic to initialize and populate the chart on the canvas element.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov8.md#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<script async src=\"https://cdn.jsdelivr.net/npm/chart.js\"></script>\n<script defer src=\"../../javascript/benchmark.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLOv5 Model to ONNX Format\nDESCRIPTION: This bash command shows how to export a YOLOv5 model to ONNX format using the export.py script from the YOLOv5 repository structure. It specifies the image size and ONNX opset version for compatibility.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-CPP-Inference/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Assuming you are in the 'ultralytics' base directory after cloning\npython export.py --weights yolov5s.pt --imgsz 640 480 --include onnx --opset 12 # Example: 640x480 resolution\n```\n\n----------------------------------------\n\nTITLE: Training YOLO Model on xView Dataset with CLI\nDESCRIPTION: Command-line interface command to train a YOLO model on the xView dataset. This snippet demonstrates how to start training from a pretrained model using the yolo CLI with the xView configuration.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/xview.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo detect train data=xView.yaml model=yolo11n.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Displaying Image Types in Markdown\nDESCRIPTION: This Markdown snippet displays an image showing different types of image annotation. It uses HTML-like syntax within Markdown for centering and sizing the image.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/steps-of-a-cv-project.md#2025-04-22_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n<p align=\"center\">\n  <img width=\"100%\" src=\"https://github.com/ultralytics/docs/releases/download/0/different-types-of-image-annotation.avif\" alt=\"Different Types of Image Annotation\">\n</p>\n```\n\n----------------------------------------\n\nTITLE: Installing ClearML Python Package\nDESCRIPTION: This command installs the ClearML Python package using pip, which is necessary for integrating ClearML with YOLOv5.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/clearml_logging_integration.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install clearml\n```\n\n----------------------------------------\n\nTITLE: ClearML Training End Callback\nDESCRIPTION: Callback function that executes at the end of model training\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/callbacks/clearml.md#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nultralytics.utils.callbacks.clearml.on_train_end\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages for YOLO11\nDESCRIPTION: Command line instructions for installing necessary Python packages including torch, opencv, and ultralytics for YOLO11 model training.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/ibm-watsonx.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the required packages\npip install torch torchvision torchaudio\npip install opencv-contrib-python-headless\npip install ultralytics==8.0.196\n```\n\n----------------------------------------\n\nTITLE: Module Documentation Note in Markdown\nDESCRIPTION: Markdown documentation header specifying the file location and contribution guidelines for the memory_attention.py module.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/models/sam/modules/memory_attention.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ndescription: Explore detailed documentation of various SAM 2 encoder modules such as MemoryAttentionLayer, MemoryAttention, available in Ultralytics' repository.\nkeywords: Ultralytics, SAM 2 encoder, MemoryAttentionLayer, MemoryAttention\n---\n\n# Reference for `ultralytics/models/sam/modules/memory_attention.py`\n```\n\n----------------------------------------\n\nTITLE: Uploading YOLOv5 Dataset to Comet Artifacts\nDESCRIPTION: Command to train YOLOv5 while uploading the dataset to Comet Artifacts for versioning and tracking, making it accessible for future experiments.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/comet_logging_integration.md#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\npython train.py \\\n  --img 640 \\\n  --batch 16 \\\n  --epochs 5 \\\n  --data coco128.yaml \\\n  --weights yolov5s.pt \\\n  --upload_dataset\n```\n\n----------------------------------------\n\nTITLE: Running Ultralytics Docker Container with CPU in Bash\nDESCRIPTION: This command runs the Ultralytics Docker container using only the CPU, with interactive mode and host IPC namespace sharing.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/docker-quickstart.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nsudo docker run -it --ipc=host $t\n```\n\n----------------------------------------\n\nTITLE: Custom Labels for Zero-Shot Action Recognition\nDESCRIPTION: Example command showing how to use custom action labels for zero-shot video classification with YOLOv8.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-Action-Recognition/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython action_recognition.py --source https://www.youtube.com/watch?v=dQw4w9WgXcQ --labels \"walking\" \"running\" \"jumping\"\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Build for MNN YOLO C++\nDESCRIPTION: Sets up a CMake build configuration for a C++ project using MNN framework. Defines minimum CMake version, project name, C++11 standard requirement, include/library directories, and creates two executables ('main' and 'main_interpreter') with MNN library dependencies.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-MNN-CPP/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.12)\nproject(mnn_yolo_cpp)\n\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++11\")\n\ninclude_directories(${CMAKE_CURRENT_LIST_DIR}/include/)\n\nlink_directories(${CMAKE_CURRENT_LIST_DIR}/libs)\n\nadd_executable(\"main\" \"${CMAKE_CURRENT_LIST_DIR}/main.cpp\")\nadd_executable(\"main_interpreter\" \"${CMAKE_CURRENT_LIST_DIR}/main_interpreter.cpp\")\n\ntarget_link_libraries(\"main\" MNN MNN_Express MNNOpenCV)\ntarget_link_libraries(\"main_interpreter\" MNN MNN_Express MNNOpenCV)\n```\n\n----------------------------------------\n\nTITLE: Combining YOLO Prediction Results and Frames with a Python Callback\nDESCRIPTION: This Python snippet shows how to use a custom callback (`on_predict_batch_end`) with an Ultralytics YOLO model to modify the prediction output. The callback function is executed at the end of each prediction batch. It accesses the batch's input image(s) (`predictor.batch`) and combines them with the corresponding prediction results (`predictor.results`) using `zip`. This modifies `predictor.results` so that iterating through the model's prediction output yields tuples containing both the result object and the original frame, useful for simultaneous processing or visualization.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/callbacks.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n\ndef on_predict_batch_end(predictor):\n    \"\"\"Combine prediction results with frames.\"\"\"\n    _, image, _, _ = predictor.batch\n    image = image if isinstance(image, list) else [image]\n    predictor.results = zip(predictor.results, image)\n\n\nmodel = YOLO(\"yolo11n.pt\")\nmodel.add_callback(\"on_predict_batch_end\", on_predict_batch_end)\nfor result, frame in model.predict():\n    pass\n```\n\n----------------------------------------\n\nTITLE: Modifying Ultralytics Settings in Python\nDESCRIPTION: Code showing how to disable analytics and crash reporting or reset settings to default values using Python. The update method allows for changing specific settings while reset returns all settings to their default values.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/help/privacy.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import settings\n\n# Disable analytics and crash reporting\nsettings.update({\"sync\": False})\n\n# Reset settings to default values\nsettings.reset()\n```\n\n----------------------------------------\n\nTITLE: Non-Thread-Safe Example: Multiple Model Instances in Python\nDESCRIPTION: This example shows another unsafe pattern where multiple YOLO model instances are shared across threads, which can still lead to concurrency issues if the internal implementation is not thread-safe.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/yolo-thread-safe-inference.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Unsafe: Sharing multiple model instances across threads can still lead to issues\nfrom threading import Thread\n\nfrom ultralytics import YOLO\n\n# Instantiate multiple models outside the thread\nshared_model_1 = YOLO(\"yolo11n_1.pt\")\nshared_model_2 = YOLO(\"yolo11n_2.pt\")\n\n\ndef predict(model, image_path):\n    \"\"\"Runs prediction on an image using a specified YOLO model, returning the results.\"\"\"\n    results = model.predict(image_path)\n    # Process results\n\n\n# Starting threads with individual model instances\nThread(target=predict, args=(shared_model_1, \"image1.jpg\")).start()\nThread(target=predict, args=(shared_model_2, \"image2.jpg\")).start()\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package for YOLO11 Export\nDESCRIPTION: Command to install the Ultralytics package required for exporting YOLO11 models to ONNX format.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/onnx.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the required package for YOLO11\npip install ultralytics\n```\n\n----------------------------------------\n\nTITLE: Running YOLOv8 Inference with CPU\nDESCRIPTION: Command to run YOLOv8 inference using an ONNX model on CPU with Rust.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-ONNXRuntime-Rust/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncargo run --release -- --model MODEL_PATH.onnx --source SOURCE_IMAGE.jpg\n```\n\n----------------------------------------\n\nTITLE: Testing Raspberry Pi Camera\nDESCRIPTION: This command tests the connected Raspberry Pi camera by displaying a live video feed for about 5 seconds.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/raspberry-pi.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nrpicam-hello\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Explorer Dependencies in Python\nDESCRIPTION: Installs the necessary dependencies for using the Ultralytics Explorer API via pip.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/api.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics[explorer]\n```\n\n----------------------------------------\n\nTITLE: Package Installation for JetPack 6.1\nDESCRIPTION: Commands for installing and updating Python packages on Jetson with JetPack 6.1, including pip installation and system updates.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/nvidia-jetson.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt update\nsudo apt install python3-pip -y\npip install -U pip\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics[export]\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo reboot\n```\n\n----------------------------------------\n\nTITLE: Running Inference with YOLO11 Model on Triton Server\nDESCRIPTION: This snippet shows how to use the Ultralytics YOLO API to load a model hosted on the Triton Inference Server and perform object detection on an image, leveraging Triton's optimizations while using the familiar YOLO interface.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/triton-inference-server.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the Triton Server model\nmodel = YOLO(\"http://localhost:8000/yolo\", task=\"detect\")\n\n# Run inference on the server\nresults = model(\"path/to/image.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Linking Local Benchmark JavaScript File in HTML\nDESCRIPTION: This HTML script tag includes a local JavaScript file located at `../../javascript/benchmark.js`. The `defer` attribute ensures the script is executed only after the HTML document has been completely parsed, making it suitable for manipulating DOM elements like the canvas. This script likely contains the custom logic for initializing and populating the comparison chart using the Chart.js library.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov7.md#2025-04-22_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<script defer src=\"../../javascript/benchmark.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: Example Repository Structure for AGPL-3.0 Compliant YOLO Projects\nDESCRIPTION: A directory tree showing the recommended file structure for organizing an AGPL-3.0 compliant project that uses Ultralytics YOLO. The structure includes essential components like license files, documentation, source code, and configuration files.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/CONTRIBUTING.md#2025-04-22_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\nmy-yolo-project/\n│\n├── LICENSE               # Full AGPL-3.0 license text\n├── README.md             # Project description, setup, usage, license info & attribution\n├── pyproject.toml        # Dependencies (or requirements.txt)\n├── scripts/              # Training/inference scripts\n│   └── train.py\n├── src/                  # Your project's source code\n│   ├── __init__.py\n│   ├── data_loader.py\n│   └── model_wrapper.py  # Code interacting with YOLO\n├── tests/                # Unit/integration tests\n├── configs/              # YAML/JSON config files\n├── docker/               # Dockerfiles, if used\n│   └── Dockerfile\n└── .github/              # GitHub specific files (e.g., workflows for CI)\n    └── workflows/\n        └── ci.yml\n```\n\n----------------------------------------\n\nTITLE: BibTeX Citation for CIFAR-100 Dataset\nDESCRIPTION: BibTeX format citation for referencing the CIFAR-100 dataset in research publications, acknowledging Alex Krizhevsky's work.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/cifar100.md#2025-04-22_snippet_2\n\nLANGUAGE: bibtex\nCODE:\n```\n@TECHREPORT{Krizhevsky09learningmultiple,\n            author={Alex Krizhevsky},\n            title={Learning multiple layers of features from tiny images},\n            institution={},\n            year={2009}\n}\n```\n\n----------------------------------------\n\nTITLE: YOLOv5 Export Process Output\nDESCRIPTION: Sample output of the YOLOv5 model export process, showing the steps and results of exporting to TorchScript and ONNX formats.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/model_export.md#2025-04-22_snippet_3\n\nLANGUAGE: output\nCODE:\n```\nexport: data=data/coco128.yaml, weights=['yolov5s.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, train=False, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=12, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['torchscript', 'onnx']\nYOLOv5 🚀 v6.2-104-ge3e5122 Python-3.8.0 torch-1.12.1+cu113 CPU\n\nDownloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5s.pt to yolov5s.pt...\n100% 14.1M/14.1M [00:00<00:00, 274MB/s]\n\nFusing layers...\nYOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n\nPyTorch: starting from yolov5s.pt with output shape (1, 25200, 85) (14.1 MB)\n\nTorchScript: starting export with torch 1.12.1+cu113...\nTorchScript: export success ✅ 1.7s, saved as yolov5s.torchscript (28.1 MB)\n\nONNX: starting export with onnx 1.12.0...\nONNX: export success ✅ 2.3s, saved as yolov5s.onnx (28.0 MB)\n\nExport complete (5.5s)\nResults saved to /content/yolov5\nDetect:          python detect.py --weights yolov5s.onnx\nValidate:        python val.py --weights yolov5s.onnx\nPyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'yolov5s.onnx')\nVisualize:       https://netron.app/\n```\n\n----------------------------------------\n\nTITLE: Installing Core Dependencies with pip\nDESCRIPTION: Installs the required dependencies using pip package manager from requirements.txt file.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/RTDETR-ONNXRuntime-Python/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Detection Models Performance Table in Markdown\nDESCRIPTION: Markdown table showing performance metrics for YOLO11 detection models trained on COCO dataset, including model size, mAP, speed, parameters, and FLOPs.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/README.md#2025-04-22_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n| Model | size<br><sup>(pixels) | mAP<sup>val<br>50-95 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n| ------------------------------------------------------------------------------------ | --------------------- | -------------------- | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\n| [YOLO11n](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt) | 640 | 39.5 | 56.1 ± 0.8 | 1.5 ± 0.0 | 2.6 | 6.5 |\n```\n\n----------------------------------------\n\nTITLE: Modifying YOLOv5 Training Script for Remote Execution with ClearML\nDESCRIPTION: This code snippet shows how to modify the YOLOv5 training script to enable remote execution using ClearML. It adds a line to execute the task remotely on a specified queue after the ClearML logger has been instantiated.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/clearml_logging_integration.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# ...\n# Loggers\ndata_dict = None\nif RANK in {-1, 0}:\n    loggers = Loggers(save_dir, weights, opt, hyp, LOGGER)  # loggers instance\n    if loggers.clearml:\n        loggers.clearml.task.execute_remotely(queue=\"my_queue\")  # <------ ADD THIS LINE\n        # Data_dict is either None is user did not choose for ClearML dataset or is filled in by ClearML\n        data_dict = loggers.clearml.data_dict\n# ...\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package for YOLO11 Model Export\nDESCRIPTION: This snippet provides the command to install the Ultralytics package, which is required for exporting YOLO11 models to TorchScript format. It uses pip to install the package from the Python Package Index.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/torchscript.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Install the required package for YOLO11\npip install ultralytics\n```\n\n----------------------------------------\n\nTITLE: Plotting AI Query Results\nDESCRIPTION: Visualizes results from natural language queries using helper functions\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/explorer.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom PIL import Image\nfrom ultralytics.data.explorer import plot_query_result\n\nplt = plot_query_result(exp.ask_ai(\"show me 10 images containing exactly 2 persons\"))\nImage.fromarray(plt)\n```\n\n----------------------------------------\n\nTITLE: Embedding YouTube Video in HTML\nDESCRIPTION: This HTML snippet embeds a YouTube video about defining computer vision project goals. It uses an iframe with specific attributes for responsive design and video playback controls.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/defining-project-goals.md#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<p align=\"center\">\n  <br>\n  <iframe loading=\"lazy\" width=\"720\" height=\"405\" src=\"https://www.youtube.com/embed/q1tXfShvbAw\"\n    title=\"YouTube video player\" frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    allowfullscreen>\n  </iframe>\n  <br>\n  <strong>Watch:</strong> How to define Computer Vision Project's Goal | Problem Statement and VisionAI Tasks Connection 🚀\n</p>\n```\n\n----------------------------------------\n\nTITLE: Iterating Detection Results\nDESCRIPTION: Processes detection results by iterating through predictions and extracting object contours, class labels, and image metadata.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/isolating-segmentation-objects.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\n\nimport numpy as np\n\n# (2) Iterate detection results (helpful for multiple images)\nfor r in res:\n    img = np.copy(r.orig_img)\n    img_name = Path(r.path).stem  # source image base-name\n\n    # Iterate each object contour (multiple detections)\n    for ci, c in enumerate(r):\n        # (1) Get detection class name\n        label = c.names[c.boxes.cls.tolist().pop()]\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Model and Logging Metrics with Comet ML\nDESCRIPTION: Initializes a YOLO11 model, trains it on the COCO128 dataset, and logs metrics to Comet ML.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/comet.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolo11n.pt\")\nresults = model.train(\n    data=\"coco8.yaml\",\n    project=\"comet-example-yolo11-coco128\",\n    batch=32,\n    save_period=1,\n    save_json=True,\n    epochs=3,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing YOLOv5 Requirements\nDESCRIPTION: Commands to clone the YOLOv5 repository and install the required dependencies from requirements.txt. This should be done in a Python >=3.8.0 environment with PyTorch >=1.8.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/model_pruning_and_sparsity.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ultralytics/yolov5 # clone\ncd yolov5\npip install -r requirements.txt # install\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package via CLI\nDESCRIPTION: Command to install the Ultralytics package using pip, which is required for exporting YOLO11 models to CoreML format.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/coreml.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics\n```\n\n----------------------------------------\n\nTITLE: Installing Sixel Library via pip\nDESCRIPTION: Command to install the python-sixel library for terminal image display\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/view-results-in-terminal.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install sixel\n```\n\n----------------------------------------\n\nTITLE: Killing All MLflow Server Instances\nDESCRIPTION: Bash command sequence to find and terminate all running MLflow server processes, useful for cleaning up server instances when they're no longer needed.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/mlflow.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nps aux | grep 'mlflow' | grep -v 'grep' | awk '{print $2}' | xargs kill -9\n```\n\n----------------------------------------\n\nTITLE: Including Attribution for Ultralytics YOLO in Project Documentation\nDESCRIPTION: A Markdown code snippet showing how to properly attribute the use of Ultralytics YOLO in your project's documentation. This acknowledgment is required as part of AGPL-3.0 compliance.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/CONTRIBUTING.md#2025-04-22_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\nThis project utilizes code from [Ultralytics YOLO](https://github.com/ultralytics/ultralytics), licensed under AGPL-3.0.\n```\n\n----------------------------------------\n\nTITLE: Logging Class-Level Metrics in YOLOv5 to Comet\nDESCRIPTION: Command to enable logging of per-class metrics (mAP, precision, recall, f1) to Comet for detailed model performance analysis.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/comet_logging_integration.md#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nenv COMET_LOG_PER_CLASS_METRICS=true python train.py \\\n  --img 640 \\\n  --batch 16 \\\n  --epochs 5 \\\n  --data coco128.yaml \\\n  --weights yolov5s.pt\n```\n\n----------------------------------------\n\nTITLE: Citation for Caltech-101 Dataset in BibTeX Format\nDESCRIPTION: This BibTeX entry provides the proper citation for the Caltech-101 dataset, referencing the original paper by Li Fei-Fei, Rob Fergus, and Pietro Perona published in the Computer Vision and Image Understanding journal in 2007.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/caltech101.md#2025-04-22_snippet_2\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{fei2007learning,\n  title={Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories},\n  author={Fei-Fei, Li and Fergus, Rob and Perona, Pietro},\n  journal={Computer vision and Image understanding},\n  volume={106},\n  number={1},\n  pages={59--70},\n  year={2007},\n  publisher={Elsevier}\n}\n```\n\n----------------------------------------\n\nTITLE: Fashion-MNIST Label Classes\nDESCRIPTION: List of class labels used in the Fashion-MNIST dataset for classifying different clothing items and accessories.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/fashion-mnist.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n0. T-shirt/top\n1. Trouser\n2. Pullover\n3. Dress\n4. Coat\n5. Sandal\n6. Shirt\n7. Sneaker\n8. Bag\n9. Ankle boot\n```\n\n----------------------------------------\n\nTITLE: Loading YOLOv5 Models for Training\nDESCRIPTION: Shows how to load YOLOv5 models for training rather than inference, with options for either pretrained weights or random initialization for training from scratch. Requires a custom training script.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/pytorch_hub_model_loading.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", autoshape=False)  # load pretrained\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", autoshape=False, pretrained=False)  # load scratch\n```\n\n----------------------------------------\n\nTITLE: Importing Ultralytics Heatmap Class\nDESCRIPTION: Shows the reference path to the Heatmap class implementation in the Ultralytics solutions package. This class provides functionality for generating heatmaps from object tracking data in real-time video analysis.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/solutions/heatmap.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nultralytics.solutions.heatmap.Heatmap\n```\n\n----------------------------------------\n\nTITLE: Installing DeepSparse Package\nDESCRIPTION: Installs DeepSparse with server, YOLO, and ONNX runtime dependencies using pip.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/neural_magic_pruning_quantization.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"deepsparse[server,yolo,onnxruntime]\"\n```\n\n----------------------------------------\n\nTITLE: Setting ONNXRuntime Library Path in Linux/macOS for YOLO Rust Integration\nDESCRIPTION: Command to export the ORT_DYLIB_PATH environment variable pointing to the ONNX Runtime library location. This is required for manual linking of the ONNXRuntime to the Rust application.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLO-Series-ONNXRuntime-Rust/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# Example path, replace with your actual path\nexport ORT_DYLIB_PATH=/path/to/onnxruntime/lib/libonnxruntime.so.1.19.0\n```\n\n----------------------------------------\n\nTITLE: Installing YOLOv5 Dependencies in AzureML\nDESCRIPTION: This snippet demonstrates how to install the necessary Python packages for YOLOv5, including the core dependencies from requirements.txt and ONNX for model export capabilities.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/environments/azureml_quickstart_tutorial.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt # Install core dependencies\npip install onnx > =1.12.0      # Install ONNX for exporting\n```\n\n----------------------------------------\n\nTITLE: Implementing Ray Tune Callback for YOLO Epoch End in Python\nDESCRIPTION: This function is a callback that reports training metrics to Ray Tune at the end of each epoch during YOLO model training. It extracts relevant metrics from the trainer object and reports them to Ray Tune for hyperparameter optimization.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/callbacks/raytune.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef on_fit_epoch_end(trainer):\n    \"\"\"Reports metrics to Ray tune at the end of each epoch.\"\"\"\n    if not trainer.metrics:\n        return\n    from ray import tune\n\n    metrics = trainer.validator.metrics if trainer.validator else trainer.metrics\n    result = {k.lower(): float(v) for k, v in metrics.items() if k in ['fitness', 'mAP50-95(B)', 'mAP50-95(M)']}\n    tune.report(**result)\n```\n\n----------------------------------------\n\nTITLE: HTML Social Media Links in Markdown\nDESCRIPTION: A centered div containing social media icon links for various Ultralytics social media channels including GitHub, LinkedIn, Twitter, YouTube, TikTok, BiliBili and Discord. Each icon is sized at 3% width with transparent spacers between them.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/README.md#2025-04-22_snippet_8\n\nLANGUAGE: HTML\nCODE:\n```\n<div align=\"center\">\n  <a href=\"https://github.com/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png\" width=\"3%\" alt=\"Ultralytics GitHub\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://www.linkedin.com/company/ultralytics/\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png\" width=\"3%\" alt=\"Ultralytics LinkedIn\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://twitter.com/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png\" width=\"3%\" alt=\"Ultralytics Twitter\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://youtube.com/ultralytics?sub_confirmation=1\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png\" width=\"3%\" alt=\"Ultralytics YouTube\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://www.tiktok.com/@ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png\" width=\"3%\" alt=\"Ultralytics TikTok\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://ultralytics.com/bilibili\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png\" width=\"3%\" alt=\"Ultralytics BiliBili\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://discord.com/invite/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png\" width=\"3%\" alt=\"Ultralytics Discord\"></a>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Markdown Document Structure\nDESCRIPTION: The main document structure showing different sections including data gathering, labeling, and management processes for YOLO models using Roboflow tools.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/roboflow.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Roboflow Integration\n\n- [Gather Data for Training a Custom YOLO11 Model](#gather-data-for-training-a-custom-yolo11-model)\n- [Upload, Convert and Label Data for YOLO11 Format](#upload-convert-and-label-data-for-yolo11-format)\n- [Pre-process and Augment Data for Model Robustness](#pre-process-and-augment-data-for-model-robustness)\n- [Dataset Management for YOLO11](#dataset-management-for-yolo11)\n- [Export Data in 40+ Formats for Model Training](#export-data-in-40-formats-for-model-training)\n- [Upload Custom YOLO11 Model Weights for Testing and Deployment](#upload-custom-yolo11-model-weights-for-testing-and-deployment)\n- [How to Evaluate YOLO11 Models](#how-to-evaluate-yolo11-models)\n- [Learning Resources](#learning-resources)\n- [Project Showcase](#project-showcase)\n- [FAQ](#faq)\n```\n\n----------------------------------------\n\nTITLE: Running Ultralytics with GPU Support\nDESCRIPTION: Command to run Ultralytics in a Docker container with GPU support enabled.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/docker-quickstart.md#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nsudo docker run -it --ipc=host --gpus all ultralytics/ultralytics:latest\n```\n\n----------------------------------------\n\nTITLE: Installing JupyterLab via pip\nDESCRIPTION: Command to install JupyterLab using pip package manager. This is the first step required before working with YOLO11 models in JupyterLab.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/jupyterlab.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install jupyterlab\n```\n\n----------------------------------------\n\nTITLE: TensorBoard Callback for Training Start in Python\nDESCRIPTION: This callback function is executed at the start of the training process. It may initialize TensorBoard logging for the training session.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/callbacks/tensorboard.md#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nultralytics.utils.callbacks.tensorboard.on_train_start\n```\n\n----------------------------------------\n\nTITLE: Syncing Dataset with ClearML\nDESCRIPTION: This command syncs a local dataset folder with ClearML, creating a versioned dataset in the ClearML system.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/clearml_logging_integration.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nclearml-data sync --project YOLOv5 --name coco128 --folder .\n```\n\n----------------------------------------\n\nTITLE: Configuring Comet via Config File\nDESCRIPTION: Example of a .comet.config file content that sets the API key and project name for Comet integration.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/comet_logging_integration.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n[comet]\napi_key=YOUR_API_KEY\nproject_name=YOUR_COMET_PROJECT_NAME # This will default to 'yolov5'\n```\n\n----------------------------------------\n\nTITLE: Configuring Comet Logging with Environment Variables\nDESCRIPTION: Environment variable settings to customize Comet's logging behavior, including mode, model name, visualization options, and prediction logging.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/comet_logging_integration.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nexport COMET_MODE=online                           # Set whether to run Comet in 'online' or 'offline' mode. Defaults to online\nexport COMET_MODEL_NAME=\"yolov5\"                   # Set the name for the saved model. Defaults to yolov5\nexport COMET_LOG_CONFUSION_MATRIX=false            # Set to disable logging a Comet Confusion Matrix. Defaults to true\nexport COMET_MAX_IMAGE_UPLOADS=30                  # Controls how many total image predictions to log to Comet. Defaults to 100.\nexport COMET_LOG_PER_CLASS_METRICS=true            # Set to log evaluation metrics for each detected class at the end of training. Defaults to false\nexport COMET_DEFAULT_CHECKPOINT_FILENAME=\"last.pt\" # Set this if you would like to resume training from a different checkpoint. Defaults to 'last.pt'\nexport COMET_LOG_BATCH_LEVEL_METRICS=true          # Set this if you would like to log training metrics at the batch level. Defaults to false.\nexport COMET_LOG_PREDICTIONS=true                  # Set this to false to disable logging model predictions\n```\n\n----------------------------------------\n\nTITLE: Navigating to Ultralytics Directory\nDESCRIPTION: Command to change the current working directory to the cloned Ultralytics repository.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd ultralytics\n```\n\n----------------------------------------\n\nTITLE: Installing YOLO11 and Comet ML Dependencies\nDESCRIPTION: Command to install the required Python packages for using YOLO11 with Comet ML integration\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/comet.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics comet_ml torch torchvision\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Explorer Dependencies\nDESCRIPTION: Command to install optional dependencies required for Ultralytics Explorer functionality, including embedding/semantic search and SQL querying powered by LanceDB.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/index.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics[explorer]\n```\n\n----------------------------------------\n\nTITLE: Caltech-256 Dataset Citation in BibTeX\nDESCRIPTION: BibTeX citation format for referencing the Caltech-256 dataset in academic work.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/caltech256.md#2025-04-22_snippet_2\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{griffin2007caltech,\n         title={Caltech-256 object category dataset},\n         author={Griffin, Gregory and Holub, Alex and Perona, Pietro},\n         year={2007}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Project Settings\nDESCRIPTION: Initializes CMake project with minimum version requirement and sets C++17 standard settings.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-LibTorch-CPP-Inference/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.18 FATAL_ERROR)\n\nproject(yolov8_libtorch_example)\n\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\nset(CMAKE_CXX_EXTENSIONS OFF)\n```\n\n----------------------------------------\n\nTITLE: Starting ClearML Agent for Remote Execution\nDESCRIPTION: This command starts a ClearML agent that listens to specified queues for remote task execution.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/clearml_logging_integration.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nclearml-agent daemon --queue QUEUES_TO_LISTEN_TO [--docker]\n```\n\n----------------------------------------\n\nTITLE: Citation Format for Crack Segmentation Dataset in BibTeX\nDESCRIPTION: The BibTeX citation format for properly acknowledging the Crack Segmentation Dataset in academic research or publications. Includes dataset title, author, publisher, and access information.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/segment/crack-seg.md#2025-04-22_snippet_3\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{ crack-bphdr_dataset,\n    title = { crack Dataset },\n    type = { Open Source Dataset },\n    author = { University },\n    url = { https://universe.roboflow.com/university-bswxt/crack-bphdr },\n    journal = { Roboflow Universe },\n    publisher = { Roboflow },\n    year = { 2022 },\n    month = { dec },\n    note = { visited on 2024-01-23 },\n}\n```\n\n----------------------------------------\n\nTITLE: Embedding YouTube Video in HTML\nDESCRIPTION: HTML code for embedding a YouTube video about maintaining computer vision models, with responsive width and height settings and standard YouTube iframe parameters.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/model-monitoring-and-maintenance.md#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<iframe loading=\"lazy\" width=\"720\" height=\"405\" src=\"https://www.youtube.com/embed/zCupPHqSLTI\"\n    title=\"YouTube video player\" frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    allowfullscreen>\n</iframe>\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 Model to NCNN with Ultralytics (Python)\nDESCRIPTION: This Python snippet demonstrates how to use the Ultralytics YOLO interface to load a YOLO11 model checkpoint and export it into the NCNN format using the model's export method. Dependency on the 'ultralytics' Python library is required. The key parameters are the model checkpoint path (e.g., 'yolo11n.pt') and the 'format' argument set to 'ncnn'. The function creates an exported NCNN model directory (e.g., '/yolo11n_ncnn_model'), and the output can be used for cross-platform deployment. No explicit input/output is required beyond file paths, and the operation assumes the model file is accessible.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/ncnn.md#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export to NCNN format\nmodel.export(format=\"ncnn\")  # creates '/yolo11n_ncnn_model'\n```\n\n----------------------------------------\n\nTITLE: Displaying Flex GPU Benchmarks Image in Markdown\nDESCRIPTION: This code snippet displays an image of Flex GPU benchmarks using Markdown syntax. It centers the image and sets its width to 800 pixels.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/openvino.md#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n<div align=\"center\">\n<img width=\"800\" src=\"https://github.com/ultralytics/docs/releases/download/0/flex-gpu-benchmarks.avif\" alt=\"Flex GPU benchmarks\">\n</div>\n```\n\n----------------------------------------\n\nTITLE: Generating Parameter Table using Jinja2 Macro\nDESCRIPTION: This macro creates a markdown table of parameters. It can use a predefined set of parameters or accept custom ones. The table includes argument names, types, default values, and descriptions.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/macros/visualization-args.md#2025-04-22_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n{% macro param_table(params=None) %}\n\n| Argument | Type | Default | Description |\n| -------- | ---- | ------- | ----------- |\n\n{%- set default_params = {\n    \"show\": [\"bool\", \"False\", \"If `True`, displays the annotated images or videos in a window. Useful for immediate visual feedback during development or testing.\"],\n    \"save\": [\"bool\", \"False or True\", \"Enables saving of the annotated images or videos to file. Useful for documentation, further analysis, or sharing results. Defaults to True when using CLI & False when used in Python.\"],\n    \"save_frames\": [\"bool\", \"False\", \"When processing videos, saves individual frames as images. Useful for extracting specific frames or for detailed frame-by-frame analysis.\"],\n    \"save_txt\": [\"bool\", \"False\", \"Saves detection results in a text file, following the format `[class] [x_center] [y_center] [width] [height] [confidence]`. Useful for integration with other analysis tools.\"],\n    \"save_conf\": [\"bool\", \"False\", \"Includes confidence scores in the saved text files. Enhances the detail available for post-processing and analysis.\"],\n    \"save_crop\": [\"bool\", \"False\", \"Saves cropped images of detections. Useful for dataset augmentation, analysis, or creating focused datasets for specific objects.\"],\n    \"show_labels\": [\"bool\", \"True\", \"Displays labels for each detection in the visual output. Provides immediate understanding of detected objects.\"],\n    \"show_conf\": [\"bool\", \"True\", \"Displays the confidence score for each detection alongside the label. Gives insight into the model's certainty for each detection.\"],\n    \"show_boxes\": [\"bool\", \"True\", \"Draws bounding boxes around detected objects. Essential for visual identification and location of objects in images or video frames.\"],\n    \"line_width\": [\"None or int\", \"None\", \"Specifies the line width of bounding boxes. If `None`, the line width is automatically adjusted based on the image size. Provides visual customization for clarity.\"],\n    \"font_size\": [\"float\", \"None\", \"Text font size for annotations. Scales automatically with image size if set to `None`.\"],\n    \"font\": [\"str\", \"'Arial.ttf'\", \"Font name or path for text annotations in the visualization.\"],\n    \"pil\": [\"bool\", \"False\", \"Return image as a PIL Image object instead of numpy array.\"],\n    \"kpt_radius\": [\"int\", \"5\", \"Radius of keypoints when visualizing pose estimation results.\"],\n    \"kpt_line\": [\"bool\", \"True\", \"Connect keypoints with lines when visualizing pose estimation.\"],\n    \"masks\": [\"bool\", \"True\", \"Display segmentation masks in the visualization output.\"],\n    \"probs\": [\"bool\", \"True\", \"Include classification probabilities in the visualization.\"],\n    \"filename\": [\"str\", \"None\", \"Path and filename to save the annotated image when `save=True`.\"],\n    \"color_mode\": [\"str\", \"'class'\", \"Specify the coloring mode for visualizations, e.g., 'instance' or 'class'.\"],\n    \"txt_color\": [\"tuple[int, int, int]\", \"(255, 255, 255)\", \"RGB text color for classification task annotations.\"]\n} %}\n\n{%- if not params %}\n{%- for param, details in default_params.items() %}\n| `{{ param }}` | `{{ details[0] }}` | `{{ details[1] }}` | {{ details[2] }} |\n{%- endfor %}\n{%- else %}\n{%- for param in params %}\n{%- if param in default_params %}\n| `{{ param }}` | `{{ default_params[param][0] }}` | `{{ default_params[param][1] }}` | {{ default_params[param][2] }} |\n{%- endif %}\n{%- endfor %}\n{%- endif %}\n\n{% endmacro %}\n```\n\n----------------------------------------\n\nTITLE: Running Inference using Streamlit with Ultralytics YOLO (CLI)\nDESCRIPTION: CLI commands to run inference using Streamlit with Ultralytics YOLO. Includes options for using a default model or specifying a custom model path.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/streamlit-live-inference.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyolo solutions inference\n\nyolo solutions inference model=\"path/to/model.pt\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Search Engine Crawling Rules for Ultralytics Documentation Site\nDESCRIPTION: This robots.txt file sets up crawling rules for search engines and provides sitemap URLs for the Ultralytics documentation site in various languages. It allows all user agents to crawl the site and lists 13 sitemap URLs.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/robots.txt#2025-04-22_snippet_0\n\nLANGUAGE: robots.txt\nCODE:\n```\nUser-agent: *\nSitemap: https://docs.ultralytics.com/sitemap.xml\nSitemap: https://docs.ultralytics.com/ar/sitemap.xml\nSitemap: https://docs.ultralytics.com/de/sitemap.xml\nSitemap: https://docs.ultralytics.com/es/sitemap.xml\nSitemap: https://docs.ultralytics.com/fr/sitemap.xml\nSitemap: https://docs.ultralytics.com/it/sitemap.xml\nSitemap: https://docs.ultralytics.com/ja/sitemap.xml\nSitemap: https://docs.ultralytics.com/ko/sitemap.xml\nSitemap: https://docs.ultralytics.com/pt/sitemap.xml\nSitemap: https://docs.ultralytics.com/ru/sitemap.xml\nSitemap: https://docs.ultralytics.com/tr/sitemap.xml\nSitemap: https://docs.ultralytics.com/vi/sitemap.xml\nSitemap: https://docs.ultralytics.com/zh/sitemap.xml\n```\n\n----------------------------------------\n\nTITLE: Staging Language Markdown Files\nDESCRIPTION: Git command to stage all new or modified Markdown files in the docs directory for multi-language builds.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit add docs/**/*.md -f\n```\n\n----------------------------------------\n\nTITLE: Validating YOLO11n Model on COCO8 Dataset using CLI\nDESCRIPTION: This command validates the YOLO11n model on the COCO8 validation dataset. It demonstrates how to use the YOLO CLI for model validation tasks.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nyolo val model=yolo11n.pt data=coco8.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing tflite-runtime via pip on Edge Devices - Bash\nDESCRIPTION: This snippet shows the pip install command to retrieve and install the minimal TensorFlow Lite interpreter ('tflite-runtime'), which is suited for devices with limited resources. Run this command in your terminal to set up only what's necessary for TFLite inference in Python, minimizing disk space and dependency requirements. No additional configuration is required beyond this installation for basic TFLite execution.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-TFLite-Python/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install tflite-runtime\n\n```\n\n----------------------------------------\n\nTITLE: Building Multi-Language Documentation\nDESCRIPTION: Script to build all language versions of the documentation into the /site directory, including clearing previous builds and handling root-level files.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Clear existing /site directory to prevent conflicts\nrm -rf site\n\n# Build the default language site using the primary config file\nmkdocs build -f docs/mkdocs.yml\n\n# Loop through each language-specific config file and build its site\nfor file in docs/mkdocs_*.yml; do\n  echo \"Building MkDocs site with $file\"\n  mkdocs build -f \"$file\"\ndone\n```\n\n----------------------------------------\n\nTITLE: Citing YOLOv9 Research in BibTeX Format\nDESCRIPTION: Provides the BibTeX citation for the YOLOv9 article published on arXiv. This citation includes details such as the authors, title, and the paper's source. The BibTeX entry is useful for referencing the YOLOv9 paper in academic publications.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov9.md#2025-04-22_snippet_4\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{wang2024yolov9,\n  title={YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information},\n  author={Wang, Chien-Yao  and Liao, Hong-Yuan Mark},\n  booktitle={arXiv preprint arXiv:2402.13616},\n  year={2024}\n}\n```\n\n----------------------------------------\n\nTITLE: Single-Line Python Docstring Example\nDESCRIPTION: Demonstrates a concise single-line docstring format for simple Python functions with type hints.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/help/contributing.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef example_small_function(arg1: int, arg2: int = 4) -> bool:\n    \"\"\"Example function with a single-line docstring.\"\"\"\n    return arg1 == arg2\n```\n\n----------------------------------------\n\nTITLE: Minimal Gradio Implementation for YOLO11 Object Detection\nDESCRIPTION: A simplified version of the Gradio interface for Ultralytics YOLO11 object detection. This minimal example demonstrates the core components needed to create a functional interface without additional features. It includes model initialization, prediction function, and basic interface setup.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/gradio.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport gradio as gr\n\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolo11n.pt\")\n\n\ndef predict_image(img, conf_threshold, iou_threshold):\n    results = model.predict(\n        source=img,\n        conf=conf_threshold,\n        iou=iou_threshold,\n        show_labels=True,\n        show_conf=True,\n    )\n    return results[0].plot() if results else None\n\n\niface = gr.Interface(\n    fn=predict_image,\n    inputs=[\n        gr.Image(type=\"pil\", label=\"Upload Image\"),\n        gr.Slider(minimum=0, maximum=1, value=0.25, label=\"Confidence threshold\"),\n        gr.Slider(minimum=0, maximum=1, value=0.45, label=\"IoU threshold\"),\n    ],\n    outputs=gr.Image(type=\"pil\", label=\"Result\"),\n    title=\"Ultralytics Gradio YOLO11\",\n    description=\"Upload images for YOLO11 object detection.\",\n)\niface.launch()\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset using Kaggle API\nDESCRIPTION: Python code to set up Kaggle credentials and download the marine litter dataset for model training.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/ibm-watsonx.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Replace \"username\" string with your username\nos.environ[\"KAGGLE_USERNAME\"] = \"username\"\n# Replace \"apiKey\" string with your key\nos.environ[\"KAGGLE_KEY\"] = \"apiKey\"\n\n# Load dataset\nos.system(\"kaggle datasets download atiqishrak/trash-dataset-icra19 --unzip\")\n\n# Store working directory path as work_dir\nwork_dir = os.getcwd()\n\n# Print work_dir path\nprint(os.getcwd())\n\n# Print work_dir contents\nprint(os.listdir(f\"{work_dir}\"))\n\n# Print trash_ICRA19 subdirectory contents\nprint(os.listdir(f\"{work_dir}/trash_ICRA19\"))\n```\n\n----------------------------------------\n\nTITLE: Creating YOLOv5 Model Directly on a Specific Device\nDESCRIPTION: Shows how to load a YOLOv5 model directly onto a specific device during creation, which can optimize memory usage and performance for different deployment scenarios.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/pytorch_hub_model_loading.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", device=\"cpu\")  # load on CPU\n```\n\n----------------------------------------\n\nTITLE: Single-Line Function Documentation Example\nDESCRIPTION: Example of a simple function with a concise single-line docstring following Google style guidelines\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/CONTRIBUTING.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef example_small_function(arg1: int, arg2: int = 4) -> bool:\n    \"\"\"Example function with a single-line docstring.\"\"\"\n    return arg1 == arg2\n```\n\n----------------------------------------\n\nTITLE: Serving Multi-Language Documentation Locally\nDESCRIPTION: Commands to navigate to the build output directory and start a Python HTTP server for previewing the complete multi-language site locally.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncd site\npython -m http.server\n```\n\n----------------------------------------\n\nTITLE: Defining Core Dependencies for Ultralytics YOLO\nDESCRIPTION: Lists the core Python package dependencies required for Ultralytics YOLO. The file specifies that 'ultralytics' and 'transformers' packages are needed for the project to function properly.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-Action-Recognition/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nultralytics\ntransformers\n```\n\n----------------------------------------\n\nTITLE: Installing Full TensorFlow Package for CPU-only Inference - Bash\nDESCRIPTION: This command installs the complete TensorFlow package, including the TFLite interpreter, for users who do not require GPU acceleration. Use it when you want the full suite of TensorFlow features in addition to TFLite, such as extra ops and tools. No extra parameters are required for CPU support; ensure Python and pip are updated for best compatibility.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-TFLite-Python/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install tensorflow\n\n```\n\n----------------------------------------\n\nTITLE: Installing Kaggle API\nDESCRIPTION: Command to install the Kaggle API for dataset access.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/ibm-watsonx.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Install kaggle\npip install kaggle\n```\n\n----------------------------------------\n\nTITLE: Creating Vector Index for LanceDB Table in Ultralytics Explorer\nDESCRIPTION: Shows how to create a vector index for faster querying on large datasets using the LanceDB table in Ultralytics Explorer.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/api.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntable.create_index(num_partitions=..., num_sub_vectors=...)\n```\n\n----------------------------------------\n\nTITLE: Checking Graphics Server in Bash\nDESCRIPTION: This command checks which graphics server (X11 or Wayland) is currently in use on the system.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/docker-quickstart.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nenv | grep -E -i 'x11|xorg|wayland'\n```\n\n----------------------------------------\n\nTITLE: COCO Dataset Citation in BibTeX\nDESCRIPTION: BibTeX citation format for referencing the COCO dataset in academic work.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/coco.md#2025-04-22_snippet_3\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{lin2015microsoft,\n      title={Microsoft COCO: Common Objects in Context},\n      author={Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Dollár},\n      year={2015},\n      eprint={1405.0312},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n----------------------------------------\n\nTITLE: Citations for YOLOv10 Research\nDESCRIPTION: BibTeX citation format for referencing the YOLOv10 research paper and acknowledging the authors from Tsinghua University.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov10.md#2025-04-22_snippet_4\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{THU-MIGyolov10,\n  title={YOLOv10: Real-Time End-to-End Object Detection},\n  author={Ao Wang, Hui Chen, Lihao Liu, et al.},\n  journal={arXiv preprint arXiv:2405.14458},\n  year={2024},\n  institution={Tsinghua University},\n  license = {AGPL-3.0}\n}\n```\n\n----------------------------------------\n\nTITLE: Inspecting Data Collection Settings in Ultralytics YOLO using CLI\nDESCRIPTION: This snippet demonstrates how to view all settings in Ultralytics YOLO using the command-line interface.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/help/privacy.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nyolo settings\n```\n\n----------------------------------------\n\nTITLE: Pulling and Running Ultralytics Conda Docker Image\nDESCRIPTION: Bash commands to pull the latest Ultralytics Conda Docker image and run it with GPU support.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/conda-quickstart.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Set image name as a variable\nt=ultralytics/ultralytics:latest-conda\n\n# Pull the latest Ultralytics image from Docker Hub\nsudo docker pull $t\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Run the Ultralytics image in a container with GPU support\nsudo docker run -it --ipc=host --gpus all $t            # all GPUs\nsudo docker run -it --ipc=host --gpus '\"device=2,3\"' $t # specify GPUs\n```\n\n----------------------------------------\n\nTITLE: ImageNet Citation in BibTeX Format\nDESCRIPTION: BibTeX citation format for referencing the ImageNet dataset in academic work.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/imagenet.md#2025-04-22_snippet_2\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{ILSVRC15,\n         author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},\n         title={ImageNet Large Scale Visual Recognition Challenge},\n         year={2015},\n         journal={International Journal of Computer Vision (IJCV)},\n         volume={115},\n         number={3},\n         pages={211-252}\n}\n```\n\n----------------------------------------\n\nTITLE: Defining HTML Canvas for Chart Rendering\nDESCRIPTION: This HTML code defines a <canvas> element, which acts as a container for the performance comparison chart. It assigns the ID 'modelComparisonChart' for JavaScript targeting, sets the display dimensions to 1024x400 pixels, and includes a custom attribute 'active-models' initialized with '[\"YOLOv8\"]'. The Chart.js library, controlled by the 'benchmark.js' script, will use this canvas element to draw and display the visual representation of the model performance data.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov8.md#2025-04-22_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<canvas id=\"modelComparisonChart\" width=\"1024\" height=\"400\" active-models='[\"YOLOv8\"]'></canvas>\n```\n\n----------------------------------------\n\nTITLE: Enabling MAX Power Mode on NVIDIA Jetson\nDESCRIPTION: Command to enable MAX Power Mode on NVIDIA Jetson devices, ensuring all CPU and GPU cores are turned on for maximum performance when running YOLO11 models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/nvidia-jetson.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nsudo nvpmodel -m 0\n```\n\n----------------------------------------\n\nTITLE: Validating YOLO11 Model with Python\nDESCRIPTION: This Python snippet is used to validate a YOLO11 model on a validation dataset. It measures model performance with metrics like mAP and IOU. The user needs a pre-trained or custom YOLO model and validation dataset in YOLO format.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/index.md#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pre-trained or custom YOLO model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Run validation on your dataset\nmodel.val(data=\"path/to/validation.yaml\")\n```\n\n----------------------------------------\n\nTITLE: ClearML Plot Logging Function\nDESCRIPTION: Function to log plot data and visualizations to ClearML\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/callbacks/clearml.md#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nultralytics.utils.callbacks.clearml._log_plot\n```\n\n----------------------------------------\n\nTITLE: Training a YOLOv5 Model using Python API (FAQ Example)\nDESCRIPTION: This Python script, presented as part of the FAQ, shows how to load a pre-trained YOLOv5 model (`yolov5n.pt`) using the `ultralytics` library, optionally display model information, and then train it on the `coco8.yaml` dataset for 100 epochs with an image size of 640. It demonstrates the basic training workflow via the Python API. Requires the `ultralytics` package.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov5.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a COCO-pretrained YOLOv5n model\nmodel = YOLO(\"yolov5n.pt\")\n\n# Display model information (optional)\nmodel.info()\n\n# Train the model on the COCO8 example dataset for 100 epochs\nresults = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: BibTeX Citation for DOTA Dataset\nDESCRIPTION: BibTeX citation information for the DOTA dataset research paper, useful for academic references when using the dataset in research projects.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/obb/dota-v2.md#2025-04-22_snippet_4\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{9560031,\n  author={Ding, Jian and Xue, Nan and Xia, Gui-Song and Bai, Xiang and Yang, Wen and Yang, Michael and Belongie, Serge and Luo, Jiebo and Datcu, Mihai and Pelillo, Marcello and Zhang, Liangpei},\n  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\n  title={Object Detection in Aerial Images: A Large-Scale Benchmark and Challenges},\n  year={2021},\n  volume={},\n  number={},\n  pages={1-1},\n  doi={10.1109/TPAMI.2021.3117983}\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Weights & Biases via CLI\nDESCRIPTION: Command line instruction for initializing Weights & Biases environment using your API key for authentication.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/weights-biases.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Initialize your Weights & Biases environment\nwandb login YOUR_API_KEY\n```\n\n----------------------------------------\n\nTITLE: Converting COCO Labels to YOLO Format\nDESCRIPTION: Python code for converting COCO dataset annotations to Ultralytics YOLO format with keypoint support. Uses the built-in conversion utility with keypoint preservation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/index.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics.data.converter import convert_coco\n\nconvert_coco(labels_dir=\"path/to/coco/annotations/\", use_keypoints=True)\n```\n\n----------------------------------------\n\nTITLE: Freezing BatchNorm Statistics During Training in Python\nDESCRIPTION: This example shows how to use a callback to freeze BatchNorm statistics when freezing layers during training. It defines a 'put_in_eval_mode' function that sets frozen layers to evaluation mode to prevent BatchNorm values from changing.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/callbacks.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n\n# Add a callback to put the frozen layers in eval mode to prevent BN values from changing\ndef put_in_eval_mode(trainer):\n    n_layers = trainer.args.freeze\n    if not isinstance(n_layers, int):\n        return\n\n    for i, (name, module) in enumerate(trainer.model.named_modules()):\n        if name.endswith(\"bn\") and int(name.split(\".\")[1]) < n_layers:\n            module.eval()\n            module.track_running_stats = False\n\n\nmodel = YOLO(\"yolo11n.pt\")\nmodel.add_callback(\"on_train_epoch_start\", put_in_eval_mode)\nmodel.train(data=\"coco.yaml\", epochs=10)\n```\n\n----------------------------------------\n\nTITLE: BibTeX Citation for COCO Dataset\nDESCRIPTION: BibTeX citation format for referencing the Microsoft COCO dataset in research or development work that uses the COCO8-Pose dataset.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/coco8-pose.md#2025-04-22_snippet_3\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{lin2015microsoft,\n      title={Microsoft COCO: Common Objects in Context},\n      author={Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Dollár},\n      year={2015},\n      eprint={1405.0312},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n----------------------------------------\n\nTITLE: Error Message Example\nDESCRIPTION: Shows the error message output from running the YOLO model on an invalid input image\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/help/minimum-reproducible-example.md#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nRuntimeError: Expected input[1, 0, 640, 640] to have 3 channels, but got 0 channels instead\n```\n\n----------------------------------------\n\nTITLE: Validating Custom-Trained YOLO Model in Bash\nDESCRIPTION: Command to validate the accuracy of a custom-trained YOLO model using the best weights file.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/cli.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nyolo detect val model=path/to/best.pt\n```\n\n----------------------------------------\n\nTITLE: YOLOv5 Dataset Directory Structure\nDESCRIPTION: Shows the recommended hierarchical directory structure for organizing YOLOv5 datasets with training and validation splits.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/train_custom_data.md#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n/datasets/\n└── coco128/  # Dataset root\n    ├── images/\n    │   ├── train2017/  # Training images\n    │   │   ├── 000000000009.jpg\n    │   │   └── ...\n    │   └── val2017/    # Validation images (optional if using same set for train/val)\n    │       └── ...\n    └── labels/\n        ├── train2017/  # Training labels\n        │   ├── 000000000009.txt\n        │   └── ...\n        └── val2017/    # Validation labels (optional if using same set for train/val)\n            └── ...\n```\n\n----------------------------------------\n\nTITLE: DOTA Dataset Citation in BibTeX Format\nDESCRIPTION: BibTeX citation for the DOTA dataset, to be used when referencing the dataset in research or development work.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/obb/dota8.md#2025-04-22_snippet_3\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{9560031,\n  author={Ding, Jian and Xue, Nan and Xia, Gui-Song and Bai, Xiang and Yang, Wen and Yang, Michael and Belongie, Serge and Luo, Jiebo and Datcu, Mihai and Pelillo, Marcello and Zhang, Liangpei},\n  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\n  title={Object Detection in Aerial Images: A Large-Scale Benchmark and Challenges},\n  year={2021},\n  volume={},\n  number={},\n  pages={1-1},\n  doi={10.1109/TPAMI.2021.3117983}\n}\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Model with Python\nDESCRIPTION: This Python snippet demonstrates how to train a YOLO11 model using a custom dataset. The code loads a pre-trained YOLO model and begins the training process with specified parameters like data path, number of epochs, and image size. Dependencies include the Ultralytics YOLO library and a dataset formatted for YOLO.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/index.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a pre-trained YOLO model (you can choose n, s, m, l, or x versions)\nmodel = YOLO(\"yolo11n.pt\")\n\n# Start training on your custom dataset\nmodel.train(data=\"path/to/dataset.yaml\", epochs=100, imgsz=640)\n```\n\n----------------------------------------\n\nTITLE: Embedding YouTube Video in Markdown\nDESCRIPTION: This code snippet demonstrates how to embed a YouTube video in a Markdown document using an iframe. It includes parameters for width, height, source URL, and various allowances for the embedded video player.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<p align=\"center\">\n  <br>\n  <iframe loading=\"lazy\" width=\"720\" height=\"405\" src=\"https://www.youtube.com/embed/YDXKa1EljmU\"\n    title=\"YouTube video player\" frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    allowfullscreen>\n  </iframe>\n  <br>\n  <strong>Watch:</strong> Ultralytics Datasets Overview\n</p>\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Docker Container\nDESCRIPTION: Command to stop and remove the Triton server Docker container\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/triton-inference-server.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Kill and remove the container at the end of the test\nsubprocess.call(f\"docker kill {container_id}\", shell=True)\n```\n\n----------------------------------------\n\nTITLE: Installing ONNX Runtime GPU Package\nDESCRIPTION: Installs the GPU-accelerated version of ONNX Runtime for NVIDIA GPU support. Requires NVIDIA drivers and CUDA toolkit as prerequisites.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/RTDETR-ONNXRuntime-Python/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install onnxruntime-gpu\n```\n\n----------------------------------------\n\nTITLE: Contribution Note Block\nDESCRIPTION: Note block in markdown format providing the GitHub repository link and instructions for contributing to the codebase.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/models/sam/modules/memory_attention.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n!!! note\n\n    This file is available at [https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/modules/memory_attention.py](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/modules/memory_attention.py). If you spot a problem please help fix it by [contributing](https://docs.ultralytics.com/help/contributing/) a [Pull Request](https://github.com/ultralytics/ultralytics/edit/main/ultralytics/models/sam/modules/memory_attention.py) 🛠️. Thank you 🙏!\n```\n\n----------------------------------------\n\nTITLE: Configuring Tiger-Pose Dataset in YAML\nDESCRIPTION: YAML configuration file for the Tiger-Pose dataset that specifies paths, class names, and other essential parameters for training and validation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/tiger-pose.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/tiger-pose.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Importing Ultralytics Library\nDESCRIPTION: Python code to import and verify the Ultralytics installation for YOLO11 model training.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/ibm-watsonx.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Import ultralytics\nimport ultralytics\n\nultralytics.checks()\n\n# Import packages to retrieve and display image files\n```\n\n----------------------------------------\n\nTITLE: Referencing YOLO12 Software in BibTeX\nDESCRIPTION: This BibTeX software entry allows users to attribute the YOLOv12 model implementation in academic or technical works. It specifies the authors, title, release year, source URL, and license. This is particularly useful for software citation practices and should be included in LaTeX bibliography management tools when referencing the official repository.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolo12.md#2025-04-22_snippet_3\n\nLANGUAGE: bibtex\nCODE:\n```\n@software{yolo12,\\n  author = {Tian, Yunjie and Ye, Qixiang and Doermann, David},\\n  title = {YOLOv12: Attention-Centric Real-Time Object Detectors},\\n  year = {2025},\\n  url = {https://github.com/sunsmarterjie/yolov12},\\n  license = {AGPL-3.0}\\n}\n```\n\n----------------------------------------\n\nTITLE: Tracking Objects Using YOLO-World with Ultralytics Python API\nDESCRIPTION: Demonstrates initializing a YOLO-World model in Python with the Ultralytics library and performing object tracking on a specified video. The snippet requires the 'ultralytics' Python package, a YOLO-World weights file (e.g., 'yolov8s-world.pt'), and a video file as input. The 'track' method returns detection results on each video frame. Output comprises detection and tracking metadata and can be post-processed for visualization or analysis.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolo-world.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Create a YOLO-World model\nmodel = YOLO(\"yolov8s-world.pt\")  # or select yolov8m/l-world.pt for different sizes\n\n# Track with a YOLO-World model on a video\nresults = model.track(source=\"path/to/video.mp4\")\n```\n\n----------------------------------------\n\nTITLE: Enabling Maximum Clock Frequencies on NVIDIA Jetson\nDESCRIPTION: Command to enable Jetson Clocks, which ensures all CPU and GPU cores operate at their maximum frequency for optimal performance with computationally intensive models like YOLO11.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/nvidia-jetson.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nsudo jetson_clocks\n```\n\n----------------------------------------\n\nTITLE: Running YOLO Object Detection Tasks Across Multiple Model Versions\nDESCRIPTION: Commands for running object detection tasks using various YOLO model versions (v5 through v11 and RT-DETR) with different scaling factors using the ONNXRuntime Rust implementation.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLO-Series-ONNXRuntime-Rust/README.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ncargo run -r -- --task detect --ver v5 --scale n  # YOLOv5 Detection\ncargo run -r -- --task detect --ver v6 --scale n  # YOLOv6 Detection\ncargo run -r -- --task detect --ver v7 --scale t  # YOLOv7 Detection\ncargo run -r -- --task detect --ver v8 --scale n  # YOLOv8 Detection\ncargo run -r -- --task detect --ver v9 --scale t  # YOLOv9 Detection\ncargo run -r -- --task detect --ver v10 --scale n # YOLOv10 Detection\ncargo run -r -- --task detect --ver v11 --scale n # YOLO11 Detection\ncargo run -r -- --task detect --ver rtdetr --scale l # RT-DETR Detection\n```\n\n----------------------------------------\n\nTITLE: MNIST Dataset Citation - BibTeX Format\nDESCRIPTION: BibTeX citation format for referencing the MNIST dataset in academic work.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/classify/mnist.md#2025-04-22_snippet_2\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{lecun2010mnist,\n         title={MNIST handwritten digit database},\n         author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n         journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n         volume={2},\n         year={2010}\n}\n```\n\n----------------------------------------\n\nTITLE: Using MobileSAM with Box Prompts in Python\nDESCRIPTION: This code snippet demonstrates how to use MobileSAM for image segmentation using box prompts. It shows loading the model and predicting segments based on bounding box inputs.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/mobile-sam.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import SAM\n\n# Load the model\nmodel = SAM(\"mobile_sam.pt\")\n\n# Predict a segment based on a single point prompt\nmodel.predict(\"ultralytics/assets/zidane.jpg\", points=[900, 370], labels=[1])\n\n# Predict multiple segments based on multiple points prompt\nmodel.predict(\"ultralytics/assets/zidane.jpg\", points=[[400, 370], [900, 370]], labels=[1, 1])\n\n# Predict a segment based on multiple points prompt per object\nmodel.predict(\"ultralytics/assets/zidane.jpg\", points=[[[400, 370], [900, 370]]], labels=[[1, 1]])\n\n# Predict a segment using both positive and negative prompts.\nmodel.predict(\"ultralytics/assets/zidane.jpg\", points=[[[400, 370], [900, 370]]], labels=[[1, 0]])\n```\n\n----------------------------------------\n\nTITLE: Iterating and Displaying Trial Configurations and Metrics from Ray Tune Python\nDESCRIPTION: This snippet iterates through individual results in the Ray Tune ResultGrid, printing hyperparameter configurations and last reported metrics for each. You need the result_grid object from Ray Tune. Inputs: result grid; outputs: printed summary per trial. Useful for detailed trial inspection after tuning.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/ray-tune.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfor i, result in enumerate(result_grid):\\n    print(f\"Trial #{i}: Configuration: {result.config}, Last Reported Metrics: {result.metrics}\")\n```\n\n----------------------------------------\n\nTITLE: Benchmarking YOLOv8 Performance via CLI\nDESCRIPTION: Demonstrates how to run YOLOv8 performance benchmarks from the command line using the `yolo benchmark` command, as shown in the FAQ section. Specifies the model (`yolov8n.pt`), data configuration (`coco8.yaml`), image size, precision (`half`), and target device as arguments. Requires the `ultralytics` CLI tool.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov8.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nyolo benchmark model=yolov8n.pt data='coco8.yaml' imgsz=640 half=False device=0\n```\n\n----------------------------------------\n\nTITLE: Profiling and Comparing Models in Python\nDESCRIPTION: This code demonstrates how to profile various segmentation models including SAM, FastSAM, and YOLO, comparing their size and speed. It assumes access to the model files and 'ultralytics' library, running tests on specified assets to obtain performance metrics.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/sam-2.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import ASSETS, SAM, YOLO, FastSAM\n\n# Profile SAM2-t, SAM2-b, SAM-b, MobileSAM\nfor file in [\"sam_b.pt\", \"sam2_b.pt\", \"sam2_t.pt\", \"mobile_sam.pt\"]:\n    model = SAM(file)\n    model.info()\n    model(ASSETS)\n\n# Profile FastSAM-s\nmodel = FastSAM(\"FastSAM-s.pt\")\nmodel.info()\nmodel(ASSETS)\n\n# Profile YOLO models\nfor file_name in [\"yolov8n-seg.pt\", \"yolo11n-seg.pt\"]:\n    model = YOLO(file_name)\n    model.info()\n    model(ASSETS)\n```\n\n----------------------------------------\n\nTITLE: Google-Style Python Docstring Example\nDESCRIPTION: Demonstrates the proper format for Google-style docstrings in Python functions, including argument types, return values, and examples.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/help/contributing.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef example_function(arg1, arg2=4):\n    \"\"\"\n    Example function demonstrating Google-style docstrings.\n\n    Args:\n        arg1 (int): The first argument.\n        arg2 (int): The second argument, with a default value of 4.\n\n    Returns:\n        (bool): True if successful, False otherwise.\n\n    Examples:\n        >>> result = example_function(1, 2)  # returns False\n    \"\"\"\n    if arg1 == arg2:\n        return True\n    return False\n```\n\n----------------------------------------\n\nTITLE: Running FastSAM Inference - Segment by Bounding Box Prompt - Shell\nDESCRIPTION: Runs FastSAM to segment objects within a given bounding box specified as xywh coordinates. Requires model and image path, box_prompt argument, and produces segmentation restricted to the area inside the bounding box. Only works as intended with images and prompt data prepared in compatible formats.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/fast-sam.md#2025-04-22_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\npython Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg --box_prompt \"[570,200,230,400]\"\n```\n\n----------------------------------------\n\nTITLE: Performing YOLOE Prompted Detection using Python SDK (Python)\nDESCRIPTION: Demonstrates how to use the Ultralytics Python library to perform prompted object detection. It initializes a YOLOE model (`yoloe-s.pt`), sets the desired detection classes (\"bowl\", \"apple\") using the `set_classes` method, runs prediction on an image (`kitchen.jpg`), and then saves the detection results.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yoloe.md#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yoloe-s.pt\")\nmodel.set_classes([\"bowl\", \"apple\"])\nresults = model.predict(\"kitchen.jpg\")\nresults[0].save()\n```\n\n----------------------------------------\n\nTITLE: CLA Signing Comment for Pull Requests\nDESCRIPTION: The required text comment to add to pull requests when signing the Contributor License Agreement for Ultralytics YOLO contributions.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/help/contributing.md#2025-04-22_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nI have read the CLA Document and I sign the CLA\n```\n\n----------------------------------------\n\nTITLE: Training and Using YOLO11 Classification Models\nDESCRIPTION: This code snippet shows how to load, train, and use a YOLO11 classification model using Python. It requires the `ultralytics` package, and parameters include `data` for the training dataset and `epochs` for training length. Outputs include trained model statistics and predictions.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolo11n-cls.pt')  # load a pretrained YOLO classification model\nmodel.train(data='mnist160', epochs=3)  # train the model\nmodel('https://ultralytics.com/images/bus.jpg')  # predict on an image\n```\n\n----------------------------------------\n\nTITLE: Adding Attribution in README.md for AGPL-3.0 Compliance\nDESCRIPTION: Code snippet showing how to properly attribute Ultralytics YOLO in a project's README.md file to comply with AGPL-3.0 license requirements.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/help/contributing.md#2025-04-22_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\nThis project utilizes code from [Ultralytics YOLO](https://github.com/ultralytics/ultralytics), licensed under AGPL-3.0.\n```\n\n----------------------------------------\n\nTITLE: Cropping Images Using Bounding Box Coordinates in Python\nDESCRIPTION: Extracts bounding box coordinates from a detection result, converts them to integers, and crops the isolated image to the object region. This technique is useful for obtaining only the part of the image containing the detected object.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/isolating-segmentation-objects.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# (1) Bounding box coordinates\nx1, y1, x2, y2 = c.boxes.xyxy.cpu().numpy().squeeze().astype(np.int32)\n# Crop image to object region\niso_crop = isolated[y1:y2, x1:x2]\n```\n\n----------------------------------------\n\nTITLE: Exporting and Using YOLO11 Model in TensorFlow.js Format (CLI)\nDESCRIPTION: This CLI command example shows how to export a YOLO11n PyTorch model to TensorFlow.js format and then use the exported model for inference. It demonstrates the command-line approach for model export and prediction using the YOLO CLI.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/tfjs.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Export a YOLO11n PyTorch model to TF.js format\nyolo export model=yolo11n.pt format=tfjs # creates '/yolo11n_web_model'\n\n# Run inference with the exported model\nyolo predict model='./yolo11n_web_model' source='https://ultralytics.com/images/bus.jpg'\n```\n\n----------------------------------------\n\nTITLE: Citing MobileSAM in Research Publications\nDESCRIPTION: This BibTeX snippet provides citation information for the MobileSAM paper, 'Faster Segment Anything: Towards Lightweight SAM for Mobile Applications'. The paper details the improvements and optimizations of MobileSAM compared to the original SAM model, making it suitable for publications and acknowledgments in academic research.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/mobile-sam.md#2025-04-22_snippet_4\n\nLANGUAGE: BibTeX\nCODE:\n```\n@article{mobile_sam,\n  title={Faster Segment Anything: Towards Lightweight SAM for Mobile Applications},\n  author={Zhang, Chaoning and Han, Dongshen and Qiao, Yu and Kim, Jung Uk and Bae, Sung Ho and Lee, Seungkyu and Hong, Choong Seon},\n  journal={arXiv preprint arXiv:2306.14289},\n  year={2023}\n}\n```\n\n----------------------------------------\n\nTITLE: Performing Object Detection with YOLOv10 using CLI\nDESCRIPTION: Shows how to use the Ultralytics command-line interface (CLI) to run object detection. It loads a COCO-pretrained YOLOv10n model (`yolov10n.pt`) and performs inference on the specified source image (`path/to/bus.jpg`).\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov10.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n```bash\n# Load a COCO-pretrained YOLOv10n model and run inference on the 'bus.jpg' image\nyolo detect predict model=yolov10n.pt source=path/to/bus.jpg\n```\n```\n\n----------------------------------------\n\nTITLE: Using Custom Tracker Configuration using CLI with Ultralytics YOLO\nDESCRIPTION: This Bash example explains how to employ a custom tracker configuration file in the CLI. By providing a custom YAML file, users can customize tracking settings effectively without programming changes. The command is designed for users proficient in using the CLI for model execution.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/track.md#2025-04-22_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\nyolo track model=yolo11n.pt source=\"https://youtu.be/LNwODJXcvt4\" tracker='custom_tracker.yaml'\n```\n\n----------------------------------------\n\nTITLE: Citing Ultralytics YOLOv5 Repository in BibTeX\nDESCRIPTION: This BibTeX entry provides the standard format for citing the Ultralytics YOLOv5 software repository in academic publications or research. It includes details such as title, author, year, version, license, URL, DOI, and ORCID, facilitating proper attribution.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov5.md#2025-04-22_snippet_3\n\nLANGUAGE: bibtex\nCODE:\n```\n@software{yolov5,\n  title = {Ultralytics YOLOv5},\n  author = {Glenn Jocher},\n  year = {2020},\n  version = {7.0},\n  license = {AGPL-3.0},\n  url = {https://github.com/ultralytics/yolov5},\n  doi = {10.5281/zenodo.3908559},\n  orcid = {0000-0001-5950-6979}\n}\n```\n\n----------------------------------------\n\nTITLE: Dataset Configuration Example in YAML\nDESCRIPTION: Example YAML configuration file for the COCO8 dataset showing the standard YOLOv5 and YOLOv8 format with path, train/val splits, and class names.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/hub/datasets.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/coco8.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Optimizing and Zipping Dataset with Ultralytics Utils\nDESCRIPTION: Python script demonstrating how to optimize images in a dataset using compression and create a zip archive using Ultralytics utility functions. The script processes all JPG images recursively and creates a compressed dataset archive.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/index.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\n\nfrom ultralytics.data.utils import compress_one_image\nfrom ultralytics.utils.downloads import zip_directory\n\n# Define dataset directory\npath = Path(\"path/to/dataset\")\n\n# Optimize images in dataset (optional)\nfor f in path.rglob(\"*.jpg\"):\n    compress_one_image(f)\n\n# Zip dataset into 'path/to/dataset.zip'\nzip_directory(path)\n```\n\n----------------------------------------\n\nTITLE: LVIS Dataset Citation in BibTeX Format\nDESCRIPTION: BibTeX citation format for referencing the LVIS dataset in academic research papers and publications, acknowledging the original creators.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/lvis.md#2025-04-22_snippet_3\n\nLANGUAGE: bibtex\nCODE:\n```\n@inproceedings{gupta2019lvis,\n  title={LVIS: A Dataset for Large Vocabulary Instance Segmentation},\n  author={Gupta, Agrim and Dollar, Piotr and Girshick, Ross},\n  booktitle={Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},\n  year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Running YOLO Inference on a PyTorch Tensor in Python\nDESCRIPTION: This snippet shows how to perform YOLO inference using a PyTorch tensor as input. It requires importing `torch` and `YOLO`. A random PyTorch tensor is created with the expected BCHW format (Batch, Channels, Height, Width), RGB channel order, and float32 data type (values normalized between 0.0 and 1.0). This tensor is passed to the loaded YOLO model for inference, returning a list of Results objects.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/modes/predict.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Create a random torch tensor of BCHW shape (1, 3, 640, 640) with values in range [0, 1] and type float32\nsource = torch.rand(1, 3, 640, 640, dtype=torch.float32)\n\n# Run inference on the source\nresults = model(source)  # list of Results objects\n```\n\n----------------------------------------\n\nTITLE: Citing Roboflow 100 Dataset using BibTeX\nDESCRIPTION: BibTeX citation format for referencing the Roboflow 100 dataset paper in academic research. Includes author information, title, year, and arXiv link.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/roboflow-100.md#2025-04-22_snippet_1\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{rf100benchmark,\n    Author = {Floriana Ciaglia and Francesco Saverio Zuppichini and Paul Guerrie and Mark McQuade and Jacob Solawetz},\n    Title = {Roboflow 100: A Rich, Multi-Domain Object Detection Benchmark},\n    Year = {2022},\n    Eprint = {arXiv:2211.13523},\n    url = {https://arxiv.org/abs/2211.13523}\n}\n```\n\n----------------------------------------\n\nTITLE: Reading YAML Configuration and Class Labels\nDESCRIPTION: Loads the dataset configuration from a YAML file to extract class label information and their indices. This is used to map numeric class IDs to their names.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/kfold-cross-validation.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport yaml\n\nyaml_file = \"path/to/data.yaml\"  # your data YAML with data directories and names dictionary\nwith open(yaml_file, encoding=\"utf8\") as y:\n    classes = yaml.safe_load(y)[\"names\"]\ncls_idx = sorted(classes.keys())\n```\n\n----------------------------------------\n\nTITLE: YOLOv5 Usage Commands in Docker\nDESCRIPTION: Common YOLOv5 commands for training, validation, inference, and model export within the Docker container.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/environments/docker_image_quickstart_tutorial.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# Train a YOLOv5 model on your custom dataset (ensure data is mounted or downloaded)\npython train.py --data your_dataset.yaml --weights yolov5s.pt --img 640 # Start training\n\n# Validate the trained model's performance (Precision, Recall, mAP)\npython val.py --weights path/to/your/best.pt --data your_dataset.yaml # Validate accuracy\n\n# Run inference on images or videos using a trained model\npython detect.py --weights yolov5s.pt --source path/to/your/images_or_videos # Perform detection\n\n# Export the trained model to various formats like ONNX, CoreML, or TFLite for deployment\npython export.py --weights yolov5s.pt --include onnx coreml tflite # Export model\n```\n\n----------------------------------------\n\nTITLE: Citations for Dog-Pose Dataset in BibTeX Format\nDESCRIPTION: BibTeX citations for the research papers related to the Dog-Pose dataset, including references to the Fine-Grained Image Categorization and ImageNet papers.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/dog-pose.md#2025-04-22_snippet_3\n\nLANGUAGE: bibtex\nCODE:\n```\n@inproceedings{khosla2011fgvc,\n  title={Novel dataset for Fine-Grained Image Categorization},\n  author={Aditya Khosla and Nityananda Jayadevaprakash and Bangpeng Yao and Li Fei-Fei},\n  booktitle={First Workshop on Fine-Grained Visual Categorization (FGVC), IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year={2011}\n}\n@inproceedings{deng2009imagenet,\n  title={ImageNet: A Large-Scale Hierarchical Image Database},\n  author={Jia Deng and Wei Dong and Richard Socher and Li-Jia Li and Kai Li and Li Fei-Fei},\n  booktitle={IEEE Computer Vision and Pattern Recognition (CVPR)},\n  year={2009}\n}\n```\n\n----------------------------------------\n\nTITLE: Citing the YOLOE Paper using BibTeX (BibTeX)\nDESCRIPTION: Provides the BibTeX citation format for the research paper \"YOLOE: Real-Time Seeing Anything\" by Ao Wang et al. (2025). This entry should be used when referencing the YOLOE model or its concepts in academic publications.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yoloe.md#2025-04-22_snippet_21\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{wang2025yoloerealtimeseeing,\n      title={YOLOE: Real-Time Seeing Anything}, \n      author={Ao Wang and Lihao Liu and Hui Chen and Zijia Lin and Jungong Han and Guiguang Ding},\n      year={2025},\n      eprint={2503.07465},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2503.07465},\n}\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n on COCO8 Dataset using CLI\nDESCRIPTION: This command trains the YOLO11n model on the COCO8 dataset for 3 epochs. It demonstrates how to use the YOLO CLI for model training tasks, specifying model, dataset, number of epochs, and image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nyolo train model=yolo11n.pt data=coco8.yaml epochs=3 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Citing Open Images V7 Dataset with BibTeX\nDESCRIPTION: BibTeX citation format for the Open Images Dataset V4 paper, including author details and publication information.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/open-images-v7.md#2025-04-22_snippet_5\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{OpenImages,\n  author = {Alina Kuznetsova and Hassan Rom and Neil Alldrin and Jasper Uijlings and Ivan Krasin and Jordi Pont-Tuset and Shahab Kamali and Stefan Popov and Matteo Malloci and Alexander Kolesnikov and Tom Duerig and Vittorio Ferrari},\n  title = {The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale},\n  year = {2020},\n  journal = {IJCV}\n}\n```\n\n----------------------------------------\n\nTITLE: PyTorch and Torchvision Installation for JetPack 6.1\nDESCRIPTION: Commands for installing compatible versions of PyTorch and Torchvision for Jetson ARM64 architecture with JetPack 6.1.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/nvidia-jetson.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install https://github.com/ultralytics/assets/releases/download/v0.0.0/torch-2.5.0a0+872d972e41.nv24.08-cp310-cp310-linux_aarch64.whl\npip install https://github.com/ultralytics/assets/releases/download/v0.0.0/torchvision-0.20.0a0+afc54f7-cp310-cp310-linux_aarch64.whl\n```\n\n----------------------------------------\n\nTITLE: Citing xView Dataset using BibTeX\nDESCRIPTION: BibTeX citation for the xView dataset paper. This citation should be used when referencing the xView dataset in academic research papers or publications.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/xview.md#2025-04-22_snippet_5\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{lam2018xview,\n    title={xView: Objects in Context in Overhead Imagery},\n    author={Darius Lam and Richard Kuzma and Kevin McGee and Samuel Dooley and Michael Laielli and Matthew Klaric and Yaroslav Bulatov and Brendan McCord},\n    year={2018},\n    eprint={1802.07856},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing _ntuple Function for Tuple Generation in Python\nDESCRIPTION: The _ntuple function is likely a utility for generating n-tuples, which could be used in various parts of the bounding box and instance handling logic, possibly for coordinate representations or other multi-value attributes.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/instance.md#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef _ntuple():\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO11 to ONNX Format for Triton Inference Server\nDESCRIPTION: This snippet demonstrates how to load a YOLO11 model and export it to ONNX format with dynamic input shapes, which is required for integration with NVIDIA Triton Inference Server.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/triton-inference-server.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load an official model\n\n# Export the model to ONNX format\nonnx_file = model.export(format=\"onnx\", dynamic=True)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Dataset Label Files\nDESCRIPTION: Uses pathlib to recursively gather all label files from the dataset directory. This code finds all .txt annotation files in the labels subdirectory.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/kfold-cross-validation.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\n\ndataset_path = Path(\"./Fruit-detection\")  # replace with 'path/to/dataset' for your custom data\nlabels = sorted(dataset_path.rglob(\"*labels/*.txt\"))  # all data in 'labels'\n```\n\n----------------------------------------\n\nTITLE: Complete YOLO Terminal Visualization Example\nDESCRIPTION: Full implementation combining all steps to display YOLO inference results in terminal\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/view-results-in-terminal.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport io\n\nimport cv2\nfrom sixel import SixelWriter\n\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Run inference on an image\nresults = model.predict(source=\"ultralytics/assets/bus.jpg\")\n\n# Plot inference results\nplot = results[0].plot()\n\n# Results image as bytes\nim_bytes = cv2.imencode(\n    \".png\",\n    plot,\n)[1].tobytes()\n\nmem_file = io.BytesIO(im_bytes)\nw = SixelWriter()\nw.draw(mem_file)\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Pose Model with Dog-Pose Dataset via CLI\nDESCRIPTION: Command-line instruction for training a YOLO11n-pose model on the Dog-pose dataset for 100 epochs with an image size of 640. Uses a pretrained model as the starting point.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/pose/dog-pose.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start training from a pretrained *.pt model\nyolo pose train data=dog-pose.yaml model=yolo11n-pose.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Logging YOLOv5 Model Predictions to Comet\nDESCRIPTION: Command to train YOLOv5 with prediction logging enabled at regular intervals (every 2nd batch) for visualization in Comet's Object Detection Panel.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/comet_logging_integration.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npython train.py \\\n  --img 640 \\\n  --batch 16 \\\n  --epochs 5 \\\n  --data coco128.yaml \\\n  --weights yolov5s.pt \\\n  --bbox_interval 2\n```\n\n----------------------------------------\n\nTITLE: Semantic Search Using Dataset Indices in Ultralytics Explorer\nDESCRIPTION: Demonstrates how to perform semantic search using dataset indices with the Ultralytics Explorer API, including searching with multiple indices.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/api.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import Explorer\n\n# create an Explorer object\nexp = Explorer(data=\"coco128.yaml\", model=\"yolo11n.pt\")\nexp.create_embeddings_table()\n\nsimilar = exp.get_similar(idx=1, limit=10)\nprint(similar.head())\n\n# Search using multiple indices\nsimilar = exp.get_similar(idx=[1, 10], limit=10)\nprint(similar.head())\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for YOLOv8 Action Recognition\nDESCRIPTION: Commands to clone the Ultralytics repository, navigate to the appropriate directory, and install the required dependencies using pip.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-Action-Recognition/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Clone ultralytics repo\ngit clone https://github.com/ultralytics/ultralytics\n\n# cd to local directory\ncd ultralytics/examples/YOLOv8-Action-Recognition\n\n# Install dependencies using Python's package manager\npip install -U -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Training YOLOv5 on Multiple Machines (Worker Node)\nDESCRIPTION: Command for training YOLOv5 on a worker node in a multi-machine setup. This connects the worker to the master node for distributed training.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/multi_gpu_training.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython -m torch.distributed.run --nproc_per_node G --nnodes N --node_rank R --master_addr \"192.168.1.1\" --master_port 1234 train.py --batch 64 --data coco.yaml --cfg yolov5s.yaml --weights ''\n```\n\n----------------------------------------\n\nTITLE: Saving K-Fold Split Records as CSV in Python\nDESCRIPTION: This optional step saves the records of the K-Fold split and label distribution DataFrames as CSV files for future reference.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/kfold-cross-validation.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfolds_df.to_csv(save_path / \"kfold_datasplit.csv\")\nfold_lbl_distrb.to_csv(save_path / \"kfold_label_distribution.csv\")\n```\n\n----------------------------------------\n\nTITLE: Using Custom Tracker Configuration in Python\nDESCRIPTION: This code snippet demonstrates how to use a custom tracker configuration file when running the YOLO tracker in Python.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/trackers/README.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\n# Load the model and run the tracker with a custom configuration file\nmodel = YOLO(\"yolo11n.pt\")\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", tracker=\"custom_tracker.yaml\")\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11n Detection Model with CLI\nDESCRIPTION: Demonstrates how to use the YOLO CLI to train a YOLO11n detection model on the coco8 dataset for 100 epochs with a 640x640 image size.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/models/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nyolo task=detect mode=train model=yolo11n.yaml data=coco8.yaml epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Using Ask AI Feature for Natural Language Queries\nDESCRIPTION: Demonstrates how to use the Ask AI feature to query datasets using natural language.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/api.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import Explorer\n\n# Create an Explorer object\nexplorer = Explorer(data=\"coco128.yaml\", model=\"yolo11n.pt\")\nexplorer.create_embeddings_table()\n\n# Query with natural language\nquery_result = explorer.ask_ai(\"show me 100 images with exactly one person and 2 dogs. There can be other objects too\")\nprint(query_result.head())\n```\n\n----------------------------------------\n\nTITLE: CLA Signing Comment for Ultralytics Pull Requests\nDESCRIPTION: A text snippet showing the exact comment to add when signing the Contributor License Agreement for Ultralytics YOLO pull requests. This is required before contributions can be accepted.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/CONTRIBUTING.md#2025-04-22_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nI have read the CLA Document and I sign the CLA\n```\n\n----------------------------------------\n\nTITLE: Training YOLO11 Model via CLI\nDESCRIPTION: Shows how to train a YOLO11 model using command-line interface with the same parameters as the Python example.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/index.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nyolo detect train data=path/to/your_dataset.yaml model=yolo11n.pt epochs=100 imgsz=640\n```\n\n----------------------------------------\n\nTITLE: Installing YOLOv5 Requirements in Python\nDESCRIPTION: Clone the YOLOv5 repository and install the required dependencies using pip. This sets up the environment for training YOLOv5 models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/multi_gpu_training.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ultralytics/yolov5 # clone\ncd yolov5\npip install -r requirements.txt # install\n```\n\n----------------------------------------\n\nTITLE: Basic Code Block Format Example in Markdown\nDESCRIPTION: Demonstrates how to format code blocks in markdown using triple backticks with language specification\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/help/minimum-reproducible-example.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n```python\n# Your Python code goes here\n```\n```\n\n----------------------------------------\n\nTITLE: Setting Comet Credentials via Environment Variables\nDESCRIPTION: Commands to set Comet API key and project name as environment variables for authentication and project organization.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/comet_logging_integration.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nexport COMET_API_KEY=YOUR_API_KEY\nexport COMET_PROJECT_NAME=YOUR_COMET_PROJECT_NAME # This will default to 'yolov5'\n```\n\n----------------------------------------\n\nTITLE: SAM Class Documentation in Python\nDESCRIPTION: Documentation reference for the SAM (Segment Anything Model) class implementation in the Ultralytics framework. The class provides functionality for real-time image segmentation with promptable and zero-shot capabilities.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/models/sam/model.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nultralytics.models.sam.model.SAM\n```\n\n----------------------------------------\n\nTITLE: Configuring COCO8 Dataset in YAML for YOLO Training\nDESCRIPTION: YAML configuration file for the COCO8 dataset that specifies paths, class names, and dataset metadata for use with Ultralytics YOLO models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/detect/coco8.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n--8<-- \"ultralytics/cfg/datasets/coco8.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Running YOLOv8 ONNX Inference\nDESCRIPTION: Command to perform object detection using a YOLOv8 ONNX model with configurable confidence and IoU thresholds.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-ONNXRuntime/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython main.py --model yolov8n.onnx --img image.jpg --conf-thres 0.5 --iou-thres 0.5\n```\n\n----------------------------------------\n\nTITLE: Dataset Directory Structure Example\nDESCRIPTION: Shows the recommended folder structure for organizing datasets in Ultralytics format with train and validation splits.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/index.md#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\ndataset/\n├── train/\n│   ├── images/\n│   └── labels/\n└── val/\n    ├── images/\n    └── labels/\n```\n\n----------------------------------------\n\nTITLE: Building MNN Library\nDESCRIPTION: Commands for building the MNN library with OpenCV integration and image codecs enabled\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-MNN-CPP/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmkdir build && cd build\n\ncmake -DMNN_BUILD_OPENCV=ON -DBUILD_SHARED_LIBS=OFF -DMNN_IMGCODECS=ON ..\n\nmake -j$(nproc)\n```\n\n----------------------------------------\n\nTITLE: CLA Signing Comment Text\nDESCRIPTION: The exact text needed to sign the Contributor License Agreement in a pull request comment.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/help/contributing.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nI have read the CLA Document and I sign the CLA\n```\n\n----------------------------------------\n\nTITLE: Initializing ClearML Configuration\nDESCRIPTION: This command initializes the ClearML configuration by connecting the SDK to the server. It requires user interaction to input credentials.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/yolov5/tutorials/clearml_logging_integration.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nclearml-init\n```\n\n----------------------------------------\n\nTITLE: Finding Free Network Port in Python\nDESCRIPTION: Function to find a free network port for distributed training. It uses socket programming to identify an available port.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/reference/utils/dist.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nultralytics.utils.dist.find_free_network_port\n```\n\n----------------------------------------\n\nTITLE: Adjusting Batch Logging Interval in Comet ML\nDESCRIPTION: Modifies the batch logging interval for Comet ML during YOLO11 training using an environment variable.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/comet.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"COMET_EVAL_BATCH_LOGGING_INTERVAL\"] = \"4\"\n```\n\n----------------------------------------\n\nTITLE: Revoking Docker Display Access\nDESCRIPTION: Command to revoke Docker's access to the local display server after finishing GUI operations.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/guides/docker-quickstart.md#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nxhost -local:docker\n```\n\n----------------------------------------\n\nTITLE: Resuming an Interrupted YOLO Training Session in Bash\nDESCRIPTION: Command to resume a previously interrupted YOLO training session using the last saved checkpoint.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/usage/cli.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nyolo detect train resume model=last.pt\n```\n\n----------------------------------------\n\nTITLE: Initializing Embeddings Table\nDESCRIPTION: Creates an Explorer instance with VOC dataset and builds embeddings table for similarity search\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/datasets/explorer/explorer.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nexp = Explorer(\"VOC.yaml\", model=\"yolo11n.pt\")\nexp.create_embeddings_table()\n```\n\n----------------------------------------\n\nTITLE: Installing Ultralytics Package for YOLO11\nDESCRIPTION: Command to install the required Ultralytics package for working with YOLO11 models.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/torchscript.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install ultralytics\n```\n\n----------------------------------------\n\nTITLE: Installing ONNX Runtime CPU Backend\nDESCRIPTION: Command to install standard ONNX Runtime package for CPU-based inference.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/examples/YOLOv8-ONNXRuntime/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install onnxruntime\n```\n\n----------------------------------------\n\nTITLE: Serving MkDocs Documentation Locally\nDESCRIPTION: Command to build and serve the MkDocs documentation locally for preview and development.\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmkdocs serve\n```\n\n----------------------------------------\n\nTITLE: Initializing ClearML SDK\nDESCRIPTION: Command to initialize the ClearML SDK and start the credentials setup process\nSOURCE: https://github.com/ultralytics/ultralytics/blob/main/docs/en/integrations/clearml.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nclearml-init\n```"
  }
]
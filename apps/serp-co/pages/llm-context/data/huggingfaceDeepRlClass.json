[
  {
    "owner": "huggingface",
    "repo": "deep-rl-class",
    "content": "TITLE: Correct Policy Network Implementation with Proper Action Sampling in PyTorch\nDESCRIPTION: Fixed implementation of the Policy network that properly samples actions from the probability distribution. This corrects the bug by replacing the deterministic argmax with stochastic sampling using m.sample().\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size):\n        super(Policy, self).__init__()\n        self.fc1 = nn.Linear(s_size, h_size)\n        self.fc2 = nn.Linear(h_size, a_size)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.softmax(x, dim=1)\n\n    def act(self, state):\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        probs = self.forward(state).cpu()\n        m = Categorical(probs)\n        action = m.sample()\n        return action.item(), m.log_prob(action)\n```\n\n----------------------------------------\n\nTITLE: REINFORCE Training Algorithm Implementation in PyTorch\nDESCRIPTION: Implementation skeleton of the REINFORCE training algorithm. The function handles episode collection, calculating discounted returns at each timestep efficiently using dynamic programming, computing policy loss, and updating network parameters using gradient descent.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n    # Help us to calculate the score during the training\n    scores_deque = deque(maxlen=100)\n    scores = []\n    # Line 3 of pseudocode\n    for i_episode in range(1, n_training_episodes+1):\n        saved_log_probs = []\n        rewards = []\n        state = # TODO: reset the environment\n        # Line 4 of pseudocode\n        for t in range(max_t):\n            action, log_prob = # TODO get the action\n            saved_log_probs.append(log_prob)\n            state, reward, done, _ = # TODO: take an env step\n            rewards.append(reward)\n            if done:\n                break\n        scores_deque.append(sum(rewards))\n        scores.append(sum(rewards))\n\n        # Line 6 of pseudocode: calculate the return\n        returns = deque(maxlen=max_t)\n        n_steps = len(rewards)\n        # Compute the discounted returns at each timestep,\n        # as the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n\n        # In O(N) time, where N is the number of time steps\n        # (this definition of the discounted return G_t follows the definition of this quantity\n        # shown at page 44 of Sutton&Barto 2017 2nd draft)\n        # G_t = r_(t+1) + r_(t+2) + ...\n\n        # Given this formulation, the returns at each timestep t can be computed\n        # by re-using the computed future returns G_(t+1) to compute the current return G_t\n        # G_t = r_(t+1) + gamma*G_(t+1)\n        # G_(t-1) = r_t + gamma* G_t\n        # (this follows a dynamic programming approach, with which we memorize solutions in order\n        # to avoid computing them multiple times)\n\n        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)\n        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...\n\n\n        ## Given the above, we calculate the returns at timestep t as:\n        #               gamma[t] * return[t] + reward[t]\n        #\n        ## We compute this starting from the last timestep to the first, in order\n        ## to employ the formula presented above and avoid redundant computations that would be needed\n        ## if we were to do it from first to last.\n\n        ## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps\n        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)\n        ## a normal python list would instead require O(N) to do this.\n        for t in range(n_steps)[::-1]:\n            disc_return_t = (returns[0] if len(returns)>0 else 0)\n            returns.appendleft(    ) # TODO: complete here\n\n        ## standardization of the returns is employed to make training more stable\n        eps = np.finfo(np.float32).eps.item()\n\n        ## eps is the smallest representable float, which is\n        # added to the standard deviation of the returns to avoid numerical instabilities\n        returns = torch.tensor(returns)\n        returns = (returns - returns.mean()) / (returns.std() + eps)\n\n        # Line 7:\n        policy_loss = []\n        for log_prob, disc_return in zip(saved_log_probs, returns):\n            policy_loss.append(-log_prob * disc_return)\n        policy_loss = torch.cat(policy_loss).sum()\n\n        # Line 8: PyTorch prefers gradient descent\n        optimizer.zero_grad()\n        policy_loss.backward()\n        optimizer.step()\n\n        if i_episode % print_every == 0:\n            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n\n    return scores\n```\n\n----------------------------------------\n\nTITLE: Implementing Q-Learning Training Loop in Python\nDESCRIPTION: This function implements the Q-Learning training loop that updates a Q-table through multiple episodes, using epsilon-greedy exploration and the Bellman equation for value updates.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n  for episode in tqdm(range(n_training_episodes)):\n    # Reduce epsilon (because we need less and less exploration)\n    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n    # Reset the environment\n    state, info = env.reset()\n    step = 0\n    terminated = False\n    truncated = False\n\n    # repeat\n    for step in range(max_steps):\n      # Choose the action At using epsilon greedy policy\n      action =\n\n      # Take action At and observe Rt+1 and St+1\n      # Take the action (a) and observe the outcome state(s') and reward (r)\n      new_state, reward, terminated, truncated, info =\n\n      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n      Qtable[state][action] =\n\n      # If terminated or truncated finish the episode\n      if terminated or truncated:\n        break\n\n      # Our next state is the new state\n      state = new_state\n  return Qtable\n```\n\n----------------------------------------\n\nTITLE: Defining Reinforce Policy Network (PyTorch) Python\nDESCRIPTION: Defines the neural network architecture for the Reinforce agent's policy using PyTorch. The `Policy` class includes three linear layers with ReLU activations and a final layer outputting probabilities via Softmax. The `act` method takes a state, passes it through the network to get action probabilities, samples an action from a Categorical distribution, and returns the action and its log probability. Requires PyTorch and its modules (`nn`, `F`, `Categorical`) and a globally defined `device`.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nclass Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size):\n        super(Policy, self).__init__()\n        self.fc1 = nn.Linear(s_size, h_size)\n        self.fc2 = nn.Linear(h_size, h_size * 2)\n        self.fc3 = nn.Linear(h_size * 2, a_size)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return F.softmax(x, dim=1)\n\n    def act(self, state):\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        probs = self.forward(state).cpu()\n        m = Categorical(probs)\n        action = m.sample()       \n        return action.item(), m.log_prob(action)\n```\n\n----------------------------------------\n\nTITLE: Training a Q-Learning Agent with Qtable in Python\nDESCRIPTION: This snippet trains a Q-Learning agent in a specified environment using a defined number of training episodes, epsilon parameters for exploration, decay rates, and a provided Q-table. The function `train` takes in hyperparameters including exploration bounds, decay rate, environment object, maximal episode steps, and the current Q-table as inputs, and updates the Q-table accordingly. The final Q-table, post-training, is returned for subsequent use. Dependencies include a compatible OpenAI Gym environment and a pre-initialized Q-table variable.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nQtable_taxi = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_taxi)\nQtable_taxi\n```\n\n----------------------------------------\n\nTITLE: Complete Q-Learning Training Function Implementation in Python\nDESCRIPTION: The complete solution for the Q-Learning training function that includes epsilon-greedy action selection, environment interaction, and Q-value updates using the Bellman equation.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n    for episode in tqdm(range(n_training_episodes)):\n        # Reduce epsilon (because we need less and less exploration)\n        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n        # Reset the environment\n        state, info = env.reset()\n        step = 0\n        terminated = False\n        truncated = False\n\n        # repeat\n        for step in range(max_steps):\n            # Choose the action At using epsilon greedy policy\n            action = epsilon_greedy_policy(Qtable, state, epsilon)\n\n            # Take action At and observe Rt+1 and St+1\n            # Take the action (a) and observe the outcome state(s') and reward (r)\n            new_state, reward, terminated, truncated, info = env.step(action)\n\n            # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n            Qtable[state][action] = Qtable[state][action] + learning_rate * (\n                reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action]\n            )\n\n            # If terminated or truncated finish the episode\n            if terminated or truncated:\n                break\n\n            # Our next state is the new state\n            state = new_state\n    return Qtable\n```\n\n----------------------------------------\n\nTITLE: Reinforce Training Loop Implementation in PyTorch (Python)\nDESCRIPTION: Implements the Reinforce policy gradient training algorithm in PyTorch. The function executes multiple training episodes where it collects log probabilities of taken actions and rewards per timestep. It computes the discounted returns efficiently in reverse order, normalizes these returns for stability, and calculates the policy loss as the sum of the negative log probabilities weighted by the returns. The policy network is updated via gradient descent on this loss. Key inputs include the policy model, optimizer, number of episodes, max timesteps, discount factor gamma, and a print frequency for progress. This implementation expects an OpenAI Gym-like environment and uses PyTorch tensors for computations.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ndef reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n    scores_deque = deque(maxlen=100)\n    scores = []\n    for i_episode in range(1, n_training_episodes+1):\n        saved_log_probs = []\n        rewards = []\n        state = env.reset()  # reset the environment at start of episode\n        for t in range(max_t):\n            action, log_prob = policy.act(state)  # get action and log probability\n            saved_log_probs.append(log_prob)\n            state, reward, done, _ = env.step(action)  # take an environment step\n            rewards.append(reward)\n            if done:\n                break\n        scores_deque.append(sum(rewards))\n        scores.append(sum(rewards))\n        \n        returns = deque(maxlen=max_t)\n        n_steps = len(rewards)\n        for t in range(n_steps)[::-1]:\n            disc_return_t = returns[0] if len(returns) > 0 else 0\n            returns.appendleft(gamma * disc_return_t + rewards[t])\n        \n        eps = np.finfo(np.float32).eps.item()\n        returns = torch.tensor(returns)\n        returns = (returns - returns.mean()) / (returns.std() + eps)\n        \n        policy_loss = []\n        for log_prob, disc_return in zip(saved_log_probs, returns):\n            policy_loss.append(-log_prob * disc_return)\n        policy_loss = torch.cat(policy_loss).sum()\n        \n        optimizer.zero_grad()\n        policy_loss.backward()\n        optimizer.step()\n        \n        if i_episode % print_every == 0:\n            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n    \n    return scores\n```\n\n----------------------------------------\n\nTITLE: Initializing and Interacting with Gymnasium LunarLander Environment in Python\nDESCRIPTION: This snippet initializes the LunarLander-v2 environment using Gymnasium, resets it to get the initial observation, and runs a loop executing random actions for 20 steps. It demonstrates obtaining environment feedback such as new observations, rewards, termination, truncation flags, and info dictionary after each action. It also shows resetting the environment when an episode ends due to termination or truncation. Required dependencies include the Gymnasium library installed and properly configured. This snippet covers basic environment lifecycle management and action sampling for training or testing RL agents.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit1/unit1.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\n\n# First, we create our environment called LunarLander-v2\nenv = gym.make(\"LunarLander-v2\")\n\n# Then we reset this environment\nobservation, info = env.reset()\n\nfor _ in range(20):\n  # Take a random action\n  action = env.action_space.sample()\n  print(\"Action taken:\", action)\n\n  # Do this action in the environment and get\n  # next_state, reward, terminated, truncated and info\n  observation, reward, terminated, truncated, info = env.step(action)\n\n  # If the game is terminated (in our case we land, crashed) or truncated (timeout)\n  if terminated or truncated:\n      # Reset the environment\n      print(\"Environment is reset\")\n      observation, info = env.reset()\n\nenv.close()\n```\n\n----------------------------------------\n\nTITLE: Importing Essential Libraries and Packages\nDESCRIPTION: This snippet imports necessary libraries for numerical computation, plotting, deep learning (PyTorch), environment interaction (gym), image processing (imageio), and Hugging Face Hub integration. Proper imports are crucial for implementing and visualizing the Reinforce algorithm.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\n\nfrom collections import deque\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Categorical\n\n# Gym\nimport gym\nimport gym_pygame\n\n# Hugging Face Hub\nfrom huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\nimport imageio\n```\n\n----------------------------------------\n\nTITLE: Agent Class Definition - Python\nDESCRIPTION: This code defines the `Agent` class, which represents the PPO agent. It consists of a critic network and an actor network. The critic network estimates the value function, while the actor network determines the policy. The `get_value` method returns the value estimate for a given state, and the `get_action_and_value` method returns the action, log probability of the action, entropy, and value estimate for a given state.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nclass Agent(nn.Module):\n    def __init__(self, envs):\n        super().__init__()\n        self.critic = nn.Sequential(\n            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, 1), std=1.0),\n        )\n        self.actor = nn.Sequential(\n            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01),\n        )\n\n    def get_value(self, x):\n        return self.critic(x)\n\n    def get_action_and_value(self, x, action=None):\n        logits = self.actor(x)\n        probs = Categorical(logits=logits)\n        if action is None:\n            action = probs.sample()\n        return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n```\n\n----------------------------------------\n\nTITLE: Inspecting LunarLander Environment Observation and Action Spaces in Python\nDESCRIPTION: This snippet demonstrates how to create the LunarLander-v2 environment and explore its observation and action spaces. It prints the shape of the observation space, samples a random observation vector, prints discrete action space size, and samples a random action. This helps in understanding the input features supplied to the agent and the set of possible actions. It requires Gymnasium installed with access to the LunarLander environment. It is useful for developers to tailor their models according to the shape and type of input and output spaces.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit1/unit1.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# We create our environment with gym.make(\"<name_of_the_environment>\")\nenv = gym.make(\"LunarLander-v2\")\nenv.reset()\nprint(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"Observation Space Shape\", env.observation_space.shape)\nprint(\"Sample observation\", env.observation_space.sample()) # Get a random observation\n\nprint(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"Action Space Shape\", env.action_space.n)\nprint(\"Action Space Sample\", env.action_space.sample()) # Take a random action\n```\n\n----------------------------------------\n\nTITLE: Training a Reinforce Policy Gradient Agent with PyTorch in Python\nDESCRIPTION: Defines the reinforce() function to train a policy gradient agent using the REINFORCE algorithm. It accepts the policy network, optimizer, number of training episodes, max timesteps per episode, discount factor gamma, and print frequency as inputs. The function collects log probabilities and rewards per episode, computes discounted returns in a numerically stable manner by iterating backwards using dynamic programming, standardizes these returns, calculates the policy loss, performs gradient descent, and tracks scores for performance monitoring. Requires PyTorch, numpy, and a gym environment named 'env' to be initialized externally.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n    # Help us to calculate the score during the training\n    scores_deque = deque(maxlen=100)\n    scores = []\n    # Line 3 of pseudocode\n    for i_episode in range(1, n_training_episodes+1):\n        saved_log_probs = []\n        rewards = []\n        state = env.reset()\n        # Line 4 of pseudocode\n        for t in range(max_t):\n            action, log_prob = policy.act(state)\n            saved_log_probs.append(log_prob)\n            state, reward, done, _ = env.step(action)\n            rewards.append(reward)\n            if done:\n                break \n        scores_deque.append(sum(rewards))\n        scores.append(sum(rewards))\n        \n        # Line 6 of pseudocode: calculate the return\n        returns = deque(maxlen=max_t) \n        n_steps = len(rewards) \n        # Compute the discounted returns at each timestep,\n        # as \n        #      the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n        #\n        # In O(N) time, where N is the number of time steps\n        # (this definition of the discounted return G_t follows the definition of this quantity \n        # shown at page 44 of Sutton&Barto 2017 2nd draft)\n        # G_t = r_(t+1) + r_(t+2) + ...\n        \n        # Given this formulation, the returns at each timestep t can be computed \n        # by re-using the computed future returns G_(t+1) to compute the current return G_t\n        # G_t = r_(t+1) + gamma*G_(t+1)\n        # G_(t-1) = r_t + gamma* G_t\n        # (this follows a dynamic programming approach, with which we memorize solutions in order \n        # to avoid computing them multiple times)\n        \n        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)\n        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...\n        \n        \n        ## Given the above, we calculate the returns at timestep t as: \n        #               gamma[t] * return[t] + reward[t]\n        #\n        ## We compute this starting from the last timestep to the first, in order\n        ## to employ the formula presented above and avoid redundant computations that would be needed \n        ## if we were to do it from first to last.\n        \n        ## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps\n        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)\n        ## a normal python list would instead require O(N) to do this.\n        for t in range(n_steps)[::-1]:\n            disc_return_t = (returns[0] if len(returns)>0 else 0)\n            returns.appendleft( gamma*disc_return_t + rewards[t]   )    \n            \n        ## standardization of the returns is employed to make training more stable\n        eps = np.finfo(np.float32).eps.item()\n        ## eps is the smallest representable float, which is \n        # added to the standard deviation of the returns to avoid numerical instabilities        \n        returns = torch.tensor(returns)\n        returns = (returns - returns.mean()) / (returns.std() + eps)\n        \n        # Line 7:\n        policy_loss = []\n        for log_prob, disc_return in zip(saved_log_probs, returns):\n            policy_loss.append(-log_prob * disc_return)\n        policy_loss = torch.cat(policy_loss).sum()\n        \n        # Line 8: PyTorch prefers gradient descent \n        optimizer.zero_grad()\n        policy_loss.backward()\n        optimizer.step()\n        \n        if i_episode % print_every == 0:\n            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n        \n    return scores\n```\n\n----------------------------------------\n\nTITLE: Importing Required Packages\nDESCRIPTION: This imports all the required libraries and modules for implementing the A2C algorithm using Stable Baselines3 in the Panda-Gym environment.  It imports necessary classes for environment creation, model training, evaluation, and Hugging Face Hub integration.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport gymnasium as gym\nimport panda_gym\n\nfrom huggingface_sb3 import load_from_hub, package_to_hub\n\nfrom stable_baselines3 import A2C\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\nfrom stable_baselines3.common.env_util import make_vec_env\n\nfrom huggingface_hub import notebook_login\n```\n\n----------------------------------------\n\nTITLE: Defining a Policy Neural Network with PyTorch (Python)\nDESCRIPTION: Defines a Policy neural network with two fully connected layers using PyTorch. The first layer maps state input to a hidden size and applies ReLU activation, while the second projects to the action space and applies Softmax to output a probability distribution over actions. The act method converts a state into a tensor, passes it through the network, and samples an action from the resulting distribution, returning both the action and its log probability. This class requires PyTorch and torch.distributions.Categorical, and expects inputs as numpy arrays of states. Proper sampling rather than greedy selection is necessary for stochastic policy behavior.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nclass Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size):\n        super(Policy, self).__init__()\n        self.fc1 = nn.Linear(s_size, h_size)\n        self.fc2 = nn.Linear(h_size, a_size)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.softmax(x, dim=1)\n    \n    def act(self, state):\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        probs = self.forward(state).cpu()\n        m = Categorical(probs)\n        action = m.sample()\n        return action.item(), m.log_prob(action)\n```\n\n----------------------------------------\n\nTITLE: Training and Saving PPO Agent for LunarLander Environment in Python\nDESCRIPTION: This snippet trains the previously instantiated PPO model on the LunarLander-v2 environment for 1,000,000 timesteps, which typically takes around 20 minutes on GPU-enabled systems. After training, it saves the model to a file with a specified name for later use or deployment. Stable Baselines3 is required to provide the PPO implementation and model saving/loading functionalities. This snippet handles only training and persistence, assuming environment and model instantiation are done separately.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit1/unit1.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# SOLUTION\n# Train it for 1,000,000 timesteps\nmodel.learn(total_timesteps=1000000)\n# Save the model\nmodel_name = \"ppo-LunarLander-v2\"\nmodel.save(model_name)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Trained Agent - Python\nDESCRIPTION: This code snippet evaluates a trained reinforcement learning agent on the LunarLander-v2 environment. It uses the `evaluate_policy` function to calculate the mean and standard deviation of rewards over multiple episodes, providing an assessment of the agent's performance.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit1/unit1.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\neval_env = Monitor(gym.make(\"LunarLander-v2\", render_mode='rgb_array'))\nmean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\nprint(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Evaluating a Taxi-v3 Model from Hugging Face Hub (Python)\nDESCRIPTION: Demonstrates using the previously defined `load_from_hub` function to download a pre-trained Taxi-v3 Q-learning model ('q-learning.pkl') from a specific repository ('ThomasSimonini/q-Taxi-v3') on the Hugging Face Hub. It then prints the loaded model dictionary, recreates the corresponding Gymnasium environment using the `env_id` from the model, and finally calls a hypothetical `evaluate_agent` function to assess its performance using parameters stored within the loaded model.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_35\n\nLANGUAGE: Python\nCODE:\n```\nmodel = load_from_hub(repo_id=\"ThomasSimonini/q-Taxi-v3\", filename=\"q-learning.pkl\") # Try to use another model\n\nprint(model)\nenv = gym.make(model[\"env_id\"])\n\nevaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])\n```\n\n----------------------------------------\n\nTITLE: Creating and Initializing CartPole-v1 Environment in Gym\nDESCRIPTION: Initializes the CartPole-v1 environment from gym, creating separate instances for training and evaluation. Retrieves state and action space sizes for defining neural network input/output parameters.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nenv_id = \"CartPole-v1\"\n# Create the env\nenv = gym.make(env_id)\n\n# Create the evaluation env\neval_env = gym.make(env_id)\n\n# Get the state space and action space\ns_size = env.observation_space.shape[0]\na_size = env.action_space.n\n```\n\n----------------------------------------\n\nTITLE: Defining a Deep Neural Network Policy for Reinforcement Learning in PyTorch\nDESCRIPTION: Defines a PyTorch neural network policy class with an initializer and forward pass for reinforcement learning. The policy includes multiple linear layers with ReLU activations ending in a softmax for action probabilities. It also contains an `act` method that samples an action from the categorical distribution derived from the network's output probabilities. Requires PyTorch, torch.nn, torch.nn.functional, torch.distributions.Categorical, and availability of device for tensor placement. Intended for environments with discrete action spaces.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nclass Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size):\n        super(Policy, self).__init__()\n        # Define the three layers here\n\n    def forward(self, x):\n        # Define the forward process here\n        return F.softmax(x, dim=1)\n    \n    def act(self, state):\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        probs = self.forward(state).cpu()\n        m = Categorical(probs)\n        action = m.sample()\n        return action.item(), m.log_prob(action)\n```\n\n----------------------------------------\n\nTITLE: Training A2C Agent on PandaPickAndPlace-v3 Environment with Normalization in Python\nDESCRIPTION: Sets up and trains an Advantage Actor-Critic (A2C) model for the PandaPickAndPlace-v3 environment using multi-environment vectorization and observation/reward normalization. The code first creates a vectorized environment with 4 parallel sub-environments, applies VecNormalize wrapper to stabilize training inputs, then initializes the A2C model with multi-input capabilities and verbosity for training logs. Finally, the model is trained over 1 million timesteps. Dependencies include stable-baselines3, panda-gym, and supporting utility functions for vectorized environments.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# 1 - 2\nenv_id = \"PandaPickAndPlace-v3\"\nenv = make_vec_env(env_id, n_envs=4)\n\n# 3\nenv = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\n\n# 4\nmodel = A2C(policy = \"MultiInputPolicy\",\n            env = env,\n            verbose=1)\n# 5\nmodel.learn(1_000_000)\n```\n\n----------------------------------------\n\nTITLE: Evaluating PPO Agent Performance\nDESCRIPTION: Function to evaluate a trained PPO agent over multiple episodes and calculate the mean and standard deviation of rewards. It runs the agent through n_eval_episodes and returns performance metrics.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef _evaluate_agent(env, n_eval_episodes, policy):\n    \"\"\"\n    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n    :param env: The evaluation environment\n    :param n_eval_episodes: Number of episode to evaluate the agent\n    :param policy: The agent\n    \"\"\"\n    episode_rewards = []\n    for episode in range(n_eval_episodes):\n        state = env.reset()\n        step = 0\n        done = False\n        total_rewards_ep = 0\n\n        while done is False:\n            state = torch.Tensor(state).to(device)\n            action, _, _, _ = policy.get_action_and_value(state)\n            new_state, reward, done, info = env.step(action.cpu().numpy())\n            total_rewards_ep += reward\n            if done:\n                break\n            state = new_state\n        episode_rewards.append(total_rewards_ep)\n    mean_reward = np.mean(episode_rewards)\n    std_reward = np.std(episode_rewards)\n\n    return mean_reward, std_reward\n```\n\n----------------------------------------\n\nTITLE: Calculating Loss and Training Step in PPO with PyTorch - Python\nDESCRIPTION: This code snippet performs the main algorithmic step of PPO training using PyTorch. It calculates the approximate KL divergence, clipping fractions, normalized advantages, policy and value losses, entropy regularization, and then applies gradients with optional gradient clipping. Required dependencies include torch (PyTorch), numpy, and gym. Inputs are batches of observations, actions, log probabilities, advantages, returns, and values; the output is a model parameter update and scalar statistics. Arguments such as clip coefficients and normalization flags control loss computation. Proper tensor shapes and environment setup are assumed.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_27\n\nLANGUAGE: Python\nCODE:\n```\nb_obs[mb_inds], b_actions.long()[mb_inds]\n)\nlogratio = newlogprob - b_logprobs[mb_inds]\nratio = logratio.exp()\n\nwith torch.no_grad():\n    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n    old_approx_kl = (-logratio).mean()\n    approx_kl = ((ratio - 1) - logratio).mean()\n    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n\nmb_advantages = b_advantages[mb_inds]\nif args.norm_adv:\n    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n\n# Policy loss\npg_loss1 = -mb_advantages * ratio\npg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\npg_loss = torch.max(pg_loss1, pg_loss2).mean()\n\n# Value loss\nnewvalue = newvalue.view(-1)\nif args.clip_vloss:\n    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n    v_clipped = b_values[mb_inds] + torch.clamp(\n        newvalue - b_values[mb_inds],\n        -args.clip_coef,\n        args.clip_coef,\n    )\n    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n    v_loss = 0.5 * v_loss_max.mean()\nelse:\n    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n\nentropy_loss = entropy.mean()\nloss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n\noptimizer.zero_grad()\nloss.backward()\nnn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\noptimizer.step()\n```\n\n----------------------------------------\n\nTITLE: Serializing Training Hyperparameters and Q-Table for Model Reproducibility in Python\nDESCRIPTION: This snippet demonstrates how to create a model dictionary containing all relevant training hyperparameters and the learned Q-table for reproducibility and sharing. The model dictionary includes environment ID, maximum steps per episode, counts of training and evaluation episodes, seeds used for evaluation, learning rate, discount factor, exploration parameters, decay rate, and the Q-table itself. This structure provides a standardized way to save and share model metadata and weights, facilitating later loading and comparison.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nmodel = {\n    \"env_id\": env_id,\n    \"max_steps\": max_steps,\n    \"n_training_episodes\": n_training_episodes,\n    \"n_eval_episodes\": n_eval_episodes,\n    \"eval_seed\": eval_seed,\n    \"learning_rate\": learning_rate,\n    \"gamma\": gamma,\n    \"max_epsilon\": max_epsilon,\n    \"min_epsilon\": min_epsilon,\n    \"decay_rate\": decay_rate,\n    \"qtable\": Qtable_taxi,\n}\n```\n\n----------------------------------------\n\nTITLE: Pushing Q-Learning Model to Hugging Face Hub\nDESCRIPTION: Comprehensive function for evaluating a trained Q-Learning model, generating documentation, recording performance videos, and uploading all assets to the Hugging Face Hub. The function handles repository creation, model serialization, evaluation metrics calculation, model card generation, and file upload.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef push_to_hub(repo_id, model, env, video_fps=1, local_repo_path=\"hub\"):\n    \"\"\"\n    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n    This method does the complete pipeline:\n    - It evaluates the model\n    - It generates the model card\n    - It generates a replay video of the agent\n    - It pushes everything to the Hub\n\n    :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\n    :param env\n    :param video_fps: how many frame per seconds to record our video replay\n    (with taxi-v3 and frozenlake-v1 we use 1)\n    :param local_repo_path: where the local repository is\n    \"\"\"\n    _, repo_name = repo_id.split(\"/\")\n\n    eval_env = env\n    api = HfApi()\n\n    # Step 1: Create the repo\n    repo_url = api.create_repo(\n        repo_id=repo_id,\n        exist_ok=True,\n    )\n\n    # Step 2: Download files\n    repo_local_path = Path(snapshot_download(repo_id=repo_id))\n\n    # Step 3: Save the model\n    if env.spec.kwargs.get(\"map_name\"):\n        model[\"map_name\"] = env.spec.kwargs.get(\"map_name\")\n        if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n            model[\"slippery\"] = False\n\n    # Pickle the model\n    with open((repo_local_path) / \"q-learning.pkl\", \"wb\") as f:\n        pickle.dump(model, f)\n\n    # Step 4: Evaluate the model and build JSON with evaluation metrics\n    mean_reward, std_reward = evaluate_agent(\n        eval_env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"]\n    )\n\n    evaluate_data = {\n        \"env_id\": model[\"env_id\"],\n        \"mean_reward\": mean_reward,\n        \"n_eval_episodes\": model[\"n_eval_episodes\"],\n        \"eval_datetime\": datetime.datetime.now().isoformat(),\n    }\n\n    # Write a JSON file called \"results.json\" that will contain the\n    # evaluation results\n    with open(repo_local_path / \"results.json\", \"w\") as outfile:\n        json.dump(evaluate_data, outfile)\n\n    # Step 5: Create the model card\n    env_name = model[\"env_id\"]\n    if env.spec.kwargs.get(\"map_name\"):\n        env_name += \"-\" + env.spec.kwargs.get(\"map_name\")\n\n    if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n        env_name += \"-\" + \"no_slippery\"\n\n    metadata = {}\n    metadata[\"tags\"] = [env_name, \"q-learning\", \"reinforcement-learning\", \"custom-implementation\"]\n\n    # Add metrics\n    eval = metadata_eval_result(\n        model_pretty_name=repo_name,\n        task_pretty_name=\"reinforcement-learning\",\n        task_id=\"reinforcement-learning\",\n        metrics_pretty_name=\"mean_reward\",\n        metrics_id=\"mean_reward\",\n        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n        dataset_pretty_name=env_name,\n        dataset_id=env_name,\n    )\n\n    # Merges both dictionaries\n    metadata = {**metadata, **eval}\n\n    model_card = f\"\"\"\n  # **Q-Learning** Agent playing1 **{env_id}**\n  This is a trained model of a **Q-Learning** agent playing **{env_id}** .\n\n  ## Usage\n\n  model = load_from_hub(repo_id=\"{repo_id}\", filename=\"q-learning.pkl\")\n\n  # Don't forget to check if you need to add additional attributes (is_slippery=False etc)\n  env = gym.make(model[\"env_id\"])\n  \"\"\"\n\n    evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])\n\n    readme_path = repo_local_path / \"README.md\"\n    readme = \"\"\n    print(readme_path.exists())\n    if readme_path.exists():\n        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n            readme = f.read()\n    else:\n        readme = model_card\n\n    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(readme)\n\n    # Save our metrics to Readme metadata\n    metadata_save(readme_path, metadata)\n\n    # Step 6: Record a video\n    video_path = repo_local_path / \"replay.mp4\"\n    record_video(env, model[\"qtable\"], video_path, video_fps)\n\n    # Step 7. Push everything to the Hub\n    api.upload_folder(\n        repo_id=repo_id,\n        folder_path=repo_local_path,\n        path_in_repo=\".\",\n    )\n\n    print(\"Your model is pushed to the Hub. You can view your model here: \", repo_url)\n```\n\n----------------------------------------\n\nTITLE: Evaluating, Creating Model Card, Recording Video and Pushing Model to Hugging Face Hub in Python\nDESCRIPTION: Defines the `push_to_hub` function which implements a full pipeline to evaluate a reinforcement learning model, generate a replay video, create metadata and a model card, and upload all artifacts to the Hugging Face Hub. It accepts the repository id, PyTorch model, training hyperparameters, evaluation environment, and video fps as parameters. Internally, it creates or accesses the repo, saves the model and hyperparameters locally, evaluates agent performance generating statistics, builds readable metadata tags and metrics, writes a markdown model card, records the agent video replay, and uploads the full folder to the Hub. This requires PyTorch, huggingface_hub, JSON, imageio, and gym environment APIs. Outputs include the uploaded model artifacts on the Hugging Face website and console confirmation.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef push_to_hub(repo_id, \n                model,\n                hyperparameters,\n                eval_env,\n                video_fps=30\n                ):\n  \"\"\"\n  Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n  This method does the complete pipeline:\n  - It evaluates the model\n  - It generates the model card\n  - It generates a replay video of the agent\n  - It pushes everything to the Hub\n\n  :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\n  :param model: the pytorch model we want to save\n  :param hyperparameters: training hyperparameters\n  :param eval_env: evaluation environment\n  :param video_fps: how many frame per seconds to record our video replay \n  \"\"\"\n\n  _, repo_name = repo_id.split(\"/\")\n  api = HfApi()\n  \n  # Step 1: Create the repo\n  repo_url = api.create_repo(\n        repo_id=repo_id,\n        exist_ok=True,\n  )\n\n  with tempfile.TemporaryDirectory() as tmpdirname:\n    local_directory = Path(tmpdirname)\n  \n    # Step 2: Save the model\n    torch.save(model, local_directory / \"model.pt\")\n\n    # Step 3: Save the hyperparameters to JSON\n    with open(local_directory / \"hyperparameters.json\", \"w\") as outfile:\n      json.dump(hyperparameters, outfile)\n    \n    # Step 4: Evaluate the model and build JSON\n    mean_reward, std_reward = evaluate_agent(eval_env, \n                                            hyperparameters[\"max_t\"],\n                                            hyperparameters[\"n_evaluation_episodes\"], \n                                            model)\n    # Get datetime\n    eval_datetime = datetime.datetime.now()\n    eval_form_datetime = eval_datetime.isoformat()\n\n    evaluate_data = {\n          \"env_id\": hyperparameters[\"env_id\"], \n          \"mean_reward\": mean_reward,\n          \"n_evaluation_episodes\": hyperparameters[\"n_evaluation_episodes\"],\n          \"eval_datetime\": eval_form_datetime,\n    }\n\n    # Write a JSON file\n    with open(local_directory / \"results.json\", \"w\") as outfile:\n        json.dump(evaluate_data, outfile)\n\n    # Step 5: Create the model card\n    env_name = hyperparameters[\"env_id\"]\n    \n    metadata = {}\n    metadata[\"tags\"] = [\n          env_name,\n          \"reinforce\",\n          \"reinforcement-learning\",\n          \"custom-implementation\",\n          \"deep-rl-class\"\n      ]\n\n    # Add metrics\n    eval = metadata_eval_result(\n        model_pretty_name=repo_name,\n        task_pretty_name=\"reinforcement-learning\",\n        task_id=\"reinforcement-learning\",\n        metrics_pretty_name=\"mean_reward\",\n        metrics_id=\"mean_reward\",\n        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n        dataset_pretty_name=env_name,\n        dataset_id=env_name,\n      )\n\n    # Merges both dictionaries\n    metadata = {**metadata, **eval}\n\n    model_card = f\"\"\"\n  # **Reinforce** Agent playing **{env_id}**\n  This is a trained model of a **Reinforce** agent playing **{env_id}** .\n  To learn to use this model and train yours check Unit 4 of the Deep Reinforcement Learning Course: https://huggingface.co/deep-rl-course/unit4/introduction\n  \"\"\"\n\n    readme_path = local_directory / \"README.md\"\n    readme = \"\"\n    if readme_path.exists():\n        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n          readme = f.read()\n    else:\n      readme = model_card\n\n    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n      f.write(readme)\n\n    # Save our metrics to Readme metadata\n    metadata_save(readme_path, metadata)\n\n    # Step 6: Record a video\n    video_path =  local_directory / \"replay.mp4\"\n    record_video(env, model, video_path, video_fps)\n\n    # Step 7. Push everything to the Hub\n    api.upload_folder(\n          repo_id=repo_id,\n          folder_path=local_directory,\n          path_in_repo=\".\",\n    )\n\n    print(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")\n```\n\n----------------------------------------\n\nTITLE: Publishing Trained Stable Baselines3 Agent to Hugging Face Hub in Python\nDESCRIPTION: Demonstrates usage of the package_to_hub utility to push a trained Stable Baselines3 RL agent to the Hugging Face Hub repository. It accepts the model object, identifiers such as model_name and environment ID, the evaluation environment for generating metrics, repository details, and a commit message. This step automates evaluation, recording replays, generating model cards, and releasing artifacts to facilitate community sharing and leaderboard participation. Dependencies include huggingface_sb3 package and active authentication.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_sb3 import package_to_hub\n\npackage_to_hub(\n    model=model,\n    model_name=f\"a2c-{env_id}\",\n    model_architecture=\"A2C\",\n    env_id=env_id,\n    eval_env=eval_env,\n    repo_id=f\"ThomasSimonini/a2c-{env_id}\", # Change the username\n    commit_message=\"Initial commit\",\n)\n```\n\n----------------------------------------\n\nTITLE: Loading and Evaluating a Q-Learning Model on FrozenLake-v1 (No Slippery) from Hugging Face Hub in Python\nDESCRIPTION: This snippet demonstrates downloading a Q-Learning agent trained on the FrozenLake-v1 environment with 'no slippery' dynamics, re-instantiating the environment accordingly, and running model evaluation. The environment is customized by passing 'is_slippery=False' to 'gym.make', enabling evaluation on deterministic transitions. The model is loaded using `load_from_hub`, and agent performance is measured via `evaluate_agent`. Dependencies include gymnasium and a compatible Hugging Face model repository.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nmodel = load_from_hub(\n    repo_id=\"ThomasSimonini/q-FrozenLake-v1-no-slippery\", filename=\"q-learning.pkl\"\n)  # Try to use another model\n\nenv = gym.make(model[\"env_id\"], is_slippery=False)\n\nevaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])\n```\n\n----------------------------------------\n\nTITLE: Loading and Evaluating a FrozenLake Model from Hugging Face Hub (Python)\nDESCRIPTION: Loads a pre-trained Q-learning model for a non-slippery FrozenLake-v1 environment from the 'ThomasSimonini/q-FrozenLake-v1-no-slippery' repository on Hugging Face Hub using the `load_from_hub` function. It then creates the specific FrozenLake environment instance with `is_slippery=False` based on the loaded model's `env_id`. Finally, it evaluates the loaded agent using the `evaluate_agent` function and parameters extracted from the model dictionary.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_36\n\nLANGUAGE: Python\nCODE:\n```\nmodel = load_from_hub(repo_id=\"ThomasSimonini/q-FrozenLake-v1-no-slippery\", filename=\"q-learning.pkl\") # Try to use another model\n\nenv = gym.make(model[\"env_id\"], is_slippery=False)\n\nevaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])\n```\n\n----------------------------------------\n\nTITLE: Initialize Policy and Optimizer in PyTorch\nDESCRIPTION: Creates an instance of the policy network using the defined hyperparameters and initializes the Adam optimizer with the policy parameters and specified learning rate. The policy is moved to the appropriate device (CPU/GPU).\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Create policy and place it to the device\ncartpole_policy = Policy(\n    cartpole_hyperparameters[\"state_space\"],\n    cartpole_hyperparameters[\"action_space\"],\n    cartpole_hyperparameters[\"h_size\"],\n).to(device)\ncartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])\n```\n\n----------------------------------------\n\nTITLE: Define Actor-Critic Neural Network Class for PPO in PyTorch\nDESCRIPTION: Implements an Actor-Critic neural network with separate actor and critic networks, both initialized with orthogonal weights. The actor outputs action probabilities, and the critic estimates state values, supporting PPO policy updates.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\nclass Agent(nn.Module):\n    def __init__(self, envs):\n        super().__init__()\n        self.critic = nn.Sequential(\n            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, 1), std=1.0),\n        )\n        self.actor = nn.Sequential(\n            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01),\n        )\n\n    def get_value(self, x):\n        return self.critic(x)\n\n    def get_action_and_value(self, x, action=None):\n        logits = self.actor(x)\n        probs = Categorical(logits=logits)\n        if action is None:\n            action = probs.sample()\n        return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n```\n\n----------------------------------------\n\nTITLE: Parsing Arguments for PPO Experiments with argparse - Python\nDESCRIPTION: This function uses Python's argparse to define and parse all command-line options for running PPO RL experiments, allowing customization of learning parameters, logging, video capture, Hugging Face repository details, and more. Dependencies include argparse, os, and distutils.util for boolean arguments. It processes arguments for experiment name, seeds, environment, learning rates, optimizer configs, and upload settings, enabling reproducible and configurable runs. Returns a namespace object with argument values and computed batch sizes, ready for downstream functions.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef parse_args():\n    # fmt: off\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--exp-name\", type=str, default=os.path.basename(__file__).rstrip(\".py\"),\n        help=\"the name of this experiment\")\n    parser.add_argument(\"--seed\", type=int, default=1,\n        help=\"seed of the experiment\")\n    parser.add_argument(\"--torch-deterministic\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"if toggled, `torch.backends.cudnn.deterministic=False`\")\n    parser.add_argument(\"--cuda\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"if toggled, cuda will be enabled by default\")\n    parser.add_argument(\"--track\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n        help=\"if toggled, this experiment will be tracked with Weights and Biases\")\n    parser.add_argument(\"--wandb-project-name\", type=str, default=\"cleanRL\",\n        help=\"the wandb's project name\")\n    parser.add_argument(\"--wandb-entity\", type=str, default=None,\n        help=\"the entity (team) of wandb's project\")\n    parser.add_argument(\"--capture-video\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n        help=\"weather to capture videos of the agent performances (check out `videos` folder)\")\n\n    # Algorithm specific arguments\n    parser.add_argument(\"--env-id\", type=str, default=\"CartPole-v1\",\n        help=\"the id of the environment\")\n    parser.add_argument(\"--total-timesteps\", type=int, default=50000,\n        help=\"total timesteps of the experiments\")\n    parser.add_argument(\"--learning-rate\", type=float, default=2.5e-4,\n        help=\"the learning rate of the optimizer\")\n    parser.add_argument(\"--num-envs\", type=int, default=4,\n        help=\"the number of parallel game environments\")\n    parser.add_argument(\"--num-steps\", type=int, default=128,\n        help=\"the number of steps to run in each environment per policy rollout\")\n    parser.add_argument(\"--anneal-lr\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"Toggle learning rate annealing for policy and value networks\")\n    parser.add_argument(\"--gae\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"Use GAE for advantage computation\")\n    parser.add_argument(\"--gamma\", type=float, default=0.99,\n        help=\"the discount factor gamma\")\n    parser.add_argument(\"--gae-lambda\", type=float, default=0.95,\n        help=\"the lambda for the general advantage estimation\")\n    parser.add_argument(\"--num-minibatches\", type=int, default=4,\n        help=\"the number of mini-batches\")\n    parser.add_argument(\"--update-epochs\", type=int, default=4,\n        help=\"the K epochs to update the policy\")\n    parser.add_argument(\"--norm-adv\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"Toggles advantages normalization\")\n    parser.add_argument(\"--clip-coef\", type=float, default=0.2,\n        help=\"the surrogate clipping coefficient\")\n    parser.add_argument(\"--clip-vloss\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\")\n    parser.add_argument(\"--ent-coef\", type=float, default=0.01,\n        help=\"coefficient of the entropy\")\n    parser.add_argument(\"--vf-coef\", type=float, default=0.5,\n        help=\"coefficient of the value function\")\n    parser.add_argument(\"--max-grad-norm\", type=float, default=0.5,\n        help=\"the maximum norm for the gradient clipping\")\n    parser.add_argument(\"--target-kl\", type=float, default=None,\n        help=\"the target KL divergence threshold\")\n\n    # Adding HuggingFace argument\n    parser.add_argument(\"--repo-id\", type=str, default=\"ThomasSimonini/ppo-CartPole-v1\", help=\"id of the model repository from the Hugging Face Hub {username/repo_name}\")\n\n    args = parser.parse_args()\n    args.batch_size = int(args.num_envs * args.num_steps)\n    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n    # fmt: on\n    return args\n\n```\n\n----------------------------------------\n\nTITLE: Loading and Evaluating a Q-Learning Model on Taxi-v3 from Hugging Face Hub in Python\nDESCRIPTION: This snippet shows how to load a pre-trained Q-Learning model for the Taxi-v3 environment from the Hugging Face Hub, instantiate the environment, and evaluate the agent's performance. It retrieves the model using `load_from_hub`, re-creates the learning environment using `gym.make`, and assesses the agent by calling `evaluate_agent` with the required parameters. This workflow enables rapid benchmarking and comparison of community-contributed RL agents.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nmodel = load_from_hub(repo_id=\"ThomasSimonini/q-Taxi-v3\", filename=\"q-learning.pkl\")  # Try to use another model\n\nprint(model)\nenv = gym.make(model[\"env_id\"])\n\nevaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])\n```\n\n----------------------------------------\n\nTITLE: Implementing Greedy Policy for Q-Learning\nDESCRIPTION: Implementing the greedy policy that selects the action with the highest Q-value for a given state, used for exploitation and final policy.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef greedy_policy(Qtable, state):\n    # Exploitation: take the action with the highest state, action value\n    action = np.argmax(Qtable[state][:])\n\n    return action\n```\n\n----------------------------------------\n\nTITLE: Package Model to Hugging Face Hub\nDESCRIPTION: This function orchestrates the process of evaluating, generating a video, and uploading a trained reinforcement learning model to the Hugging Face Hub.  It creates a temporary directory to save the model, evaluates it, generates a replay video, creates a model card, adds logs, and then pushes the repository to the Hugging Face Hub. It requires the model, evaluation environment, and relevant hyperparameters.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef package_to_hub(repo_id,\n                model,\n                hyperparameters,\n                eval_env,\n                video_fps=30,\n                commit_message=\"Push agent to the Hub\",\n                token= None,\n                logs=None\n                ):\n  \"\"\"\n  Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n  This method does the complete pipeline:\n  - It evaluates the model\n  - It generates the model card\n  - It generates a replay video of the agent\n  - It pushes everything to the hub\n  :param repo_id: id of the model repository from the Hugging Face Hub\n  :param model: trained model\n  :param eval_env: environment used to evaluate the agent\n  :param fps: number of fps for rendering the video\n  :param commit_message: commit message\n  :param logs: directory on local machine of tensorboard logs you'd like to upload\n  \"\"\"\n  msg.info(\n        \"This function will save, evaluate, generate a video of your agent, \"\n        \"create a model card and push everything to the hub. \"\n        \"It might take up to 1min. \\n \"\n        \"This is a work in progress: if you encounter a bug, please open an issue.\"\n    )\n  # Step 1: Clone or create the repo\n  repo_url = HfApi().create_repo(\n        repo_id=repo_id,\n        token=token,\n        private=False,\n        exist_ok=True,\n    )\n  \n  with tempfile.TemporaryDirectory() as tmpdirname:\n    tmpdirname = Path(tmpdirname)\n\n    # Step 2: Save the model\n    torch.save(model.state_dict(), tmpdirname / \"model.pt\")\n  \n    # Step 3: Evaluate the model and build JSON\n    mean_reward, std_reward = _evaluate_agent(eval_env, \n                                           10, \n                                           model)\n\n    # First get datetime\n    eval_datetime = datetime.datetime.now()\n    eval_form_datetime = eval_datetime.isoformat()\n\n    evaluate_data = {\n        \"env_id\": hyperparameters.env_id, \n        \"mean_reward\": mean_reward,\n        \"std_reward\": std_reward,\n        \"n_evaluation_episodes\": 10,\n        \"eval_datetime\": eval_form_datetime,\n    }\n \n    # Write a JSON file\n    with open(tmpdirname / \"results.json\", \"w\") as outfile:\n      json.dump(evaluate_data, outfile)\n\n    # Step 4: Generate a video\n    video_path =  tmpdirname / \"replay.mp4\"\n    record_video(eval_env, model, video_path, video_fps)\n  \n    # Step 5: Generate the model card\n    generated_model_card, metadata = _generate_model_card(\"PPO\", hyperparameters.env_id, mean_reward, std_reward, hyperparameters)\n    _save_model_card(tmpdirname, generated_model_card, metadata)\n\n    # Step 6: Add logs if needed\n    if logs:\n      _add_logdir(tmpdirname, Path(logs))\n  \n    msg.info(f\"Pushing repo {repo_id} to the Hugging Face Hub\")\n  \n    repo_url = upload_folder(\n            repo_id=repo_id,\n            folder_path=tmpdirname,\n            path_in_repo=\"\",\n            commit_message=commit_message,\n            token=token,\n        )\n\n    msg.info(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")\n  return repo_url\n```\n\n----------------------------------------\n\nTITLE: PPO Training Loop - Python\nDESCRIPTION: This code implements the main training loop for the PPO agent. It includes environment setup, agent initialization, data collection, advantage estimation, and policy/value network optimization. It also includes logging functionality using TensorBoard and WandB (optional).\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    args = parse_args()\n    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n    if args.track:\n        import wandb\n\n        wandb.init(\n            project=args.wandb_project_name,\n            entity=args.wandb_entity,\n            sync_tensorboard=True,\n            config=vars(args),\n            name=run_name,\n            monitor_gym=True,\n            save_code=True,\n        )\n    writer = SummaryWriter(f\"runs/{run_name}\")\n    writer.add_text(\n        \"hyperparameters\",\n        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n    )\n\n    # TRY NOT TO MODIFY: seeding\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.backends.cudnn.deterministic = args.torch_deterministic\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n\n    # env setup\n    envs = gym.vector.SyncVectorEnv(\n        [make_env(args.env_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n    )\n    assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n\n    agent = Agent(envs).to(device)\n    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n\n    # ALGO Logic: Storage setup\n    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n    values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n\n    # TRY NOT TO MODIFY: start the game\n    global_step = 0\n    start_time = time.time()\n    next_obs = torch.Tensor(envs.reset()).to(device)\n    next_done = torch.zeros(args.num_envs).to(device)\n    num_updates = args.total_timesteps // args.batch_size\n\n    for update in range(1, num_updates + 1):\n        # Annealing the rate if instructed to do so.\n        if args.anneal_lr:\n            frac = 1.0 - (update - 1.0) / num_updates\n            lrnow = frac * args.learning_rate\n            optimizer.param_groups[0][\"lr\"] = lrnow\n\n        for step in range(0, args.num_steps):\n            global_step += 1 * args.num_envs\n            obs[step] = next_obs\n            dones[step] = next_done\n\n            # ALGO LOGIC: action logic\n            with torch.no_grad():\n                action, logprob, _, value = agent.get_action_and_value(next_obs)\n                values[step] = value.flatten()\n            actions[step] = action\n            logprobs[step] = logprob\n\n            # TRY NOT TO MODIFY: execute the game and log data.\n            next_obs, reward, done, info = envs.step(action.cpu().numpy())\n            rewards[step] = torch.tensor(reward).to(device).view(-1)\n            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n\n            for item in info:\n                if \"episode\" in item.keys():\n                    print(f\"global_step={global_step}, episodic_return={item['episode']['r']}\")\n                    writer.add_scalar(\"charts/episodic_return\", item[\"episode\"][\"r\"], global_step)\n                    writer.add_scalar(\"charts/episodic_length\", item[\"episode\"][\"l\"], global_step)\n                    break\n\n        # bootstrap value if not done\n        with torch.no_grad():\n            next_value = agent.get_value(next_obs).reshape(1, -1)\n            if args.gae:\n                advantages = torch.zeros_like(rewards).to(device)\n                lastgaelam = 0\n                for t in reversed(range(args.num_steps)):\n                    if t == args.num_steps - 1:\n                        nextnonterminal = 1.0 - next_done\n                        nextvalues = next_value\n                    else:\n                        nextnonterminal = 1.0 - dones[t + 1]\n                        nextvalues = values[t + 1]\n                    delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n                    advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n                returns = advantages + values\n            else:\n                returns = torch.zeros_like(rewards).to(device)\n                for t in reversed(range(args.num_steps)):\n                    if t == args.num_steps - 1:\n                        nextnonterminal = 1.0 - next_done\n                        next_return = next_value\n                    else:\n                        nextnonterminal = 1.0 - dones[t + 1]\n                        next_return = returns[t + 1]\n                    returns[t] = rewards[t] + args.gamma * nextnonterminal * next_return\n                advantages = returns - values\n\n        # flatten the batch\n        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n        b_logprobs = logprobs.reshape(-1)\n        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n        b_advantages = advantages.reshape(-1)\n        b_returns = returns.reshape(-1)\n        b_values = values.reshape(-1)\n\n        # Optimizing the policy and value network\n        b_inds = np.arange(args.batch_size)\n        clipfracs = []\n        for epoch in range(args.update_epochs):\n            np.random.shuffle(b_inds)\n            for start in range(0, args.batch_size, args.minibatch_size):\n                end = start + args.minibatch_size\n                mb_inds = b_inds[start:end]\n\n                _, newlogprob, entropy, newvalue = agent.get_action_and_value(\n\n```\n\n----------------------------------------\n\nTITLE: Normalizing Observations and Rewards\nDESCRIPTION: This section sets up the environment with `VecNormalize`, normalizing observations and rewards.  Normalization stabilizes training and often improves results by scaling input features and rewards.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nenv = make_vec_env(env_id, n_envs=4)\n\n# Adding this wrapper to normalize the observation and the reward\nenv = # TODO: Add the wrapper\n```\n\n----------------------------------------\n\nTITLE: Code for Pyramid Environment Reward Function\nDESCRIPTION: This snippet defines the reward function used in the Pyramid environment, combining extrinsic environment feedback and an intrinsic curiosity-based reward to encourage exploration. Dependencies may include environment-specific variables and methods for calculating rewards based on agent actions and states.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit5/pyramids.mdx#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef compute_reward(state, action):\n    extrinsic_reward = get_environment_reward(state, action)\n    curiosity_reward = compute_curiosity(state, action)\n    total_reward = extrinsic_reward + curiosity_reward\n    return total_reward\n```\n\n----------------------------------------\n\nTITLE: Checking GPU Availability with PyTorch\nDESCRIPTION: This code snippet checks if a GPU is available and sets the device accordingly. If a GPU is available, it uses the CUDA device; otherwise, it defaults to the CPU. This helps in accelerating the training process when a GPU is present.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n```\n\n----------------------------------------\n\nTITLE: Evaluating Q-Learning Agent with Example Usage in Python\nDESCRIPTION: Simple usage example showing how to call the evaluate_agent function to obtain the mean and standard deviation of rewards from the Q-Learning agent. This snippet prints the mean reward with its uncertainty, illustrating agent performance evaluation in a typical reinforcement learning workflow.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# Evaluate our Agent\nmean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\nprint(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")\n```\n\n----------------------------------------\n\nTITLE: Interacting with Gymnasium Environment in Python\nDESCRIPTION: Demonstrates the basic interaction loop with a Gymnasium environment (LunarLander-v2). It shows how to create, reset, step through the environment using random actions, and handle episode termination or truncation by resetting.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit1/hands-on.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\n\n# First, we create our environment called LunarLander-v2\nenv = gym.make(\"LunarLander-v2\")\n\n# Then we reset this environment\nobservation, info = env.reset()\n\nfor _ in range(20):\n    # Take a random action\n    action = env.action_space.sample()\n    print(\"Action taken:\", action)\n\n    # Do this action in the environment and get\n    # next_state, reward, terminated, truncated and info\n    observation, reward, terminated, truncated, info = env.step(action)\n\n    # If the game is terminated (in our case we land, crashed) or truncated (timeout)\n    if terminated or truncated:\n        # Reset the environment\n        print(\"Environment is reset\")\n        observation, info = env.reset()\n\nenv.close()\n```\n\n----------------------------------------\n\nTITLE: Saving, loading, and evaluating an A2C model in Python\nDESCRIPTION: Code to save a trained model with normalization statistics, load it for evaluation, and publish it to the Hugging Face Hub. This demonstrates proper handling of normalized environments for evaluation.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit6/hands-on.mdx#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# 6\nmodel_name = \"a2c-PandaPickAndPlace-v3\";\nmodel.save(model_name)\nenv.save(\"vec_normalize.pkl\")\n\n# 7\nfrom stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n\n# Load the saved statistics\neval_env = DummyVecEnv([lambda: gym.make(\"PandaPickAndPlace-v3\")])\neval_env = VecNormalize.load(\"vec_normalize.pkl\", eval_env)\n\n#  do not update them at test time\neval_env.training = False\n# reward normalization is not needed at test time\neval_env.norm_reward = False\n\n# Load the agent\nmodel = A2C.load(model_name)\n\nmean_reward, std_reward = evaluate_policy(model, eval_env)\n\nprint(f\"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")\n\n# 8\npackage_to_hub(\n    model=model,\n    model_name=f\"a2c-{env_id}\",\n    model_architecture=\"A2C\",\n    env_id=env_id,\n    eval_env=eval_env,\n    repo_id=f\"ThomasSimonini/a2c-{env_id}\", # TODO: Change the username\n    commit_message=\"Initial commit\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Vectorized LunarLander Environment with Gymnasium in Python\nDESCRIPTION: This one-liner snippet shows how to create a vectorized LunarLander-v2 environment, which stacks multiple independent instances (16 in this case) into one environment to provide more diverse training data and improve sample efficiency. It requires the `make_vec_env` utility to be imported, typically from Stable Baselines3 or related RL wrappers. This approach is useful in training parallel agents to accelerate learning.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit1/unit1.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Create the environment\nenv = make_vec_env('LunarLander-v2', n_envs=16)\n```\n\n----------------------------------------\n\nTITLE: Packaging and Uploading Trained PPO Agent to Hugging Face Hub - Python\nDESCRIPTION: This function automates saving a trained PyTorch model, evaluating its performance, generating a replay video, creating a model card with metadata, and uploading the complete experiment package to the Hugging Face Hub. Dependencies required include torch, imageio, huggingface_hub, and required submodules for metadata handling. Inputs include the trained model, environment, user-specified repository id, and optional TensorBoard log directory. Outputs a URL to the created or updated model repository. This workflow ensures all experiment artifacts are preserved and published for reproducibility; the function is intended as the final deployment step for RL experiments.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef package_to_hub(\n    repo_id,\n    model,\n    hyperparameters,\n    eval_env,\n    video_fps=30,\n    commit_message=\"Push agent to the Hub\",\n    token=None,\n    logs=None,\n):\n    \"\"\"\n    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n    This method does the complete pipeline:\n    - It evaluates the model\n    - It generates the model card\n    - It generates a replay video of the agent\n    - It pushes everything to the hub\n    :param repo_id: id of the model repository from the Hugging Face Hub\n    :param model: trained model\n    :param eval_env: environment used to evaluate the agent\n    :param fps: number of fps for rendering the video\n    :param commit_message: commit message\n    :param logs: directory on local machine of tensorboard logs you'd like to upload\n    \"\"\"\n    msg.info(\n        \"This function will save, evaluate, generate a video of your agent, \"\n        \"create a model card and push everything to the hub. \"\n        \"It might take up to 1min. \\n \"\n        \"This is a work in progress: if you encounter a bug, please open an issue.\"\n    )\n    # Step 1: Clone or create the repo\n    repo_url = HfApi().create_repo(\n        repo_id=repo_id,\n        token=token,\n        private=False,\n        exist_ok=True,\n    )\n\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        tmpdirname = Path(tmpdirname)\n\n        # Step 2: Save the model\n        torch.save(model.state_dict(), tmpdirname / \"model.pt\")\n\n        # Step 3: Evaluate the model and build JSON\n        mean_reward, std_reward = _evaluate_agent(eval_env, 10, model)\n\n        # First get datetime\n        eval_datetime = datetime.datetime.now()\n        eval_form_datetime = eval_datetime.isoformat()\n\n        evaluate_data = {\n            \"env_id\": hyperparameters.env_id,\n            \"mean_reward\": mean_reward,\n            \"std_reward\": std_reward,\n            \"n_evaluation_episodes\": 10,\n            \"eval_datetime\": eval_form_datetime,\n        }\n\n        # Write a JSON file\n        with open(tmpdirname / \"results.json\", \"w\") as outfile:\n            json.dump(evaluate_data, outfile)\n\n        # Step 4: Generate a video\n        video_path = tmpdirname / \"replay.mp4\"\n        record_video(eval_env, model, video_path, video_fps)\n\n        # Step 5: Generate the model card\n        generated_model_card, metadata = _generate_model_card(\n            \"PPO\", hyperparameters.env_id, mean_reward, std_reward, hyperparameters\n        )\n        _save_model_card(tmpdirname, generated_model_card, metadata)\n\n        # Step 6: Add logs if needed\n        if logs:\n            _add_logdir(tmpdirname, Path(logs))\n\n        msg.info(f\"Pushing repo {repo_id} to the Hugging Face Hub\")\n\n        repo_url = upload_folder(\n            repo_id=repo_id,\n            folder_path=tmpdirname,\n            path_in_repo=\"\",\n            commit_message=commit_message,\n            token=token,\n        )\n\n        msg.info(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")\n    return repo_url\n\n```\n\n----------------------------------------\n\nTITLE: Environment with Normalization Wrapper\nDESCRIPTION: This code snippet adds the `VecNormalize` wrapper. The wrapper computes a running average and standard deviation of input features, along with reward normalization. `norm_obs` and `norm_reward` are set to true, while `clip_obs` is set to 10.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nenv = make_vec_env(env_id, n_envs=4)\n\nenv = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\n```\n\n----------------------------------------\n\nTITLE: Uploading Reinforce Model to Hugging Face Hub Python\nDESCRIPTION: Implements a comprehensive pipeline to evaluate, document, and upload a trained reinforcement learning model to the Hugging Face Hub. It creates a repository, saves the model and hyperparameters, evaluates the agent, generates a model card with metadata, records a video replay, and uploads all assets using the Hugging Face Hub API. Requires an HF token with write access, the trained PyTorch model, hyperparameters, an evaluation environment, and a pre-defined `evaluate_agent` function.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndef push_to_hub(repo_id,\n                model,\n                hyperparameters,\n                eval_env,\n                video_fps=30\n                ):\n  \"\"\"\n  Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n  This method does the complete pipeline:\n  - It evaluates the model\n  - It generates the model card\n  - It generates a replay video of the agent\n  - It pushes everything to the Hub\n\n  :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\n  :param model: the pytorch model we want to save\n  :param hyperparameters: training hyperparameters\n  :param eval_env: evaluation environment\n  :param video_fps: how many frame per seconds to record our video replay\n  \"\"\"\n\n  _, repo_name = repo_id.split(\"/\")\n  api = HfApi()\n\n  # Step 1: Create the repo\n  repo_url = api.create_repo(\n        repo_id=repo_id,\n        exist_ok=True,\n  )\n\n  with tempfile.TemporaryDirectory() as tmpdirname:\n    local_directory = Path(tmpdirname)\n\n    # Step 2: Save the model\n    torch.save(model, local_directory / \"model.pt\")\n\n    # Step 3: Save the hyperparameters to JSON\n    with open(local_directory / \"hyperparameters.json\", \"w\") as outfile:\n      json.dump(hyperparameters, outfile)\n\n    # Step 4: Evaluate the model and build JSON\n    mean_reward, std_reward = evaluate_agent(eval_env,\n                                            hyperparameters[\"max_t\"],\n                                            hyperparameters[\"n_evaluation_episodes\"],\n                                            model)\n    # Get datetime\n    eval_datetime = datetime.datetime.now()\n    eval_form_datetime = eval_datetime.isoformat()\n\n    evaluate_data = {\n          \"env_id\": hyperparameters[\"env_id\"],\n          \"mean_reward\": mean_reward,\n          \"n_evaluation_episodes\": hyperparameters[\"n_evaluation_episodes\"],\n          \"eval_datetime\": eval_form_datetime,\n    }\n\n    # Write a JSON file\n    with open(local_directory / \"results.json\", \"w\") as outfile:\n        json.dump(evaluate_data, outfile)\n\n    # Step 5: Create the model card\n    env_name = hyperparameters[\"env_id\"]\n\n    metadata = {}\n    metadata[\"tags\"] = [\n          env_name,\n          \"reinforce\",\n          \"reinforcement-learning\",\n          \"custom-implementation\",\n          \"deep-rl-class\"\n      ]\n\n    # Add metrics\n    eval = metadata_eval_result(\n        model_pretty_name=repo_name,\n        task_pretty_name=\"reinforcement-learning\",\n        task_id=\"reinforcement-learning\",\n        metrics_pretty_name=\"mean_reward\",\n        metrics_id=\"mean_reward\",\n        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n        dataset_pretty_name=env_name,\n        dataset_id=env_name,\n      )\n\n    # Merges both dictionaries\n    metadata = {**metadata, **eval}\n\n    model_card = f\"\"\"\n  # **Reinforce** Agent playing **{env_id}**\n  This is a trained model of a **Reinforce** agent playing **{env_id}** .\n  To learn to use this model and train yours check Unit 4 of the Deep Reinforcement Learning Course: https://huggingface.co/deep-rl-course/unit4/introduction\n  \"\"\"\n\n    readme_path = local_directory / \"README.md\"\n    readme = \"\"\n    if readme_path.exists():\n        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n          readme = f.read()\n    else:\n      readme = model_card\n\n    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n      f.write(readme)\n\n    # Save our metrics to Readme metadata\n    metadata_save(readme_path, metadata)\n\n    # Step 6: Record a video\n    video_path =  local_directory / \"replay.mp4\"\n    record_video(env, model, video_path, video_fps)\n\n    # Step 7. Push everything to the Hub\n    api.upload_folder(\n          repo_id=repo_id,\n          folder_path=local_directory,\n          path_in_repo=\".\",\n    )\n\n    print(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")\n```\n\n----------------------------------------\n\nTITLE: Model Saving and Evaluation Environment Creation with Gym and Hugging Face - Python\nDESCRIPTION: This snippet creates an evaluation environment using OpenAI Gym and uses the package_to_hub utility to upload the trained agent and related artifacts to the Hugging Face Hub. Dependencies: gym, a Hugging Face integration utility (package_to_hub), and a Hugging Face account. Inputs are model, hyperparameters, environment details, and log locations; acts as a post-training sharing/export step. The agent must be compatible with gym, and the Hugging Face repo ID and authentication should be configured prior.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_29\n\nLANGUAGE: Python\nCODE:\n```\neval_env = gym.make(args.env_id)\n\npackage_to_hub(\n    repo_id=args.repo_id,\n    model=agent,  # The model we want to save\n    hyperparameters=args,\n    eval_env=gym.make(args.env_id),\n    logs=f\"runs/{run_name}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Recording Agent Gameplay Video from Gym Environment in Python\nDESCRIPTION: Defines a function `record_video` that captures and saves a replay video of a reinforcement learning agent interacting with a gym environment. It takes as inputs the environment, the agent's policy object, output file directory, and frames per second (fps). The function resets the environment, repeatedly queries the policy for actions until a terminal state, renders each frame as an RGB array, stores frames, and writes them to a video file using imageio. Dependencies include a gym environment, a policy with an `act` method returning actions, numpy, and imageio. The video frames per second controls the playback speed.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef record_video(env, policy, out_directory, fps=30):\n  \"\"\"\n  Generate a replay video of the agent\n  :param env\n  :param Qtable: Qtable of our agent\n  :param out_directory\n  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n  \"\"\"\n  images = []  \n  done = False\n  state = env.reset()\n  img = env.render(mode='rgb_array')\n  images.append(img)\n  while not done:\n    # Take the action (index) that have the maximum expected future reward given that state\n    action, _ = policy.act(state)\n    state, reward, done, info = env.step(action) # We directly put next_state = state for recording logic\n    img = env.render(mode='rgb_array')\n    images.append(img)\n  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n```\n\n----------------------------------------\n\nTITLE: Implementing Agent Evaluation Function in Python\nDESCRIPTION: This function evaluates the trained agent over multiple episodes, using the greedy policy to select actions and calculating the average reward and standard deviation.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n    \"\"\"\n    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n    :param env: The evaluation environment\n    :param n_eval_episodes: Number of episode to evaluate the agent\n    :param Q: The Q-table\n    :param seed: The evaluation seed array (for taxi-v3)\n    \"\"\"\n    episode_rewards = []\n    for episode in tqdm(range(n_eval_episodes)):\n        if seed:\n            state, info = env.reset(seed=seed[episode])\n        else:\n            state, info = env.reset()\n        step = 0\n        truncated = False\n        terminated = False\n        total_rewards_ep = 0\n\n        for step in range(max_steps):\n            # Take the action (index) that have the maximum expected future reward given that state\n            action = greedy_policy(Q, state)\n            new_state, reward, terminated, truncated, info = env.step(action)\n            total_rewards_ep += reward\n\n            if terminated or truncated:\n                break\n            state = new_state\n        episode_rewards.append(total_rewards_ep)\n    mean_reward = np.mean(episode_rewards)\n    std_reward = np.std(episode_rewards)\n\n    return mean_reward, std_reward\n```\n\n----------------------------------------\n\nTITLE: Training the Reinforce Agent using Defined Policy and Optimizer in Python\nDESCRIPTION: Calls the reinforce() training function with the initialized policy network, optimizer, and hyperparameters such as number of training episodes, max timesteps, discount factor gamma, and print frequency. This executes the policy gradient training loop and returns the list of episode scores during training for further analysis or plotting.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nscores = reinforce(cartpole_policy,\n                   cartpole_optimizer,\n                   cartpole_hyperparameters[\"n_training_episodes\"], \n                   cartpole_hyperparameters[\"max_t\"],\n                   cartpole_hyperparameters[\"gamma\"], \n                   100)\n```\n\n----------------------------------------\n\nTITLE: Downloading and Visualizing a Large-Scale Trained Agent\nDESCRIPTION: This sequence downloads a high-performance agent trained for 10B steps from the HF Hub, loads it into the environment, runs evaluation with video recording, and displays the output video inline.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-sf.mdx#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n#download the agent from the hub\npython -m sample_factory.huggingface.load_from_hub -r edbeeching/doom_health_gathering_supreme_2222 -d ./train_dir\n```\n\n----------------------------------------\n\nTITLE: Publishing a trained A2C model to Hugging Face Hub\nDESCRIPTION: Code to package and publish a trained A2C reinforcement learning model to the Hugging Face Hub using the package_to_hub function.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit6/hands-on.mdx#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_sb3 import package_to_hub\n\npackage_to_hub(\n    model=model,\n    model_name=f\"a2c-{env_id}\",\n    model_architecture=\"A2C\",\n    env_id=env_id,\n    eval_env=eval_env,\n    repo_id=f\"ThomasSimonini/a2c-{env_id}\", # Change the username\n    commit_message=\"Initial commit\",\n)\n```\n\n----------------------------------------\n\nTITLE: Defining the Model Dictionary for FrozenLake Q-Learning (Python)\nDESCRIPTION: Creates a Python dictionary named `model` to store essential information about the trained FrozenLake-v1 Q-learning agent. This includes environment details, training/evaluation parameters (steps, episodes, seed), learning hyperparameters (learning rate, gamma, epsilon values, decay rate), and the learned Q-table (`Qtable_frozenlake`). This dictionary structure is used by the `push_to_hub` function.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_24\n\nLANGUAGE: Python\nCODE:\n```\nmodel = {\n    \"env_id\": env_id,\n    \"max_steps\": max_steps,\n    \"n_training_episodes\": n_training_episodes,\n    \"n_eval_episodes\": n_eval_episodes,\n    \"eval_seed\": eval_seed,\n\n    \"learning_rate\": learning_rate,\n    \"gamma\": gamma,\n\n    \"max_epsilon\": max_epsilon,\n    \"min_epsilon\": min_epsilon,\n    \"decay_rate\": decay_rate,\n\n    \"qtable\": Qtable_frozenlake\n}\n```\n\n----------------------------------------\n\nTITLE: Registering All ViZDoom Components\nDESCRIPTION: This function calls environment and model registration functions to set up all ViZDoom scenarios and architectures in Sample Factory, preparing the environment for training.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-sf.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef register_vizdoom_components():\n    register_vizdoom_envs()\n    register_vizdoom_models()\n```\n\n----------------------------------------\n\nTITLE: Computing Policy Loss with Clipping for PPO\nDESCRIPTION: This snippet calculates the PPO policy surrogate loss by comparing advantages scaled by the probability ratio, with a clipped version to prevent large policy updates. It takes the element-wise maximum of the unclipped and clipped losses, then averages the result.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\npg_loss1 = -mb_advantages * ratio\npg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\npg_loss = torch.max(pg_loss1, pg_loss2).mean()\n```\n\n----------------------------------------\n\nTITLE: Evaluate A2C Agent\nDESCRIPTION: This python script evaluates the trained A2C agent. The VecNormalize statistics are loaded, and the environment's render mode is set to \"rgb_array\". The trained model is loaded and evaluated.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit6/hands-on.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n\n# Load the saved statistics\neval_env = DummyVecEnv([lambda: gym.make(\"PandaReachDense-v3\")])\neval_env = VecNormalize.load(\"vec_normalize.pkl\", eval_env)\n\n# We need to override the render_mode\neval_env.render_mode = \"rgb_array\"\n\n#  do not update them at test time\neval_env.training = False\n# reward normalization is not needed at test time\neval_env.norm_reward = False\n\n# Load the agent\nmodel = A2C.load(\"a2c-PandaReachDense-v3\")\n\nmean_reward, std_reward = evaluate_policy(model, eval_env)\n\nprint(f\"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")\n```\n\n----------------------------------------\n\nTITLE: Evaluate Agent and Return Rewards\nDESCRIPTION: This function evaluates a given agent in a specified environment for a set number of episodes. It returns the average reward and standard deviation of rewards obtained across these episodes. It requires an environment, number of evaluation episodes, and the trained policy/agent.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef _evaluate_agent(env, n_eval_episodes, policy):\n  \"\"\"\n  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n  :param env: The evaluation environment\n  :param n_eval_episodes: Number of episode to evaluate the agent\n  :param policy: The agent\n  \"\"\"\n  episode_rewards = []\n  for episode in range(n_eval_episodes):\n    state = env.reset()\n    step = 0\n    done = False\n    total_rewards_ep = 0\n    \n    while done is False:\n      state = torch.Tensor(state).to(device)\n      action, _, _, _ = policy.get_action_and_value(state)\n      new_state, reward, done, info = env.step(action.cpu().numpy())\n      total_rewards_ep += reward    \n      if done:\n        break\n      state = new_state\n    episode_rewards.append(total_rewards_ep)\n  mean_reward = np.mean(episode_rewards)\n  std_reward = np.std(episode_rewards)\n\n  return mean_reward, std_reward\n```\n\n----------------------------------------\n\nTITLE: Getting State and Action Space Dimensions\nDESCRIPTION: Retrieving the number of possible states and actions from the environment to determine Q-table dimensions.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nstate_space = env.observation_space.n\nprint(\"There are \", state_space, \" possible states\")\n\naction_space = env.action_space.n\nprint(\"There are \", action_space, \" possible actions\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Epsilon-Greedy Policy for Q-Learning\nDESCRIPTION: Implementing the epsilon-greedy policy that balances exploration and exploitation during training by sometimes taking random actions.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef epsilon_greedy_policy(Qtable, state, epsilon):\n    # Randomly generate a number between 0 and 1\n    random_num = random.uniform(0, 1)\n    # if random_num > greater than epsilon --> exploitation\n    if random_num > epsilon:\n        # Take the action with the highest value given a state\n        # np.argmax can be useful here\n        action = greedy_policy(Qtable, state)\n    # else --> exploration\n    else:\n        action = env.action_space.sample()\n\n    return action\n```\n\n----------------------------------------\n\nTITLE: Start Reinforce Training in Python\nDESCRIPTION: Calls the `reinforce` function to begin the training process for the CartPole policy. Passes the initialized policy, optimizer, and relevant hyperparameters such as episode counts, maximum timesteps, and discount factor.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nscores = reinforce(\n    cartpole_policy,\n    cartpole_optimizer,\n    cartpole_hyperparameters[\"n_training_episodes\"],\n    cartpole_hyperparameters[\"max_t\"],\n    cartpole_hyperparameters[\"gamma\"],\n    100,\n)\n```\n\n----------------------------------------\n\nTITLE: Normalize Observations and Rewards\nDESCRIPTION: This python code normalizes the observations and rewards of the environment using VecNormalize from Stable-Baselines3. It is used to improve the training stability and convergence by scaling the input features and rewards.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit6/hands-on.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nenv = make_vec_env(env_id, n_envs=4)\n\nenv = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\n```\n\n----------------------------------------\n\nTITLE: Printing Observation Space Information\nDESCRIPTION: This code snippet prints the observation space size and a sample observation from the CartPole-v1 environment.  It retrieves and displays the state space size and provides an example of a possible observation.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprint(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"The State Space is: \", s_size)\nprint(\"Sample observation\", env.observation_space.sample())  # Get a random observation\n```\n\n----------------------------------------\n\nTITLE: Normalizing Advantages in PPO Policy Update\nDESCRIPTION: This segment normalizes advantages across minibatches to stabilize training, based on the mean and standard deviation of advantages. Normalization is conditional on the argument `args.norm_adv`.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_24\n\nLANGUAGE: Python\nCODE:\n```\nmb_advantages = b_advantages[mb_inds]\nif args.norm_adv:\n    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n```\n\n----------------------------------------\n\nTITLE: Publishing Trained Reinforcement Learning Model on Hugging Face Hub with Python\nDESCRIPTION: This snippet demonstrates uploading the trained model, including the policy network, hyperparameters, and evaluation environment, to Hugging Face Hub using the push_to_hub function. It requires the repo ID, trained model, hyperparameters, and evaluation environment, and specifies video frame rate. Dependencies include the Hugging Face Hub library.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_31\n\nLANGUAGE: Python\nCODE:\n```\nrepo_id = \"\"  # TODO Define your repo id {username/Reinforce-{model-id}}\npush_to_hub(\n    repo_id,\n    pixelcopter_policy,  # The model we want to save\n    pixelcopter_hyperparameters,  # Hyperparameters\n    eval_env,  # Evaluation environment\n    video_fps=30\n)\n```\n\n----------------------------------------\n\nTITLE: Create A2C Model\nDESCRIPTION: This python code creates the A2C model using Stable-Baselines3 library. The MultiInputPolicy is used due to the dictionary observation space.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit6/hands-on.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmodel = A2C(policy = \"MultiInputPolicy\",\n            env = env,\n            verbose=1)\n```\n\n----------------------------------------\n\nTITLE: Installing Deep RL Dependencies\nDESCRIPTION: This cell installs core packages required for the deep reinforcement learning environment, including Stable-Baselines3, Gymnasium, Panda-Gym, Hugging Face integration for SB3 and the Hub.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n!pip install stable-baselines3[extra]\n!pip install gymnasium\n```\n\nLANGUAGE: python\nCODE:\n```\n!pip install huggingface_sb3\n!pip install huggingface_hub\n!pip install panda_gym\n```\n\n----------------------------------------\n\nTITLE: Evaluating Q-Learning Agent in Python\nDESCRIPTION: Defines a function to evaluate the Q-Learning agent's performance over a specified number of episodes within an environment. It takes as input the environment, maximum steps per episode, number of evaluation episodes, the Q-table policy, and an optional seed array for deterministic resets. The function returns the mean and standard deviation of episode rewards, enabling performance assessment of the trained policy. It depends on numpy for calculating statistics and uses a helper greedy_policy function to select optimal actions.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n  \"\"\"\n  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n  :param env: The evaluation environment\n  :param max_steps: Maximum number of steps per episode\n  :param n_eval_episodes: Number of episode to evaluate the agent\n  :param Q: The Q-table\n  :param seed: The evaluation seed array (for taxi-v3)\n  \"\"\"\n  episode_rewards = []\n  for episode in tqdm(range(n_eval_episodes)):\n    if seed:\n      state, info = env.reset(seed=seed[episode])\n    else:\n      state, info = env.reset()\n    step = 0\n    truncated = False\n    terminated = False\n    total_rewards_ep = 0\n\n    for step in range(max_steps):\n      # Take the action (index) that have the maximum expected future reward given that state\n      action = greedy_policy(Q, state)\n      new_state, reward, terminated, truncated, info = env.step(action)\n      total_rewards_ep += reward\n\n      if terminated or truncated:\n        break\n      state = new_state\n    episode_rewards.append(total_rewards_ep)\n  mean_reward = np.mean(episode_rewards)\n  std_reward = np.std(episode_rewards)\n\n  return mean_reward, std_reward\n```\n\n----------------------------------------\n\nTITLE: Installing Gymnasium Atari Environment Shell\nDESCRIPTION: Installs the Gymnasium library specifically with support for Atari environments and accepts the Atari ROM license. This is required to use Atari game environments such as Space Invaders within Gymnasium.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit3/unit3.ipynb#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\n!pip install gymnasium[atari]\n!pip install gymnasium[accept-rom-license]\n```\n\n----------------------------------------\n\nTITLE: Training the ViZDoom Agent for Health Gathering Supreme\nDESCRIPTION: This code performs environment and model setup, then initiates training of the RL agent on the 'Health Gathering Supreme' scenario for 4 million steps. It configures environment parameters, training duration, and number of workers.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-sf.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nregister_vizdoom_components()\n\n# The scenario we train on today is health gathering\nenv = \"doom_health_gathering_supreme\"\ncfg = parse_vizdoom_cfg(\n    argv=[f\"--env={env}\", \"--num_workers=8\", \"--num_envs_per_worker=4\", \"--train_for_env_steps=4000000\"]\n)\n\nstatus = run_rl(cfg)\n```\n\n----------------------------------------\n\nTITLE: Implementing Q-Learning Training Loop (Python)\nDESCRIPTION: Defines the main function `train` that executes the Q-learning algorithm over a specified number of episodes. It includes epsilon decay, environment interaction (stepping), action selection using the epsilon-greedy policy, and updating the Q-table according to the Q-learning update rule (Bellman equation). It handles episode termination and returns the trained Q-table.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n  for episode in tqdm(range(n_training_episodes)):\n    # Reduce epsilon (because we need less and less exploration)\n    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n    # Reset the environment\n    state, info = env.reset()\n    step = 0\n    terminated = False\n    truncated = False\n\n    # repeat\n    for step in range(max_steps):\n      # Choose the action At using epsilon greedy policy\n      action = epsilon_greedy_policy(Qtable, state, epsilon)\n\n      # Take action At and observe Rt+1 and St+1\n      # Take the action (a) and observe the outcome state(s') and reward (r)\n      new_state, reward, terminated, truncated, info = env.step(action)\n\n      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n\n      # If terminated or truncated finish the episode\n      if terminated or truncated:\n        break\n\n      # Our next state is the new state\n      state = new_state\n  return Qtable\n```\n\n----------------------------------------\n\nTITLE: Installing Environment Dependencies for Deep RL in Python\nDESCRIPTION: This code installs environment libraries such as gym, gym-games, and huggingface_hub, and additional dependencies listed in a requirements file. It enables simulation and interaction with various RL environments and facilitates model sharing via Hugging Face Hub.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt\n```\n\n----------------------------------------\n\nTITLE: Defining the Model Dictionary for Taxi-v3 Q-Learning (Python)\nDESCRIPTION: Creates a Python dictionary named `model` to store essential information about the trained Taxi-v3 Q-learning agent. Similar to the FrozenLake example, it includes environment details, training/evaluation parameters, learning hyperparameters, and the learned Q-table (`Qtable_taxi`). This dictionary is prepared for uploading to the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_32\n\nLANGUAGE: Python\nCODE:\n```\nmodel = {\n    \"env_id\": env_id,\n    \"max_steps\": max_steps,\n    \"n_training_episodes\": n_training_episodes,\n    \"n_eval_episodes\": n_eval_episodes,\n    \"eval_seed\": eval_seed,\n\n    \"learning_rate\": learning_rate,\n    \"gamma\": gamma,\n\n    \"max_epsilon\": max_epsilon,\n    \"min_epsilon\": min_epsilon,\n    \"decay_rate\": decay_rate,\n\n    \"qtable\": Qtable_taxi\n}\n```\n\n----------------------------------------\n\nTITLE: Training the Q-Learning Agent for Taxi-v3 (Python)\nDESCRIPTION: Executes the training process for the Q-learning agent on the Taxi-v3 environment by calling a hypothetical `train` function. This function takes the defined training hyperparameters (`n_training_episodes`, epsilon values, `decay_rate`), the environment instance (`env`), `max_steps`, and the initialized `Qtable_taxi` as input. It returns the updated Q-table after training.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_31\n\nLANGUAGE: Python\nCODE:\n```\nQtable_taxi = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_taxi)\nQtable_taxi\n```\n\n----------------------------------------\n\nTITLE: Print Observation Space\nDESCRIPTION: This python script prints the observation space of the environment. It also prints an example of the observation to get a feel for what the agent will be provided.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit6/hands-on.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprint(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"The State Space is: \", s_size)\nprint(\"Sample observation\", env.observation_space.sample()) # Get a random observation\n```\n\n----------------------------------------\n\nTITLE: Training A2C Agent\nDESCRIPTION: This code trains the A2C agent in the environment.  The `learn` method trains for 1,000,000 timesteps.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmodel.learn(1_000_000)\n```\n\n----------------------------------------\n\nTITLE: Installing RL Baselines3 Zoo via pip\nDESCRIPTION: Install the RL Baselines3 Zoo library directly from GitHub to obtain the latest features and updates necessary for training RL agents. This command handles dependencies and prepares the environment for subsequent training procedures.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit3/hands-on.mdx#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\npip install git+https://github.com/DLR-RM/rl-baselines3-zoo\n```\n\n----------------------------------------\n\nTITLE: Creating A2C Model\nDESCRIPTION: This code instantiates the A2C model with the policy specified.  The `MultiInputPolicy` is used given the dictionary-based observation space. The `verbose` parameter is used for monitoring progress.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmodel = # Create the A2C model and try to find the best parameters\n```\n\n----------------------------------------\n\nTITLE: Installing RL Baselines3 Zoo Library Shell\nDESCRIPTION: Installs the RL Baselines3 Zoo library directly from its GitHub repository using the pip package manager. This library provides training and evaluation scripts for various reinforcement learning algorithms, built on top of Stable-Baselines3.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit3/unit3.ipynb#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\n!pip install git+https://github.com/DLR-RM/rl-baselines3-zoo\n```\n\n----------------------------------------\n\nTITLE: Executing Q-Learning Training (Python)\nDESCRIPTION: Calls the `train` function to initiate the Q-learning training process for the FrozenLake environment. It uses the defined hyperparameters (`n_training_episodes`, `min_epsilon`, `max_epsilon`, `decay_rate`, `max_steps`) and the initialized Q-table (`Qtable_frozenlake`) and environment (`env`). The updated Q-table after training is stored back into `Qtable_frozenlake`.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nQtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)\n```\n\n----------------------------------------\n\nTITLE: Calling push_to_hub Function Example Python\nDESCRIPTION: Provides an example call to the `push_to_hub` function, demonstrating how to pass the required arguments: repository ID (placeholder), the trained policy model, hyperparameters, and the evaluation environment. This initiates the process of pushing the agent to the Hugging Face Hub. Requires the `push_to_hub` function and the specified variables (`repo_id`, `cartpole_policy`, `cartpole_hyperparameters`, `eval_env`) to be defined.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nrepo_id = \"\"  # TODO Define your repo id {username/Reinforce-{model-id}}\npush_to_hub(\n    repo_id,\n    cartpole_policy,  # The model we want to save\n    cartpole_hyperparameters,  # Hyperparameters\n    eval_env,  # Evaluation environment\n    video_fps=30\n)\n```\n\n----------------------------------------\n\nTITLE: Training a Deep Q-Learning agent with RL Baselines3 Zoo\nDESCRIPTION: Run the training script specifying the algorithm (dqn), environment (SpaceInvadersNoFrameskip-v4), output folder, and hyperparameter configuration file. This command initiates training of the agent on the Atari environment.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit3/hands-on.mdx#_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\npython -m rl_zoo3.train --algo dqn --env SpaceInvadersNoFrameskip-v4 -f logs/ -c dqn.yml\n```\n\n----------------------------------------\n\nTITLE: Displaying Action Space Sample for PixelCopter Environment in Python\nDESCRIPTION: Prints out the size of the discrete action space and a sample action from the PixelCopter environment. This informs what actions an agent can choose from at each timestep.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nprint(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"The Action Space is: \", a_size)\nprint(\"Action Space Sample\", env.action_space.sample()) # Take a random action\n```\n\n----------------------------------------\n\nTITLE: Installing Sample Factory and ViZDoom via pip\nDESCRIPTION: This snippet installs the necessary packages for environment setup and RL training in ViZDoom scenarios. It prepares the environment by installing 'sample-factory' and 'vizdoom'.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-sf.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install sample-factory\npip install vizdoom\n```\n\n----------------------------------------\n\nTITLE: Importing RL Libraries in Python\nDESCRIPTION: Imports necessary Python libraries for Reinforcement Learning development. This includes Gymnasium for creating environments, Stable Baselines3 for RL algorithms (like PPO), and Hugging Face Hub libraries for model sharing and management.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit1/hands-on.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium\n\nfrom huggingface_sb3 import load_from_hub, package_to_hub\nfrom huggingface_hub import (\n    notebook_login,\n)  # To log to our Hugging Face account to be able to upload models to the Hub.\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.common.monitor import Monitor\n```\n\n----------------------------------------\n\nTITLE: Publishing a Trained Q-Learning Model to the Hugging Face Hub in Python\nDESCRIPTION: This snippet pushes a serialized model dictionary, along with metadata and the environment instance, to the Hugging Face Hub for sharing and leaderboard evaluation. The function `push_to_hub` requires a repository identifier (repo_id), the complete model dictionary, and an environment object. Users must set their Hugging Face Hub username and repository name for proper publishing. This enables reproducible sharing and comparison of reinforcement learning agents across the community.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nusername = \"\"  # FILL THIS\nrepo_name = \"\"  # FILL THIS\npush_to_hub(repo_id=f\"{username}/{repo_name}\", model=model, env=env)\n```\n\n----------------------------------------\n\nTITLE: Initializing Policy Network with PyTorch for Deep RL\nDESCRIPTION: This snippet defines and initializes the policy network using a custom Policy class, setting hyperparameters such as state and action space sizes, hidden layer size, and moving the model to the specified device. It also sets up the optimizer with Adam for training. Dependencies include PyTorch and the custom Policy class.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_29\n\nLANGUAGE: Python\nCODE:\n```\n# torch.manual_seed(50)\npolicy = Policy(\n    hyperparameters[\"state_space\"],\n    hyperparameters[\"action_space\"],\n    hyperparameters[\"h_size\"],\n).to(device)\n```\n\n----------------------------------------\n\nTITLE: Generating Hugging Face Model Card Metadata for PPO Agent - Python\nDESCRIPTION: Prepares the Hugging Face model card and associated metadata in compliance with the hub's requirements to accompany a trained PPO agent submission. It generates metadata tags from the provided model name, environment, mean reward, standard deviation, and hyperparameters namespace. The returned values include a Markdown-formatted string and a dict of metadata. Dependencies: custom generate_metadata function and correct structure of the hyperparameters argument. This procedure ensures uploaded models are discoverable, documented, and evaluated quantitatively on the Hub.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef _generate_model_card(model_name, env_id, mean_reward, std_reward, hyperparameters):\n    \"\"\"\n    Generate the model card for the Hub\n    :param model_name: name of the model\n    :env_id: name of the environment\n    :mean_reward: mean reward of the agent\n    :std_reward: standard deviation of the mean reward of the agent\n    :hyperparameters: training arguments\n    \"\"\"\n    # Step 1: Select the tags\n    metadata = generate_metadata(model_name, env_id, mean_reward, std_reward)\n\n    # Transform the hyperparams namespace to string\n\n```\n\n----------------------------------------\n\nTITLE: Initializing Policy Network and Optimizer with PyTorch in Python\nDESCRIPTION: Creates an instance of the Policy neural network using predefined state and action space sizes and hidden layer size, moving the model to the specified device (CPU or GPU). An Adam optimizer is instantiated for the policy parameters with the given learning rate. These initializations are required before training the policy with the reinforce function.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Create policy and place it to the device\ncartpole_policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\ncartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])\n```\n\n----------------------------------------\n\nTITLE: Evaluate Stable-Baselines3 Agent Performance\nDESCRIPTION: This Python snippet demonstrates how to evaluate the performance of a trained Stable-Baselines3 reinforcement learning agent on a specified environment. It uses the `evaluate_policy` function, requiring a separate evaluation environment wrapped with `Monitor` to record episode statistics.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit1/hands-on.mdx#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# @title\neval_env = Monitor(gym.make(\"LunarLander-v2\"))\nmean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\nprint(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Q-table for Taxi-v3 Environment\nDESCRIPTION: Creates and initializes a Q-table with the appropriate dimensions for the Taxi-v3 environment. The table has 500 rows (states) and 6 columns (actions), filled with zeros as initial values for the Q-learning algorithm.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n# Create our Q table with state_size rows and action_size columns (500x6)\nQtable_taxi = initialize_q_table(state_space, action_space)\nprint(Qtable_taxi)\nprint(\"Q-table shape: \", Qtable_taxi.shape)\n```\n\n----------------------------------------\n\nTITLE: Embedding Video Playback in Jupyter Notebook Using Python\nDESCRIPTION: This snippet leverages IPython's HTML display capabilities to embed a video player within the notebook, showing a recording of the trained agent's gameplay in Doom. It requires the IPython.display.HTML module and a reachable video source URL. The video is displayed with specified width and height and includes controls for playback. This snippet does not take inputs or produce outputs beyond rendering the video within the notebook interface.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part2.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import HTML\n\nHTML('''<video width=\"640\" height=\"480\" controls>\n  <source src=\"https://huggingface.co/edbeeching/doom_health_gathering_supreme_3333/resolve/main/replay.mp4\"\n  type=\"video/mp4\">Your browser does not support the video tag.</video>''')\n```\n\n----------------------------------------\n\nTITLE: Evaluating Trained PPO Agent on LunarLander Environment using Stable Baselines3 in Python\nDESCRIPTION: This snippet outlines the evaluation phase of a trained PPO model. It involves creating a separate evaluation environment to prevent training data leakage, evaluating the model over 10 deterministic episodes to obtain mean and standard deviation rewards, and printing the results. The environment wrapper `Monitor` is suggested to be used for better logging, and evaluate_policy function from Stable Baselines3 is implied for evaluation. This snippet depends on having a trained PPO model and access to Stable Baselines3 evaluation utilities.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit1/unit1.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# TODO: Evaluate the agent\n# Create a new environment for evaluation\neval_env = \n\n# Evaluate the model with 10 evaluation episodes and deterministic=True\nmean_reward, std_reward = \n\n# Print the results\n\n```\n\n----------------------------------------\n\nTITLE: Creating the Taxi-v3 Gymnasium Environment (Python)\nDESCRIPTION: Initializes the Taxi-v3 environment from the Gymnasium library. The `render_mode` is set to \"rgb_array\", which allows capturing frames as NumPy arrays, often used for recording or visualization.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_26\n\nLANGUAGE: Python\nCODE:\n```\nimport gymnasium as gym\n\nenv = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n```\n\n----------------------------------------\n\nTITLE: Evaluating a Trained ViZDoom Agent and Saving Video (Python)\nDESCRIPTION: Evaluates a trained policy using Sample Factory's `enjoy` function. It parses the configuration for evaluation mode, specifying the environment, enabling video saving (`--save_video`), disabling rendering (`--no_render`), and limiting the number of evaluation episodes.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part2.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom sample_factory.enjoy import enjoy\ncfg = parse_vizdoom_cfg(argv=[f\"--env={env}\", \"--num_workers=1\", \"--save_video\", \"--no_render\", \"--max_num_episodes=10\"], evaluation=True)\nstatus = enjoy(cfg)\n```\n\n----------------------------------------\n\nTITLE: Saving the Trained Model\nDESCRIPTION: This saves the trained A2C model along with the vector normalization statistics to the local file system, allowing for later use.  This is crucial to restore the model correctly.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Save the model and  VecNormalize statistics when saving the agent\nmodel.save(\"a2c-PandaReachDense-v3\")\nenv.save(\"vec_normalize.pkl\")\n```\n\n----------------------------------------\n\nTITLE: Importing Hugging Face Hub & Utility Libraries (Python)\nDESCRIPTION: Imports necessary functions and classes for interacting with the Hugging Face Hub (`HfApi`, `upload_folder`, etc.) and standard Python libraries for file/path manipulation, date/time handling, JSON processing, and image I/O. Also imports the `Printer` utility from `wasabi` for formatted messaging.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom huggingface_hub import HfApi, upload_folder\nfrom huggingface_hub.repocard import metadata_eval_result, metadata_save\n\nfrom pathlib import Path\nimport datetime\nimport tempfile\nimport json\nimport shutil\nimport imageio\n\nfrom wasabi import Printer\n\nmsg = Printer()\n```\n\n----------------------------------------\n\nTITLE: Pushing a Trained Taxi-v3 Q-Learning Agent to Hugging Face Hub (Python)\nDESCRIPTION: Calls the `push_to_hub` function to upload the trained Taxi-v3 agent to the Hugging Face Hub. It requires the target repository ID (`repo_id`, formatted as 'username/repo_name'), the `model` dictionary containing hyperparameters and the Q-table, and the Gymnasium environment instance (`env`). Requires prior authentication and user-filled `username` and `repo_name`.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_33\n\nLANGUAGE: Python\nCODE:\n```\nusername = \"\" # FILL THIS\nrepo_name = \"\" # FILL THIS\npush_to_hub(\n    repo_id=f\"{username}/{repo_name}\",\n    model=model,\n    env=env)\n```\n\n----------------------------------------\n\nTITLE: Loading Model from Hub - Python\nDESCRIPTION: This code snippet demonstrates how to load a pre-trained reinforcement learning model from the Hugging Face Hub. It uses the `load_from_hub` function to download the model and then `PPO.load()` to load it. It handles potential version compatibility issues with a custom objects parameter.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit1/unit1.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_sb3 import load_from_hub\nrepo_id = \"Classroom-workshop/assignment2-omar\" # The repo_id\nfilename = \"ppo-LunarLander-v2.zip\" # The model filename.zip\n\n# When the model was trained on Python 3.8 the pickle protocol is 5\n# But Python 3.6, 3.7 use protocol 4\n# In order to get compatibility we need to:\n# 1. Install pickle5 (we done it at the beginning of the colab)\n# 2. Create a custom empty object we pass as parameter to PPO.load()\ncustom_objects = {\n            \"learning_rate\": 0.0,\n            \"lr_schedule\": lambda _: 0.0,\n            \"clip_range\": lambda _: 0.0,\n}\n\ncheckpoint = load_from_hub(repo_id, filename)\nmodel = PPO.load(checkpoint, custom_objects=custom_objects, print_system_info=True)\n```\n\n----------------------------------------\n\nTITLE: Environment Creation - Python\nDESCRIPTION: This function creates a Gym environment with specified configurations. It wraps the environment with `RecordEpisodeStatistics` to track episode rewards and lengths, and optionally wraps it with `RecordVideo` to record videos of the agent's performance during training.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndef make_env(env_id, seed, idx, capture_video, run_name):\n    def thunk():\n        env = gym.make(env_id)\n        env = gym.wrappers.RecordEpisodeStatistics(env)\n        if capture_video:\n            if idx == 0:\n                env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n        env.seed(seed)\n        env.action_space.seed(seed)\n        env.observation_space.seed(seed)\n        return env\n\n    return thunk\n```\n\n----------------------------------------\n\nTITLE: Authentication for Hugging Face Hub using Notebook Login\nDESCRIPTION: Sets up authentication to the Hugging Face Hub by logging in through a notebook interface and configuring git credentials for uploading models.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit3/unit3.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\nnotebook_login()\n!git config --global credential.helper store\n```\n\n----------------------------------------\n\nTITLE: Training and Saving Stable Baselines3 PPO Model in Python\nDESCRIPTION: Trains the instantiated Stable Baselines3 PPO agent using the provided environment for a total of 1,000,000 timesteps. After training is complete, the trained model is saved to a file named 'ppo-LunarLander-v2'.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit1/hands-on.mdx#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# SOLUTION\n# Train it for 1,000,000 timesteps\nmodel.learn(total_timesteps=1000000)\n# Save the model\nmodel_name = \"ppo-LunarLander-v2\"\nmodel.save(model_name)\n```\n\n----------------------------------------\n\nTITLE: Displaying Trained Q-Table (Python)\nDESCRIPTION: Outputs the content of the `Qtable_frozenlake` variable to the console. This is typically done after the training process is complete to inspect the learned Q-values, which represent the agent's learned policy for each state-action pair.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nQtable_frozenlake\n```\n\n----------------------------------------\n\nTITLE: Generating Agent Replay Video Python\nDESCRIPTION: Defines a function to record a video replay of an agent interacting with an environment. It simulates an episode, captures frames using `env.render`, and saves them as a video file using `imageio.mimsave`. Requires a Gym/Gymnasium environment and an agent policy with an `act` method.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef record_video(env, policy, out_directory, fps=30):\n    \"\"\"\n    Generate a replay video of the agent\n    :param env\n    :param Qtable: Qtable of our agent\n    :param out_directory\n    :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n    \"\"\"\n    images = []\n    done = False\n    state = env.reset()\n    img = env.render(mode=\"rgb_array\")\n    images.append(img)\n    while not done:\n        # Take the action (index) that have the maximum expected future reward given that state\n        action, _ = policy.act(state)\n        state, reward, done, info = env.step(action)  # We directly put next_state = state for recording logic\n        img = env.render(mode=\"rgb_array\")\n        images.append(img)\n    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n```\n\n----------------------------------------\n\nTITLE: Creating FrozenLake Environment in Gymnasium\nDESCRIPTION: Setting up the FrozenLake-v1 environment with a 4x4 map, non-slippery mode, and rgb_array render mode for video recording.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nenv = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")\n```\n\n----------------------------------------\n\nTITLE: Create FrozenLake Environment - Python\nDESCRIPTION: This code creates a FrozenLake-v1 environment using `gym.make()`.  It initializes the environment with a 4x4 grid (`map_name=\"4x4\"`), the non-slippery version (`is_slippery=False`), and specifies `render_mode=\"rgb_array\"` to record video frames.  The environment is the core of the reinforcement learning task.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nenv = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")\n```\n\n----------------------------------------\n\nTITLE: Defining Q-Learning Hyperparameters (Python)\nDESCRIPTION: Defines various parameters used for configuring the Q-learning training and evaluation processes. These include the number of episodes, learning rate, discount factor (gamma), maximum steps per episode, and parameters controlling the epsilon-greedy exploration strategy (max epsilon, min epsilon, decay rate). These values significantly impact the agent's learning performance.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Training parameters\nn_training_episodes = 10000  # Total training episodes\nlearning_rate = 0.7          # Learning rate\n\n# Evaluation parameters\nn_eval_episodes = 100        # Total number of test episodes\n\n# Environment parameters\nenv_id = \"FrozenLake-v1\"     # Name of the environment\nmax_steps = 99               # Max steps per episode\ngamma = 0.95                 # Discounting rate\neval_seed = []               # The evaluation seed of the environment\n\n# Exploration parameters\nmax_epsilon = 1.0             # Exploration probability at start\nmin_epsilon = 0.05            # Minimum exploration probability\ndecay_rate = 0.0005            # Exponential decay rate for exploration prob\n```\n\n----------------------------------------\n\nTITLE: Recording Replay Video of Q-Learning Agent with Python\nDESCRIPTION: Defines a function that generates a replay video of the Q-Learning agent interacting with the environment. It repeatedly selects actions according to the Q-table's greedy policy until the episode terminates or truncates, rendering and saving images of each step. The images are compiled into a video file with a configurable frame rate, allowing visual inspection of the agent's learned behavior. Requires environment rendering support and the imageio library to save the gif/video.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndef record_video(env, Qtable, out_directory, fps=1):\n  \"\"\"\n  Generate a replay video of the agent\n  :param env\n  :param Qtable: Qtable of our agent\n  :param out_directory\n  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n  \"\"\"\n  images = []\n  terminated = False\n  truncated = False\n  state, info = env.reset(seed=random.randint(0,500))\n  img = env.render()\n  images.append(img)\n  while not terminated or truncated:\n    # Take the action (index) that have the maximum expected future reward given that state\n    action = np.argmax(Qtable[state][:])\n    state, reward, terminated, truncated, info = env.step(action) # We directly put next_state = state for recording logic\n    img = env.render()\n    images.append(img)\n  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n```\n\n----------------------------------------\n\nTITLE: Training Agent with REINFORCE Algorithm\nDESCRIPTION: Trains the Pixelcopter policy using the REINFORCE algorithm with specified hyperparameters including training episodes, maximum timesteps per episode, and discount factor. Returns the scores achieved during training and displays progress every 1000 episodes.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nscores = reinforce(pixelcopter_policy,\n                   pixelcopter_optimizer,\n                   pixelcopter_hyperparameters[\"n_training_episodes\"], \n                   pixelcopter_hyperparameters[\"max_t\"],\n                   pixelcopter_hyperparameters[\"gamma\"], \n                   1000)\n```\n\n----------------------------------------\n\nTITLE: Authenticate to Hugging Face Hub in Notebooks - Python\nDESCRIPTION: This snippet enables authentication with the Hugging Face Hub from a notebook environment such as Google Colab or Jupyter. It imports the notebook_login function from huggingface_hub, prompting the user to input their access token (with write permissions). The prerequisite is to have the huggingface_hub library installed and a valid HF account and token. On success, the notebook can interact with the Hub for uploading/downloading assets.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit5/unit5.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import notebook_login\nnotebook_login()\n```\n\n----------------------------------------\n\nTITLE: Defining Stable Baselines3 PPO Model in Python\nDESCRIPTION: Instantiates a Proximal Policy Optimization (PPO) agent from the Stable Baselines3 library. It configures the model with specific hyperparameters optimized for the LunarLander environment, using an MLPPolicy suitable for vector observations.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit1/hands-on.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# SOLUTION\n# We added some parameters to accelerate the training\nmodel = PPO(\n    policy=\"MlpPolicy\",\n    env=env,\n    n_steps=1024,\n    batch_size=64,\n    n_epochs=4,\n    gamma=0.999,\n    gae_lambda=0.98,\n    ent_coef=0.01,\n    verbose=1,\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating and Visualizing the Trained Policy\nDESCRIPTION: This snippet configures the evaluation environment to run a limited number of episodes without rendering, saving a video of the agent's performance. It uses the 'enjoy' function from Sample Factory to visualize agent behavior.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-sf.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom sample_factory.enjoy import enjoy\n\ncfg = parse_vizdoom_cfg(\n    argv=[f\"--env={env}\", \"--num_workers=1\", \"--save_video\", \"--no_render\", \"--max_num_episodes=10\"], evaluation=True\n)\nstatus = enjoy(cfg)\n```\n\n----------------------------------------\n\nTITLE: Initialize Neural Network Layer with Orthogonal Initialization in PyTorch\nDESCRIPTION: Initializes the weights of a neural network layer orthogonally with specified standard deviation and biases with a constant. Ensures stable training and controlled learning dynamics.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\ndef layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n    torch.nn.init.orthogonal_(layer.weight, std)\n    torch.nn.init.constant_(layer.bias, bias_const)\n    return layer\n```\n\n----------------------------------------\n\nTITLE: Training the Q-Learning Agent in Python\nDESCRIPTION: A single line of code to initiate the training of the Q-Learning agent using the defined parameters and environment.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nQtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)\n```\n\n----------------------------------------\n\nTITLE: Exploring Environment Observation Space\nDESCRIPTION: Displaying information about the observation space in FrozenLake, which represents the agent's position as an integer in a discrete space.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprint(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"Observation Space\", env.observation_space)\nprint(\"Sample observation\", env.observation_space.sample())  # Get a random observation\n```\n\n----------------------------------------\n\nTITLE: Downloading a Pretrained Doom Deathmatch Agent from HF Hub\nDESCRIPTION: This command uses the sample_factory utility to fetch a pre-trained Doom Deathmatch agent from the HF Hub and saves it locally for evaluation.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-sf.mdx#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n# Download the agent from the hub\npython -m sample_factory.huggingface.load_from_hub -r edbeeching/doom_deathmatch_bots_2222 -d ./train_dir\n```\n\n----------------------------------------\n\nTITLE: Downloading Model from Hugging Face Hub (Shell)\nDESCRIPTION: Uses the Sample Factory utility `load_from_hub` (run as a Python module) to download a pre-trained model checkpoint from a specified Hugging Face Hub repository (`-r`) into a local directory (`-d`).\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part2.ipynb#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n#download the agent from the hub\n!python -m sample_factory.huggingface.load_from_hub -r edbeeching/doom_health_gathering_supreme_2222 -d ./train_dir\n```\n\n----------------------------------------\n\nTITLE: Creating Custom FrozenLake Grid\nDESCRIPTION: Example of creating a custom grid layout for the FrozenLake environment with specified state positions and slippery mode enabled.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndesc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\ngym.make('FrozenLake-v1', desc=desc, is_slippery=True)\n```\n\n----------------------------------------\n\nTITLE: Implementing Greedy Policy (Python)\nDESCRIPTION: Defines the `greedy_policy` function which selects the action with the maximum Q-value for the specified state from the Q-table. This policy is used for exploitation, choosing the action currently believed to be the best. It uses `np.argmax` to find the index of the maximum value.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef greedy_policy(Qtable, state):\n  # Exploitation: take the action with the highest state, action value\n  action = np.argmax(Qtable[state][:])\n\n  return action\n```\n\n----------------------------------------\n\nTITLE: Pushing an ML-Agents Model to Hugging Face Hub using Bash\nDESCRIPTION: This command uploads a trained ML-Agents model to the Hugging Face Hub. It requires specifying the training run ID (`--run-id`), the local directory where the results are saved (`--local-dir`), the target repository ID on Hugging Face (`--repo-id` in the format 'username/repo-name'), and a commit message (`--commit-message`). If the repository doesn't exist, it will be created automatically.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unitbonus1/train.mdx#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nmlagents-push-to-hf --run-id=\"HuggyTraining\" --local-dir=\"./results/Huggy\" --repo-id=\"ThomasSimonini/ppo-Huggy\" --commit-message=\"Huggy\"\n```\n\n----------------------------------------\n\nTITLE: Loading Pre-trained DQN Model from Hugging Face Hub\nDESCRIPTION: Downloads a pre-trained Deep Q-Learning model for Beam Rider from the Stable-Baselines3 organization on the Hugging Face Hub and saves it to a local directory.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit3/unit3.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Download model and save it into the logs/ folder\n!python -m rl_zoo3.load_from_hub --algo dqn --env BeamRiderNoFrameskip-v4 -orga sb3 -f rl_trained/\n```\n\n----------------------------------------\n\nTITLE: Initializing Policy Network and Optimizer for Pixelcopter with PyTorch\nDESCRIPTION: Creates a policy network for the Pixelcopter environment and configures an Adam optimizer with the specified learning rate from hyperparameters. The policy network is constructed with the state space, action space, and hidden layer size.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n# torch.manual_seed(50)\npixelcopter_policy = Policy(pixelcopter_hyperparameters[\"state_space\"], pixelcopter_hyperparameters[\"action_space\"], pixelcopter_hyperparameters[\"h_size\"]).to(device)\npixelcopter_optimizer = optim.Adam(pixelcopter_policy.parameters(), lr=pixelcopter_hyperparameters[\"lr\"])\n```\n\n----------------------------------------\n\nTITLE: Evaluating a Reinforce Agent on Environment Episodes with Python\nDESCRIPTION: Defines a function evaluate_agent() that assesses the trained Reinforce agent's performance over a specified number of evaluation episodes. For each episode, the function resets the environment and repeatedly obtains actions from the policy, accumulating rewards until episode termination or max steps. Returns the mean and standard deviation of episode rewards. Requires a policy with an act() method and an evaluation environment. Useful for benchmarking the trained policy.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_agent(env, max_steps, n_eval_episodes, policy):\n  \"\"\"\n  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n  :param env: The evaluation environment\n  :param n_eval_episodes: Number of episode to evaluate the agent\n  :param policy: The Reinforce agent\n  \"\"\"\n  episode_rewards = []\n  for episode in range(n_eval_episodes):\n    state = env.reset()\n    step = 0\n    done = False\n    total_rewards_ep = 0\n    \n    for step in range(max_steps):\n      action, _ = policy.act(state)\n      new_state, reward, done, info = env.step(action)\n      total_rewards_ep += reward\n        \n      if done:\n        break\n      state = new_state\n    episode_rewards.append(total_rewards_ep)\n  mean_reward = np.mean(episode_rewards)\n  std_reward = np.std(episode_rewards)\n\n  return mean_reward, std_reward\n```\n\n----------------------------------------\n\nTITLE: Navigating to ML-Agents and Installing Packages\nDESCRIPTION: This snippet navigates to the cloned ml-agents directory and installs the required python packages for the project using pip. This is essential for using the ML-Agents framework.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd ml-agents\npip install -e ./ml-agents-envs\npip install -e ./ml-agents\n```\n\n----------------------------------------\n\nTITLE: Instantiating FrozenLake Q-Table (Python)\nDESCRIPTION: Calls the `initialize_q_table` function to create the Q-table specific to the FrozenLake environment using the previously determined state and action space sizes. The resulting table, initialized with zeros, is stored in the `Qtable_frozenlake` variable, ready for training.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nQtable_frozenlake = initialize_q_table(state_space, action_space)\n```\n\n----------------------------------------\n\nTITLE: Pushing a Trained FrozenLake Q-Learning Agent to Hugging Face Hub (Python)\nDESCRIPTION: Calls the `push_to_hub` function to upload the trained FrozenLake-v1 agent to the Hugging Face Hub. It requires the target repository ID (`repo_id`, formatted as 'username/repo_name'), the `model` dictionary containing hyperparameters and the Q-table, and the Gymnasium environment instance (`env`). Requires prior authentication using `notebook_login` or `huggingface-cli login`.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\nusername = \"\" # FILL THIS\nrepo_name = \"q-FrozenLake-v1-4x4-noSlippery\"\npush_to_hub(\n    repo_id=f\"{username}/{repo_name}\",\n    model=model,\n    env=env)\n```\n\n----------------------------------------\n\nTITLE: Training the Huggy Agent with mlagents-learn (Shell)\nDESCRIPTION: Shell command to initiate the training process for the Huggy agent using Unity ML-Agents within a notebook environment (indicated by '!'). It specifies the configuration file (`./config/ppo/Huggy.yaml`), the environment executable path (`./trained-envs-executables/linux/Huggy/Huggy`), a unique run ID (`Huggy2`), and disables graphical output (`--no-graphics`). Depends on ML-Agents being installed and the environment executable being available at the specified path.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus-unit1.ipynb#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\n!mlagents-learn ./config/ppo/Huggy.yaml --env=./trained-envs-executables/linux/Huggy/Huggy --run-id=\"Huggy2\" --no-graphics\n```\n\n----------------------------------------\n\nTITLE: Loading and Evaluating Pretrained A2C Agent in Python\nDESCRIPTION: Loads a pre-trained Advantage Actor-Critic (A2C) model from a saved checkpoint file and evaluates its performance on the provided evaluation environment using the evaluate_policy function. Prints the mean and standard deviation of the reward as measures of agent quality. Dependencies include Stable Baselines3 for model loading, and a compatible evaluation environment 'eval_env'. Expected inputs are the model checkpoint path and the evaluation environment. Outputs are the printed mean and standard deviation of rewards.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nmodel = A2C.load(\"a2c-PandaReachDense-v3\")\n\nmean_reward, std_reward = evaluate_policy(model, eval_env)\n\nprint(f\"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")\n```\n\n----------------------------------------\n\nTITLE: Generic Command to Push Agent\nDESCRIPTION: A template/example showing the needed parameters for uploading the trained agent to Hugging Face, specifying run ID, local directory, repository ID, and commit message.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nmlagents-push-to-hf  --run-id= # Add your run id  --local-dir= # Your local dir  --repo-id= # Your repo id --commit-message=\"First Push\"\n```\n\n----------------------------------------\n\nTITLE: Model and Environment Evaluation Setup\nDESCRIPTION: This code creates an evaluation environment using OpenAI Gym and calls a function to save the trained model along with hyperparameters, enabling community sharing via Hugging Face Hub.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_28\n\nLANGUAGE: Python\nCODE:\n```\neval_env = gym.make(args.env_id)\npackage_to_hub(repo_id = args.repo_id,\n            model = agent, # The model we want to save\n            hyperparameters = args,\n            eval_env = gym.make(args.env_id),\n            logs= f\"runs/{run_name}\",\n            )\n```\n\n----------------------------------------\n\nTITLE: Push Trained Agent to Hugging Face Hub - Python\nDESCRIPTION: This notebook shell command uploads a trained MLAgents agent to the Hugging Face Hub for sharing and visualization. It requires specifying a run ID, the local directory path to the saved agent results, the Hugging Face repo ID in format <username>/<repo>, and a commit message. Prerequisites: MLAgents and huggingface_hub must be installed, and the user must be authenticated. The command uploads model files and logs to the specified repository, making them accessible via the Hub web interface.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit5/unit5.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n!mlagents-push-to-hf --run-id=\"SnowballTarget1\" --local-dir=\"./results/SnowballTarget1\" --repo-id=\"ThomasSimonini/ppo-SnowballTarget\" --commit-message=\"First Push\"\n```\n\n----------------------------------------\n\nTITLE: Evaluating Agent and Pushing to Hugging Face Hub (Python)\nDESCRIPTION: Runs the Sample Factory evaluation process (`enjoy`) with added configuration flags to push the resulting model checkpoint and replay video to a specified Hugging Face Hub repository (`--push_to_hub`, `--hf_repository`). Requires prior authentication.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part2.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom sample_factory.enjoy import enjoy\n\nhf_username = \"ThomasSimonini\" # insert your HuggingFace username here\n\ncfg = parse_vizdoom_cfg(argv=[f\"--env={env}\", \"--num_workers=1\", \"--save_video\", \"--no_render\", \"--max_num_episodes=10\", \"--max_num_frames=100000\", \"--push_to_hub\", f\"--hf_repository={hf_username}/rl_course_vizdoom_health_gathering_supreme\"], evaluation=True)\nstatus = enjoy(cfg)\n```\n\n----------------------------------------\n\nTITLE: Saving Model Card - Python\nDESCRIPTION: This function saves the generated model card to a README.md file within the repository directory. It checks if a README.md already exists and either overwrites it with the new model card or appends the new content to the existing README. It also saves the model's metadata to the Readme.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndef _save_model_card(local_path, generated_model_card, metadata):\n    \"\"\"Saves a model card for the repository.\n    :param local_path: repository directory\n    :param generated_model_card: model card generated by _generate_model_card()\n    :param metadata: metadata\n    \"\"\"\n    readme_path = local_path / \"README.md\"\n    readme = \"\"\n    if readme_path.exists():\n        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n            readme = f.read()\n    else:\n        readme = generated_model_card\n\n    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(readme)\n\n    # Save our metrics to Readme metadata\n    metadata_save(readme_path, metadata)\n```\n\n----------------------------------------\n\nTITLE: Save or Update Model Card Markdown File in Python\nDESCRIPTION: Saves a model card markdown to a README.md file in the specified directory. If the file exists, updates its content; otherwise, creates new. Also saves associated metadata using a dedicated function.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\ndef _save_model_card(local_path, generated_model_card, metadata):\n    \"\"\"Saves a model card for the repository.\n    :param local_path: repository directory\n    :param generated_model_card: model card generated by _generate_model_card()\n    :param metadata: metadata\n    \"\"\"\n    readme_path = local_path / \"README.md\"\n    readme = \"\"\n    if readme_path.exists():\n        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n            readme = f.read()\n    else:\n        readme = generated_model_card\n\n    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(readme)\n\n    # Save our metrics to Readme metadata\n    metadata_save(readme_path, metadata)\n```\n\n----------------------------------------\n\nTITLE: Defining Hyperparameters for PixelCopter Reinforcement Learning Agent in Python\nDESCRIPTION: Defines a dictionary containing the training hyperparameters for the PixelCopter deep reinforcement learning agent. These hyperparameters include hidden layer size, number of training and evaluation episodes, max timesteps, discount factor gamma, learning rate, environment id, and sizes of state and action spaces. These parameters control training behavior and model capacity.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\npixelcopter_hyperparameters = {\n    \"h_size\": 64,\n    \"n_training_episodes\": 50000,\n    \"n_evaluation_episodes\": 10,\n    \"max_t\": 10000,\n    \"gamma\": 0.99,\n    \"lr\": 1e-4,\n    \"env_id\": env_id,\n    \"state_space\": s_size,\n    \"action_space\": a_size,\n}\n```\n\n----------------------------------------\n\nTITLE: Publishing Trained Model to Hugging Face Hub\nDESCRIPTION: Pushes the trained Pixelcopter policy model to the Hugging Face Hub with the specified repository ID, along with hyperparameters and evaluation environment. Also generates a demo video of the agent with the specified frame rate.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nrepo_id = \"\" #TODO Define your repo id {username/Reinforce-{model-id}}\npush_to_hub(repo_id,\n                pixelcopter_policy, # The model we want to save\n                pixelcopter_hyperparameters, # Hyperparameters\n                eval_env, # Evaluation environment\n                video_fps=30\n                )\n```\n\n----------------------------------------\n\nTITLE: Saving, Evaluating, and Publishing A2C Model with VecNormalize State in Python\nDESCRIPTION: Illustrates how to save the trained A2C model along with VecNormalize statistics, reload them for evaluation without updating normalization parameters, evaluate agent performance using evaluate_policy, and publish the model to the Hugging Face Hub. Loading VecNormalize state ensures consistent normalization between training and evaluation. The code assumes the environment and evaluation setup are compatible and highlights the need to disable training mode and reward normalization during evaluation. Publishing steps reuse package_to_hub with required metadata.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# 6\nmodel_name = \"a2c-PandaPickAndPlace-v3\";\nmodel.save(model_name)\nenv.save(\"vec_normalize.pkl\")\n\n# 7\nfrom stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n\n# Load the saved statistics\neval_env = DummyVecEnv([lambda: gym.make(\"PandaPickAndPlace-v3\")])\neval_env = VecNormalize.load(\"vec_normalize.pkl\", eval_env)\n\n#  do not update them at test time\neval_env.training = False\n# reward normalization is not needed at test time\neval_env.norm_reward = False\n\n# Load the agent\nmodel = A2C.load(model_name)\n\nmean_reward, std_reward = evaluate_policy(model, eval_env)\n\nprint(f\"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")\n\n# 8\npackage_to_hub(\n    model=model,\n    model_name=f\"a2c-{env_id}\",\n    model_architecture=\"A2C\",\n    env_id=env_id,\n    eval_env=eval_env,\n    repo_id=f\"ThomasSimonini/a2c-{env_id}\", # TODO: Change the username\n    commit_message=\"Initial commit\",\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Log Directory - Python\nDESCRIPTION: This function adds a log directory to the repository. It checks if the log directory exists and copies its content to a new directory named 'logs' within the repository. If a 'logs' directory already exists, it is deleted before copying the new log directory to prevent conflicts.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef _add_logdir(local_path: Path, logdir: Path):\n    \"\"\"Adds a logdir to the repository.\n    :param local_path: repository directory\n    :param logdir: logdir directory\n    \"\"\"\n    if logdir.exists() and logdir.is_dir():\n        # Add the logdir to the repository under new dir called logs\n        repo_logdir = local_path / \"logs\"\n\n        # Delete current logs if they exist\n        if repo_logdir.exists():\n            shutil.rmtree(repo_logdir)\n\n        # Copy logdir into repo logdir\n        shutil.copytree(logdir, repo_logdir)\n```\n\n----------------------------------------\n\nTITLE: Printing Observation Space Information (Python)\nDESCRIPTION: Prints the structure and shape of the observation space of the Gym environment and demonstrates sampling a random observation. This helps understand the representation of states in the environment. Requires an initialized Gym `env` object.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprint(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"Observation Space\", env.observation_space)\nprint(\"Sample observation\", env.observation_space.sample()) # Get a random observation\n```\n\n----------------------------------------\n\nTITLE: Initializing Virtual Display (Python)\nDESCRIPTION: Imports the `Display` class from the `pyvirtualdisplay` library and instantiates it to create a virtual display. The display is set to be invisible (`visible=0`) and given a specific size (`1400x900`). Calling `virtual_display.start()` activates the virtual display, making it possible to render and record environments.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit1/unit1.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()\n```\n\n----------------------------------------\n\nTITLE: Saving Model Card to Repository\nDESCRIPTION: Function to save the generated model card to the repository directory. It creates or updates the README.md file with the model card content and adds the metadata to it.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef _save_model_card(local_path, generated_model_card, metadata):\n    \"\"\"\n    Saves a model card for the repository.\n    :param local_path: repository directory\n    :param generated_model_card: model card generated by _generate_model_card()\n    :param metadata: metadata\n    \"\"\"\n    readme_path = local_path / \"README.md\"\n    readme = \"\"\n    if readme_path.exists():\n        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n            readme = f.read()\n    else:\n        readme = generated_model_card\n\n    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(readme)\n\n    # Save our metrics to Readme metadata\n    metadata_save(readme_path, metadata)\n```\n\n----------------------------------------\n\nTITLE: Setting Up a Virtual Environment and Installing Compatible Python via Miniconda - Python\nDESCRIPTION: This sequence combines pip, shell, and conda commands to create a virtualenv, install Miniconda, set up Python 3.10.12, and configure environment variables for ML-Agents compatibility. The script requires internet connectivity and admin rights in the Colab instance. It ensures the notebook uses a Python version supported by ML-Agents, preventing setup errors.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus_unit1.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Install virtualenv and create a virtual environment\n!pip install virtualenv\n!virtualenv myenv\n\n# Download and install Miniconda\n!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n!chmod +x Miniconda3-latest-Linux-x86_64.sh\n!./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n\n# Activate Miniconda and install Python ver 3.10.12\n!source /usr/local/bin/activate\n!conda install -q -y --prefix /usr/local python=3.10.12 ujson  # Specify the version here\n\n# Set environment variables for Python and conda paths\n!export PYTHONPATH=/usr/local/lib/python3.10/site-packages/\n!export CONDA_PREFIX=/usr/local/envs/myenv\n```\n\n----------------------------------------\n\nTITLE: Evaluating Loaded Model - Python\nDESCRIPTION: This snippet evaluates a loaded reinforcement learning agent using the same approach as the initial evaluation. It demonstrates how to evaluate a loaded model on the 'LunarLander-v2' environment, calculating mean reward and standard deviation.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit1/unit1.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\neval_env = Monitor(gym.make(\"LunarLander-v2\"))\nmean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\nprint(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n```\n\n----------------------------------------\n\nTITLE: Adding Logs Directory to Repository\nDESCRIPTION: Function to add a logs directory to the repository. It copies the logs from a specified directory to the repository under a 'logs' folder, which can be used for TensorBoard visualization.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef _add_logdir(local_path: Path, logdir: Path):\n    \"\"\"\n    Adds a logdir to the repository.\n    :param local_path: repository directory\n    :param logdir: logdir directory\n    \"\"\"\n    if logdir.exists() and logdir.is_dir():\n        # Add the logdir to the repository under new dir called logs\n        repo_logdir = local_path / \"logs\"\n\n        # Delete current logs if they exist\n        if repo_logdir.exists():\n            shutil.rmtree(repo_logdir)\n\n        # Copy logdir into repo logdir\n        shutil.copytree(logdir, repo_logdir)\n```\n\n----------------------------------------\n\nTITLE: Recording Agent Performance Video in Reinforcement Learning Environments\nDESCRIPTION: Creates a replay video of an agent's performance using a trained Q-table. The function takes an environment, Q-table, output directory, and frames per second parameter, then generates a visualization of the agent's behavior by stepping through the environment.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndef record_video(env, Qtable, out_directory, fps=1):\n    \"\"\"\n    Generate a replay video of the agent\n    :param env\n    :param Qtable: Qtable of our agent\n    :param out_directory\n    :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n    \"\"\"\n    images = []\n    terminated = False\n    truncated = False\n    state, info = env.reset(seed=random.randint(0, 500))\n    img = env.render()\n    images.append(img)\n    while not terminated or truncated:\n        # Take the action (index) that have the maximum expected future reward given that state\n        action = np.argmax(Qtable[state][:])\n        state, reward, terminated, truncated, info = env.step(\n            action\n        )  # We directly put next_state = state for recording logic\n        img = env.render()\n        images.append(img)\n    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n```\n\n----------------------------------------\n\nTITLE: Creating Vectorized Gymnasium Environment in Python\nDESCRIPTION: Shows how to create a vectorized version of a Gymnasium environment using `make_vec_env`. This creates multiple parallel instances of the environment (16 in this case) to accelerate training by providing diverse experiences.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit1/hands-on.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Create the environment\nenv = make_vec_env(\"LunarLander-v2\", n_envs=16)\n```\n\n----------------------------------------\n\nTITLE: Define Model Metadata with Tags and Metrics in Python\nDESCRIPTION: Creates metadata dictionary for a model card, including tags such as environment ID, algorithm type, and custom tags. Incorporates evaluation results like mean reward and standard deviation to summarize model performance.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\ndef generate_metadata(model_name, env_id, mean_reward, std_reward):\n  \"\"\"\n  Define the tags for the model card\n  :param model_name: name of the model\n  :param env_id: name of the environment\n  :mean_reward: mean reward of the agent\n  :std_reward: standard deviation of the mean reward of the agent\n  \"\"\"\n  metadata = {}\n  metadata[\"tags\"] = [\n        env_id,\n        \"ppo\",\n        \"deep-reinforcement-learning\",\n        \"reinforcement-learning\",\n        \"custom-implementation\",\n        \"deep-rl-course\"\n  ]\n\n  # Add metrics\n  eval = metadata_eval_result(\n      model_pretty_name=model_name,\n      task_pretty_name=\"reinforcement-learning\",\n      task_id=\"reinforcement-learning\",\n      metrics_pretty_name=\"mean_reward\",\n      metrics_id=\"mean_reward\",\n      metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n      dataset_pretty_name=env_id,\n      dataset_id=env_id,\n  )\n\n  # Merges both dictionaries\n  metadata = {**metadata, **eval}\n\n  return metadata\n```\n\n----------------------------------------\n\nTITLE: Evaluating Downloaded DQN Model Performance\nDESCRIPTION: Evaluates the performance of the downloaded Deep Q-Learning model on the Beam Rider environment for 5000 timesteps without rendering the game visually.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit3/unit3.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n!python -m rl_zoo3.enjoy --algo dqn --env BeamRiderNoFrameskip-v4 -n 5000  -f rl_trained/ --no-render\n```\n\n----------------------------------------\n\nTITLE: Recording Evaluation Video for PPO Agent in Gym Environment - Python\nDESCRIPTION: Records the trajectory of a trained RL agent interacting with an environment and saves it as a video (e.g., MP4) using imageio, for visualizing and demonstrating the policy's performance. Inputs include a Gym environment, PPO agent (policy), output file path, and optional FPS. The function loops through an episode, rendering each frame and collecting images, then serializes to a video using imageio.mimsave. The policy must provide a 'get_action_and_value' interface and use torch for state conversion. Required dependencies: imageio, torch, numpy. Only RGB array mode of rendering is supported.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef record_video(env, policy, out_directory, fps=30):\n    images = []\n    done = False\n    state = env.reset()\n    img = env.render(mode=\"rgb_array\")\n    images.append(img)\n    while not done:\n        state = torch.Tensor(state).to(device)\n        # Take the action (index) that have the maximum expected future reward given that state\n        action, _, _, _ = policy.get_action_and_value(state)\n        state, reward, done, info = env.step(\n            action.cpu().numpy()\n        )  # We directly put next_state = state for recording logic\n        img = env.render(mode=\"rgb_array\")\n        images.append(img)\n    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n\n```\n\n----------------------------------------\n\nTITLE: Create Custom FrozenLake Environment - Python\nDESCRIPTION: This snippet provides an example of how to create a custom FrozenLake environment by defining a custom grid. The `desc` parameter is used to define the layout of the environment. This allows for environment customization, but in this context, the default environment will be used.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndesc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\ngym.make('FrozenLake-v1', desc=desc, is_slippery=True)\n```\n\n----------------------------------------\n\nTITLE: Running the PPO Training Script with Custom Arguments - Shell\nDESCRIPTION: This shell command runs the PPO training script with specified hyperparameters such as environment ID, Hugging Face repository ID, and total timesteps. Requires: a script (ppo.py) compatible with argparse command-line arguments, and the necessary Python dependencies and environment. Input arguments determine the training configuration. The script should be made executable and contain the PPO implementation with HF integration.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_32\n\nLANGUAGE: Shell\nCODE:\n```\npython ppo.py --env-id=\"LunarLander-v2\" --repo-id=\"YOUR_REPO_ID\" --total-timesteps=50000\n```\n\n----------------------------------------\n\nTITLE: Download Unity Environment Executable via wget - Python\nDESCRIPTION: This cell downloads a zipped Unity RL environment executable necessary for training (e.g., Pyramids) using wget. The output will be placed in a specific directory required for later training commands. Prerequisites include public internet access, wget availability in the environment, and sufficient disk quota. Expects no input from the user aside from optional path modification; outputs a ZIP file at the nominated path suitable for subsequent extraction.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit5/unit5.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n!wget \"https://huggingface.co/spaces/unity/ML-Agents-Pyramids/resolve/main/Pyramids.zip\" -O ./training-envs-executables/linux/Pyramids.zip\n```\n\n----------------------------------------\n\nTITLE: Authenticating Hugging Face Hub (Notebook) Python\nDESCRIPTION: Calls the `notebook_login` function to authenticate with the Hugging Face Hub when running in a notebook environment like Google Colab or Jupyter. This is necessary to gain permissions for uploading files to the Hub. Requires the `huggingface_hub` library.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nnotebook_login()\n```\n\n----------------------------------------\n\nTITLE: Hugging Face Authentication for Model Sharing\nDESCRIPTION: This snippet guides users through creating a Hugging Face account, generating a write-role token, and logging in via notebook or CLI to enable model sharing. It involves installing the 'huggingface_hub' library, using `notebook_login()`, or `huggingface-cli login`.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_29\n\nLANGUAGE: Python\nCODE:\n```\nfrom huggingface_hub import notebook_login\nnotebook_login()\n!git config --global credential.helper store\n```\n\n----------------------------------------\n\nTITLE: Logging into Hugging Face via Python in Jupyter Notebook\nDESCRIPTION: This code enables authentication with Hugging Face Hub within a Jupyter Notebook or Google Colab by prompting the user to log in with their token, facilitating subsequent model uploads.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit5/hands-on.mdx#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n```\n\n----------------------------------------\n\nTITLE: Starting Training of PPO Agent\nDESCRIPTION: This section instructs users on organizing their code in 'ppo.py' and running the training script with appropriate arguments for environment ID, repository ID, and total timesteps. It emphasizes hyperparameter tuning for stability.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_30\n\nLANGUAGE: Python\nCODE:\n```\n!python ppo.py --env-id=\"LunarLander-v2\" --repo-id=\"YOUR_REPO_ID\" --total-timesteps=50000\n```\n\n----------------------------------------\n\nTITLE: Generate Model Card for Hugging Face Hub\nDESCRIPTION: This function generates a model card in markdown format, describing the trained model, hyperparameters, and evaluation results. It takes the model name, environment ID, mean and standard deviation of rewards, and the hyperparameters as input. It also defines the tags for the model card.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef _generate_model_card(model_name, env_id, mean_reward, std_reward, hyperparameters):\n  \"\"\"\n  Generate the model card for the Hub\n  :param model_name: name of the model\n  :env_id: name of the environment\n  :mean_reward: mean reward of the agent\n  :std_reward: standard deviation of the mean reward of the agent\n  :hyperparameters: training arguments\n  \"\"\"\n  # Step 1: Select the tags\n  metadata = generate_metadata(model_name, env_id, mean_reward, std_reward)\n\n  # Transform the hyperparams namespace to string\n  converted_dict = vars(hyperparameters)\n  converted_str = str(converted_dict)\n  converted_str = converted_str.split(\", \")\n  converted_str = '\\n'.join(converted_str)\n \n  # Step 2: Generate the model card\n  model_card = f\"\"\"\n  # PPO Agent Playing {env_id}\n\n  This is a trained model of a PPO agent playing {env_id}.\n    \n  # Hyperparameters\n  ```python\n  {converted_str}\n  ```\n  \"\"\"\n  return model_card, metadata\n```\n\n----------------------------------------\n\nTITLE: Pushing trained RL model to Hugging Face Hub in Bash\nDESCRIPTION: This command uploads the trained RL agent to a Hugging Face repository, creating or updating it. It requires specifying run ID, local directory, repo ID, and a commit message to version the model.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit5/hands-on.mdx#_snippet_12\n\nLANGUAGE: Bash\nCODE:\n```\nmlagents-push-to-hf  --run-id= # Add your run id  --local-dir= # Your local dir  --repo-id= # Your repo id  --commit-message= # Your commit message\n```\n\n----------------------------------------\n\nTITLE: Calculating Approximate KL Divergence in PPO Training Loop\nDESCRIPTION: This snippet computes the approximate KL divergence to monitor how much the policy diverges from the previous iteration, crucial for limiting policy updates during training. It relies on log ratio calculations, following the method described at http://joschu.net/blog/kl-approx.html. It updates clip fraction statistics for analysis.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\n# calculate approx_kl http://joschu.net/blog/kl-approx.html\nold_approx_kl = (-logratio).mean()\napprox_kl = ((ratio - 1) - logratio).mean()\nclipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n```\n\n----------------------------------------\n\nTITLE: Define CartPole Hyperparameters in Python\nDESCRIPTION: Specifies a dictionary containing hyperparameters for training the Reinforce agent on the CartPole environment. Includes parameters like hidden layer size, number of training episodes, maximum timesteps per episode, discount factor, learning rate, and environment specific details.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ncartpole_hyperparameters = {\n    \"h_size\": 16,\n    \"n_training_episodes\": 1000,\n    \"n_evaluation_episodes\": 10,\n    \"max_t\": 1000,\n    \"gamma\": 1.0,\n    \"lr\": 1e-2,\n    \"env_id\": env_id,\n    \"state_space\": s_size,\n    \"action_space\": a_size,\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Observation and Reward Functions in Godot Using GDScript\nDESCRIPTION: Defines the get_obs() method to collect sensory data from raycast sensors and the relative positions and states of game objects as observations for the RL agent. It normalizes vectors, clamps distances, and encodes boolean game state flags as floats. The get_reward() method returns the current scalar reward value. These methods are critical for providing the agent with a structured state representation and scalar reward signal. The script depends on node references such as raycast_sensors, chest, lever, key, raft, and player being properly assigned in the scene.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unitbonus5/getting-started.mdx#_snippet_0\n\nLANGUAGE: GDScript\nCODE:\n```\nfunc get_obs() -> Dictionary:\n\tvar observations: Array[float] = []\n\tfor raycast_sensor in raycast_sensors:\n\t\tobservations.append_array(raycast_sensor.get_observation())\n\n\tvar level_size = 16.0\n\n\tvar chest_local = to_local(chest.global_position)\n\tvar chest_direction = chest_local.normalized()\n\tvar chest_distance = clampf(chest_local.length(), 0.0, level_size)\n\t\n\tvar lever_local = to_local(lever.global_position)\n\tvar lever_direction = lever_local.normalized()\n\tvar lever_distance = clampf(lever_local.length(), 0.0, level_size)\n\t\t\n\tvar key_local = to_local(key.global_position)\n\tvar key_direction = key_local.normalized()\n\tvar key_distance = clampf(key_local.length(), 0.0, level_size)\n\t\n\tvar raft_local = to_local(raft.global_position)\n\tvar raft_direction = raft_local.normalized()\n\tvar raft_distance = clampf(raft_local.length(), 0.0, level_size)\n\t\n\tvar player_speed = player.global_basis.inverse() * player.velocity.limit_length(5.0) / 5.0\n\n\t(\n\t\tobservations\n\t\t.append_array(\n\t\t\t[\n\t\t\t\tchest_direction.x,\n\t\t\t\tchest_direction.y,\n\t\t\t\tchest_direction.z,\n\t\t\t\tchest_distance,\n\t\t\t\tlever_direction.x,\n\t\t\t\tlever_direction.y,\n\t\t\t\tlever_direction.z,\n\t\t\t\tlever_distance,\n\t\t\t\tkey_direction.x,\n\t\t\t\tkey_direction.y,\n\t\t\t\tkey_direction.z,\n\t\t\t\tkey_distance,\n\t\t\t\traft_direction.x,\n\t\t\t\traft_direction.y,\n\t\t\t\traft_direction.z,\n\t\t\t\traft_distance,\n\t\t\t\traft.movement_direction_multiplier,\n\t\t\t\tfloat(player._is_lever_pulled),\n\t\t\t\tfloat(player._is_chest_opened),\n\t\t\t\tfloat(player._is_key_collected),\n\t\t\t\tfloat(player.is_on_floor()),\n\t\t\t\tplayer_speed.x,\n\t\t\t\tplayer_speed.y,\n\t\t\t\tplayer_speed.z,\n\t\t\t]\n\t\t)\n\t)\n\treturn {\"obs\": observations}\n\nfunc get_reward() -> float:\n\treturn reward\n```\n\n----------------------------------------\n\nTITLE: Training a ViZDoom Agent with Sample Factory (Python)\nDESCRIPTION: Registers the ViZDoom components (environments and models), sets up the configuration for the 'doom_health_gathering_supreme' environment with specific training parameters (workers, steps), and starts the reinforcement learning training process using Sample Factory's `run_rl` function.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part2.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n## Start the training, this should take around 15 minutes\nregister_vizdoom_components()\n\n# The scenario we train on today is health gathering\n# other scenarios include \"doom_basic\", \"doom_two_colors_easy\", \"doom_dm\", \"doom_dwango5\", \"doom_my_way_home\", \"doom_deadly_corridor\", \"doom_defend_the_center\", \"doom_defend_the_line\"\nenv = \"doom_health_gathering_supreme\"\ncfg = parse_vizdoom_cfg(argv=[f\"--env={env}\", \"--num_workers=8\", \"--num_envs_per_worker=4\", \"--train_for_env_steps=4000000\"])\n\nstatus = run_rl(cfg)\n```\n\n----------------------------------------\n\nTITLE: Creating the Policy Network Architecture Framework in PyTorch\nDESCRIPTION: Initial implementation skeleton of the Policy class that defines a neural network for policy gradient. The network takes state size, action size, and hidden layer size as parameters and provides methods for forward pass and action selection.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size):\n        super(Policy, self).__init__()\n        # Create two fully connected layers\n\n\n\n    def forward(self, x):\n        # Define the forward pass\n        # state goes to fc1 then we apply ReLU activation function\n\n        # fc1 outputs goes to fc2\n\n        # We output the softmax\n\n    def act(self, state):\n        \"\"\"\n        Given a state, take action\n        \"\"\"\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        probs = self.forward(state).cpu()\n        m = Categorical(probs)\n        action = np.argmax(m)\n        return action.item(), m.log_prob(action)\n```\n\n----------------------------------------\n\nTITLE: Implement Agent Evaluation in Python\nDESCRIPTION: Defines a function `evaluate_agent` to assess the performance of a trained Reinforce policy. It runs the policy for a specified number of evaluation episodes in an environment, collects total rewards for each episode, and returns the mean and standard deviation of these rewards.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate_agent(env, max_steps, n_eval_episodes, policy):\n    \"\"\"\n    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n    :param env: The evaluation environment\n    :param n_eval_episodes: Number of episode to evaluate the agent\n    :param policy: The Reinforce agent\n    \"\"\"\n    episode_rewards = []\n    for episode in range(n_eval_episodes):\n        state = env.reset()\n        step = 0\n        done = False\n        total_rewards_ep = 0\n\n        for step in range(max_steps):\n            action, _ = policy.act(state)\n            new_state, reward, done, info = env.step(action)\n            total_rewards_ep += reward\n\n            if done:\n                break\n            state = new_state\n        episode_rewards.append(total_rewards_ep)\n    mean_reward = np.mean(episode_rewards)\n    std_reward = np.std(episode_rewards)\n\n    return mean_reward, std_reward\n```\n\n----------------------------------------\n\nTITLE: Embedding Video Player for Beam Rider Agent Replay\nDESCRIPTION: HTML code to embed and play a video showing a pre-trained DQN agent playing the Beam Rider Atari game, loaded from the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit3/unit3.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n%%html\n<video controls autoplay><source src=\"https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4/resolve/main/replay.mp4\" type=\"video/mp4\"></video>\n```\n\n----------------------------------------\n\nTITLE: Computing Value Function Loss with Optional Clipping\nDESCRIPTION: This section computes the value function loss for actor-critic optimization, optionally clipping the value change to improve stability. It handles both clipped and unclipped scenarios, returning the mean squared error of the value estimates and the returns.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_26\n\nLANGUAGE: Python\nCODE:\n```\nnewvalue = newvalue.view(-1)\nif args.clip_vloss:\n    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n    v_clipped = b_values[mb_inds] + torch.clamp(\n        newvalue - b_values[mb_inds],\n        -args.clip_coef,\n        args.clip_coef,\n    )\n    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n    v_loss = 0.5 * v_loss_max.mean()\nelse:\n    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n```\n\n----------------------------------------\n\nTITLE: Creating CartPole Environment\nDESCRIPTION: This code snippet creates the CartPole-v1 environment using Gym and retrieves the state and action space sizes. It initializes both a training environment (`env`) and an evaluation environment (`eval_env`).\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nenv_id = \"CartPole-v1\"\n# Create the env\nenv = gym.make(env_id)\n\n# Create the evaluation env\neval_env = gym.make(env_id)\n\n# Get the state space and action space\ns_size = env.observation_space.shape[0]\na_size = env.action_space.n\n```\n\n----------------------------------------\n\nTITLE: Setting Q-Learning Hyperparameters\nDESCRIPTION: Defining the hyperparameters for Q-Learning training, including learning rate, discount factor, exploration parameters, and episode counts.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Training parameters\nn_training_episodes = 10000  # Total training episodes\nlearning_rate = 0.7  # Learning rate\n\n# Evaluation parameters\nn_eval_episodes = 100  # Total number of test episodes\n\n# Environment parameters\nenv_id = \"FrozenLake-v1\"  # Name of the environment\nmax_steps = 99  # Max steps per episode\ngamma = 0.95  # Discounting rate\neval_seed = []  # The evaluation seed of the environment\n\n# Exploration parameters\nmax_epsilon = 1.0  # Exploration probability at start\nmin_epsilon = 0.05  # Minimum exploration probability\ndecay_rate = 0.0005  # Exponential decay rate for exploration prob\n```\n\n----------------------------------------\n\nTITLE: Initialize and start virtual display\nDESCRIPTION: This Python script initializes and starts a virtual display using `pyvirtualdisplay`. This allows the notebook to render the Lunar Lander environment and record replay videos, even in a headless environment like Google Colab. The display size is set to 1400x900.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit1/hands-on.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()\n```\n\n----------------------------------------\n\nTITLE: Setting up Virtual Display\nDESCRIPTION: This code snippet sets up a virtual display using `pyvirtualdisplay` to allow rendering of the environment for creating replay videos in a Colab environment. It initializes a virtual display with a specified size and starts it.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()\n```\n\n----------------------------------------\n\nTITLE: Configuring the SnowballTarget Training\nDESCRIPTION: This is a YAML configuration file that defines the training parameters for the SnowballTarget environment. It sets parameters for the trainer type, training frequency, checkpointing, the overall number of training steps, time horizon, hyperparameters for PPO algorithm (learning rate, batch size, etc.), and network settings. The configuration file is used by the ML-Agents framework during training.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit5/hands-on.mdx#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nbehaviors:\n  SnowballTarget:\n    trainer_type: ppo\n    summary_freq: 10000\n    keep_checkpoints: 10\n    checkpoint_interval: 50000\n    max_steps: 200000\n    time_horizon: 64\n    threaded: true\n    hyperparameters:\n      learning_rate: 0.0003\n      learning_rate_schedule: linear\n      batch_size: 128\n      buffer_size: 2048\n      beta: 0.005\n      epsilon: 0.2\n      lambd: 0.95\n      num_epoch: 3\n    network_settings:\n      normalize: false\n      hidden_units: 256\n      num_layers: 2\n      vis_encode_type: simple\n    reward_signals:\n      extrinsic:\n        gamma: 0.99\n        strength: 1.0\n```\n\n----------------------------------------\n\nTITLE: Inspecting Environment Observation and Action Spaces\nDESCRIPTION: Outputs the shape of the environment's observation space and a sample observation, as well as the number of discrete actions and a sample action. These details inform neural network architecture and interaction logic.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nprint(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"The State Space is: \", s_size)\nprint(\"Sample observation\", env.observation_space.sample()) # Get a random observation\n```\n\nLANGUAGE: Python\nCODE:\n```\nprint(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"The Action Space is: \", a_size)\nprint(\"Action Space Sample\", env.action_space.sample()) # Take a random action\n```\n\n----------------------------------------\n\nTITLE: Layer Initialization - Python\nDESCRIPTION: This function initializes the weights and biases of a given layer. It uses orthogonal initialization for the weights and sets the bias to a constant value. It requires `torch` and `numpy`.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndef layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n    torch.nn.init.orthogonal_(layer.weight, std)\n    torch.nn.init.constant_(layer.bias, bias_const)\n    return layer\n```\n\n----------------------------------------\n\nTITLE: Extracting Huggy Environment Files in Bash\nDESCRIPTION: Unzips the Huggy environment files to the appropriate directory for use with ML-Agents.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus-unit1.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n!unzip -d ./trained-envs-executables/linux/ ./trained-envs-executables/linux/Huggy.zip\n```\n\n----------------------------------------\n\nTITLE: Visualizing Doom Deathmatch Agent Performance\nDESCRIPTION: This code sets up environment registration, loads the pre-trained agent, runs evaluation episodes with video recording, and generates an inline display of the gameplay replay from the agent's perspective.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-sf.mdx#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nregister_vizdoom_components()\n\nenv = \"doom_deathmatch_bots\"\ncfg = parse_vizdoom_cfg(\n    argv=[\n        f\"--env={env}\",\n        \"--num_workers=1\",\n        \"--save_video\",\n        \"--no_render\",\n        \"--max_num_episodes=1\",\n        \"--experiment=doom_deathmatch_bots_2222\",\n        \"--train_dir=train_dir\",\n    ],\n    evaluation=True\n)\nstatus = enjoy(cfg)\nmp4 = open(\"/content/train_dir/doom_deathmatch_bots_2222/replay.mp4\", \"rb\").read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\n    \"\"\"\n<video width=640 controls>\n      <source src=\"%s\" type=\"video/mp4\">\n</video>\n\"\"\" % data_url\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating a Trained PPO Agent in a Gym Environment - Python\nDESCRIPTION: This function evaluates the average and standard deviation of the total reward over multiple episodes for a given PyTorch policy and an OpenAI Gym environment. It handles agent inference, sequential environment stepping, reward accumulation, and episode control. Required dependencies include torch and numpy. Inputs include an environment instance, number of evaluation episodes, and a PyTorch-compatible policy. Returns mean and standard deviation of rewards; all episodes are run with no gradient tracking for performance. Assumes the policy exposes a 'get_action_and_value' method; correct device management is also required.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef _evaluate_agent(env, n_eval_episodes, policy):\n    \"\"\"\n    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n    :param env: The evaluation environment\n    :param n_eval_episodes: Number of episode to evaluate the agent\n    :param policy: The agent\n    \"\"\"\n    episode_rewards = []\n    for episode in range(n_eval_episodes):\n        state = env.reset()\n        step = 0\n        done = False\n        total_rewards_ep = 0\n\n        while done is False:\n            state = torch.Tensor(state).to(device)\n            action, _, _, _ = policy.get_action_and_value(state)\n            new_state, reward, done, info = env.step(action.cpu().numpy())\n            total_rewards_ep += reward\n            if done:\n                break\n            state = new_state\n        episode_rewards.append(total_rewards_ep)\n    mean_reward = np.mean(episode_rewards)\n    std_reward = np.std(episode_rewards)\n\n    return mean_reward, std_reward\n\n```\n\n----------------------------------------\n\nTITLE: Initializing the Q-Table for Taxi-v3 (Python)\nDESCRIPTION: Calls a hypothetical `initialize_q_table` function to create the Q-table for the Taxi-v3 environment. The table's dimensions are determined by the previously obtained `state_space` and `action_space` sizes (500 states x 6 actions). The initialized Q-table and its shape are then printed.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_29\n\nLANGUAGE: Python\nCODE:\n```\n# Create our Q table with state_size rows and action_size columns (500x6)\nQtable_taxi = initialize_q_table(state_space, action_space)\nprint(Qtable_taxi)\nprint(\"Q-table shape: \", Qtable_taxi .shape)\n```\n\n----------------------------------------\n\nTITLE: Evaluating and Displaying Doom Deathmatch Agent Video (Python)\nDESCRIPTION: Runs evaluation for a downloaded 'doom_deathmatch_bots' agent using Sample Factory's `enjoy` function, saves the video, and then immediately reads and displays the generated video within an IPython environment. It ensures ViZDoom components are registered first.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part2.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom sample_factory.enjoy import enjoy\nregister_vizdoom_components()\nenv = \"doom_deathmatch_bots\"\ncfg = parse_vizdoom_cfg(argv=[f\"--env={env}\", \"--num_workers=1\", \"--save_video\", \"--no_render\", \"--max_num_episodes=1\", \"--experiment=doom_deathmatch_bots_2222\", \"--train_dir=train_dir\"], evaluation=True)\nstatus = enjoy(cfg)\nmp4 = open('/content/train_dir/doom_deathmatch_bots_2222/replay.mp4','rb').read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=640 controls>\n      <source src=\"%s\" type=\"video/mp4\">\n</video>\n\"\"\" % data_url)\n```\n\n----------------------------------------\n\nTITLE: Defining a Function to Load Models from Hugging Face Hub (Python)\nDESCRIPTION: Defines a reusable Python function `load_from_hub` that takes a Hugging Face Hub repository ID (`repo_id`) and a filename (`filename`) as input. It uses `hf_hub_download` to fetch the specified file (expected to be a pickled object) from the Hub, then loads and returns the unpickled object using the `pickle` library. Basic `HTTPError` handling is included.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_34\n\nLANGUAGE: Python\nCODE:\n```\nfrom urllib.error import HTTPError\n\nfrom huggingface_hub import hf_hub_download\nimport pickle\n\ndef load_from_hub(repo_id: str, filename: str) -> str:\n    \"\"\"\n    Download a model from Hugging Face Hub.\n    :param repo_id: id of the model repository from the Hugging Face Hub\n    :param filename: name of the model zip file from the repository\n    \"\"\"\n    # Get the model from the Hub, download and cache the model on your local disk\n    try:\n        pickle_model = hf_hub_download(\n            repo_id=repo_id,\n            filename=filename\n        )\n\n        with open(pickle_model, 'rb') as f:\n          downloaded_model_file = pickle.load(f)\n\n        return downloaded_model_file\n    except HTTPError as e:\n        print(f\"Failed to download model: {e}\")\n        raise e\n```\n\n----------------------------------------\n\nTITLE: Installing ML-Agents Dependencies in Bash\nDESCRIPTION: Navigates to the ML-Agents repository directory and installs the required packages for ML-Agents.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus-unit1.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n# Go inside the repository and install the package (can take 3min)\n%cd ml-agents\n!pip3 install -e ./ml-agents-envs\n!pip3 install -e ./ml-agents\n```\n\n----------------------------------------\n\nTITLE: Complete Policy Network Class with Three Linear Layers and Action Sampling in PyTorch\nDESCRIPTION: A complete implementation of the Policy class with three fully connected linear layers progressively increasing complexity, activated with ReLU, followed by a softmax output for probabilistic discrete action selection. The `act` method converts a numpy state to a tensor, computes action probabilities, samples an action from the distribution, and returns both the selected action and its log probability for reinforcement learning algorithms. Dependencies include torch, torch.nn, torch.nn.functional, torch.distributions.Categorical, and device for tensor placement.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nclass Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size):\n        super(Policy, self).__init__()\n        self.fc1 = nn.Linear(s_size, h_size)\n        self.fc2 = nn.Linear(h_size, h_size*2)\n        self.fc3 = nn.Linear(h_size*2, a_size)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return F.softmax(x, dim=1)\n    \n    def act(self, state):\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        probs = self.forward(state).cpu()\n        m = Categorical(probs)\n        action = m.sample()\n        return action.item(), m.log_prob(action)\n```\n\n----------------------------------------\n\nTITLE: Defining Hyperparameters (Pixelcopter Reinforce) Python\nDESCRIPTION: Defines a dictionary containing hyperparameters specific to training the Reinforce agent on the 'Pixelcopter-PLE-v0' environment. It includes settings for the policy network's hidden size, the number of training and evaluation episodes, maximum steps per episode, discount factor (gamma), learning rate, environment ID, and the sizes of the state and action spaces. These parameters are used during the training (code not shown) and evaluation/pushing processes.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\npixelcopter_hyperparameters = {\n    \"h_size\": 64,\n    \"n_training_episodes\": 50000,\n    \"n_evaluation_episodes\": 10,\n    \"max_t\": 10000,\n    \"gamma\": 0.99,\n    \"lr\": 1e-4,\n    \"env_id\": env_id,\n    \"state_space\": s_size,\n    \"action_space\": a_size,\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Virtual Display Dependencies (Shell)\nDESCRIPTION: Installs necessary Ubuntu packages (opengl, ffmpeg, xvfb) and Python libraries (pyglet, pyvirtualdisplay) required to create a virtual display. This is essential for rendering and recording Gym environments in headless environments like Google Colab.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\napt install python-opengl\napt install ffmpeg\napt install xvfb\npip install pyglet==1.5\npip install pyvirtualdisplay\n```\n\n----------------------------------------\n\nTITLE: Generate Metadata for Model Card\nDESCRIPTION: This function defines and generates metadata (tags and evaluation metrics) for the model card. It requires the model name, environment ID, mean reward, and standard deviation of the reward as input. The generated metadata includes tags related to the environment, algorithm, and reinforcement learning.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef generate_metadata(model_name, env_id, mean_reward, std_reward):\n  \"\"\"\n  Define the tags for the model card\n  :param model_name: name of the model\n  :param env_id: name of the environment\n  :mean_reward: mean reward of the agent\n  :std_reward: standard deviation of the mean reward of the agent\n  \"\"\"\n  metadata = {}\n  metadata[\"tags\"] = [\n        env_id,\n        \"ppo\",\n        \"deep-reinforcement-learning\",\n        \"reinforcement-learning\",\n        \"custom-implementation\",\n        \"deep-rl-course\"\n  ]\n\n  # Add metrics\n  eval = metadata_eval_result(\n      model_pretty_name=model_name,\n      task_pretty_name=\"reinforcement-learning\",\n      task_id=\"reinforcement-learning\",\n      metrics_pretty_name=\"mean_reward\",\n      metrics_id=\"mean_reward\",\n      metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n      dataset_pretty_name=env_id,\n      dataset_id=env_id,\n  )\n\n  # Merges both dictionaries\n  metadata = {**metadata, **eval}\n\n  return metadata\n```\n\n----------------------------------------\n\nTITLE: Set Up Compatible Python Environment (Shell)\nDESCRIPTION: Installs virtualenv and Miniconda to create and manage a Python environment with a version specifically compatible with ML-Agents (Python 3.10.12). It downloads the Miniconda installer, runs it to set up conda, and then uses conda to install the specified Python version along with the ujson package. Environment variables are then set to ensure the new Python environment is used.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit5/unit5.ipynb#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\n# Install virtualenv and create a virtual environment\n!pip install virtualenv\n!virtualenv myenv\n\n# Download and install Miniconda\n!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n!chmod +x Miniconda3-latest-Linux-x86_64.sh\n!./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n\n# Activate Miniconda and install Python ver 3.10.12\n!source /usr/local/bin/activate\n!conda install -q -y --prefix /usr/local python=3.10.12 ujson  # Specify the version here\n\n# Set environment variables for Python and conda paths\n!export PYTHONPATH=/usr/local/lib/python3.10/site-packages/\n!export CONDA_PREFIX=/usr/local/envs/myenv\n```\n\n----------------------------------------\n\nTITLE: Recording Training Statistics with Tensorboard - Python\nDESCRIPTION: This snippet records various training metrics such as learning rate, value loss, policy loss, entropy, KL-divergence, clipping fraction, and explained variance using a Tensorboard SummaryWriter. It also prints steps per second (SPS) for monitoring performance. Requires torch, numpy, and tensorboard. Inputs are loss values, optimizer state, and other computed metrics; outputs are logged for visualization. The code expects proper initialization of the writer and tracking variables.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_28\n\nLANGUAGE: Python\nCODE:\n```\nwriter.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\nwriter.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\nwriter.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\nwriter.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\nwriter.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\nwriter.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\nwriter.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\nwriter.add_scalar(\"losses/explained_variance\", explained_var, global_step)\nprint(\"SPS:\", int(global_step / (time.time() - start_time)))\nwriter.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n```\n\n----------------------------------------\n\nTITLE: Training an Agent with MLAgents - Python\nDESCRIPTION: This Python (shell command) snippet uses mlagents-learn to train an agent on a custom environment. It specifies the path to the PPO hyperparameter config YAML, the executable environment, a unique run ID, and disables graphical output. MLAgents must be installed (pip install mlagents), and the specified YAML/config/executable files must be present at provided paths. The input is a command-line invocation; the training outputs results and progress logs in the directory named per run ID. Use the --resume flag to continue training after an interruption.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit5/unit5.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n!mlagents-learn ./config/ppo/SnowballTarget.yaml --env=./training-envs-executables/linux/SnowballTarget/SnowballTarget --run-id=\"SnowballTarget1\" --no-graphics\n```\n\n----------------------------------------\n\nTITLE: Evaluating the Trained Q-Learning Agent in Python\nDESCRIPTION: Code to evaluate the performance of the trained Q-Learning agent and print the mean reward and standard deviation, typically aiming for a mean reward of 1.0 in this environment.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# Evaluate our Agent\nmean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\nprint(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")\n```\n\n----------------------------------------\n\nTITLE: Generating Metadata for Model Card\nDESCRIPTION: Function to generate the metadata for the model card, including tags and evaluation metrics. It creates a structured dictionary of metadata information that will be added to the model card.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef generate_metadata(model_name, env_id, mean_reward, std_reward):\n    \"\"\"\n    Define the tags for the model card\n    :param model_name: name of the model\n    :param env_id: name of the environment\n    :mean_reward: mean reward of the agent\n    :std_reward: standard deviation of the mean reward of the agent\n    \"\"\"\n    metadata = {}\n    metadata[\"tags\"] = [\n        env_id,\n        \"ppo\",\n        \"deep-reinforcement-learning\",\n        \"reinforcement-learning\",\n        \"custom-implementation\",\n        \"deep-rl-course\",\n    ]\n\n    # Add metrics\n    eval = metadata_eval_result(\n        model_pretty_name=model_name,\n        task_pretty_name=\"reinforcement-learning\",\n        task_id=\"reinforcement-learning\",\n        metrics_pretty_name=\"mean_reward\",\n        metrics_id=\"mean_reward\",\n        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n        dataset_pretty_name=env_id,\n        dataset_id=env_id,\n    )\n\n    # Merges both dictionaries\n    metadata = {**metadata, **eval}\n\n    return metadata\n```\n\n----------------------------------------\n\nTITLE: Defining Hyperparameters for Taxi-v3 Q-Learning (Python)\nDESCRIPTION: Sets various hyperparameters required for training and evaluating the Q-learning agent on the Taxi-v3 environment. This includes the number of training/evaluation episodes, learning rate, evaluation seeds (fixed for comparability), environment ID, maximum steps per episode, discount factor (gamma), and exploration parameters (initial/final epsilon, decay rate).\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_30\n\nLANGUAGE: Python\nCODE:\n```\n# Training parameters\nn_training_episodes = 25000   # Total training episodes\nlearning_rate = 0.7           # Learning rate\n\n# Evaluation parameters\nn_eval_episodes = 100        # Total number of test episodes\n\n# DO NOT MODIFY EVAL_SEED\neval_seed = [16,54,165,177,191,191,120,80,149,178,48,38,6,125,174,73,50,172,100,148,146,6,25,40,68,148,49,167,9,97,164,176,61,7,54,55,\n 161,131,184,51,170,12,120,113,95,126,51,98,36,135,54,82,45,95,89,59,95,124,9,113,58,85,51,134,121,169,105,21,30,11,50,65,12,43,82,145,152,97,106,55,31,85,38,\n 112,102,168,123,97,21,83,158,26,80,63,5,81,32,11,28,148] # Evaluation seed, this ensures that all classmates agents are trained on the same taxi starting position\n                                                          # Each seed has a specific starting state\n\n# Environment parameters\nenv_id = \"Taxi-v3\"           # Name of the environment\nmax_steps = 99               # Max steps per episode\ngamma = 0.95                 # Discounting rate\n\n# Exploration parameters\nmax_epsilon = 1.0             # Exploration probability at start\nmin_epsilon = 0.05           # Minimum exploration probability\ndecay_rate = 0.005            # Exponential decay rate for exploration prob\n```\n\n----------------------------------------\n\nTITLE: Installing Virtual Display Dependencies (Shell)\nDESCRIPTION: Installs system packages (`python3-opengl`, `ffmpeg`, `xvfb`) using `apt-get` and the Python library `pyvirtualdisplay` using `pip3`. These are necessary for creating a virtual framebuffer (headless display), allowing graphical environments like LunarLander to be rendered and recorded in environments without a physical display (e.g., Colab).\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit1/unit1.ipynb#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n!sudo apt-get update\n!sudo apt-get install -y python3-opengl\n!apt install ffmpeg\n!apt install xvfb\n!pip3 install pyvirtualdisplay\n```\n\n----------------------------------------\n\nTITLE: Loading and Visualizing a Downloaded Agent\nDESCRIPTION: This code loads the downloaded high-performance model into the environment, sets up evaluation parameters, and uses Sample Factory's enjoy function to generate a video of the agent performing in the environment.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-sf.mdx#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nenv = \"doom_health_gathering_supreme\"\ncfg = parse_vizdoom_cfg(\n    argv=[\n        f\"--env={env}\",\n        \"--num_workers=1\",\n        \"--save_video\",\n        \"--no_render\",\n        \"--max_num_episodes=10\",\n        \"--experiment=doom_health_gathering_supreme_2222\",\n        \"--train_dir=train_dir\",\n    ],\n    evaluation=True\n)\nstatus = enjoy(cfg)\n```\n\n----------------------------------------\n\nTITLE: Initializing ViZDoom Environment and Model Registration in Sample Factory (Python)\nDESCRIPTION: Defines functions to register ViZDoom environments and custom neural network models within the Sample Factory framework. It also includes a function to parse command-line arguments, apply Doom-specific configurations, and override default parameters.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part2.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport functools\n\nfrom sample_factory.algo.utils.context import global_model_factory\nfrom sample_factory.cfg.arguments import parse_full_cfg, parse_sf_args\nfrom sample_factory.envs.env_utils import register_env\nfrom sample_factory.train import run_rl\n\nfrom sf_examples.vizdoom.doom.doom_model import make_vizdoom_encoder\nfrom sf_examples.vizdoom.doom.doom_params import add_doom_env_args, doom_override_defaults\nfrom sf_examples.vizdoom.doom.doom_utils import DOOM_ENVS, make_doom_env_from_spec\n\n\n# Registers all the ViZDoom environments\ndef register_vizdoom_envs():\n    for env_spec in DOOM_ENVS:\n        make_env_func = functools.partial(make_doom_env_from_spec, env_spec)\n        register_env(env_spec.name, make_env_func)\n\n# Sample Factory allows the registration of a custom Neural Network architecture\n# See https://github.com/alex-petrenko/sample-factory/blob/master/sf_examples/vizdoom/doom/doom_model.py for more details\ndef register_vizdoom_models():\n    global_model_factory().register_encoder_factory(make_vizdoom_encoder)\n\n\ndef register_vizdoom_components():\n    register_vizdoom_envs()\n    register_vizdoom_models()\n\n# parse the command line args and create a config\ndef parse_vizdoom_cfg(argv=None, evaluation=False):\n    parser, _ = parse_sf_args(argv=argv, evaluation=evaluation)\n    # parameters specific to Doom envs\n    add_doom_env_args(parser)\n    # override Doom default values for algo parameters\n    doom_override_defaults(parser)\n    # second parsing pass yields the final configuration\n    final_cfg = parse_full_cfg(parser, argv)\n    return final_cfg\n```\n\n----------------------------------------\n\nTITLE: Configuring Huggy Agent Training Hyperparameters (YAML)\nDESCRIPTION: YAML configuration for the Huggy agent using the PPO trainer in Unity ML-Agents. Defines hyperparameters like batch size, learning rate, network architecture (512 hidden units, 3 layers), reward signals, and checkpoint settings (every 200k steps, keep 15). This configuration should be saved as `Huggy.yaml` and is required by the `mlagents-learn` command.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus-unit1.ipynb#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nbehaviors:\n  Huggy:\n    trainer_type: ppo\n    hyperparameters:\n      batch_size: 2048\n      buffer_size: 20480\n      learning_rate: 0.0003\n      beta: 0.005\n      epsilon: 0.2\n      lambd: 0.95\n      num_epoch: 3\n      learning_rate_schedule: linear\n    network_settings:\n      normalize: true\n      hidden_units: 512\n      num_layers: 3\n      vis_encode_type: simple\n    reward_signals:\n      extrinsic:\n        gamma: 0.995\n        strength: 1.0\n    checkpoint_interval: 200000\n    keep_checkpoints: 15\n    max_steps: 2e6\n    time_horizon: 1000\n    summary_freq: 50000\n```\n\n----------------------------------------\n\nTITLE: Defining Agent Observation, Reward, and Action Space in Godot GDScript\nDESCRIPTION: This GDScript snippet defines the basic components of a reinforcement learning agent's interface within the Godot engine, specifying how observations, actions, and rewards are handled. It calculates the observation vector by transforming the ball's position and velocity into the paddle's local coordinate frame, defines a continuous one-dimensional action space for horizontal movement, and ensures the action is clamped between -1 and 1. Key dependencies include access to the _player node with the ball object, and the script expects to return dictionaries for observations and action spaces aligned with the Godot RL Agents framework conventions.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unitbonus3/godotrl.mdx#_snippet_3\n\nLANGUAGE: GDScript\nCODE:\n```\nvar move_action : float = 0.0\n\nfunc get_obs() -> Dictionary:\n\t# get the balls position and velocity in the paddle's frame of reference\n\tvar ball_pos = to_local(_player.ball.global_position)\n\tvar ball_vel = to_local(_player.ball.linear_velocity)\n\tvar obs = [ball_pos.x, ball_pos.z, ball_vel.x/10.0, ball_vel.z/10.0]\n\n\treturn {\"obs\":obs}\n\nfunc get_reward() -> float:\n\treturn reward\n\nfunc get_action_space() -> Dictionary:\n\treturn {\n\t\t\"move_action\" : {\n\t\t\t\"size\": 1,\n\t\t\t\"action_type\": \"continuous\"\n\t\t},\n\t\t}\n\nfunc set_action(action) -> void:\n\tmove_action = clamp(action[\"move_action\"][0], -1.0, 1.0)\n```\n\n----------------------------------------\n\nTITLE: Displaying Example Agent Performance HTML\nDESCRIPTION: Embeds an HTML video player to showcase the performance of a pre-trained agent on the Space Invaders environment. This snippet uses the Colab `%%html` magic command to execute HTML code directly within the notebook cell.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit3/unit3.ipynb#_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n%%html\n<video controls autoplay><source src=\"https://huggingface.co/ThomasSimonini/ppo-SpaceInvadersNoFrameskip-v4/resolve/main/replay.mp4\" type=\"video/mp4\"></video>\n```\n\n----------------------------------------\n\nTITLE: Registering ViZDoom Environments in Sample Factory\nDESCRIPTION: This function registers all predefined ViZDoom scenarios by creating environment specifications and registering them with Sample Factory. It leverages partial function application for environment creation.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-sf.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef register_vizdoom_envs():\n    for env_spec in DOOM_ENVS:\n        make_env_func = functools.partial(make_doom_env_from_spec, env_spec)\n        register_env(env_spec.name, make_env_func)\n```\n\n----------------------------------------\n\nTITLE: Loading Saved Model and Statistics for Evaluation\nDESCRIPTION: This sets up the environment for evaluating the trained agent.  It creates a dummy environment, loads the saved normalization statistics from `vec_normalize.pkl`, and overrides the render mode.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Load the saved statistics\neval_env = DummyVecEnv([lambda: gym.make(\"PandaReachDense-v3\")])\neval_env = VecNormalize.load(\"vec_normalize.pkl\", eval_env)\n\n# We need to override the render_mode\neval_env.render_mode = \"rgb_array\"\n\n#  do not update them at test time\neval_env.training = False\n# reward normalization is not needed at test time\neval_env.norm_reward = False\n```\n\n----------------------------------------\n\nTITLE: Displaying Agent Performance Video in IPython (Python)\nDESCRIPTION: Reads a previously saved MP4 video file ('replay.mp4') containing the agent's performance, encodes it in base64, and embeds it within an HTML5 video player for display in an IPython environment (like Jupyter or Colab).\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part2.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom base64 import b64encode\nfrom IPython.display import HTML\n\nmp4 = open('/content/train_dir/default_experiment/replay.mp4','rb').read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=640 controls>\n      <source src=\"%s\" type=\"video/mp4\">\n</video>\n\"\"\" % data_url)\n```\n\n----------------------------------------\n\nTITLE: Recording Agent Performance Video\nDESCRIPTION: Function to record a video of the agent's performance in the environment. It captures frames of the environment rendering while the agent performs actions, and saves them as an MP4 video file.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef record_video(env, policy, out_directory, fps=30):\n    images = []\n    done = False\n    state = env.reset()\n    img = env.render(mode=\"rgb_array\")\n    images.append(img)\n    while not done:\n        state = torch.Tensor(state).to(device)\n        # Take the action (index) that have the maximum expected future reward given that state\n        action, _, _, _ = policy.get_action_and_value(state)\n        state, reward, done, info = env.step(\n            action.cpu().numpy()\n        )  # We directly put next_state = state for recording logic\n        img = env.render(mode=\"rgb_array\")\n        images.append(img)\n    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n```\n\n----------------------------------------\n\nTITLE: Restarting Python Kernel/Runtime (Python)\nDESCRIPTION: Uses the `os` module to send a `SIGKILL` signal (signal number 9) to the current process ID (`os.getpid()`). This forcibly terminates the Python kernel, effectively restarting the runtime. This is often necessary in notebook environments after installing system-level dependencies to ensure they are properly recognized by the subsequent Python sessions.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit1/unit1.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.kill(os.getpid(), 9)\n```\n\n----------------------------------------\n\nTITLE: Embedding and Displaying Downloaded Agent Replay Video\nDESCRIPTION: Similarly to previous visualization, this snippet reads the replay video of the downloaded model, encodes it and embeds it inline in IPython for visualization purposes.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-sf.mdx#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nmp4 = open(\"/content/train_dir/doom_health_gathering_supreme_2222/replay.mp4\", \"rb\").read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\n    \"\"\"\n<video width=640 controls>\n      <source src=\"%s\" type=\"video/mp4\">\n</video>\n\"\"\" % data_url\n)\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Neural Network Encoder for ViZDoom Models\nDESCRIPTION: This code registers a custom encoder architecture for ViZDoom models within Sample Factory, allowing customization of neural network components for RL agents. It depends on the global model factory instance and the specific encoder creation function.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-sf.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef register_vizdoom_models():\n    global_model_factory().register_encoder_factory(make_vizdoom_encoder)\n```\n\n----------------------------------------\n\nTITLE: Displaying Downloaded Agent's Performance Video (Python)\nDESCRIPTION: Reads the MP4 video file generated during the evaluation of the downloaded agent (from a specific experiment sub-directory), encodes it, and embeds it using `IPython.display.HTML` for visualization.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part2.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nmp4 = open('/content/train_dir/doom_health_gathering_supreme_2222/replay.mp4','rb').read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=640 controls>\n      <source src=\"%s\" type=\"video/mp4\">\n</video>\n\"\"\" % data_url)\n```\n\n----------------------------------------\n\nTITLE: Implementing Epsilon-Greedy Policy (Python)\nDESCRIPTION: Defines the `epsilon_greedy_policy` function for balancing exploration and exploitation during training. With a probability `epsilon`, it selects a random action; otherwise, it selects the greedy action using the `greedy_policy` function. Requires `random` and an environment object (`env`).\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef epsilon_greedy_policy(Qtable, state, epsilon):\n  # Randomly generate a number between 0 and 1\n  random_num = random.uniform(0,1)\n  # if random_num > greater than epsilon --> exploitation\n  if random_num > epsilon:\n    # Take the action with the highest value given a state\n    # np.argmax can be useful here\n    action = greedy_policy(Qtable, state)\n  # else --> exploration\n  else:\n    action = env.action_space.sample()\n\n  return action\n```\n\n----------------------------------------\n\nTITLE: Printing Environment Action Space\nDESCRIPTION: This code prints and samples the action space. It outputs the action space information and generates a random action sample.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"The Action Space is: \", a_size)\nprint(\"Action Space Sample\", env.action_space.sample()) # Take a random action\n```\n\n----------------------------------------\n\nTITLE: Unzipping Huggy Environment Executable (Bash)\nDESCRIPTION: Extracts the contents of the downloaded `Huggy.zip` file using the `unzip` command. The `-d` flag specifies the destination directory (`./trained-envs-executables/linux/`) for the extracted files. The `%%capture` magic command prevents the potentially lengthy output of the unzip process from being displayed in the notebook.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unitbonus1/train.mdx#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n%%capture\nunzip -d ./trained-envs-executables/linux/ ./trained-envs-executables/linux/Huggy.zip\n```\n\n----------------------------------------\n\nTITLE: Training DQN Agent Shell\nDESCRIPTION: Executes the RL Baselines3 Zoo training script (`rl_zoo3.train`) to train a Deep Q-Learning (DQN) agent on the SpaceInvadersNoFrameskip-v4 environment. It uses hyperparameters defined in the `dqn.yml` configuration file and saves training logs and the trained model to the `logs/` directory. Requires installed libraries and the `dqn.yml` file.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit3/unit3.ipynb#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\n!python -m rl_zoo3.train --algo dqn  --env SpaceInvadersNoFrameskip-v4 -f logs/ -c dqn.yml\n```\n\n----------------------------------------\n\nTITLE: Logging into Hugging Face Hub from Notebook (Python/Shell)\nDESCRIPTION: Uses the `huggingface_hub` library to initiate a notebook login flow for Hugging Face Hub authentication. It also configures git to store credentials globally.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part2.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import notebook_login\nnotebook_login()\n!git config --global credential.helper store\n```\n\n----------------------------------------\n\nTITLE: Implementing the PPO Policy Update Mechanism in Python\nDESCRIPTION: This code snippet demonstrates the core logic for updating the policy within the PPO algorithm by calculating the probability ratio of new and old policies and applying clipping to limit policy updates. It requires dependencies such as NumPy or PyTorch for tensor operations and is essential for maintaining training stability. The code takes as input the current policy probabilities, previous policy probabilities, and the epsilon hyperparameter for clipping bounds, outputting a clipped ratio to be used in the objective function.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/intuition-behind-ppo.mdx#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\n\ndef compute_ratio(new_policy_probs, old_policy_probs):\n    \"\"\"Calculate the probability ratio of new to old policies.\"\"\"\n    return new_policy_probs / (old_policy_probs + 1e-10)\n\n\ndef clip_ratio(ratio, epsilon):\n    \"\"\"Clip the ratio within [1 - epsilon, 1 + epsilon] to limit policy updates.\"\"\"\n    return torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n\n# Example usage:\n# new_probs = torch.tensor([...])\n# old_probs = torch.tensor([...])\n# epsilon = 0.2\n# ratio = compute_ratio(new_probs, old_probs)\n# clipped_ratio = clip_ratio(ratio, epsilon)\n```\n\n----------------------------------------\n\nTITLE: Generating Model Card for Hugging Face Hub\nDESCRIPTION: Function to generate a model card with metadata for the Hugging Face Hub. It creates a Markdown description of the model, including environment information, performance metrics, and hyperparameters.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef _generate_model_card(model_name, env_id, mean_reward, std_reward, hyperparameters):\n    \"\"\"\n    Generate the model card for the Hub\n    :param model_name: name of the model\n    :env_id: name of the environment\n    :mean_reward: mean reward of the agent\n    :std_reward: standard deviation of the mean reward of the agent\n    :hyperparameters: training arguments\n    \"\"\"\n    # Step 1: Select the tags\n    metadata = generate_metadata(model_name, env_id, mean_reward, std_reward)\n\n    # Transform the hyperparams namespace to string\n    converted_dict = vars(hyperparameters)\n    converted_str = str(converted_dict)\n    converted_str = converted_str.split(\", \")\n    converted_str = \"\\n\".join(converted_str)\n\n    # Step 2: Generate the model card\n    model_card = f\"\"\"\n  # PPO Agent Playing {env_id}\n\n  This is a trained model of a PPO agent playing {env_id}.\n\n  # Hyperparameters\n  \"\"\"\n    return model_card, metadata\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies Python\nDESCRIPTION: Imports necessary libraries from the Hugging Face Hub, standard Python modules, and external libraries like imageio and numpy (implicitly used elsewhere) for handling model sharing, file operations, date/time, JSON, and video generation.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import HfApi, snapshot_download\nfrom huggingface_hub.repocard import metadata_eval_result, metadata_save\n\nfrom pathlib import Path\nimport datetime\nimport json\nimport imageio\n\nimport tempfile\n\nimport os\n```\n\n----------------------------------------\n\nTITLE: Solution for Pushing DQN Model to Hub\nDESCRIPTION: Example command showing how to correctly push a trained Deep Q-Learning model for Space Invaders to a specific Hugging Face Hub repository.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit3/unit3.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n!python -m rl_zoo3.push_to_hub  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --repo-name dqn-SpaceInvadersNoFrameskip-v4  -orga ThomasSimonini  -f logs/\n```\n\n----------------------------------------\n\nTITLE: Inspecting Gymnasium Observation Space in Python\nDESCRIPTION: Shows how to create and reset a specific Gymnasium environment (LunarLander-v2) and then inspect its observation space. It prints the shape of the observation space and a sample observation vector.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit1/hands-on.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# We create our environment with gym.make(\"<name_of_the_environment>\")\nenv = gym.make(\"LunarLander-v2\")\nenv.reset()\nprint(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"Observation Space Shape\", env.observation_space.shape)\nprint(\"Sample observation\", env.observation_space.sample())  # Get a random observation\n```\n\n----------------------------------------\n\nTITLE: Recording Mouse Movement Input in Godot AI Controller for Human and Demo Record Modes (GDScript)\nDESCRIPTION: Implements an override to the _input(event) function to capture mouse motion events when in 'human' or 'demo_record' heuristics modes. It scales and clamps mouse movement deltas to the range [-1.0, 1.0] with a sensitivity factor, storing them for subsequent rotation handling. This separates raw input capturing from action application to allow frame skipping and consistent input sampling. The method requires the heuristic variable to be set and InputEventMouseMotion to be properly recognized.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unitbonus5/getting-started.mdx#_snippet_3\n\nLANGUAGE: GDScript\nCODE:\n```\n# Record mouse movement for human and demo_record modes\n# We don't directly rotate in input to allow for frame skipping (action_repeat setting) which\n# will also be applied to the AI agent in training/inference modes.\nfunc _input(event):\n\tif not (heuristic == \"human\" or heuristic == \"demo_record\"):\n\t\treturn\n\n\tif event is InputEventMouseMotion:\n\t\tvar movement_scale: float = 0.005\n\t\tmouse_movement.y = clampf(event.relative.y * movement_scale, -1.0, 1.0)\n\t\tmouse_movement.x = clampf(event.relative.x * movement_scale, -1.0, 1.0)\n```\n\n----------------------------------------\n\nTITLE: Training an RL agent using mlagents-learn command in Bash\nDESCRIPTION: This script initiates training for an RL agent using Unity ML-Agents, specifying configuration and environment parameters. It relies on the presence of a YAML config file and an environment executable, and allows training resumption with '--resume'.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit5/hands-on.mdx#_snippet_10\n\nLANGUAGE: Bash\nCODE:\n```\nmlagents-learn ./config/ppo/SnowballTarget.yaml --env=./training-envs-executables/linux/SnowballTarget/SnowballTarget --run-id=\"SnowballTarget1\" --no-graphics\n```\n\n----------------------------------------\n\nTITLE: Evaluating a Downloaded Model - Bash\nDESCRIPTION: This command evaluates a previously downloaded reinforcement learning model. It uses the `rl_zoo3.enjoy` command, specifying the algorithm, environment, the number of time steps to evaluate for (`-n`), the location of the trained model, and a flag to disable rendering (`--no-render`). The command allows you to see how well an agent plays in its environment.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit3/hands-on.mdx#_snippet_10\n\nLANGUAGE: Bash\nCODE:\n```\npython -m rl_zoo3.enjoy --algo dqn --env BeamRiderNoFrameskip-v4 -n 5000  -f rl_trained/ --no-render\n```\n\n----------------------------------------\n\nTITLE: Getting Environment State and Action Space Sizes (Python)\nDESCRIPTION: Retrieves the total number of possible states (`state_space`) and actions (`action_space`) directly from the Gym environment's observation and action spaces. It then prints these values, which are necessary for initializing the Q-table. Requires an initialized Gym `env` object.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nstate_space = env.observation_space.n\nprint(\"There are \", state_space, \" possible states\")\n\naction_space = env.action_space.n\nprint(\"There are \", action_space, \" possible actions\")\n```\n\n----------------------------------------\n\nTITLE: Displaying Embedded HTML Video in Jupyter - Python\nDESCRIPTION: This snippet uses the %%html Jupyter magic to embed an HTML5 video player within a notebook cell, allowing direct playback of a demonstration video hosted at a URL. No explicit dependencies beyond Jupyter are required. There are no parameters; the displayed video provides a visual example of expected RL agent behavior.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus_unit1.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%html\n<video controls autoplay><source src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/huggy.mp4\" type=\"video/mp4\"></video>\n```\n\n----------------------------------------\n\nTITLE: Incorrect Policy Network Implementation in PyTorch\nDESCRIPTION: Initial solution for the Policy network that contains a critical bug in the action selection method. The bug is in the act method where actions are selected using argmax instead of sampling from the probability distribution.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size):\n        super(Policy, self).__init__()\n        self.fc1 = nn.Linear(s_size, h_size)\n        self.fc2 = nn.Linear(h_size, a_size)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.softmax(x, dim=1)\n\n    def act(self, state):\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        probs = self.forward(state).cpu()\n        m = Categorical(probs)\n        action = np.argmax(m)\n        return action.item(), m.log_prob(action)\n```\n\n----------------------------------------\n\nTITLE: Add Log Directory to Repository\nDESCRIPTION: This function adds the log directory (containing TensorBoard logs) to the repository being uploaded to the Hugging Face Hub. It takes the local path of the repository and the path to the log directory as input. Before copying, it removes any existing logs directory within the repository and then copies the content of the log directory into it.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef _add_logdir(local_path: Path, logdir: Path):\n  \"\"\"Adds a logdir to the repository.\n  :param local_path: repository directory\n  :param logdir: logdir directory\n  \"\"\"\n  if logdir.exists() and logdir.is_dir():\n    # Add the logdir to the repository under new dir called logs\n    repo_logdir = local_path / \"logs\"\n    \n    # Delete current logs if they exist\n    if repo_logdir.exists():\n      shutil.rmtree(repo_logdir)\n\n    # Copy logdir into repo logdir\n    shutil.copytree(logdir, repo_logdir)\n```\n\n----------------------------------------\n\nTITLE: Evaluating the trained agent with RL Baselines3 Zoo\nDESCRIPTION: Execute the enjoyment script to evaluate the trained agent over 5000 timesteps without rendering the environment, analyzing its performance based on the learned policy.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit3/hands-on.mdx#_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\npython -m rl_zoo3.enjoy --algo dqn --env SpaceInvadersNoFrameskip-v4 --no-render --n-timesteps 5000 --folder logs/\n```\n\n----------------------------------------\n\nTITLE: Package and Push Stable-Baselines3 Model to Hugging Face Hub\nDESCRIPTION: This Python code demonstrates how to package a trained Stable-Baselines3 model and push it to a specified repository on the Hugging Face Hub. It uses the `huggingface_sb3.package_to_hub` function, which handles evaluation, video recording, model card generation, and the actual push.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit1/hands-on.mdx#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.env_util import make_vec_env\n\nfrom huggingface_sb3 import package_to_hub\n\n# PLACE the variables you've just defined two cells above\n# Define the name of the environment\nenv_id = \"LunarLander-v2\"\n\n# TODO: Define the model architecture we used\nmodel_architecture = \"PPO\"\n\n## Define a repo_id\n## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n## CHANGE WITH YOUR REPO ID\nrepo_id = \"ThomasSimonini/ppo-LunarLander-v2\"  # Change with your repo id, you can't push with mine 😄\n\n## Define the commit message\ncommit_message = \"Upload PPO LunarLander-v2 trained agent\"\n\n# Create the evaluation env and set the render_mode=\"rgb_array\"\neval_env = DummyVecEnv([lambda: Monitor(gym.make(env_id, render_mode=\"rgb_array\"))])\n\n# PLACE the package_to_hub function you've just filled here\npackage_to_hub(\n    model=model,  # Our trained model\n    model_name=model_name,  # The name of our trained model\n    model_architecture=model_architecture,  # The model architecture we used: in our case PPO\n    env_id=env_id,  # Name of the environment\n    eval_env=eval_env,  # Evaluation Environment\n    repo_id=repo_id,  # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n    commit_message=commit_message,\n)\n```\n\n----------------------------------------\n\nTITLE: Exploring Environment Action Space\nDESCRIPTION: Displaying information about the action space in FrozenLake, which consists of four discrete actions (left, down, right, up).\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprint(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"Action Space Shape\", env.action_space.n)\nprint(\"Action Space Sample\", env.action_space.sample())  # Take a random action\n```\n\n----------------------------------------\n\nTITLE: Installing Python RL Dependencies (Shell)\nDESCRIPTION: Installs specific versions of Python libraries required for the PPO implementation and Hugging Face Hub integration using pip. This includes Gym (with Box2D extras for LunarLander), imageio-ffmpeg for video processing, and the huggingface_hub library.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\npip install gym==0.22\npip install imageio-ffmpeg\npip install huggingface_hub\npip install gym[box2d]==0.22\n```\n\n----------------------------------------\n\nTITLE: Installing Python Requirements via pip (Shell)\nDESCRIPTION: Uses the `pip` package manager to install Python libraries specified in a remote requirements file (`requirements-unit1.txt`). This file, hosted on GitHub, contains essential libraries for the tutorial, such as `gymnasium`, `stable-baselines3`, and `huggingface_sb3`.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit1/unit1.ipynb#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt\n```\n\n----------------------------------------\n\nTITLE: Install Python dependencies\nDESCRIPTION: This bash script installs Python dependencies specified in the `requirements-unit1.txt` file using `pip`. The file is located on the Hugging Face Deep RL Class GitHub repository. It includes packages like `gymnasium`, `stable-baselines3`, and `huggingface_sb3`.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit1/hands-on.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt\n```\n\n----------------------------------------\n\nTITLE: Code Defining the Discrete Action Space for the Environment\nDESCRIPTION: This snippet defines the environment's action space as a discrete set with four possible actions, likely corresponding to movement or interaction commands. Dependencies include the environment's control API or action handler, and the actions are designed for reinforcement learning algorithms to select.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit5/pyramids.mdx#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\naction_space = Discrete(4)\n\n# Actions could be: 0 = move forward, 1 = turn left, 2 = turn right, 3 = interact\n```\n\n----------------------------------------\n\nTITLE: Checking for GPU Availability in PyTorch\nDESCRIPTION: This snippet verifies if CUDA-enabled GPU is accessible. It sets the device to 'cuda:0' if available, otherwise defaults to CPU, enabling faster training and inference for the Reinforce algorithm.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n```\n\n----------------------------------------\n\nTITLE: Authenticating with Hugging Face Hub in Python\nDESCRIPTION: Code to authenticate with the Hugging Face Hub using notebook_login() function and configure git credentials for storing authentication token.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit6/hands-on.mdx#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nnotebook_login()\n!git config --global credential.helper store\n```\n\n----------------------------------------\n\nTITLE: Uploading a Specific Trained Model to Hub - Bash\nDESCRIPTION: This is an example of how to upload a Deep Q-Learning agent trained on SpaceInvadersNoFrameskip-v4 environment to the Hugging Face Hub. It specifies the algorithm, environment, repository name, your Hugging Face username, and the directory containing the trained model. The command demonstrates the specific values that can be used to push a model.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit3/hands-on.mdx#_snippet_8\n\nLANGUAGE: Bash\nCODE:\n```\npython -m rl_zoo3.push_to_hub  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --repo-name dqn-SpaceInvadersNoFrameskip-v4  -orga ThomasSimonini  -f logs/\n```\n\n----------------------------------------\n\nTITLE: Train A2C Agent\nDESCRIPTION: This python code trains the A2C agent for 1,000,000 timesteps using the learn method of the A2C model.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit6/hands-on.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmodel.learn(1_000_000)\n```\n\n----------------------------------------\n\nTITLE: Printing Action Space Information (Python)\nDESCRIPTION: Prints the size (number of actions) and demonstrates sampling a random action from the action space of the Gym environment. This clarifies the available actions for the agent. Requires an initialized Gym `env` object.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprint(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"Action Space Shape\", env.action_space.n)\nprint(\"Action Space Sample\", env.action_space.sample()) # Take a random action\n```\n\n----------------------------------------\n\nTITLE: Code for Observation Space Setup with Raycasts, Switch State, and Agent Speed\nDESCRIPTION: This snippet initializes and processes the observation space for the agent, utilizing 148 raycasts to detect objects, a boolean for the switch state, and a vector representing the agent's speed. Dependencies include raycast sensors, environment status variables, and methods to update and interpret these observations.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit5/pyramids.mdx#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nobservation = {\n    'raycasts': [detect_object(ray) for ray in rays],\n    'switch': switch_state,\n    'speed': agent_speed\n}\n```\n\n----------------------------------------\n\nTITLE: Calling Evaluation Function on Trained Policy to Obtain Performance Metrics in Python\nDESCRIPTION: Executes the evaluate_agent() function using the evaluation environment, maximum steps, number of evaluation episodes, and the trained policy network to compute the average reward and standard deviation. This snippet is used to quantitatively assess agent performance post-training.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nevaluate_agent(eval_env, \n               cartpole_hyperparameters[\"max_t\"], \n               cartpole_hyperparameters[\"n_evaluation_episodes\"],\n               cartpole_policy)\n```\n\n----------------------------------------\n\nTITLE: Install Dependencies\nDESCRIPTION: This bash script installs the required dependencies: stable-baselines3, gymnasium, huggingface_sb3, huggingface_hub, and panda_gym. These libraries are essential for creating and training the reinforcement learning agent.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit6/hands-on.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install stable-baselines3[extra]\n!pip install gymnasium\n!pip install huggingface_sb3\n!pip install huggingface_hub\n!pip install panda_gym\n```\n\n----------------------------------------\n\nTITLE: SoccerTwos Configuration File\nDESCRIPTION: This YAML file configures the training parameters for the SoccerTwos environment using the POCA trainer. It defines hyperparameters such as batch size, buffer size, learning rate, and epsilon, as well as network settings, reward signals, and self-play parameters. The file is located at `./config/poca/SoccerTwos.yaml`.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx#_snippet_4\n\nLANGUAGE: csharp\nCODE:\n```\nbehaviors:\n  SoccerTwos:\n    trainer_type: poca\n    hyperparameters:\n      batch_size: 2048\n      buffer_size: 20480\n      learning_rate: 0.0003\n      beta: 0.005\n      epsilon: 0.2\n      lambd: 0.95\n      num_epoch: 3\n      learning_rate_schedule: constant\n    network_settings:\n      normalize: false\n      hidden_units: 512\n      num_layers: 2\n      vis_encode_type: simple\n    reward_signals:\n      extrinsic:\n        gamma: 0.99\n        strength: 1.0\n    keep_checkpoints: 5\n    max_steps: 5000000\n    time_horizon: 1000\n    summary_freq: 10000\n    self_play:\n      save_steps: 50000\n      team_change: 200000\n      swap_steps: 2000\n      window: 10\n      play_against_latest_model_ratio: 0.5\n      initial_elo: 1200.0\n```\n\n----------------------------------------\n\nTITLE: Logging into Hugging Face Hub using Python\nDESCRIPTION: This Python snippet uses the `notebook_login` function from the `huggingface_hub` library to facilitate logging into the Hugging Face Hub from a Jupyter Notebook or Google Colab environment. It prompts the user to paste a Hugging Face access token with write permissions. An alternative for non-notebook environments is the `huggingface-cli login` command.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unitbonus1/train.mdx#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n```\n\n----------------------------------------\n\nTITLE: Installing Python Libraries: Sample Factory and ViZDoom\nDESCRIPTION: These Python commands install the required Python packages for using the Sample Factory reinforcement learning library and the ViZDoom environment. 'faster-fifo' is installed to provide efficient inter-process communication, 'vizdoom' installs the Doom environment wrapper, and 'sample-factory' version 2.1.1 ensures compatibility with available examples and environment integrations. Python's package manager pip is used to perform the installations within the notebook or Python environment. This snippet must be executed after system dependencies are installed and internet connectivity is required to fetch packages from PyPI.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part2.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n!pip install faster-fifo==1.4.2\n!pip install vizdoom\n```\n\nLANGUAGE: python\nCODE:\n```\n!pip install sample-factory==2.1.1\n```\n\n----------------------------------------\n\nTITLE: Start Virtual Display - Python\nDESCRIPTION: This code initializes and starts a virtual display using the pyvirtualdisplay library. This is crucial for rendering environments, such as FrozenLake, within a Colab environment, which doesn't have a physical display. The `visible=0` argument means the display isn't shown, and `size` sets the dimensions.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()\n```\n\n----------------------------------------\n\nTITLE: Training Command (Mac)\nDESCRIPTION: This bash command starts the training process for the SoccerTwos environment on macOS. It specifies the path to the configuration file, the environment executable (as an `.app` bundle), the run ID, and disables graphics. The executable path needs to be adjusted based on where the SoccerTwos.app is located.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nmlagents-learn ./config/poca/SoccerTwos.yaml --env=./training-envs-executables/SoccerTwos/SoccerTwos.app --run-id=\"SoccerTwos\" --no-graphics\n```\n\n----------------------------------------\n\nTITLE: Authenticating with Hugging Face Hub in Notebook (Python)\nDESCRIPTION: Python code snippet using the `huggingface_hub` library to authenticate the user within a notebook environment (like Jupyter or Colab). This function prompts the user to enter their Hugging Face Hub token. This step is necessary before pushing models to the Hugging Face Hub and requires a user token with write permissions.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus-unit1.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import notebook_login\nnotebook_login()\n```\n\n----------------------------------------\n\nTITLE: Training the Policy Network using REINFORCE Algorithm\nDESCRIPTION: This snippet performs training of the policy network through the REINFORCE algorithm, utilizing hyperparameters such as number of episodes, maximum steps per episode, and discount factor gamma. It requires the policy, optimizer, environment parameters, and training configuration. Inputs include current policy, optimizer, and training hyperparameters; output is the trained policy.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_30\n\nLANGUAGE: Python\nCODE:\n```\nscores = reinforce(\n    policy,\n    optimizer,\n    hyperparameters[\"n_training_episodes\"],\n    hyperparameters[\"max_t\"],\n    hyperparameters[\"gamma\"],\n    1000,\n)\n```\n\n----------------------------------------\n\nTITLE: Save A2C Model and VecNormalize Statistics\nDESCRIPTION: This python script saves the trained A2C model and VecNormalize statistics. Saving VecNormalize statistics is important, because it stores the mean and standard deviation used to normalize the observations and rewards.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit6/hands-on.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmodel.save(\"a2c-PandaReachDense-v3\")\nenv.save(\"vec_normalize.pkl\")\n```\n\n----------------------------------------\n\nTITLE: Uploading Model and Video to Hugging Face Hub\nDESCRIPTION: This snippet guides the user through creating an account, obtaining an access token, and then pushing training artifacts (model checkpoints and videos) to the Hugging Face Hub for sharing.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-sf.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import notebook_login\nnotebook_login()\n!git config --global credential.helper store\n```\n\n----------------------------------------\n\nTITLE: Evaluating Downloaded ViZDoom Agent (Python)\nDESCRIPTION: Evaluates a downloaded pre-trained agent by parsing the configuration for the 'doom_health_gathering_supreme' environment, specifying the experiment name (`--experiment`) and the training directory (`--train_dir`) containing the downloaded model.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part2.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nenv = \"doom_health_gathering_supreme\"\ncfg = parse_vizdoom_cfg(argv=[f\"--env={env}\", \"--num_workers=1\", \"--save_video\", \"--no_render\", \"--max_num_episodes=10\", \"--experiment=doom_health_gathering_supreme_2222\", \"--train_dir=train_dir\"], evaluation=True)\nstatus = enjoy(cfg)\n```\n\n----------------------------------------\n\nTITLE: Displaying a Video of Trained Doom Agent in Python\nDESCRIPTION: Code to display a video of a pre-trained agent playing the Doom health gathering scenario using IPython's HTML display functionality.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-sf.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import HTML\n\nHTML(\n    \"\"\"<video width=\"640\" height=\"480\" controls>\n  <source src=\"https://huggingface.co/edbeeching/doom_health_gathering_supreme_3333/resolve/main/replay.mp4\"\n  type=\"video/mp4\">Your browser does not support the video tag.</video>\"\"\"\n)\n```\n\n----------------------------------------\n\nTITLE: Displaying the Trained Q-table in Python\nDESCRIPTION: Code to display the Q-table after training, which contains the learned action values for each state.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nQtable_frozenlake\n```\n\n----------------------------------------\n\nTITLE: Instantiating PPO Model with MLP Policy using Stable Baselines3 in Python\nDESCRIPTION: This snippet defines and instantiates a Proximal Policy Optimization (PPO) model using Stable Baselines3 with an MLP (Multi-Layer Perceptron) policy suitable for vector input like LunarLander observations. Key parameters such as the number of steps per update, batch size, number of epochs, discount factor (gamma), GAE lambda, and entropy coefficient are explicitly set to optimize training speed and performance. The environment must be already created and passed to the model. Dependencies include the stable_baselines3 library and Gymnasium environment.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit1/unit1.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# SOLUTION\n# We added some parameters to accelerate the training\nmodel = PPO(\n    policy = 'MlpPolicy',\n    env = env,\n    n_steps = 1024,\n    batch_size = 64,\n    n_epochs = 4,\n    gamma = 0.999,\n    gae_lambda = 0.98,\n    ent_coef = 0.01,\n    verbose=1)\n```\n\n----------------------------------------\n\nTITLE: Huggy Agent Configuration YAML\nDESCRIPTION: This YAML configuration file defines the training parameters for the Huggy agent using the PPO algorithm within the ML-Agents framework. It specifies hyperparameters such as batch size, learning rate, and network settings, as well as reward signals and checkpoint intervals. This file is crucial for customizing the agent's learning process.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus_unit1.ipynb#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nbehaviors:\n  Huggy:\n    trainer_type: ppo\n    hyperparameters:\n      batch_size: 2048\n      buffer_size: 20480\n      learning_rate: 0.0003\n      beta: 0.005\n      epsilon: 0.2\n      lambd: 0.95\n      num_epoch: 3\n      learning_rate_schedule: linear\n    network_settings:\n      normalize: true\n      hidden_units: 512\n      num_layers: 3\n      vis_encode_type: simple\n    reward_signals:\n      extrinsic:\n        gamma: 0.995\n        strength: 1.0\n    checkpoint_interval: 200000\n    keep_checkpoints: 15\n    max_steps: 2e6\n    time_horizon: 1000\n    summary_freq: 50000\n```\n\n----------------------------------------\n\nTITLE: Create a Virtual Display\nDESCRIPTION: This snippet initializes a virtual display using the pyvirtualdisplay library.  This is necessary to render the environment during training and generate replay videos in a Colab environment without a physical display. The virtual display is set to be invisible and has a specified size.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()\n```\n\n----------------------------------------\n\nTITLE: Displaying Observation Space Sample for PixelCopter Environment in Python\nDESCRIPTION: Prints out the dimension of the observation space and a sample observation from the PixelCopter environment. This helps in understanding what an agent's state input looks like during training or evaluation.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nprint(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"The State Space is: \", s_size)\nprint(\"Sample observation\", env.observation_space.sample()) # Get a random observation\n```\n\n----------------------------------------\n\nTITLE: Initializing PixelCopter Gym Environment and Setting Observation and Action Space in Python\nDESCRIPTION: This snippet sets up the PixelCopter environment from the pygame-learning-environment gym interface by creating training and evaluation environments, and extracts the state (observation) space size and action space size. It prints samples and sizes of spaces to provide insight into the input and output dimensions relevant for agent policy design. Requires gym and pygame-learning-environment installed.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nenv_id = \"Pixelcopter-PLE-v0\"\nenv = gym.make(env_id)\neval_env = gym.make(env_id)\ns_size = env.observation_space.shape[0]\na_size = env.action_space.n\n```\n\n----------------------------------------\n\nTITLE: Logging in to Hugging Face CLI - Shell\nDESCRIPTION: This shell command logs the user into the Hugging Face CLI, which is necessary for uploading models programmatically from the terminal. Prerequisites: huggingface-cli should be installed and requires an authentication token. The command initiates an interactive login process. Use this if not running in Jupyter or Colab.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_31\n\nLANGUAGE: Shell\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Training the PyramidsRND Agent with MLAgents - Python\nDESCRIPTION: This cell executes MLAgents' mlagents-learn to initiate PPO-based RL training for the PyramidsRND environment using a Unity-provided configuration file. It points to the correct executable, sets a descriptive run ID, and disables graphics. Prerequisites: MLAgents (pip install mlagents), available config/ppo/PyramidsRND.yaml, and correct environment executable path. The cell outputs live training logs and writes the model/state under the run-specific directory.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit5/unit5.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n!mlagents-learn ./config/ppo/PyramidsRND.yaml --env=./training-envs-executables/linux/Pyramids/Pyramids --run-id=\"Pyramids Training\" --no-graphics\n```\n\n----------------------------------------\n\nTITLE: Install Additional Dependencies for Training\nDESCRIPTION: This code block installs additional packages needed, including python-opengl, ffmpeg, xvfb, swig, cmake, pyglet, and pyvirtualdisplay. These dependencies are necessary for rendering the environment and creating a virtual display within the Colab environment, enabling the recording of the environment frames and generating a replay video.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n!apt install python-opengl\n!apt install ffmpeg\n!apt install xvfb\n!apt install swig cmake\n!pip install pyglet==1.5\n!pip3 install pyvirtualdisplay\n```\n\n----------------------------------------\n\nTITLE: Getting the Action Space Size for Taxi-v3 (Python)\nDESCRIPTION: Retrieves the total number of possible actions the agent can take in the initialized Taxi-v3 environment using the `action_space.n` attribute and prints the result. This value is needed for defining the dimensions of the Q-table.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_28\n\nLANGUAGE: Python\nCODE:\n```\naction_space = env.action_space.n\nprint(\"There are \", action_space, \" possible actions\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Q-table for Q-Learning\nDESCRIPTION: Creating a Q-table as a numpy array of zeros with dimensions matching the state and action spaces of the environment.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef initialize_q_table(state_space, action_space):\n    Qtable = np.zeros((state_space, action_space))\n    return Qtable\n\nQtable_frozenlake = initialize_q_table(state_space, action_space)\n```\n\n----------------------------------------\n\nTITLE: Push Agent to Hugging Face Hub\nDESCRIPTION: This command utilizes the `mlagents-push-to-hf` tool to upload the trained Huggy agent to the Hugging Face Hub. It requires parameters such as the run ID, local directory where the agent is saved, the repository ID, and a commit message. This step allows users to share their trained agents with the community.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus_unit1.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n!mlagents-push-to-hf --run-id=\"HuggyTraining\" --local-dir=\"./results/Huggy2\" --repo-id=\"ThomasSimonini/ppo-Huggy\" --commit-message=\"Huggy\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Object Reset Method in GDScript\nDESCRIPTION: Every object that belongs to the 'resetable' group must implement a reset() method that handles returning the object to its initial state. This method is called when an episode ends to prepare the environment for the next training iteration.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unitbonus5/customize-the-environment.mdx#_snippet_1\n\nLANGUAGE: GDScript\nCODE:\n```\n# Required method for all objects in 'resetable' group\nfunc reset():\n    # Implementation to restore object to initial state\n```\n\n----------------------------------------\n\nTITLE: Providing Expandable Answer Key using React <details> Component in JSX\nDESCRIPTION: This snippet utilizes the <details> HTML element, integrated within JSX, to reveal a solution or explanation upon user interaction. It provides a collapsible answer for a quiz question, supporting deeper learner understanding without immediate answer exposure. Dependencies include a React environment capable of rendering standard HTML elements within JSX. The input is text content within <summary> and the expanded details; output is an interactive UI block that toggles answer visibility. It is suitable for inline explanations or self-checking exercises.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit7/quiz.mdx#_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\n<details>\n<summary>Solution</summary>\n\n`Self-play` is an approach to instantiate copies of agents with the same policy as your as opponents, so that your agent learns from agents with same training level.\n\n</details>\n```\n\n----------------------------------------\n\nTITLE: Installing ML-Agents Dependencies\nDESCRIPTION: This bash snippet installs the required dependencies for the ML-Agents library. It uses `pip3 install -e` to install the packages from their source code in the cloned repository, enabling editable installs of the necessary packages for the ML-Agents environment and the agents themselves.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit5/hands-on.mdx#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Go inside the repository and install the package (can take 3min)\n%cd ml-agents\npip3 install -e ./ml-agents-envs\npip3 install -e ./ml-agents\n```\n\n----------------------------------------\n\nTITLE: Calling package_to_hub Function\nDESCRIPTION: This code snippet shows how to use the `package_to_hub` function to upload a trained agent along with its evaluation environment, hyperparameters, and TensorBoard logs. The code creates an evaluation environment using the environment ID specified in the arguments and then calls `package_to_hub` function.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Create the evaluation environment\neval_env = gym.make(args.env_id)\n\npackage_to_hub(repo_id = args.repo_id,\n                model = agent, # The model we want to save\n                hyperparameters = args,\n                eval_env = gym.make(args.env_id),\n                logs= f\"runs/{run_name}\",\n                )\n```\n\n----------------------------------------\n\nTITLE: Authenticate and Configure Git for Hugging Face Hub Access\nDESCRIPTION: This snippet contains commands used in a notebook environment to authenticate with the Hugging Face Hub and configure git to store credentials globally. `notebook_login()` prompts for a Hugging Face token with write access, and `!git config --global credential.helper store` makes git remember the credentials.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit1/hands-on.mdx#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nnotebook_login()\n!git config --global credential.helper store\n```\n\n----------------------------------------\n\nTITLE: Initializing Virtual Display (Python)\nDESCRIPTION: Imports the `Display` class from `pyvirtualdisplay` and initializes and starts a virtual display instance. This is necessary in environments like Google Colab to render graphical output from Gym environments without a physical screen. The display is set to be invisible (`visible=0`) and configured with a specific size.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()\n```\n\n----------------------------------------\n\nTITLE: Initializing Gym Environment (Pixelcopter) Python\nDESCRIPTION: Initializes two instances of the 'Pixelcopter-PLE-v0' environment using the Gym library: one for training and one for evaluation. It also extracts and stores the size of the observation space and action space for use in defining the agent's policy network. Requires the `gym` library and the 'Pixelcopter-PLE-v0' environment.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nenv_id = \"Pixelcopter-PLE-v0\"\nenv = gym.make(env_id)\neval_env = gym.make(env_id)\ns_size = env.observation_space.shape[0]\na_size = env.action_space.n\n```\n\n----------------------------------------\n\nTITLE: Install ML-Agents Library and Envs (Shell)\nDESCRIPTION: Changes the current directory to the cloned `ml-agents` repository using the `%cd` magic command. Subsequently, it uses `pip3` to install the `ml-agents-envs` and `ml-agents` Python packages in editable mode (`-e`). Installing in editable mode allows the packages to be used directly from the source code directory, which is common in development or tutorial environments.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit5/unit5.ipynb#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\n%%capture\n# Go inside the repository and install the package (can take 3min)\n%cd ml-agents\n!pip3 install -e ./ml-agents-envs\n!pip3 install -e ./ml-agents\n```\n\n----------------------------------------\n\nTITLE: Downloading the Huggy Environment Zip File - Python\nDESCRIPTION: This snippet uses wget to download the Huggy.zip environment executable from a GitHub raw link, saving it to the prepared directory. It is a necessary step so the custom Unity RL environment can later be extracted and run within Colab. No parameters or dependencies beyond shell utilities are needed.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus_unit1.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n!wget \"https://github.com/huggingface/Huggy/raw/main/Huggy.zip\" -O ./trained-envs-executables/linux/Huggy.zip\n```\n\n----------------------------------------\n\nTITLE: Publishing Q-Learning Model to Hugging Face Hub in Python\nDESCRIPTION: Defines a comprehensive function that executes the full pipeline for publishing a Q-Learning model to the Hugging Face Hub. It creates or updates a repository, pickles and uploads the model, evaluates it over several episodes to record performance metrics, generates a model card with usage instructions, records a replay video of the agent's behavior, and uploads all artifacts to the Hub. The function handles environment-specific metadata, file management, and integration with Hugging Face APIs, requiring dependencies such as huggingface_hub, gym, datetime, json, and pickle.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef push_to_hub(\n    repo_id, model, env, video_fps=1, local_repo_path=\"hub\"\n):\n    \"\"\"\n    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n    This method does the complete pipeline:\n    - It evaluates the model\n    - It generates the model card\n    - It generates a replay video of the agent\n    - It pushes everything to the Hub\n\n    :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\n    :param env\n    :param video_fps: how many frame per seconds to record our video replay\n    (with taxi-v3 and frozenlake-v1 we use 1)\n    :param local_repo_path: where the local repository is\n    \"\"\"\n    _, repo_name = repo_id.split(\"/\")\n\n    eval_env = env\n    api = HfApi()\n\n    # Step 1: Create the repo\n    repo_url = api.create_repo(\n        repo_id=repo_id,\n        exist_ok=True,\n    )\n\n    # Step 2: Download files\n    repo_local_path = Path(snapshot_download(repo_id=repo_id))\n\n    # Step 3: Save the model\n    if env.spec.kwargs.get(\"map_name\"):\n        model[\"map_name\"] = env.spec.kwargs.get(\"map_name\")\n        if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n            model[\"slippery\"] = False\n\n    # Pickle the model\n    with open((repo_local_path) / \"q-learning.pkl\", \"wb\") as f:\n        pickle.dump(model, f)\n\n    # Step 4: Evaluate the model and build JSON with evaluation metrics\n    mean_reward, std_reward = evaluate_agent(\n        eval_env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"]\n    )\n\n    evaluate_data = {\n        \"env_id\": model[\"env_id\"],\n        \"mean_reward\": mean_reward,\n        \"n_eval_episodes\": model[\"n_eval_episodes\"],\n        \"eval_datetime\": datetime.datetime.now().isoformat()\n    }\n\n    # Write a JSON file called \"results.json\" that will contain the\n    # evaluation results\n    with open(repo_local_path / \"results.json\", \"w\") as outfile:\n        json.dump(evaluate_data, outfile)\n\n    # Step 5: Create the model card\n    env_name = model[\"env_id\"]\n    if env.spec.kwargs.get(\"map_name\"):\n        env_name += \"-\" + env.spec.kwargs.get(\"map_name\")\n\n    if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n        env_name += \"-\" + \"no_slippery\"\n\n    metadata = {}\n    metadata[\"tags\"] = [env_name, \"q-learning\", \"reinforcement-learning\", \"custom-implementation\"]\n\n    # Add metrics\n    eval = metadata_eval_result(\n        model_pretty_name=repo_name,\n        task_pretty_name=\"reinforcement-learning\",\n        task_id=\"reinforcement-learning\",\n        metrics_pretty_name=\"mean_reward\",\n        metrics_id=\"mean_reward\",\n        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n        dataset_pretty_name=env_name,\n        dataset_id=env_name,\n    )\n\n    # Merges both dictionaries\n    metadata = {**metadata, **eval}\n\n    model_card = f\"\"\"\n  # **Q-Learning** Agent playing1 **{env_id}**\n  This is a trained model of a **Q-Learning** agent playing **{env_id}** .\n\n  ## Usage\n\n  ```python\n\n  model = load_from_hub(repo_id=\"{repo_id}\", filename=\"q-learning.pkl\")\n\n  # Don't forget to check if you need to add additional attributes (is_slippery=False etc)\n  env = gym.make(model[\"env_id\"])\n  ```\n  \"\"\"\n\n    evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])\n\n    readme_path = repo_local_path / \"README.md\"\n    readme = \"\"\n    print(readme_path.exists())\n    if readme_path.exists():\n        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n            readme = f.read()\n    else:\n        readme = model_card\n\n    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(readme)\n\n    # Save our metrics to Readme metadata\n    metadata_save(readme_path, metadata)\n\n    # Step 6: Record a video\n    video_path = repo_local_path / \"replay.mp4\"\n    record_video(env, model[\"qtable\"], video_path, video_fps)\n\n    # Step 7. Push everything to the Hub\n    api.upload_folder(\n        repo_id=repo_id,\n        folder_path=repo_local_path,\n        path_in_repo=\".\",\n    )\n\n    print(\"Your model is pushed to the Hub. You can view your model here: \", repo_url)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Compatible Python Virtual Environment (Bash)\nDESCRIPTION: Creates a dedicated Python virtual environment compatible with ML-Agents requirements. It first installs `virtualenv`, then downloads and installs Miniconda. Subsequently, it uses `conda` to install a specific Python version (3.10.12) along with `ujson`. Finally, it sets environment variables for `PYTHONPATH` and `CONDA_PREFIX` to configure the environment. This setup ensures the ML-Agents library installs and runs correctly, avoiding potential version conflicts with the default Colab Python.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unitbonus1/train.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Install virtualenv and create a virtual environment\n!pip install virtualenv\n!virtualenv myenv\n\n# Download and install Miniconda\n!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n!chmod +x Miniconda3-latest-Linux-x86_64.sh\n!./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n\n# Activate Miniconda and install Python ver 3.10.12\n!source /usr/local/bin/activate\n!conda install -q -y --prefix /usr/local python=3.10.12 ujson  # Specify the version here\n\n# Set environment variables for Python and conda paths\n!export PYTHONPATH=/usr/local/lib/python3.10/site-packages/\n!export CONDA_PREFIX=/usr/local/envs/myenv\n```\n\n----------------------------------------\n\nTITLE: Defining and Managing Action Space and Actions in Godot AI Controller Using GDScript\nDESCRIPTION: Specifies the action space dictionary describing continuous actions for movement (2 floats), rotation (1 float), jump (1 float), and use_action (1 float). Implements get_action() to return an ordered flat array containing all action values as floats, scaled appropriately to fit expected ranges (notably for boolean states transformed to -1 or 1). The set_action() method applies actions either from human input (when argument is null) or from the AI agent by setting player control variables accordingly. This approach supports multiple control modes and ensures consistent input processing for the robot. Dependencies include player input mappings, and proper synchronization of calls.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unitbonus5/getting-started.mdx#_snippet_2\n\nLANGUAGE: GDScript\nCODE:\n```\n# Defines the actions for the AI agent (\"size\": 2 means 2 floats for this action)\nfunc get_action_space() -> Dictionary:\n\treturn {\n\t\t\"movement\": {\"size\": 2, \"action_type\": \"continuous\"},\n\t\t\"rotation\": {\"size\": 1, \"action_type\": \"continuous\"},\n\t\t\"jump\": {\"size\": 1, \"action_type\": \"continuous\"},\n\t\t\"use_action\": {\"size\": 1, \"action_type\": \"continuous\"}\n\t}\n\n# We return the action values in the same order as defined in get_action_space() (important), but all in one array\n# For actions of size 1, we return 1 float in the array, for size 2, 2 floats in the array, etc.\n# set_action is called just before get_action by the sync node, so we can read the newly set values\nfunc get_action():\n\treturn [\n\t\t# \"movement\" action values\n\t\tplayer.requested_movement.x,\n\t\tplayer.requested_movement.y,\n\t\t# \"rotation\" action value\n\t\tplayer.requested_rotation.x,\n\t\t# \"jump\" action value (-1 if not requested, 1 if requested)\n\t\t-1.0 + 2.0 * float(player.jump_requested),\n\t\t# \"use_action\" action value (-1 if not requested, 1 if requested)\n\t\t-1.0 + 2.0 * float(player.use_action_requested)\n\t]\n\n# Here we set human control and AI control actions to the robot\nfunc set_action(action = null) -> void:\n\t# If there's no action provided, it means that AI is not controlling the robot (human control),\n\tif not action:\n\t\t# Only rotate if the mouse has moved since the last set_action call\n\t\tif previous_mouse_movement == mouse_movement:\n\t\t\tmouse_movement = Vector2.ZERO\n\n\t\tplayer.requested_movement = Input.get_vector(\n\t\t\t\"move_left\", \"move_right\", \"move_forward\", \"move_back\"\n\t\t)\n\t\tplayer.requested_rotation = mouse_movement\n\n\t\tvar use_action = Input.is_action_pressed(\"requested_action\")\n\t\tvar jump = Input.is_action_pressed(\"requested_jump\")\n\n\t\tplayer.use_action_requested = use_action\n\t\tplayer.jump_requested = jump\n\n\t\tprevious_mouse_movement = mouse_movement\t\n\telse:\n\t\t# If there is action provided, we set the actions received from the AI agent \n\t\tplayer.requested_movement = Vector2(action.movement[0], action.movement[1])\n\t\t# The agent only rotates the robot along the Y axis, no need to rotate the camera along X axis\n\t\tplayer.requested_rotation = Vector2(action.rotation[0], 0.0)\n\t\tplayer.jump_requested = bool(action.jump[0] > 0)\n\t\tplayer.use_action_requested = bool(action.use_action[0] > 0)\n```\n\n----------------------------------------\n\nTITLE: Pushing Trained DQN Model to Hugging Face Hub\nDESCRIPTION: Uploads a trained Deep Q-Learning model for Space Invaders to the Hugging Face Hub using rl_zoo3's push_to_hub utility, creating a repository with model files and evaluation metrics.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit3/unit3.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n!python -m rl_zoo3.push_to_hub  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --repo-name _____________________ -orga _____________________ -f logs/\n```\n\n----------------------------------------\n\nTITLE: Quiz Question 2: Bias and Variance Statements in RL (JSX)\nDESCRIPTION: Defines the second quiz question, which asks the user to identify true statements about bias and variance in Reinforcement Learning models. It includes multiple choices, each with a boolean 'correct' flag and an optional explanation.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit6/quiz.mdx#_snippet_1\n\nLANGUAGE: JSX\nCODE:\n```\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"An unbiased reward signal returns rewards similar to the real / expected ones from the environment\",\n\t\t\texplain: \"\",\n      \t\t\tcorrect: true,\n\t\t},\n    \t\t{\n\t\t\ttext: \"A biased reward signal returns rewards similar to the real / expected ones from the environment\",\n\t\t\texplain: \"If a reward signal is biased, it means the reward signal we get differs from the real reward we should be getting from an environment\",\n      \t\t\tcorrect: false,\n\t\t},\n    \t\t{\n\t\t\ttext: \"A reward signal with high variance has much noise in it and gets affected by, for example, stochastic (non constant) elements in the environment\",\n\t\t\texplain: \"\",\n      \t\t\tcorrect: true,\n\t\t},\t\t\n    \t\t{\n\t\t\ttext: \"A reward signal with low variance has much noise in it and gets affected by, for example, stochastic (non constant) elements in the environment\",\n\t\t\texplain: \"If a reward signal has low variance, then it's less affected by the noise of the environment and produce similar values regardless the random elements in the environment\",\n      \t\t\tcorrect: false,\n\t\t},\n\t]}\n/>\n```\n\n----------------------------------------\n\nTITLE: Create Environment Initialization Function for Gym with Video Recording in Python\nDESCRIPTION: Generates a callable that creates and initializes a Gym environment with optional video recording, seeding, and wrappers for episode statistics. Supports multiple environment instances for parallel training.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\ndef make_env(env_id, seed, idx, capture_video, run_name):\n    def thunk():\n        env = gym.make(env_id)\n        env = gym.wrappers.RecordEpisodeStatistics(env)\n        if capture_video:\n            if idx == 0:\n                env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n        env.seed(seed)\n        env.action_space.seed(seed)\n        env.observation_space.seed(seed)\n        return env\n\n    return thunk\n```\n\n----------------------------------------\n\nTITLE: Installing ML-Agents Dependencies (Bash)\nDESCRIPTION: Installs the required ML-Agents Python packages from the cloned repository. It first changes the current directory to `ml-agents` using the Colab magic command `%cd`. Then, it uses `pip3 install -e` to install both `ml-agents-envs` and `ml-agents` packages in editable mode, meaning changes in the source code will be reflected without reinstalling.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unitbonus1/train.mdx#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Go inside the repository and install the package (can take 3min)\n%cd ml-agents\npip3 install -e ./ml-agents-envs\npip3 install -e ./ml-agents\n```\n\n----------------------------------------\n\nTITLE: Import Packages\nDESCRIPTION: This python script imports necessary libraries such as gymnasium, panda_gym, stable_baselines3 and huggingface_sb3. These libraries are used for environment creation, agent training, evaluation and for uploading models to the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit6/hands-on.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport gymnasium as gym\nimport panda_gym\n\nfrom huggingface_sb3 import load_from_hub, package_to_hub\n\nfrom stable_baselines3 import A2C\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\nfrom stable_baselines3.common.env_util import make_vec_env\n\nfrom huggingface_hub import notebook_login\n```\n\n----------------------------------------\n\nTITLE: Using Package to Hub Function with PPO Agent\nDESCRIPTION: Example of how to use the package_to_hub function with a trained PPO agent. It creates an evaluation environment and calls the function with the agent model, hyperparameters, and logs directory.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Create the evaluation environment\neval_env = gym.make(args.env_id)\n\npackage_to_hub(\n    repo_id=args.repo_id,\n    model=agent,  # The model we want to save\n    hyperparameters=args,\n    eval_env=gym.make(args.env_id),\n    logs=f\"runs/{run_name}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Create Environment Executable Directories (Shell)\nDESCRIPTION: Creates the directory path `./training-envs-executables/linux` within the current working directory (expected to be the cloned `ml-agents` directory). This specific directory structure is required by ML-Agents to locate and load the Unity environment executables for training agents.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit5/unit5.ipynb#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\n# Here, we create training-envs-executables and linux\n!mkdir ./training-envs-executables\n!mkdir ./training-envs-executables/linux\n```\n\n----------------------------------------\n\nTITLE: Creating a virtual display for environment rendering\nDESCRIPTION: Set up a headless virtual display using pyvirtualdisplay to allow environment rendering and video recording during training on non-GUI systems such as servers or Colab notebooks.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit3/hands-on.mdx#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()\n```\n\n----------------------------------------\n\nTITLE: Record Video of Agent Playing\nDESCRIPTION: This function records a video of the agent playing in a given environment. It takes the environment, the trained policy, and the output directory as input, rendering each frame and saving them using imageio. The frame rate (fps) is also configurable.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef record_video(env, policy, out_directory, fps=30):\n  images = []  \n  done = False\n  state = env.reset()\n  img = env.render(mode='rgb_array')\n  images.append(img)\n  while not done:\n    state = torch.Tensor(state).to(device)\n    # Take the action (index) that have the maximum expected future reward given that state\n    action, _, _, _  = policy.get_action_and_value(state)\n    state, reward, done, info = env.step(action.cpu().numpy()) # We directly put next_state = state for recording logic\n    img = env.render(mode='rgb_array')\n    images.append(img)\n  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n```\n\n----------------------------------------\n\nTITLE: Creating Directories for Environment Executables in Bash\nDESCRIPTION: Creates the directory structure needed to store the Huggy environment executable files.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus-unit1.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n!mkdir ./trained-envs-executables\n!mkdir ./trained-envs-executables/linux\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries (Python)\nDESCRIPTION: Imports necessary Python libraries for the reinforcement learning task. This includes `gymnasium` for creating and interacting with the environment, `huggingface_sb3` functions (`load_from_hub`, `package_to_hub`) and `huggingface_hub` (`notebook_login`) for Hugging Face Hub integration, and components from `stable_baselines3` (`PPO` algorithm, `make_vec_env` for vectorized environments, `evaluate_policy` for evaluation, and `Monitor` for environment monitoring).\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit1/unit1.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium\n\nfrom huggingface_sb3 import load_from_hub, package_to_hub\nfrom huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.common.monitor import Monitor\n```\n\n----------------------------------------\n\nTITLE: Running Python Stable Baselines 3 Training with ONNX Export via Bash Command\nDESCRIPTION: This Bash snippet shows how to invoke a Python training script for reinforcement learning using the Stable Baselines 3 framework with Godot RL integration. It sets parameters for training timesteps, ONNX model export path, checkpoint saving frequency, and experiment naming. Successful execution requires the Godot project directory as the working directory and appropriate dependencies such as stable_baselines3 package and Godot RL wrappers. The command triggers training and exports the trained model as ONNX for inference within Godot.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unitbonus3/godotrl.mdx#_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\ncd <....> # go into this Godot project directory\npython stable_baselines3_example.py --timesteps=100_000 --onnx_export_path=model.onnx --save_model_path=model.zip --save_checkpoint_frequency=20_000 --experiment_name=exp1\n```\n\n----------------------------------------\n\nTITLE: Printing Action Space Details (Pixelcopter) Python\nDESCRIPTION: Prints the size of the action space and a sample action from the initialized 'Pixelcopter-PLE-v0' environment. This clarifies the number of possible actions the agent can take. Requires the `env` object to be initialized.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nprint(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"The Action Space is: \", a_size)\nprint(\"Action Space Sample\", env.action_space.sample())  # Take a random action\n```\n\n----------------------------------------\n\nTITLE: Generate Model Card String for PPO Agent in Python\nDESCRIPTION: Constructs a string representing a model card for a trained PPO agent, including hyperparameters specified in converted_str and environment ID environment. Used for documenting model details.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nconverted_str = '\\n'.join(converted_str)\n \n  # Step 2: Generate the model card\n  model_card = f\"\"\"\n  # PPO Agent Playing {env_id}\n\n  This is a trained model of a PPO agent playing {env_id}.\n    \n  # Hyperparameters\n  ```python\n  {converted_str}\n  ```\n  \"\"\"\n  return model_card, metadata\n```\n\n----------------------------------------\n\nTITLE: Implement Reinforce Training Loop in Python\nDESCRIPTION: Defines the main training function for the Reinforce algorithm. It iterates through episodes, collects trajectories, calculates discounted returns using a dynamic programming approach, standardizes returns, computes the policy loss based on log probabilities and returns, and performs gradient descent to update policy parameters.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n    # Help us to calculate the score during the training\n    scores_deque = deque(maxlen=100)\n    scores = []\n    # Line 3 of pseudocode\n    for i_episode in range(1, n_training_episodes + 1):\n        saved_log_probs = []\n        rewards = []\n        state = env.reset()\n        # Line 4 of pseudocode\n        for t in range(max_t):\n            action, log_prob = policy.act(state)\n            saved_log_probs.append(log_prob)\n            state, reward, done, _ = env.step(action)\n            rewards.append(reward)\n            if done:\n                break\n        scores_deque.append(sum(rewards))\n        scores.append(sum(rewards))\n\n        # Line 6 of pseudocode: calculate the return\n        returns = deque(maxlen=max_t)\n        n_steps = len(rewards)\n        # Compute the discounted returns at each timestep,\n        # as\n        #      the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n        #\n        # In O(N) time, where N is the number of time steps\n        # (this definition of the discounted return G_t follows the definition of this quantity\n        # shown at page 44 of Sutton&Barto 2017 2nd draft)\n        # G_t = r_(t+1) + r_(t+2) + ...\n\n        # Given this formulation, the returns at each timestep t can be computed\n        # by re-using the computed future returns G_(t+1) to compute the current return G_t\n        # G_t = r_(t+1) + gamma*G_(t+1)\n        # G_(t-1) = r_t + gamma* G_t\n        # (this follows a dynamic programming approach, with which we memorize solutions in order\n        # to avoid computing them multiple times)\n\n        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)\n        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...\n\n        ## Given the above, we calculate the returns at timestep t as:\n        #               gamma[t] * return[t] + reward[t]\n        #\n        ## We compute this starting from the last timestep to the first, in order\n        ## to employ the formula presented above and avoid redundant computations that would be needed\n        ## if we were to do it from first to last.\n\n        ## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps\n        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)\n        ## a normal python list would instead require O(N) to do this.\n        for t in range(n_steps)[::-1]:\n            disc_return_t = returns[0] if len(returns) > 0 else 0\n            returns.appendleft(gamma * disc_return_t + rewards[t])\n\n        ## standardization of the returns is employed to make training more stable\n        eps = np.finfo(np.float32).eps.item()\n        ## eps is the smallest representable float, which is\n        # added to the standard deviation of the returns to avoid numerical instabilities\n        returns = torch.tensor(returns)\n        returns = (returns - returns.mean()) / (returns.std() + eps)\n\n        # Line 7:\n        policy_loss = []\n        for log_prob, disc_return in zip(saved_log_probs, returns):\n            policy_loss.append(-log_prob * disc_return)\n        policy_loss = torch.cat(policy_loss).sum()\n\n        # Line 8: PyTorch prefers gradient descent\n        optimizer.zero_grad()\n        policy_loss.backward()\n        optimizer.step()\n\n        if i_episode % print_every == 0:\n            print(\"Episode {}\tAverage Score: {:.2f}\".format(i_episode, np.mean(scores_deque)))\n\n    return scores\n```\n\n----------------------------------------\n\nTITLE: Implementing Player Control Script for AI Action Application in Godot GDScript\nDESCRIPTION: This GDScript snippet is intended for a Player Node3D in Godot. It integrates AI control by initializing an AIController instance, applying agent actions to the player's rotation in the physics process, and handling game state changes, including resetting on training reset and updating reward upon collision events. Dependencies include the AIController3D node and the Ball node. The script supports both human and AI-driven control determined by the AI controller's heuristic property, where human input uses input axes and AI input uses the move_action variable. Rotations are applied smoothly with respect to delta time and a configurable rotation speed.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unitbonus3/godotrl.mdx#_snippet_4\n\nLANGUAGE: GDScript\nCODE:\n```\nextends Node3D\n\n@export var rotation_speed = 3.0\n@onready var ball = get_node(\"../Ball\")\n@onready var ai_controller = $AIController3D\n\nfunc _ready():\n\tai_controller.init(self)\n\nfunc game_over():\n\tai_controller.done = true\n\tai_controller.needs_reset = true\n\nfunc _physics_process(delta):\n\tif ai_controller.needs_reset:\n\t\tai_controller.reset()\n\t\tball.reset()\n\t\treturn\n\n\tvar movement : float\n\tif ai_controller.heuristic == \"human\":\n\t\tmovement = Input.get_axis(\"rotate_anticlockwise\", \"rotate_clockwise\")\n\telse:\n\t\tmovement = ai_controller.move_action\n\trotate_y(movement*delta*rotation_speed)\n\nfunc _on_area_3d_body_entered(body):\n\tai_controller.reward += 1.0\n```\n\n----------------------------------------\n\nTITLE: Import Required Libraries - Python\nDESCRIPTION: This snippet imports the Python libraries that will be used in the Q-Learning implementation. It includes essential libraries for numerical computation (NumPy), environment interaction (Gymnasium), random number generation (random), image and video handling (imageio), system operations (os), progress bar (tqdm), and pickle for saving and loading models.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport gymnasium as gym\nimport random\nimport imageio\nimport os\nimport tqdm\n\nimport pickle5 as pickle\nfrom tqdm.notebook import tqdm\n```\n\n----------------------------------------\n\nTITLE: Importing Required Python Libraries\nDESCRIPTION: Imports essential Python libraries for the Q-Learning implementation. This includes NumPy for numerical operations and Q-table management, Gymnasium for accessing RL environments (like FrozenLake), `random` for implementing exploration strategies (epsilon-greedy), `imageio` for creating replay videos, `os` for file system operations, `tqdm` for displaying progress bars during training, and `pickle` for saving/loading the trained Q-table.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport gymnasium as gym\nimport random\nimport imageio\nimport os\nimport tqdm\n\nimport pickle5 as pickle\nfrom tqdm.notebook import tqdm\n```\n\n----------------------------------------\n\nTITLE: Login to Hugging Face Hub\nDESCRIPTION: This Python code snippet uses the `huggingface_hub` library to log in to the Hugging Face Hub using a notebook environment. It prompts the user to enter their authentication token, enabling them to push models and datasets to their Hugging Face account. This assumes the user has `huggingface_hub` installed.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus_unit1.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import notebook_login\nnotebook_login()\n```\n\n----------------------------------------\n\nTITLE: Install system dependencies (swig, cmake)\nDESCRIPTION: This bash script installs `swig` and `cmake`, which are dependencies for some of the python packages used in the notebook. These are installed using `apt` (Advanced Package Tool) in the Colab environment.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit1/hands-on.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\napt install swig cmake\n```\n\n----------------------------------------\n\nTITLE: Printing Environment Observation Space\nDESCRIPTION: This code prints the observation space, revealing the different components of observations: achieved goal, desired goal, and observation. It also samples and displays a random observation.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"The State Space is: \", s_size)\nprint(\"Sample observation\", env.observation_space.sample()) # Get a random observation\n```\n\n----------------------------------------\n\nTITLE: Packaging and Uploading Model to Hugging Face Hub\nDESCRIPTION: Main function that handles the complete pipeline for packaging and uploading a model to the Hugging Face Hub. It evaluates the agent, generates a video, creates a model card, and uploads everything in a temporary directory.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef package_to_hub(\n    repo_id,\n    model,\n    hyperparameters,\n    eval_env,\n    video_fps=30,\n    commit_message=\"Push agent to the Hub\",\n    token=None,\n    logs=None,\n):\n    \"\"\"\n    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n    This method does the complete pipeline:\n    - It evaluates the model\n    - It generates the model card\n    - It generates a replay video of the agent\n    - It pushes everything to the hub\n    :param repo_id: id of the model repository from the Hugging Face Hub\n    :param model: trained model\n    :param eval_env: environment used to evaluate the agent\n    :param fps: number of fps for rendering the video\n    :param commit_message: commit message\n    :param logs: directory on local machine of tensorboard logs you'd like to upload\n    \"\"\"\n    msg.info(\n        \"This function will save, evaluate, generate a video of your agent, \"\n        \"create a model card and push everything to the hub. \"\n        \"It might take up to 1min. \\n \"\n        \"This is a work in progress: if you encounter a bug, please open an issue.\"\n    )\n    # Step 1: Clone or create the repo\n    repo_url = HfApi().create_repo(\n        repo_id=repo_id,\n        token=token,\n        private=False,\n        exist_ok=True,\n    )\n\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        tmpdirname = Path(tmpdirname)\n\n        # Step 2: Save the model\n        torch.save(model.state_dict(), tmpdirname / \"model.pt\")\n\n        # Step 3: Evaluate the model and build JSON\n        mean_reward, std_reward = _evaluate_agent(eval_env, 10, model)\n\n        # First get datetime\n        eval_datetime = datetime.datetime.now()\n        eval_form_datetime = eval_datetime.isoformat()\n\n        evaluate_data = {\n            \"env_id\": hyperparameters.env_id,\n            \"mean_reward\": mean_reward,\n            \"std_reward\": std_reward,\n            \"n_evaluation_episodes\": 10,\n            \"eval_datetime\": eval_form_datetime,\n        }\n\n        # Write a JSON file\n        with open(tmpdirname / \"results.json\", \"w\") as outfile:\n            json.dump(evaluate_data, outfile)\n\n        # Step 4: Generate a video\n        video_path = tmpdirname / \"replay.mp4\"\n        record_video(eval_env, model, video_path, video_fps)\n\n        # Step 5: Generate the model card\n        generated_model_card, metadata = _generate_model_card(\n            \"PPO\", hyperparameters.env_id, mean_reward, std_reward, hyperparameters\n        )\n        _save_model_card(tmpdirname, generated_model_card, metadata)\n\n        # Step 6: Add logs if needed\n        if logs:\n            _add_logdir(tmpdirname, Path(logs))\n\n        msg.info(f\"Pushing repo {repo_id} to the Hugging Face Hub\")\n\n        repo_url = upload_folder(\n            repo_id=repo_id,\n            folder_path=tmpdirname,\n            path_in_repo=\"\",\n            commit_message=commit_message,\n            token=token,\n        )\n\n        msg.info(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")\n    return repo_url\n```\n\n----------------------------------------\n\nTITLE: Installing Virtual Display Packages Shell\nDESCRIPTION: Installs necessary packages (`python-opengl`, `xvfb`, `pyvirtualdisplay`) required for setting up a virtual display. This is crucial in headless environments like Google Colab to simulate a screen for rendering the environment and recording video replays.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit3/unit3.ipynb#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\n%%capture\n!apt install python-opengl\n!apt install xvfb\n!pip3 install pyvirtualdisplay\n```\n\n----------------------------------------\n\nTITLE: Setting Up Virtual Display Python\nDESCRIPTION: Initializes and starts a virtual display using the `pyvirtualdisplay` library. This creates a virtual screen environment that allows the reinforcement learning environment to render frames even without a physical display, enabling tasks like video recording.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit3/unit3.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()\n```\n\n----------------------------------------\n\nTITLE: Importing Python Libraries\nDESCRIPTION: This Python snippet imports necessary libraries for implementing the Reinforce algorithm. It includes libraries such as numpy, matplotlib, torch (PyTorch), gym, gym_pygame, huggingface_hub, and imageio.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nfrom collections import deque\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Categorical\n\n# Gym\nimport gym\nimport gym_pygame\n\n# Hugging Face Hub\nfrom huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\nimport imageio\n```\n\n----------------------------------------\n\nTITLE: Authenticating Hugging Face Hub from Jupyter or Colab in Python\nDESCRIPTION: Shows the authentication procedure to store a Hugging Face API token enabling model upload rights from notebook environments. It uses notebook_login from huggingface_hub to prompt for authentication and configures Git to store credentials for future CLI operations seamlessly. Prerequisites include having a Hugging Face account and generated write permission token. Expected inputs are the user’s API token via an interactive prompt. No outputs are directly produced but sets up environment for later uploading.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nnotebook_login()\n!git config --global credential.helper store\n```\n\n----------------------------------------\n\nTITLE: Load Stable-Baselines3 Model from Hugging Face Hub\nDESCRIPTION: This Python snippet demonstrates how to load a saved Stable-Baselines3 model from a specific repository on the Hugging Face Hub using the `huggingface_sb3.load_from_hub` function. It also includes handling for potential pickle protocol compatibility issues by providing custom objects during loading.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit1/hands-on.mdx#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_sb3 import load_from_hub\n\nrepo_id = \"Classroom-workshop/assignment2-omar\"  # The repo_id\nfilename = \"ppo-LunarLander-v2.zip\"  # The model filename.zip\n\n# When the model was trained on Python 3.8 the pickle protocol is 5\n# But Python 3.6, 3.7 use protocol 4\n# In order to get compatibility we need to:\n# 1. Install pickle5 (we done it at the beginning of the colab)\n# 2. Create a custom empty object we pass as parameter to PPO.load()\ncustom_objects = {\n    \"learning_rate\": 0.0,\n    \"lr_schedule\": lambda _: 0.0,\n    \"clip_range\": lambda _: 0.0,\n}\n\ncheckpoint = load_from_hub(repo_id, filename)\nmodel = PPO.load(checkpoint, custom_objects=custom_objects, print_system_info=True)\n```\n\n----------------------------------------\n\nTITLE: Rendering Multiple-Choice Quiz Question with React <Question> Component in JSX\nDESCRIPTION: This snippet uses a custom <Question> React component to present multiple-choice questions with immediate feedback and explanations. Each question specifies a 'choices' array containing answer options, explanations, and boolean correctness flags. The component requires React, an implementation of <Question>, and expects the 'choices' prop as an array of objects including 'text', 'explain', and 'correct' keys. It is designed for interactive quiz applications, enabling automatic answer validation and user guidance. Inputs include the choices list; outputs are rendered UI elements for each option with correct/incorrect selection feedback.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit7/quiz.mdx#_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"competitive, cooperative\",\n\t\t\texplain: \"You maximize common benefit in cooperative, while in competitive you also aim to reduce opponent's score\",\n   \t\t\tcorrect: false,\n\t\t},\n   \t\t{\n\t\t\ttext: \"cooperative, competitive\",\n\t\t\texplain: \"\",\n   \t\t\tcorrect: true,\n\t\t},\n\t]}\n/>\n```\n\nLANGUAGE: jsx\nCODE:\n```\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"Each agent is trained independently from the others\",\n\t\t\texplain: \"\",\n      \t\tcorrect: true,\n\t\t},\n    \t{\n\t\t\ttext: \"Inputs from other agents are just considered environment data\",\n\t\t\texplain: \"\",\n\t\t\tcorrect: true,\n\t\t},\n\t\t{\n\t\t\ttext: \"Considering other agents part of the environment makes the environment stationary\",\n\t\t\texplain: \"In decentralized learning, agents ignore the existence of other agents and consider them part of the environment. However, this means the environment is in constant change, becoming non-stationary.\",\n\t\t\tcorrect: false,\n\t\t},\n\t]}\n/>\n```\n\nLANGUAGE: jsx\nCODE:\n```\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"It learns one common policy based on the learnings from all agents' interactions\",\n\t\t\texplain: \"\",\n      \t\tcorrect: true,\n\t\t},\n    \t{\n\t\t\ttext: \"The reward is global\",\n\t\t\texplain: \"\",\n\t\t\tcorrect: true,\n\t\t},\n\t\t{\n\t\t\ttext: \"The environment with this approach is stationary\",\n\t\t\texplain: \"\",\n\t\t\tcorrect: true,\n\t\t},\n\t]}\n/>\n```\n\nLANGUAGE: jsx\nCODE:\n```\n<Question\n\tchoices={[\n   \t\t {\n\t\t\ttext: \"window, play_against_latest_model_ratio, save_steps, swap_steps+team_change\",\n\t\t\texplain: \"\",\n      \t\tcorrect: false,\n\t\t},\n\t\t{\n\t\t\ttext: \"play_against_latest_model_ratio, save_steps, window, swap_steps+team_change\",\n\t\t\texplain: \"\",\n\t\t\tcorrect: false,\n\t\t},\n\t\t{\n\t\t\ttext: \"play_against_latest_model_ratio, window, save_steps, swap_steps+team_change\",\n\t\t\texplain: \"\",\n\t\t\tcorrect: true,\n\t\t},\n    \t{\n\t\t\ttext: \"swap_steps+team_change, save_steps, play_against_latest_model_ratio, window\",\n\t\t\texplain: \"\",\n      \t\tcorrect: false,\n\t\t},\n\t]}\n/>\n```\n\nLANGUAGE: jsx\nCODE:\n```\n<Question\n\tchoices={[\n   \t\t {\n\t\t\ttext: \"The score takes into account the different of skills between you and your opponent\",\n\t\t\texplain: \"\",\n      \t\tcorrect: true,\n\t\t},\n\t\t{\n\t\t\ttext: \"Although more points can be exchanged depending on the result of the match and given the levels of the agents, the sum is always the same\",\n\t\t\texplain: \"\",\n\t\t\tcorrect: true,\n\t\t},\n\t\t{\n\t\t\ttext: \"It's easy for an agent to keep a high score rate\",\n\t\t\texplain: \"That is called the `Rating deflation`: keeping a high rate requires much skill over time\",\n\t\t\tcorrect: false,\n\t\t},\n    \t{\n\t\t\ttext: \"It works well calculating the individual contributions of each player in a team\",\n\t\t\texplain: \"ELO uses the score achieved by the whole team, but individual contributions are not calculated\",\n      \t\tcorrect: false,\n\t\t},\n\t]}\n/>\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Q-Learning and Hugging Face Integration\nDESCRIPTION: Imports necessary libraries for Hugging Face Hub interaction, file handling, datetime manipulation, and JSON processing. These imports support model submission functionality.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import HfApi, snapshot_download\nfrom huggingface_hub.repocard import metadata_eval_result, metadata_save\n\nfrom pathlib import Path\nimport datetime\nimport json\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Hugging Face Hub Integration in Python\nDESCRIPTION: This snippet imports necessary libraries for interacting with the Hugging Face Hub and working with file paths, dates, and JSON data. It includes the Hugging Face API client and utilities for managing model metadata, which are prerequisites for uploading models and metadata to the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import HfApi, snapshot_download\nfrom huggingface_hub.repocard import metadata_eval_result, metadata_save\n\nfrom pathlib import Path\nimport datetime\nimport json\n```\n\n----------------------------------------\n\nTITLE: Publishing Model to Hub - Configured Example - Python\nDESCRIPTION: This code provides a concrete example of how to configure the `package_to_hub` function. It defines the necessary variables like `repo_id`, `env_id`, `model_architecture`, and `commit_message`, and then calls `package_to_hub` to upload the trained model to the Hugging Face Hub. The provided code is intended to be completed with user-specific information, as indicated in the comments.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit1/unit1.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.env_util import make_vec_env\n\nfrom huggingface_sb3 import package_to_hub\n\n# PLACE the variables you've just defined two cells above\n# Define the name of the environment\nenv_id = \"LunarLander-v2\"\n\n# TODO: Define the model architecture we used\nmodel_architecture = \"PPO\"\n\n## Define a repo_id\n## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n## CHANGE WITH YOUR REPO ID\nrepo_id = \"ThomasSimonini/ppo-LunarLander-v2\" # Change with your repo id, you can't push with mine 😄\n\n## Define the commit message\ncommit_message = \"Upload PPO LunarLander-v2 trained agent\"\n\n# Create the evaluation env and set the render_mode=\"rgb_array\"\neval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n\n# PLACE the package_to_hub function you've just filled here\npackage_to_hub(model=model, # Our trained model\n               model_name=model_name, # The name of our trained model\n               model_architecture=model_architecture, # The model architecture we used: in our case PPO\n               env_id=env_id, # Name of the environment\n               eval_env=eval_env, # Evaluation Environment\n               repo_id=repo_id, # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n               commit_message=commit_message)\n```\n\n----------------------------------------\n\nTITLE: Importing Evaluation Tools\nDESCRIPTION: This cell imports the necessary modules for evaluating the trained agent, including `DummyVecEnv` and `VecNormalize`.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n```\n\n----------------------------------------\n\nTITLE: Uploading Trained Model to Hub - Bash\nDESCRIPTION: This command uses the `python -m rl_zoo3.push_to_hub` command to upload a trained reinforcement learning agent to the Hugging Face Hub. The command takes several arguments: `--algo` (the algorithm used), `--env` (the environment the agent was trained in), `--repo-name` (the name of the repository on the Hub), `-orga` (your Hugging Face username), and `-f` (the folder where the trained model is located). The command effectively packages and pushes the model, along with associated metadata, to your specified repository.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit3/hands-on.mdx#_snippet_7\n\nLANGUAGE: Bash\nCODE:\n```\npython -m rl_zoo3.push_to_hub  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --repo-name _____________________ -orga _____________________ -f logs/\n```\n\n----------------------------------------\n\nTITLE: Pushing Trained Huggy Agent to Hugging Face Hub (Shell)\nDESCRIPTION: Shell command using `mlagents-push-to-hf` executed within a notebook environment (indicated by '!') to upload the trained Huggy agent model artifacts to a Hugging Face Hub repository. It requires specifying the training run ID (`HuggyTraining`), the local directory containing results (`./results/Huggy2`), the target repository ID (`ThomasSimonini/ppo-Huggy`), and a commit message (`Huggy`). Requires prior authentication via `notebook_login` or `huggingface-cli login`.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus-unit1.ipynb#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\n!mlagents-push-to-hf --run-id=\"HuggyTraining\" --local-dir=\"./results/Huggy2\" --repo-id=\"ThomasSimonini/ppo-Huggy\" --commit-message=\"Huggy\"\n```\n\n----------------------------------------\n\nTITLE: Force runtime crash to apply updates\nDESCRIPTION: This Python script forces the Google Colab runtime to crash using `os.kill`. This is done to ensure that the newly installed libraries for the virtual screen are correctly loaded and used. The notebook needs to be restarted after this.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit1/hands-on.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.kill(os.getpid(), 9)\n```\n\n----------------------------------------\n\nTITLE: Defining a Download Function to Load Models from Hugging Face Hub in Python\nDESCRIPTION: This snippet defines a utility function `load_from_hub` for downloading and deserializing a model file from the Hugging Face Hub. It uses `hf_hub_download` to fetch files and standard pickle loading to unserialize model dictionaries. The function is robust to download errors and expects a valid repository identifier and filename corresponding to the stored model. Key dependencies include 'huggingface_hub' and 'pickle'. The output is a loaded model dictionary ready for evaluation or further processing.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nfrom urllib.error import HTTPError\n\nfrom huggingface_hub import hf_hub_download\n\n\ndef load_from_hub(repo_id: str, filename: str) -> str:\n    \"\"\"\n    Download a model from Hugging Face Hub.\n    :param repo_id: id of the model repository from the Hugging Face Hub\n    :param filename: name of the model zip file from the repository\n    \"\"\"\n    # Get the model from the Hub, download and cache the model on your local disk\n    pickle_model = hf_hub_download(repo_id=repo_id, filename=filename)\n\n    with open(pickle_model, \"rb\") as f:\n        downloaded_model_file = pickle.load(f)\n\n    return downloaded_model_file\n```\n\n----------------------------------------\n\nTITLE: Installing Virtual Display Libraries and Setting Up Virtual Display in Python\nDESCRIPTION: This snippet installs necessary system libraries and Python packages like python-opengl, ffmpeg, xvfb, pyvirtualdisplay, and pyglet, then initializes a virtual display for rendering environments in Colab. It is essential for recording environment videos during training.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n%%capture\n!apt install python-opengl\n!apt install ffmpeg\n!apt install xvfb\n!pip install pyvirtualdisplay\n!pip install pyglet==1.5.1\n```\n\nLANGUAGE: Python\nCODE:\n```\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Hub Integration\nDESCRIPTION: This imports necessary libraries from huggingface_hub for saving and uploading model to the Hugging Face Hub. It includes HfApi, upload_folder, repocard utilities, pathlib, datetime, and other modules for file handling, result metadata, and console output.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import HfApi, upload_folder\nfrom huggingface_hub.repocard import metadata_eval_result, metadata_save\n\nfrom pathlib import Path\nimport datetime\nimport tempfile\nimport json\nimport shutil\nimport imageio\n\nfrom wasabi import Printer\nmsg = Printer()\n```\n\n----------------------------------------\n\nTITLE: Creating Virtual Display\nDESCRIPTION: This code creates a virtual display using `pyvirtualdisplay` to enable video recording and rendering of the environment within the Colab environment.  It is essential for generating video replays during training.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()\n```\n\n----------------------------------------\n\nTITLE: Installing SWIG and CMake Dependencies (Shell)\nDESCRIPTION: Uses the `apt` package manager in a Debian-based Linux environment (like Google Colab) to install `swig` and `cmake`. These are build tools often required as dependencies for installing certain Python packages, particularly those with C/C++ extensions like Box2D (used by LunarLander).\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit1/unit1.ipynb#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n!apt install swig cmake\n```\n\n----------------------------------------\n\nTITLE: Set up Virtual Display\nDESCRIPTION: This code installs necessary packages for setting up a virtual display and creates a virtual display instance using `pyvirtualdisplay`. This is required for rendering the environment in environments like Google Colab where a display server is not available by default.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit6/hands-on.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies\nDESCRIPTION: This snippet installs necessary libraries for running the notebook, including OpenAI Gym, Panda-Gym for robotics simulations, Stable-Baselines3 for RL algorithms, and Hugging Face libraries for model saving and loading.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n!apt install python-opengl\n!apt install ffmpeg\n!apt install xvfb\n!pip3 install pyvirtualdisplay\n```\n\n----------------------------------------\n\nTITLE: Install System Dependencies - Python\nDESCRIPTION: This snippet installs system-level dependencies needed for the project, including libraries for virtual display and video encoding. It uses `apt-get` to update the package lists, install OpenGL, FFmpeg, and xvfb, and then pip to install pyvirtualdisplay.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!sudo apt-get update\n!sudo apt-get install -y python3-opengl\n!apt install ffmpeg xvfb\n!pip3 install pyvirtualdisplay\n```\n\n----------------------------------------\n\nTITLE: Defining Hyperparameters for CartPole Training in Python\nDESCRIPTION: Defines a Python dictionary to store hyperparameters necessary for training the cartpole policy gradient agent. Parameters include hidden layer size, training episodes, evaluation episodes, maximum timesteps, discount factor gamma, learning rate, environment ID, and sizes of state and action spaces. These parameters configure the training setup and enable easy adjustments.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ncartpole_hyperparameters = {\n    \"h_size\": 16,\n    \"n_training_episodes\": 1000,\n    \"n_evaluation_episodes\": 10,\n    \"max_t\": 1000,\n    \"gamma\": 1.0,\n    \"lr\": 1e-2,\n    \"env_id\": env_id,\n    \"state_space\": s_size,\n    \"action_space\": a_size,\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies using pip\nDESCRIPTION: This shell command installs the required dependencies using pip. It reads the requirements from a specified URL, ensuring all necessary libraries for the project are installed.  It includes libraries such as gym, gym-games, and huggingface_hub.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt\n```\n\n----------------------------------------\n\nTITLE: Creating Model Dictionary for Hugging Face Hub Submission\nDESCRIPTION: Creates a model dictionary containing all hyperparameters and the trained Q-table for the FrozenLake environment. This structured representation is required for submission to the Hugging Face Hub and ensures reproducibility.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nmodel = {\n    \"env_id\": env_id,\n    \"max_steps\": max_steps,\n    \"n_training_episodes\": n_training_episodes,\n    \"n_eval_episodes\": n_eval_episodes,\n    \"eval_seed\": eval_seed,\n    \"learning_rate\": learning_rate,\n    \"gamma\": gamma,\n    \"max_epsilon\": max_epsilon,\n    \"min_epsilon\": min_epsilon,\n    \"decay_rate\": decay_rate,\n    \"qtable\": Qtable_frozenlake,\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Lunar Lander Replay Video (HTML)\nDESCRIPTION: Embeds an HTML video element using the `%%html` magic command in a Jupyter environment to display a pre-recorded replay of a trained Lunar Lander agent. The video source is hosted on the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit1/unit1.ipynb#_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n%%html\n<video controls autoplay><source src=\"https://huggingface.co/sb3/ppo-LunarLander-v2/resolve/main/replay.mp4\" type=\"video/mp4\"></video>\n```\n\n----------------------------------------\n\nTITLE: Restart Notebook Runtime - Python\nDESCRIPTION: This code restarts the notebook runtime to ensure that all the newly installed libraries are loaded correctly. It uses the os module to terminate the current process with a specific signal, which forces a crash and restart. This is a workaround to refresh the environment.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.kill(os.getpid(), 9)\n```\n\n----------------------------------------\n\nTITLE: React Question Component Usage in Markdown Quiz\nDESCRIPTION: A React component called 'Question' is used to create multiple-choice quiz questions with various options and explanations. The component accepts an array of choice objects with text, explanation, and correct properties.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit5/quiz.mdx#_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"Unity (C#)\",\n\t\t\texplain: \"\",\n            correct: true,\n\t\t},\n\t\t{\n\t\t\ttext: \"Unreal Engine (C++)\",\n\t\t\texplain: \"\",\n            correct: true,\n\t\t},\n\t\t{\n\t\t\ttext: \"Godot (GDScript, C++, C#)\",\n\t\t\texplain: \"\",\n            correct: true,\n\t\t},\n\t\t{\n\t\t\ttext: \"JetBrains' Rider\",\n\t\t\texplain: \"Although useful for its support of C# for Unity, it's not a video games development IDE\",\n            correct: false,\n\t\t},\n\t\t{\n\t\t\ttext: \"JetBrains' CLion\",\n\t\t\texplain: \"Although useful for its support of C++ for Unreal Engine, it's not a video games development IDE\",\n            correct: false,\n\t\t},\n\t\t{\n\t\t\ttext: \"Microsoft Visual Studio and Visual Studio Code\",\n\t\t\texplain: \"Including support for both Unity and Unreal, they are generic IDEs, not video games oriented.\",\n            correct: false,\n\t\t},\n\t]}\n/>\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for Unit 2 (Bash)\nDESCRIPTION: Installs the required Python libraries specified in the `requirements-unit2.txt` file from the deep-rl-class GitHub repository using pip. These libraries include Gymnasium, Pygame, NumPy, and others needed for the Q-Learning exercises.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt\n```\n\n----------------------------------------\n\nTITLE: Printing Device Information\nDESCRIPTION: This code prints the selected device (GPU or CPU) to the console. This allows the user to verify whether the code is running on the desired device.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(device)\n```\n\n----------------------------------------\n\nTITLE: Install Packages for PPO with PyTorch\nDESCRIPTION: This snippet installs the necessary libraries for running the PPO agent, including setuptools, gym, imageio-ffmpeg, huggingface_hub, and gym[box2d]. It ensures that the correct versions of these packages are installed to avoid compatibility issues. This is a crucial step to prepare the environment for the training process.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install setuptools==65.5.0\n```\n\n----------------------------------------\n\nTITLE: Adding Hugging Face Repository Argument to Command Line Parser\nDESCRIPTION: Adds a command-line argument for specifying the Hugging Face Hub repository ID where the model will be uploaded. The default repository is set to 'ThomasSimonini/ppo-CartPole-v1'.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nparser.add_argument(\n    \"--repo-id\",\n    type=str,\n    default=\"ThomasSimonini/ppo-CartPole-v1\",\n    help=\"id of the model repository from the Hugging Face Hub {username/repo_name}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Verifying Python Version After Environment Setup in Bash\nDESCRIPTION: Checks the Python version after setting up the virtual environment to ensure compatibility with ML-Agents.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus-unit1.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Python Version in New Virtual Environment (Compatible with ML-Agents)\n!python --version\n```\n\n----------------------------------------\n\nTITLE: Installing gymnasium with Atari and ROM license support\nDESCRIPTION: Install the gymnasium package with atari environment support and accept-rom-license to enable downloading and running Atari ROMs, essential for training agents on Atari games like Space Invaders.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit3/hands-on.mdx#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n!pip install gymnasium[atari]\n!pip install gymnasium[accept-rom-license]\n```\n\n----------------------------------------\n\nTITLE: Logging Metrics for Training Monitoring\nDESCRIPTION: This snippet logs various training metrics to TensorBoard, including learning rate, value loss, policy loss, entropy, KL divergence, clip fraction, and explained variance, facilitating progress visualization.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_27\n\nLANGUAGE: Python\nCODE:\n```\nwriter.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\nwriter.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\nwriter.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\nwriter.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\nwriter.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\nwriter.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\nwriter.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\nwriter.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies via apt and pip\nDESCRIPTION: This code snippet installs dependencies needed for rendering the environment in Google Colab, specifically for creating a virtual screen to record frames. It uses apt to install system-level dependencies like python-opengl, ffmpeg, and xvfb, and pip to install python packages like pyvirtualdisplay and pyglet.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n!apt install python-opengl\n!apt install ffmpeg\n!apt install xvfb\n!pip install pyvirtualdisplay\n!pip install pyglet==1.5.1\n```\n\n----------------------------------------\n\nTITLE: Implementing Environment Reset in GDScript\nDESCRIPTION: This code snippet references the reset_all_resetables() method in level_manager.gd which is responsible for resetting all objects within the scene that belong to the 'resetable' group. It's called when the robot script determines a reset is needed for a new training episode.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unitbonus5/customize-the-environment.mdx#_snippet_0\n\nLANGUAGE: GDScript\nCODE:\n```\n# Referenced method in level_manager.gd\nfunc reset_all_resetables():\n    # Resets all objects in the 'resetable' group within the current scene\n```\n\n----------------------------------------\n\nTITLE: Running Training Script with SB3\nDESCRIPTION: This Python snippet shows how to execute the training script with specific arguments. It specifies the path to the environment executable, the number of behavior cloning (BC) epochs, the GAIL training timesteps, the path to the demonstration files, the number of parallel processes, a speedup factor, the ONNX export path, and the experiment name.  The script trains the agent using the imitation learning technique.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unitbonus5/train-our-robot.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsb3_imitation.py --env_path=\"path_to_ILTutorial_executable\" --bc_epochs=100 --gail_timesteps=1450000 --demo_files \"path_to_expert_demos.json\" --n_parallel=4 --speedup=20 --onnx_export_path=model.onnx --experiment_name=ILTutorial\n```\n\n----------------------------------------\n\nTITLE: Defining Huggy Training Configuration (YAML)\nDESCRIPTION: Specifies the training configuration for the Huggy agent within the ML-Agents framework, intended to be saved as `Huggy.yaml` in the `/content/ml-agents/config/ppo` directory. It defines the `ppo` trainer type and sets various hyperparameters like `batch_size`, `learning_rate`, and PPO-specific parameters (`beta`, `epsilon`, `lambd`). It also configures the neural network architecture (`hidden_units`, `num_layers`), reward signal processing (`gamma`), and training parameters like `max_steps` and `checkpoint_interval`.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unitbonus1/train.mdx#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nbehaviors:\n  Huggy:\n    trainer_type: ppo\n    hyperparameters:\n      batch_size: 2048\n      buffer_size: 20480\n      learning_rate: 0.0003\n      beta: 0.005\n      epsilon: 0.2\n      lambd: 0.95\n      num_epoch: 3\n      learning_rate_schedule: linear\n    network_settings:\n      normalize: true\n      hidden_units: 512\n      num_layers: 3\n      vis_encode_type: simple\n    reward_signals:\n      extrinsic:\n        gamma: 0.995\n        strength: 1.0\n    checkpoint_interval: 200000\n    keep_checkpoints: 15\n    max_steps: 2e6\n    time_horizon: 1000\n    summary_freq: 50000\n```\n\n----------------------------------------\n\nTITLE: Logging into Hugging Face Hub - Python\nDESCRIPTION: This snippet uses the `notebook_login()` function from the `huggingface_hub` library to authenticate a user with their Hugging Face account. This allows the user to upload models and interact with the Hub. It's essential for sharing trained models. The command also sets up git credentials to store the authentication token for future usage.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit3/hands-on.mdx#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\nnotebook_login()\n!git config --global credential.helper store\n```\n\n----------------------------------------\n\nTITLE: Checking Python Version for Compatibility in Bash\nDESCRIPTION: Displays the current Python version in the Colab environment to check compatibility with ML-Agents requirements.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus-unit1.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Colab's Current Python Version (Incompatible with ML-Agents)\n!python --version\n```\n\n----------------------------------------\n\nTITLE: Initializing Hugging Face Hub and Utility Imports in Python\nDESCRIPTION: This snippet imports essential modules and libraries used for interacting with the Hugging Face Hub, handling file paths, dates, JSON processing, image/video I/O, and system operations. Dependencies include huggingface_hub, pathlib, datetime, json, imageio, tempfile, and os.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import HfApi, snapshot_download\nfrom huggingface_hub.repocard import metadata_eval_result, metadata_save\n\nfrom pathlib import Path\nimport datetime\nimport json\nimport imageio\n\nimport tempfile\n\nimport os\n```\n\n----------------------------------------\n\nTITLE: Publishing Model to Hub - Python\nDESCRIPTION: This snippet demonstrates how to use the `package_to_hub` function from the `huggingface_sb3` library to upload a trained reinforcement learning model to the Hugging Face Hub. It includes parameters for the model, environment, and repository, and utilizes a commit message for version control.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit1/unit1.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.env_util import make_vec_env\n\nfrom huggingface_sb3 import package_to_hub\n\n## TODO: Define a repo_id\n## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\nrepo_id =\n\n# TODO: Define the name of the environment\nenv_id =\n\n# Create the evaluation env and set the render_mode=\"rgb_array\"\neval_env = DummyVecEnv([lambda: Monitor(gym.make(env_id, render_mode=\"rgb_array\"))])\n\n\n# TODO: Define the model architecture we used\nmodel_architecture = \"\"\n\n## TODO: Define the commit message\ncommit_message = \"\"\n\n# method save, evaluate, generate a model card and record a replay video of your agent before pushing the repo to the hub\npackage_to_hub(model=model, # Our trained model\n               model_name=model_name, # The name of our trained model\n               model_architecture=model_architecture, # The model architecture we used: in our case PPO\n               env_id=env_id, # Name of the environment\n               eval_env=eval_env, # Evaluation Environment\n               repo_id=repo_id, # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n               commit_message=commit_message)\n```\n\n----------------------------------------\n\nTITLE: Parsing Configuration for ViZDoom RL Training\nDESCRIPTION: This snippet parses command-line arguments to generate a configuration object for training or evaluation, including Doom-specific environment parameters and default overrides. It supports optional arguments and returns the final configuration.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-sf.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef parse_vizdoom_cfg(argv=None, evaluation=False):\n    parser, _ = parse_sf_args(argv=argv, evaluation=evaluation)\n    add_doom_env_args(parser)\n    doom_override_defaults(parser)\n    final_cfg = parse_full_cfg(parser, argv)\n    return final_cfg\n```\n\n----------------------------------------\n\nTITLE: Implementing Physics Process and Reset Logic for AI Controller in Godot (GDScript)\nDESCRIPTION: Overrides the _physics_process method to handle step counting and reset logic based on a timeout or game conditions such as lever not pulled. It ensures the game resets by calling player.game_over() under specified conditions. The reset() method extends the parent reset logic by additionally resetting the lever pull step counter. This snippet enables controlled episode management during training and gameplay. It depends on class variables like n_steps, reset_after, steps_without_lever_pulled, and player object methods.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unitbonus5/getting-started.mdx#_snippet_1\n\nLANGUAGE: GDScript\nCODE:\n```\nfunc _physics_process(delta: float) -> void:\n\t# Reset on timeout, this is implemented in parent class to set needs_reset to true,\n\t# we are re-implementing here to call player.game_over() that handles the game reset.\n\tn_steps += 1\n\tif n_steps > reset_after:\n\t\tplayer.game_over()\n\n\t# In training or onnx inference modes, this method will be called by sync node with actions provided,\n\t# For expert demo recording mode, it will be called without any actions (as we set the actions based on human input),\n\t# For human control mode the method will not be called, so we call it here without any actions provided.\n\tif control_mode == ControlModes.HUMAN:\n\t\tset_action()\n\n\t# Reset the game faster if the lever is not pulled.\n\tsteps_without_lever_pulled += 1\n\tif steps_without_lever_pulled > 200 and (not player._is_lever_pulled):\n\t\tplayer.game_over()\n\nfunc reset():\n\tsuper.reset()\n\tsteps_without_lever_pulled = 0\n```\n\n----------------------------------------\n\nTITLE: Embedding and Displaying Evaluation Video in IPython\nDESCRIPTION: This code loads an evaluation video file, encodes it in Base64, and embeds it into an IPython HTML widget for inline visualization in notebooks.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-sf.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom base64 import b64encode\nfrom IPython.display import HTML\n\nmp4 = open(\"/content/train_dir/default_experiment/replay.mp4\", \"rb\").read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\n    \"\"\"\n<video width=640 controls>\n      <source src=\"%s\" type=\"video/mp4\">\n</video>\n\"\"\" % data_url\n)\n```\n\n----------------------------------------\n\nTITLE: Install Shimmy for Gymnasium/Gym Compatibility\nDESCRIPTION: This shell command installs the `shimmy` library. Shimmy is used as an API conversion tool to ensure compatibility when loading models trained with older versions of Gym into environments using Gymnasium.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit1/hands-on.mdx#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n!pip install shimmy\n```\n\n----------------------------------------\n\nTITLE: Creating Conda Environment for RL\nDESCRIPTION: This snippet demonstrates the creation of a new Conda environment named 'rl' with Python version 3.10.12. It then activates the environment to allow the user to install the necessary packages for the project.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create --name rl python=3.10.12\nconda activate rl\n```\n\n----------------------------------------\n\nTITLE: Quiz Question 1: Bias-Variance in RL (JSX)\nDESCRIPTION: Defines the first question of the quiz, focusing on the bias-variance tradeoff specifically within the context of Reinforcement Learning. It provides multiple choice options with explanations for both correct and incorrect answers. The structure is intended to be parsed and rendered by a frontend component.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit6/quiz.mdx#_snippet_0\n\nLANGUAGE: JSX\nCODE:\n```\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"The bias-variance tradeoff reflects how my model is able to generalize the knowledge to previously tagged data we give to the model during training time.\",\n\t\t\texplain: \"This is the traditional bias-variance tradeoff in Machine Learning. In our specific case of Reinforcement Learning, we don't have previously tagged data, but only a reward signal.\",\n      \t\t\tcorrect: false,\n\t\t},\n   \t\t{\n\t\t\ttext: \"The bias-variance tradeoff reflects how well the reinforcement signal reflects the true reward the agent should get from the enviromment\",\n\t\t\texplain: \"\",\n      \t\t\tcorrect: true,\n\t\t},\t\t\n\t]}\n/>\n```\n\n----------------------------------------\n\nTITLE: Citing the Hugging Face Deep RL Course with BibTeX\nDESCRIPTION: This snippet provides a BibTeX citation entry for referencing the Hugging Face Deep Reinforcement Learning Class repository in academic publications. No additional dependencies are required beyond standard BibTeX usage. As input, copy the entry into your .bib file; the output is a properly formatted citation in your bibliography. Limitations include the lack of a DOI and that citation fields must be updated if the repository moves or updates.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/README.md#_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{deep-rl-course,\n  author = {Simonini, Thomas and Sanseviero, Omar},\n  title = {The Hugging Face Deep Reinforcement Learning Class},\n  year = {2023},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/huggingface/deep-rl-class}},\n}\n```\n\n----------------------------------------\n\nTITLE: Instantiating A2C Model\nDESCRIPTION: This code initializes the A2C model.  It uses the `MultiInputPolicy` because the observation space is a dictionary. It also sets the environment and the verbosity level.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nmodel = A2C(policy = \"MultiInputPolicy\",\n            env = env,\n            verbose=1)\n```\n\n----------------------------------------\n\nTITLE: Installing ML-Agents and ML-Agents-Envs Packages in Editable Mode - Python\nDESCRIPTION: This snippet uses %%capture to suppress output, changes the working directory to the cloned ml-agents repo, and installs both ml-agents-envs and ml-agents in editable mode using pip3. The '-e' flag ensures development installs from local source. These steps are prerequisites for running RL training experiments with ML-Agents.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus_unit1.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n# Go inside the repository and install the package (can take 3min)\n%cd ml-agents\n!pip3 install -e ./ml-agents-envs\n!pip3 install -e ./ml-agents\n```\n\n----------------------------------------\n\nTITLE: Setting Up Virtual Environment\nDESCRIPTION: This code block demonstrates how to create a virtual environment using `virtualenv` to manage Python dependencies, download and install Miniconda, and create and activate a conda environment. It sets Python and conda paths for proper execution of ML-Agents.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit5/hands-on.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Install virtualenv and create a virtual environment\n!pip install virtualenv\n!virtualenv myenv\n\n# Download and install Miniconda\n!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n!chmod +x Miniconda3-latest-Linux-x86_64.sh\n!./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n\n# Activate Miniconda and install Python ver 3.10.12\n!source /usr/local/bin/activate\n!conda install -q -y --prefix /usr/local python=3.10.12 ujson  # Specify the version here\n\n# Set environment variables for Python and conda paths\n!export PYTHONPATH=/usr/local/lib/python3.10/site-packages/\n!export CONDA_PREFIX=/usr/local/envs/myenv\n```\n\n----------------------------------------\n\nTITLE: Getting the State Space Size for Taxi-v3 (Python)\nDESCRIPTION: Retrieves the total number of possible states in the initialized Taxi-v3 environment using the `observation_space.n` attribute and prints the result. This value is crucial for defining the dimensions of the Q-table.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_27\n\nLANGUAGE: Python\nCODE:\n```\nstate_space = env.observation_space.n\nprint(\"There are \", state_space, \" possible states\")\n```\n\n----------------------------------------\n\nTITLE: Adding Argument for Hugging Face Repo ID\nDESCRIPTION: This code adds a command-line argument to specify the ID of the model repository on the Hugging Face Hub. It allows users to specify the repository name where the trained model will be uploaded, defaulting to a specific repository if no other is provided.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nparser.add_argument(\"--repo-id\", type=str, default=\"ThomasSimonini/ppo-CartPole-v1\", help=\"id of the model repository from the Hugging Face Hub {username/repo_name}\")\n```\n\n----------------------------------------\n\nTITLE: Evaluating Trained DQN Agent Shell\nDESCRIPTION: Executes the RL Baselines3 Zoo evaluation script (`rl_zoo3.enjoy`) for the trained DQN agent on the SpaceInvadersNoFrameskip-v4 environment. It evaluates the agent's performance for a specified number of timesteps (5000 in this case) without rendering the environment visually, using the model saved in the `logs/` folder. Requires a trained model to be present.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit3/unit3.ipynb#_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\n!python -m rl_zoo3.enjoy  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --no-render  --n-timesteps 5000  --folder logs/\n```\n\n----------------------------------------\n\nTITLE: Installing grpcio\nDESCRIPTION: This snippet provides a workaround to fix potential installation issues on Apple Silicon systems, in particular the ONNX wheel build failures by installing the grpcio package.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nconda install grpcio\n```\n\n----------------------------------------\n\nTITLE: Uploading Agent to HF Hub with Configuration\nDESCRIPTION: This code runs the 'enjoy' visualization while pushing the trained model to the specified Hugging Face repository. It configures the environment with parameters to save videos, prevent rendering, and specify the repository name.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-sf.mdx#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nhf_username = \"ThomasSimonini\"  # insert your HuggingFace username here\n\ncfg = parse_vizdoom_cfg(\n    argv=[\n        f\"--env={env}\",\n        \"--num_workers=1\",\n        \"--save_video\",\n        \"--no_render\",\n        \"--max_num_episodes=10\",\n        \"--max_num_frames=100000\",\n        \"--push_to_hub\",\n        f\"--hf_repository={hf_username}/rl_course_vizdoom_health_gathering_supreme\",\n    ],\n    evaluation=True\n)\nstatus = enjoy(cfg)\n```\n\n----------------------------------------\n\nTITLE: Installing ML-Agents Package\nDESCRIPTION: This snippet clones the ML-Agents repository and installs the required packages. The -e flag is used to install the packages in editable mode. Additionally, it shows how to install grpcio to fix install issues in Apple Silicon and git-lfs for managing large files. It downloads the executables to run in the ML-Agents environment.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/Unity-Technologies/ml-agents\n```\n\n----------------------------------------\n\nTITLE: Cloning ML-Agents Repository (Bash)\nDESCRIPTION: Clones the Unity ML-Agents GitHub repository using `git clone`. The `--depth 1` option performs a shallow clone, downloading only the latest commit history to save time and space. This step is necessary to obtain the ML-Agents library code for setting up the training environment.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unitbonus1/train.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Clone the repository (can take 3min)\ngit clone --depth 1 https://github.com/Unity-Technologies/ml-agents\n```\n\n----------------------------------------\n\nTITLE: Save Model Card to Repository\nDESCRIPTION: This function saves the generated model card to the README.md file within the repository. It takes the local path of the repository, the generated model card content, and the metadata as input. It reads any existing README.md content and then writes the new content including the generated model card and metadata.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef _save_model_card(local_path, generated_model_card, metadata):\n    \"\"\"Saves a model card for the repository.\n    :param local_path: repository directory\n    :param generated_model_card: model card generated by _generate_model_card()\n    :param metadata: metadata\n    \"\"\"\n    readme_path = local_path / \"README.md\"\n    readme = \"\"\n    if readme_path.exists():\n        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n            readme = f.read()\n    else:\n        readme = generated_model_card\n\n    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(readme)\n\n    # Save our metrics to Readme metadata\n    metadata_save(readme_path, metadata)\n```\n\n----------------------------------------\n\nTITLE: Downloading and Saving a Model from Hub - Bash\nDESCRIPTION: This command downloads a pre-trained reinforcement learning model from the Hugging Face Hub using the `rl_zoo3.load_from_hub` command. It specifies the algorithm, environment, organization, and folder where to save the model. The download places the model within a new directory. This is useful to use pre-trained agents.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit3/hands-on.mdx#_snippet_9\n\nLANGUAGE: Bash\nCODE:\n```\npython -m rl_zoo3.load_from_hub --algo dqn --env BeamRiderNoFrameskip-v4 -orga sb3 -f rl_trained/\n```\n\n----------------------------------------\n\nTITLE: Train Agent with ML-Agents\nDESCRIPTION: This command uses the `mlagents-learn` tool to train the Huggy agent using the specified configuration file and environment executable. The `--run-id` parameter sets a unique identifier for the training run, and the `--no-graphics` flag disables visualization during training. It leverages the ML-Agents framework to facilitate reinforcement learning.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus_unit1.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n!mlagents-learn ./config/ppo/Huggy.yaml --env=./trained-envs-executables/linux/Huggy/Huggy --run-id=\"Huggy2\" --no-graphics\n```\n\n----------------------------------------\n\nTITLE: Install Dependencies - Python\nDESCRIPTION: This code snippet installs necessary Python packages required for the Q-Learning project. It uses pip to install packages from a requirements file hosted on GitHub.  Dependencies include libraries such as Gymnasium, Pygame, and NumPy.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt\n```\n\n----------------------------------------\n\nTITLE: Pushing Agent to Hugging Face Hub\nDESCRIPTION: This bash command pushes the trained SoccerTwos agent to the Hugging Face Hub using `mlagents-push-to-hf`. It requires specifying the run ID, local directory where the agent is saved, the repository ID on Hugging Face, and a commit message.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nmlagents-push-to-hf  --run-id=\"SoccerTwos\" --local-dir=\"./results/SoccerTwos\" --repo-id=\"ThomasSimonini/poca-SoccerTwos\" --commit-message=\"First Push\"`\n```\n\n----------------------------------------\n\nTITLE: Clone ML-Agents Repository (Shell)\nDESCRIPTION: Clones the official Unity Technologies ML-Agents GitHub repository into the current working directory in the Google Colab environment. The `--depth 1` flag performs a shallow clone, downloading only the latest commit, which speeds up the process and reduces storage requirements. This repository contains the necessary source code and scripts for installing and running ML-Agents.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit5/unit5.ipynb#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n%%capture\n# Clone the repository (can take 3min)\n!git clone --depth 1 https://github.com/Unity-Technologies/ml-agents\n```\n\n----------------------------------------\n\nTITLE: Cloning the ML-Agents Repository - Python\nDESCRIPTION: This code uses the %%capture magic to suppress output while cloning the Unity ML-Agents GitHub repository with git. The '--depth 1' flag ensures a shallow clone for faster download. No input parameters are required. This set-up step is necessary before installing and using ML-Agents in the notebook.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus_unit1.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n# Clone the repository (can take 3min)\n!git clone --depth 1 https://github.com/Unity-Technologies/ml-agents\n```\n\n----------------------------------------\n\nTITLE: Generating Model Card Metadata - Python\nDESCRIPTION: This function defines metadata for a model card, including tags related to the environment, algorithm (PPO), and course information. It also incorporates evaluation metrics such as mean and standard deviation of the reward. The result is a dictionary containing the model's metadata.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef generate_metadata(model_name, env_id, mean_reward, std_reward):\n    \"\"\"\n    Define the tags for the model card\n    :param model_name: name of the model\n    :param env_id: name of the environment\n    :mean_reward: mean reward of the agent\n    :std_reward: standard deviation of the mean reward of the agent\n    \"\"\"\n    metadata = {}\n    metadata[\"tags\"] = [\n        env_id,\n        \"ppo\",\n        \"deep-reinforcement-learning\",\n        \"reinforcement-learning\",\n        \"custom-implementation\",\n        \"deep-rl-course\",\n    ]\n\n    # Add metrics\n    eval = metadata_eval_result(\n        model_pretty_name=model_name,\n        task_pretty_name=\"reinforcement-learning\",\n        task_id=\"reinforcement-learning\",\n        metrics_pretty_name=\"mean_reward\",\n        metrics_id=\"mean_reward\",\n        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n        dataset_pretty_name=env_id,\n        dataset_id=env_id,\n    )\n\n    # Merges both dictionaries\n    metadata = {**metadata, **eval}\n\n    return metadata\n```\n\n----------------------------------------\n\nTITLE: Verifying Python Version in Virtual Environment (Bash)\nDESCRIPTION: Re-runs the `python --version` command within the Colab notebook (indicated by `!`) after attempting to set up the virtual environment. This step confirms whether the environment activation and Python version installation were successful, ensuring the correct Python version (expected to be 3.10.12) is now active for subsequent ML-Agents installation.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unitbonus1/train.mdx#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Python Version in New Virtual Environment (Compatible with ML-Agents)\n!python --version\n```\n\n----------------------------------------\n\nTITLE: Setting up Virtual Environment with Compatible Python Version in Bash\nDESCRIPTION: Creates a virtual environment with Miniconda and installs a Python version compatible with ML-Agents requirements.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus-unit1.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Install virtualenv and create a virtual environment\n!pip install virtualenv\n!virtualenv myenv\n\n# Download and install Miniconda\n!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n!chmod +x Miniconda3-latest-Linux-x86_64.sh\n!./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n\n# Activate Miniconda and install Python ver 3.10.12\n!source /usr/local/bin/activate\n!conda install -q -y --prefix /usr/local python=3.10.12 ujson  # Specify the version here\n\n# Set environment variables for Python and conda paths\n!export PYTHONPATH=/usr/local/lib/python3.10/site-packages/\n!export CONDA_PREFIX=/usr/local/envs/myenv\n```\n\n----------------------------------------\n\nTITLE: Add Log Directory to Repository by Copying Files in Python\nDESCRIPTION: Copies a log directory into the repository's logs folder, removing existing logs if present. Used for including training logs or visualizations in the project directory.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\ndef _add_logdir(local_path: Path, logdir: Path):\n  \"\"\"Adds a logdir to the repository.\n  :param local_path: repository directory\n  :param logdir: logdir directory\n  \"\"\"\n  if logdir.exists() and logdir.is_dir():\n    # Add the logdir to the repository under new dir called logs\n    repo_logdir = local_path / \"logs\"\n    \n    # Delete current logs if they exist\n    if repo_logdir.exists():\n      shutil.rmtree(repo_logdir)\n\n    # Copy logdir into repo logdir\n    shutil.copytree(logdir, repo_logdir)\n```\n\n----------------------------------------\n\nTITLE: Checking Python Version\nDESCRIPTION: This snippet checks the currently active Python version within the Colab environment using the `!python --version` command. This is important to ensure the correct version for ML-Agents compatibility.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit5/hands-on.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Colab's Current Python Version (Incompatible with ML-Agents)\n!python --version\n```\n\n----------------------------------------\n\nTITLE: Initializing Q-Table with Zeros (Python)\nDESCRIPTION: Defines a function `initialize_q_table` that creates a NumPy array representing the Q-table. The table has dimensions based on the number of states and actions and is initialized with all values set to 0, preparing it for the Q-learning updates. Requires the `numpy` library.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros\ndef initialize_q_table(state_space, action_space):\n  Qtable = np.zeros((state_space, action_space))\n  return Qtable\n```\n\n----------------------------------------\n\nTITLE: Checking Current Python Version (Bash)\nDESCRIPTION: Executes the `python --version` command within the Colab notebook (indicated by `!`) to display the currently active Python version. This check is performed to determine if the environment's Python version meets the compatibility requirements of the ML-Agents library.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unitbonus1/train.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Colab's Current Python Version (Incompatible with ML-Agents)\n!python --version\n```\n\n----------------------------------------\n\nTITLE: Verify New Python Version (Shell)\nDESCRIPTION: Displays the Python version active in the environment *after* the steps to set up a compatible virtual environment have been executed. This confirms that the intended Python version (3.10.12) is now the default interpreter, verifying the successful completion of the environment setup process for ML-Agents compatibility.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit5/unit5.ipynb#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\n# Python Version in New Virtual Environment (Compatible with ML-Agents)\n!python --version\n```\n\n----------------------------------------\n\nTITLE: Authenticating with Hugging Face Hub in Notebooks (Python)\nDESCRIPTION: Uses the `notebook_login` function from the `huggingface_hub` library to authenticate with Hugging Face Hub within a Jupyter Notebook or Google Colab environment. This prompts the user to enter an API token with write permissions, which is necessary for pushing models.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb#_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\nfrom huggingface_hub import notebook_login\nnotebook_login()\n```\n\n----------------------------------------\n\nTITLE: Embedding YouTube Video in Notebook (Python)\nDESCRIPTION: Uses IPython's HTML display functionality to embed a YouTube video directly within a Jupyter Notebook or similar environment. This snippet specifically embeds Costa Huang's PPO tutorial video.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom IPython.display import HTML\n\nHTML(\n    '<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/MEt6rrxH8W4\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>'\n)\n```\n\n----------------------------------------\n\nTITLE: Download SnowballTarget Executable (Shell)\nDESCRIPTION: Downloads the compressed executable file for the SnowballTarget Unity environment from a specified GitHub URL. The `-O` flag directs the output to a specific file path, saving the zip file directly into the `./training-envs-executables/linux/` directory created previously.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit5/unit5.ipynb#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\n!wget \"https://github.com/huggingface/Snowball-Target/raw/main/SnowballTarget.zip\" -O ./training-envs-executables/linux/SnowballTarget.zip\n```\n\n----------------------------------------\n\nTITLE: Installing System Dependencies for Virtual Display (Bash)\nDESCRIPTION: Installs system packages required for creating a virtual display in Google Colab, which is necessary for rendering and recording environment replays. It updates the package list and installs `python3-opengl`, `ffmpeg`, `xvfb`, and the Python library `pyvirtualdisplay`.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get update\nsudo apt-get install -y python3-opengl\napt install ffmpeg xvfb\npip3 install pyvirtualdisplay\n```\n\n----------------------------------------\n\nTITLE: Install virtual screen dependencies\nDESCRIPTION: This bash script installs dependencies required for creating a virtual screen, including `python3-opengl`, `ffmpeg`, and `xvfb`. These are needed to render the environment and record replay videos in a headless environment like Google Colab.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit1/hands-on.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get update\napt install python3-opengl\napt install ffmpeg\napt install xvfb\npip3 install pyvirtualdisplay\n```\n\n----------------------------------------\n\nTITLE: Creating Directory for Executables\nDESCRIPTION: This command creates the directories to store the executable environments.  It sets up the file structure to hold the downloaded executable files for training within the Colab environment.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit5/hands-on.mdx#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Here, we create training-envs-executables and linux\nmkdir ./training-envs-executables\nmkdir ./training-envs-executables/linux\n```\n\n----------------------------------------\n\nTITLE: Push Agent with Parameter Placeholders - Python\nDESCRIPTION: This generic shell command (in Python notebook context) presents the required parameters to push a trained agent to the Hugging Face Hub. Users must fill in their experiment's run-id, local directory, Hugging Face repo ID, and commit message. Intended for copy-paste modification, with no functional output until parameters are set. Dependencies are as above: MLAgents, huggingface_hub, and prior authentication. Used for both SnowballTarget and Pyramids agents.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit5/unit5.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n!mlagents-push-to-hf  --run-id= # Add your run id  --local-dir= # Your local dir  --repo-id= # Your repo id  --commit-message= # Your commit message\n```\n\n----------------------------------------\n\nTITLE: Creating Directories for Environment Executables - Python\nDESCRIPTION: This pair of shell commands creates the directory structure './trained-envs-executables/linux/' to contain the Huggy environment executables. No dependencies are required. The snippet lays the groundwork for managing environment assets before downloading.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus_unit1.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n!mkdir ./trained-envs-executables\n!mkdir ./trained-envs-executables/linux\n```\n\n----------------------------------------\n\nTITLE: Set Executable Permissions for Unity Environment - Python\nDESCRIPTION: This shell command (for Python notebooks) modifies permissions on the Unity environment executable needed for training agents, making it executable by all users. It recursively applies mode 755, ensuring both file system accessibility and execution. The snippet assumes the file path matches that of the extracted executable. No output is produced on success.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit5/unit5.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n!chmod -R 755 ./training-envs-executables/linux/Pyramids/Pyramids\n```\n\n----------------------------------------\n\nTITLE: Set SnowballTarget Executable Permissions (Shell)\nDESCRIPTION: Recursively sets executable permissions for the extracted SnowballTarget environment files located in `./training-envs-executables/linux/SnowballTarget`. The `chmod -R 755` command grants read, write, and execute permissions to the file owner and read and execute permissions to the group and others, ensuring the environment binary can be executed by the system.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit5/unit5.ipynb#_snippet_8\n\nLANGUAGE: Shell\nCODE:\n```\n!chmod -R 755 ./training-envs-executables/linux/SnowballTarget\n```\n\n----------------------------------------\n\nTITLE: Setting Executable Permissions for the Huggy Environment - Python\nDESCRIPTION: This shell command uses chmod recursively to set executable permissions on the Huggy environment binary in Linux. This is critical to allow Colab to launch and use the Unity binary for RL training. No parameters other than the target path are required.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus_unit1.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n!chmod -R 755 ./trained-envs-executables/linux/Huggy\n```\n\n----------------------------------------\n\nTITLE: Initializing Virtual Display (Python)\nDESCRIPTION: Imports the `Display` class from the `pyvirtualdisplay` library and starts a virtual display instance. This allows Gym environments to be rendered visually, enabling video recording even without a physical screen.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()\n```\n\n----------------------------------------\n\nTITLE: Checking Python Version After Virtual Environment Setup - Python\nDESCRIPTION: This code snippet re-checks the active Python version after the new virtual environment and Miniconda-based setup to confirm successful installation and switching to a compatible version for ML-Agents. The command outputs the new Python version string, ensuring environment consistency.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus_unit1.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Python Version in New Virtual Environment (Compatible with ML-Agents)\n!python --version\n```\n\n----------------------------------------\n\nTITLE: Downloading Doom Deathmatch Model from Hub (Shell)\nDESCRIPTION: Downloads a pre-trained agent for the 'Doom Deathmatch' scenario from a specified Hugging Face Hub repository using the Sample Factory `load_from_hub` utility.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part2.ipynb#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\n# Download the agent from the hub\n!python -m sample_factory.huggingface.load_from_hub -r edbeeching/doom_deathmatch_bots_2222 -d ./train_dir\n```\n\n----------------------------------------\n\nTITLE: Training Command (Windows)\nDESCRIPTION: This bash command starts the training process for the SoccerTwos environment on Windows. It specifies the path to the configuration file, the environment executable, the run ID, and disables graphics.  The executable path needs to be adjusted based on where the SoccerTwos.exe is located.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmlagents-learn ./config/poca/SoccerTwos.yaml --env=./training-envs-executables/SoccerTwos.exe --run-id=\"SoccerTwos\" --no-graphics\n```\n\n----------------------------------------\n\nTITLE: Installing ViZDoom Dependencies on Linux\nDESCRIPTION: Bash commands to install all necessary system dependencies for building and running ViZDoom on a Linux environment, including graphics libraries, audio processing tools, and development packages.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-sf.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\napt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev \nnasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev \nlibopenal-dev timidity libwildmidi-dev unzip ffmpeg\n\n# Boost libraries\napt-get install libboost-all-dev\n\n# Lua binding dependencies\napt-get install liblua5.1-dev\n```\n\n----------------------------------------\n\nTITLE: Printing Device Info for Hardware Acceleration\nDESCRIPTION: Outputs the selected device (GPU or CPU) to confirm hardware acceleration setup, ensuring the training process utilizes available GPU resources if present.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nprint(device)\n```\n\n----------------------------------------\n\nTITLE: Install Dependencies (virtual display)\nDESCRIPTION: This bash command installs the necessary dependencies for running a virtual display, which is required for rendering environments in headless environments like Google Colab. The dependencies include python-opengl, ffmpeg, and xvfb.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit6/hands-on.mdx#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n%%capture\n!apt install python-opengl\n!apt install ffmpeg\n!apt install xvfb\n!pip3 install pyvirtualdisplay\n```\n\n----------------------------------------\n\nTITLE: Inspecting Gymnasium Action Space in Python\nDESCRIPTION: Demonstrates how to inspect the action space of a Gymnasium environment. It prints the shape of the action space (number of discrete actions) and a sample action, illustrating the possible actions the agent can take.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit1/hands-on.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprint(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"Action Space Shape\", env.action_space.n)\nprint(\"Action Space Sample\", env.action_space.sample())  # Take a random action\n```\n\n----------------------------------------\n\nTITLE: Creating PandaReachDense Environment\nDESCRIPTION: This creates and initializes the PandaReachDense-v3 environment from Panda-Gym. It then accesses the observation and action space shapes, setting the stage for defining the model and interaction.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nenv_id = \"PandaReachDense-v3\"\n\n# Create the env\nenv = gym.make(env_id)\n\n# Get the state space and action space\ns_size = env.observation_space.shape\na_size = env.action_space\n```\n\n----------------------------------------\n\nTITLE: Creating Directories for Environment Executable (Bash)\nDESCRIPTION: Creates the necessary directory structure (`./trained-envs-executables/linux/`) using the `mkdir` command. This structure is required by ML-Agents to locate and load the pre-built Unity environment executable for training or inference.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unitbonus1/train.mdx#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmkdir ./trained-envs-executables\nmkdir ./trained-envs-executables/linux\n```\n\n----------------------------------------\n\nTITLE: Printing Action Space Information\nDESCRIPTION: This code snippet prints the action space size and a sample action from the CartPole-v1 environment. It retrieves and displays the action space size and gives a random sample from possible actions.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprint(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"The Action Space is: \", a_size)\nprint(\"Action Space Sample\", env.action_space.sample())  # Take a random action\n```\n\n----------------------------------------\n\nTITLE: Training an A2C model for PandaPickAndPlace-v3 in Python\nDESCRIPTION: Code for creating, normalizing, and training an A2C reinforcement learning model on the PandaPickAndPlace-v3 environment using Stable Baselines3.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit6/hands-on.mdx#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# 1 - 2\nenv_id = \"PandaPickAndPlace-v3\"\nenv = make_vec_env(env_id, n_envs=4)\n\n# 3\nenv = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\n\n# 4\nmodel = A2C(policy = \"MultiInputPolicy\",\n            env = env,\n            verbose=1)\n# 5\nmodel.learn(1_000_000)\n```\n\n----------------------------------------\n\nTITLE: Installing system dependencies for RL Baselines3 Zoo\nDESCRIPTION: Install required system packages such as swig, cmake, and ffmpeg to enable compilation and multimedia processing features needed for training and rendering Atari game environments.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit3/hands-on.mdx#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\napt-get install swig cmake ffmpeg\n```\n\n----------------------------------------\n\nTITLE: Unzip Unity Environment Executable - Python\nDESCRIPTION: This notebook cell uses the %%capture magic to suppress output, then unzips the previously downloaded Unity environment executable (Pyramids.zip) into a specific directory structure for MLAgents. Requires the zip file be present at the specified path. Assumes 'unzip' utility is available in the OS. Output is the unzipped environment executable ready for training.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit5/unit5.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n!unzip -d ./training-envs-executables/linux/ ./training-envs-executables/linux/Pyramids.zip\n```\n\n----------------------------------------\n\nTITLE: Cloning ML-Agents Repository\nDESCRIPTION: This bash snippet clones the ML-Agents repository from GitHub.  It uses the `--depth 1` option for a shallow clone, which fetches only the latest commit to save time and storage space.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit5/hands-on.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone --depth 1 https://github.com/Unity-Technologies/ml-agents\n```\n\n----------------------------------------\n\nTITLE: Hugging Face CLI Login\nDESCRIPTION: This bash command is used to log in to the Hugging Face Hub using the `huggingface-cli` tool. It requires an authentication token from the Hugging Face website, which grants write access to the user's repositories.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Create PandaReachDense-v3 environment\nDESCRIPTION: This python script creates the PandaReachDense-v3 environment using Gymnasium. It retrieves and prints the state space and action space of the environment to better understand the environment.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit6/hands-on.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nenv_id = \"PandaReachDense-v3\"\n\n# Create the env\nenv = gym.make(env_id)\n\n# Get the state space and action space\ns_size = env.observation_space.shape\na_size = env.action_space\n```\n\n----------------------------------------\n\nTITLE: Setting Executable Permissions\nDESCRIPTION: This command sets executable permissions on the extracted SnowballTarget executable using `chmod -R 755`. It ensures the file can be executed by the user and group.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit5/hands-on.mdx#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nchmod -R 755 ./training-envs-executables/linux/SnowballTarget\n```\n\n----------------------------------------\n\nTITLE: Downloading the SnowballTarget Executable\nDESCRIPTION: This snippet downloads the executable for the SnowballTarget environment from a specified GitHub repository using `wget`. The downloaded file is saved into the created directories.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit5/hands-on.mdx#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nwget \"https://github.com/huggingface/Snowball-Target/raw/main/SnowballTarget.zip\" -O ./training-envs-executables/linux/SnowballTarget.zip\n```\n\n----------------------------------------\n\nTITLE: Displaying Video of Trained Huggy Agent in Python\nDESCRIPTION: A HTML video element embedded in a Python notebook cell to demonstrate the final trained agent in action.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus-unit1.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%html\n<video controls autoplay><source src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/huggy.mp4\" type=\"video/mp4\"></video>\n```\n\n----------------------------------------\n\nTITLE: Setting Execute Permissions for Environment (Bash)\nDESCRIPTION: Modifies the file permissions of the unzipped Huggy environment directory and its contents using `chmod -R 755`. This grants read and execute permissions to all users and write permissions only to the owner, ensuring the environment executable can be run by the ML-Agents training process.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unitbonus1/train.mdx#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nchmod -R 755 ./trained-envs-executables/linux/Huggy\n```\n\n----------------------------------------\n\nTITLE: Installing ViZDoom System Dependencies on Linux Using Bash\nDESCRIPTION: This bash snippet is designed to install all necessary Linux system dependencies required to build and run the ViZDoom environment, which enables reinforcement learning agents to interact with the Doom game. The installation includes build tools, multimedia libraries, Boost, Lua bindings, and other game-related dependencies. The commands must be run with root or sudo privileges and are intended for Debian-based distributions. Inputs include the target system's package manager and availability of internet access. Outputs are installed libraries and development headers necessary for compiling ViZDoom.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part2.ipynb#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\napt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev \\\nnasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev \\\nlibopenal-dev timidity libwildmidi-dev unzip ffmpeg\n\n# Boost libraries\napt-get install libboost-all-dev\n\n# Lua binding dependencies\napt-get install liblua5.1-dev\n```\n\n----------------------------------------\n\nTITLE: Unzipping the Huggy Environment - Python\nDESCRIPTION: This snippet uses the %%capture magic to suppress unzip output while extracting the Huggy.zip file to './trained-envs-executables/linux/'. It ensures the environment binary is available for ML-Agents to interface with. All paths are relative from the project root, and no parameters are required.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus_unit1.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n!unzip -d ./trained-envs-executables/linux/ ./trained-envs-executables/linux/Huggy.zip\n```\n\n----------------------------------------\n\nTITLE: Extending AI Controller in GDScript\nDESCRIPTION: A basic template for extending the AIController3D class with empty ready and process methods. This is the starting point for implementing the required AI controller functionality.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unitbonus3/godotrl.mdx#_snippet_1\n\nLANGUAGE: GDScript\nCODE:\n```\nextends AIController3D\n\n# Called when the node enters the scene tree for the first time.\nfunc _ready():\n\tpass # Replace with function body.\n\n# Called every frame. 'delta' is the elapsed time since the previous frame.\nfunc _process(delta):\n\tpass\n```\n\n----------------------------------------\n\nTITLE: Downloading Huggy Environment Zip File in Bash\nDESCRIPTION: Downloads the Huggy environment zip file from GitHub repository to the local directory structure.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus-unit1.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n!wget \"https://github.com/huggingface/Huggy/raw/main/Huggy.zip\" -O ./trained-envs-executables/linux/Huggy.zip\n```\n\n----------------------------------------\n\nTITLE: Downloading Huggy Environment Executable (Bash)\nDESCRIPTION: Downloads the zipped Huggy environment executable file from the specified GitHub URL using `wget`. The `-O` flag directs `wget` to save the downloaded file as `Huggy.zip` within the `./trained-envs-executables/linux/` directory.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unitbonus1/train.mdx#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nwget \"https://github.com/huggingface/Huggy/raw/main/Huggy.zip\" -O ./trained-envs-executables/linux/Huggy.zip\n```\n\n----------------------------------------\n\nTITLE: Cloning ML-Agents Repository in Bash\nDESCRIPTION: Clones the Unity ML-Agents repository which contains the required libraries and tools for training reinforcement learning agents.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus-unit1.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n# Clone the repository (can take 3min)\n!git clone --depth 1 https://github.com/Unity-Technologies/ml-agents\n```\n\n----------------------------------------\n\nTITLE: Python Installation Command for Godot RL Agents\nDESCRIPTION: Simple pip installation command for installing the Godot RL Agents library in Python environment.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unitbonus3/godotrl.mdx#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\npip install godot-rl\n```\n\n----------------------------------------\n\nTITLE: Setting File Permissions for Environment Executable in Bash\nDESCRIPTION: Sets the required permissions for the Huggy environment executable to ensure it can be run.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus-unit1.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n!chmod -R 755 ./trained-envs-executables/linux/Huggy\n```\n\n----------------------------------------\n\nTITLE: Checking Python Version in New Environment\nDESCRIPTION: This snippet executes `!python --version` to check the Python version after the virtual environment has been activated. It confirms that the virtual environment setup was successful.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit5/hands-on.mdx#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Python Version in New Virtual Environment (Compatible with ML-Agents)\n!python --version\n```\n\n----------------------------------------\n\nTITLE: Unzipping the Executable\nDESCRIPTION: This bash command unzips the downloaded executable file (SnowballTarget.zip) to extract the environment executable to a designated directory.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit5/hands-on.mdx#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nunzip -d ./training-envs-executables/linux/ ./training-envs-executables/linux/SnowballTarget.zip\n```\n\n----------------------------------------\n\nTITLE: Training an ML-Agents Agent using Bash\nDESCRIPTION: This command initiates the training process for an ML-Agents agent using the specified configuration file (`Huggy.yaml`), environment executable (`./trained-envs-executables/linux/Huggy/Huggy`), and assigns a run ID ('Huggy'). The `--no-graphics` flag prevents the visualization window from launching during training. The `--resume` flag (mentioned in text) can be added to continue interrupted training.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unitbonus1/train.mdx#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nmlagents-learn ./config/ppo/Huggy.yaml --env=./trained-envs-executables/linux/Huggy/Huggy --run-id=\"Huggy\" --no-graphics\n```\n\n----------------------------------------\n\nTITLE: Print Action Space\nDESCRIPTION: This python script prints the action space and gives an example of a possible action, so the range of actions can be determined.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit6/hands-on.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"The Action Space is: \", a_size)\nprint(\"Action Space Sample\", env.action_space.sample()) # Take a random action\n```\n\n----------------------------------------\n\nTITLE: Listing Downloaded Model Directory Contents (Shell)\nDESCRIPTION: Uses the `ls` command to list the files and directories within the target location where the Hugging Face model was downloaded.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part2.ipynb#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\n!ls train_dir/doom_health_gathering_supreme_2222\n```\n\n----------------------------------------\n\nTITLE: Printing Observation Space Details (Pixelcopter) Python\nDESCRIPTION: Prints the size of the observation space and a sample observation from the initialized 'Pixelcopter-PLE-v0' environment. This helps understand the input format and dimensionality expected by the agent's policy. Requires the `env` object to be initialized.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nprint(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"The State Space is: \", s_size)\nprint(\"Sample observation\", env.observation_space.sample())  # Get a random observation\n```\n\n----------------------------------------\n\nTITLE: Check Current Python Version (Shell)\nDESCRIPTION: Executes a shell command to display the version of the Python interpreter currently active in the Google Colab environment. This step is performed to check if the default Python version meets the compatibility requirements of the ML-Agents library before proceeding with setup.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit5/unit5.ipynb#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\n# Colab's Current Python Version (Incompatible with ML-Agents)\n!python --version\n```\n\n----------------------------------------\n\nTITLE: Evaluate Trained CartPole Agent in Python\nDESCRIPTION: Executes the `evaluate_agent` function using the trained CartPole policy and evaluation environment. It uses hyperparameters like maximum timesteps per episode and the number of evaluation episodes to get performance metrics.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit4/hands-on.mdx#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nevaluate_agent(\n    eval_env, cartpole_hyperparameters[\"max_t\"], cartpole_hyperparameters[\"n_evaluation_episodes\"], cartpole_policy\n)\n```\n\n----------------------------------------\n\nTITLE: Unimplemented AI Controller Methods in GDScript\nDESCRIPTION: These are the required methods that need to be implemented when extending the AI controller class. They include observation collection, reward calculation, action space definition, and action setting functions.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unitbonus3/godotrl.mdx#_snippet_0\n\nLANGUAGE: GDScript\nCODE:\n```\n#-- Methods that need implementing using the \"extend script\" option in Godot --#\nfunc get_obs() -> Dictionary:\n\tassert(false, \"the get_obs method is not implemented when extending from ai_controller\")\n\treturn {\"obs\":[]}\n\nfunc get_reward() -> float:\n\tassert(false, \"the get_reward method is not implemented when extending from ai_controller\")\n\treturn 0.0\n\nfunc get_action_space() -> Dictionary:\n\tassert(false, \"the get get_action_space method is not implemented when extending from ai_controller\")\n\treturn {\n\t\t\"example_actions_continous\" : {\n\t\t\t\"size\": 2,\n\t\t\t\"action_type\": \"continuous\"\n\t\t},\n\t\t\"example_actions_discrete\" : {\n\t\t\t\"size\": 2,\n\t\t\t\"action_type\": \"discrete\"\n\t\t},\n\t\t}\n\nfunc set_action(action) -> void:\n\tassert(false, \"the get set_action method is not implemented when extending from ai_controller\")\n# -----------------------------------------------------------------------------#\n```\n\n----------------------------------------\n\nTITLE: Configuring Level Size in GDScript\nDESCRIPTION: After modifying the level dimensions, the level_size variable in robot_ai_controller.gd needs to be updated to reflect the longest dimension of the level. This variable is likely used for normalizing position values or determining observation space boundaries.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unitbonus5/customize-the-environment.mdx#_snippet_2\n\nLANGUAGE: GDScript\nCODE:\n```\n# In robot_ai_controller.gd\nvar level_size = 30.0  # Example value, adjust based on level dimensions\n```\n\n----------------------------------------\n\nTITLE: Quiz Question 5: Actor-Critic Statements (JSX)\nDESCRIPTION: Defines the fifth question of the quiz, which assesses understanding of key characteristics and functions of the Actor-Critic method in Reinforcement Learning, particularly the roles of the Actor and Critic components.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit6/quiz.mdx#_snippet_3\n\nLANGUAGE: JSX\nCODE:\n```\n<Question\n\tchoices={[\n   \t\t {\n\t\t\ttext: \"The Critic does not learn any function during the training process\",\n\t\t\texplain: \"Both the Actor and the Critic function parameters are updated during training time\",\n      \t\t\tcorrect: false,\n\t\t},\n\t\t{\n\t\t\ttext: \"The Actor learns a policy function, while the Critic learns a value function\",\n\t\t\texplain: \"\",\n      \t\t\tcorrect: true,\n\t\t},\n    \t\t{\n\t\t\ttext: \"It adds resistance to stochasticity and reduces high variance\",\n\t\t\texplain: \"\",\n      \t\t\tcorrect: true,\n\t\t},\t\t    \n\t]}\n/>\n```\n\n----------------------------------------\n\nTITLE: Adding Repo ID Argument to Parser (Python)\nDESCRIPTION: Placeholder comment indicating where to modify an argument parsing function (assumed to be named `parse_args()` and likely using `argparse`). The modification involves adding a new argument to specify the Hugging Face Hub repository ID (`repo-id`) where the trained model should be uploaded.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# Add new argument in `parse_args()` function to define the repo-id where we want to push the model.\n```\n\n----------------------------------------\n\nTITLE: Installing Required System Dependencies Shell\nDESCRIPTION: Installs necessary system packages (swig, cmake, ffmpeg) using the apt-get package manager. These dependencies are often required for building libraries, compiling code, and processing media files like videos, which are essential for environment rendering and recording replays.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit3/unit3.ipynb#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\n!apt-get install swig cmake ffmpeg\n```\n\n----------------------------------------\n\nTITLE: Restarting Colab Runtime (Python)\nDESCRIPTION: Kills the current Python process (Colab runtime) using the `os` module to ensure newly installed system and Python libraries are correctly loaded and recognized. This forces a runtime restart, after which the user needs to reconnect and continue execution from the subsequent cells.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.kill(os.getpid(), 9)\n```\n\n----------------------------------------\n\nTITLE: Install Gym and Hugging Face Packages\nDESCRIPTION: This code installs the necessary dependencies, specifically gym with the box2d extension for the LunarLander-v2 environment, imageio-ffmpeg for video handling and huggingface_hub for model uploading and management.  This ensures that all the libraries required for training the PPO agent, rendering the environment, and uploading the model to the Hugging Face Hub are correctly installed.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n!pip install gym==0.22\n!pip install imageio-ffmpeg\n!pip install huggingface_hub\n!pip install gym[box2d]==0.22\n```\n\n----------------------------------------\n\nTITLE: Authenticating to Hugging Face Hub in Jupyter/Colab - Python\nDESCRIPTION: This snippet logs into the Hugging Face Hub from a Jupyter notebook or Colab environment by running notebook_login and configuring git credentials. Required dependencies: huggingface_hub and access to a Hugging Face account with a valid auth token. Expected input is an interactive token prompt; upon success, the user's credentials are stored for API uploads and downloads. Optional: running huggingface-cli login via shell for non-notebook environments.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx#_snippet_30\n\nLANGUAGE: Python\nCODE:\n```\nfrom huggingface_hub import notebook_login\nnotebook_login()\n!git config --global credential.helper store\n```\n\n----------------------------------------\n\nTITLE: Unzip SnowballTarget Executable (Shell)\nDESCRIPTION: Extracts the contents of the downloaded `SnowballTarget.zip` file into the `./training-envs-executables/linux/` directory. The `-d` flag specifies the destination directory for the unzipped files, making the environment executable accessible to the ML-Agents training process.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit5/unit5.ipynb#_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\n%%capture\n!unzip -d ./training-envs-executables/linux/ ./training-envs-executables/linux/SnowballTarget.zip\n```\n\n----------------------------------------\n\nTITLE: Checking Python Version in Colab - Python\nDESCRIPTION: This snippet checks and outputs the currently active Python interpreter version in the Colab environment using the shell command '!python --version'. This helps verify compatibility with library requirements before proceeding. No dependencies are needed, and the sole output is the version string.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus_unit1.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Colab's Current Python Version (Incompatible with ML-Agents)\n!python --version\n```\n\n----------------------------------------\n\nTITLE: Import YouTube video\nDESCRIPTION: Imports the HTML module from the IPython display library to display the YouTube video tutorial in the notebook. This allows users to directly access the Costa Huang PPO tutorial within their Colab environment.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import HTML\n\nHTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/MEt6rrxH8W4\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n```\n\n----------------------------------------\n\nTITLE: Quiz Question 3: Monte Carlo Method Statements (JSX)\nDESCRIPTION: Defines the third quiz question, focusing on statements related to the Monte Carlo method in Reinforcement Learning. It provides multiple choices to test understanding of its sampling nature, resistance to stochasticity, and variance reduction techniques.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/units/en/unit6/quiz.mdx#_snippet_2\n\nLANGUAGE: JSX\nCODE:\n```\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"It's a sampling mechanism, which means we don't analyze all the possible states, but a sample of those\",\n\t\t\texplain: \"\",\n      \t\t\tcorrect: true,\n\t\t},\n    \t\t{\n\t\t\ttext: \"It's very resistant to stochasticity (random elements in the trajectory)\",\n\t\t\texplain: \"Monte Carlo randomly estimates everytime a sample of trajectories. However, even same trajectories can have different reward values if they contain stochastic elements\",\n      \t\t\tcorrect: false,\n\t\t},\n    \t\t{\n\t\t\ttext: \"To reduce the impact of stochastic elements in Monte Carlo, we take `n` strategies and average them, reducing their individual impact\",\n\t\t\texplain: \"\",\n\t\t\tcorrect: true,\n\t\t},\t\t    \n\t]}\n/>\n```\n\n----------------------------------------\n\nTITLE: Code Section for PPO Implementation\nDESCRIPTION: This section provides a placeholder for the user's PPO code. It is left blank, prompting the user to implement their PPO agent logic based on the tutorial and other resources provided.\nSOURCE: https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n### Your code here:\n```"
  }
]
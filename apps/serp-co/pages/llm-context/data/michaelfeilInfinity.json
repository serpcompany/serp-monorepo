[
  {
    "owner": "michaelfeil",
    "repo": "infinity",
    "content": "TITLE: Using BatchedInference for Multiple ML Tasks\nDESCRIPTION: Example showing how to initialize and use BatchedInference for various ML tasks including embeddings generation, classification, reranking and image embeddings. Demonstrates model initialization with different configurations and processing different types of inputs.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/libs/embed_package/README.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom embed import BatchedInference\nfrom concurrent.futures import Future\n\n# Run any model\nregister = BatchedInference(\n    model_id=[\n        # sentence-embeddings\n        \"michaelfeil/bge-small-en-v1.5\",\n        # sentence-embeddings and image-embeddings\n        \"jinaai/jina-clip-v1\",\n        # classification models\n        \"philschmid/tiny-bert-sst2-distilled\",\n        # rerankers\n        \"mixedbread-ai/mxbai-rerank-xsmall-v1\",\n    ],\n    # engine to `torch` or `optimum`\n    engine=\"torch\",\n    # device `cuda` (Nvidia/AMD) or `cpu`\n    device=\"cpu\",\n)\n\nsentences = [\"Paris is in France.\", \"Berlin is in Germany.\", \"A image of two cats.\"]\nimages = [\"http://images.cocodataset.org/val2017/000000039769.jpg\"]\nquestion = \"Where is Paris?\"\n\nfuture: \"Future\" = register.embed(\n    sentences=sentences, model_id=\"michaelfeil/bge-small-en-v1.5\"\n)\nfuture.result()\nregister.rerank(\n    query=question, docs=sentences, model_id=\"mixedbread-ai/mxbai-rerank-xsmall-v1\"\n)\nregister.classify(model_id=\"philschmid/tiny-bert-sst2-distilled\", sentences=sentences)\nregister.image_embed(model_id=\"jinaai/jina-clip-v1\", images=images)\n\n# manually stop the register upon termination to free model memory.\nregister.stop()\n```\n\n----------------------------------------\n\nTITLE: Generating Text Embeddings with Infinity\nDESCRIPTION: This snippet demonstrates how to use Infinity's AsyncEngineArray to generate embeddings for text sentences. It shows both the context manager approach and the manual start/stop approach for handling the embedding engine.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/README.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom infinity_emb import AsyncEngineArray, EngineArgs, AsyncEmbeddingEngine\n\nsentences = [\"Embed this is sentence via Infinity.\", \"Paris is in France.\"]\narray = AsyncEngineArray.from_args([\n  EngineArgs(model_name_or_path = \"BAAI/bge-small-en-v1.5\", engine=\"torch\", embedding_dtype=\"float32\", dtype=\"auto\")\n])\n\nasync def embed_text(engine: AsyncEmbeddingEngine): \n    async with engine: \n        embeddings, usage = await engine.embed(sentences=sentences)\n    # or handle the async start / stop yourself.\n    await engine.astart()\n    embeddings, usage = await engine.embed(sentences=sentences)\n    await engine.astop()\nasyncio.run(embed_text(array[0]))\n```\n\n----------------------------------------\n\nTITLE: Generating Image Embeddings with CLIP Models\nDESCRIPTION: This snippet shows how to use Infinity with CLIP models to generate embeddings for both text and images. CLIP models are multimodal and can encode both text and image data in the same vector space.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/README.md#2025-04-11_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom infinity_emb import AsyncEngineArray, EngineArgs, AsyncEmbeddingEngine\n\nsentences = [\"This is awesome.\", \"I am bored.\"]\nimages = [\"http://images.cocodataset.org/val2017/000000039769.jpg\"]\nengine_args = EngineArgs(\n    model_name_or_path = \"wkcn/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M\", \n    engine=\"torch\"\n)\narray = AsyncEngineArray.from_args([engine_args])\n\nasync def embed(engine: AsyncEmbeddingEngine): \n    await engine.astart()\n    embeddings, usage = await engine.embed(sentences=sentences)\n    embeddings_image, _ = await engine.image_embed(images=images)\n    await engine.astop()\n\nasyncio.run(embed(array[\"wkcn/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M\"]))\n```\n\n----------------------------------------\n\nTITLE: Reranking Documents with Infinity\nDESCRIPTION: This example shows how to use Infinity for reranking documents based on their relevance to a query. The reranker provides similarity scores between a query and multiple documents, which can be used to prioritize the most relevant information.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/README.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom infinity_emb import AsyncEngineArray, EngineArgs, AsyncEmbeddingEngine\nquery = \"What is the python package infinity_emb?\"\ndocs = [\"This is a document not related to the python package infinity_emb, hence...\", \n    \"Paris is in France!\",\n    \"infinity_emb is a package for sentence embeddings and rerankings using transformer models in Python!\"]\narray = AsyncEmbeddingEngine.from_args(\n  [EngineArgs(model_name_or_path = \"mixedbread-ai/mxbai-rerank-xsmall-v1\", engine=\"torch\")]\n)\n\nasync def rerank(engine: AsyncEmbeddingEngine): \n    async with engine:\n        ranking, usage = await engine.rerank(query=query, docs=docs)\n        print(list(zip(ranking, docs)))\n    # or handle the async start / stop yourself.\n    await engine.astart()\n    ranking, usage = await engine.rerank(query=query, docs=docs)\n    await engine.astop()\n\nasyncio.run(rerank(array[0]))\n```\n\n----------------------------------------\n\nTITLE: Using BatchedInference for Model Operations\nDESCRIPTION: Demonstrates initialization and usage of BatchedInference class for various model operations including embeddings, classification, and reranking. Shows how to handle different model types and process both text and image inputs.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/embed.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom embed import BatchedInference\n\n# Run any model\nregister = BatchedInference(model_id=[\n  # sentence-embeddings\n  \"michaelfeil/bge-small-en-v1.5\",\n  # sentence-embeddings and image-embeddings\n  \"wkcn/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M\",\n  # classification models\n  \"philschmid/tiny-bert-sst2-distilled\",\n  # rerankers\n  \"mixedbread-ai/mxbai-rerank-xsmall-v1\"\n],\n# engine to `torch` or `optimum`\nengine=\"torch\",\n# device `cuda` (Nvidia/AMD) or `cpu`\ndevice=\"cpu\"\n)\n\nsentences = [\"Paris is in France.\", \"Berlin is in Germany.\", \"I love SF\"]\nimages = [\"http://images.cocodataset.org/val2017/000000039769.jpg\"]\nquestion = \"Where is Paris?\"\n\nregister.embed(sentences=sentences, model_id=\"michaelfeil/bge-small-en-v1.5\")\nregister.rerank(query=question, docs=sentences, model_id=\"mixedbread-ai/mxbai-rerank-xsmall-v1\")\nregister.classify(model_id=\"philschmid/tiny-bert-sst2-distilled\", sentences=sentences)\nregister.image_embed(model_id=\"wkcn/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M\", images=images)\n\n# Always manually stop the register upon termination to free model memory.\nregister.stop()\n```\n\n----------------------------------------\n\nTITLE: Using CLIP Models for Image and Text Encoding with Infinity\nDESCRIPTION: Demonstrates how to use CLIP models in Infinity to encode both text and images. The example initializes a TinyCLIP model, embeds text sentences and image URLs, and retrieves the resulting embeddings.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/python_engine.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom infinity_emb import AsyncEngineArray, EngineArgs, AsyncEmbeddingEngine\n\nsentences = [\"This is awesome.\", \"I am bored.\"]\nimages = [\"http://images.cocodataset.org/val2017/000000039769.jpg\"]\nengine_args = EngineArgs(\n    model_name_or_path = \"wkcn/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M\", \n    engine=\"torch\"\n)\narray = AsyncEngineArray.from_args([engine_args])\n\nasync def embed(engine: AsyncEmbeddingEngine): \n    await engine.astart()\n    embeddings, usage = await engine.embed(sentences=sentences)\n    embeddings_image, _ = await engine.image_embed(images=images)\n    await engine.astop()\n\nasyncio.run(embed(array[\"wkcn/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M\"]))\n```\n\n----------------------------------------\n\nTITLE: Implementing Asynchronous Embedding Generation with Infinity in Python\nDESCRIPTION: Demonstrates how to use asyncio for concurrent embedding generation using Infinity. This example initializes an embedding engine with the BGE small model, submits multiple embedding jobs in parallel, and handles their results asynchronously.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/python_engine.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom infinity_emb import AsyncEngineArray, EngineArgs, AsyncEmbeddingEngine\nfrom infinity_emb.log_handler import logger\nlogger.setLevel(5) # Debug\n\n# Define sentences for embedding\nsentences = [\"Embed this sentence via Infinity.\", \"Paris is in France.\"]\n# Initialize the embedding engine with model specifications\narray = AsyncEngineArray.from_args([\n    EngineArgs(\n        model_name_or_path=\"BAAI/bge-small-en-v1.5\",\n        engine=\"torch\", \n        lengths_via_tokenize=True\n    )]\n)\n\nasync def embed_image(engine: AsyncEmbeddingEngine): \n    await engine.astart()  # initializes  the engine\n    job1 = asyncio.create_task(engine.embed(sentences=sentences))\n    # submit a second job in parallel\n    job2 = asyncio.create_task(engine.embed(sentences=[\"Hello world\"]))\n    # usage is total token count according to tokenizer.\n    embeddings, usage = await job1\n    embeddings2, usage2 = await job2\n    # Embeddings are now available for use - they ran in the same batch.\n    print(f\"for {sentences}, generated embeddings {len(embeddings)} with tot_tokens={usage}\")\n    await engine.astop() \n\nasyncio.run(\n    embed_image(array[\"BAAI/bge-small-en-v1.5\"])\n)\n```\n\n----------------------------------------\n\nTITLE: Using Infinity API for Embeddings with Python OpenAI Client\nDESCRIPTION: Python code demonstrating how to use the deployed Infinity instance to generate embeddings using the OpenAI-compatible API. It shows how to set up the client with the correct API key and base URL.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/infra/vast/README.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\n# Modify OpenAI's API key and API base to use Infinity's API server.\nopenai_api_key = \"simple_key\"\nopenai_api_base = \"http://<Instance-IP-Address>:<Port>\"\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\nmodel = \"michaelfeil/bge-small-en-v1.5\"\nembeddings = client.embeddings.create(model=model, input=\"What is Deep Learning?\").data[0].embedding\nprint(\"Embeddings:\")\nprint(embeddings)\n```\n\n----------------------------------------\n\nTITLE: Generating Audio Embeddings with CLAP Models\nDESCRIPTION: This example demonstrates how to use Infinity with CLAP models to generate embeddings for both text and audio. Similar to CLIP for images, CLAP models allow encoding audio and text in the same embedding space.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/README.md#2025-04-11_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom infinity_emb import AsyncEngineArray, EngineArgs, AsyncEmbeddingEngine\nimport requests\nimport soundfile as sf\nimport io\n\nsentences = [\"This is awesome.\", \"I am bored.\"]\n\nurl = \"https://bigsoundbank.com/UPLOAD/wav/2380.wav\"\nraw_bytes = requests.get(url, stream=True).content\n\naudios = [raw_bytes]\nengine_args = EngineArgs(\n    model_name_or_path = \"laion/clap-htsat-unfused\",\n    dtype=\"float32\", \n    engine=\"torch\"\n\n)\narray = AsyncEngineArray.from_args([engine_args])\n\nasync def embed(engine: AsyncEmbeddingEngine): \n    await engine.astart()\n    embeddings, usage = await engine.embed(sentences=sentences)\n    embedding_audios = await engine.audio_embed(audios=audios)\n    await engine.astop()\n\nasyncio.run(embed(array[\"laion/clap-htsat-unfused\"]))\n```\n\n----------------------------------------\n\nTITLE: Implementing Document Reranking with Infinity in Python\nDESCRIPTION: Shows how to use Infinity for reranking documents based on their relevance to a query. The code initializes a reranker model, processes a query against multiple documents, and returns relevance scores for each document-query pair.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/python_engine.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom infinity_emb import AsyncEngineArray, EngineArgs, AsyncEmbeddingEngine\nquery = \"What is the python package infinity_emb?\"\ndocs = [\"This is a document not related to the python package infinity_emb, hence...\", \n    \"Paris is in France!\",\n    \"infinity_emb is a package for sentence embeddings and rerankings using transformer models in Python!\"]\narray = AsyncEmbeddingEngine.from_args(\n  [EngineArgs(model_name_or_path = \"mixedbread-ai/mxbai-rerank-xsmall-v1\", engine=\"torch\")]\n)\n\nasync def rerank(engine: AsyncEmbeddingEngine): \n    async with engine:\n        ranking, usage = await engine.rerank(query=query, docs=docs)\n        print(list(zip(ranking, docs)))\n    # or handle the async start / stop yourself.\n    await engine.astart()\n    ranking, usage = await engine.rerank(query=query, docs=docs)\n    await engine.astop()\n\nasyncio.run(rerank(array[0]))\n```\n\n----------------------------------------\n\nTITLE: Implementing Text Classification with Infinity in Python\nDESCRIPTION: Shows how to use Infinity for text classification tasks like sentiment analysis. The code initializes a classification model, processes sentences, and retrieves classification predictions asynchronously.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/python_engine.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom infinity_emb import AsyncEngineArray, EngineArgs, AsyncEmbeddingEngine\n\nsentences = [\"This is awesome.\", \"I am bored.\"]\nengine_args = EngineArgs(\n    model_name_or_path = \"SamLowe/roberta-base-go_emotions\", \n    engine=\"torch\", model_warmup=True)\narray = AsyncEngineArray.from_args([engine_args])\n\nasync def classifier(): \n    async with engine:\n        predictions, usage = await engine.classify(sentences=sentences)\n    # or handle the async start / stop yourself.\n    await engine.astart()\n    predictions, usage = await engine.classify(sentences=sentences)\n    await engine.astop()\nasyncio.run(classifier(array[\"SamLowe/roberta-base-go_emotions\"]))\n```\n\n----------------------------------------\n\nTITLE: Text Classification with Infinity\nDESCRIPTION: This snippet shows how to perform text classification tasks like sentiment analysis or emotion detection using Infinity. The example uses a pre-trained model to classify text into different emotional categories.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/README.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom infinity_emb import AsyncEngineArray, EngineArgs, AsyncEmbeddingEngine\n\nsentences = [\"This is awesome.\", \"I am bored.\"]\nengine_args = EngineArgs(\n    model_name_or_path = \"SamLowe/roberta-base-go_emotions\", \n    engine=\"torch\", model_warmup=True)\narray = AsyncEngineArray.from_args([engine_args])\n\nasync def classifier(engine: AsyncEmbeddingEngine): \n    async with engine:\n        predictions, usage = await engine.classify(sentences=sentences)\n    # or handle the async start / stop yourself.\n    await engine.astart()\n    predictions, usage = await engine.classify(sentences=sentences)\n    await engine.astop()\nasyncio.run(classifier(array[\"SamLowe/roberta-base-go_emotions\"]))\n```\n\n----------------------------------------\n\nTITLE: Working with Future Objects in Embed\nDESCRIPTION: Example showing how to work with Future objects returned by the Embed package, demonstrating the async nature of operations and how to retrieve results.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/embed.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> embedding_fut = register.embed(sentences=sentences, model_id=\"michaelfeil/bge-small-en-v1.5\")\n>>> print(embedding_fut)\n<Future at 0x7fa0e97e8a60 state=pending>\n>>> time.sleep(1) and print(embedding_fut)\n<Future at 0x7fa0e97e9c30 state=finished returned tuple>\n>>> embedding_fut.result()\n([array([-3.35943862e-03, ..., -3.22808176e-02], dtype=float32)], 19)\n```\n\n----------------------------------------\n\nTITLE: Working with Future Objects in Embed\nDESCRIPTION: Example demonstrating how to work with Future objects returned by embed operations, including checking Future states and retrieving results. Shows the asynchronous nature of the operations.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/libs/embed_package/README.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> embedding_fut = register.embed(sentences=sentences, model_id=\"michaelfeil/bge-small-en-v1.5\")\n>>> print(embedding_fut)\n<Future at 0x7fa0e97e8a60 state=pending>\n>>> time.sleep(1) and print(embedding_fut)\n<Future at 0x7fa0e97e9c30 state=finished returned tuple>\n>>> embedding_fut.result()\n([array([-3.35943862e-03, ..., -3.22808176e-02], dtype=float32)], 19)\n```\n\n----------------------------------------\n\nTITLE: Langchain Integration with Infinity API Server\nDESCRIPTION: Demonstrates how to use Infinity embeddings with a running API server at localhost:7997/v1. Uses the BAAI/bge-small model to generate embeddings for documents.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/integrations.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.embeddings.infinity import InfinityEmbeddings\nfrom langchain.docstore.document import Document\n\ndocuments = [Document(page_content=\"Hello world!\", metadata={\"source\": \"unknown\"})]\n\nemb_model = InfinityEmbeddings(model=\"BAAI/bge-small\", infinity_api_url=\"http://localhost:7997/v1\")\nprint(emb_model.embed_documents([doc.page_content for doc in documents]))\n```\n\n----------------------------------------\n\nTITLE: Langchain Integration with Local Python Inference\nDESCRIPTION: Shows how to use Infinity embeddings without an API server, using local Python inference. Includes async context management and supports GPU acceleration with configurable batch size.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/integrations.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.embeddings.infinity import InfinityEmbeddings\nfrom langchain.docstore.document import Document\n\nembeddings = InfinityEmbeddingsLocal(\n    model=\"sentence-transformers/all-MiniLM-L6-v2\",\n    # revision\n    revision=None,\n    # best to keep at 32\n    batch_size=32,\n    # for AMD/Nvidia GPUs via torch\n    device=\"cuda\",\n    # warm up model before execution\n)\ndocuments = [Document(page_content=\"Hello world!\", metadata={\"source\": \"unknown\"})]\n\n# important: use engine inside of `async with` statement to start/stop the batching engine.\nasync with embeddings:\n    # avoid closing and starting the engine often.\n    # rather keep it running.\n    # you may call `await embeddings.__aenter__()` and `__aexit__()`\n    # if you are sure when to manually start/stop execution` in a more granular way\n    documents_embedded = await embeddings.aembed_documents(documents)\n    query_result = await embeddings.aembed_query(query)\n    print(\"embeddings created successful\")\nprint(documents_embedded, query_result)\n```\n\n----------------------------------------\n\nTITLE: Calling the Infinity Model with OpenAI Python SDK\nDESCRIPTION: Python code using the OpenAI SDK to generate embeddings from the deployed model. Requires setting a base URL to Baseten's bridge and providing model and deployment IDs in the request body.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/infra/baseten/README.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ[\"YOUR_API_KEY\"],\n    base_url=\"https://bridge.baseten.co/v1/direct\"\n)\n\nmodel_id = \"xxx\"\ndeployment_id = \"xxx\"\n\nresponse = client.embeddings.create(\n    input=\"text string\",\n    model=\"BAAI/bge-small-en-v1.5\",\n    extra_body={\n        \"baseten\": {\n            \"model_id\": model_id,\n            \"deployment_id\": deployment_id\n        }\n    }\n)\n\nprint(response.data[0].embedding)\n```\n\n----------------------------------------\n\nTITLE: Calling the Infinity Model with Python Requests\nDESCRIPTION: Python code using the requests library to make a POST request to the deployed model endpoint, requiring an API key for authentication and a JSON payload containing the input text.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/infra/baseten/README.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nresp = requests.post(\n    \"https://model-xxx.api.baseten.co/development/predict\",\n    headers={\"Authorization\": \"Api-Key YOUR_API_KEY\"},\n    json={\"input\": \"text string\"},\n)\n\nprint(resp.json())\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings with POST Request\nDESCRIPTION: Shows how to generate embeddings for text inputs by posting an embedding request to the Infinity API.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/libs/client_infinity/infinity_client/README.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom infinity_client.models import OpenAIEmbeddingInput, OpenAIEmbeddingResult\nfrom infinity_client.api.default import classify, embeddings, embeddings_image, rerank\nfrom infinity_client.types import Response\n\nwith i_client as client:\n    embeds: OpenAIEmbeddingResult = embeddings.sync(client=client, body=OpenAIEmbeddingInput.from_dict({\n        \"input\": [\"Hello, this is a sentence-to-embed\", \"Hello, my cat is cute\"],\n        \"model\": \"michaelfeil/bge-small-en-v1.5\",\n    }))\n```\n\n----------------------------------------\n\nTITLE: Making POST Requests for Embeddings\nDESCRIPTION: Shows how to make POST requests to generate embeddings for text inputs using specific models\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/client_infinity.md#2025-04-11_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom infinity_client.models import OpenAIEmbeddingInput, OpenAIEmbeddingResult\nfrom infinity_client.api.default import classify, embeddings, embeddings_image, rerank\nfrom infinity_client.types import Response\n\nwith i_client as client:\n    embeds: OpenAIEmbeddingResult = embeddings.sync(client=client, body=OpenAIEmbeddingInput.from_dict({\n        \"input\": [\"Hello, this is a sentence-to-embed\", \"Hello, my cat is cute\"],\n        \"model\": \"michaelfeil/bge-small-en-v1.5\",\n    }))\n```\n\n----------------------------------------\n\nTITLE: Using Async API Methods\nDESCRIPTION: Demonstrates the asynchronous version of API calls for retrieving model information using async/await syntax.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/libs/client_infinity/infinity_client/README.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom infinity_client.models import OpenAIModelInfo\nfrom infinity_client.api.default import models\nfrom infinity_client.types import Response\n\nasync with i_client as client:\n    model_info: OpenAIModelInfo = await models.asyncio(client=client)\n    response: Response[OpenAIModelInfo] = await models.asyncio_detailed(client=client)\n```\n\n----------------------------------------\n\nTITLE: Making Asynchronous API Requests\nDESCRIPTION: Demonstrates how to make asynchronous API calls using async/await syntax\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/client_infinity.md#2025-04-11_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom infinity_client.models import OpenAIModelInfo\nfrom infinity_client.api.default import models\nfrom infinity_client.types import Response\n\nasync with i_client as client:\n    model_info: OpenAIModelInfo = await models.asyncio(client=client)\n    response: Response[OpenAIModelInfo] = await models.asyncio_detailed(client=client)\n```\n\n----------------------------------------\n\nTITLE: Making Synchronous API Requests\nDESCRIPTION: Demonstrates how to make synchronous API calls and handle responses using model info endpoint\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/client_infinity.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom infinity_client.models import OpenAIModelInfo\nfrom infinity_client.api.default import models, health\nfrom infinity_client.types import Response\n\nwith i_client as client:\n    model_info: OpenAIModelInfo = models.sync(client=client)\n    # or if you need more info (e.g. status_code)\n    response: Response[OpenAIModelInfo] = models.sync_detailed(client=client)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Model Information\nDESCRIPTION: Demonstrates how to retrieve model information from the Infinity API using both simple and detailed response methods.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/libs/client_infinity/infinity_client/README.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom infinity_client.models import OpenAIModelInfo\nfrom infinity_client.api.default import models, health\nfrom infinity_client.types import Response\n\nwith i_client as client:\n    model_info: OpenAIModelInfo = models.sync(client=client)\n    # or if you need more info (e.g. status_code)\n    response: Response[OpenAIModelInfo] = models.sync_detailed(client=client)\n```\n\n----------------------------------------\n\nTITLE: Running Infinity via Docker Container\nDESCRIPTION: Docker command to run Infinity with GPU support, mounting volumes and specifying multiple models.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nport=7997\nmodel1=michaelfeil/bge-small-en-v1.5\nmodel2=mixedbread-ai/mxbai-rerank-xsmall-v1\nvolume=$PWD/data\n\ndocker run -it --gpus all \\\n -v $volume:/app/.cache \\\n -p $port:$port \\\n michaelf34/infinity:latest \\\n v2 \\\n --model-id $model1 \\\n --model-id $model2 \\\n --port $port\n```\n\n----------------------------------------\n\nTITLE: Running Infinity with TensorRT and ONNX\nDESCRIPTION: Docker command to run Infinity with NVIDIA GPU support using TensorRT and ONNX optimizations.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/README.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it \\\n-v $volume:/app/.cache \\\n-p $port:$port \\\nmichaelf34/infinity:latest-trt-onnx \\\nv2 \\\n--engine optimum \\\n--device cuda \\\n--model-id $model1 \\\n--port $port\n```\n\n----------------------------------------\n\nTITLE: Deploying Local Models with Docker\nDESCRIPTION: Commands to clone a model locally and run it with Infinity in a Docker container by mounting the local model directory.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/README.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ngit lfs install \ncd /tmp\nmkdir models && cd models && git clone https://huggingface.co/BAAI/bge-small-en-v1.5\ndocker run -it   -v /tmp/models:/models  -p 8081:8081  michaelf34/infinity:latest v2  --model-id \"/models/bge-small-en-v1.5\" --port 8081\n```\n\n----------------------------------------\n\nTITLE: Deploying Infinity with NVIDIA Docker\nDESCRIPTION: Docker command to run Infinity with NVIDIA GPU support, mounting local storage and exposing a port for the service. Uses pre-built container with two embedding models.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/deploy.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nport=7997\nmodel1=michaelfeil/bge-small-en-v1.5\nmodel2=mixedbread-ai/mxbai-rerank-xsmall-v1\nvolume=$PWD/data\n\ndocker run -it --gpus all \\\n -v $volume:/app/.cache \\\n -p $port:$port \\\n michaelf34/infinity:latest \\\n v2 \\\n --model-id $model1 \\\n --model-id $model2 \\\n --port $port\n```\n\n----------------------------------------\n\nTITLE: Deploying Infinity with AMD ROCm Docker\nDESCRIPTION: Docker configuration for running Infinity on AMD GPUs (MI200/MI300 Series) with necessary device mounts and security settings. Includes torch engine and compilation flags.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/deploy.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nport=7997\nmodel1=michaelfeil/bge-small-en-v1.5\nmodel2=mixedbread-ai/mxbai-rerank-xsmall-v1\nvolume=$PWD/data\n\ndocker run -it \\\n  --cap-add=SYS_PTRACE \\\n  --security-opt seccomp=unconfined \\\n  --device=/dev/kfd \\\n  --device=/dev/dri \\\n  --group-add video \\\n  --network host \\\n  -v $volume:/app/.cache \\\n  -p $port:$port \\\n  michaelf34/infinity:latest-rocm \\\n  v2 \\\n  --model-id $model1 \\\n  --model-id $model2 \\\n  --port $port \\\n  --engine torch \\\n  --compile \\\n  --no-bettertransformer\n```\n\n----------------------------------------\n\nTITLE: Building and Running TensorRT Optimized Docker Container\nDESCRIPTION: Command to build a TensorRT-optimized Docker image and run it with the BAAI/bge-large model, supporting both CUDA and TensorRT backends.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/benchmarking.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker buildx build --target production-tensorrt -t inf-trt . && docker run -it -p \"7997:7997\" --gpus all inf-trt v2 --model-id BAAI/bge-large-en-v1.5 --engine optimum --device \"cuda OR tensorrt\"\n```\n\n----------------------------------------\n\nTITLE: Launching Rerankers via CLI with Infinity\nDESCRIPTION: A simple command-line interface example for launching rerankers using the infinity_emb CLI tool. This command initializes a reranker model specified by the model ID.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/README.md#2025-04-11_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ninfinity_emb v2 --model-id mixedbread-ai/mxbai-rerank-xsmall-v1\n```\n\n----------------------------------------\n\nTITLE: Launching Rerankers via Command Line Interface in Infinity\nDESCRIPTION: Shows the command line interface command for launching reranker models in Infinity. This is useful for users who prefer CLI over Python API integration.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/python_engine.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ninfinity_emb v2 --model-id mixedbread-ai/mxbai-rerank-xsmall-v1\n```\n\n----------------------------------------\n\nTITLE: Creating Authenticated Client\nDESCRIPTION: Initializes an authenticated client for accessing protected Infinity API endpoints using a token for authentication.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/libs/client_infinity/infinity_client/README.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom infinity_client import AuthenticatedClient\n\ni_client = AuthenticatedClient(base_url=\"https://infinity.modal.michaelfeil.eu\", token=\"SuperSecretToken\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Authenticated Client\nDESCRIPTION: Creates an authenticated client instance with token-based authentication\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/client_infinity.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom infinity_client import AuthenticatedClient\n\ni_client = AuthenticatedClient(base_url=\"https://infinity.modal.michaelfeil.eu\", token=\"SuperSecretToken\")\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Unauthenticated Client\nDESCRIPTION: Initializes a basic unauthenticated client for accessing the Infinity API by specifying the base URL.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/libs/client_infinity/infinity_client/README.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom infinity_client import Client\ni_client = Client(base_url=\"https://infinity.modal.michaelfeil.eu\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Basic Client\nDESCRIPTION: Creates a basic client instance for accessing the Infinity API without authentication\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/client_infinity.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom infinity_client import Client\ni_client = Client(base_url=\"https://infinity.modal.michaelfeil.eu\")\n```\n\n----------------------------------------\n\nTITLE: Running Infinity CLI with Model\nDESCRIPTION: Example command to run the Infinity CLI version 2 with a specific model ID from HuggingFace.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ninfinity_emb v2 --model-id BAAI/bge-small-en-v1.5\n```\n\n----------------------------------------\n\nTITLE: Displaying Infinity CLI Help Command\nDESCRIPTION: Command to display the help menu for Infinity API CLI v2.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/cli_v2.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ninfinity_emb v2 --help\n```\n\n----------------------------------------\n\nTITLE: Infinity CLI Help Output\nDESCRIPTION: Detailed help output showing all available CLI options including model configuration, device settings, caching options, and data type specifications. Includes environment variable mappings and default values for each option.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/cli_v2.md#2025-04-11_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n Usage: infinity_emb v2 [OPTIONS]                                                                       \n\n Infinity API ♾️  cli v2. MIT License. Copyright (c) 2023-now Michael Feil                                               \n Multiple Model CLI Playbook:                                                                                           \n - 1. cli options can be overloaded i.e. `v2 --model-id model/id1 --model-id model/id2 --batch-size 8 --batch-size 4`   \n - 2. or adapt the defaults by setting ENV Variables separated by `;`: INFINITY_MODEL_ID=\"model/id1;model/id2;\" &&      \n INFINITY_BATCH_SIZE=\"8;4;\"                                                                                             \n - 3. single items are broadcasted to `--model-id` length, making `v2 --model-id model/id1 --model-id/id2 --batch-size  \n 8` both models have batch-size 8.\n```\n\n----------------------------------------\n\nTITLE: Displaying Infinity CLI Help\nDESCRIPTION: Command to show the help documentation for Infinity CLI version 2.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ninfinity_emb v2 --help\n```\n\n----------------------------------------\n\nTITLE: Running Infinity Docker with CPU Engine\nDESCRIPTION: Docker command to run Infinity using CPU-only configuration with ONNX optimization.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/README.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it \\\n-v $volume:/app/.cache \\\n-p $port:$port \\\nmichaelf34/infinity:latest-cpu \\\nv2 \\\n--engine optimum \\\n--model-id $model1 \\\n--model-id $model2 \\\n--port $port\n```\n\n----------------------------------------\n\nTITLE: Configuring SSL Certificate Verification\nDESCRIPTION: Shows how to configure custom SSL certificate verification for secure connections\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/client_infinity.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ni_client = AuthenticatedClient(\n    base_url=\"https://internal_api.example.com\",\n    token=\"SuperSecretToken\",\n    verify_ssl=\"/path/to/certificate_bundle.pem\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom SSL Certificate\nDESCRIPTION: Shows how to configure the client to use a custom SSL certificate bundle for authentication to internal servers.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/libs/client_infinity/infinity_client/README.md#2025-04-11_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ni_client = AuthenticatedClient(\n    base_url=\"https://internal_api.example.com\",\n    token=\"SuperSecretToken\",\n    verify_ssl=\"/path/to/certificate_bundle.pem\",\n)\n```\n\n----------------------------------------\n\nTITLE: Disabling SSL Certificate Verification\nDESCRIPTION: Demonstrates how to disable SSL certificate verification (with security warning)\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/client_infinity.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ni_client = AuthenticatedClient(\n    base_url=\"https://internal_api.example.com\", \n    token=\"SuperSecretToken\", \n    verify_ssl=False\n)\n```\n\n----------------------------------------\n\nTITLE: Disabling SSL Certificate Validation\nDESCRIPTION: Demonstrates how to disable SSL certificate validation for the client, with a warning about security risks.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/libs/client_infinity/infinity_client/README.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ni_client = AuthenticatedClient(\n    base_url=\"https://internal_api.example.com\", \n    token=\"SuperSecretToken\", \n    verify_ssl=False\n)\n```\n\n----------------------------------------\n\nTITLE: Searching and Deploying Infinity on Vast.ai using CLI\nDESCRIPTION: Commands to search for suitable instances on Vast.ai and deploy Infinity with different configurations. It includes options for single and multiple model deployments.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/infra/vast/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nvastai search offers 'compute_cap > 800 gpu_ram > 20 num_gpus = 1 static_ip=true direct_port_count > 1' \n# Select the instance and paste it in for <instance-id> to launch\n# Single embedding model deployment:\nvastai create instance <instance-id> --image michaelf34/infinity:latest --env '-p 8000:8000' --disk 40 --args v2 --model-id michaelfeil/bge-small-en-v1.5 --port 8000 --api-key simple_key\n# Multiple models:\nvastai create instance <instance-id> --image michaelf34/infinity:latest --env '-p 8000:8000' --disk 40 --args v2 --model-id mixedbread-ai/mxbai-rerank-xsmall-v1 --model-id  SamLowe/roberta-base-go_emotions --port 8000\n```\n\n----------------------------------------\n\nTITLE: Running Infinity Docker Container on EC2\nDESCRIPTION: Command to run the Infinity container with Neuron device access, specifying model and batch size parameters\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/infra/aws_neuron/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it --rm --device=/dev/neuron0 michaelf34/infinity:0.0.71-neuron v2 --model-id BAAI/bge-small-en-v1.5 --batch-size 8 --log-level debug\n```\n\n----------------------------------------\n\nTITLE: ECS Task Definition for Infinity Neuron\nDESCRIPTION: JSON configuration for creating an ECS task definition with Neuron device mapping and container settings\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/infra/aws_neuron/README.md#2025-04-11_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"family\": \"ecs-infinity-neuron\",\n    \"requiresCompatibilities\": [\"EC2\"],\n    \"placementConstraints\": [\n        {\n            \"type\": \"memberOf\",\n            \"expression\": \"attribute:ecs.os-type == linux\"\n        },\n        {\n            \"type\": \"memberOf\",\n            \"expression\": \"attribute:ecs.instance-type == inf2.xlarge\"\n        }\n    ],\n    \"executionRoleArn\": \"${YOUR_EXECUTION_ROLE}\",\n    \"containerDefinitions\": [\n        {\n            \"entryPoint\": [\n                \"infinity_emb\",\n                \"v2\"\n            ],\n            \"portMappings\": [\n                {\n                    \"hostPort\": 7997,\n                    \"protocol\": \"tcp\",\n                    \"containerPort\": 7997\n                }\n            ],\n            \"linuxParameters\": {\n                \"devices\": [\n                    {\n                        \"containerPath\": \"/dev/neuron0\",\n                        \"hostPath\": \"/dev/neuron0\",\n                        \"permissions\": [\n                            \"read\",\n                            \"write\"\n                        ]\n                    }\n                ],\n                \"capabilities\": {\n                    \"add\": [\n                        \"IPC_LOCK\"\n                    ]\n                }\n            },\n            \"cpu\": 0,\n            \"memoryReservation\": 1000,\n            \"image\": \"michaelf34/infinity:0.0.71-neuron\",\n            \"essential\": true,\n            \"name\": \"infinity-neuron\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Customizing HTTP Client with Event Hooks\nDESCRIPTION: Shows how to add custom request and response logging using event hooks\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/client_infinity.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom infinity_client import Client\n\ndef log_request(request):\n    print(f\"Request event hook: {request.method} {request.url} - Waiting for response\")\n\ndef log_response(response):\n    request = response.request\n    print(f\"Response event hook: {request.method} {request.url} - Status {response.status_code}\")\n\ni_client = Client(\n    base_url=\"https://infinity.modal.michaelfeil.eu\",\n    httpx_args={\"event_hooks\": {\"request\": [log_request], \"response\": [log_response]}},\n)\n```\n\n----------------------------------------\n\nTITLE: Customizing HTTP Clients with Event Hooks\nDESCRIPTION: Shows how to customize the underlying httpx client with event hooks for logging requests and responses.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/libs/client_infinity/infinity_client/README.md#2025-04-11_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom infinity_client import Client\n\ndef log_request(request):\n    print(f\"Request event hook: {request.method} {request.url} - Waiting for response\")\n\ndef log_response(response):\n    request = response.request\n    print(f\"Response event hook: {request.method} {request.url} - Status {response.status_code}\")\n\ni_client = Client(\n    base_url=\"https://infinity.modal.michaelfeil.eu\",\n    httpx_args={\"event_hooks\": {\"request\": [log_request], \"response\": [log_response]}},\n)\n\n# Or get the underlying httpx client to modify directly with client.get_httpx_client() or client.get_async_httpx_client()\n```\n\n----------------------------------------\n\nTITLE: Setting Custom HTTP Client\nDESCRIPTION: Demonstrates how to configure a custom HTTP client with proxy settings\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/client_infinity.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\nfrom infinity_client import Client\n\ni_client = Client(\n    base_url=\"https://infinity.modal.michaelfeil.eu\",\n)\n# Note that base_url needs to be re-set, as would any shared cookies, headers, etc.\ni_client.set_httpx_client(httpx.Client(base_url=\"https://infinity.modal.michaelfeil.eu\", proxies=\"http://localhost:8030\"))\n```\n\n----------------------------------------\n\nTITLE: Directly Setting the HTTPX Client\nDESCRIPTION: Demonstrates how to set the underlying httpx client directly, with a note about overriding existing settings.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/libs/client_infinity/infinity_client/README.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\nfrom infinity_client import Client\n\ni_client = Client(\n    base_url=\"https://infinity.modal.michaelfeil.eu\",\n)\n# Note that base_url needs to be re-set, as would any shared cookies, headers, etc.\ni_client.set_httpx_client(httpx.Client(base_url=\"https://infinity.modal.michaelfeil.eu\", proxies=\"http://localhost:8030\"))\n```\n\n----------------------------------------\n\nTITLE: Calling the Infinity Model with Curl\nDESCRIPTION: Example of sending a POST request to the deployed model endpoint using curl, requiring an API key for authentication and a JSON payload containing the input text.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/infra/baseten/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST https://model-xxx.api.baseten.co/development/predict \\\n        -H \"Authorization: Api-Key YOUR_API_KEY\" \\\n        -d '{\"input\": \"text string\"}'\n```\n\n----------------------------------------\n\nTITLE: Deploying the Infinity Embedding Server on Baseten\nDESCRIPTION: Command to push and publish the Truss to Baseten, with the trusted flag enabled.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/infra/baseten/README.md#2025-04-11_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ntruss push --publish --trusted\n```\n\n----------------------------------------\n\nTITLE: Deploying Infinity Project Locally via Modal\nDESCRIPTION: This snippet shows the sequence of commands needed to deploy the Infinity project locally. It involves cloning the repository, installing Modal version 0.66.0, and deploying the webserver module using Modal's CLI.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/infra/modal/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/michaelfeil/infinity\npip install modal==0.66.0\nmodal deploy --env main infra.modal.webserver\n```\n\n----------------------------------------\n\nTITLE: Deploying Infinity in Offline Mode\nDESCRIPTION: Docker build commands for creating an offline-capable Infinity container with pre-downloaded models and custom dependencies.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/deploy.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# clone the repo\ngit clone https://github.com/michaelfeil/infinity\ngit checkout tags/0.0.52\ncd libs/infinity_emb\n# build download stage using docker buildx buildkit.\ndocker buildx build --target=production-with-download \\\n--build-arg MODEL_NAME=michaelfeil/bge-small-en-v1.5 --build-arg ENGINE=torch \\\n-f Dockerfile -t infinity-model-small .\n```\n\n----------------------------------------\n\nTITLE: Configuring Infinity Service with dstack\nDESCRIPTION: YAML configuration for deploying Infinity service using dstack, specifying the model, port, and environment variables.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/deploy.md#2025-04-11_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ntype: service\n\nimage: michaelf34/infinity:latest\nenv:\n  - INFINITY_MODEL_ID=BAAI/bge-small-en-v1.5;BAAI/bge-reranker-base;\n  - INFINITY_PORT=80\ncommands:\n  - infinity_emb v2\nport: 80\n```\n\n----------------------------------------\n\nTITLE: Installing Infinity via pip\nDESCRIPTION: Command to install the Infinity package with all dependencies using pip package manager.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install infinity-emb[all]\n```\n\n----------------------------------------\n\nTITLE: Installing Embed Library via pip\nDESCRIPTION: Command to install the embed Python package using pip package manager\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/libs/embed_package/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install embed\n```\n\n----------------------------------------\n\nTITLE: Installing Embed Package with pip\nDESCRIPTION: Command to install the Embed package using pip package manager.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/embed.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install embed\n```\n\n----------------------------------------\n\nTITLE: Installing Infinity Client with pip\nDESCRIPTION: Command to install the infinity_client package using pip.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/libs/client_infinity/infinity_client/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install infinity_client\n```\n\n----------------------------------------\n\nTITLE: Installing Infinity Client via pip\nDESCRIPTION: Command to install the infinity_client package using pip package manager\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/client_infinity.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install infinity_client\n```\n\n----------------------------------------\n\nTITLE: Installing Infinity Client for REST API Access\nDESCRIPTION: A simple command to install the Infinity client package, which provides generated client code for interacting with remote Infinity instances via REST API.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/README.md#2025-04-11_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\npip install infinity_client\n```\n\n----------------------------------------\n\nTITLE: Disabling Telemetry in Infinity Project using Environment Variables\nDESCRIPTION: This snippet shows how to disable telemetry tracking in the Infinity project by setting environment variables. Two options are provided: a general DO_NOT_TRACK variable and an Infinity-specific setting.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/telemetry.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# set \nexport DO_NOT_TRACK=\"1\"\n# infinity specific setting\nexport INFINITY_ANONYMOUS_USAGE_STATS=\"0\"\n```\n\n----------------------------------------\n\nTITLE: Disabling Telemetry in Infinity via Environment Variable\nDESCRIPTION: This command shows how to opt out of anonymous usage statistics collection in the Infinity embedding server by setting an environment variable to '0'.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/README.md#2025-04-11_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# Disable\nexport INFINITY_ANONYMOUS_USAGE_STATS=\"0\"\n```\n\n----------------------------------------\n\nTITLE: Cloning the Repository for Infinity Embedding Server\nDESCRIPTION: Commands to clone the truss-examples repository and navigate to the infinity-embedding-server directory.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/infra/baseten/README.md#2025-04-11_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://github.com/basetenlabs/truss-examples.git\ncd custom-server/infinity-embedding-server\n```\n\n----------------------------------------\n\nTITLE: Installing Infinity Project with Poetry on Ubuntu\nDESCRIPTION: Instructions for cloning the repository and installing dependencies using Poetry 1.8.4 with Python 3.11 on Ubuntu 22.04. This installs the infinity_emb package with all extras and test dependencies.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/contribution.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/michaelfeil/infinity\ncd infinity\ncd libs/infinity_emb\npoetry install --extras all --with test\n```\n\n----------------------------------------\n\nTITLE: Installing Infinity for Development\nDESCRIPTION: Commands for setting up a development environment for Infinity using Poetry on Ubuntu 22.04. Includes installation of all dependencies with test and lint extras.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/README.md#2025-04-11_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ncd libs/infinity_emb\npoetry install --extras all --with lint,test\n```\n\n----------------------------------------\n\nTITLE: Running Code Quality Checks and Tests\nDESCRIPTION: Commands to ensure contributions pass Continuous Integration by running formatting, linting, Docker template generation, and tests from the Makefile in the infinity_emb directory.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/contribution.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd libs/infinity_emb\nmake format\nmake lint\nmake template-docker\npoetry run pytest ./tests\n```\n\n----------------------------------------\n\nTITLE: Using the Precommit Command\nDESCRIPTION: An alternative command that bundles multiple code quality checks into a single make target, simplifying the process of preparing code for submission.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/contribution.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd libs/infinity_emb\nmake precommit\n```\n\n----------------------------------------\n\nTITLE: Running Pre-commit Checks for Infinity\nDESCRIPTION: A command to run the pre-commit checks required to pass the continuous integration (CI) pipeline for the Infinity project.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/README.md#2025-04-11_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ncd libs/infinity_emb\nmake precommit\n```\n\n----------------------------------------\n\nTITLE: Installing Vast.ai CLI in Python\nDESCRIPTION: Command to install the Vast.ai command line interface using pip.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/infra/vast/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade vastai\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Infinity Project\nDESCRIPTION: Commands to clone the Infinity repository and build a Docker image with Neuron support\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/infra/aws_neuron/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/michaelfeil/infinity\ncd infinity\ndocker buildx build -t michaelf34/infinity:0.0.x-neuron -f ./infra/aws_neuron/Dockerfile.neuron\n```\n\n----------------------------------------\n\nTITLE: Embedding Swagger UI with OpenAPI JSON in Markdown\nDESCRIPTION: This code snippet shows how to embed Swagger UI in a markdown document by referencing an external OpenAPI JSON file from the project's GitHub repository.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/swagger_ui.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<swagger-ui src=\"https://raw.githubusercontent.com/michaelfeil/infinity/main/docs/assets/openapi.json\"/>\n```\n\n----------------------------------------\n\nTITLE: Installing Infinity Embedding Environment\nDESCRIPTION: Command to install the infinity_emb package with all optional dependencies at version 0.0.25.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/benchmarking.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"infinity_emb[all]==0.0.25\"\n```\n\n----------------------------------------\n\nTITLE: Running Benchmarks for sentence-transformers, fastembed, and infinity\nDESCRIPTION: Commands to clone the infinity repository, checkout a specific tag, and run the benchmark script.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/benchmarking.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/michaelfeil/infinity.git\ncd infinity\ngit checkout tags/0.0.25\npython ./docs/benchmarks/simple_app.py\n```\n\n----------------------------------------\n\nTITLE: Running HuggingFace Text-Embeddings-Inference with CPU\nDESCRIPTION: Docker command to run the HuggingFace text-embeddings-inference service on CPU with the BAAI/bge-small model.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/benchmarking.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it -p 7997:80 --pull always ghcr.io/huggingface/text-embeddings-inference:cpu-0.6 \n--model-id BAAI/bge-small-en-v1.5 --max-client-batch-size 256\n```\n\n----------------------------------------\n\nTITLE: Running HuggingFace Text-Embeddings-Inference with CUDA\nDESCRIPTION: Docker command to run the HuggingFace text-embeddings-inference service with CUDA support for the BAAI/bge-large model.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/benchmarking.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it -p \"7997:80\" --gpus all --pull always ghcr.io/huggingface/text-embeddings-inference:89-0.6 \n--model-id BAAI/bge-large-en-v1.5 --max-client-batch-size 256\n```\n\n----------------------------------------\n\nTITLE: Launching Benchmarks via Make\nDESCRIPTION: Command to run all embedding benchmarks via the project's Makefile.\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/docs/docs/benchmarking.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake benchmark_embed\n```\n\n----------------------------------------\n\nTITLE: ECS Logging Configuration\nDESCRIPTION: Additional JSON configuration for AWS CloudWatch logging setup in ECS task definition\nSOURCE: https://github.com/michaelfeil/infinity/blob/main/infra/aws_neuron/README.md#2025-04-11_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"logConfiguration\": {\n        \"logDriver\": \"awslogs\",\n        \"options\": {\n            \"awslogs-group\": \"/ecs/ecs-infinity-neuron\",\n            \"mode\": \"non-blocking\",\n            \"awslogs-create-group\": \"true\",\n            \"max-buffer-size\": \"25m\",\n            \"awslogs-region\": \"us-west-2\",\n            \"awslogs-stream-prefix\": \"ecs\"\n        },\n        \"secretOptions\": []\n    }\n}\n```"
  }
]